{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from bert_score import score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, confusion_matrix\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "associations_valid = [('data/test_data/few_shot/test_data_2024_abstract_prompts_fewshot.jsonl', 'results/gpt4o/test_data_answer_not_in_prompt/test_data_2024_abstract_prompts_fewshot_gpt4o_results.jsonl', 'GPT4o-Abstract-Fewshot'),\n",
    "                      ('data/test_data/few_shot/test_data_2024_abstract_prompts_fewshot.jsonl', 'results/gpt4o/test_data_answer_not_in_prompt/test_data_2024_abstract_prompts_oneshot_gpt4o_results.jsonl', 'GPT4o-Abstract-Oneshot'),\n",
    "                      ('data/test_data/few_shot/test_data_2024_abstract_prompts_fewshot.jsonl', 'results/gpt4o/test_data_answer_not_in_prompt/test_data_2024_abstract_prompts_gpt4o_results.jsonl', 'GPT4o-Abstract-Zeroshot'),\n",
    "                      ('data/test_data/few_shot/test_data_2024_summary_prompts_fewshot.jsonl', 'results/gpt4o/test_data_answer_not_in_prompt/test_data_2024_summary_prompts_fewshot_gpt4o_results.jsonl', 'GPT4o-Summary-Fewshot'),\n",
    "                      ('data/test_data/few_shot/test_data_2024_summary_prompts_fewshot.jsonl', 'results/gpt4o/test_data_answer_not_in_prompt/test_data_2024_summary_prompts_oneshot_gpt4o_results.jsonl', 'GPT4o-Summary-Oneshot'),\n",
    "                      ('data/test_data/few_shot/test_data_2024_summary_prompts_fewshot.jsonl', 'results/gpt4o/test_data_answer_not_in_prompt/test_data_2024_summary_prompts_gpt4o_results.jsonl', 'GPT4o-Summary-Zeroshot')]\n",
    "\n",
    "associations_test = [('data/test_data/few_shot/test_data_2025_abstract_prompts_fewshot.jsonl', 'results/gpt4o/test_data_answer_not_in_prompt/test_data_2025_abstract_prompts_fewshot_gpt4o_results.jsonl', 'GPT4o-Abstract-Fewshot'),\n",
    "                     ('data/test_data/few_shot/test_data_2025_abstract_prompts_fewshot.jsonl', 'results/gpt4o/test_data_answer_not_in_prompt/test_data_2025_abstract_prompts_oneshot_gpt4o_results.jsonl', 'GPT4o-Abstract-Oneshot'),\n",
    "                     ('data/test_data/few_shot/test_data_2025_abstract_prompts_fewshot.jsonl', 'results/gpt4o/test_data_answer_not_in_prompt/test_data_2025_abstract_prompts_gpt4o_results.jsonl', 'GPT4o-Abstract-Zeroshot'),\n",
    "                     ('data/test_data/few_shot/test_data_2025_summary_prompts_fewshot.jsonl', 'results/gpt4o/test_data_answer_not_in_prompt/test_data_2025_summary_prompts_fewshot_gpt4o_results.jsonl', 'GPT4o-Summary-Fewshot'),\n",
    "                     ('data/test_data/few_shot/test_data_2025_summary_prompts_fewshot.jsonl', 'results/gpt4o/test_data_answer_not_in_prompt/test_data_2025_summary_prompts_oneshot_gpt4o_results.jsonl', 'GPT4o-Summary-Oneshot'),\n",
    "                     ('data/test_data/few_shot/test_data_2025_summary_prompts_fewshot.jsonl', 'results/gpt4o/test_data_answer_not_in_prompt/test_data_2025_summary_prompts_gpt4o_results.jsonl', 'GPT4o-Summary-Zeroshot')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gold_and_predictions(association):\n",
    "    with open(association[0], 'r') as f:\n",
    "        gold_standard = [json.loads(line) for line in f]\n",
    "        #gold_standard = json.load(f)\n",
    "\n",
    "    with open(association[1], 'r') as f:\n",
    "        predictions = [json.loads(line) for line in f]\n",
    "\n",
    "    gold_standard = [{item['paper_id']:json.loads(item['messages'][-1]['content'])} for item in gold_standard]\n",
    "    predictions = [{item['paper_id']:item['review']} for item in predictions if 'review' in item.keys()]\n",
    "    \n",
    "    return gold_standard, predictions\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 3., 3., 7., 4.],\n",
       "       [3., 3., 3., 7., 4.]])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_standard, predictions = get_gold_and_predictions(associations_valid[0])\n",
    "prediction = list(predictions[0].values())[0]\n",
    "#list([item for item in gold_standard if item.keys() == predictions[0].keys()][0].values())[0]\n",
    "arr = np.array([])\n",
    "arr = np.append(arr, [prediction[cat] for cat in numerical_scores])\n",
    "np.append(arr, [prediction[cat] for cat in numerical_scores]).reshape(-1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.666666666666667"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error([1,7, 0],[2,3, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The paper proposes a scheme to adapt gradient-based methods for weight-entangled spaces in neural architecture search (NAS). This integration of weight-entanglement and gradient-based NAS is a new approach that has not been explored before. The paper also presents a comprehensive evaluation of the properties of single and two-stage approaches, including any-time performance, memory consumption, robustness to training fraction, and the effect of fine-tuning.',\n",
       " '1. Contrastive learning has demonstrated promise in the field of computer vision; this paper shows a novel application of this concept to the field of Biology.\\n\\n2. The paper has a strong theoretical foundation and is well presented. \\n\\n3. Quantitative results are convincing and promising.',\n",
       " '1. This paper is overall clearly clarified and well organized.\\n2. The use of adapters for lifelong test-time adaptation seems novel.',\n",
       " 'The authors introduce a method that selects features based on neural estimation of mutual information.\\n\\nThe method outperforms existing techniques in two synthetic datasets.',\n",
       " '* The approach is novel\\n* The authors ran several experiments with good ablation.',\n",
       " 'This paper proposed conditional adversarial support alignment (CASA) to minimize the conditional symmetric support divergence between the source’s and target domain’s feature representation distributions. Generally, the paper is well-written and easy to follow. They evaluate the model on several benchmarks from various types of results.',\n",
       " '1. The authors have considered somewhat large language models for the evaluation. It would be very grateful if the authors could compare it on a bigger scale, e.g., llama2 70b-chat, falcon-40b instruct.',\n",
       " '1. Overall, the paper presents a novel and promising approach to handling label noise in graph neural networks based on probabilistic graphical model.\\n\\n2. The sequentially introduced models PRGNN-V1 and PRGNN-V2 are clearly depicted and the proposed objectives are well motivated. \\n\\n3. Experiments are conducted on both noisy Homophilous and Heterophilous graphs. Clear improvements are observed compared to the implemented methods.',\n",
       " '**Clear Structure and Writing:** The paper benefits from a clear structure and concise writing style.\\n\\n**Addressing a Complex Issue:** The authors tackle an underexplored yet challenging problem, and I appreciate their efforts to address online continual learning.\\n\\n**Mathematical Foundation:** The definition of utility introduced in the paper is based on simple and sound mathematical derivations. \\n\\n**New metric:** The introduction of a new plasticity metric is a nice contribution to the relatively uncharted territory of streaming learning.',\n",
       " 'The paper addresses the robustness of widely used models, a pertinent topic given the real-world deployment of these models.',\n",
       " '+ The paper is well-written and easy to follow. Illustrative figures are well plotted and easy to understand.\\n+ Co-designing the hardware MAC architecture for the proposed data type SuFP enables better hardware efficiency.\\n+ The evaluation benchmarks are diverse, including both vision and language tasks.',\n",
       " '- Originality and Significance: The latent parametrization that makes FARZI optimization friendly, and the proposed trick that enables reverse mode differentiation of Adam such that its memory complexity is independent of the number of inner loop steps are great contributions and of practical value.\\n- Quality and Clarity: The paper is well written with extensive experiments whose details and evaluations are that are clearly described. The results are impressive. The method is able to achieve better performance on downstream tasks compared with using the full dataset.',\n",
       " '- The paper proposes an algorithm for offline RL with non-linear function approximation that has instance-dependent regret guarantees, which is new in literature.\\n- The author extend techniques used for linear MDPs (including variance-weighted ridge regression and reference-advantage decomposition) to nonlinear function classes.\\n- The paper is generally well-written and clear.',\n",
       " 'The problem setup are well-motivated: using game to simulate LLMs behaviors and as a tool to assess their intelligence level is definitely a very promising direction. The presentation is clear.',\n",
       " 'the authors provide conditions under which using square-based losses will lead a wrong DAG result in structure learning. The conclusions drawn in the paper are reasonable.',\n",
       " '1. The proposed method seems to be accurate and efficient in practice.',\n",
       " 'The proposed approach serves as a plug-in method, resulting in enhanced versions of Llama 2 and ChatGPT that outperform their original counterparts in terms of generalizability and factuality. SuperContext can bring decent performance benefit compared to few-shot in-context learning and outperform original SLMs and LLMs with both zero-shot and few-shot settings.',\n",
       " '1.This work extends the parameter perturbation in existing SAM optimization to data perturbation, achieving loss landscape alignment between source domains and unknown domains. Experiments show the validity of the proposed objective.  \\n\\n2.This work establishes an upper bound for DG by merging SAM optimization with the proposed objective.\\n\\n3.The proposed objective can be combined with multiple SAM optimizers and further enhance their performance, demonstrating the necessity of the loss landscape consistency between source and unknown domains.',\n",
       " '- Promising idea support by theory proofs \\n- The newly introduced method does not cause additional significant computational cost. \\n- Nice application demo on multiple vision tasks.',\n",
       " '1. The motivation for the architecture is interesting and reasonable.\\n2. The modular nature of the reasoning pipeline makes the overall steps seem intuitive. \\n3. The experiments show improved performance on several benchmarks over other methods that do not use explicit parallel / iterative reasoning.',\n",
       " '- originality: This is a new application domain for SSL as far as I know, but the methodology is not particularly new.\\n\\n- quality: The submission is well written and formatted, with strong experimental results and ablations.\\n\\n- clarity: most of the training details are there, though much is relegated to the appendix.\\n\\n- significance: the work is significant as an application of SSL to a specific domain with greater performance, and finds relevance in the current post-pandemic context.',\n",
       " 'The paper tackles an important problem. It has a detailed discussion of related works. Performance improvements are consistent.',\n",
       " 'The main strength of the paper is its approach to tackling the important problem of disentangled representation learning, which may contribute to reducing carbon footprint.',\n",
       " '- Open sourcing the model is going to help the research community\\n- The model shows decent multimodal capability, especially in terms of bounding box reasoning',\n",
       " \"+ The introduction of behaviour distillation can enrich the literature and direction of Dataset Distillation. Different from standard DD, behaviour distillation does not require access to the expert datasets. This is quite close to a standard /basic RL setting and could be a good starting point.\\n\\n+ The author's writing is easy to follow and pretty clear on the technical details\\n\\n+ The experimental section show promising results using the proposed algorithm HADES.\",\n",
       " '- The paper is well written and easy to follow, with sufficient literature references and reasonable and intuitive design in terms of extended controlling space.\\n\\n- The findings illustrated in Fig. 3 in terms of per-layer prompting is interesting.',\n",
       " '- To the best of my own knowledge, this paper is the first attempt to use multiple source hypotheses in SFUDA and to transform SFUDA into a semi-supervised learning task. They provide a novel view and framework for SFUDA.',\n",
       " 'Strengths: \\nOverall, the paper is well written and reads easily. \\nThis work presents one of the first approaches to post-hoc interpretation of structured output prediction. The proposed approach applies when the output to be predicted is a binary vector which includes a broad variety of tasks like multi-label classification, semantic segmentation... Any structured output method that predicts a bag of items for instance (bag of substructure) will be also eligible (even if not considered in this work, except for text).\\nThe optimization problem solved to learn the interpreter is appealing with a nice way to rely on the difference of energies associated to the perturbation of inputs by the interpreter. This is really the strong novelty of the paper, for me far beyond its application to structured output prediction.',\n",
       " 'Addressing the use of simpler methods to match and outperform complex DNN-based architectures.',\n",
       " '**Strength:**\\n\\n-   The motivation that optimizing both adversarial and natural objectives through feature extractor yields divergent gradient directions make sense. The proposed disentangling of the training objective by introducing the LoRA branch is consistent with the motivation.\\n-   Extensive empirical evaluation (including the P-test) demonstrates the improvements in Robust Fine-Tuning on various tasks.',\n",
       " '1. The paper is well-written and organized. As compared to previous UGS, the proposed AdaGLT jointly sparsifies both graphs and GNN weights and adopts an adaptive layer sparsification process. In addition, it offers the practitioner the chance to adopt automatic pruning and dynamic restoration for extracting the graph lottery tickets.\\n\\n2. The effective combination of automation, layer adaptivity, and dynamic pruning/restoration provides better properties as compared to previous methods.',\n",
       " '1. Very sound elaboration and experiments. The proposed improvement is reasonable and effective.\\n2. The method is open sourced.',\n",
       " '1. The study of applying AP to different layers provides interest insights related to model interworks such as attention distance.\\n2. Experimental results are presented with many datasets, settings, and compared with various benchmarks.',\n",
       " 'The new benchmark is a valuable contribution to the continual learning community. The method has a strong neuroscientific grounding and it brings together existing components in an original way. The paper is well presented and nicely structured. The writing is clear and the figures are very helpful in conveying the main points of the argument. The ablation study provides sufficient justification for the individual design choices.',\n",
       " '- The statistical analysis performed in the paper is sound and written in a manner that is easy to follow. The authors include a helpful intuitive understanding of the bias term that connects it back to the empirical observations about the behavior of OOD detection models under the class-imbalanced setting.\\n- ImOOD is well-motivated and theoretically-grounded on the the above analysis. The general thinking behind the method is sound, and for the most part, it’s described clearly enough to understand and re-implement the design.',\n",
       " '1. This paper proposes a simple and effective method for detection adversarial examples for ASR systems.\\n2. The paper is presented with comprehensive experiments. The authors not only present the benign detection performance of adversarial attacks but also analysis the robustness under adaptive attacks with known detection method.',\n",
       " '- sufficiently clearly written paper\\n- natural idea of the algorithm\\n- detailed description of the algorithm and experimental study \\n- discussion of the features of the computational implementation of the algorithm\\n- interesting practical results',\n",
       " \"The paper's originality derives from improvements it makes in the class of AR-HMM and SLDS models by removing exchangeability without removing input recurrence. This enables greater efficiency with expressibility. Overall, this work nicely integrates the heat-equation generalization and recurrent switching state space models. They present overall improvements on two different experiments with greater efficiency of parameters. Visuals are strong, particularly ones pertaining to the flow fields and the switching states of the rSLDS and irSLDS.\",\n",
       " 'I like this paper. It is well written, it is interesting, the method seems sensible (thought I have a few questions), and the results look good.',\n",
       " '1. The presented method achieves competitive performance while training models from scratch (with the exception of using DINO for pseudo-supervision).\\n2. The paper investigates performance scaling with increasing dataset size, showing improvements. \\n3. The writing is clear, and the authors promise to release code and checkpoints soon.\\n4. The method obtains competitive results.',\n",
       " '[S1] The references and related works cited by the authors are very recent and relevant.\\n\\n[S2] The Lesion study presented in Sec.4.1 is comprehensive for demonstrating the proposed shared attention is highly related to ensemble behavior of the sequentially placed attention mechanism. Deleting or permutating some self-attention layers could smoothly increase the reconstruction loss. This can be the evidence that the shared attention is not the ensembles of shallow networks.\\n\\n[S3] The performances of ShareFormer for large SR and lightweight SR are notably enhanced, compared to recent SOTA methods. These results were achieved with smaller model size, fewer computations, and faster speed than the others. (But an unfair issue remain. See question Q4.)',\n",
       " '1. This paper presented a new perspective that extends a pre-trained diffusion model into an arbitrary-scale super-resolution by adding a specific amount of noise to LR images before the diffusion model backward diffusion process. The idea is interesting.\\n2. This paper proposed a metric Perceptual Recoverable Field for achieving the optimal trade-off between fidelity and realness.',\n",
       " \"--comprehensive evaluations on a few different RLHF settings/datasets for helpfulness optimization\\n\\n--nice in-depth exploration confirming the extent of a phenomenon (length correlations for helpfulness optimization) that i think hasn't been comprehensively analyzed previously\",\n",
       " '1.\\tThe general motivation for analyzing the generalization of evolving graphs is interesting and important. It is also interesting to see the theoretical analysis with a synthetic random graph demonstration with a closed-form generalization error.\\n2.\\tThe presentation quality of this paper is in general satisfactory, in terms of the organization and figure demonstration.\\n3.\\tThe experiments and analysis on the proposed method itself are relatively sufficient including model component ablation and hyper-parameter study.',\n",
       " 'Comprehensive numerical results are provided to support the proposed method',\n",
       " '+ The proposed benchmark covers three CL scenarios and can be beneficial to the community. \\n+ The analysis in Section 3 makes sense and provides empirical evidence for the superiority of multiple modalities over single modality in CL.\\n+ The paper has good motivation and is well organized.',\n",
       " '- This paper is well-written and easy to follow.\\n- The empirical analysis is comprehensive and solid regarding datasets and backbone models.\\n- The proposed method is simple yet effective, providing a solid and lightweight framework to solve zero-shot text classification problems.',\n",
       " 'S1. The provision of an open-access, real-world dataset in this article has a favorable and constructive influence on research within the domain of Pickup & Delivery.\\n\\nS2. Following the release of the dataset associated with this article, it can be employed to corroborate and verify findings from research papers that had not previously made their datasets openly available.\\n\\nS3. The dataset introduced in this article incorporates measures to protect crucial user privacy information.',\n",
       " '- Being able to mine large data for pairs is a relevant task in representation learning.\\n\\n- They outperform multiview habitat on their evaluations.\\n\\n- The method, being based on classical techniques like SIFT and RANSAC, should scale well.\\n\\n- The method is simple but effective.',\n",
       " \"While alternative splicing modeling has been extensively studied, systematic methods for designing/altering sequences are relatively new, and the authors' Bayesian optimization method appears promising.  RBP knockdown analysis is also interesting. The authors have also undertaken a significant amount of data processing and batch correction to facilitate model training and validation.\",\n",
       " '- Transferring multi-modal knowledge learned by multiple streams to a single-modality network via knowledge distillation seems to be an innovative and practical idea in the CSLR community. The advantage of this method is that while enjoying multi-modal knowledge from the teacher model, the student model itself does not need to take additional modality inputs such as optical flow and pose, where such extra modalities actually incur computational overhead during inference time.\\n\\n- The presentation for the effect of each proposed component is quite clear from both teacher-side and student-side. For the teacher side, improvements are gradually observed when more modalities are combined, and more fusion layers are applied (Table 3). From the student-side, similar to the teacher model, the performance increases when the knowledge distillation loss terms are applied in feature levels and the output-level (Table 4b). \\n\\n- Compared to the provided previous CSLR methods, the proposed scheme achieves strong results in both cases of multi-modal network and single-modal network in some standard CSLR benchmarks.',\n",
       " '+This submission proposes a unified grid-based representation that incorporates geometry and material properties. The grid-based representation has shown to be more friendly for optimization.\\n\\n+Optimization becomes more straightforward with shallower MLPs. The combination of grid-based volume representation and shallow MLPs leads to more efficient optimization. \\n\\n+Spherical Gaussian (SG) representation exhibits higher-order frequency characteristics for illumination compared to spherical harmonics (SH).',\n",
       " '1. The paper builds on successful recent advances in diffusion models, which excel in high-fidelity image generation within generative learning frameworks. Thus, the proposed TIW method shows promise for diverse applications, including text-to-image generation, image-to-image translation, and video generation.\\n2. The paper addresses the understudied issue of dataset bias in generative modeling, introducing a method to enhance the fairness and reliability of ML systems.\\n3.The paper establishes a theoretical equivalence between the proposed method and existing score-matching objectives from unbiased distributions, which could provide a strong foundation for the validity of their approach.',\n",
       " 'The paper presents several novel ideas, by combining RBF networks, sparse regressison trees, and online updating for time series prediction.',\n",
       " '- This paper studies the important problem of utilizing and managing spatial and temporal memory experienced by embodied agents that are essential to address various downstream tasks. Particularly, it considers the practical issue of memory constraint and perform experiments based on the popular transformer architectures. The research presented in this paper is very well motivated.\\n\\n- The paper is very technical solid. Almost all arguments are well-supported/justified by the highly-relevant and classic references and thorough experiments (in Appendix). It carefully compares place- and time-centric store and read, and progressively studies FIFO to the proposed Adaptive Memory Allocator based on SAT. I believe the extensive settings and results presented in this paper have the potential to inspire many future works.\\n\\n- Overall, this paper was a very enjoyable read to me. All details have been clearly presented (especially with the Appendix and all nice visualizations); The paper is nicely-structured, concise but contains massive valuable information. I believe many arguments and thoughts presented in this paper will be very constructive to relevant future research.',\n",
       " '* This work considers the problem of interpreting the intermediary layers and controlling the output of transformer networks, which are of significant interest to the machine learning community. Furthermore, quantizing features with codebooks is a popular technique which the work demonstrates leads to a minimal degradation in model performance.\\n* Experiments with the TokFSM dataset are intuitive and clearly demonstrate the ability to intervene in the output of a transformer trained with codebook quantization. Experiments such as the JS divergence with the target token distribution in Figure 4, are used to demonstrate effective intervention. Given that one has access to the semanticity of entries in the learned codebook, the proposed method seems to be effective at steering the output of transformer network.\\n* The work contains a comprehensive related works section in the appendix that clarifies the benefits of a discrete codebook over using a dictionary approach (referred to as “features-as-directions”)',\n",
       " '1. The problem, partially annotated multi-label classifcation, is novel.\\n\\n2. A novel method named PAPG is proposed, which can overcome the challenges associated with a scarcity of positive annotations and severe label imbalance. \\n\\n3. Experiments validate the effectiveness of the proposed PAPG method.',\n",
       " '1. The authors present a novel pairwise data synthesis pipeline by extracting pseudo-ligand-pocket pairs from protein data\\n2. The authors develop large-scale datasets and new pretraining methods to exploit the full potential of pocket representation learning, emphasizing the interactions between pockets and ligands.\\n3. ProFSA achieves significant performance gains in a variety of downstream tasks.',\n",
       " 'The paper is well-written and easy to understand.  There are good explanations for each step of the approach.  The \"Transitional Representation\" section does a really good job of approaching the symbolic and neural representations.  \\n\\n\\nThe method is topical and will be of interest to the ICLR community and the method seems to be novel for how to produce a dictionary of neuro-symbolic part segmentation.  \\n\\nI really like the overarching goal for self-supervised part segmentation and the method seems to attack the problem directly.  The neural symbolic approach to ML has been of interest for a while and part segmentation is a good problem to apply it towards.',\n",
       " \"- Originality: The Generalizable Neural Point Field (GPF) is a fresh perspective in NeRF research, emphasizing point-based over image-based rendering.\\n- Quality: The proposed methods exhibit high-quality research and innovation, from the nonuniform log sampling strategy to the feature-augmented learnable kernel.\\n- Clarity: The paper's overall structure is clear, presenting a logical flow of ideas and discussions.\\n- Significance: If the claims are validated further, this research could serve as a benchmark in the NeRF domain.\",\n",
       " '- The authors have demonstrated the cause of NCC through the use of label encoding and have further enhanced it to improve network performance. \\n- The evidence supporting NCC and LRM is explicitly presented.\\n- Experimental results show that LRM achieves good performance in various tasks such as semi-supervised learning (SSL), unsupervised domain adaptation (UDA), and semi-supervised heterogeneous domain adaptation (SHDA).',\n",
       " 'The subject explored in this paper is of profound interest. It poses an essential and inherent inquiry regarding the maintenance of stability for the Jacobian matrix in neural networks from initialization throughout training. This is the inaugural study on this particular topic.\\n\\nThe manuscript is well written and has a clear structure.\\n\\nThe paper employs novel methodologies to examine the spectrum of the Jacobian matrix. The findings from Bandeira et al. (2021) are indeed aptly integrated into the analysis of neural networks and warrant recognition within the learning theory community.',\n",
       " 'The solution of using latent Gaussian mixture model in the encoder to tackle the close-set recognition is valid.',\n",
       " 'Most works in MDTC are empirical, while this paper, to this reviewer, appears to be the only one that is strongly justified theoretically.  Bringing the approach of [Zhang et al, 2019] to the context of MDTC is the key contribution of this work. Extending the development of [Zhang et al, 2019] from domain adaptation to MDTC as done in this paper is natural and sound.\\n\\nThe paper is in general well written, and an easy read.',\n",
       " '1.\\tAn approach the optimizes both diversity and representativeness has advantage\\n2.\\tSome theoretical results support the advantage of the method.\\n3.\\tEmpirical results show some advantage over Random baseline and for cifar 10 clear advantage to using flexMatch with UPA',\n",
       " '* This work is clearly written and proposes a principled and efficient method of solving the non-uniformity of gradients during backward propagation.\\n* Whereas previous works have proposed log scaling of values, this is impractical to implement efficiently on hardware. Also, it is difficult to guess the necessary value range beforehand. The Hadamard transform is both efficient to implement and is ideally suited to removing gradient spikes while preserving information. As such, there is potential for Hadamard domain backpropagation in reduced-precision training. \\n* In the backpropagation process, utilizing HDQT allows setting the input and accumulation bit-width to 4 and 8 bits, respectively, showcasing improved energy efficiency compared to conventional CIL methods.\\n* The authors present promising results on image datasets (CIFAR100) and human activity recognition datasets (DSADS, PAMAP2, HAPT), demonstrating a degradation of ≤ 2.5% through quantization in vision tasks.',\n",
       " '1. The writing is clear and intuitive. It was easy to follow the technical arguments, and I believe they are correct (did not look at some of the full proofs though).\\n2. Experimentally, the mechanism proposed here seems to outperform the existing approach from Bravo-Hermsdorff et al., and slightly outperform the open source implementations for locally private heavy hitters.',\n",
       " \"1. The paper is very clear and pleasant to read.\\n2. The integration of various existing frameworks for the task of molecules' ground state conformation is original to some extent.\\n3. The performances compared to the baselines are significant.\",\n",
       " '1. The paper proposes a novel task, text-to-audio storytelling. The task opens up a huge space for AI-powered general audio creation.\\n\\n2. The proposed system outperforms or achieves on-par  performance against previous SOTA methods, i.e., AudioGen and AudioLDM, in both objective and subjective metrics.\\n\\n3. The new established benchmark is another good contribution.\\n\\n4. The developed prototype and demos look amazing.',\n",
       " '1. The motivation is convincing that availability poisoning attack should be effective for different learning paradigms.\\n2. The proposed algorithm improves the transferability of poisoning attack across evaluation algorithms including SL, CL, Semi-Supervised Learning on CIFAR-10, CIFAR-100 and Tiny-ImageNet.\\n3. This paper provides plenty of investigation on alternative methods including HF, CC, SCP, and WTP.',\n",
       " '- Estimate the Lipschitz constant of GNNs via Jacobian matrix\\n- Comprehensive experiments on GNNs for the bias',\n",
       " 'This article presents an improvement other than the theoretical framework of non-contrastive learning using solely the Euclidian loss.\\nThe paper is well-written and easy to follow. The assumptions taken are relatively well justified and allow for an interesting analysis.',\n",
       " '- The paper gives a good introduction, formal treatment and extensive background of the basic concepts and background literature on Gaussian continuation',\n",
       " '1. The studied problem is interesting and significant for distilling black-box models into user-accessible models. \\n\\n2. The authors provide thorough experimental details for reproducing this paper.',\n",
       " '1) This paper is generally presented well;\\n\\n2) The idea of use cost score matrix for both self- and cross-attention feature updating is simple and effective;\\n\\n3) Experimental results are good.',\n",
       " '+ The paper discusses robustness for the first time in the relatively new meta-learning problem setting of DFML.\\n+ The paper proposes an effective method for replaying past tasks in DFML and confirms its effectiveness through experiments.\\n+ The paper proposes a reinforcement-learning-based model selection method that can maintain meta-testing performance even when unreliable models are included.',\n",
       " '- This paper studies a very interesting problem: what values are hold in the LLMs. Understanding the correct way to probe and measure this will be beneficial to the whole community.\\n- To me, the way the authors proposed to achieve this goal is novel.',\n",
       " 'Discretizing planning into sub-goals with diffusion models is shown to be advantageous as more diverse scenarios can be solved. Low-level diffusion planning becomes task-agnostic.\\n\\nEven the sparse diffuser or SD version is better than diffuser because of increased receptive field. The authors validate this by showing that increasing the kernel-size (hence the receptive field) of diffuser leads to better performance but weaker generalization.\\n\\nSD with dense actions leads to better fitting of the sparse objective, a typical bottleneck in hierarchical RL.\\n\\nHD takes lesser time than Diffuser due to shorter high-level sequence and parallelly solving low-level plans for all the segments.',\n",
       " '- The derivation of the CAM and saliency map of SNNs itself is a clear contribution. The results in Table 1 prove the correctness of this\\n- EventRPG is able to consistently improve performance across event datasets\\n- The time cost of EventRPG is comparable to similar augmentations in conventional vision',\n",
       " '* The paper combines strategies from voice conversion and singing voice conversion to improve zero-shot voice conversion. One noteworthy contribution is the introduction of a novel training strategy that utilizes self-synthesized examples for data augmentation and iterative improvement of the generation model. This represents a significant advancement from traditional approaches that heavily relied on heuristic transformations.\\n\\n* The paper conducts comprehensive experiments. By extensively comparing the proposed framework with several baseline models, the authors effectively demonstrate its efficacy. \\n\\n* Overall, the paper successfully presents a valuable contribution to the field of voice conversion and offers an approach for achieving zero-shot voice conversion.',\n",
       " '1. Combining LLM and MPC is a smart way to leverage the high-level reasoning capability of LLM. Since LLM is not good at low-level control as shown in previous literature, using MPC to output low-level actions helps incorporate strong prior knowledge into the whole pipeline.\\n\\n2. Rare and long-tail events put great challenges to data-driven algorithms. Using common sense to solve such long-tail problems is a promising way. Since LLM already contains a lot of common sense, this paper is a good attempt to explore this direction.\\n\\n3. The paper is well-written and easy to follow. Figure 2 provides a clear illustration of the entire pipeline, which makes me easy to understand the method proposed in this paper.',\n",
       " '- As the work mentions, this is the first work which has simplified the transformer architecture with training throughput gains. Previous works which have tried to simplify transformer architectures have led to increase in training speeds.\\n- The paper is overall well written. The authors discuss each design choice in detail, which helps in understanding the motivation behind the simplifications proposed.\\n- The authors have appropriately discussed the limitations of their work.',\n",
       " '1. The paper studies an important problem.\\n2. The paper proposes a new approach to calculate gradients in convex QP layers.',\n",
       " 'Proposed a new framework to solve robust contextual stochastic optimization problem. Please see my comments in Summary for details.',\n",
       " 'S1: I like their presentation and language writing, which is clear.\\nS2: they proposed two-step search algorithm, which leverages semantical similar- ity for demo assignment and routing function and region-based joint search for instruction assignment, achieves significant performance gains on the APE Benchmark.',\n",
       " '1. (main) Conceptually interesting: a richer understanding of regularization for both mean and deviation.\\n2. Uncertain quantification: an important topic, this paper provides some ideas about how to tame overparametrization',\n",
       " 'This paper looks at an interesting problem: Unsupervised domain adaptation is a very relevant problem (e.g., in many cases practitioners have labelled datasets available from a curated/lab setting, but need to generalize to actual deployment conditions). At the same time, self-supervised learning has become very popular, but its performance in the context of unsupervised domain adaptation (UDA) is still unclear.\\n\\nThis paper introduces a new dataset (RedShifts), and contains some encouraging results on two different domains (camera trips and astronomical observations) while performing several relevant ablations (model scale, whether to train the last layer before fine-tuning, strength of pre-training augmentations).\\n\\nThe paper is relatively easy to read, with good writing and a clear structure.',\n",
       " '1.\\tAddress the privacy issue in the LLM augment generation process.\\n2.\\tThe design provides security guarantees for both users and external information supplier.',\n",
       " '1. It is reasonable to utilize the surface where the point clouds should be to compute the distance of two 3D point clouds.\\n2.The experiments show the proposed method improves the performance of all the downstream tasks.\\n3. The implementation of methods of all the downstream tasks are explained in detail.',\n",
       " 'This paper proposes a relatively high efficiency approach to training LLMs. It is impressive to get such results from a model trained on 8 A100s in 4 days. This makes this work approachable from academic research labs, which takes a step towards reversing the trend of pursuing ever larger datasets with larger models and computational requirements. This also has implications for energy efficiency and sustainability.\\n\\nThe paper itself is well-written and quite clear.\\n\\nThe authors have commited to the release of the \"model for usage and evaluation by the broader community\". This is important for open science and a big strength.',\n",
       " '- The authors highlight the task retrieval and task learning aspects of ICL. \\n- The authors provide risk bounds for both the setup under a mixture of linear Gaussian generative model.',\n",
       " '- The authors perform extensive experimentation on synthetic datasets, one dataset with real label noise, and compare to a large number of methods. \\n- The convergence analysis is a great contribution to the paper. \\n- The paper is well-written. Before getting to the experiments section, I was curious about whether or not the means of simplex projection for \\\\alpha and \\\\beta mattered in the method, since the authors perform an approximation. The authors seem to have pre-empted this potential question of mine, as they include an ablation that evaluates using the softmax of \\\\alpha and \\\\beta.',\n",
       " '1.The physical perspective of using fluid equations to elucidate the modeling of probability distributions in generative models is highly innovative. It provides a theoretical foundation for better analyzing existing generative models and designing more optimal ones.\\n\\n2.The article offers a comprehensive mathematical analysis of using fluid equations to explain diffusion models and Poisson flow models, providing strong evidence for this analytical approach.\\n\\n3.The proposed model achieves a good balance between audio quality in generation and generation speed. The provided samples sound of good quality.',\n",
       " \"- **Noteworthy Contribution**: The paper introduces a novel framework that has the potential to address the challenges of achieving robust and interpretable classifiers. This contribution is particularly relevant in the context of existing classifiers that often struggle with the trade-off between robustness and interpretability.\\n\\n- **Varied Evaluation**: The paper's assesses the proposed method using different model architectures, including convolutional networks and transformer models. This varied evaluation demonstrates the adaptability of the approach across different scenarios and model types, highlighting its potential for broader applicability.\",\n",
       " 'The paper certainly represents a significant amount of work by the authors, especially in the experimental section. The related work section is also extensive.',\n",
       " \"- The author provides very clear examples to introduce their method, which allows me to quickly understand the work done in this paper. In particular, Figure 1 helps me to understand the work done in this paper and how it is implemented as soon as I see it.\\n\\n- The author's formulas are very clear, especially the repeated declarations of symbols in each formula, which prevents me from having to flip back to the beginning of the paper to find the definition of a symbol when I encounter it later in the paper.\",\n",
       " 'Strong Points:\\n- Dual node encodings in DiGT elegantly capture directionality throughout the model.\\n- Directional attention neatly exploits query/key asymmetry for modeling direction.\\n- DiGT significantly outperforms baselines on directional graphs.\\n- Quantifies dataset directionality via entropy measure.\\n- New directional graph datasets enable better evaluation, although it may be biased in the context of this work.',\n",
       " '1. The studied restricted black-box attack is a practical scenario and an important problem.\\n2. The motivation of this paper is clear and the idea of introducing the Silhouette Score to measure the quality of a graph is interesting.\\n3. Extensive experiments are conducted.']"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_text[:,0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tensor(0.8499)'"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(torch.mean(score(gold_text[:,0].tolist(), pred_text[:,0].tolist(), lang=\"en\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bertscore(gold_cat, pred_cat):\n",
    "    bert = score(gold_cat.tolist(), pred_cat.tolist(), lang=\"en\")\n",
    "    mean_precision = torch.mean(bert[0])\n",
    "    mean_recall = torch.mean(bert[1])\n",
    "    mean_f1 = torch.mean(bert[2])\n",
    "    return mean_precision, mean_recall, mean_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT4o-Abstract-Fewshot MSE_soundness = 0.56\n",
      "GPT4o-Abstract-Fewshot MSE_presentation = 0.63\n",
      "GPT4o-Abstract-Fewshot MSE_contribution = 0.71\n",
      "GPT4o-Abstract-Fewshot MSE_rating = 5.6\n",
      "GPT4o-Abstract-Fewshot MSE_confidence = 0.95\n",
      "GPT4o-Abstract-Fewshot Strengths_BertScore = tensor(0.8437) / tensor(0.8397) / tensor(0.8415)\n",
      "GPT4o-Abstract-Fewshot Weaknesses_BertScore = tensor(0.8116) / tensor(0.8472) / tensor(0.8288)\n",
      "GPT4o-Abstract-Fewshot Questions_BertScore = tensor(0.8220) / tensor(0.8298) / tensor(0.8255)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT4o-Abstract-Oneshot MSE_soundness = 2.1818181818181817\n",
      "GPT4o-Abstract-Oneshot MSE_presentation = 0.6363636363636364\n",
      "GPT4o-Abstract-Oneshot MSE_contribution = 2.757575757575758\n",
      "GPT4o-Abstract-Oneshot MSE_rating = 9.929292929292929\n",
      "GPT4o-Abstract-Oneshot MSE_confidence = 0.8181818181818182\n",
      "GPT4o-Abstract-Oneshot Strengths_BertScore = tensor(0.8437) / tensor(0.8412) / tensor(0.8423)\n",
      "GPT4o-Abstract-Oneshot Weaknesses_BertScore = tensor(0.8128) / tensor(0.8592) / tensor(0.8351)\n",
      "GPT4o-Abstract-Oneshot Questions_BertScore = tensor(0.8077) / tensor(0.8265) / tensor(0.8166)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT4o-Abstract-Zeroshot MSE_soundness = 0.56\n",
      "GPT4o-Abstract-Zeroshot MSE_presentation = 2.21\n",
      "GPT4o-Abstract-Zeroshot MSE_contribution = 2.77\n",
      "GPT4o-Abstract-Zeroshot MSE_rating = 9.92\n",
      "GPT4o-Abstract-Zeroshot MSE_confidence = 0.81\n",
      "GPT4o-Abstract-Zeroshot Strengths_BertScore = tensor(0.8402) / tensor(0.8550) / tensor(0.8474)\n",
      "GPT4o-Abstract-Zeroshot Weaknesses_BertScore = tensor(0.8028) / tensor(0.8615) / tensor(0.8309)\n",
      "GPT4o-Abstract-Zeroshot Questions_BertScore = tensor(0.8156) / tensor(0.8616) / tensor(0.8376)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT4o-Summary-Fewshot MSE_soundness = 2.2\n",
      "GPT4o-Summary-Fewshot MSE_presentation = 0.63\n",
      "GPT4o-Summary-Fewshot MSE_contribution = 0.71\n",
      "GPT4o-Summary-Fewshot MSE_rating = 9.92\n",
      "GPT4o-Summary-Fewshot MSE_confidence = 0.81\n",
      "GPT4o-Summary-Fewshot Strengths_BertScore = tensor(0.8391) / tensor(0.8399) / tensor(0.8394)\n",
      "GPT4o-Summary-Fewshot Weaknesses_BertScore = tensor(0.8060) / tensor(0.8489) / tensor(0.8267)\n",
      "GPT4o-Summary-Fewshot Questions_BertScore = tensor(0.8153) / tensor(0.8239) / tensor(0.8192)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT4o-Summary-Oneshot MSE_soundness = 2.2222222222222223\n",
      "GPT4o-Summary-Oneshot MSE_presentation = 2.2222222222222223\n",
      "GPT4o-Summary-Oneshot MSE_contribution = 2.787878787878788\n",
      "GPT4o-Summary-Oneshot MSE_rating = 10.02020202020202\n",
      "GPT4o-Summary-Oneshot MSE_confidence = 0.8181818181818182\n",
      "GPT4o-Summary-Oneshot Strengths_BertScore = tensor(0.8519) / tensor(0.8455) / tensor(0.8486)\n",
      "GPT4o-Summary-Oneshot Weaknesses_BertScore = tensor(0.8153) / tensor(0.8536) / tensor(0.8338)\n",
      "GPT4o-Summary-Oneshot Questions_BertScore = tensor(0.8188) / tensor(0.8281) / tensor(0.8230)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT4o-Summary-Zeroshot MSE_soundness = 2.2\n",
      "GPT4o-Summary-Zeroshot MSE_presentation = 2.21\n",
      "GPT4o-Summary-Zeroshot MSE_contribution = 2.77\n",
      "GPT4o-Summary-Zeroshot MSE_rating = 9.92\n",
      "GPT4o-Summary-Zeroshot MSE_confidence = 0.81\n",
      "GPT4o-Summary-Zeroshot Strengths_BertScore = tensor(0.8459) / tensor(0.8564) / tensor(0.8510)\n",
      "GPT4o-Summary-Zeroshot Weaknesses_BertScore = tensor(0.8072) / tensor(0.8599) / tensor(0.8325)\n",
      "GPT4o-Summary-Zeroshot Questions_BertScore = tensor(0.8128) / tensor(0.8449) / tensor(0.8281)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numerical_scores = [\"Soundness\", \"Presentation\", \"Contribution\", \"Rating\", \"Confidence\"]\n",
    "text_fields = [\"Strengths\", \"Weaknesses\", \"Questions\"]\n",
    "for association in associations_test:\n",
    "    gold_standard, predictions = get_gold_and_predictions(association)\n",
    "\n",
    "    gold_scores = []\n",
    "    pred_scores = []\n",
    "\n",
    "    gold_text = []\n",
    "    pred_text = []\n",
    "    for prediction in predictions:\n",
    "        gold = list([item for item in gold_standard if item.keys() == prediction.keys()][0].values())[0]\n",
    "        prediction = list(predictions[0].values())[0]\n",
    "        gold_scores.append([gold[category] for category in numerical_scores])\n",
    "        pred_scores.append([prediction[category] for category in numerical_scores])\n",
    "\n",
    "        gold_text.append([gold[category] for category in text_fields])\n",
    "        pred_text.append([prediction[category] for category in text_fields])\n",
    "    gold_scores = np.array(gold_scores)\n",
    "    pred_scores = np.array(pred_scores)\n",
    "\n",
    "    gold_text = np.array(gold_text)\n",
    "    pred_text = np.array(pred_text)\n",
    "\n",
    "    mean_precision_strengths, mean_recall_strengths, mean_f1_strengths = calc_bertscore(gold_text[:,0], pred_text[:,0])\n",
    "    mean_precision_weaknesses, mean_recall_weaknesses, mean_f1_weaknesses = calc_bertscore(gold_text[:,1], pred_text[:,1])\n",
    "    mean_precision_questions, mean_recall_questions, mean_f1_questions = calc_bertscore(gold_text[:,2], pred_text[:,2])\n",
    "\n",
    "    print(association[2] + ' MSE_soundness = ' + str(mean_squared_error(gold_scores[:,0], pred_scores[:,0])))\n",
    "    print(association[2] + ' MSE_presentation = ' + str(mean_squared_error(gold_scores[:,1], pred_scores[:,1])))\n",
    "    print(association[2] + ' MSE_contribution = ' + str(mean_squared_error(gold_scores[:,2], pred_scores[:,2])))\n",
    "    print(association[2] + ' MSE_rating = ' + str(mean_squared_error(gold_scores[:,3], pred_scores[:,3])))\n",
    "    print(association[2] + ' MSE_confidence = ' + str(mean_squared_error(gold_scores[:,4], pred_scores[:,4])))\n",
    "    print(association[2] + ' Strengths_BertScore = ' + str(mean_precision_strengths) + ' / ' + str(mean_recall_strengths) + ' / ' + str(mean_f1_strengths))\n",
    "    print(association[2] + ' Weaknesses_BertScore = ' + str(mean_precision_weaknesses) + ' / ' + str(mean_recall_weaknesses) + ' / ' + str(mean_f1_weaknesses))\n",
    "    print(association[2] + ' Questions_BertScore = ' + str(mean_precision_questions) + ' / ' + str(mean_recall_questions) + ' / ' + str(mean_f1_questions))\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_review(text):\n",
    "    text = text.replace('\\nExplanation:\\n', '',1)\n",
    "    review_dict = {}\n",
    "    \n",
    "    # Split text into sections using double newlines as a separator\n",
    "    sections = re.split(r\"\\n\\n\", text.strip())\n",
    "\n",
    "    for section in sections:\n",
    "        lines = section.split(\"\\n\")\n",
    "        #lines = section\n",
    "        \n",
    "        # First part contains numerical ratings\n",
    "        if \": \" in lines[0]:\n",
    "            for line in lines:\n",
    "                if \": \" in line:\n",
    "                    key, value = line.split(\": \", 1)\n",
    "                    review_dict[key.strip()] = int(value) if value.isdigit() else value.strip()\n",
    "        \n",
    "        # Later parts contain explanations, strengths, weaknesses, questions\n",
    "        else:\n",
    "            key = lines[0].replace(\":\", \"\").strip()  # Remove colons and extra spaces\n",
    "            review_dict[key] = \"\\n\".join(lines[1:]).strip()  # Store the rest of the section\n",
    "\n",
    "    return review_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_review1(text):\n",
    "    # Define regex patterns for extracting different sections\n",
    "    score_pattern = r\"(\\w+): (\\d+)\"\n",
    "    section_pattern = r\"(Strengths|Weaknesses|Questions):\\n\"\n",
    "\n",
    "    # Initialize dictionary\n",
    "    review_dict = {}\n",
    "\n",
    "    # Extract numerical scores\n",
    "    scores = re.findall(score_pattern, text)\n",
    "    for key, value in scores:\n",
    "        review_dict[key] = int(value)  # Convert scores to integers\n",
    "\n",
    "    # Extract text sections\n",
    "    sections = re.split(section_pattern, text)\n",
    "    \n",
    "    # Process sections\n",
    "    for i in range(1, len(sections), 2):\n",
    "        key = sections[i].strip()\n",
    "        value = sections[i + 1].strip()\n",
    "        review_dict[key] = value\n",
    "\n",
    "    return review_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[152], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gold_standard \u001b[38;5;241m=\u001b[39m {item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaper_id\u001b[39m\u001b[38;5;124m'\u001b[39m]: parse_review1(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m gold_standard}\n\u001b[1;32m      2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m {item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaper_id\u001b[39m\u001b[38;5;124m'\u001b[39m]: parse_review1(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m predictions}\n",
      "Cell \u001b[0;32mIn[152], line 1\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gold_standard \u001b[38;5;241m=\u001b[39m {\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpaper_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m: parse_review1(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m gold_standard}\n\u001b[1;32m      2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m {item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaper_id\u001b[39m\u001b[38;5;124m'\u001b[39m]: parse_review1(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m predictions}\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "gold_standard = {item['paper_id']: parse_review1(item['response']) for item in gold_standard}\n",
    "predictions = {item['paper_id']: parse_review1(item['response']) for item in predictions}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('matched_papers_reviews_test.json', 'r') as f:\n",
    "    papers_2025 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_citations(full_text):\n",
    "    idx = full_text.find('\\n\\nREFERENCES')\n",
    "    if idx == -1:\n",
    "        idx = full_text.find('\\n\\nReferences')\n",
    "    return full_text[:idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('instruction_finetuning_data.jsonl', 'r') as f:\n",
    "    train_data = [json.loads(line) for line in f]\n",
    "with open('instruction_finetuning_data_test.jsonl', 'r') as f:\n",
    "    test_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_string = \"\"\"\n",
    "Categories (1–5):\n",
    "- Soundness: The rigor of the methods for the stated problem\n",
    "- Presentation: The clarity of writing and organization\n",
    "- Contribution: The paper’s novelty or added value to the domain\n",
    "\n",
    "Additionally:\n",
    "- Rating (1–10): Overall recommendation for acceptance\n",
    "- Confidence (1–5): How confident the reviewer is in their assessment\n",
    "\n",
    "Please provide:\n",
    "1) Soundness\n",
    "2) Presentation\n",
    "3) Contribution\n",
    "4) Rating\n",
    "5) Confidence\n",
    "6) Explanation (strengths, weaknesses)\n",
    "7) Questions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_txt = [{'paper_id': item['paper_id'], \n",
    "             'summary': item['prompt'].replace(prompt_string, ''), \n",
    "             'full text': remove_citations(papers_2025[item['paper_id']]['full_text']['value']), \n",
    "             'abstract': papers_2025[item['paper_id']]['abstract']['value'],\n",
    "             'response': parse_review1(item['response'])} \n",
    "             for item in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('instruction_finetuning_test_data_reformatted.json', \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(json_txt, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_gen(num_pred, num_gold):\n",
    "    num_pred = np.array(num_pred)\n",
    "    num_gold = np.array(num_gold)\n",
    "\n",
    "    soundness_conf = confusion_matrix(num_pred[:,0], num_gold[:,0], labels=[1,2,3,4,5])\n",
    "    presentation_conf = confusion_matrix(num_pred[:,1], num_gold[:,1], labels=[1,2,3,4,5])\n",
    "    contribution_conf = confusion_matrix(num_pred[:,2], num_gold[:,2], labels=[1,2,3,4,5])\n",
    "    rating_conf = confusion_matrix(num_pred[:,3], num_gold[:,3], labels=[1,2,3,4,5,6,7,8,9,10])\n",
    "    confidence_conf = confusion_matrix(num_pred[:,4], num_gold[:,4], labels=[1,2,3,4,5])\n",
    "\n",
    "    return(soundness_conf, presentation_conf, contribution_conf, rating_conf, confidence_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aAcOaJYbUg': {'Soundness': 3,\n",
       "  'Presentation': 3,\n",
       "  'Contribution': 4,\n",
       "  'Rating': 6,\n",
       "  'Confidence': 3,\n",
       "  'Strengths': '1. Clear structure and presentation: The paper is well-organized, with a logical structure and smooth flow that improves readability and comprehension.\\n2. Insightful motivation for OOD benchmarking: This dataset addresses the critical issue of data leakage in current benchmarks. This approach highlights the limitations of evaluating current models trained on web-scale data.\\n3. Interesting distortion design: The authors designed six distinct distortion types that minimize overlap with common real-world corruptions, reducing the likelihood of data leakage. This approach improves the dataset’s utility as a truly challenging OOD benchmark.',\n",
       "  'Weaknesses': \"1. Limited novel insights in model ranking comparisons: Figure 4 shows minimal divergence in the ranking of model performance between ImageNet-C and LAION-C. Similarly, Figure 3 demonstrates a strong linear correlation between model performance on ImageNet-C and LAION-C. Since a benchmark dataset's purpose is to reveal different performance characteristics across models, more analysis is needed to clarify the unique insights LAION-C provides beyond what ImageNet-C already offers.\",\n",
       "  'Questions': '1. Performance differences between CNNs and transformers: Given that LAION-C primarily features patch-based transformations, are there observed differences in how CNNs and transformers perform on these types of distortions?',\n",
       "  '2. Class disparity between datasets': 'ImageNet-C includes 1,000 classes, whereas LAION-C has only 16, potentially creating an imbalance in comparison. Excluding the psychophysical experiments, what methods or adjustments could be employed to address this disparity and ensure a fair evaluation between the datasets?'},\n",
       " 'dgR6i4TSng': {'Soundness': 3,\n",
       "  'Presentation': 3,\n",
       "  'Contribution': 2,\n",
       "  'Rating': 6,\n",
       "  'Confidence': 3,\n",
       "  'Strengths': \"1. The paper introduces a parameterization based on quantum circuits that require only (2L+1)log₂(N)-2L parameters, a significant improvement over LoRA's 2NK parameters.\\n2. The reduction in parameters is experimentally verified.\\n3. The authors proposed generalized RY and CZ quantum gates for arbitrary dimensions beyond power-of-2.\\n4. The proposed method seems robust under quantization.\",\n",
       "  'Weaknesses': '1. Analysis of how L layers affect entanglement capacity is limited. There is no evaluation of entanglement entropy between layers.',\n",
       "  '2. The paper only focuses on benchmarking against LoRA/adapter variants on small models (such as GPT2). Benchmarks on larger (and newer) models such as LLaMA seem to be quite standard and more related to practical use cases.': '',\n",
       "  '1. Some of the recently proposed PEFT methods are not adequately addressed/discussed (at least mentioned somewhere in the paper), e.g.': '',\n",
       "  '[1] https//arxiv.org/abs/2403.17919 (NeurIPS)': '',\n",
       "  '[2] https//arxiv.org/abs/2404.03592 (NeurIPS)': '',\n",
       "  '[3] https//arxiv.org/abs/2405.12130': '',\n",
       "  '[4] https//arxiv.org/abs/2406.00132 (NeurIPS)': '',\n",
       "  'Both [3] and [4] seem to be able to model high-rank matrices, and [4] appears to be also based on quantum circuit.': '',\n",
       "  'Questions': '1. How does Eq. 4 work?\\n2. What is the optimal number of alternating entanglement layers L for different model scales?\\n3. For the quantum Shannon decomposition approach to handle non-power-of-two dimensions, how does the decomposition choice (N₁, N₂) affect model performance?\\n4. Does the proposed method have a large computational overhead from the entanglement layers?\\n5. How does the proposed method compare with other methods mentioned above? My understanding of the main difference between this method and [4] is the use of unitary and diagonal matrices. Is this correct?\\n6. The proposed method focuses on reduction in parameters. However, [2] and [4] claim both reduction in parameter and performance improvement. Does the proposed method use even fewer parameters?\\n6. How is the proposed method related to quantum machine learning?'},\n",
       " 'UatDdAlr2x': {'Soundness': 3,\n",
       "  'Presentation': 3,\n",
       "  'Contribution': 2,\n",
       "  'Rating': 6,\n",
       "  'Confidence': 3,\n",
       "  'Strengths': '* The paper is well-written, easy to follow, and provides sufficient analysis for its claims.\\n* Discovers and Discusses two strategies which 1-layer transformers can use to perform the counting task (i.e., relation-based and inventory-based) and in doing so considers a range of possible hyperparameter selections that affect this strategic choice.\\n* During its analysis further studies the role of BoS token showing that together with softmax in the counting task, can help models with smaller p and d sizes to still reach perfect accuracy.',\n",
       "  'Weaknesses': '* Limited scope and applicability, specifically: (1) Only examines single-layer transformers, (2)Focuses on just one task (histogram counting)\\n* No clear path to extending insights to more complicated settings more specifically multi-layer transformers\\n* Lack of practical impact in the sense that it is hard to find a direct translation from this paper findings to real-world transformer design for real-world tasks',\n",
       "  'Questions': 'Have you done any analysis or insight you can share on: \\n1. what behaviour can we expect in terms of the same hyperparameter choices (embedding dimension, Hidden Layer Size, and Vocab Size) whether as a  2 or 3 layer blackbox and/or an isolation of one of the transformer layers in a multi-layer setting?\\n2. Have you done any initial analysis to share about whether the same relationships for parameters and potential solving strategies hold for other tasks (such as the sorting or lookup problems you referred to or any other tasks)'}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = {k: predictions[k] for k in ('aAcOaJYbUg', 'dgR6i4TSng', 'UatDdAlr2x')}\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#def evaluate_review(predictions, gold_standard):\n",
    "results = {}\n",
    "numerical_scores = [\"Soundness\", \"Presentation\", \"Contribution\", \"Rating\", \"Confidence\"]\n",
    "text_fields = [\"Strengths\", \"Weaknesses\", \"Questions\"]\n",
    "\n",
    "num_pred = []\n",
    "num_gold = []\n",
    "for paper_id, response in predictions.items():\n",
    "    gold = gold_standard[paper_id]\n",
    "    results[paper_id] = {}\n",
    "\n",
    "    num_pred = num_pred + [[response[category] for category in numerical_scores]]\n",
    "    num_gold = num_gold + [[gold[category] for category in numerical_scores]]\n",
    "\n",
    "    results[paper_id][\"MAE\"] = mean_absolute_error(num_gold, num_pred)\n",
    "    results[paper_id][\"MSE\"] = mean_squared_error(num_gold, num_pred)\n",
    "\n",
    "    text_results = {}\n",
    "    #for text in text_fields:\n",
    "    P, R, F1 = score([response[text_fields[0]], response[text_fields[1]], response[text_fields[2]]], \n",
    "                     [gold[text_fields[0]], gold[text_fields[1]], gold[text_fields[2]]], lang=\"en\", rescale_with_baseline=True)\n",
    "    text_results[text_fields[0] + \"_BERTScore\"] = F1[0]\n",
    "    text_results[text_fields[1] + \"_BERTScore\"] = F1[1]\n",
    "    text_results[text_fields[2] + \"_BERTScore\"] = F1[2]\n",
    "\n",
    "    embeddings = model.encode([response[text_fields[0]], gold[text_fields[0]], response[text_fields[1]], gold[text_fields[1]], response[text_fields[1]], gold[text_fields[2]]])\n",
    "    text_results[text_fields[0] + \"_Embedding\"] = (embeddings[0], embeddings[1])\n",
    "    text_results[text_fields[1] + \"_Embedding\"] = (embeddings[2], embeddings[3])\n",
    "    text_results[text_fields[2] + \"_Embedding\"] = (embeddings[4], embeddings[5])\n",
    "\n",
    "    results[paper_id]['Text Comparisons'] = text_results\n",
    "    \n",
    "matrices = confusion_matrix_gen(num_pred, num_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['* The paper is well-written, easy to follow, and provides sufficient analysis for its claims.\\n* Discovers and Discusses two strategies which 1-layer transformers can use to perform the counting task (i.e., relation-based and inventory-based) and in doing so considers a range of possible hyperparameter selections that affect this strategic choice.\\n* During its analysis further studies the role of BoS token showing that together with softmax in the counting task, can help models with smaller p and d sizes to still reach perfect accuracy.',\n",
       " '* Limited scope and applicability, specifically: (1) Only examines single-layer transformers, (2)Focuses on just one task (histogram counting)\\n* No clear path to extending insights to more complicated settings more specifically multi-layer transformers\\n* Lack of practical impact in the sense that it is hard to find a direct translation from this paper findings to real-world transformer design for real-world tasks',\n",
       " 'Have you done any analysis or insight you can share on: \\n1. what behaviour can we expect in terms of the same hyperparameter choices (embedding dimension, Hidden Layer Size, and Vocab Size) whether as a  2 or 3 layer blackbox and/or an isolation of one of the transformer layers in a multi-layer setting?\\n2. Have you done any initial analysis to share about whether the same relationships for parameters and potential solving strategies hold for other tasks (such as the sorting or lookup problems you referred to or any other tasks)']"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[response[text_fields[0]], response[text_fields[1]], response[text_fields[2]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 384)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encode([predictions['aAcOaJYbUg']['Strengths'], gold_standard['aAcOaJYbUg']['Strengths']]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1.0000]), tensor([1.0000]), tensor([1.0000]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score([predictions['aAcOaJYbUg']['Explanation']], [gold_standard['aAcOaJYbUg']['Explanation']], lang='en', device='mps')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
