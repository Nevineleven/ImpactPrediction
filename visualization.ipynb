{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from bert_score import score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, confusion_matrix\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('instruction_finetuning_data_test.jsonl', 'r') as f:\n",
    "    gold_standard = [json.loads(line) for line in f]\n",
    "\n",
    "with open('instruction_finetuning_data_test.jsonl', 'r') as f:\n",
    "    predictions = [json.loads(line) for line in f]\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Soundness: 3\\nPresentation: 3\\nContribution: 4\\nRating: 6\\nConfidence: 3\\n\\nStrengths:\\n1. Clear structure and presentation: The paper is well-organized, with a logical structure and smooth flow that improves readability and comprehension.\\n2. Insightful motivation for OOD benchmarking: This dataset addresses the critical issue of data leakage in current benchmarks. This approach highlights the limitations of evaluating current models trained on web-scale data.\\n3. Interesting distortion design: The authors designed six distinct distortion types that minimize overlap with common real-world corruptions, reducing the likelihood of data leakage. This approach improves the dataset’s utility as a truly challenging OOD benchmark.\\n\\nWeaknesses:\\n1. Limited novel insights in model ranking comparisons: Figure 4 shows minimal divergence in the ranking of model performance between ImageNet-C and LAION-C. Similarly, Figure 3 demonstrates a strong linear correlation between model performance on ImageNet-C and LAION-C. Since a benchmark dataset's purpose is to reveal different performance characteristics across models, more analysis is needed to clarify the unique insights LAION-C provides beyond what ImageNet-C already offers.\\n\\nQuestions:\\n1. Performance differences between CNNs and transformers: Given that LAION-C primarily features patch-based transformations, are there observed differences in how CNNs and transformers perform on these types of distortions?\\n\\n2. Class disparity between datasets: ImageNet-C includes 1,000 classes, whereas LAION-C has only 16, potentially creating an imbalance in comparison. Excluding the psychophysical experiments, what methods or adjustments could be employed to address this disparity and ensure a fair evaluation between the datasets?\\n\""
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]['response'].replace('Explanation:\\n', '',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_review(text):\n",
    "    text = text.replace('\\nExplanation:\\n', '',1)\n",
    "    review_dict = {}\n",
    "    \n",
    "    # Split text into sections using double newlines as a separator\n",
    "    sections = re.split(r\"\\n\\n\", text.strip())\n",
    "\n",
    "    for section in sections:\n",
    "        lines = section.split(\"\\n\")\n",
    "        #lines = section\n",
    "        \n",
    "        # First part contains numerical ratings\n",
    "        if \": \" in lines[0]:\n",
    "            for line in lines:\n",
    "                if \": \" in line:\n",
    "                    key, value = line.split(\": \", 1)\n",
    "                    review_dict[key.strip()] = int(value) if value.isdigit() else value.strip()\n",
    "        \n",
    "        # Later parts contain explanations, strengths, weaknesses, questions\n",
    "        else:\n",
    "            key = lines[0].replace(\":\", \"\").strip()  # Remove colons and extra spaces\n",
    "            review_dict[key] = \"\\n\".join(lines[1:]).strip()  # Store the rest of the section\n",
    "\n",
    "    return review_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_review1(text):\n",
    "    # Define regex patterns for extracting different sections\n",
    "    score_pattern = r\"(\\w+): (\\d+)\"\n",
    "    section_pattern = r\"(Strengths|Weaknesses|Questions):\\n\"\n",
    "\n",
    "    # Initialize dictionary\n",
    "    review_dict = {}\n",
    "\n",
    "    # Extract numerical scores\n",
    "    scores = re.findall(score_pattern, text)\n",
    "    for key, value in scores:\n",
    "        review_dict[key] = int(value)  # Convert scores to integers\n",
    "\n",
    "    # Extract text sections\n",
    "    sections = re.split(section_pattern, text)\n",
    "    \n",
    "    # Process sections\n",
    "    for i in range(1, len(sections), 2):\n",
    "        key = sections[i].strip()\n",
    "        value = sections[i + 1].strip()\n",
    "        review_dict[key] = value\n",
    "\n",
    "    return review_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[152], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gold_standard \u001b[38;5;241m=\u001b[39m {item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaper_id\u001b[39m\u001b[38;5;124m'\u001b[39m]: parse_review1(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m gold_standard}\n\u001b[1;32m      2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m {item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaper_id\u001b[39m\u001b[38;5;124m'\u001b[39m]: parse_review1(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m predictions}\n",
      "Cell \u001b[0;32mIn[152], line 1\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gold_standard \u001b[38;5;241m=\u001b[39m {\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpaper_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m: parse_review1(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m gold_standard}\n\u001b[1;32m      2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m {item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaper_id\u001b[39m\u001b[38;5;124m'\u001b[39m]: parse_review1(item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m predictions}\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "gold_standard = {item['paper_id']: parse_review1(item['response']) for item in gold_standard}\n",
    "predictions = {item['paper_id']: parse_review1(item['response']) for item in predictions}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('matched_papers_reviews_test.json', 'r') as f:\n",
    "    papers_2025 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_citations(full_text):\n",
    "    idx = full_text.find('\\n\\nREFERENCES')\n",
    "    if idx == -1:\n",
    "        idx = full_text.find('\\n\\nReferences')\n",
    "    return full_text[:idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('instruction_finetuning_data.jsonl', 'r') as f:\n",
    "    train_data = [json.loads(line) for line in f]\n",
    "with open('instruction_finetuning_data_test.jsonl', 'r') as f:\n",
    "    test_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_string = \"\"\"\n",
    "Categories (1–5):\n",
    "- Soundness: The rigor of the methods for the stated problem\n",
    "- Presentation: The clarity of writing and organization\n",
    "- Contribution: The paper’s novelty or added value to the domain\n",
    "\n",
    "Additionally:\n",
    "- Rating (1–10): Overall recommendation for acceptance\n",
    "- Confidence (1–5): How confident the reviewer is in their assessment\n",
    "\n",
    "Please provide:\n",
    "1) Soundness\n",
    "2) Presentation\n",
    "3) Contribution\n",
    "4) Rating\n",
    "5) Confidence\n",
    "6) Explanation (strengths, weaknesses)\n",
    "7) Questions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_txt = [{'paper_id': item['paper_id'], \n",
    "             'summary': item['prompt'].replace(prompt_string, ''), \n",
    "             'full text': remove_citations(papers_2025[item['paper_id']]['full_text']['value']), \n",
    "             'abstract': papers_2025[item['paper_id']]['abstract']['value'],\n",
    "             'response': parse_review1(item['response'])} \n",
    "             for item in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('instruction_finetuning_test_data_reformatted.json', \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(json_txt, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_gen(num_pred, num_gold):\n",
    "    num_pred = np.array(num_pred)\n",
    "    num_gold = np.array(num_gold)\n",
    "\n",
    "    soundness_conf = confusion_matrix(num_pred[:,0], num_gold[:,0], labels=[1,2,3,4,5])\n",
    "    presentation_conf = confusion_matrix(num_pred[:,1], num_gold[:,1], labels=[1,2,3,4,5])\n",
    "    contribution_conf = confusion_matrix(num_pred[:,2], num_gold[:,2], labels=[1,2,3,4,5])\n",
    "    rating_conf = confusion_matrix(num_pred[:,3], num_gold[:,3], labels=[1,2,3,4,5,6,7,8,9,10])\n",
    "    confidence_conf = confusion_matrix(num_pred[:,4], num_gold[:,4], labels=[1,2,3,4,5])\n",
    "\n",
    "    return(soundness_conf, presentation_conf, contribution_conf, rating_conf, confidence_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aAcOaJYbUg': {'Soundness': 3,\n",
       "  'Presentation': 3,\n",
       "  'Contribution': 4,\n",
       "  'Rating': 6,\n",
       "  'Confidence': 3,\n",
       "  'Strengths': '1. Clear structure and presentation: The paper is well-organized, with a logical structure and smooth flow that improves readability and comprehension.\\n2. Insightful motivation for OOD benchmarking: This dataset addresses the critical issue of data leakage in current benchmarks. This approach highlights the limitations of evaluating current models trained on web-scale data.\\n3. Interesting distortion design: The authors designed six distinct distortion types that minimize overlap with common real-world corruptions, reducing the likelihood of data leakage. This approach improves the dataset’s utility as a truly challenging OOD benchmark.',\n",
       "  'Weaknesses': \"1. Limited novel insights in model ranking comparisons: Figure 4 shows minimal divergence in the ranking of model performance between ImageNet-C and LAION-C. Similarly, Figure 3 demonstrates a strong linear correlation between model performance on ImageNet-C and LAION-C. Since a benchmark dataset's purpose is to reveal different performance characteristics across models, more analysis is needed to clarify the unique insights LAION-C provides beyond what ImageNet-C already offers.\",\n",
       "  'Questions': '1. Performance differences between CNNs and transformers: Given that LAION-C primarily features patch-based transformations, are there observed differences in how CNNs and transformers perform on these types of distortions?',\n",
       "  '2. Class disparity between datasets': 'ImageNet-C includes 1,000 classes, whereas LAION-C has only 16, potentially creating an imbalance in comparison. Excluding the psychophysical experiments, what methods or adjustments could be employed to address this disparity and ensure a fair evaluation between the datasets?'},\n",
       " 'dgR6i4TSng': {'Soundness': 3,\n",
       "  'Presentation': 3,\n",
       "  'Contribution': 2,\n",
       "  'Rating': 6,\n",
       "  'Confidence': 3,\n",
       "  'Strengths': \"1. The paper introduces a parameterization based on quantum circuits that require only (2L+1)log₂(N)-2L parameters, a significant improvement over LoRA's 2NK parameters.\\n2. The reduction in parameters is experimentally verified.\\n3. The authors proposed generalized RY and CZ quantum gates for arbitrary dimensions beyond power-of-2.\\n4. The proposed method seems robust under quantization.\",\n",
       "  'Weaknesses': '1. Analysis of how L layers affect entanglement capacity is limited. There is no evaluation of entanglement entropy between layers.',\n",
       "  '2. The paper only focuses on benchmarking against LoRA/adapter variants on small models (such as GPT2). Benchmarks on larger (and newer) models such as LLaMA seem to be quite standard and more related to practical use cases.': '',\n",
       "  '1. Some of the recently proposed PEFT methods are not adequately addressed/discussed (at least mentioned somewhere in the paper), e.g.': '',\n",
       "  '[1] https//arxiv.org/abs/2403.17919 (NeurIPS)': '',\n",
       "  '[2] https//arxiv.org/abs/2404.03592 (NeurIPS)': '',\n",
       "  '[3] https//arxiv.org/abs/2405.12130': '',\n",
       "  '[4] https//arxiv.org/abs/2406.00132 (NeurIPS)': '',\n",
       "  'Both [3] and [4] seem to be able to model high-rank matrices, and [4] appears to be also based on quantum circuit.': '',\n",
       "  'Questions': '1. How does Eq. 4 work?\\n2. What is the optimal number of alternating entanglement layers L for different model scales?\\n3. For the quantum Shannon decomposition approach to handle non-power-of-two dimensions, how does the decomposition choice (N₁, N₂) affect model performance?\\n4. Does the proposed method have a large computational overhead from the entanglement layers?\\n5. How does the proposed method compare with other methods mentioned above? My understanding of the main difference between this method and [4] is the use of unitary and diagonal matrices. Is this correct?\\n6. The proposed method focuses on reduction in parameters. However, [2] and [4] claim both reduction in parameter and performance improvement. Does the proposed method use even fewer parameters?\\n6. How is the proposed method related to quantum machine learning?'},\n",
       " 'UatDdAlr2x': {'Soundness': 3,\n",
       "  'Presentation': 3,\n",
       "  'Contribution': 2,\n",
       "  'Rating': 6,\n",
       "  'Confidence': 3,\n",
       "  'Strengths': '* The paper is well-written, easy to follow, and provides sufficient analysis for its claims.\\n* Discovers and Discusses two strategies which 1-layer transformers can use to perform the counting task (i.e., relation-based and inventory-based) and in doing so considers a range of possible hyperparameter selections that affect this strategic choice.\\n* During its analysis further studies the role of BoS token showing that together with softmax in the counting task, can help models with smaller p and d sizes to still reach perfect accuracy.',\n",
       "  'Weaknesses': '* Limited scope and applicability, specifically: (1) Only examines single-layer transformers, (2)Focuses on just one task (histogram counting)\\n* No clear path to extending insights to more complicated settings more specifically multi-layer transformers\\n* Lack of practical impact in the sense that it is hard to find a direct translation from this paper findings to real-world transformer design for real-world tasks',\n",
       "  'Questions': 'Have you done any analysis or insight you can share on: \\n1. what behaviour can we expect in terms of the same hyperparameter choices (embedding dimension, Hidden Layer Size, and Vocab Size) whether as a  2 or 3 layer blackbox and/or an isolation of one of the transformer layers in a multi-layer setting?\\n2. Have you done any initial analysis to share about whether the same relationships for parameters and potential solving strategies hold for other tasks (such as the sorting or lookup problems you referred to or any other tasks)'}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = {k: predictions[k] for k in ('aAcOaJYbUg', 'dgR6i4TSng', 'UatDdAlr2x')}\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#def evaluate_review(predictions, gold_standard):\n",
    "results = {}\n",
    "numerical_scores = [\"Soundness\", \"Presentation\", \"Contribution\", \"Rating\", \"Confidence\"]\n",
    "text_fields = [\"Strengths\", \"Weaknesses\", \"Questions\"]\n",
    "\n",
    "num_pred = []\n",
    "num_gold = []\n",
    "for paper_id, response in predictions.items():\n",
    "    gold = gold_standard[paper_id]\n",
    "    results[paper_id] = {}\n",
    "\n",
    "    num_pred = num_pred + [[response[category] for category in numerical_scores]]\n",
    "    num_gold = num_gold + [[gold[category] for category in numerical_scores]]\n",
    "\n",
    "    results[paper_id][\"MAE\"] = mean_absolute_error(num_gold, num_pred)\n",
    "    results[paper_id][\"MSE\"] = mean_squared_error(num_gold, num_pred)\n",
    "\n",
    "    text_results = {}\n",
    "    #for text in text_fields:\n",
    "    P, R, F1 = score([response[text_fields[0]], response[text_fields[1]], response[text_fields[2]]], \n",
    "                     [gold[text_fields[0]], gold[text_fields[1]], gold[text_fields[2]]], lang=\"en\", rescale_with_baseline=True)\n",
    "    text_results[text_fields[0] + \"_BERTScore\"] = F1[0]\n",
    "    text_results[text_fields[1] + \"_BERTScore\"] = F1[1]\n",
    "    text_results[text_fields[2] + \"_BERTScore\"] = F1[2]\n",
    "\n",
    "    embeddings = model.encode([response[text_fields[0]], gold[text_fields[0]], response[text_fields[1]], gold[text_fields[1]], response[text_fields[1]], gold[text_fields[2]]])\n",
    "    text_results[text_fields[0] + \"_Embedding\"] = (embeddings[0], embeddings[1])\n",
    "    text_results[text_fields[1] + \"_Embedding\"] = (embeddings[2], embeddings[3])\n",
    "    text_results[text_fields[2] + \"_Embedding\"] = (embeddings[4], embeddings[5])\n",
    "\n",
    "    results[paper_id]['Text Comparisons'] = text_results\n",
    "    \n",
    "matrices = confusion_matrix_gen(num_pred, num_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 384)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encode([predictions['aAcOaJYbUg']['Strengths'], gold_standard['aAcOaJYbUg']['Strengths']]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1.0000]), tensor([1.0000]), tensor([1.0000]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score([predictions['aAcOaJYbUg']['Explanation']], [gold_standard['aAcOaJYbUg']['Explanation']], lang='en', device='mps')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
