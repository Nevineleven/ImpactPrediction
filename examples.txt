############# GOLD STANDARD RESPONSE PAPER 1 (use for testing) ID bUGzjiUsIq #############

Please read the following paper summary and produce reviewer scores:

Background: Traditional multi-label classification relies heavily on fully annotated datasets, which are difficult and often impractical to obtain, especially in domains requiring specific expert knowledge. Incomplete annotations are common in multi-label tasks due to the large number of possible labels and complexities involved in providing full annotations.
Gap: The problem addressed in this paper is learning from partially annotated multi-label classification datasets where only a subset of positive classes is annotated, leading to issues of scarcity in positive annotations and severe label imbalance.
Hypothesis: The authors propose that by integrating reinforcement learning with partial annotation using a Policy Gradient (PAPG) approach, they can effectively overcome the limitations of traditional methods and improve performance in partially annotated multi-label classification by balancing exploration and exploitation.
Method: The authors introduce PAPG, a framework that combines the exploration strengths of reinforcement learning with the exploitation capabilities of supervised learning. The model is structured as a Markov Decision Process (MDP) with local and global rewards. The local reward comes from a value network trained on annotated data and the global reward is based on recall metrics. The training process involves iterative updates between a policy network and a value network, using data enhancement and an adaptive threshold to adjust training data.
Conclusion: The proposed PAPG framework effectively handles the challenges of partially annotated multi-label classification, demonstrating superior performance over traditional methods and baselines. The framework shows robustness and generalizes well across different datasets and domains, substantially improving F1 scores and other metrics compared to existing methods.

Categories (1–5):
- Soundness: The rigor of the methods for the stated problem
- Presentation: The clarity of writing and organization
- Contribution: The paper’s novelty or added value to the domain

Additionally:
- Rating (1–10): Overall recommendation for acceptance
- Confidence (1–5): How confident the reviewer is in their assessment

Please provide:
1) Soundness
2) Presentation
3) Contribution
4) Rating
5) Confidence
6) Strengths
7) Weaknesses
8) Questions

"response": 
Soundness: 2 fair
Presentation: 2 fair
Contribution: 2 fair
Rating: 3: reject, not good enough

Confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

Explanation:
Strengths:
1. The problem, partially annotated multi-label classifcation, is novel.
2. A novel method named PAPG is proposed, which can overcome the challenges associated with a scarcity of positive annotations and severe label imbalance. 
3. Experiments validate the effectiveness of the proposed PAPG method.

Weaknesses:
1. The motivation might be unreasonable. As stated in abstract, \"This task encounters challenges associated with a scarcity of positive annotations and severe label imbalance\". These challenges are indeed problem to deal with, but in my opinion, the main challenges correspond to that the negative samples are unknown or we do not know which are negative samples among the remaining unlabeled samples. In other words, the main challenges are the same as PU learning.\n\n2. For the two challenges mentioned in this paper, i.e., a scarcity of positive annotations and severe label imbalance, it is unclear how can the proposed PAPG method overcome them.\n\n3. As stated in Introduction, \"Consequently, many advanced methods of PU learning (Su et al., 2021; Acharya et al., 2022; Luo et al., 2021) cannot readily adapt to our multi-label settings\". In my opinion, if we focus each label one by one, it is a PU learning problem. Thus, the partially annotated multi-label classifcation problem can be solved via binary relevance strategy where the base classifier is trained with any off-the-shelf PU learning methods.\n\n4. For multi-label classification, it is very important to model the correlations among labels. But it is unclear how can the proposed PAPG method model label correlations.\n\n5. The writtting can be greatly improved. For exampe:\n(1) The title is inappropriate.\n(2) Errors exist in notations (especially the first paragraph in section 3.1).\n(3) What does RARL mean in the last sentence of this paper?\n\nQuestions:\nIf author disagree with my comments in Weaknesses, please clarify them in the rebuttal phase.\n"}

############# GOLD STANDARD RESPONSE PAPER 2 (use for one shot) ID vsW5vJqBuv #############

Background: Empowering embodied AI, such as robots, has become crucial as these systems transition from solving specific, constrained problems to handling a variety of tasks in the physical world, such as household chores and healthcare. Traditional methods often struggle with the inherent diversity and complexity of open-ended task goals that are multifaceted and dynamic.

Gap: Current AI techniques are limited in their ability to handle open-ended tasks due to the traditional goal-conditioned frameworks that expect specific end-states. This restriction impedes the flexibility required for robots to adapt to novel tasks not encountered during training, which is necessary for more realistic and practical applications.

Hypothesis: The authors propose that combining diffusion models with training-free guidance can enhance the adaptability and competency of embodied AI systems in solving open-ended tasks. The diffusion models can generate various potential future states without the constraints of pre-defined goals, allowing for more flexible planning and control.

Method: The proposed method, DOG (Diffusion for Open-ended Goals), uses diffusion models to understand world dynamics from offline data without goal-conditioning and performs energy minimization for planning during inference. The method involves training with Markov decision frameworks and using classifier-guided diffusion models for conditional score functions. The approach was tested on diverse tasks such as maze navigation, robot movement control, and robotic arm manipulation, demonstrating its ability to handle unscripted objectives dynamically.

Conclusion: DOG significantly advances the flexibility and competence of embodied AI, enabling agents to plan and execute actions for open-ended task goals not defined during training. The framework allows for zero-shot transfer to new tasks and shows robust performance in handling various types of goals, representing a potential leap in achieving more human-like intelligence in embodied systems.

Categories (1–5):
- Soundness: The rigor of the methods for the stated problem
- Presentation: The clarity of writing and organization
- Contribution: The paper’s novelty or added value to the domain

Additionally:
- Rating (1–10): Overall recommendation for acceptance
- Confidence (1–5): How confident the reviewer is in their assessment

Please provide:
1) Soundness
2) Presentation
3) Contribution
4) Rating
5) Confidence
6) Explanation (strengths, weaknesses)
7) Questions

"response": 

Soundness: 1 poor
Presentation: 1 poor
Contribution: 2 fair

Rating: 3: reject, not good enough
Confidence: 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

Explanation:
Strengths:
Using diffusion models to perform Goal-Conditioned Reinforcement Learning (GCRL) looks like a good idea, although the authors are not the first to put it forward, and are not clear about the novelty they bring.

Weaknesses:
- The paper is very unclear about the \"training-free\" property of the framework, which seems to be the main contribution with respect to Janner et al. (2022) and Liang et al. (2023).
- The paper is unclear about the difference between defining an energy function by hand and defining a goal-related reward function by hand.
- The experiments about using language-based prompts are not explained at all
- The paper is written poorly and often in a non-scientific way with a lot of unnecessary over-statements (see below).

Questions:
- could the authors explain the difference between defining energy functions for partial goals, negative goals, sequence goals, hybrid goals as they do in the bottom of page 3 and defining goal-related reward functions that do the same, as often done in GCRL? If they argue that this makes a difference, can they backup this claim with comparative experiments?
- \"This way of defining a goal is fundamentally different from the conventional way of treating the goal as a variable\" -> could the authors explain why it is \"fundamentally different\"? You could have a goal represented with \"Clean = True\", this would be using a boolean variable, or \"clean level = X\", that would be using a scalar variable. Doesn't your energy function (c or g?) compute something similar?

############# GOLD STANDARD RESPONSE PAPER 3 (use for few shot) ID Aq35gl2c1k #############

Background: Critical learning periods are well-established phenomena in both biological systems, where temporary sensory deficits early in development can permanently affect behavior, and in artificial networks. Research has shown that these periods are tied to neural plasticity in biology. In artificial deep neural networks, analogous phenomena have been observed, raising questions about whether critical periods are inherent to the learning process of any flexible learning system, rather than being purely biologically driven.

Gap: While critical periods have been observed in artificial systems, the underlying causes are not well understood, particularly whether these periods depend on specific architectural or optimization details. There is a need to isolate the fundamental factors that cause critical periods to emerge in deep networks to better understand their generality across biological and artificial systems.

Hypothesis: The authors propose that critical learning periods are a fundamental feature of deep learning systems, driven by the depth of networks and data distribution structure rather than specific architectural or biochemical components. They suggest that these properties promote competitive dynamics between pathways, leading to critical periods.

Method: The study uses deep linear networks which lack the complexity of nonlinear activations to analytically explore the emergence of critical learning periods. The approach is two-pronged: (1) investigating the role of competition in a multi-pathway linear network model and (2) analyzing generalization in matrix completion tasks. Simulations and differential equation models reveal how depth and initial learning phases influence feature learning and generalization in networks. Experiments include analyses of singular value trajectories under various scenarios, such as temporary deprivations or pre-training relays on different tasks.

Conclusion: The analysis shows that critical periods arise from fundamental aspects such as network depth and the underlying data structures. These factors independently drive competitive learning dynamics and affect generalization for transfer learning. The simplicity of the linear models demonstrates that these phenomena are not reliant on complex neural architectures, supporting the idea that critical periods are inherent to deep learning processes, providing insights into both artificial and biological learning systems.

Categories (1–5):
- Soundness: The rigor of the methods for the stated problem
- Presentation: The clarity of writing and organization
- Contribution: The paper’s novelty or added value to the domain\n\nAdditionally:
- Rating (1–10): Overall recommendation for acceptance\n- Confidence (1–5): How confident the reviewer is in their assessment

Please provide:
1) Soundness
2) Presentation
3) Contribution
4) Rating
5) Confidence
6) Explanation (strengths, weaknesses)
7) Questions

response: 
Soundness: 4 excellent
Presentation: 4 excellent
Contribution: 3 good
Rating: 10: strong accept, should be highlighted at the conference
Confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

Explanation:
Strengths: Well written, interesting avenue of research. Results well explained early in the text, with clear examples and clear explanations of experimental settings and technical details. Particularly nice clarity given the cross-disciplinary nature of the work.\nRelated work is well written, reasonably thorough (to my knowledge), and well-explained.
Nice phase portraits.
Captions are mostly well-explained and self contained! (I don't know why this is so rare, but good job)

Weaknesses:
It's a bit of a tradeoff with having a clear hypothesis, but I find the case oversimplified/misframed  in the beginning and conclusion. Sure the literal biochemical explanation doesn't hold in an artificial setting, but analogies of this (reduced plasticity) could well occur. I don't think these are 'competing' hypotheses as they are framed; plasticity, imperfect optimization, etc. are *mechanisms* by which critical periods could arise in both artificial and biological systems; it doesn't tell us the reason  (inherent to learning, quirks of substrate, something else, etc.). In the conclusion \"natural information processing constraints\" are contrasted with biochemical processes like plasticity as an alternative hypothesis, but what is (decreased) plasticity buyt a natural information processing constraint? Demonstrating presence in artificial systems doesn't estabilsh it as a general feature of learning, except insofar as we've defined learning narrowly to be done by biological and ANNs (does naive bayes learn? does it exhibit critical periods?). It becomes sort of a circular argument about what does \"learning\" even mean. This commonality is actually supported by your experiments in Fig 3 recreating Guillery's. Anyway, I think the key sentences/claims (e.g. last of the abs) and results stand and are extremely interesting,  I would just like to see this framed around mechanisms or instantiations of information processing constraints, or something, rather than competing hypotheses. and maybe acknowledge more of the ambiguity of what \"learning systems in general\" means/ (without compromising clarity and narrative, which I believe are strong points of the paper).\n\nTerms used in equations are not explained when introduced  in 3.1 -- I know it's from a citation and you might be short on space, but you will lose a lot of readers here and it's a shame not to continue the clear prose you've had thus far.\n\nAround eqn 9, connection of matric completion to what you're trying to do is not clear -- be explicit about how imputing missing values is the same as / allows us to specify  task relationships with flexibility. \n\nWhile the relationship of experiments to showing dependence on depth is very clear and well-explained, the insights and relationship of experiments to data distribution is much less clear.

Questions:
See \"weaknesses\" for some less concrete suggestions (feel free to tell me what you plan to write if you address them and I'm happy to provide feedback!)\n\nThe main other thing I'd like to see is connections made/discussed with nonlinear networks -- what should we expect to change vs. hold? maybe according to the sources you cite (e.g. Saxe), or ideally, a suite of experiments with e.g. MLPs of increasing depth. \nAnother specific link could be made in Fig. 6 to scaling laws.\n\nsmall things:\n - first sentence shouldnt repeat abstract\n - 'competition between pathways affect' -> affects\n- inconsistent use of \"artificial neural network\", \"deep net\", \"neural net\", et al. For me it's fine, but could be confusing for someone outside the field; better to be consistent \n- title for plot should be removed (conflicts/causes potential confusion vs caption and X axis label, nonstandard for technical works to have an embedded title)\n - remind us what \"Path Singular Value\" is in the caption/change caption e.g .singular value dimensions (confusing that its referred to as 1-5 but labelled 0-4 on plot and called something different). All this stuff explaining the axis and basics of the plots should go first, before the interpretation. - I didn't understand the comment \"and do not affect performance on the final gask\" in fig $. Does it mean they have equal performance?\n"

############# GOLD STANDARD RESPONSE PAPER 4 (use for few shot) ID Aq35gl2c1k #############



############# PROMPT: Full text zero shot (o1 pro) #############

Please read the following paper summary and produce reviewer scores:

\nMAKE SMALL DATA GREAT AGAIN: LEARNING FROM\nPARTIALLY ANNOTATED DATA VIA POLICY GRADIENT\nFOR MULTI-LABEL CLASSIFICATION TASKS\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nBesides:::the:::::::\n\nTraditional supervised learning methods are heavily reliant on human-annotated\ndatasets. However, obtaining comprehensive human annotations proves challeng-\ning in numerous tasks, especially multi-label tasks. Therefore, we investigate the\nunderstudied problem of partially annotated multi-label classi\ufb01cation. This sce-\nnario involves learning from a multi-label dataset where only a subset of positive\nclasses is annotated. ::::::\nproblem, this task en-\nnegative::::::\ncounters challenges associated with a scarcity of positive class annotations and\nsevere label imbalance. To overcome these challenges, we propose Partially An-\nnotated reinforcement learning with a Policy Gradient algorithm (PAPG), a frame-\nwork combining the exploration capabilities of reinforcement learning with the\nexploitation strengths of supervised learning. By introducing local and global re-\nwards to address class imbalance issues and employing an iterative training strat-\negy equipped with data enhancement, our framework showcases its effectiveness\nand superiority across diverse classi\ufb01cation tasks.\n\nunknown:::::::\n\nclasses::::::::\n\nINTRODUCTION\n\n1\nTraditional supervised learning methods heavily rely on human-annotated data sets, especially neu-\nral approaches that are data-hungry and susceptible to over-\ufb01tting when lacking training data. Nev-\nertheless, in some tasks, human annotations are dif\ufb01cult for the needs of speci\ufb01c domain knowledge\nprovided by professional experts. For instance, the task of document-level relation extraction (Yao\net al., 2019) seeks to identify meaningful relationships between entity pairs within a document.\nHowever, human annotators \ufb01nd it hard to completely annotate all the relations due to the confusion\nof understanding relation de\ufb01nitions and long-context semantics. This phenomenon of incomplete\nannotation is likely to exist in many multi-label tasks that generally have dozens, hundreds, or even\nthousands sizes of class sets (Cole et al., 2021; Tan et al., 2022; Ben-Baruch et al., 2022).\n\nBased on the above observation, we focus on addressing a noteworthy yet under-studied problem,\npartially annotated multi-label classi\ufb01cation (PAMLC), which involves learning from a multi-\nlabel dataset in which only a subset of positive classes is annotated, while all the remaining classes\nunknown:::::::\nare unknown. :::::::\nnegative\nmissing\nfaced::::and ::::::\nclasses;::2)::::::\n::::::\npositive ::::::\n::of ::::::\nclasses;\npositive:::::\n::3) ::::both :::::::\ndistribution\ninestimable :::::::::\n::::prior. Figure 1 A exhibits an example from partially annotated relation data, and Figure 1 B illustrates\nthat the entire dataset contains a limited number of positive annotations, with the majority of labels\nremaining unannotated.\n\nDealing ::::with ::::\nsevere::::::::::::::\nclass::::::::::\n\npositive-negative:::::::::\nannotations ::::::::::\n\nproblem:::::\nimbalance :::that::::::::::\n\nclassi\ufb01cation :::task:::::\nplaguing::::::::::\n\nimbalance::::and :::::::\npartially ::::::::\n\nvalidation ::set:::::being::::::::\n\nexacerbating:::the:::::::::\n\ntraining :::set :::and:::::::::\n\nchallenges:::1)::::::::::\n\nmulti-label :::::::::::\n\nfaces :::::many :::::::::\n\nleading ::to ::::::::::\n\nrecognizing:::::::\n\ncompletely :::::::\n\nannotated:::::::\n\nsuch :a:::::::\n\nmissing:::::\n\nthree :::::::::\n\nlabels::::::mainly:::::\n\nclassi\ufb01cation ::::with ::::::\n\nmulti-label :::::::::::\nsettings ::of :::::::::\n\nPrevious:::::works:::on :::::::::\n:::::::\nSpeci\ufb01cally, :::the :::::::\ntasks.:::::::::::\n::::\ncategories: :::::::\n:::into:::::\nlabeled;::::::::\nclasses :::are ::::::\n::::::\nobserved :::but :::::::\nclasses :::are ::::::::\n::::::\n(SPL) :::::which::::::::\n:::::\n::to ::us::is::::PPL.:::::::::\n:::We :::::::\n\nrequires::at:::::\nsupposes:::::only :::one:::::::\nHowever, :it::::::::::\ngeneral::::::\n\npartially :::::::\npartially ::::::\n\nmulti-label:::::::::::\nlabeling::::::\nobserved:::::::\nleast :::one:::::::\n\nclassi\ufb01cation::::with:::::::\n(POL) :::::which::::::::\nlabeling::::::\npositive:::::\nclass :::per:::::::\nleast :::one:::::::\n\nmissing:::::\nsupposes :::::\n(PPL) :::::which::::::::\nlabel :::per::::::::\ninstance::is:::::::::\npositive:::::\nlimited::to::::::\n\nimage ::::::::::\nclassi\ufb01cation\nroughly::::::\ndivided\npositive:::and:::::::\nnegative\npositive\nsupposes ::::only :::part:::of ::::::\npositive:::::::\nlabeling\nsingle:::::::\nrelated::::::\nsetting\nobserved. ::::The ::::most::::::\nsuitable ::for::::our :::::::\nproblem.\nfacing ::::tasks\n\nlabel ::is :::not :::::::\nimage::::::::::::\n\npositive:::::\nassumption::of::at:::::\n\nobserved:::::::\npositive::::::::\n\nclassi\ufb01cation, ::::thus::::::\n\nsetting::::that::is::::not ::::::\n\nconsider::a :::::more ::::::\n\nlabels:::are:::::::\n\nfocus::on::::::\n\ninstance; :::::\n\npartial:::::::\n\n1\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 1: Illustration of PAMLC task. A. Partially annotated data samples in document relation\nextraction. B. Severe imbalanced distribution of positive (red scatters) and unannotated classes (blue\nscatters). C. Performance comparison of different approaches for document-level relation extraction.\n\nextraction:::::\n\nnegative ::::::\n\npositive:::::::\n\nsetting:::but::::::\n\nhave :::any:::::::\n\nclasses::or:::at ::::\n\nsuffer ::::::::::\n\ntask. :::For :::the :::::\n\nimage:::::::::::\n\nclasses. :::For::::::::\n\nexample,::it::is:::::::\n\ninstance ::::may:::not:::::\n\napplied::to::::our :::::\npartial::::::::\n\nlearning::::only:::::::\nunlabeled::::::::\n\nPositive-Unlabeled:::::(PU)::::::::\nfocuses ::on:::::::\n\nentity :::pair::in:::::::\nsettings:::::\nnecessary ::::::::::\n\nrelation::::::::\ncould::be:::::::\ndirectly::::::\ninformation::::(i.e.,::::::\nmentioning:::::task ::is::::::::::::::::\ntraditional:::PU:::::::\nsetting, ::::::::\npositives::::and :a:::set ::of::::::::\n\nsamples. :::::::\ngenerally::::::\n(Su et al., 2021; Su et al., 2022; Su et al., 2021)::::::::\nreadily:::::\nwhich :::::\ncannot::::::\nsection.:\nrelated ::::work:::::::\n\nthere ::is ::no\npossible::::that:::::\nclassi\ufb01cation::::task,:::::::\nmethods\ndrops:::due\nperformance:::::\nleast::::one :::::::\nclass).\npositive :::::\nlearning:::::::::::::::::::::\n(Jaskie & Spanias, 2019).\nbinary :::::::\nclassi\ufb01er::::with::an\nlearning:a::::::\nmethods::of:::PU\nBesides,:::::many::::::::\nadvanced:::::::\nassume::a ::::given::::::::::::::\npositive-negative\nmulti-label\nadapt::to:::our:::::::::\n\n:::::where::an::::::::\nbetween:::an :::::\nrelation:::::::\n::::::\nthese ::::::\ndesigned:::for:::::\n:::::::\n::to :::::::::\ninsuf\ufb01cient:::::::::\nAnother :::::worth::::::::::\n:::::::\nfrom :::our ::::::\nDifferent:::::\n:::::::\nincomplete:::set ::of :::::::\n:::::::::\nlearning ::::::::::::::::::::::::::::::::::::::\n:::::::\ndistribution:::::\n:::::::::\nsettings. :::We ::::::\n:::::::\nThere are some simple approaches, i.e., negative mode or re-weight strategies, that could be applied\nto our problem settings. Negative mode treats all unknown classes as negatives, which learns a biased\ndistribution and causes high precision and low recall evaluation scores on the complete test set. Re-\nweight strategies based on negative mode generally contain positive up-weight and negative under-\nweight or sampling (Li et al., 2020). We attempt these approaches and observe that the performance\nof the negative mode drops signi\ufb01cantly when the ratio of annotated positive classes decreases. Re-\nweight strategies partly improve the model performance but still perform unsatisfactorily when only\na very small set (10%) of positive class annotations is available (shown in Figure 1 C).\n\nbalanced ::::::::::::::\ndetails ::of ::all:::the:::::::\n\nratio,::::::\nsettings::in:::the::::::\n\nprior ::or :a::::::::\ndiscuss:::the::::::\n\npositive-negative ::::\n\nPrevious works (Silver et al., 2016; Feng et al., 2018; Nooralahzadeh et al., 2019) have demon-\nstrated the powerful exploration ability of Reinforcement Learning (RL). Furthermore, RL has\nbeen shown great success on distant or even zero annotations. To deal with the partially anno-\ntated multi-label classi\ufb01cation problem with imbalanced issues, we propose an RL-based frame-\nwork, namely Partially Annotated Policy Gradient (PAPG), that is devoted to estimating an unbiased\ncombine:::the:::::::::\ndistribution only with the observation of annotated labels. ::::::::::\nexploration\ndesigning:a\nlearning:::by ::::::::\nability::of::::::::::::\n:::::\nalgorithm :::and::a ::::value\npolicy:::::::\nmulti-label::::::::\n:::::\nbeyond:::the::::local\nnetwork ::(as::a :::::\ntrained ::::with :a:::::::::\n:::::::\nglobal::::::\ncontributes::::most\nreward,:::we::::::\nreward::::::::\n::::::\n::to ::::::::\nproblem. In addition, inspired by the actor-critic RL algorithm (Bah-\nimbalanced::::::::\ndanau et al., 2016), we iteratively train the value network and the policy network, which achieves\ndynamic reward estimation and a one-stage training procedure. To gradually enhance the assessment\naccuracy of our value network, we carefully select predicted positive classes with high con\ufb01dence\nto enhance the training set of the value network.\n\nSpeci\ufb01cally, :::we :::::::\nsupervised:::::::\npolicy-based:::RL:::::::::\nreward. :::::::\ninstance,:::::which::::::::::\n\nreinforcement:::::::\nnetwork:::(as::a :::::::::\ncritic) ::::::\ndesign :a::::::\nalleviating:::the::::::::::\n\nlearning::::and :::the ::::::::::\nclassi\ufb01er)::::::::\nsupervised::::loss ::to ::::::\nassessing::::::::::\n\nlearning ::::with :a:::::::::::\nprovide::::local:::::::\npredictions ::of::::each::::::::\n\nability ::of :::::::::\n\nexploitation::::::\n\nBesides,::::::\n\nMoreover, our RL framework is concise and \ufb02exible to guarantee its generalization and adaptation\nto many tasks. We conduct suf\ufb01cient experiments across various complicated domains: synthetic toy\nsetting (\u00a74.1), multi-label image classi\ufb01cation task (\u00a74.2), and multi-relation document extraction\ntask (\u00a74.3). All experimental results show the effectiveness of our framework and demonstrate a\nsigni\ufb01cant improvement over previous work.\n\n2 RELATED WORK\n\nProblem Settings on Weak Annotation Learning Weakly supervised learning has long attracted\nresearchers\u2019 interest because large-scale datasets with high-quality human annotations are time-\nconsuming and dif\ufb01cult. Weakly supervised learning includes different settings according to dif-\nferent assumptions Among them, noisy-label learning (Coscia & Neffke, 2017; Yang et al., 2018;\n\n2\n\nResidence, Living inDeveloper, Sibling,\u2026,Place of birthAlbemarleCountyisacountylocatedinthePiedmontregionoftheCommonwealthofVirginia.ItscountyseatisCharlottesville,whichisanindependentcityandenclaveentirelysurroundedbythecounty.[\u2026]However,itsmostfamousinhabitantwasThomasJefferson,whobuilthisestatehome,Monticello,inthecounty.<Thomas Jefferson,?,Commonwealthof Virginia>zQueryAnnotationUnknown94 Labels Probably CorrectPAPGABC\fUnder review as a conference paper at ICLR 2024\n\nFeng et al., 2018; Ren et al., 2020; Cai et al., 2022) typically utilize datasets containing label noise,\nencompassing instances of both false positives and false negatives; partial-label learning (Zhang &\nYu, 2015; Xu et al., 2019; Wang et al., 2021; Wu et al., 2022; Tian et al., 2023) refers to the task\nwhere each training instance is annotated with a set of candidate labels in which only one is the tar-\nget; semi-supervised learning (Berthelot et al., 2019; Zheng et al., 2022; Yang et al., 2022) typically\nleverages a small set of full-labeled data and an amount of unlabeled data; PU learning (Kiryo et al.,\n2017; Su et al., 2021; Acharya et al., 2022) typically refers to learning a binary classi\ufb01er (positive vs.\nnegative) using an incomplete set of positives and a set of unlabeled samples without any explicitly\nlabeled negative examples. Our problem setting belongs to the domain of weak-supervised learning\nbut distinguishes itself from all preceding problem settings.\n\nSimilarly,::::::::::::::::::::::::::::::::::\nnegative ::::::\n\nLearning Methods for Partial Supervision Similar to our setting, some previous work addresses\npartial supervision problems. There are some methods, such as partial conditional random \ufb01eld, that\ndeal with single-label multi-class tasks with partial supervision (Mayhew et al., 2019; Ef\ufb02and &\nCollins, 2021; Li et al., 2021; Zhou et al., 2022). In the multi-label classi\ufb01cation area, Cole et al.\n(2021); Kim et al. (2022) assumes a speci\ufb01c setting where only one label for each image is available.\nBen-Baruch et al. (2022) designs a class-selective loss to tackle the partial observation of a small set\nof positives and negatives. :::::::\nDurand et al. (2019); Durand et al. (2022)::::also:::::::\npartial\nsuppose:::::\nclasses. The methods of PU learning contain cost-sensitive ap-\npositive::::and :::::::\nobservation::of:::::::\n:::::::::\nproaches (Christoffel et al., 2016; Su et al., 2021) which assume the data distribution prior to achiev-\ning unbiased risk estimation; representation clustering approaches Acharya et al. (2022) leveraging\ncontrastive learning to generate pseudo-positive/negative labels; and sample-selection approaches\n(Zhang & Zuo, 2009; Luo et al., 2021) which are devoted to \ufb01nding likely negatives from the un-\ncannot ::be\nlabeled data according to the heuristic methods or sample con\ufb01dence. ::::These::::::::\nlabel :::::::::\nestimate:::the:::::\ndistribution\nbecause::it ::is :::::::::\neffectively:::::::\n::::::::\n::::prior::to:::the::::::::::\nmissing. We note a\nlabels:::are::::::::\nconcurrent work (Yuan et al., 2023) extending positive-unlabeled learning into a multi-label classi-\n\ufb01cation task. Nevertheless, their research has primarily concentrated on method development within\nthe domain of image classi\ufb01cation. In contrast, our framework exhibits broader applicability, ex-\ntending its utility to various tasks.\n\ntask,:::::::\ndataset ::in:::our::::::\n\nadapted::to::::our ::::\nmulti-label ::::::\n\nknow ::or :::::::\nnegative :::::\n\nsetting::::::where ::all::::the :::::::\n\nimpractical:::to :::::\n\nmethods ::::::\n\nReinforcement Learning under Weak Supervision There are many previous works leveraging\nReinforcement Learning (RL) to solve tasks only with weak supervision (Feng et al., 2018; Zeng\net al., 2018; Luo et al., 2021; Chen et al., 2023). In the NLP \ufb01eld, to precisely leverage distant data,\nQin et al. (2018); Feng et al. (2018) train an agent as a noisy-sentence \ufb01lter, taking performance\nvariation on development or probabilities of selected samples as a reward and adopting policy gra-\ndient to update. Nooralahzadeh et al. (2019) expand their methods to NER task. Recent work of\nChen et al. (2023) also conducts RL to remove the noisy sentence so as to improve the fault diag-\nnosis system. Notably, RL learning on distantly supervised learning aims to \ufb01lter false positives,\nwhereas our goal is to identify false negatives. A closer related work to us is Luo et al. (2021), in\nwhich an RL method is designed to solve the PU learning problem. But unlike us, their agent is a\nnegative sample selector, aiming to \ufb01nd negatives with high con\ufb01dence and then couple them with\npartial positives to train a classi\ufb01er. Besides, they suppose a fully annotated validation dataset and a\nbalanced positive-negative distribution prior.\n\n3 REINFORCEMENT LEARNING WITH PARTIAL SUPERVISION\n\nWe propose a new RL framework to solve the partially annotated multi-label classi\ufb01cation (PAMLC)\ntask. We formulate the multi-label prediction as the action execution in the Markov Decision Process\n(MDP) (Puterman, 1990). We design both local and global rewards for actions to guide the action\ndecision process. The policy-based RL method is adopted to train our policy network. We introduce\nthe details of our RL framework in the following subsections.\n\n3.1 PROBLEM SETTING\n\n=\nxi}\nWe \ufb01rst mathematically formulate the PAMLC task. Given a multi-label dataset\n, each\nX\n{\n, yc\nis labeled with a partially annotated multi-hot vector yi = [y1\nxi 2X\n,\ni ,\ni ,\ny|C|i\n], where\n\u00b7 \u00b7 \u00b7\n::::::\nyc\nis TRUE for instance xi and ::|C|::is:::the:::::::::\ndenotes whether class c\ncardinality:::of ::set\ni 2{\nc; yc\ni = 1\nclasses\npositive::::::\n. In terms of partial annotation, we assume that\nis a subset of :::gold:::::::\n:C\n}\nUNKNOWN::::::\nregarding ::to :::xi, :::i.e.,::::::::::\nclasses. Typically, in the multi-label setting,\n} :is:::the:::set::of:::::::::\n::::::::\n\nc; yc\n{\n\n0, 1\n}\n\ni = 0\n\n2C\n\n\u00b7 \u00b7 \u00b7\n\n{\n\n3\n\n\fUnder review as a conference paper at ICLR 2024\n\nInput $\n\nPolicy Network\n(#)\n\"%\n\n$!(&)\n\n0.9\n0.2\n0.8\n0.1 \n\u2026\n\n1\n0\n1\n0 \n\u2026\n\nAnnotation # 1\n0\n0\n0 \n\u2026\n\nAction \n!\"~$!(&)\n\nGlobal Reward\n\n0.3       \n\nReward %\n\nUpdate $! with\n\u2207!)\"# *\n\nLocal Reward\n\n-0.3 -0.9 1 0.5       \u2026\n\nPolicy Network\n(#&')\n\n\"%\n\nValue Network\n(#)\n!!\n\nMax Prob\nAction\n\nAnnota\ntion #\n\nUpdate +$\twith\nSupervised Loss\n\nValue Network\n(#&')\n\n!!\n\n\u2026\n\n+\n\n1\n0\n1\n0 \n\u2026\n\n1\n0\n0\n0 \n\u2026\n\nFigure 2: Illustration of our RL framework.\nrepresents union operation. We iteratively update the\npolicy network and value network. The augmented training data are curated for the value network.\n\nL\n\n. In some\nthe size of the label set is dozens or hundreds; thus\ncomplicated tasks, such as Relation Extraction, we further de\ufb01ne a special label <None> to xi if\nyi = [yc\ni = 0]|C|c=1. It should be mentioned that we do not have any fully annotated data, both the\ntraining set and validation set being partially annotated.\n\n}| \uf8ff |C|\n\ni = 1\n\ni = 0\n\n<<\n\n|{\n\n|{\n\n}|\n\nc; yc\n\nc; yc\n\nAs aforementioned, negative mode approach is susceptible to training models that conform to the\nimbalanced data distribution, resulting in high precision but low recall on the test set. Based on\nthe negative mode, we introduce a reinforcement learning (RL) framework designed to mitigate\ndistribution bias and encourage the multi-label classi\ufb01er to predict more potential positive classes.\n\n3.2 MODELING\n, R) which contains a set of environment and\nTypically, basic RL is modeled as an MDP (S, A, \u21e1,\nagent states S, a set of actions A of the agent, the transition probabilities\nfrom a state to another\nstate under action a, and the reward R. The goal of an RL agent is to learn a policy \u21e1 that maximizes\nthe expected cumulative reward. In our problem setting, we do not take state transitions into account\nbecause our action execution does not change the agent and environment. The policy \u21e1\u2713 of our RL\nframework is a multi-label classi\ufb01er constructed by a neural network with parameter \u2713. We following\nde\ufb01ne the constituents in our RL framework in detail.\n\nT\n\nT\n\nStates A state s includes the potential information of an instance to be labeled. In our setting,\nthis information consists of instance features, which are essentially continuous real-valued vectors\nderived from a neural network.\n\nActions Due to the multi-label setting, our agent is required to determine the label of each class c\nfor one instance. There are two actions for our agent: setting the current class as TRUE (yc\ni = 1) or\nUNKNOWN (yc\ni = 0). Consequently, it is necessary to execute a sequence of (size of class set) actions\n::::::::\nto completely label an instance.\nPolicy Our policy network outputs the probability \u21e1\u2713(yc\ns = xi) for each\ni |\naction condition on the current state. we adopt the model structure commonly utilized in previous\nsupervised studies as the architecture for our policy network.\n\nxi) = P (a = yc\ni |\n\nRewards Recall that our primary objective is to acquire a less biased label distribution compared\nto the supervised negative mode training approach using the partially annotated training dataset.\nWe anticipate that our PAPG possesses the capacity for balanced consideration of both exploitation\nand exploration. exploitation ensures that our agent avoids straying from local optima direction,\nwhile exploration motivates our agent to adapt its policy, preventing over\ufb01tting to partial supervision\nwithin the broader global context. Inspired by the actor-critic RL algorithm (Bahdanau et al., 2016),\nwe design our rewards function containing two parts: a local reward from a trainable value network,\nwhich provides immediate value estimation of each action and a global reward regarding the overall\nperformance of the whole actions sequence for each instance.\n\nSpeci\ufb01cally, the local reward calculates the reward of each action (for each class) of each instance\naccording to the value network con\ufb01dence to an action:\n\nrc\ni (V\u0000, xi, c) =\n\n1, log\n\n1, log\n\n1\n1\n\nC(\n\nC(\n\n\u0000\n\n\u0000\n\n4\n\n8\n<\n\n:\n\npc\nV\u0000\npc\nV\u0000\npc\nV\u0000\n\n\u0000\n\u0000\npc\n\n(xi)\n(xi) , 1)\n(xi)\n, 1)\n\n(xi)\n\nV\u0000\n\nif \u02c6yc\n\ni = 1,\n\nif \u02c6yc\n\ni = 0.\n\n(1)\n\n\fUnder review as a conference paper at ICLR 2024\n\nwhere pc\nV\u0000\nV\u0000, and C(\n\u0000\nx > 1; c) otherwise, C(\nrewards of an instance.\n\n\u0000\n\n(xi) is the probability of class c being TRUE for instance xi, calculated by a value network\n1, x, 1) = 1 if\n\u0000\n1, x, 1) = x. We sum out the local rewards of all classes as part of the \ufb01nal\n\n, 1) is a clamping function: a) C(\n\n1, x, 1) =\n\n1; b) C(\n\n1 if x <\n\n1,\n\n\u0000\n\n\u0000\n\n\u0000\n\n\u00b7\n\nIntuitively, the local rewards offer a preliminary yet instructive signal to guide the learning process in\nour PAPG framework. This signal inherits the exploitation aspect from the supervised loss training\n(as the value network is trained through supervised learning). Its purpose is to prevent the PAPG\nfrom engaging in excessively invalid exploratory behavior within the large action space, thereby\nenhancing the overall learning ef\ufb01ciency. Nevertheless, relying solely on these local rewards may\npotentially lead the PAPG system to converge to a biased \u201cnegative mode\u201d solution. To mitigate this\nrisk, we introduce global rewards to stimulate more comprehensive exploration during the learning\nprocess.\n\nAs for the global reward, we employ a straightforward yet highly effective scoring function, which\nis computed based on the recall metric. In detail, for the whole classes prediction \u02c6yi of xi with the\nobserved ground truth yi, the recall score is:\n\nyc\ni = 1\n\nrecall(yi, \u02c6yi) = |{\n\n^\n|{\nThe key insight here revolves around the \u201crecall\u201d metric, which serves as an exactly accurate evalua-\ntion measure (compared to \u201cprecision\u201d and \u201cF1\u201d) because a substantial portion of positive instances\nremains unlabeled within the context of partially annotated datasets. Furthermore, to enhance recall\nscores, the policy network is encouraged to predict a greater number of classes as TRUE.\n\n(2)\n\ni = 1, yc\n\u02c6yc\ni = 1, yc\nyc\n\nyi, \u02c6yc\nyi}|\n\ni 2\ni 2\n\ni 2\n\n\u02c6yi}|\n\nThe complete reward function for a sampled action sequence of an instance is:\n\nR(xi, \u02c6yi, V\u0000, yi) =\n\n1\n\nrc\ni (V\u0000, xi, c) + w\n\nrecall(yi, \u02c6yi),\n\n(3)\n\n\u21e4\n\n|C| Xc\n2C\nwhere w is a weight controlling the scale balance between local reward and global reward.\n\n3.3 PREDICTION\ninstance:::are:::::::\npredictions ::of::::each:::::::\n:::The::::\ufb01nal::::::::::\n(\u21e1\u2713(yc\nprobabilities:::are ::::more::::than:::0.5:::::::::::::::\ni |\n::::::::::\nprediction::of::::each::::class:::for:::an :::::::\ndirectly::::::\n::::::\nreason:::of :::::::\n::to ::::\n\noutput ::the:::::::::\nenhancement:::for :::the :::::\n\ndecided:::::::::\nxi) > 0.5):::are ::set::to::as::::::\ninstance.::::::::\n\nreducing:::the::::::::\n\nlabel:::::::::::\n\naccording ::to:::the::::::\n\npolicy :::::::\n\nTRUE.::::Note::::that :::our ::::value:::::::\n\nnetwork.:::::::\n\nClasses :::::whose\nnetwork:::can\npredictions\nleverage::its:::::::::\n\nHowever,:::we ::::only :::::::\nenhanced:::::\n\nin\ufb02uence::of::::::::\n\nlabels.\nnoisy::::::\n\n3.4 LEARNING\nWe iteratively train our value network and policy network in an end-to-end fashion. Since the value\nnetwork plays a critical role in guiding policy network learning, we employ data enhancement tech-\nniques during the training of the value network to enhance the precision of value estimations. It is\nimportant to emphasize that we intentionally exclude the enhanced data from participation in the\ncalculation of the recall reward. This decision is motivated by the desire to maintain the precision\nof the global reward and prevent potential noise introduced by the enhanced data. Additionally, it is\nnoteworthy that the enhanced data could contribute to the bene\ufb01t of the policy network through the\nlocal reward signal provided by the value network.\n\nIt is widely acknowledged that the training process in RL can experience slow convergence when\nconfronted with a vast exploration space. Inspired by previous RL-related works (Silver et al., 2016;\nQin et al., 2018), we initiate our process by conducting pre-training for both our policy and value\nnetworks before proceeding with the RL phase. Typically, pre-training is executed through a super-\nvised method. In our settings, a range of trivial solutions for PAMLC can serve as suitable candidates\nfor pre-training. We simply utilize the negative mode for pre-training in most cases. However, as pre-\nviously mentioned, the negative mode has a tendency to acquire a biased label distribution due to\nthe presence of severe label imbalance. Thus, we implement an early-stopping strategy during the\npre-training phase to prevent convergence. We following introduce the detailed learning strategies\nand objectives.\n\nObjective for Value Model Generally, a well-designed supervised objective urges models to learn\nexpected outputs by learning from annotated data. This process typically refers to the exploitation,\nwhere the supervised model \ufb01ts the distribution of label annotations. We denote the supervised\n\n5\n\n\fUnder review as a conference paper at ICLR 2024\n\nobjective by a general formulation:\n\nLSU P (\u2713) =\n\np(xi)\n\nD\n\n(yi, \u02c6yi),\n\n(4)\n\nXxi2X\n\nis a task-speci\ufb01c distance metric measuring the distance between annotation yi and predic-\nwhere\ntion \u02c6yi. Recall that we treat all the unannotated classes as negatives in the partially annotated setting\nin order to perform supervised learning.\n\nD\n\nObjective for Policy Model As stated in previous work (Qin et al., 2018), policy-based RL is more\neffective than value-based RL in classi\ufb01cation tasks because the stochastic policies of the policy\nnetwork are capable of preventing the agent from getting stuck in an intermediate state. We leverage\npolicy-based optimization for RL training. The objective is to maximize the expected reward:\n\nJP G(\u2713) = E\u21e1\u2713 [R(\u2713)]\n\n\u21e1\n\np(xi)\n\nbatch\n\nXxi2\n\n\u21e1\u2713( \u02c6yi|\nX\u02c6yi\u21e0\n\n\u21e1\u2713( \u02c6yi|\n\nxi)\n\nxi)R( \u02c6yi.xi),\n\n(5)\n\nThe policy network \u21e1\u2713 can be optimized w.r.t. the policy gradient REINFORCE algorithm (Williams,\n1992), where the gradient is computed by\nr\u2713JP G(\u2713) =\n\nr\u2713 ln(\u21e1\u2713(\u02c6y\n\nxi))R(\u02c6y, xi),\n\np(xi)\n\n(6)\n\n|\n\nbatch\n\nXxi2\n\n\u21e1\u2713(\u02c6y\nX\u02c6y\n\u21e0\n\n|\n\nxi)\n\nwhere p(xi) is a prior distribution of input data. Speci\ufb01c to uniform distribution, p(x) =\n\n1\nxbatch|\n\n|\n\n.\n\nprediction::of:::::\n\nrefers ::to:::the:::::::::\n\nOverall Training Procedure The overall training process is demonstrated in Algorithm 1, ::::where\n\u02c6yc\nsample::xi\ni = V\u0000(xi)c = 1 :::::\n:::::::::::::::\nTRUE. It needs to be clari\ufb01ed that our computation of local rewards for actions is based\n::::being::::::\non the annotated positive classes and a randomly selected subset of unknown classes rather than\nconsidering the entire sequence of actions of an instance, which is intended to emphasize the impact\nof positive classes within the computation of local rewards. We empirically prove the effectiveness\nof this strategy. Furthermore, we \ufb01x the value network once it converges, a strategy employed to\nenhance training ef\ufb01ciency.\n\noutputs :::for :::the ::::::\n\nclass :c::::that:::::\n\nnetwork:::::::\n\nvalue :::::::\n\nAlgorithm 1: Partially Annotated Policy Gradient Algorithm\nyi}\nxi}\nInput: Observed data\nOutput: Optimal parameters \u2713\u21e4\n\n, partial annotations\nvalue network V\u0000, REINFORCE learning rate \u21b5, con\ufb01dence threshold \u0000\n\n=\n\n=\n\nX\n\nY\n\n{\n\n{\n\n, pre-trained policy network \u21e1\u27130 ,\n\n1 e\n\n0, \u2713\u21e4\n\n\u27130, ::::::::\n\nenhanced ::::::::\n\nannotation set \u00af\n\nY Y::::\n\n2 while e < total training epoches do\nTraining set for value network: (\n3\n\nX\nTraining set for policy network: (\nfor\n\n, \u00af\nY: )\n)\n,\nY\nX\nin total batches do\nXbatch 2X\nUpdate \u0000 by minimizing Equation (4) with \u00af\n\nfor step t < sample steps T do\n\n\u21e1\u2713(\u02c6yi|\nFor each xi 2X batch, sample \u02c6yi w.r.t. \u02c6yi \u21e0\nxi)\nCompute R(xi, \u02c6yi, V\u0000, yi) according to Equation (3)\n\nfor value network\n\nYbatch\n\n:::::\n\n\u2713 + \u21b5\n\nr\u2713JP G(\u2713) according to Equation (6)\nxi) >\u0000 and \u02c6yc\n(\u21e1\u2713(\u02c6yc\ni = 1::or::::::::::::::::::::::::::::::::::::\n\ni = V\u0000(xi)c = 1 ::))\n\ni :=::1 ::if ::::::\n\n(yc\n\ni = 1\n|\n\nupdate policy network using \u2713\n\n,:::::where:::\u00afyc\n\n\u00af\n[\u00afyc\ni ]|C|c=1}\n:::::::::::::\nY {\nif eval(\u21e1\u2713) > eval(\u21e1\u2713\u21e4 ) then\n\u2713\n\u2713\u21e4\ne + 1\n\ne\n14\n15 return \u2713\u21e4\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n4 EXPERIMENTS\n\nTo verify the effectiveness of our proposed RL framework, we experiment with three widely-\nconcerned tasks. The \ufb01rst one follows the general setting in PU learning, but we consider an im-\n\n6\n\n \n \n \n \n \n\fUnder review as a conference paper at ICLR 2024\n\nTable 1: F1 scores with varying ratios of positive annotations. We take images of the \u201cAirplane\u201d\ncategory as positives in this table.\n\nMethod\n\n10% 20% 30% 40% 50% 60% 70% 80% 90%\n\n44.8\nnnPU\nImbalancednnPU 48.6\n\nNegative Mode\nPAPG (Ours)\n\n7.0\n50.2\n\n47.9\n53.0\n\n17.1\n64.6\n\n49.1\n59.1\n\n28.7\n66.6\n\n50.4\n62.8\n\n37.6\n67.8\n\n52.8\n65.2\n\n56.9\n69.8\n\n54.1\n64.6\n\n55.6\n70.8\n\n56.1\n64.9\n\n64.2\n75.1\n\n57.4\n67.7\n\n71.2\n75.8\n\n56.7\n69.2\n\n75.2\n76.9\n\nbalanced situation. This experiment aims to prove the adaptation of our framework to binary clas-\nsi\ufb01cation problems. The second and the third focus on two partially annotated multi-label tasks,\ndocument-level relation extraction and multi-label image classi\ufb01cation, selected from classical Nat-\nural Language Processing (NLP) tasks and Computer Vision (CV) tasks, respectively.\n\n4.1 EXP1: SYNTHETIC CLASSIFICATION\nWe conduct binary image classi\ufb01cation experiments with the same setting following Su et al. (2021)\nthat concentrates on positive/negative imbalanced problems in PU learning. With our formulation, an\ninstance xi is an image, and the label of an instance in binary classi\ufb01cation settings can be denoted\nas yi = [y1\ni is the label of negative. The prediction for\neach image is conducted by setting y corresponding to the higher score as 1 and the other as 0.\n\ni is the label of positive and y2\n\ni ] where y1\n\ni , y2\n\nDataset The imbalanced datasets are constructed\nfrom CIFAR101 by picking only one category as\npositives and treating all other data as negatives.\nHence, there are 50,000 training data and 10,000 test\ndata as provided by the original CIFAR10. To make\nthe training data into a partially annotated learning\nproblem, we randomly sample a ratio of positives as\nannotated data and all the leaving training data as an\nunknown set.\n\nFigure 3: Experimental results of the setting\nwith \u201ctruck\u201d category as positives.\n\nCon\ufb01guration and Baselines Of note, our frame-\nwork can integrate any supervised model architec-\nture. For a fair comparison, we take the same archi-\ntecture of Kiryo et al. (2017); Su et al. (2021) as our\nvalue and policy networks, i.e., a 13-layer CNN with\nReLU and Adam as the optimizer. Kiryo et al. (2017)\ndesigned an algorithm nnPU for PU learning with balanced binary classi\ufb01cation data, while Su\net al. (2021) proposed ImbalancednnPU considering imbalanced setting. We take these two previ-\nous state-of-the-art models as our compared baselines. We re-run nnPU and ImbalancennPU with\ntheir provided codes and con\ufb01gurations and report the results. We tune the hyper-parameter w in\nwith different experiment settings, and w is dynamically adjusted during\nEq.3 between\ntraining2. The action sampling number T is 100, and the negative reward sampling is 20% for all\nsettings. The threshold \u0000 to choose enhancement labels is 0.8. We keep the values of other hyper-\nparameters the same as Su et al. (2021). Following previous work, we evaluate all the methods with\nF1 scores. Unless stated otherwise, the hyper-parameters speci\ufb01ed in our framework remain the\nsame in the following experiments.\n\n10, 20, 50\n{\n\n}\n\nResults We show F1 scores with varying ratios of annotated positives in Table 1 (Precision and\nRecall metrics can be found in Appendix B.2). We perform supervised training with Binary Cross-\nEntropy (BCE) loss in Negative Mode. Naturally, the supervised training performs worse by worse\nas the ratios of annotated positives decrease. It is obvious that Our PAPG achieves signi\ufb01cant\nimprovements over Negative Mode, especially when the ratio of annotated data becomes lower.\nCompared to previous work, our framework still demonstrates its superiority even though Imbal-\nancednnPU is speci\ufb01cally designed for PU learning with imbalanced settings. Note that according\n\n1A multi-class dataset containing ten categories. https://web.cs.toronto.edu/\n2Intuitively, the w of recall reward should be dropped along with the training epochs because our value\n\nnetwork provides more and more accurate local rewards bene\ufb01cial by data enhancement before convergence.\n\n7\n\n\fUnder review as a conference paper at ICLR 2024\n\nTable 2: Experimental Results on COCO datasets with varying ratios of positive classes annotations.\n\nMethod\n\nERP\nROLE\nP-ASL+Negative\nP-ASL+Counting\n\nNegative Mode\nPAPG (Ours)\n\n10%\n\n30%\n\n50%\n\n70%\n\n90%\n\nF1\n\nmAP\n\nF1\n\nmAP\n\nF1\n\nmAP\n\nF1\n\nmAP\n\nF1\n\nmAP\n\n-\n-\n45.2\n5.1\n\n6.4\n68.3\n\n63.8\n58.2\n66.9\n46.4\n\n50.6\n66.6\n\n-\n-\n52.1\n26.4\n\n33.7\n77.0\n\n71.0\n72.4\n74.6\n63.4\n\n64.3\n77.5\n\n-\n-\n54.0\n53.7\n\n52.9\n79.1\n\n73.5\n76.6\n76.9\n76.1\n\n73.8\n80.4\n\n-\n-\n71.9\n71.6\n\n72.3\n79.0\n\n73.8\n79.5\n81.0\n80.1\n\n81.2\n81.4\n\n-\n-\n80.3\n60.4\n\n80.1\n80.5\n\n74.4\n81.1\n83.3\n80.4\n\n83.5\n83.4\n\nto the data construction, any category of data can be chosen as the positive set. To further make our\nexperiments convincing, we show the results of different data construction in Figure 3.\n\n4.2 EXP2: MULTI-LABEL IMAGE CLASSIFICATION\nWe demonstrate the effectiveness of our method on the multi-label image classi\ufb01cation task.\n\nDataset Following Ben-Baruch et al. (2022) (P-ASL), which deals with partial annotations con-\ntaining both positive and negative classes, we utilize MS-COCO dataset (Lin et al., 2014) containing\n80 classes. We keep the original split with 82081 training samples, and 40137 test samples. We sim-\nulate the partial annotations following the operations in P-ASL. But different from them, we only\nretain the positive classes in their partial annotations and take all the rest of the classes as UNKNOWN.\n\nCon\ufb01guration and Baselines For a fair comparison, our value and policy networks have the same\narchitecture as P-ASL. Due to the different partially annotated settings, we re-run P-ASL utilizing\ntheir codebase but with our datasets. P-ASL+Negative means training a model taking all UNKNOWN\nas negative classes to predict label distribution as prior. P-ASL+Counting means counting partially\nlabeled positive classes as distribution prior. ::We::::also::::::\nfrom Cole\ncode. We tune the hyper-parameter w between\net al. (2021) ::::with :::our:::::::\ndatasets,::::::::\n5, 7, 12\nin this task. Following previous work (Ridnik et al., 2021; Huynh & Elhamifar, 2020),\n{\n}\nwe use both F1 scores and mAP as evaluation metrics in this task. Detailed methodology of the\nre-weight approach and the detailed formula of metric calculations can be found in Appendix A.\n\nre-run ::::EPR::::and ::::::\n\nROLE::::::::\n\nmethods ::::\n\nutilizing ::::\n\nof\ufb01cial :::::\n\ntheir ::::::\n\nResults Experimental results with different annotation ratios are shown in Table 12. We can \ufb01nd\nthat without annotated negative classes to estimate a relatively accurate distribution prior, P-ASL\nperforms unsatisfactorily, especially in F1 scores when the annotation ratios decrease. Our PAPG\ntimes:::and:::::\nmodel outperforms all baselines in all settings. :::::::::::\nfound\n::::very ::::\nrobustness:::and\nsmall::::::::\nscores:::and:::::mAP,::::::which :::::::::::\nframework. Standard deviations of three runs and more experimental results can be\nstability:::of :::our::::::::::\n::::::\nfound in Appendix B.2.\n\nFurthermore, ::we:::ran::::our :::::model::::\n\ndemonstrates:::the::::high:::::::::\n\ndeviations:::of ::F1::::::\n\nstandard ::::::::\n\nthree:::::\n\n4.3 EXP3: DOCUMENT-LEVEL RELATION EXTRACTION\nDocument-level Relation Extraction (DocRE) is a task that focuses on extracting \ufb01ne-grained rela-\ntions between entity pairs within a lengthy context. Align to our formulation, an input xi is an entity\npair, and yi represents relation types between the entity pair. An entity pair may have multiple rela-\ntions or have no relation in DocRE. Thus, we conduct this experiment to study how our framework\nperforms when <None> label exists in a multi-label classi\ufb01cation task.\n\nDataset We chose the Re-DocRED which is the most complete annotated dataset in DocRE. The\nis 97 (contains <None>) in Re-DocRED. To simulate partial annotation, we\nsize of label set\nrandomly kept a ratio of annotated relations as the above experiments do.\n\nC\n\nthe state-of-the-art model of\nIn this experiment, we adopt\nCon\ufb01guration and Baselines\nDREEAM (Ma et al., 2023) as the architectures of our value and policy networks. Meanwhile,\nwe keep the same training method with an Adaptive Thresholding Loss (ATL) of DREEAM for our\nvalue network. To the best of our knowledge, there are currently no existing methods speci\ufb01cally\ndesigned for addressing partially annotated DocRE, so we only take Pos Weight and Neg Weight,\nadopting the same methodologies as the image classi\ufb01cation experiments, as our compared base-\nlines. The threshold \u0000 of choosing enhancement labels is 0.95, and we \ufb01nd our framework performs\nrobust to \u0000 varying from 0.5 to 0.95.\n\n8\n\n\fUnder review as a conference paper at ICLR 2024\n\nTable 3: Results on Re-DocRED datasets with varying ratios of positive classes annotations\n\nMethod\n\nP\n\n10%\nR\n\nDREEAM\n\n89.8 3.8\n\nF1\n\n7.3\n\n30%\nR\n\nP\n\nF1\n\nP\n\n50%\nR\n\nF1\n\nP\n\n70%\nR\n\nF1\n\nP\n\n100%\nR\n\nF1\n\n92.0 20.8\n\n33.8 91.8 42.1 57.7 89.6 58.8 70.6 86.0 72.4 78.6\n\n70.0 85.0 66.8 74.8 83.4 72.5 77.6 82.9 77.3 80.0\nPos Weight\n69.8 84.1 68.2 75.3 82.8 72.8 77.5 79.8 78.7 79.3\nNeg Weight\nPAPG (Ours) 58.5 77.0 66.0 83.5 67.71 74.7 81.4 73.6 77.3 83.3 73.9 78.3 80.9 80.8 80.9\n\n84.9 39.8 54.1 85.5 59.3\n86.7 30.7 45.4 85.0 59.3\n\nTable 4: Ablation study on our rewards.\n\nTable 5: Ablation study on our training strategy.\n\nMethod\n\nPAPG (Ours)\nw/o. Local reward\nw/o. Global reward\nw. Prec\nw. F1\n\nRe-DocRED\n\nCOCO\n\nP\n\n64.5\n12.2\n84.5\n85.9\n86.0\n\nR\n\n72.8\n93.3\n45.9\n44.0\n43.3\n\nF1\n\n68.4\n21.6\n59.5\n58.1\n57.6\n\nP\n\n80.9\n61.5\n89.7\n96.2\n89.2\n\nR\n\n59.2\n65.5\n6.9\n29.8\n46.8\n\nF1\n\n68.3\n63.4\n12.8\n45.5\n61.4\n\nMethod\n\nPAPG (Ours)\nw/o. Iterative training\nw/o. Label enhancement\nw/o. Action sampling\nSupervised self-training\n\nRe-DocRED\n\nCOCO\n\nP\n\n64.5\n89.9\n83.6\n88.2\n68.0\n\nR\n\n72.8\n34.6\n47.2\n36.5\n29.0\n\nF1\n\n68.4\n49.9\n60.3\n51.7\n40.7\n\nP\n\n80.9\n76.4\n51.7\n96.4\n89.7\n\nR\n\n59.2\n33.8\n54.8\n20.4\n6.6\n\nF1\n\n68.3\n46.9\n53.2\n33.7\n12.3\n\naverage::::::\n\nresults ::::over :::::\n\ndatasets::::and :::::\n\nversions::of:::::::\n\nsample ::::three\nResults Experimental results in Re-DocRED are shown in Table 3. :::We::::::::\nthem. Similar to Negative Mode,\nreport:::the:::::::\ndifferent :::::::\n:::::::\nDREEAM performs supervised training with all unknown classes as negatives in the partially\nannotated settings, which is our base model in this task. Similar to the above experiments, Our\nPAPG demonstrates its advantage in all annotation ratios. It is worth noting that our framework\nalso achieves improved performance with the full annotated dataset because the full annotations of\nRe-DocRED still miss some actual relations as aforementioned in the introduction. We also show\nprecision and recall evaluation metrics in this task. It can be seen that our framework achieves con-\nsistent improvement in recall scores, suggesting its ability to deal with imbalanced problems and\npredict more positive labels. We provide more detailed experimental results in Appendix B.1.\n\nrandomly:::::::\n\n4.4 ANALYSIS\nWe conduct ablation studies to analyze our PAPG framework both on modeling and training strategy.\n\nRewards Design: To show the effectiveness of combining exploitation and exploration and the\nbene\ufb01t of local and global rewards, we train our framework in the 10% annotations setting with-\nout local rewards and global rewards, respectively. Additionally, we replace the recall scores with\nprecision w. Prec or F1 scores w. F1 as our global rewards to show the effects of different global\nreward designs. Experimental results are shown in Tabel 4. It can be observed that it is hard for an\nRL framework to achieve comparable performance without local rewards to guide exploitation. The\nreason is that the action space of multi-label classi\ufb01cation is too large to \ufb01nd the global optimal di-\nrections. Without our global reward, the recall evaluation score drops a lot (72.78 vs. 45.94), which\ndemonstrates the big advantage of the global reward in alleviating imbalance distribution. Moreover,\nboth the two variants of global reward damage the performance, revealing the advance of taking the\nexactly accurate evaluation as rewards in the partially annotated setting.\n\nTraining Strategy: To verify the effectiveness of our training procedure, we attempt different\ntraining strategies shown in Tabel 5. w/o. Iterative training means that we \ufb01x the value network\nafter pretraining and only train the policy network in the RL training procedure. w/o. Data enhance-\nment means that we still iteratively train our value and policy network but do not enhance pseudo\nlabels for the value network. w/o. Action sampling means that we leverage the whole action se-\nquence to calculate local rewards without sampling operation illustrated in Section 3.4. Supervised\nself-training means that we conduct self-training of the value network. It is obvious that our training\nmethod makes remarkable achievements. More analysis experiments are in Appendix B.1.\n\n5 CONCLUSION\n\nIn this work, we propose an RL framework to deal with partially annotated multi-label classi\ufb01cation\ntasks. We design local rewards assessed by a value network and global rewards assessed by recall\nfunctions to guide the learning of our policy network, achieving both exploitation and exploration.\nWith an iterative training procedure and a cautious data enhancement, our PAPG has demonstrated\nits effectiveness and superiority on different tasks in different domains.\n\n9\n\n\fUnder review as a conference paper at ICLR 2024\n\nREPRODUCIBILITY STATEMENT\n\nThe detailed descriptions of the three experiments covered in this article are described in the con-\n\ufb01guration of the corresponding subsections in Section 4, and the implementation details of the Pos\nWeight and Neg Weight methods are introduced in Appendix A.1. The Dataset module of each\nsubsection clearly describes the construction of the data set used in the experiment. For experiments\non hyperparameter selection, see Appendix B.1. All dataset we used are publicly accessible. Codes\nthat can reproduce our results are available at https://anonymous.4open.science/r/\niclr2024-64EB/.\n\nREFERENCES\n\nAnish Acharya, Sujay Sanghavi, Li Jing, Bhargav Bhushanam, Dhruv Choudhary, Michael Rabbat,\nand Inderjit Dhillon. Positive unlabeled contrastive learning. arXiv preprint arXiv:2206.01206,\n2022. 3\n\nDzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron\nCourville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. arXiv preprint\narXiv:1607.07086, 2016. 2, 4\n\nEmanuel Ben-Baruch, Tal Ridnik, Itamar Friedman, Avi Ben-Cohen, Nadav Zamir, Asaf Noy, and\nLihi Zelnik-Manor. Multi-label classi\ufb01cation with partial annotations using class-aware selective\nloss. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 4764\u20134772, 2022. 1, 3, 8\n\nDavid Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A\nRaffel. Mixmatch: A holistic approach to semi-supervised learning. Advances in neural informa-\ntion processing systems, 32, 2019. 3\n\nHomgmin Cai, Wenxiong Liao, Zhengliang Liu, Xiaoke Huang, Yiyang Zhang, Siqi Ding, Sheng Li,\nQuanzheng Li, Tianming Liu, and Xiang Li. Coarse-to-\ufb01ne knowledge graph domain adaptation\nbased on distantly-supervised iterative training. arXiv preprint arXiv:2211.02849, 2022. 3\n\nChong Chen, Tao Wang, Yu Zheng, Ying Liu, Haojia Xie, Jianfeng Deng, and Lianglun Cheng. Re-\ninforcement learning-based distant supervision relation extraction for fault diagnosis knowledge\ngraph construction under industry 4.0. Advanced Engineering Informatics, 55:101900, 2023. 3\n\nMarthinus Christoffel, Gang Niu, and Masashi Sugiyama. Class-prior estimation for learning from\npositive and unlabeled data. In Asian Conference on Machine Learning, pp. 221\u2013236. PMLR,\n2016. 3\n\nElijah Cole, Oisin Mac Aodha, Titouan Lorieul, Pietro Perona, Dan Morris, and Nebojsa Jojic.\nMulti-label learning from single positive labels. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 933\u2013942, 2021. 1, 3, 8\n\nMichele Coscia and Frank MH Neffke. Network backboning with noisy data. In 2017 IEEE 33rd\n\ninternational conference on data engineering (ICDE), pp. 425\u2013436. IEEE, 2017. 2\n\nThibaut Durand, Nazanin Mehrasa, and Greg Mori. Learning a deep convnet for multi-label classi-\n\ufb01cation with partial labels. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pp. 647\u2013657, 2019. 3\n\nThomas Ef\ufb02and and Michael Collins. Partially supervised named entity recognition via the expected\nentity ratio loss. Transactions of the Association for Computational Linguistics, 9:1320\u20131335,\n2021. doi: 10.1162/tacl a 00429. URL https://aclanthology.org/2021.tacl-1.\n78. 3\n\nJun Feng, Minlie Huang, Li Zhao, Yang Yang, and Xiaoyan Zhu. Reinforcement learning for relation\nclassi\ufb01cation from noisy data. In Proceedings of the aaai conference on arti\ufb01cial intelligence,\nvolume 32, 2018. 2, 3\n\n10\n\n\fUnder review as a conference paper at ICLR 2024\n\nDat Huynh and Ehsan Elhamifar. Interactive multi-label cnn learning with partial labels. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9423\u20139432,\n2020. 8\n\nKristen Jaskie and Andreas Spanias. Positive and unlabeled learning algorithms and applications: A\nsurvey. In 2019 10th International Conference on Information, Intelligence, Systems and Appli-\ncations (IISA), pp. 1\u20138. IEEE, 2019. 2\n\nYoungwook Kim, Jae Myung Kim, Zeynep Akata, and Jungwoo Lee. Large loss matters in weakly\nsupervised multi-label classi\ufb01cation. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 14156\u201314165, 2022. 3\n\nRyuichi Kiryo, Gang Niu, Marthinus C Du Plessis, and Masashi Sugiyama. Positive-unlabeled\nlearning with non-negative risk estimator. Advances in neural information processing systems,\n30, 2017. 3, 7\n\nYangming Li, Shuming Shi, et al. Empirical analysis of unlabeled entity problem in named entity\n\nrecognition. In International Conference on Learning Representations, 2020. 2, 14\n\nYangming Li, lemao liu, and Shuming Shi. Empirical analysis of unlabeled entity problem in\nnamed entity recognition. In International Conference on Learning Representations, 2021. URL\nhttps://openreview.net/forum?id=5jRVa89sZk. 3\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nIn Computer\nDoll\u00b4ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context.\nVision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Pro-\nceedings, Part V 13, pp. 740\u2013755. Springer, 2014. 8\n\nChuan Luo, Pu Zhao, Chen Chen, Bo Qiao, Chao Du, Hongyu Zhang, Wei Wu, Shaowei Cai, Bing\nHe, Saravanakumar Rajmohan, et al. Pulns: Positive-unlabeled learning with effective negative\nsample selector. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, volume 35, pp.\n8784\u20138792, 2021. 3\n\nYoumi Ma, An Wang, and Naoaki Okazaki. Dreeam: Guiding attention with evidence for improv-\ning document-level relation extraction. In Proceedings of the 17th Conference of the European\nChapter of the Association for Computational Linguistics, pp. 1963\u20131975, 2023. 8\n\nStephen Mayhew, Snigdha Chaturvedi, Chen-Tse Tsai, and Dan Roth. Named entity recognition\nwith partially annotated training data. In Proceedings of the 23rd Conference on Computational\nNatural Language Learning (CoNLL), pp. 645\u2013655, 2019. 3\n\nFarhad Nooralahzadeh, Jan Tore L\u00f8nning, and Lilja \u00d8vrelid. Reinforcement-based denoising of\ndistantly supervised ner with partial annotation. Association for Computational Linguistics, 2019.\n2, 3\n\nMartin L Puterman. Markov decision processes. Handbooks in operations research and management\n\nscience, 2:331\u2013434, 1990. 3\n\nPengda Qin, Weiran Xu, and William Yang Wang. Robust distant supervision relation extraction via\ndeep reinforcement learning. In Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 2137\u20132147, 2018. 3, 5, 6\n\nPengjie Ren, Zhumin Chen, Christof Monz, Jun Ma, and Maarten de Rijke. Thinking globally,\nacting locally: Distantly supervised global-to-local knowledge selection for background based\nconversation. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, volume 34, pp.\n8697\u20138704, 2020. 3\n\nTal Ridnik, Emanuel Ben-Baruch, Nadav Zamir, Asaf Noy, Itamar Friedman, Matan Protter, and Lihi\nZelnik-Manor. Asymmetric loss for multi-label classi\ufb01cation. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp. 82\u201391, 2021. 8\n\n11\n\n\fUnder review as a conference paper at ICLR 2024\n\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,\nJulian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering\nthe game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016. 2,\n5\n\nGuangxin Su, Weitong Chen, and Miao Xu. Positive-unlabeled learning from imbalanced data. In\n\nIJCAI, pp. 2995\u20133001, 2021. 2, 3, 7\n\nQingyu Tan, Lu Xu, Lidong Bing, Hwee Tou Ng, and Sharifah Mahani Aljunied. Revisiting docred\n\u2013 addressing the false negative problem in relation extraction. 2022. URL https://arxiv.\norg/abs/2205.12696. 1\n\nYingjie Tian, Xiaotong Yu, and Saiji Fu. Partial label learning: Taxonomy, analysis and outlook.\n\nNeural Networks, 2023. 3\n\nHaobo Wang, Ruixuan Xiao, Yixuan Li, Lei Feng, Gang Niu, Gang Chen, and Junbo Zhao. Pico:\nContrastive label disambiguation for partial label learning. In International Conference on Learn-\ning Representations, 2021. 3\n\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\n\nlearning. Machine learning, 8:229\u2013256, 1992. 6\n\nDong-Dong Wu, Deng-Bao Wang, and Min-Ling Zhang. Revisiting consistency regularization for\ndeep partial label learning. In International Conference on Machine Learning, pp. 24212\u201324225.\nPMLR, 2022. 3\n\nNing Xu, Jiaqi Lv, and Xin Geng. Partial label learning via label enhancement. In Proceedings of\n\nthe AAAI Conference on arti\ufb01cial intelligence, volume 33, pp. 5557\u20135564, 2019. 3\n\nFan Yang, Kai Wu, Shuyi Zhang, Guannan Jiang, Yong Liu, Feng Zheng, Wei Zhang, Chengjie\nWang, and Long Zeng. Class-aware contrastive semi-supervised learning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14421\u201314430, 2022. 3\n\nYaosheng Yang, Wenliang Chen, Zhenghua Li, Zhengqiu He, and Min Zhang. Distantly super-\nvised ner with partial annotation learning and reinforcement learning. In Proceedings of the 27th\nInternational Conference on Computational Linguistics, pp. 2159\u20132169, 2018. 2\n\nYuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin Huang,\nJie Zhou, and Maosong Sun. DocRED: A large-scale document-level relation extraction dataset.\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp.\n764\u2013777, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/\nv1/P19-1074. URL https://aclanthology.org/P19-1074. 1\n\nZhixiang Yuan, Kaixin Zhang, and Tao Huang. Positive label is all you need for multi-label classi-\n\n\ufb01cation. arXiv preprint arXiv:2306.16016, 2023. 3\n\nXiangrong Zeng, Shizhu He, Kang Liu, and Jun Zhao. Large scaled relation extraction with rein-\nforcement learning. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, volume 32,\n2018. 3\n\nBangzuo Zhang and Wanli Zuo. Reliable negative extracting based on knn for learning from positive\n\nand unlabeled examples. J. Comput., 4(1):94\u2013101, 2009. 3\n\nMin-Ling Zhang and Fei Yu. Solving the partial label learning problem: An instance-based ap-\n\nproach. In IJCAI, pp. 4048\u20134054, 2015. 3\n\nXin Zhang, Rabab Abdelfattah, Yuqi Song, and Xiaofeng Wang. An effective approach for multi-\nlabel classi\ufb01cation with missing labels. In 2022 IEEE 24th Int Conf on High Performance Com-\nputing & Communications; 8th Int Conf on Data Science & Systems; 20th Int Conf on Smart\nCity; 8th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPC-\nC/DSS/SmartCity/DependSys), pp. 1713\u20131720. IEEE, 2022.\n\n12\n\n\fUnder review as a conference paper at ICLR 2024\n\nMingkai Zheng, Shan You, Lang Huang, Fei Wang, Chen Qian, and Chang Xu. Simmatch: Semi-\nsupervised learning with similarity matching. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 14471\u201314481, 2022. 3\n\nKang Zhou, Yuepei Li, and Qi Li. Distantly supervised named entity recognition via con\ufb01dence-\nbased multi-class positive and unlabeled learning. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), pp. 7198\u20137211, Dublin,\nIreland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.\n498. URL https://aclanthology.org/2022.acl-long.498.

Categories (1–5):
- Soundness: The rigor of the methods for the stated problem
- Presentation: The clarity of writing and organization
- Contribution: The paper’s novelty or added value to the domain

Additionally:
- Rating (1–10): Overall recommendation for acceptance
- Confidence (1–5): How confident the reviewer is in their assessment

Please provide:
1) Soundness
2) Presentation
3) Contribution
4) Rating
5) Confidence
6) Explanation (strengths, weaknesses)
7) Questions

############# RESPONSE: Full text zero shot (o1 pro) #############

**1) Soundness (1–5): 4**  
The paper’s methodology—proposing a reinforcement learning (RL) framework with local and global rewards for partially annotated multi-label classification—appears technically sound and well-reasoned. The authors provide experimental results on synthetic data, image classification, and document-level relation extraction, all suggesting that the method outperforms baselines. The iterative procedure and data enhancement to refine the value network seem logically coherent and well-structured. A more extensive theoretical justification (e.g., formal proofs of convergence or tighter bounds on error estimates) would further strengthen the soundness, but the current empirical evidence is convincing for most practical settings.

---

**2) Presentation (1–5): 4**  
Overall, the paper is clearly written, with a logical flow from problem motivation, method formulation, reward design, to experiments. The inclusion of relevant background, along with ablation studies and hyperparameter discussions, aids comprehensibility. A few sections (especially the comparisons with other partial-label or PU learning settings) could be more succinct and better cross-referenced, but the explanations of local/global rewards and the iterative training procedure are well-articulated.

---

**3) Contribution (1–5): 4**  
The paper addresses a relatively underexplored scenario—partially annotated multi-label classification—by combining reinforcement learning with a hybrid reward design. While many works exist on PU learning or partial-label learning, the proposed framework explicitly tackles the label-imbalance and incomplete-annotation challenges for multi-label tasks in a unified manner. The novelty lies in the design of local rewards via a learned value network, alongside a global recall-based reward to encourage exploration. This synergy is reasonably original and has potential applications across various domains.

---

**4) Rating (1–10): 8**  
I recommend acceptance. The paper’s experimental results are solid, covering multiple datasets (synthetic, image classification, document-level relation extraction). The approach demonstrates clear gains over straightforward baselines like negative-mode labeling and weighting heuristics. Although the paper could still benefit from additional theoretical depth or expanded comparisons to state-of-the-art partial-label methods, it makes a strong practical contribution.

---

**5) Confidence (1–5): 4**  
I am fairly confident in this assessment. I am familiar with multi-label classification, PU learning, partial-label approaches, and policy gradient methods. The paper’s method, experiments, and explanations appear sound and comprehensive enough for a thorough evaluation, though a deeper exploration into theoretical guarantees would require more specialized expertise.

---

**6) Explanation (strengths, weaknesses)**  
- **Strengths:**
  - A well-motivated solution to the partially annotated multi-label classification problem, addressing the frequent issue of label scarcity and imbalance.
  - The RL component with separate local and global rewards effectively balances exploitation (fitting known labels) and exploration (uncovering potential unlabeled positives).
  - Thorough experimental evaluation across three tasks, with consistent performance gains.
  - Iterative training with data enhancement is a practical way to improve the value network’s reliability over time.

- **Weaknesses:**
  - The paper could provide a clearer theoretical perspective on why local/global reward synergy converges or an upper bound on label estimation error. 
  - While the ablation studies are informative, the approach’s robustness to extreme degrees of label noise or extremely large label sets remains less explored. 
  - Some related methods in partial supervision or weak labeling are briefly mentioned but not extensively compared (e.g., more nuanced partial-label or PU learning frameworks).

---

**7) Questions**  
1. **Scalability**: How does the method scale to very large label sets (hundreds or thousands of labels)? Are there any computational bottlenecks introduced by sampling actions for the local reward or iterative value-network updates?  
2. **Error Sensitivity**: How robust is the approach when partial labels are heavily biased or mislabeled (i.e., if some annotated positives might actually be incorrect)? Does the data enhancement step risk propagating early-stage misclassifications?  
3. **Other RL Objectives**: Did the authors experiment with alternative global objectives (e.g., cost-sensitive loss or rank-based metrics) instead of recall, and if so, how did that compare in practice?  


############# PROMPT: Full text one shot (o1 pro) #############

Here is an example of how to respond:

\nTOWARD OPEN-ENDED EMBODIED TASKS SOLVING\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nEmpowering embodied agents, such as robots, with Artificial Intelligence (AI) has\nbecome increasingly important in recent years. A major challenge is task open-\nendedness. In practice, robots often need to perform tasks with novel goals that\nare multifaceted, dynamic, lack a definitive \"end-state\", and were not encountered\nduring training. To tackle this problem, this paper introduces Diffusion for Open-\nended Goals (DOG), a novel framework designed to enable embodied AI to plan\nand act flexibly and dynamically for open-ended task goals. DOG synergizes the\ngenerative prowess of diffusion models with state-of-the-art, training-free guid-\nance techniques to adaptively perform online planning and control. Our evalua-\ntions demonstrate that DOG can handle various kinds of novel task goals not seen\nduring training, in both maze navigation and robot control problems. Our work\nsheds light on enhancing embodied AI\u2019s adaptability and competency in tackling\nopen-ended goals.\n\n1\n\nINTRODUCTION\n\nFigure 1: Handling open-ended goals.\n\nTask solving for open-ended goals (Fig. 1) in embodied artificial intelligence (AI) (Jin & Zhang,\n2020) represent a cornerstone in the pursuit of creating machines that can assist humans in real-world\n(Taylor et al., 2016). Unlike traditional AI that operates in virtual realms or specific, constrained\nsettings (Silver et al., 2016), embodied AI is situated in the physical world\u2014think robots, drones,\nor self-driving cars. Here, the utility is not merely in solving a specific problem but in the system\u2019s\nability to perform a broad range of tasks, enabling everything from advanced automation to assistive\ntechnologies for the disabled, much like humans and animals do.\n\nYet, this endeavor presents a myriad of challenges. Real-world tasks with open-ended goals are\nhighly diverse and often cannot be described by a single variable or a single type of variables.\nFor example, an embodied agent tasked with \"assisting in household chores\" would require the\ncapabilities to perform various tasks, from vacuuming to cooking, while adapting to new challenges\nand human preferences over time. These goals are almost impossible to fully cover in learning. The\ninherent complexity and variability of such goals necessitate a significant advancement in decision-\nmaking capacity.\n\nTo create embodied AI that can flexibly handle open-ended goals, both the knowledge about the\nworld and skills of motor actions need to be equipped. Only until recent times, a handful of works\n(Driess et al., 2023; Dai et al., 2023) started the attempts for ambition by leveraging the real-world\nknowledge from pre-trained vision (Rombach et al., 2022) and/or language (Brown et al., 2020)\n\n1\n\nOur FrameworkOffline ExperiencewithoutGoal-ConditionHandleOpen-ended GoalsTraining-freeAchieve Desired StateDesiredStateStartMovement Control(speed)Zero-shot Transfer(to new environment)No longer passable!Language Guided\"Open thedoor wide\"\fUnder review as a conference paper at ICLR 2024\n\nfoundation models. On the other hand, Stooke et al. (2021); Bauer et al. (2023) endeavors to perform\nlarge-scale multi-task training in a game world so that the agent can quickly adapt to novel tasks.\nThese works are worthy of recognition on the path to embodied AI that can truly tackle open-ended\ntasks. Nonetheless, these studies are still trapped by the conventional goal-as-an-input paradigm\n(Fig. 2 Left), and thus the flexibility of goals is limited (e.g., if a robot is trained to go anywhere in\nthe world, after training it cannot be asked to keep away from somewhere.).\n\nIn the presence of these challenges, we propose a novel framework for solving embodied planning\nand control for open-ended goals. This work is a trial to approach the ultimate embodied AI that can\nassist people with diverse tasks such as healthcare, driving, and housework, though further endeavor\nis, of course, still largely demanded. Here, we empower embodied AI with the recent advance of\ndiffusion models (Ho et al., 2020) and training-free guidance (Yu et al., 2023b) to overcome the\nchallenges of open-ended goals. We refer to our framework as Diffusion for Open-ended Goals\n(DOG). Our contributions can be summarized as follows:\n\n1. By borrowing the concept of energy functions into the Markov decision process, we provide\na novel formulation for modeling open-ended embodied tasks. This scheme enjoys much\nhigher flexibility than traditional goal-conditioned decision-making methods.\n\n2. We propose the DOG framework to solve open-ended tasks.\n\nIn the training phase, the\nunconditional diffusion models are employed to learn world knowledge from the offline\nexperience without goal-conditioning. During the inference stage, the agent would make\nplans and act based on the world knowledge in diffusion models and the knowledge of\ngoals by performing energy minimization.\n\n3. We evaluate the proposed method in a wide range of embodied decision-making tasks in-\ncluding maze navigation, robot movement control, and robot arm manipulation. DOG is\nshown to effectively and flexibly handle diverse goals that are not involved in training.\n\n2 PRELIMINARIES\n\n2.1 MARKOV DECISION PROCESS\n\nA Markov Decision Process (MDP) (Bellman, 1957) is a mathematical framework used for modeling\ndecision-making problems. An MDP is formally defined as a tuple (S, A, P, R), where S is a space\nof states, A is a space of actions, P : S \u00d7 S \u2192 A is the state transition probability function, and R\nis the reward function. Solving an MDP involves finding a policy \u03c0 : S \u2192 A, which is a mapping\nfrom states to actions, that maximizes the expected sum of rewards over time. Our work borrows\nthe terminologies and notations from MDPs, while we consider general task goals (Sec. 3) instead\nof getting more rewards as in original MDPs.\n\n2.2 DIFFUSION MODELS AND CLASSIFIER GUIDANCE\n\nDiffusion models have emerged as a powerful framework for generative modeling in deep learning\n(Ho et al., 2020). These models iteratively refine a noisy initial input towards a data sample through\na series of reverse-diffusion steps. Diffusion models are a popular type of diffusion, whose core idea\nis to estimate the score function \u2207\u2126n log p(\u2126n), where \u2126n is the noisy data at the time step t. Given\na random noise \u2126n, diffusion models progressively predict \u2126n\u22121 from \u2126n using the estimated score\n\u221a\n\u2126n\u22121 = (1 + 1\n\u03b2n\u03f5, n \u2264 N , where \u03b2n is a coefficient and\n\u03f5 \u223c N (0, I) is Gaussian noise. During the training process, the goal is to learn a neural network\ns\u03b8(xn, t) \u2248 \u2207xn log p(xn), which will be used to replace \u2207\u2126n log p(\u2126n) during inference.\n\n2 \u03b2n)\u2126n + \u03b2n\u2207\u2126n log p(\u2126n) +\n\nA unique advantage of diffusion models is training-free conditional generation. To allow con-\nditional guidance, one should compute \u2207\u2126n log p(\u2126n|c), where c is the condition. Using the\nBayesian formula, the conditional score function can be written as two terms: \u2207\u2126n log p(\u2126n|c) =\n\u2207\u2126n log p(\u2126n) + \u2207\u2126n log p(c|\u2126n), where \u2207\u2126n log p(\u2126n) is the unconditional score obtained by\nthe pretrained diffusion model and p(c|\u2126n) \u221d exp(\u2212\u2113(xn)), where \u2113(\u00b7) is an easy-to-compute loss\nfunction. For example, in image generation, unconditional diffusion can generate natural images,\nand p(c|\u2126n) is a classifier. By adding the gradient of the classifier to the pretrained diffusion neural\nnetwork, the model can perform conditional generation based on classes.\n\n2\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 2: Difference from conventional goal-conditioned decision-making (left) and diagram of our\nframework (right).\n\n3 OPEN-ENDED TASK GOALS FOR EMBODIED AI\n\nOur work aims to solve practical tasks with embodied agents (Jin & Zhang, 2020; Gupta et al.,\n2021) such as robots. Embodied decision-making faces distinguished challenges from computer\nvision (Rombach et al., 2022) or nature language processing (Brown et al., 2020). First, embodied\ntasks are constrained by real-world physics. Generative models such as diffusion models for image\ngeneration can \u201cdraw\u201d the pixels with any color, whereas a real-world painter can only use the colors\navailable in the palette. Second, decision-making involves sequence modeling, which requires an\nagent to predict and plan actions over time. This is different from traditional machine learning tasks,\nwhere the focus is on finding a single optimal output for a given input. Oftentimes we care about\nmultiple steps rather than a single state, e.g, asking a robot to move in circles. Therefore, tackling\nopen-ended task goals in embodied AI becomes challenging. To meet the need of describing open-\nended goals, we define an open-ended goal, using the notations in MDP, in an environment1 with the\nformat of energy function of a contiguous sequence of states (suppose the task horizon is T , which\ncan be finite or infinite):\n\nTo minimize\n\ng(s1:T ) : RT \u00b7ns \u2192 R,\n\n(1)\n\nwhere state st \u2208 S and ns is its dimension (Here we consider continuous state and actions for\nembodied decision-making problems while the ideas can also apply to discrete actions.) The goal\nenergy function g is any differentiable 2 function with a real scalar output. Note that g is not fixed\nnor pre-determined before training, but can be any function when needed. To consider an intuitive\nexample, imagine that a nanny robot is first trained to understand the structure of your house, then\nyou may ask the robot to perform various tasks in the house as long as you can define a goal func-\ntion, e.g., using CLIP (Radford et al., 2021) to evaluate how much the robot\u2019s visual observation\nis consistent with the embedding of \u201cclean, tidy\u201d. This way of defining a goal is fundamentally\ndifferent from the conventional way of treating the goal as a variable (so it can be an input argument\nto the decision-making model) (Andrychowicz et al., 2017; Liu et al., 2022).\n\nOur goal definition by energy function (Eq. 1) offers several notable advantages (Fig. 2). Some\nintuitive examples are explained as follows. Suppose a robot\u2019s state at step t is represented by\nhorizontal and vertical coordinates (xt, yt). The goal can be to\n\n\u2022 partial goal: Go left (smaller x) regardless of y by letting c(x1:T , y1:T ) = (cid:80)\n\u2022 negative goal: Avoid a position (\u02c6x, \u02c6y) via c(x1:T , y1:T ) = \u2212 (cid:80)\n1:T ((xt \u2212 \u02c6x)2 + (yt \u2212 \u02c6y)2);\n\u2022 sequence goal: Moving close to a given trajectory (\u02c6x1:T , \u02c6y1:T ) with c(x1:T , y1:T ) =\n\n1:T xt;\n\n(cid:80)\n\n1:T ((xt \u2212 \u02c6xt)2 + (yt \u2212 \u02c6yt)2);\n\n\u2022 hybrid goal: The combination of several goal energy functions by summation.\n\nDue to the diverse input types, these goals cannot be directly handled by goal-as-input scheme\n(Fig. 2) without training.\n\n1We consider an environment as a world with fixed physical laws, such as the earth, and various tasks (goals)\n\ncan be defined in this environment.\n\n2In this work, we consider a differentiable goal energy function since it can already be applied to many\n\ncases. For surrogate gradient or Monte-Carlo method could be used, which remains as future work.\n\n3\n\n\fUnder review as a conference paper at ICLR 2024\n\n4 METHODS\n\n4.1 OBJECTIVES AND CHALLENGES\n\nIn this section, we define the main goals and inherent challenges of our proposed method, formulated\nfor a two-phase approach. The primary objectives of the training and testing stages are as follows:\n\n\u2022 Training Stage. The agent is designed to learn and internalize knowledge of the environ-\n\nment from offline data. The testing goal is unknown.\n\n\u2022 Testing Stage. Given a novel task, the agent must interpret of goal description g \u2208 G and\ngenerate and then execute plans in alignment with the knowledge obtained during training\nto complete the goal g.\n\nThe realization of these objectives poses significant challenges within the existing frameworks. Tra-\nditional offline RL algorithms are often black-box optimization processes, usually targeting singu-\nlar goals and exhibiting resilience to modifications for diverse objectives. Goal-conditioned algo-\nrithms can only work when the task is achieve specific states s (refer to A.3 for more discussion).\nLarge language models (Huang et al., 2023) can handle open-goal tasks but struggle to utilize of-\nfline data to learn the environment transition. In summary, a gap exists for methods that can both\nleverage offline data for environmental learning during training and can accommodate diverse goals\nduring testing.\n\n4.2 TRAINING STAGE: UNDERSTANDING WORLD DYNAMICS BY DIFFUSION MODEL\n\nFigure 3: Illustration of the proposed framework.\n\ni\n\nconsisting of Nd trajectories {\u03c4i}Nd\n\nGiven an offline dataset D = {\u03c4i}Nd\nas depicted in Fig. 3a,\nwhere each line of identical color represents a distinct trajectory, our objective is to develop a pa-\nrameterized probabilistic model, denoted as p\u03b8(\u2126|st). In this model, \u2126 = {st+i}T\ni=1 signifies the\nobservation sequence of the succeeding T steps from st, intending to emulate the data distribution\np(\u2126|st) found in the offline dataset. Possessing such a model enables the prediction of plausible\nfuture trajectories originating from st, as illustrated in Fig. 3b. The training can be finished by\nminimizing the following:\n\ni\n\n\u2113\u03b8(\u00b7) = E(s,\u2126)\u223cD||p\u03b8(\u00b7|s), \u2126||2\n\n(2)\n\n4.3 TESTING STAGE: OPEN-ENDED GOAL-CONDITIONED PLANNING\n\nMerely simulating the data distribution p(\u2126|st) is inadequate for planning with open-ended goals.\nThe generated \u2126\u2217 \u223c p\u03b8(\u2126|st) largely mimics the behavior of the dataset, making it suboptimal or\nunavailable for innovative applications since it only replicates existing patterns. For more utility,\nthe sampling process needs to be aligned with the goal condition g. However, accomplishing this\nis challenging, given that the available resources are confined to a trained model p\u03b8 and a goal-\ndescriptive function g.\n\nTo mitigate the identified limitations, we integrate innovations from conditional generative strate-\ngies, widely acknowledged in the field of computer vision, into our planning process. Specifically,\nwe utilize the classifier guided diffusion model (Dhariwal & Nichol, 2021; Yu et al., 2023b), rec-\nognized for its expertise in creating sophisticated content. This model facilitates the generation\nof optimal trajectories that are conditioned on precise goals, eliminating the need for supplemental\n\n4\n\n(a)  Trajectories in dataset(b) Unintentional plans(c) Plans under different goalsImpossiblePossibleA2A1B2B1C2Goals  A:To target stateGoals  B:Avoid stateGoals  C:Direction\fUnder review as a conference paper at ICLR 2024\n\ntraining. By doing so, it expedites the sampling process and ensures adherence to defined objectives,\nproficiently fulfilling our requirements and presenting a viable resolution to our challenges.\n\nThe training phase proceeds conventionally to train a U-Net (Ronneberger et al., 2015) to learn the\nscore function s\u03b8(\u2126n, n) \u2248 \u2207\u2126n log pt(\u2126n). During the test phase, we define the conditional score\nfunction as exp(\u2212\u03b7g(\u00b7)) and calculate the gradient\n\ngradn = \u2207\u2126n log pt(c|\u2126n) \u2248 \u2207\u2126n log exp(\u2212\u03b7g(\u21260)) = \u2212\u03b7\u2207\u2126n g(\u21260)\n\nwhere c represents the goal, \u00af\u21260 =\n1 \u2212 \u00af\u03b1n\u03f5 is a denoised version of \u2126n, g is the goal\nenergy function defined in equation 1, and the approximation follows (Chung et al., 2022). Then\nthe reversed diffusion process becomes:\n\n\u00af\u03b1n\u2126n +\n\n\u221a\n\n\u221a\n\n\u2126n\u22121 = (1 +\n\n1\n2\n\n\u03b2n)\u2126n + \u03b2ts\u03b8(\u2126n, n) + (cid:112)\u03b2n\u03f5 + gradn, n \u2264 N\n\n(3)\n\nNote that, depending on whether our goal needs to include the history, we may concatate the history\ninto the decision process to guide the generation of future plans.\n\nBy implementing this methodology, we facilitate efficient generation conforming to the goal con-\ndition g. This ensures that the generated trajectories are not only optimal but are also in alignment\nwith predetermined objectives, enhancing the model\u2019s versatility and reliability across various ap-\nplications.\n\n4.4 PLAN EXECUTING\n\nOnce the plans are generated, an actor is required to execute them. Our framework is designed\nto be flexible, allowing the incorporation of various types of actors, provided they are capable of\ndirecting the agent according to the intended state transitions. Several optional can be made for the\nplan executor which selects action at at st to achieve the middle waypoint \u02c6s \u2208 \u2126. We summarize\nthe possible implementation in Sec. F.\n\nHere, we elaborate on a supervised learning implementation, drawing ideas from Hindsight Experi-\nence Replay (HER) (Andrychowicz et al., 2017). The executor is a mapping function p\u03d5 : S \u00d7 S \u2192\nA. Training is conducted by sampling state pairs s, a, s\u2032, where s and s\u2032 are within one episode with\nan interval of t \u223c Uniform(1, ..., Ta). In this context, Ta represents the maximum tolerance of state\nshifting, allowing us to sample non-adjacent states for a multi-step target state \u02c6s from the planner,\navoiding the constraints of only sampling adjacent states. The recorded action at state s serves as\nthe predictive label. This approach enables utilization of offline data to deduce the action needed to\nreach \u02c6s from s. The training loss is expressed as:\n\n\u2113\u03d5(\u00b7) = E(s,a,\u02c6s)\u223cD \u2225p\u03d5(a|s, \u02c6s)\u22252\n\n(4)\n\nSince our executor to multiple-step actions, the generated plan can be preserved and deployed over\nmultiple steps. A summary of the algorithm is provided in Alg. 1 in Sec. E\n\n5 RESULTS\n\n5.1 EXPERIMENT SETTINGS AND IMPLEMENTATION DETAILS\n\n5.1.1\n\nIMPLEMENTATION DETAILS\n\nWe defer the model structure design and parameter setting to the Appendix. We summarize the\ndetails of the design goal function in B.2. We defer environment introduction to C.\n\n5.2 MAZE NAVIGATION IN OPEN-ENDED SCENARIOS\n\nOur methodology is adept at generating plausible plan distributions for future H steps. As depicted\nin Fig. 4a, when the agent performs rollouts without any guidance, it is observable that the endpoints\nare distributed randomly across the map. This random distribution represents all the possible future\ndistributions of the agent interacting with the environment. The source model is trained using 1\nmillion steps of offline data, with the generating policy consistent with D4RL (Fu et al., 2020).\n\n5\n\n\fUnder review as a conference paper at ICLR 2024\n\nNevertheless, the utility of this method is maximized when it is conditioned on a specific goal as\nmentioned previously. Here, we illustrate various meaningful goal types within 2D Maze envi-\nronments to showcase the extensive applicability of our methods. The original maze is devoid of\nobstacles, enabling an unobstructed view of the episode distribution. For each setting, we present\n100 episodes both from the same start point (marked as green star). The end point of the trajectories\nare marked are red point. For clear visualization, we only show 10 full trajectories, each as one line.\n\n(a) Unintentional\n\n(b) To destination state\n\n(c) Stay close\n\n(d) Move faraway\n\n(e) Direction\n\n(f) Partial:(x, y) \u2192 (1, \u00b7)\n\n(g) Partial:(x, y) \u2192 (5, \u00b7)\n\n(h) Zero-Shot transfer\n\nFigure 4: Exploring Open-Ended Goals in Maze2D Environments.\n\nGoal as State. In Fig. 4b, we exhibit our proficiency in addressing traditional goal-conditioned\nRL. The agent is prompted to navigate to a designated location, it shows that it explores varied\ntrajectories to reach the location.\n\nPartial goal. Often, our objectives align only partially with observed states, requiring more flexibil-\nity in goal setting. Consider a scenario where the goal is to stand higher, however, typically requires\nthe specification of all observation s values. In Fig.4f and Fig.4f, we instruct the agent to navigate\nto locations where the x value is either 5 or 1, without specifying the y value. This allows the\nagent to identify viable end (x, y) pairs and the corresponding paths, demonstrating our method\u2019s\nadaptability to partial goals.\n\nRelative goal. Goal can be the relationship between states. In Fig. 4c- 4d, our goal is to control\nthe moving distance, which could be calculated as the sum of the distance of consecutive points.\nCorresponding plans would be generated.\n\nNon-specific Goal. Often, our objective isn\u2019t state-specific but aims at maximizing or minimizing\nsome certain property, like speed or height. Traditional goal-conditioned RL falters in such rudi-\nmentary and prevalent cases. In Fig. 4e, we direct the agent towards a direction (right-bottom),\nresulting in episode ends congregating around the right-bottom corner, substantiating our aptitude\nin managing directional goals.\n\nZero-shot transfer to new environment. It\u2019s commonplace to deploy agents in environments dis-\ntinct from their training grounds. Humans, leveraging knowledge acquired during training, can adapt\nwhen informed of changes; contrastingly, conventional RL frameworks often falter in integrating\nnew insights into policies. Fig. 4h exemplifies our algorithm\u2019s prowess in zero-shot adaptability.\nWhen deployed in a maze with impassable grids, the agent is directed to attain the goal without\ntraversing these zones and successfully formulates plans in compliance with these stipulations.\n\nHybrid goals Our algorithm transcends mere single-goal accomplishments. By incorporating the\ngradients of multiple goals, the agent endeavors to address problems under varied conditions. Fig. 4h\n\n6\n\n\fUnder review as a conference paper at ICLR 2024\n\ndisplays outcomes where the agent concurrently avoids fire and attains the goal position. Fig. 4e\nguide the x and y position at the same time.\n\n5.3 MUJOCO PERFORMANCE BY REWARD AS METRIC\n\nDT\n\nTT MOPO MOReL MBOP Diffuser AdaptDiffuser Ours\n\nDataset\n\nEnvironment\n\nBC\n\nMed-Expert\nMed-Expert\nMed-Expert\n\nMedium\nMedium\nMedium\n\nMed-Replay\nMed-Replay\nMed-Replay\n\nHalfCheetah\nHopper\nWalker2d\n\nHalfCheetah\nHopper\nWalker2d\n\nHalfCheetah\nHopper\nWalker2d\n\nAverage\n\n55.2\n52.5\n107.5\n\n42.6\n52.9\n75.3\n\n36.6\n18.1\n26.0\n\n51.9\n\nCQL\n\n91.6\n105.4\n108.8\n\n44.0\n58.5\n72.5\n\n45.5\n95.0\n77.2\n\n77.6\n\nIQL\n\n86.7\n91.5\n109.6\n\n47.4\n66.3\n78.3\n\n44.2\n94.7\n73.9\n\n77.0\n\n86.8\n107.6\n108.1\n\n94.0\n110.0\n101.9\n\n42.6\n67.6\n74.0\n\n36.6\n82.7\n66.6\n\n74.7\n\n46.9\n61.1\n79.0\n\n41.9\n91.5\n82.6\n\n78.9\n\n63.3\n23.7\n44.6\n\n42.3\n28.0\n17.8\n\n53.1\n67.5\n39.0\n\n42.1\n\n53.3\n108.7\n95.6\n\n42.1\n95.4\n77.8\n\n40.2\n93.6\n49.8\n\n72.9\n\n105.9\n55.1\n70.2\n\n44.6\n48.8\n41.0\n\n42.3\n12.4\n9.7\n\n47.8\n\n88.9\n103.3\n106.9\n\n42.8\n74.3\n79.6\n\n37.7\n93.6\n70.6\n\n77.5\n\nAverage on Mixed Dataset\n\n49.32\n\n87.25\n\n83.43\n\n81.40\n\n87.15\n\n48.53\n\n73.53\n\n49.27\n\n83.50\n\n89.6\n111.6\n108.2\n\n44.2\n96.6\n84.4\n\n38.3\n92.2\n84.7\n\n83.40\n\n87.60\n\n98.7\n111.2\n106.3\n\n41.0\n83.8\n80.6\n\n43.9\n94.2\n85.3\n\n82.89\n\n89.67\n\nTable 1: Performance on D4RL (Fu et al., 2020) MuJoCo environment using default reward metric (nor-\nmalized average returns). This table presents the the mean of 5 rollouts reward after learning from the D4RL\ndataset. Values over 95% of the best are marked as bold of each dataset row. Baseline results are adopted from\nLiang et al. (2023).\n\nWe utilize MuJoCo tasks as a benchmark to assess the capability of our DOG when learning from\ndiverse data with different quality levels sourced from the widely recognized D4RL datasets (Fu\net al., 2020). To contextualize the efficacy of our technique, we compare it against a spectrum of\ncontemporary algorithms encompassing various data-driven strategies. Overview of these methods\nare deferred to Sec. D. Comprehensive outcomes of this comparison can be found in Table 1.\n\nNote that our scenario is more challenging compared to the offline RL baselines as we lack knowl-\nedge of the goal during the training phase. Our primary objective is to increase speed guide\ngrad = \u2207\u2126speed(\u2126), aiming to generate a high-speed plan.\n\nThe findings are documented in Tab.1. In single dataset settings, our model demonstrates compet-\nitiveness with other offline RL algorithms. Specifically, on mixed datasets with expert data, our\nmodel consistently outperforms both Diffuser and AdaptDiffuser.\n\nOur model excels particularly in mixed dataset scenarios. This substantiates our ability to sample\nfrom the distribution aligned with the goal while mitigating the impact of inferior samples. BC\ncan be construed as the standard distribution, allowing us to surpass the original behavior. An\nenhancement in performance is observed when using Med-Replay in lieu of Medium, illustrating our\nproficiency in assimilating the high-value segments of diverse data without significant disturbance\nfrom the lower-value segments. This holds considerable significance for offline RL, especially in\nscenarios where acquiring high-quality expert data poses challenges.\n\n5.4 ROBOTIC MOVEMENT CONTROL\n\nFig. 5 demonstrates that controlling robotic deployment involves calibrating properties like speed\nand height to comply with environmental constraints. Conventional RL techniques are typically\nlimited to training with a single goal, and goal-conditioned RL primarily reaches specific states\nwithout extending control over the process. Here, we emphasize the aptitude of our method in\naltering the action sequence during the evaluation phase.\n\nTo illustrate, we deploy Hopper-v2, with the outcomes displayed in Fig. 5c - 5b, revealing various\nadaptations in speed and height upon the implementation of respective guidance. These modifica-\ntions combine the accuracy of manual tuning with the merits of autonomous learning from demon-\nstrations, all while staying within logical distribution ranges. There is no obligation to predefine\nthe speed value, empowering the agent to make autonomous recalibrations to align with the target\ndirection.\n\n7\n\n\fUnder review as a conference paper at ICLR 2024\n\n(a) Visualization of Hopper robot rollout trajectories guided by different goals using DOG\n\n(b) Speed\n\n(c) Height\n\n(d) Reward\n\nFigure 5: Guidance of Robotic Movement. (a) For the Hopper-v4 environment, we present guided\nrollouts depicted in 128 frames, displayed in intervals of 8 within a single figure. All four originate\nfrom the same state s0, yet exhibit varied behavior under goal guidance. (b, c, d) The distribution of\nSpeed, Height and original task reward under goal guidance.\n\n5.5 ROBOT ARM CONTROL\n\nWe present result of interacting with the object with robot hand in this section. The goal of the\noriginal environment is to open a door with hand, which including sub steps: hold the handle and\nopen as show in Fig. 6a. We use the \u201ccloned\u201d part of the D4RL dataset to train the planner model.\n\n(a) Environment Illustration\n\n(b) Text to goal function\n\n(c) Close door\n\n(d) Open door\n\n(e) Different angles\n\nFigure 6: Demonstrations on D4RL Adroit Door-v1 environments.\n\n5.5.1 GOAL OF INTERACTING WITH OBJECT\n\nIn Fig. 6c-6e, we exhibit some example results of generating varied plans by assigning goals to\ndifferent end states of the door. Notably, while the primary intention of this dataset is to accumulate\ndata related to opening the door, it proves versatile enough to allow the formulation of plans that\ninclude keeping the door closed, opening it, and opening it to a specified angle.\n\n8\n\nSlowerFasterLowerHigherNo GoalSlowerFasterTypes of Goal0.00.20.40.60.8Normalized SpeedNo GoalLowerHigherTypes of Goal0.00.20.40.60.8Normalized HeightSlowerFasterLowerHigherTypes of Goal0.000.250.500.751.00Normalized RewardInitHoldOpen### Objective: Create function g, ...### Information Provided:Env: ...Observation: ...Goal: ...### Python Code Template ...#### Examples... (In-Context Learning) ...Language based promptPrompt: \u201cGrasp the handle\u201dPrompt: \u201cOpen the door as wide as possible\u201dPrompt: \u201cKeep the door closed\u201d\fUnder review as a conference paper at ICLR 2024\n\n5.5.2 FORMULATING GOAL FUNCTIONS WITH LLMS\n\nManually creating goal functions can be deemed a meticulous and laborious task. Here, we try to\nuse a large language model (LLM) to translate abstract concepts into tangible goal functions. The\nobservation space of the Android-Door-v1 environment harbors 39 distinct dimensions of informa-\ntion including angle, position, and speed, serving as an optimal candidate for testing the formulation\nof goal functions.\n\nWe employ GPT-4 32K (OpenAI, 2022) by feeding it prompts to generate requisite functions. The\ninput prompt comprises our objective and comprehensive environmental data including each ob-\nservation dimension\u2019s description and significance, coupled with several input-output pairs for in-\ncontext learning\u2014a standard practice in prompt engineering (Min et al., 2021; 2022). Detailed input\ninstances and dialogue history are available in Sec.G in the Appendix. Three prompts are input as\ndepicted in Fig.6b. The outcomes confirm that the contemporary LLMs adeptly interpret our re-\nquirements and transfigure them into goal functions within our framework. And we only need to\nquery once for one task.\n\n6 RELATED WORK: DIFFUSION MODELS FOR DECISION MAKING\n\nWith the popularity of diffusion models in many generation tasks recently, there emerges a few at-\ntempts to leverage the power of diffusion models for decision-making problems. Wang et al. (2023);\nPearce et al. (2023) used diffusion models to approximate more sophisticated policy distribution\nfrom offline RL datasets, achieving promising performance on GCRL tasks. By contrast, we use\ndiffusion models to generate state trajectories. Janner et al. (2022) and Liang et al. (2023) proposed\nDiffuser and AdaptDiffuser, from which we have been inspired. While they focused on conventional\noffline RL/GCRL scheme, we extend to open-ended goal space by leveraging latest training-free\nguidance techniques (Yu et al., 2023b). Another key difference is that they used the diffusion model\nto generate both states and actions, thus need to struggle with the consistency between them (Liang\net al., 2023). Besides, Ajay et al. (2022) and Dai et al. (2023) used diffusion models to generate\nstate trajectories and use inverse kinematics to compute the action, similar to ours. However, they\nrely on classifier-free diffusion guidance (Ho & Salimans, 2021), meaning that the goal space need\nto be involved in training, thus it can not handle open-ended goals in evaluation.\n\n7 CONCLUSIONS AND DISCUSSIONS\n\nIn this work, we proposed the scheme of open-ended goals for embodied AI and Diffusion for\nOpen-ended Goals (DOG), a framework for open-ended task planning of embodied AI. Trained\nwith general offline/demonstration data, DOG is featured with its capacity to handle novel goals that\nare not involved in training, thanks to the recent development of diffusion models and training-free\nguidance. While recent AI has been competitive with humans on natural language (Bubeck et al.,\n2023) and vision (Kirillov et al., 2023) tasks, currently embodied AI is far behind humans on e.g.,\ncooking, room-cleaning and autonomous driving. Our work introduces a novel way of solving open-\nended embodied tasks, which may shed light on cognitive neuroscientific research on understanding\nhuman intelligence (Taylor et al., 2016).\n\nOur framework also comes with limitations. First, an energy function describing the goal (1) needs\nto be given. While we used human-defined function for the goals in this work, it could also be a\nneural network like a classifier (Dhariwal & Nichol, 2021). Future work should consider removing\nthe method\u2019s dependence on humans. Another limitation is that DOG may fail to achieve a goal that\ncannot be accomplished by composing the existing state transitions in the training dataset. We argue\nthat this is also highly challenging for a human, e.g., a normal person cannot make a helicopter, since\nan ability demands an accurate world model and extremely high computation power (Deisenroth &\nRasmussen, 2011). Generalizability to a wider range of goals requires higher diversity in the offline\ndataset used for training. Nonetheless, we have demonstrated various training-free controllability\nsuch as speed and height regularization of moving robots using the D4RL dataset (Fu et al., 2020)\nwhich was not created for our purpose. Last but not least, future work should compromise multi-\nmodalities including vision, audition, olfaction, tactile sensation, and text.\n\n9\n\n\fUnder review as a conference paper at ICLR 2024\n\nREFERENCES\n\nAnurag Ajay, Yilun Du, Abhi Gupta, Joshua B Tenenbaum, Tommi S Jaakkola, and Pulkit Agrawal.\nIs conditional generative modeling all you need for decision making? In The Eleventh Interna-\ntional Conference on Learning Representations, 2022.\n\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob\nMcGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience re-\nplay. In Advances in Neural Information Processing Systems, volume 30, 2017.\n\nArthur Argenson and Gabriel Dulac-Arnold. Model-based offline planning.\n\narXiv preprint\n\narXiv:2008.05556, 2020.\n\nJakob Bauer, Kate Baumli, Satinder Baveja, Feryal Behbahani, Avishkar Bhoopchand, Nathalie\nBradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, et al. Human-timescale adap-\ntation in an open-ended task space. In International Conference on Machine Learning, 2023.\n\nRichard Bellman. A Markovian decision process. Journal of Mathematics and Mechanics, pp.\n\n679\u2013684, 1957.\n\nAnthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho,\nJulian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, et al. Do as i can, not as i say: Grounding\nlanguage in robotic affordances. In Conference on Robot Learning, pp. 287\u2013318. PMLR, 2023.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. In Advances in Neural Information Processing Systems, 2020.\n\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka-\nmar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general\nintelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\n\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel,\nAravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence\nmodeling. In Advances in Neural Information Processing Systems, volume 34, 2021.\n\nHyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Diffusion\nposterior sampling for general noisy inverse problems. arXiv preprint arXiv:2209.14687, 2022.\n\nYilun Dai, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans,\nand Pieter Abbeel. Learning universal policies via text-guided video generation. arXiv preprint\narXiv:2302.00111, 2023.\n\nMarc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy\nsearch. In Proceedings of the International Conference on Machine Learning, pp. 465\u2013472, 2011.\n\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances\n\nin neural information processing systems, 34:8780\u20138794, 2021.\n\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multi-\nmodal language model. arXiv preprint arXiv:2303.03378, 2023.\n\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\nagents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35:\n18343\u201318362, 2022.\n\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep\n\ndata-driven reinforcement learning, 2020.\n\nAgrim Gupta, Silvio Savarese, Surya Ganguli, and Li Fei-Fei. Embodied intelligence via learning\n\nand evolution. Nature communications, 12(1):5721, 2021.\n\n10\n\n\fUnder review as a conference paper at ICLR 2024\n\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning\nbehaviors by latent imagination. In International Conference on Learning Representations, 2019.\n\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on\n\nDeep Generative Models and Downstream Applications, 2021.\n\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\n\nneural information processing systems, 33:6840\u20136851, 2020.\n\nTaylor Howell, Nimrod Gileadi, Saran Tunyasuvunakool, Kevin Zakka, Tom Erez, and Yuval\narXiv preprint\n\nPredictive sampling: Real-time behaviour synthesis with mujoco.\n\nTassa.\narXiv:2212.00541, 2022.\n\nWenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer:\nComposable 3d value maps for robotic manipulation with language models. arXiv preprint\narXiv:2307.05973, 2023.\n\nMichael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence\nmodeling problem. In Advances in Neural Information Processing Systems, volume 34, 2021.\n\nMichael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for\nflexible behavior synthesis. In International Conference on Machine Learning, pp. 9902\u20139915.\nPMLR, 2022.\n\nDongdong Jin and Li Zhang. Embodied intelligence weaves a better future. Nature Machine Intel-\n\nligence, 2(11):663\u2013664, 2020.\n\nRahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-\nbased offline reinforcement learning. Advances in neural information processing systems, 33:\n21810\u201321823, 2020.\n\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv\npreprint arXiv:2304.02643, 2023.\n\nIlya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-\n\nlearning. arXiv preprint arXiv:2110.06169, 2021.\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-\nlutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097\u20131105,\n2012.\n\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline\nreinforcement learning. Advances in Neural Information Processing Systems, 33:1179\u20131191,\n2020.\n\nSergey Levine and Vladlen Koltun. Guided policy search. In International conference on machine\n\nlearning, pp. 1\u20139. PMLR, 2013.\n\nZhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni, Masayoshi Tomizuka, and Ping Luo. Adaptdiffuser:\nDiffusion models as adaptive self-evolving planners. arXiv preprint arXiv:2302.01877, 2023.\n\nMinghuan Liu, Menghui Zhu, and Weinan Zhang. Goal-conditioned reinforcement learning: Prob-\n\nlems and solutions. arXiv preprint arXiv:2201.08299, 2022.\n\nCorey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and\nPierre Sermanet. Learning latent plans from play. In Conference on robot learning, pp. 1113\u2013\n1132. PMLR, 2020.\n\nYecheng Jason Ma, Jason Yan, Dinesh Jayaraman, and Osbert Bastani. How far i\u2019ll go: Offline goal-\nconditioned reinforcement learning via f -advantage regression. arXiv preprint arXiv:2206.03023,\n2022.\n\n11\n\n\fUnder review as a conference paper at ICLR 2024\n\nRussell Mendonca, Oleh Rybkin, Kostas Daniilidis, Danijar Hafner, and Deepak Pathak. Discover-\ning and achieving goals via world models. Advances in Neural Information Processing Systems,\n34:24379\u201324391, 2021.\n\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn\n\nin context. arXiv preprint arXiv:2110.15943, 2021.\n\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke\nZettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv\npreprint arXiv:2202.12837, 2022.\n\nMasashi Okada, Norio Kosaka, and Tadahiro Taniguchi. Planet of the Bayesians: Reconsidering\nIn International\n\nand improving deep planning network by incorporating Bayesian inference.\nConference on Intelligent Robots and Systems, 2020.\n\nOpenAI. Introducing chatgpt, 2022. URL https://openai.com/blog/chatgpt/.\n\nPaavo Parmas, Carl Edward Rasmussen, Jan Peters, and Kenji Doya. PIPPS: Flexible model-based\npolicy search robust to the curse of chaos. In International Conference on Machine Learning, pp.\n4065\u20134074. PMLR, 2018.\n\nTim Pearce, Tabish Rashid, Anssi Kanervisto, Dave Bignell, Mingfei Sun, Raluca Georgescu, Ser-\ngio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, et al. Imitating human\nbehaviour with diffusion models. arXiv preprint arXiv:2301.10677, 2023.\n\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748\u20138763. PMLR, 2021.\n\nAravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel\nTodorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement\nlearning and demonstrations. arXiv preprint arXiv:1709.10087, 2017.\n\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, pp. 10684\u201310695, 2022.\n\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomed-\nIn Medical Image Computing and Computer-Assisted Intervention\u2013\nical image segmentation.\nMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceed-\nings, Part III 18, pp. 234\u2013241. Springer, 2015.\n\nErick Rosete-Beas, Oier Mees, Gabriel Kalweit, Joschka Boedecker, and Wolfram Burgard. Latent\nIn Conference on Robot Learning, pp.\n\nplans for task-agnostic offline reinforcement learning.\n1838\u20131849. PMLR, 2023.\n\nKepa Ruiz-Mirazo, Juli Peret\u00f3, and Alvaro Moreno. A universal definition of life: autonomy and\n\nopen-ended evolution. Origins of Life and Evolution of the Biosphere, 34:323\u2013346, 2004.\n\nArchit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware\nunsupervised discovery of skills. In International Conference on Learning Representations, 2020.\n\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,\nJulian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering\nthe game of Go with deep neural networks and tree search. Nature, 529(7587):484, 2016.\n\nAdam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja\nTrebacz, Max Jaderberg, Michael Mathieu, et al. Open-ended learning leads to generally capable\nagents. arXiv preprint arXiv:2107.12808, 2021.\n\nTim Taylor, Mark Bedau, Alastair Channon, David Ackley, Wolfgang Banzhaf, Guillaume Beslon,\nEmily Dolson, Tom Froese, Simon Hickinbotham, Takashi Ikegami, et al. Open-ended evolution:\nPerspectives from the oee workshop in york. Artificial life, 22(3):408\u2013423, 2016.\n\n12\n\n\fUnder review as a conference paper at ICLR 2024\n\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\nIn 2012 IEEE/RSJ international conference on intelligent robots and systems, pp. 5026\u20135033.\nIEEE, 2012.\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nIn Advances in Neural Infor-\n\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need.\nmation Processing Systems, pp. 5998\u20136008, 2017.\n\nHsiang-Chun Wang, Shang-Fu Chen, and Shao-Hua Sun. Diffusion model-augmented behavioral\n\ncloning. arXiv preprint arXiv:2302.13335, 2023.\n\nSherry Yang, Dale Schuurmans, Pieter Abbeel, and Ofir Nachum. Chain of thought imitation with\nIn Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho\n\nprocedure cloning.\n(eds.), Advances in Neural Information Processing Systems, 2022.\n\nJiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and Jian Zhang. Freedom: Training-free\n\nenergy-guided conditional diffusion model. arXiv preprint arXiv:2303.09833, 2023a.\n\nJiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and Jian Zhang. Freedom: Training-free\n\nenergy-guided conditional diffusion model. arXiv preprint arXiv:2303.09833, 2023b.\n\nTianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn,\nand Tengyu Ma. Mopo: Model-based offline policy optimization. Advances in Neural Information\nProcessing Systems, 33:14129\u201314142, 2020.\n\nWenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Are-\nnas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, et al. Language to\nrewards for robotic skill synthesis. arXiv preprint arXiv:2306.08647, 2023c.\n\n13\n\n\fUnder review as a conference paper at ICLR 2024\n\nA MORE RELATED WORKS\n\nA.1 TASK OPEN-ENDEDNESS IN EMBODIED AI\n\nThe concept of task open-endedness (Ruiz-Mirazo et al., 2004; Taylor et al., 2016) (while the exact\ndefinition remains debatable) has long been recognized in biological learning and evolution. How-\never, in machine learning and deep learning, models and algorithms until 2010\u2019s could only deal with\nsingle-objective tasks with real-world level difficulty (Krizhevsky et al., 2012; Silver et al., 2016).\nWith the blossom with deep learning AI in recent few years, arguably represented by GPT-3 (Brown\net al., 2020), AI researchers started to investigate the way to solve open-ended tasks by embodied\nAI in more realistic environments (Stooke et al., 2021; Bauer et al., 2023). One way is to exploit the\nplanning capacity of large language models (LLMs) to decompose a task a into a sequence of primi-\ntive tasks described by nature language (e.g., \u201cgrab \u00b7 \u00b7 \u00b7 \u201d, \u201cmove to \u00b7 \u00b7 \u00b7 \u201d) (Brohan et al., 2023; Driess\net al., 2023). However, the primitive tasks need to be pre-defined so that the robot can execute, thus\nlimits its versatility. Moreover, nature languages may not be suitable for describing meticulous ac-\ntion movements such as shadow play using fingers. Another idea from Yu et al. (2023c) is to leverage\nthe coding ability of LLMs to translate task instructions into reward functions, which can be used to\ncompute low-level actions via model predictive control (Howell et al., 2022). However, this requires\nthe LLM to know the specific robot used and it may struggle with high-dimensional action space.\nBesides relying on pre-trained LLMs, training large transformer (Vaswani et al., 2017) models from\nscratch for embodied decision-making on multi-tasks (or meta-learning) is also under investigation\nStooke et al. (2021); Fan et al. (2022); Bauer et al. (2023). They differ from our method in terms\nof that their agents are heavily trained with a variety of human-defined goals (thus can generalize\nto novel goals) while our agents are trained without a goal. Finally, utilizing the power of diffusion\nmodels (Rombach et al., 2022; Dai et al., 2023) is another direction of exploration, which we defer\nto Sec. 6 for detailed discussion.\n\nA.2 MODEL-BASED PLANNING\n\nModel-based planning methods searches (optimizes) a sequence of of state transitions that lead to\ndesired outcome (e.g., more rewards) using a world model, typically by backpropagation (Deisenroth\n& Rasmussen, 2011; Parmas et al., 2018) or amortized inference (Hafner et al., 2019; Okada et al.,\n2020). The diffusion model in our framework serves for the same purpose, while employing totally\ndistinct methodology. Model-based planning usually leverages state space models such as RNNs\n(Hafner et al., 2019) to model the state transition function in MDP explicitly, whereas our framework\nskips learning the exact state transition function, but straightforwardly generates state trajectories\n(with variant intervals) by using diffusion models and training-free guidance (Yu et al., 2023b). This\nis similar to people\u2019s planning in often cases, where one can imagine a trajectory of a basketball\nwithout an exact state-transition models or knowing Newton\u2019s law.\n\nA.3 GOAL-CONDITIONED DECISION-MAKING\n\nA bunch of studies have worked on reinforcement learning (RL) and imitation learning (IL) in pres-\nence of a goal (Liu et al., 2022), known as goal-conditioned RL/IL (we refer to both as GCRL for\nsimplicity). Studies of GCRL typically include controlling robots or mechanical arms to manipulate\nobjects (Andrychowicz et al., 2017; Mendonca et al., 2021), going to a target position in a specific\nenvironment (Sharma et al., 2020; Yang et al., 2022). These work usually model the goal by a de-\nsired state to reach. Guided policy search (Levine & Koltun, 2013) and latent plans from play (Lynch\net al., 2020; Rosete-Beas et al., 2023) share some high-level idea with ours, that is constraining the\ngoal-directed policy close to that learned from offline experience so as to reduce search space. How-\never, the key difference between GCRL and our work is that our training process does not involve\na goal while can open-ended goals can be handled in evaluation, whereas GCRL pre-defines a goal\nspace on which both training and evaluation are based, and treats the goal as input to the policy\nnetwork. In short, unlike our framework, GCRL cannot handle goals undefined in training (e.g.,\nkeeping away from a state).\n\n14\n\n\fUnder review as a conference paper at ICLR 2024\n\nB IMPLEMENTATION DETAILS\n\nB.1 MODEL STRUCTURE\n\nWe follow Liang et al. (2023) and Janner et al. (2022) to use a a temporal U-Net (Ronneberger\net al., 2015) with 6 repeated residual blocks is employed to model the noise of the diffusion process.\nTimestep embeddings are generated by a single fully-connected layer and added to the activation\noutput after the first temporal convolution of each block.\n\nB.2 GOAL FUNCTION DESIGN SUMMARY\n\nIn this section, we provide a consolidated overview of the various goal functions implemented within\nour experiments, organized and summarized for coherent assimilation. The meticulous design of\ngoal functions plays a pivotal role in shaping the trajectory of our experiments, facilitating nuanced\ninteractions between the agent and its environment.\nIt is essential to comprehend the multiplic-\nity and specificity of these goal functions to appreciate their impact on the respective experimental\noutcomes. We aim to elucidate the intricate relations between observations, methods, and the cor-\nresponding goal function designs, aiding in a deeper understanding of the experimental setup and\nresults.\n\nTo offer a structured perspective, we have tabulated all the goal functions, aligning them with their\ncorresponding observations and methods, in Tab. 2. This table serves as a succinct reference, aid-\ning readers in correlating the diversity of goal functions with the experimental setups and drawing\ninsights from their interrelations.\n\nC ENVIRONMENT INFORMATION\n\nMaze2D Envrionment (Fu et al., 2020) serves as a navigation task platform where a 2D agent\nnavigates from a randomly chosen start location to a predetermined goal location. Here, the agent is\nawarded a reward of 1 upon reaching the goal, with no intermediate rewards provided, necessitating\nefficient trajectory planning to secure optimal rewards.\n\nThis environment is structured to test the proficiency of offline RL algorithms in leveraging pre-\nviously accumulated sub-trajectories to discern the optimal path to the designated goal. It offers\nthree distinct maze layouts named \"umaze,\" \"medium,\" and \"large.\" For the dataset we used in this\npaper, we build a maze with both height and width as 5 with no walls. Then generate 1 million steps\nfollowint the genration policy provided by Fu et al. (2020).\n\nMuJoCo Environment developed by Todorov et al. (2012), is a physics engine esteemed for its\nIt\u2019s primarily used for developing\nability to simulate intricate mechanical systems in real-time.\nand testing algorithms in robotic and biomechanical contexts. Within MuJoCo, various tasks like\nHopper, HalfCheetah, and Walker2d are available, each providing a unique set of challenges and\nlearning opportunities for reinforcement learning models.\n\nEach task within MuJoCo is associated with four different datasets: \u201cmedium,\u201d \u201crandom,\u201d \u201cmedium-\nreplay,\u201d and \u201cmedium-expert\u201d by D4RL dataset. The \"medium\" dataset is formulated by training a\npolicy using a specific algorithm and accumulating 1M samples. The \u201crandom\u201d dataset is derived\nfrom a policy initialized randomly.\n\nThe \u201cmedium-replay\u201d dataset comprises all samples recorded during training until the policy attains\na predefined performance level. Additionally, there exists a \u201cmedium-expert\u201d dataset, representing a\ncollection of more advanced, intricate scenarios and data points. These datasets are crucial for eval-\nuating the adaptability, efficacy, and robustness of reinforcement learning algorithms under varying\nconditions and degrees of complexity.\n\nAdroit-Door Environment (Rajeswaran et al., 2017) is designed as a sophisticated platform for\ninteraction and manipulation tasks involving door objects. It presents an intricate set of challenges\nfocused on analyzing the agent\u2019s capacity to interact with and manipulate objects effectively within\ndefined parameters.\n\n15\n\n\fUnder review as a conference paper at ICLR 2024\n\nObservations\n\nInclude observations\n{x, y, vx, vy}.\n\nInclude observations\n{x, y, vx, vy}. (x, y) is the\ncurrent position of the agent\nand (vx, vy) is the velocity.\n\nGoal\n\nGoal Function Design\n\nGoal as State\n\n(x \u2212 \u02c6x)2 + (y \u2212 \u02c6y)2\n\nPartial goal\n\n(x \u2212 \u02c6x)2\n\nDistance - Go faraway\n\nDistance - Stay close\n\nNon-specific Goal\n\nZero-shot transfer to new\nenvironment\n\nHybrid goals\n\n(cid:113)(cid:80)N \u22121\n\u2212\n(cid:113)(cid:80)N \u22121\n\ni=1 (xi+1 \u2212 xi)2\n\ni=1 (xi+1 \u2212 xi)2\n\nx\n\n\u2212|x \u2212 x0| \u00b7 I(|x \u2212 x0| < \u03c3),\nwhere \u03c3 = 1 is the radius of\npoints affected\n\n(cid:80)Ng\n\ni \u03b7i \u00b7 gi(\u00b7) where g1:Ng is\nthe series of goal functions\nand \u03b7i is the weight for each\nfunction.\n\n\u2212vx (we consider one\ndirection)\n\nvx\n\n\u2212xh\n\nxh\n\nr\n\no\n\n\u2212o\n\nHopper HalfCheetah have\ndifferent observations, while\nthey both have dimension for\nthe x-velocity and z-position\nof the robot tip. We denote\nthe velocity as vx and\nz-position as xh.\n\nFaster\n\nSlower\n\nHigher\n\nLower\n\nInclude observations r as the\nangle of the hinge and a flag o\nindicate whether the door is\nopen, open as 1.\n\nOpen the door wide\n\nClose the door\n\nOpen the door\n\nTable 2: Summary of Methods and Goal Function Design\n\nThis environment incorporates a complex observation space consisting of 39 dimensions, encom-\npassing various aspects such as angle, position, and speed. This multi-dimensional space is con-\nducive to examining the interaction dynamics between the agent and the door, providing insights\ninto the efficiency and adaptability of the applied algorithms.\n\nThe environment aims to simulate real-world interactions with objects, making it a suitable platform\nto test reinforcement learning algorithms\u2019 efficacy in performing tasks with high precision and re-\nliability. The tasks designed in this environment range from simple interactions, like opening and\nclosing doors, to more complex ones requiring a fine-tuned understanding of the object\u2019s state and\nthe surrounding environment.\n\nD MUJOCO BASELINES\n\nThe Mujoco baselines include model-free RL algorithms like CQL Kumar et al. (2020) and IQL\nKostrikov et al. (2021), return-modulated techniques such as the Decision Transformer (DT) Chen\net al. (2021), and model-based RL algorithms like the Trajectory Transformer (TT) Janner et al.\n(2021), MOPO Yu et al. (2020), MOReL Kidambi et al. (2020), and MBOP Argenson & Dulac-\nArnold (2020).\n\n16\n\n\fUnder review as a conference paper at ICLR 2024\n\nE ALGORITHM SUMMARY\n\nWe summarize the algorithm in Alg. 1.\n\nAlgorithm 1 DOG\n\nTraining Stage\nInput: Offline data D = {\u03c4i}Nd\nOutput: Planner p\u03b8 and Executor p\u03d5\nLearn unconditional Planner p\u03b8(\u2126 | st) (Eq. 2)\nLearn Executor p\u03d5(at | st, s\u2032) (Eq. 4) stage\n\n.\n\ni\n\nDeployment Stage\nInput: Goal description g \u2208 G.\nfor each step t do\n\n(Optional) Accept new goal g\u2032 \u2208 G, set g = g\u2032.\nif need to update plan then\n\nCalculate plan \u2126\u2217 with Eq. 3. Record plan step tp = t.\n\nend if\nDecide next waypoint as s\u2032 = \u2126\u2217[t \u2212 tp].\nSelect and execute action a with executor p\u03d5(at | st, s\u2032).\n\nend for\n\nF LIST OF POSSIBLE EXECUTOR IMPLEMENTATIONS\n\n1. Hindsight Experience Replay (HER)-based (Andrychowicz et al., 2017). The actor\nfunction, denoted by , takes as input the current state st and an expected future state s\u2032,\nproducing an action at as output. Training is finished by sampling state pair {s, s\u2032} within\none episode with interval of t \u223c Uniform(1, ..., Ta), where Ta is the max tolerance of state\nshifting. The recorded action of state s would be used as predicting label.\n\n2. Goal-conditioned controller Goal-conditioned RL study how to reach custom s \u2208 S,\nwhich is a great option for our underline controller since what we need is to achieve a\nwaypoint \u02c6s from current state st. Specifically, we adopt the Ma et al. (2022) as the underline\ncontroller for its great performance and easy implementation.\n\n3. Model Predictive Control (MPC) (Yu et al., 2023a) We can learn an environmental model\nE : S \u00d7 A \u2192 S so that next state sk+1 can be estimated by E(sk, ak). Then, we find the\noptimal at as arg mina\u2206t,t:t+\u2206t\u22121 \u2113(st+\u2206t, \u02c6s).\n\nG GPT PROMPT DEMONSTRATIONS\n\n17\n\n\fUnder review as a conference paper at ICLR 2024\n\nLLM Prompt - Prefix\n\n### Objective:\nCreate a Python function, \u2018g\u2018, that returns the loss value, \u2018res\u2018,\n\nfor a given input \u2018ep\u2018. The function \u2018g\u2018 should be\ndifferentiable and minimizeable via gradient descent. This\nfunction is intended to construct a sequence \\( \\tau = \\{s_1,\n..., s_T\\} \\) such that \\( \\tau = \\arg\\min_\\tau g(\\tau) \\) based\non specified language-based requirements described in the \u2019Goal\n\n\u2019.\n\n### Information Provided:\n- **Env: ** A text description of the environment\n- **Observation (\u2018s\u2018):** Provides the shape and the meaning of each\n\nposition in \u2018ep\u2018.\n\n- **Goal:** A text description outlining the objective for which \u2018g\n\n\u2018 needs to be developed.\n\n### Python Code Template:\ndef goal(ep):\n\n# code start\n\nres = ...\n\n# code end\n\nreturn res\n\n#### Example 1\nUSER INPUT:\nEnv: Move a ball in 2d World\nObservation: 4 dim\n\n0: position x of the ball\n1: position y\n2: velocity x\n3: velocity y\n\nGoal: \"Move the ball to (1, 3)\"\nYOUR REPLAY:\nres = ((ep[-1,:2] - torch.tensor([1,3])) ** 2).sum()\n\n#### Example 2\nUSER INPUT:\nEnv: Move a ball in a room\nObservation: 4 dim\n\n0: position x\n1: position y\n2: velocity x\n3: velocity y\n\nGoal: \"Find the shortest path\"\nYOUR REPLAY:\nres = ((ep[1:]-ep[:-1])[:2]**2).sum(dim=-1).sqrt().sum()\n\n#### Example 3\nUSER INPUT:\nEnv: Driving a car in a 2D world\nObservation: 2 dim\n\n0: velocity x of the car\n1: velocity y\n\nGoal: \"Move faster\"\nYOUR REPLAY:\nres = ep[:,0].mean()\n\nNow, Let Us Begin, my input:\n\n18\n\n\fUnder review as a conference paper at ICLR 2024\n\nLLM Prompt - Example 1 (Open the door as wide as possible)\n\nEnv:\n\nA 28 degree of freedom system which consists of a 24\n\ndegrees of freedom ShadowHand and a 4 degree of freedom\narm. The latch has significant dry friction and a bias\ntorque that forces the door to stay closed.\n\nObservation:\n\n0\n1\n2\n3\n4\n5\n\n6\n\n7\n8\n9\n\n10\n\n11\n\n12\n\n...\n32\n33\n34\n35\n\n36\n\n37\n\n38\n\nAngular position of the vertical arm joint\nAngular position of the horizontal arm joint\nRoll angular value of the arm\nAngular position of the horizontal wrist joint\nAngular position of the vertical wrist joint\nHorizontal angular position of the MCP joint of the\n\nforefinger\n\nVertical angular position of the MCP joint of the\n\nforefinge\n\nAngular position of the PIP joint of the forefinger\nAngular position of the DIP joint of the forefinger\nHorizontal angular position of the MCP joint of the\n\nmiddle finger\n\nVertical angular position of the MCP joint of the\n\nmiddle finger\n\nAngular position of the PIP joint of the middle\n\nfinger\n\nAngular position of the DIP joint of the middle\n\nfinger\n\nx position of the handle of the door\ny position of the handle of the door\nz position of the handle of the door\nx positional difference from the palm of the hand\n\nto the door handle\n\ny positional difference from the palm of the hand\n\nto the door handle\n\nz positional difference from the palm of the hand\n\nto the door handle\n\n1 if the door is open, otherwise -1\n\nGoal:\n\nOpen the door as wide as possible\n\nLLM Prompt - Example 2 (Keep the door closed)\n\nEnv:\n\nA 28 degree of freedom system which consists of a 24\n\ndegrees of freedom ShadowHand and a 4 degree of freedom\narm. The latch has significant dry friction and a bias\ntorque that forces the door to stay closed.\n\nObservation:\n\n0\n1\n2\n3\n...\n38\n\nAngular position of the vertical arm joint\nAngular position of the horizontal arm joint\nRoll angular value of the arm\nAngular position of the horizontal wrist joint\n\n1 if the door is open, otherwise -1\n\nGoal:\n\nKeep the door closed\n\n19\n\n\fUnder review as a conference paper at ICLR 2024\n\nLLM Prompt - Example 3 (Grasp the handle)\n\nEnv:\n\nA 28 degree of freedom system which consists of a 24\n\ndegrees of freedom ShadowHand and a 4 degree of freedom\narm. The latch has significant dry friction and a bias\ntorque that forces the door to stay closed.\n\nObservation:\n\n0\n1\n2\n3\n...\n38\n\nAngular position of the vertical arm joint\nAngular position of the horizontal arm joint\nRoll angular value of the arm\nAngular position of the horizontal wrist joint\n\n1 if the door is open, otherwise -1\n\nGoal:\n\nGrasp the handle\n\n20

Categories (1–5):
- Soundness: The rigor of the methods for the stated problem
- Presentation: The clarity of writing and organization
- Contribution: The paper’s novelty or added value to the domain

Additionally:
- Rating (1–10): Overall recommendation for acceptance
- Confidence (1–5): How confident the reviewer is in their assessment

Please provide:
1) Soundness
2) Presentation
3) Contribution
4) Rating
5) Confidence
6) Explanation (strengths, weaknesses)
7) Questions

response: 

Soundness: 1 poor
Presentation: 1 poor
Contribution: 2 fair

Rating: 3: reject, not good enough
Confidence: 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

Explanation:
Strengths:
Using diffusion models to perform Goal-Conditioned Reinforcement Learning (GCRL) looks like a good idea, although the authors are not the first to put it forward, and are not clear about the novelty they bring.

Weaknesses:
- The paper is very unclear about the \"training-free\" property of the framework, which seems to be the main contribution with respect to Janner et al. (2022) and Liang et al. (2023).
- The paper is unclear about the difference between defining an energy function by hand and defining a goal-related reward function by hand.
- The experiments about using language-based prompts are not explained at all
- The paper is written poorly and often in a non-scientific way with a lot of unnecessary over-statements (see below).

Questions:
- could the authors explain the difference between defining energy functions for partial goals, negative goals, sequence goals, hybrid goals as they do in the bottom of page 3 and defining goal-related reward functions that do the same, as often done in GCRL? If they argue that this makes a difference, can they backup this claim with comparative experiments?
- \"This way of defining a goal is fundamentally different from the conventional way of treating the goal as a variable\" -> could the authors explain why it is \"fundamentally different\"? You could have a goal represented with \"Clean = True\", this would be using a boolean variable, or \"clean level = X\", that would be using a scalar variable. Doesn't your energy function (c or g?) compute something similar?

Please read the following paper summary and produce reviewer scores:

\nMAKE SMALL DATA GREAT AGAIN: LEARNING FROM\nPARTIALLY ANNOTATED DATA VIA POLICY GRADIENT\nFOR MULTI-LABEL CLASSIFICATION TASKS\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nBesides:::the:::::::\n\nTraditional supervised learning methods are heavily reliant on human-annotated\ndatasets. However, obtaining comprehensive human annotations proves challeng-\ning in numerous tasks, especially multi-label tasks. Therefore, we investigate the\nunderstudied problem of partially annotated multi-label classi\ufb01cation. This sce-\nnario involves learning from a multi-label dataset where only a subset of positive\nclasses is annotated. ::::::\nproblem, this task en-\nnegative::::::\ncounters challenges associated with a scarcity of positive class annotations and\nsevere label imbalance. To overcome these challenges, we propose Partially An-\nnotated reinforcement learning with a Policy Gradient algorithm (PAPG), a frame-\nwork combining the exploration capabilities of reinforcement learning with the\nexploitation strengths of supervised learning. By introducing local and global re-\nwards to address class imbalance issues and employing an iterative training strat-\negy equipped with data enhancement, our framework showcases its effectiveness\nand superiority across diverse classi\ufb01cation tasks.\n\nunknown:::::::\n\nclasses::::::::\n\nINTRODUCTION\n\n1\nTraditional supervised learning methods heavily rely on human-annotated data sets, especially neu-\nral approaches that are data-hungry and susceptible to over-\ufb01tting when lacking training data. Nev-\nertheless, in some tasks, human annotations are dif\ufb01cult for the needs of speci\ufb01c domain knowledge\nprovided by professional experts. For instance, the task of document-level relation extraction (Yao\net al., 2019) seeks to identify meaningful relationships between entity pairs within a document.\nHowever, human annotators \ufb01nd it hard to completely annotate all the relations due to the confusion\nof understanding relation de\ufb01nitions and long-context semantics. This phenomenon of incomplete\nannotation is likely to exist in many multi-label tasks that generally have dozens, hundreds, or even\nthousands sizes of class sets (Cole et al., 2021; Tan et al., 2022; Ben-Baruch et al., 2022).\n\nBased on the above observation, we focus on addressing a noteworthy yet under-studied problem,\npartially annotated multi-label classi\ufb01cation (PAMLC), which involves learning from a multi-\nlabel dataset in which only a subset of positive classes is annotated, while all the remaining classes\nunknown:::::::\nare unknown. :::::::\nnegative\nmissing\nfaced::::and ::::::\nclasses;::2)::::::\n::::::\npositive ::::::\n::of ::::::\nclasses;\npositive:::::\n::3) ::::both :::::::\ndistribution\ninestimable :::::::::\n::::prior. Figure 1 A exhibits an example from partially annotated relation data, and Figure 1 B illustrates\nthat the entire dataset contains a limited number of positive annotations, with the majority of labels\nremaining unannotated.\n\nDealing ::::with ::::\nsevere::::::::::::::\nclass::::::::::\n\npositive-negative:::::::::\nannotations ::::::::::\n\nproblem:::::\nimbalance :::that::::::::::\n\nclassi\ufb01cation :::task:::::\nplaguing::::::::::\n\nimbalance::::and :::::::\npartially ::::::::\n\nvalidation ::set:::::being::::::::\n\nexacerbating:::the:::::::::\n\ntraining :::set :::and:::::::::\n\nchallenges:::1)::::::::::\n\nmulti-label :::::::::::\n\nfaces :::::many :::::::::\n\nleading ::to ::::::::::\n\nrecognizing:::::::\n\ncompletely :::::::\n\nannotated:::::::\n\nsuch :a:::::::\n\nmissing:::::\n\nthree :::::::::\n\nlabels::::::mainly:::::\n\nclassi\ufb01cation ::::with ::::::\n\nmulti-label :::::::::::\nsettings ::of :::::::::\n\nPrevious:::::works:::on :::::::::\n:::::::\nSpeci\ufb01cally, :::the :::::::\ntasks.:::::::::::\n::::\ncategories: :::::::\n:::into:::::\nlabeled;::::::::\nclasses :::are ::::::\n::::::\nobserved :::but :::::::\nclasses :::are ::::::::\n::::::\n(SPL) :::::which::::::::\n:::::\n::to ::us::is::::PPL.:::::::::\n:::We :::::::\n\nrequires::at:::::\nsupposes:::::only :::one:::::::\nHowever, :it::::::::::\ngeneral::::::\n\npartially :::::::\npartially ::::::\n\nmulti-label:::::::::::\nlabeling::::::\nobserved:::::::\nleast :::one:::::::\n\nclassi\ufb01cation::::with:::::::\n(POL) :::::which::::::::\nlabeling::::::\npositive:::::\nclass :::per:::::::\nleast :::one:::::::\n\nmissing:::::\nsupposes :::::\n(PPL) :::::which::::::::\nlabel :::per::::::::\ninstance::is:::::::::\npositive:::::\nlimited::to::::::\n\nimage ::::::::::\nclassi\ufb01cation\nroughly::::::\ndivided\npositive:::and:::::::\nnegative\npositive\nsupposes ::::only :::part:::of ::::::\npositive:::::::\nlabeling\nsingle:::::::\nrelated::::::\nsetting\nobserved. ::::The ::::most::::::\nsuitable ::for::::our :::::::\nproblem.\nfacing ::::tasks\n\nlabel ::is :::not :::::::\nimage::::::::::::\n\npositive:::::\nassumption::of::at:::::\n\nobserved:::::::\npositive::::::::\n\nclassi\ufb01cation, ::::thus::::::\n\nsetting::::that::is::::not ::::::\n\nconsider::a :::::more ::::::\n\nlabels:::are:::::::\n\nfocus::on::::::\n\ninstance; :::::\n\npartial:::::::\n\n1\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 1: Illustration of PAMLC task. A. Partially annotated data samples in document relation\nextraction. B. Severe imbalanced distribution of positive (red scatters) and unannotated classes (blue\nscatters). C. Performance comparison of different approaches for document-level relation extraction.\n\nextraction:::::\n\nnegative ::::::\n\npositive:::::::\n\nsetting:::but::::::\n\nhave :::any:::::::\n\nclasses::or:::at ::::\n\nsuffer ::::::::::\n\ntask. :::For :::the :::::\n\nimage:::::::::::\n\nclasses. :::For::::::::\n\nexample,::it::is:::::::\n\ninstance ::::may:::not:::::\n\napplied::to::::our :::::\npartial::::::::\n\nlearning::::only:::::::\nunlabeled::::::::\n\nPositive-Unlabeled:::::(PU)::::::::\nfocuses ::on:::::::\n\nentity :::pair::in:::::::\nsettings:::::\nnecessary ::::::::::\n\nrelation::::::::\ncould::be:::::::\ndirectly::::::\ninformation::::(i.e.,::::::\nmentioning:::::task ::is::::::::::::::::\ntraditional:::PU:::::::\nsetting, ::::::::\npositives::::and :a:::set ::of::::::::\n\nsamples. :::::::\ngenerally::::::\n(Su et al., 2021; Su et al., 2022; Su et al., 2021)::::::::\nreadily:::::\nwhich :::::\ncannot::::::\nsection.:\nrelated ::::work:::::::\n\nthere ::is ::no\npossible::::that:::::\nclassi\ufb01cation::::task,:::::::\nmethods\ndrops:::due\nperformance:::::\nleast::::one :::::::\nclass).\npositive :::::\nlearning:::::::::::::::::::::\n(Jaskie & Spanias, 2019).\nbinary :::::::\nclassi\ufb01er::::with::an\nlearning:a::::::\nmethods::of:::PU\nBesides,:::::many::::::::\nadvanced:::::::\nassume::a ::::given::::::::::::::\npositive-negative\nmulti-label\nadapt::to:::our:::::::::\n\n:::::where::an::::::::\nbetween:::an :::::\nrelation:::::::\n::::::\nthese ::::::\ndesigned:::for:::::\n:::::::\n::to :::::::::\ninsuf\ufb01cient:::::::::\nAnother :::::worth::::::::::\n:::::::\nfrom :::our ::::::\nDifferent:::::\n:::::::\nincomplete:::set ::of :::::::\n:::::::::\nlearning ::::::::::::::::::::::::::::::::::::::\n:::::::\ndistribution:::::\n:::::::::\nsettings. :::We ::::::\n:::::::\nThere are some simple approaches, i.e., negative mode or re-weight strategies, that could be applied\nto our problem settings. Negative mode treats all unknown classes as negatives, which learns a biased\ndistribution and causes high precision and low recall evaluation scores on the complete test set. Re-\nweight strategies based on negative mode generally contain positive up-weight and negative under-\nweight or sampling (Li et al., 2020). We attempt these approaches and observe that the performance\nof the negative mode drops signi\ufb01cantly when the ratio of annotated positive classes decreases. Re-\nweight strategies partly improve the model performance but still perform unsatisfactorily when only\na very small set (10%) of positive class annotations is available (shown in Figure 1 C).\n\nbalanced ::::::::::::::\ndetails ::of ::all:::the:::::::\n\nratio,::::::\nsettings::in:::the::::::\n\nprior ::or :a::::::::\ndiscuss:::the::::::\n\npositive-negative ::::\n\nPrevious works (Silver et al., 2016; Feng et al., 2018; Nooralahzadeh et al., 2019) have demon-\nstrated the powerful exploration ability of Reinforcement Learning (RL). Furthermore, RL has\nbeen shown great success on distant or even zero annotations. To deal with the partially anno-\ntated multi-label classi\ufb01cation problem with imbalanced issues, we propose an RL-based frame-\nwork, namely Partially Annotated Policy Gradient (PAPG), that is devoted to estimating an unbiased\ncombine:::the:::::::::\ndistribution only with the observation of annotated labels. ::::::::::\nexploration\ndesigning:a\nlearning:::by ::::::::\nability::of::::::::::::\n:::::\nalgorithm :::and::a ::::value\npolicy:::::::\nmulti-label::::::::\n:::::\nbeyond:::the::::local\nnetwork ::(as::a :::::\ntrained ::::with :a:::::::::\n:::::::\nglobal::::::\ncontributes::::most\nreward,:::we::::::\nreward::::::::\n::::::\n::to ::::::::\nproblem. In addition, inspired by the actor-critic RL algorithm (Bah-\nimbalanced::::::::\ndanau et al., 2016), we iteratively train the value network and the policy network, which achieves\ndynamic reward estimation and a one-stage training procedure. To gradually enhance the assessment\naccuracy of our value network, we carefully select predicted positive classes with high con\ufb01dence\nto enhance the training set of the value network.\n\nSpeci\ufb01cally, :::we :::::::\nsupervised:::::::\npolicy-based:::RL:::::::::\nreward. :::::::\ninstance,:::::which::::::::::\n\nreinforcement:::::::\nnetwork:::(as::a :::::::::\ncritic) ::::::\ndesign :a::::::\nalleviating:::the::::::::::\n\nlearning::::and :::the ::::::::::\nclassi\ufb01er)::::::::\nsupervised::::loss ::to ::::::\nassessing::::::::::\n\nlearning ::::with :a:::::::::::\nprovide::::local:::::::\npredictions ::of::::each::::::::\n\nability ::of :::::::::\n\nexploitation::::::\n\nBesides,::::::\n\nMoreover, our RL framework is concise and \ufb02exible to guarantee its generalization and adaptation\nto many tasks. We conduct suf\ufb01cient experiments across various complicated domains: synthetic toy\nsetting (\u00a74.1), multi-label image classi\ufb01cation task (\u00a74.2), and multi-relation document extraction\ntask (\u00a74.3). All experimental results show the effectiveness of our framework and demonstrate a\nsigni\ufb01cant improvement over previous work.\n\n2 RELATED WORK\n\nProblem Settings on Weak Annotation Learning Weakly supervised learning has long attracted\nresearchers\u2019 interest because large-scale datasets with high-quality human annotations are time-\nconsuming and dif\ufb01cult. Weakly supervised learning includes different settings according to dif-\nferent assumptions Among them, noisy-label learning (Coscia & Neffke, 2017; Yang et al., 2018;\n\n2\n\nResidence, Living inDeveloper, Sibling,\u2026,Place of birthAlbemarleCountyisacountylocatedinthePiedmontregionoftheCommonwealthofVirginia.ItscountyseatisCharlottesville,whichisanindependentcityandenclaveentirelysurroundedbythecounty.[\u2026]However,itsmostfamousinhabitantwasThomasJefferson,whobuilthisestatehome,Monticello,inthecounty.<Thomas Jefferson,?,Commonwealthof Virginia>zQueryAnnotationUnknown94 Labels Probably CorrectPAPGABC\fUnder review as a conference paper at ICLR 2024\n\nFeng et al., 2018; Ren et al., 2020; Cai et al., 2022) typically utilize datasets containing label noise,\nencompassing instances of both false positives and false negatives; partial-label learning (Zhang &\nYu, 2015; Xu et al., 2019; Wang et al., 2021; Wu et al., 2022; Tian et al., 2023) refers to the task\nwhere each training instance is annotated with a set of candidate labels in which only one is the tar-\nget; semi-supervised learning (Berthelot et al., 2019; Zheng et al., 2022; Yang et al., 2022) typically\nleverages a small set of full-labeled data and an amount of unlabeled data; PU learning (Kiryo et al.,\n2017; Su et al., 2021; Acharya et al., 2022) typically refers to learning a binary classi\ufb01er (positive vs.\nnegative) using an incomplete set of positives and a set of unlabeled samples without any explicitly\nlabeled negative examples. Our problem setting belongs to the domain of weak-supervised learning\nbut distinguishes itself from all preceding problem settings.\n\nSimilarly,::::::::::::::::::::::::::::::::::\nnegative ::::::\n\nLearning Methods for Partial Supervision Similar to our setting, some previous work addresses\npartial supervision problems. There are some methods, such as partial conditional random \ufb01eld, that\ndeal with single-label multi-class tasks with partial supervision (Mayhew et al., 2019; Ef\ufb02and &\nCollins, 2021; Li et al., 2021; Zhou et al., 2022). In the multi-label classi\ufb01cation area, Cole et al.\n(2021); Kim et al. (2022) assumes a speci\ufb01c setting where only one label for each image is available.\nBen-Baruch et al. (2022) designs a class-selective loss to tackle the partial observation of a small set\nof positives and negatives. :::::::\nDurand et al. (2019); Durand et al. (2022)::::also:::::::\npartial\nsuppose:::::\nclasses. The methods of PU learning contain cost-sensitive ap-\npositive::::and :::::::\nobservation::of:::::::\n:::::::::\nproaches (Christoffel et al., 2016; Su et al., 2021) which assume the data distribution prior to achiev-\ning unbiased risk estimation; representation clustering approaches Acharya et al. (2022) leveraging\ncontrastive learning to generate pseudo-positive/negative labels; and sample-selection approaches\n(Zhang & Zuo, 2009; Luo et al., 2021) which are devoted to \ufb01nding likely negatives from the un-\ncannot ::be\nlabeled data according to the heuristic methods or sample con\ufb01dence. ::::These::::::::\nlabel :::::::::\nestimate:::the:::::\ndistribution\nbecause::it ::is :::::::::\neffectively:::::::\n::::::::\n::::prior::to:::the::::::::::\nmissing. We note a\nlabels:::are::::::::\nconcurrent work (Yuan et al., 2023) extending positive-unlabeled learning into a multi-label classi-\n\ufb01cation task. Nevertheless, their research has primarily concentrated on method development within\nthe domain of image classi\ufb01cation. In contrast, our framework exhibits broader applicability, ex-\ntending its utility to various tasks.\n\ntask,:::::::\ndataset ::in:::our::::::\n\nadapted::to::::our ::::\nmulti-label ::::::\n\nknow ::or :::::::\nnegative :::::\n\nsetting::::::where ::all::::the :::::::\n\nimpractical:::to :::::\n\nmethods ::::::\n\nReinforcement Learning under Weak Supervision There are many previous works leveraging\nReinforcement Learning (RL) to solve tasks only with weak supervision (Feng et al., 2018; Zeng\net al., 2018; Luo et al., 2021; Chen et al., 2023). In the NLP \ufb01eld, to precisely leverage distant data,\nQin et al. (2018); Feng et al. (2018) train an agent as a noisy-sentence \ufb01lter, taking performance\nvariation on development or probabilities of selected samples as a reward and adopting policy gra-\ndient to update. Nooralahzadeh et al. (2019) expand their methods to NER task. Recent work of\nChen et al. (2023) also conducts RL to remove the noisy sentence so as to improve the fault diag-\nnosis system. Notably, RL learning on distantly supervised learning aims to \ufb01lter false positives,\nwhereas our goal is to identify false negatives. A closer related work to us is Luo et al. (2021), in\nwhich an RL method is designed to solve the PU learning problem. But unlike us, their agent is a\nnegative sample selector, aiming to \ufb01nd negatives with high con\ufb01dence and then couple them with\npartial positives to train a classi\ufb01er. Besides, they suppose a fully annotated validation dataset and a\nbalanced positive-negative distribution prior.\n\n3 REINFORCEMENT LEARNING WITH PARTIAL SUPERVISION\n\nWe propose a new RL framework to solve the partially annotated multi-label classi\ufb01cation (PAMLC)\ntask. We formulate the multi-label prediction as the action execution in the Markov Decision Process\n(MDP) (Puterman, 1990). We design both local and global rewards for actions to guide the action\ndecision process. The policy-based RL method is adopted to train our policy network. We introduce\nthe details of our RL framework in the following subsections.\n\n3.1 PROBLEM SETTING\n\n=\nxi}\nWe \ufb01rst mathematically formulate the PAMLC task. Given a multi-label dataset\n, each\nX\n{\n, yc\nis labeled with a partially annotated multi-hot vector yi = [y1\nxi 2X\n,\ni ,\ni ,\ny|C|i\n], where\n\u00b7 \u00b7 \u00b7\n::::::\nyc\nis TRUE for instance xi and ::|C|::is:::the:::::::::\ndenotes whether class c\ncardinality:::of ::set\ni 2{\nc; yc\ni = 1\nclasses\npositive::::::\n. In terms of partial annotation, we assume that\nis a subset of :::gold:::::::\n:C\n}\nUNKNOWN::::::\nregarding ::to :::xi, :::i.e.,::::::::::\nclasses. Typically, in the multi-label setting,\n} :is:::the:::set::of:::::::::\n::::::::\n\nc; yc\n{\n\n0, 1\n}\n\ni = 0\n\n2C\n\n\u00b7 \u00b7 \u00b7\n\n{\n\n3\n\n\fUnder review as a conference paper at ICLR 2024\n\nInput $\n\nPolicy Network\n(#)\n\"%\n\n$!(&)\n\n0.9\n0.2\n0.8\n0.1 \n\u2026\n\n1\n0\n1\n0 \n\u2026\n\nAnnotation # 1\n0\n0\n0 \n\u2026\n\nAction \n!\"~$!(&)\n\nGlobal Reward\n\n0.3       \n\nReward %\n\nUpdate $! with\n\u2207!)\"# *\n\nLocal Reward\n\n-0.3 -0.9 1 0.5       \u2026\n\nPolicy Network\n(#&')\n\n\"%\n\nValue Network\n(#)\n!!\n\nMax Prob\nAction\n\nAnnota\ntion #\n\nUpdate +$\twith\nSupervised Loss\n\nValue Network\n(#&')\n\n!!\n\n\u2026\n\n+\n\n1\n0\n1\n0 \n\u2026\n\n1\n0\n0\n0 \n\u2026\n\nFigure 2: Illustration of our RL framework.\nrepresents union operation. We iteratively update the\npolicy network and value network. The augmented training data are curated for the value network.\n\nL\n\n. In some\nthe size of the label set is dozens or hundreds; thus\ncomplicated tasks, such as Relation Extraction, we further de\ufb01ne a special label <None> to xi if\nyi = [yc\ni = 0]|C|c=1. It should be mentioned that we do not have any fully annotated data, both the\ntraining set and validation set being partially annotated.\n\n}| \uf8ff |C|\n\ni = 1\n\ni = 0\n\n<<\n\n|{\n\n|{\n\n}|\n\nc; yc\n\nc; yc\n\nAs aforementioned, negative mode approach is susceptible to training models that conform to the\nimbalanced data distribution, resulting in high precision but low recall on the test set. Based on\nthe negative mode, we introduce a reinforcement learning (RL) framework designed to mitigate\ndistribution bias and encourage the multi-label classi\ufb01er to predict more potential positive classes.\n\n3.2 MODELING\n, R) which contains a set of environment and\nTypically, basic RL is modeled as an MDP (S, A, \u21e1,\nagent states S, a set of actions A of the agent, the transition probabilities\nfrom a state to another\nstate under action a, and the reward R. The goal of an RL agent is to learn a policy \u21e1 that maximizes\nthe expected cumulative reward. In our problem setting, we do not take state transitions into account\nbecause our action execution does not change the agent and environment. The policy \u21e1\u2713 of our RL\nframework is a multi-label classi\ufb01er constructed by a neural network with parameter \u2713. We following\nde\ufb01ne the constituents in our RL framework in detail.\n\nT\n\nT\n\nStates A state s includes the potential information of an instance to be labeled. In our setting,\nthis information consists of instance features, which are essentially continuous real-valued vectors\nderived from a neural network.\n\nActions Due to the multi-label setting, our agent is required to determine the label of each class c\nfor one instance. There are two actions for our agent: setting the current class as TRUE (yc\ni = 1) or\nUNKNOWN (yc\ni = 0). Consequently, it is necessary to execute a sequence of (size of class set) actions\n::::::::\nto completely label an instance.\nPolicy Our policy network outputs the probability \u21e1\u2713(yc\ns = xi) for each\ni |\naction condition on the current state. we adopt the model structure commonly utilized in previous\nsupervised studies as the architecture for our policy network.\n\nxi) = P (a = yc\ni |\n\nRewards Recall that our primary objective is to acquire a less biased label distribution compared\nto the supervised negative mode training approach using the partially annotated training dataset.\nWe anticipate that our PAPG possesses the capacity for balanced consideration of both exploitation\nand exploration. exploitation ensures that our agent avoids straying from local optima direction,\nwhile exploration motivates our agent to adapt its policy, preventing over\ufb01tting to partial supervision\nwithin the broader global context. Inspired by the actor-critic RL algorithm (Bahdanau et al., 2016),\nwe design our rewards function containing two parts: a local reward from a trainable value network,\nwhich provides immediate value estimation of each action and a global reward regarding the overall\nperformance of the whole actions sequence for each instance.\n\nSpeci\ufb01cally, the local reward calculates the reward of each action (for each class) of each instance\naccording to the value network con\ufb01dence to an action:\n\nrc\ni (V\u0000, xi, c) =\n\n1, log\n\n1, log\n\n1\n1\n\nC(\n\nC(\n\n\u0000\n\n\u0000\n\n4\n\n8\n<\n\n:\n\npc\nV\u0000\npc\nV\u0000\npc\nV\u0000\n\n\u0000\n\u0000\npc\n\n(xi)\n(xi) , 1)\n(xi)\n, 1)\n\n(xi)\n\nV\u0000\n\nif \u02c6yc\n\ni = 1,\n\nif \u02c6yc\n\ni = 0.\n\n(1)\n\n\fUnder review as a conference paper at ICLR 2024\n\nwhere pc\nV\u0000\nV\u0000, and C(\n\u0000\nx > 1; c) otherwise, C(\nrewards of an instance.\n\n\u0000\n\n(xi) is the probability of class c being TRUE for instance xi, calculated by a value network\n1, x, 1) = 1 if\n\u0000\n1, x, 1) = x. We sum out the local rewards of all classes as part of the \ufb01nal\n\n, 1) is a clamping function: a) C(\n\n1, x, 1) =\n\n1; b) C(\n\n1 if x <\n\n1,\n\n\u0000\n\n\u0000\n\n\u0000\n\n\u00b7\n\nIntuitively, the local rewards offer a preliminary yet instructive signal to guide the learning process in\nour PAPG framework. This signal inherits the exploitation aspect from the supervised loss training\n(as the value network is trained through supervised learning). Its purpose is to prevent the PAPG\nfrom engaging in excessively invalid exploratory behavior within the large action space, thereby\nenhancing the overall learning ef\ufb01ciency. Nevertheless, relying solely on these local rewards may\npotentially lead the PAPG system to converge to a biased \u201cnegative mode\u201d solution. To mitigate this\nrisk, we introduce global rewards to stimulate more comprehensive exploration during the learning\nprocess.\n\nAs for the global reward, we employ a straightforward yet highly effective scoring function, which\nis computed based on the recall metric. In detail, for the whole classes prediction \u02c6yi of xi with the\nobserved ground truth yi, the recall score is:\n\nyc\ni = 1\n\nrecall(yi, \u02c6yi) = |{\n\n^\n|{\nThe key insight here revolves around the \u201crecall\u201d metric, which serves as an exactly accurate evalua-\ntion measure (compared to \u201cprecision\u201d and \u201cF1\u201d) because a substantial portion of positive instances\nremains unlabeled within the context of partially annotated datasets. Furthermore, to enhance recall\nscores, the policy network is encouraged to predict a greater number of classes as TRUE.\n\n(2)\n\ni = 1, yc\n\u02c6yc\ni = 1, yc\nyc\n\nyi, \u02c6yc\nyi}|\n\ni 2\ni 2\n\ni 2\n\n\u02c6yi}|\n\nThe complete reward function for a sampled action sequence of an instance is:\n\nR(xi, \u02c6yi, V\u0000, yi) =\n\n1\n\nrc\ni (V\u0000, xi, c) + w\n\nrecall(yi, \u02c6yi),\n\n(3)\n\n\u21e4\n\n|C| Xc\n2C\nwhere w is a weight controlling the scale balance between local reward and global reward.\n\n3.3 PREDICTION\ninstance:::are:::::::\npredictions ::of::::each:::::::\n:::The::::\ufb01nal::::::::::\n(\u21e1\u2713(yc\nprobabilities:::are ::::more::::than:::0.5:::::::::::::::\ni |\n::::::::::\nprediction::of::::each::::class:::for:::an :::::::\ndirectly::::::\n::::::\nreason:::of :::::::\n::to ::::\n\noutput ::the:::::::::\nenhancement:::for :::the :::::\n\ndecided:::::::::\nxi) > 0.5):::are ::set::to::as::::::\ninstance.::::::::\n\nreducing:::the::::::::\n\nlabel:::::::::::\n\naccording ::to:::the::::::\n\npolicy :::::::\n\nTRUE.::::Note::::that :::our ::::value:::::::\n\nnetwork.:::::::\n\nClasses :::::whose\nnetwork:::can\npredictions\nleverage::its:::::::::\n\nHowever,:::we ::::only :::::::\nenhanced:::::\n\nin\ufb02uence::of::::::::\n\nlabels.\nnoisy::::::\n\n3.4 LEARNING\nWe iteratively train our value network and policy network in an end-to-end fashion. Since the value\nnetwork plays a critical role in guiding policy network learning, we employ data enhancement tech-\nniques during the training of the value network to enhance the precision of value estimations. It is\nimportant to emphasize that we intentionally exclude the enhanced data from participation in the\ncalculation of the recall reward. This decision is motivated by the desire to maintain the precision\nof the global reward and prevent potential noise introduced by the enhanced data. Additionally, it is\nnoteworthy that the enhanced data could contribute to the bene\ufb01t of the policy network through the\nlocal reward signal provided by the value network.\n\nIt is widely acknowledged that the training process in RL can experience slow convergence when\nconfronted with a vast exploration space. Inspired by previous RL-related works (Silver et al., 2016;\nQin et al., 2018), we initiate our process by conducting pre-training for both our policy and value\nnetworks before proceeding with the RL phase. Typically, pre-training is executed through a super-\nvised method. In our settings, a range of trivial solutions for PAMLC can serve as suitable candidates\nfor pre-training. We simply utilize the negative mode for pre-training in most cases. However, as pre-\nviously mentioned, the negative mode has a tendency to acquire a biased label distribution due to\nthe presence of severe label imbalance. Thus, we implement an early-stopping strategy during the\npre-training phase to prevent convergence. We following introduce the detailed learning strategies\nand objectives.\n\nObjective for Value Model Generally, a well-designed supervised objective urges models to learn\nexpected outputs by learning from annotated data. This process typically refers to the exploitation,\nwhere the supervised model \ufb01ts the distribution of label annotations. We denote the supervised\n\n5\n\n\fUnder review as a conference paper at ICLR 2024\n\nobjective by a general formulation:\n\nLSU P (\u2713) =\n\np(xi)\n\nD\n\n(yi, \u02c6yi),\n\n(4)\n\nXxi2X\n\nis a task-speci\ufb01c distance metric measuring the distance between annotation yi and predic-\nwhere\ntion \u02c6yi. Recall that we treat all the unannotated classes as negatives in the partially annotated setting\nin order to perform supervised learning.\n\nD\n\nObjective for Policy Model As stated in previous work (Qin et al., 2018), policy-based RL is more\neffective than value-based RL in classi\ufb01cation tasks because the stochastic policies of the policy\nnetwork are capable of preventing the agent from getting stuck in an intermediate state. We leverage\npolicy-based optimization for RL training. The objective is to maximize the expected reward:\n\nJP G(\u2713) = E\u21e1\u2713 [R(\u2713)]\n\n\u21e1\n\np(xi)\n\nbatch\n\nXxi2\n\n\u21e1\u2713( \u02c6yi|\nX\u02c6yi\u21e0\n\n\u21e1\u2713( \u02c6yi|\n\nxi)\n\nxi)R( \u02c6yi.xi),\n\n(5)\n\nThe policy network \u21e1\u2713 can be optimized w.r.t. the policy gradient REINFORCE algorithm (Williams,\n1992), where the gradient is computed by\nr\u2713JP G(\u2713) =\n\nr\u2713 ln(\u21e1\u2713(\u02c6y\n\nxi))R(\u02c6y, xi),\n\np(xi)\n\n(6)\n\n|\n\nbatch\n\nXxi2\n\n\u21e1\u2713(\u02c6y\nX\u02c6y\n\u21e0\n\n|\n\nxi)\n\nwhere p(xi) is a prior distribution of input data. Speci\ufb01c to uniform distribution, p(x) =\n\n1\nxbatch|\n\n|\n\n.\n\nprediction::of:::::\n\nrefers ::to:::the:::::::::\n\nOverall Training Procedure The overall training process is demonstrated in Algorithm 1, ::::where\n\u02c6yc\nsample::xi\ni = V\u0000(xi)c = 1 :::::\n:::::::::::::::\nTRUE. It needs to be clari\ufb01ed that our computation of local rewards for actions is based\n::::being::::::\non the annotated positive classes and a randomly selected subset of unknown classes rather than\nconsidering the entire sequence of actions of an instance, which is intended to emphasize the impact\nof positive classes within the computation of local rewards. We empirically prove the effectiveness\nof this strategy. Furthermore, we \ufb01x the value network once it converges, a strategy employed to\nenhance training ef\ufb01ciency.\n\noutputs :::for :::the ::::::\n\nclass :c::::that:::::\n\nnetwork:::::::\n\nvalue :::::::\n\nAlgorithm 1: Partially Annotated Policy Gradient Algorithm\nyi}\nxi}\nInput: Observed data\nOutput: Optimal parameters \u2713\u21e4\n\n, partial annotations\nvalue network V\u0000, REINFORCE learning rate \u21b5, con\ufb01dence threshold \u0000\n\n=\n\n=\n\nX\n\nY\n\n{\n\n{\n\n, pre-trained policy network \u21e1\u27130 ,\n\n1 e\n\n0, \u2713\u21e4\n\n\u27130, ::::::::\n\nenhanced ::::::::\n\nannotation set \u00af\n\nY Y::::\n\n2 while e < total training epoches do\nTraining set for value network: (\n3\n\nX\nTraining set for policy network: (\nfor\n\n, \u00af\nY: )\n)\n,\nY\nX\nin total batches do\nXbatch 2X\nUpdate \u0000 by minimizing Equation (4) with \u00af\n\nfor step t < sample steps T do\n\n\u21e1\u2713(\u02c6yi|\nFor each xi 2X batch, sample \u02c6yi w.r.t. \u02c6yi \u21e0\nxi)\nCompute R(xi, \u02c6yi, V\u0000, yi) according to Equation (3)\n\nfor value network\n\nYbatch\n\n:::::\n\n\u2713 + \u21b5\n\nr\u2713JP G(\u2713) according to Equation (6)\nxi) >\u0000 and \u02c6yc\n(\u21e1\u2713(\u02c6yc\ni = 1::or::::::::::::::::::::::::::::::::::::\n\ni = V\u0000(xi)c = 1 ::))\n\ni :=::1 ::if ::::::\n\n(yc\n\ni = 1\n|\n\nupdate policy network using \u2713\n\n,:::::where:::\u00afyc\n\n\u00af\n[\u00afyc\ni ]|C|c=1}\n:::::::::::::\nY {\nif eval(\u21e1\u2713) > eval(\u21e1\u2713\u21e4 ) then\n\u2713\n\u2713\u21e4\ne + 1\n\ne\n14\n15 return \u2713\u21e4\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n4 EXPERIMENTS\n\nTo verify the effectiveness of our proposed RL framework, we experiment with three widely-\nconcerned tasks. The \ufb01rst one follows the general setting in PU learning, but we consider an im-\n\n6\n\n \n \n \n \n \n\fUnder review as a conference paper at ICLR 2024\n\nTable 1: F1 scores with varying ratios of positive annotations. We take images of the \u201cAirplane\u201d\ncategory as positives in this table.\n\nMethod\n\n10% 20% 30% 40% 50% 60% 70% 80% 90%\n\n44.8\nnnPU\nImbalancednnPU 48.6\n\nNegative Mode\nPAPG (Ours)\n\n7.0\n50.2\n\n47.9\n53.0\n\n17.1\n64.6\n\n49.1\n59.1\n\n28.7\n66.6\n\n50.4\n62.8\n\n37.6\n67.8\n\n52.8\n65.2\n\n56.9\n69.8\n\n54.1\n64.6\n\n55.6\n70.8\n\n56.1\n64.9\n\n64.2\n75.1\n\n57.4\n67.7\n\n71.2\n75.8\n\n56.7\n69.2\n\n75.2\n76.9\n\nbalanced situation. This experiment aims to prove the adaptation of our framework to binary clas-\nsi\ufb01cation problems. The second and the third focus on two partially annotated multi-label tasks,\ndocument-level relation extraction and multi-label image classi\ufb01cation, selected from classical Nat-\nural Language Processing (NLP) tasks and Computer Vision (CV) tasks, respectively.\n\n4.1 EXP1: SYNTHETIC CLASSIFICATION\nWe conduct binary image classi\ufb01cation experiments with the same setting following Su et al. (2021)\nthat concentrates on positive/negative imbalanced problems in PU learning. With our formulation, an\ninstance xi is an image, and the label of an instance in binary classi\ufb01cation settings can be denoted\nas yi = [y1\ni is the label of negative. The prediction for\neach image is conducted by setting y corresponding to the higher score as 1 and the other as 0.\n\ni is the label of positive and y2\n\ni ] where y1\n\ni , y2\n\nDataset The imbalanced datasets are constructed\nfrom CIFAR101 by picking only one category as\npositives and treating all other data as negatives.\nHence, there are 50,000 training data and 10,000 test\ndata as provided by the original CIFAR10. To make\nthe training data into a partially annotated learning\nproblem, we randomly sample a ratio of positives as\nannotated data and all the leaving training data as an\nunknown set.\n\nFigure 3: Experimental results of the setting\nwith \u201ctruck\u201d category as positives.\n\nCon\ufb01guration and Baselines Of note, our frame-\nwork can integrate any supervised model architec-\nture. For a fair comparison, we take the same archi-\ntecture of Kiryo et al. (2017); Su et al. (2021) as our\nvalue and policy networks, i.e., a 13-layer CNN with\nReLU and Adam as the optimizer. Kiryo et al. (2017)\ndesigned an algorithm nnPU for PU learning with balanced binary classi\ufb01cation data, while Su\net al. (2021) proposed ImbalancednnPU considering imbalanced setting. We take these two previ-\nous state-of-the-art models as our compared baselines. We re-run nnPU and ImbalancennPU with\ntheir provided codes and con\ufb01gurations and report the results. We tune the hyper-parameter w in\nwith different experiment settings, and w is dynamically adjusted during\nEq.3 between\ntraining2. The action sampling number T is 100, and the negative reward sampling is 20% for all\nsettings. The threshold \u0000 to choose enhancement labels is 0.8. We keep the values of other hyper-\nparameters the same as Su et al. (2021). Following previous work, we evaluate all the methods with\nF1 scores. Unless stated otherwise, the hyper-parameters speci\ufb01ed in our framework remain the\nsame in the following experiments.\n\n10, 20, 50\n{\n\n}\n\nResults We show F1 scores with varying ratios of annotated positives in Table 1 (Precision and\nRecall metrics can be found in Appendix B.2). We perform supervised training with Binary Cross-\nEntropy (BCE) loss in Negative Mode. Naturally, the supervised training performs worse by worse\nas the ratios of annotated positives decrease. It is obvious that Our PAPG achieves signi\ufb01cant\nimprovements over Negative Mode, especially when the ratio of annotated data becomes lower.\nCompared to previous work, our framework still demonstrates its superiority even though Imbal-\nancednnPU is speci\ufb01cally designed for PU learning with imbalanced settings. Note that according\n\n1A multi-class dataset containing ten categories. https://web.cs.toronto.edu/\n2Intuitively, the w of recall reward should be dropped along with the training epochs because our value\n\nnetwork provides more and more accurate local rewards bene\ufb01cial by data enhancement before convergence.\n\n7\n\n\fUnder review as a conference paper at ICLR 2024\n\nTable 2: Experimental Results on COCO datasets with varying ratios of positive classes annotations.\n\nMethod\n\nERP\nROLE\nP-ASL+Negative\nP-ASL+Counting\n\nNegative Mode\nPAPG (Ours)\n\n10%\n\n30%\n\n50%\n\n70%\n\n90%\n\nF1\n\nmAP\n\nF1\n\nmAP\n\nF1\n\nmAP\n\nF1\n\nmAP\n\nF1\n\nmAP\n\n-\n-\n45.2\n5.1\n\n6.4\n68.3\n\n63.8\n58.2\n66.9\n46.4\n\n50.6\n66.6\n\n-\n-\n52.1\n26.4\n\n33.7\n77.0\n\n71.0\n72.4\n74.6\n63.4\n\n64.3\n77.5\n\n-\n-\n54.0\n53.7\n\n52.9\n79.1\n\n73.5\n76.6\n76.9\n76.1\n\n73.8\n80.4\n\n-\n-\n71.9\n71.6\n\n72.3\n79.0\n\n73.8\n79.5\n81.0\n80.1\n\n81.2\n81.4\n\n-\n-\n80.3\n60.4\n\n80.1\n80.5\n\n74.4\n81.1\n83.3\n80.4\n\n83.5\n83.4\n\nto the data construction, any category of data can be chosen as the positive set. To further make our\nexperiments convincing, we show the results of different data construction in Figure 3.\n\n4.2 EXP2: MULTI-LABEL IMAGE CLASSIFICATION\nWe demonstrate the effectiveness of our method on the multi-label image classi\ufb01cation task.\n\nDataset Following Ben-Baruch et al. (2022) (P-ASL), which deals with partial annotations con-\ntaining both positive and negative classes, we utilize MS-COCO dataset (Lin et al., 2014) containing\n80 classes. We keep the original split with 82081 training samples, and 40137 test samples. We sim-\nulate the partial annotations following the operations in P-ASL. But different from them, we only\nretain the positive classes in their partial annotations and take all the rest of the classes as UNKNOWN.\n\nCon\ufb01guration and Baselines For a fair comparison, our value and policy networks have the same\narchitecture as P-ASL. Due to the different partially annotated settings, we re-run P-ASL utilizing\ntheir codebase but with our datasets. P-ASL+Negative means training a model taking all UNKNOWN\nas negative classes to predict label distribution as prior. P-ASL+Counting means counting partially\nlabeled positive classes as distribution prior. ::We::::also::::::\nfrom Cole\ncode. We tune the hyper-parameter w between\net al. (2021) ::::with :::our:::::::\ndatasets,::::::::\n5, 7, 12\nin this task. Following previous work (Ridnik et al., 2021; Huynh & Elhamifar, 2020),\n{\n}\nwe use both F1 scores and mAP as evaluation metrics in this task. Detailed methodology of the\nre-weight approach and the detailed formula of metric calculations can be found in Appendix A.\n\nre-run ::::EPR::::and ::::::\n\nROLE::::::::\n\nmethods ::::\n\nutilizing ::::\n\nof\ufb01cial :::::\n\ntheir ::::::\n\nResults Experimental results with different annotation ratios are shown in Table 12. We can \ufb01nd\nthat without annotated negative classes to estimate a relatively accurate distribution prior, P-ASL\nperforms unsatisfactorily, especially in F1 scores when the annotation ratios decrease. Our PAPG\ntimes:::and:::::\nmodel outperforms all baselines in all settings. :::::::::::\nfound\n::::very ::::\nrobustness:::and\nsmall::::::::\nscores:::and:::::mAP,::::::which :::::::::::\nframework. Standard deviations of three runs and more experimental results can be\nstability:::of :::our::::::::::\n::::::\nfound in Appendix B.2.\n\nFurthermore, ::we:::ran::::our :::::model::::\n\ndemonstrates:::the::::high:::::::::\n\ndeviations:::of ::F1::::::\n\nstandard ::::::::\n\nthree:::::\n\n4.3 EXP3: DOCUMENT-LEVEL RELATION EXTRACTION\nDocument-level Relation Extraction (DocRE) is a task that focuses on extracting \ufb01ne-grained rela-\ntions between entity pairs within a lengthy context. Align to our formulation, an input xi is an entity\npair, and yi represents relation types between the entity pair. An entity pair may have multiple rela-\ntions or have no relation in DocRE. Thus, we conduct this experiment to study how our framework\nperforms when <None> label exists in a multi-label classi\ufb01cation task.\n\nDataset We chose the Re-DocRED which is the most complete annotated dataset in DocRE. The\nis 97 (contains <None>) in Re-DocRED. To simulate partial annotation, we\nsize of label set\nrandomly kept a ratio of annotated relations as the above experiments do.\n\nC\n\nthe state-of-the-art model of\nIn this experiment, we adopt\nCon\ufb01guration and Baselines\nDREEAM (Ma et al., 2023) as the architectures of our value and policy networks. Meanwhile,\nwe keep the same training method with an Adaptive Thresholding Loss (ATL) of DREEAM for our\nvalue network. To the best of our knowledge, there are currently no existing methods speci\ufb01cally\ndesigned for addressing partially annotated DocRE, so we only take Pos Weight and Neg Weight,\nadopting the same methodologies as the image classi\ufb01cation experiments, as our compared base-\nlines. The threshold \u0000 of choosing enhancement labels is 0.95, and we \ufb01nd our framework performs\nrobust to \u0000 varying from 0.5 to 0.95.\n\n8\n\n\fUnder review as a conference paper at ICLR 2024\n\nTable 3: Results on Re-DocRED datasets with varying ratios of positive classes annotations\n\nMethod\n\nP\n\n10%\nR\n\nDREEAM\n\n89.8 3.8\n\nF1\n\n7.3\n\n30%\nR\n\nP\n\nF1\n\nP\n\n50%\nR\n\nF1\n\nP\n\n70%\nR\n\nF1\n\nP\n\n100%\nR\n\nF1\n\n92.0 20.8\n\n33.8 91.8 42.1 57.7 89.6 58.8 70.6 86.0 72.4 78.6\n\n70.0 85.0 66.8 74.8 83.4 72.5 77.6 82.9 77.3 80.0\nPos Weight\n69.8 84.1 68.2 75.3 82.8 72.8 77.5 79.8 78.7 79.3\nNeg Weight\nPAPG (Ours) 58.5 77.0 66.0 83.5 67.71 74.7 81.4 73.6 77.3 83.3 73.9 78.3 80.9 80.8 80.9\n\n84.9 39.8 54.1 85.5 59.3\n86.7 30.7 45.4 85.0 59.3\n\nTable 4: Ablation study on our rewards.\n\nTable 5: Ablation study on our training strategy.\n\nMethod\n\nPAPG (Ours)\nw/o. Local reward\nw/o. Global reward\nw. Prec\nw. F1\n\nRe-DocRED\n\nCOCO\n\nP\n\n64.5\n12.2\n84.5\n85.9\n86.0\n\nR\n\n72.8\n93.3\n45.9\n44.0\n43.3\n\nF1\n\n68.4\n21.6\n59.5\n58.1\n57.6\n\nP\n\n80.9\n61.5\n89.7\n96.2\n89.2\n\nR\n\n59.2\n65.5\n6.9\n29.8\n46.8\n\nF1\n\n68.3\n63.4\n12.8\n45.5\n61.4\n\nMethod\n\nPAPG (Ours)\nw/o. Iterative training\nw/o. Label enhancement\nw/o. Action sampling\nSupervised self-training\n\nRe-DocRED\n\nCOCO\n\nP\n\n64.5\n89.9\n83.6\n88.2\n68.0\n\nR\n\n72.8\n34.6\n47.2\n36.5\n29.0\n\nF1\n\n68.4\n49.9\n60.3\n51.7\n40.7\n\nP\n\n80.9\n76.4\n51.7\n96.4\n89.7\n\nR\n\n59.2\n33.8\n54.8\n20.4\n6.6\n\nF1\n\n68.3\n46.9\n53.2\n33.7\n12.3\n\naverage::::::\n\nresults ::::over :::::\n\ndatasets::::and :::::\n\nversions::of:::::::\n\nsample ::::three\nResults Experimental results in Re-DocRED are shown in Table 3. :::We::::::::\nthem. Similar to Negative Mode,\nreport:::the:::::::\ndifferent :::::::\n:::::::\nDREEAM performs supervised training with all unknown classes as negatives in the partially\nannotated settings, which is our base model in this task. Similar to the above experiments, Our\nPAPG demonstrates its advantage in all annotation ratios. It is worth noting that our framework\nalso achieves improved performance with the full annotated dataset because the full annotations of\nRe-DocRED still miss some actual relations as aforementioned in the introduction. We also show\nprecision and recall evaluation metrics in this task. It can be seen that our framework achieves con-\nsistent improvement in recall scores, suggesting its ability to deal with imbalanced problems and\npredict more positive labels. We provide more detailed experimental results in Appendix B.1.\n\nrandomly:::::::\n\n4.4 ANALYSIS\nWe conduct ablation studies to analyze our PAPG framework both on modeling and training strategy.\n\nRewards Design: To show the effectiveness of combining exploitation and exploration and the\nbene\ufb01t of local and global rewards, we train our framework in the 10% annotations setting with-\nout local rewards and global rewards, respectively. Additionally, we replace the recall scores with\nprecision w. Prec or F1 scores w. F1 as our global rewards to show the effects of different global\nreward designs. Experimental results are shown in Tabel 4. It can be observed that it is hard for an\nRL framework to achieve comparable performance without local rewards to guide exploitation. The\nreason is that the action space of multi-label classi\ufb01cation is too large to \ufb01nd the global optimal di-\nrections. Without our global reward, the recall evaluation score drops a lot (72.78 vs. 45.94), which\ndemonstrates the big advantage of the global reward in alleviating imbalance distribution. Moreover,\nboth the two variants of global reward damage the performance, revealing the advance of taking the\nexactly accurate evaluation as rewards in the partially annotated setting.\n\nTraining Strategy: To verify the effectiveness of our training procedure, we attempt different\ntraining strategies shown in Tabel 5. w/o. Iterative training means that we \ufb01x the value network\nafter pretraining and only train the policy network in the RL training procedure. w/o. Data enhance-\nment means that we still iteratively train our value and policy network but do not enhance pseudo\nlabels for the value network. w/o. Action sampling means that we leverage the whole action se-\nquence to calculate local rewards without sampling operation illustrated in Section 3.4. Supervised\nself-training means that we conduct self-training of the value network. It is obvious that our training\nmethod makes remarkable achievements. More analysis experiments are in Appendix B.1.\n\n5 CONCLUSION\n\nIn this work, we propose an RL framework to deal with partially annotated multi-label classi\ufb01cation\ntasks. We design local rewards assessed by a value network and global rewards assessed by recall\nfunctions to guide the learning of our policy network, achieving both exploitation and exploration.\nWith an iterative training procedure and a cautious data enhancement, our PAPG has demonstrated\nits effectiveness and superiority on different tasks in different domains.\n\n9\n\n\fUnder review as a conference paper at ICLR 2024\n\nREPRODUCIBILITY STATEMENT\n\nThe detailed descriptions of the three experiments covered in this article are described in the con-\n\ufb01guration of the corresponding subsections in Section 4, and the implementation details of the Pos\nWeight and Neg Weight methods are introduced in Appendix A.1. The Dataset module of each\nsubsection clearly describes the construction of the data set used in the experiment. For experiments\non hyperparameter selection, see Appendix B.1. All dataset we used are publicly accessible. Codes\nthat can reproduce our results are available at https://anonymous.4open.science/r/\niclr2024-64EB/.\n\nREFERENCES\n\nAnish Acharya, Sujay Sanghavi, Li Jing, Bhargav Bhushanam, Dhruv Choudhary, Michael Rabbat,\nand Inderjit Dhillon. Positive unlabeled contrastive learning. arXiv preprint arXiv:2206.01206,\n2022. 3\n\nDzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron\nCourville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. arXiv preprint\narXiv:1607.07086, 2016. 2, 4\n\nEmanuel Ben-Baruch, Tal Ridnik, Itamar Friedman, Avi Ben-Cohen, Nadav Zamir, Asaf Noy, and\nLihi Zelnik-Manor. Multi-label classi\ufb01cation with partial annotations using class-aware selective\nloss. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 4764\u20134772, 2022. 1, 3, 8\n\nDavid Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A\nRaffel. Mixmatch: A holistic approach to semi-supervised learning. Advances in neural informa-\ntion processing systems, 32, 2019. 3\n\nHomgmin Cai, Wenxiong Liao, Zhengliang Liu, Xiaoke Huang, Yiyang Zhang, Siqi Ding, Sheng Li,\nQuanzheng Li, Tianming Liu, and Xiang Li. Coarse-to-\ufb01ne knowledge graph domain adaptation\nbased on distantly-supervised iterative training. arXiv preprint arXiv:2211.02849, 2022. 3\n\nChong Chen, Tao Wang, Yu Zheng, Ying Liu, Haojia Xie, Jianfeng Deng, and Lianglun Cheng. Re-\ninforcement learning-based distant supervision relation extraction for fault diagnosis knowledge\ngraph construction under industry 4.0. Advanced Engineering Informatics, 55:101900, 2023. 3\n\nMarthinus Christoffel, Gang Niu, and Masashi Sugiyama. Class-prior estimation for learning from\npositive and unlabeled data. In Asian Conference on Machine Learning, pp. 221\u2013236. PMLR,\n2016. 3\n\nElijah Cole, Oisin Mac Aodha, Titouan Lorieul, Pietro Perona, Dan Morris, and Nebojsa Jojic.\nMulti-label learning from single positive labels. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 933\u2013942, 2021. 1, 3, 8\n\nMichele Coscia and Frank MH Neffke. Network backboning with noisy data. In 2017 IEEE 33rd\n\ninternational conference on data engineering (ICDE), pp. 425\u2013436. IEEE, 2017. 2\n\nThibaut Durand, Nazanin Mehrasa, and Greg Mori. Learning a deep convnet for multi-label classi-\n\ufb01cation with partial labels. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pp. 647\u2013657, 2019. 3\n\nThomas Ef\ufb02and and Michael Collins. Partially supervised named entity recognition via the expected\nentity ratio loss. Transactions of the Association for Computational Linguistics, 9:1320\u20131335,\n2021. doi: 10.1162/tacl a 00429. URL https://aclanthology.org/2021.tacl-1.\n78. 3\n\nJun Feng, Minlie Huang, Li Zhao, Yang Yang, and Xiaoyan Zhu. Reinforcement learning for relation\nclassi\ufb01cation from noisy data. In Proceedings of the aaai conference on arti\ufb01cial intelligence,\nvolume 32, 2018. 2, 3\n\n10\n\n\fUnder review as a conference paper at ICLR 2024\n\nDat Huynh and Ehsan Elhamifar. Interactive multi-label cnn learning with partial labels. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9423\u20139432,\n2020. 8\n\nKristen Jaskie and Andreas Spanias. Positive and unlabeled learning algorithms and applications: A\nsurvey. In 2019 10th International Conference on Information, Intelligence, Systems and Appli-\ncations (IISA), pp. 1\u20138. IEEE, 2019. 2\n\nYoungwook Kim, Jae Myung Kim, Zeynep Akata, and Jungwoo Lee. Large loss matters in weakly\nsupervised multi-label classi\ufb01cation. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 14156\u201314165, 2022. 3\n\nRyuichi Kiryo, Gang Niu, Marthinus C Du Plessis, and Masashi Sugiyama. Positive-unlabeled\nlearning with non-negative risk estimator. Advances in neural information processing systems,\n30, 2017. 3, 7\n\nYangming Li, Shuming Shi, et al. Empirical analysis of unlabeled entity problem in named entity\n\nrecognition. In International Conference on Learning Representations, 2020. 2, 14\n\nYangming Li, lemao liu, and Shuming Shi. Empirical analysis of unlabeled entity problem in\nnamed entity recognition. In International Conference on Learning Representations, 2021. URL\nhttps://openreview.net/forum?id=5jRVa89sZk. 3\n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nIn Computer\nDoll\u00b4ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context.\nVision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Pro-\nceedings, Part V 13, pp. 740\u2013755. Springer, 2014. 8\n\nChuan Luo, Pu Zhao, Chen Chen, Bo Qiao, Chao Du, Hongyu Zhang, Wei Wu, Shaowei Cai, Bing\nHe, Saravanakumar Rajmohan, et al. Pulns: Positive-unlabeled learning with effective negative\nsample selector. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, volume 35, pp.\n8784\u20138792, 2021. 3\n\nYoumi Ma, An Wang, and Naoaki Okazaki. Dreeam: Guiding attention with evidence for improv-\ning document-level relation extraction. In Proceedings of the 17th Conference of the European\nChapter of the Association for Computational Linguistics, pp. 1963\u20131975, 2023. 8\n\nStephen Mayhew, Snigdha Chaturvedi, Chen-Tse Tsai, and Dan Roth. Named entity recognition\nwith partially annotated training data. In Proceedings of the 23rd Conference on Computational\nNatural Language Learning (CoNLL), pp. 645\u2013655, 2019. 3\n\nFarhad Nooralahzadeh, Jan Tore L\u00f8nning, and Lilja \u00d8vrelid. Reinforcement-based denoising of\ndistantly supervised ner with partial annotation. Association for Computational Linguistics, 2019.\n2, 3\n\nMartin L Puterman. Markov decision processes. Handbooks in operations research and management\n\nscience, 2:331\u2013434, 1990. 3\n\nPengda Qin, Weiran Xu, and William Yang Wang. Robust distant supervision relation extraction via\ndeep reinforcement learning. In Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 2137\u20132147, 2018. 3, 5, 6\n\nPengjie Ren, Zhumin Chen, Christof Monz, Jun Ma, and Maarten de Rijke. Thinking globally,\nacting locally: Distantly supervised global-to-local knowledge selection for background based\nconversation. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, volume 34, pp.\n8697\u20138704, 2020. 3\n\nTal Ridnik, Emanuel Ben-Baruch, Nadav Zamir, Asaf Noy, Itamar Friedman, Matan Protter, and Lihi\nZelnik-Manor. Asymmetric loss for multi-label classi\ufb01cation. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp. 82\u201391, 2021. 8\n\n11\n\n\fUnder review as a conference paper at ICLR 2024\n\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,\nJulian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering\nthe game of go with deep neural networks and tree search. nature, 529(7587):484\u2013489, 2016. 2,\n5\n\nGuangxin Su, Weitong Chen, and Miao Xu. Positive-unlabeled learning from imbalanced data. In\n\nIJCAI, pp. 2995\u20133001, 2021. 2, 3, 7\n\nQingyu Tan, Lu Xu, Lidong Bing, Hwee Tou Ng, and Sharifah Mahani Aljunied. Revisiting docred\n\u2013 addressing the false negative problem in relation extraction. 2022. URL https://arxiv.\norg/abs/2205.12696. 1\n\nYingjie Tian, Xiaotong Yu, and Saiji Fu. Partial label learning: Taxonomy, analysis and outlook.\n\nNeural Networks, 2023. 3\n\nHaobo Wang, Ruixuan Xiao, Yixuan Li, Lei Feng, Gang Niu, Gang Chen, and Junbo Zhao. Pico:\nContrastive label disambiguation for partial label learning. In International Conference on Learn-\ning Representations, 2021. 3\n\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\n\nlearning. Machine learning, 8:229\u2013256, 1992. 6\n\nDong-Dong Wu, Deng-Bao Wang, and Min-Ling Zhang. Revisiting consistency regularization for\ndeep partial label learning. In International Conference on Machine Learning, pp. 24212\u201324225.\nPMLR, 2022. 3\n\nNing Xu, Jiaqi Lv, and Xin Geng. Partial label learning via label enhancement. In Proceedings of\n\nthe AAAI Conference on arti\ufb01cial intelligence, volume 33, pp. 5557\u20135564, 2019. 3\n\nFan Yang, Kai Wu, Shuyi Zhang, Guannan Jiang, Yong Liu, Feng Zheng, Wei Zhang, Chengjie\nWang, and Long Zeng. Class-aware contrastive semi-supervised learning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14421\u201314430, 2022. 3\n\nYaosheng Yang, Wenliang Chen, Zhenghua Li, Zhengqiu He, and Min Zhang. Distantly super-\nvised ner with partial annotation learning and reinforcement learning. In Proceedings of the 27th\nInternational Conference on Computational Linguistics, pp. 2159\u20132169, 2018. 2\n\nYuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin Huang,\nJie Zhou, and Maosong Sun. DocRED: A large-scale document-level relation extraction dataset.\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp.\n764\u2013777, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/\nv1/P19-1074. URL https://aclanthology.org/P19-1074. 1\n\nZhixiang Yuan, Kaixin Zhang, and Tao Huang. Positive label is all you need for multi-label classi-\n\n\ufb01cation. arXiv preprint arXiv:2306.16016, 2023. 3\n\nXiangrong Zeng, Shizhu He, Kang Liu, and Jun Zhao. Large scaled relation extraction with rein-\nforcement learning. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, volume 32,\n2018. 3\n\nBangzuo Zhang and Wanli Zuo. Reliable negative extracting based on knn for learning from positive\n\nand unlabeled examples. J. Comput., 4(1):94\u2013101, 2009. 3\n\nMin-Ling Zhang and Fei Yu. Solving the partial label learning problem: An instance-based ap-\n\nproach. In IJCAI, pp. 4048\u20134054, 2015. 3\n\nXin Zhang, Rabab Abdelfattah, Yuqi Song, and Xiaofeng Wang. An effective approach for multi-\nlabel classi\ufb01cation with missing labels. In 2022 IEEE 24th Int Conf on High Performance Com-\nputing & Communications; 8th Int Conf on Data Science & Systems; 20th Int Conf on Smart\nCity; 8th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPC-\nC/DSS/SmartCity/DependSys), pp. 1713\u20131720. IEEE, 2022.\n\n12\n\n\fUnder review as a conference paper at ICLR 2024\n\nMingkai Zheng, Shan You, Lang Huang, Fei Wang, Chen Qian, and Chang Xu. Simmatch: Semi-\nsupervised learning with similarity matching. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 14471\u201314481, 2022. 3\n\nKang Zhou, Yuepei Li, and Qi Li. Distantly supervised named entity recognition via con\ufb01dence-\nbased multi-class positive and unlabeled learning. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), pp. 7198\u20137211, Dublin,\nIreland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.\n498. URL https://aclanthology.org/2022.acl-long.498.

Categories (1–5):
- Soundness: The rigor of the methods for the stated problem
- Presentation: The clarity of writing and organization
- Contribution: The paper’s novelty or added value to the domain

Additionally:
- Rating (1–10): Overall recommendation for acceptance
- Confidence (1–5): How confident the reviewer is in their assessment

Please provide:
1) Soundness
2) Presentation
3) Contribution
4) Rating
5) Confidence
6) Explanation (strengths, weaknesses)
7) Questions

############# RESPONSE: Full text one shot (o1 pro) #############

1) Soundness: 3 (average)
2) Presentation: 3 (average)
3) Contribution: 3 (average)
4) Rating (1–10): 5 (borderline)
5) Confidence (1–5): 3 (moderate)

6) Explanation (strengths, weaknesses)
Strengths:
* The paper addresses an important and under-explored setting of partially annotated multi-label classification (PAMLC), where many existing techniques do not easily apply.
* The idea of leveraging a reinforcement learning (RL) framework with both local and global rewards to balance exploitation (fitting known partial labels) and exploration (identifying potential true positives among unlabeled classes) is interesting and non-trivial.
* The iterative training strategy and pseudo-label enhancement (data augmentation for the value network) show promise, helping to improve coverage of false negatives.
* Experiments span multiple domains (synthetic PU-style image classification, multi-label image classification on MS-COCO, and document-level relation extraction), which strengthens the empirical evaluation.
* The ablation studies are reasonably thorough, examining the impact of local/global rewards, iterative training, and data enhancement.
Weaknesses:
* The approach relies on a careful balance between local and global reward terms (the hyperparameter w) and threshold (ϵ\epsilon) for label enhancement; the paper could be clearer about how w is adapted in practice (e.g., a heuristic or schedule). Sensitivity analyses and deeper theoretical motivations for the chosen reward weighting would be welcome.
* The justification for using recall (rather than F1 or an alternative partial annotation–aware metric) as the primary global reward is intuitive but might overlook scenarios where large numbers of false positives can degrade performance. It would help to show results on real tasks where a partial but substantial negative set is also known.
* While the authors mention that local reward alone can push the policy to a “negative mode” bias, the interaction between local reward (value network) and global reward (recall) is somewhat under-explained. More detailed discussion/visualization of how the policy evolves over training would be beneficial.
* The paper’s writing could be streamlined, particularly the problem statement and motivations for partial annotation in multiple tasks. Some references to related work (like multi-label partial annotation settings outside of RL) could be clarified or compared more directly.
* The method appears computationally heavier than standard partial-label or PU-learning techniques because of repeated RL sampling steps and iterative enhancements; the paper does not provide a thorough complexity analysis or training time comparison.

7) Questions
1. Could the authors elaborate on the practical strategy for updating or scheduling the global reward weight w over training? Are there scenarios where a constant w outperforms a decaying schedule (or vice versa)?
2. How does the method behave when the ratio of annotated positives is extremely small (e.g., <5%)? Have the authors tested such extreme low-resource partial annotation cases?
3. Have the authors considered more sophisticated ways to compute global rewards, for instance weighting recall with a small cost for the number of predicted positives, to mitigate an explosion of false positives?
4. In the DocRE case, it was mentioned that even the fully annotated data can still be incomplete. Is there a way to distinguish or quantify truly missing annotations from known “None” relations more explicitly in the reward design?
Overall, the approach demonstrates a compelling application of reinforcement learning for partial annotation. Addressing the questions above and clarifying certain design choices would further strengthen the work.


{
  "role": "user",
  "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Traditional multi-label classification relies heavily on fully annotated datasets, which are difficult and often impractical to obtain, especially in domains requiring specific expert knowledge. Incomplete annotations are common in multi-label tasks due to the large number of possible labels and complexities involved in providing full annotations.\n\nGap: The problem addressed in this paper is learning from partially annotated multi-label classification datasets where only a subset of positive classes is annotated, leading to issues of scarcity in positive annotations and severe label imbalance.\n\nHypothesis: The authors propose that by integrating reinforcement learning with partial annotation using a Policy Gradient (PAPG) approach, they can effectively overcome the limitations of traditional methods and improve performance in partially annotated multi-label classification by balancing exploration and exploitation.\n\nMethod: The authors introduce PAPG, a framework that combines the exploration strengths of reinforcement learning with the exploitation capabilities of supervised learning. The model is structured as a Markov Decision Process (MDP) with local and global rewards. The local reward comes from a value network trained on annotated data and the global reward is based on recall metrics. The training process involves iterative updates between a policy network and a value network, using data enhancement and an adaptive threshold to adjust training data.\n\nConclusion: The proposed PAPG framework effectively handles the challenges of partially annotated multi-label classification, demonstrating superior performance over traditional methods and baselines. The framework shows robustness and generalizes well across different datasets and domains, substantially improving F1 scores and other metrics compared to existing methods.\n\n--- REQUIRED CATEGORIES & DEFINITIONS ---\n\nWe need the following fields in the output (as **integers** or **strings**), with **no extra keys**:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n### Possible Values for Soundness, Presentation, and Contribution (1–4)\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\n### Possible Values for Rating (1–10)\n[\n  { \"value\": 10, \"description\": \"Top 5% of accepted papers, seminal paper\" },\n  { \"value\": 9,  \"description\": \"Top 15% of accepted papers, strong accept\" },\n  { \"value\": 8,  \"description\": \"Top 50% of accepted papers, clear accept\" },\n  { \"value\": 7,  \"description\": \"Good paper, accept\" },\n  { \"value\": 6,  \"description\": \"Marginally above acceptance threshold\" },\n  { \"value\": 5,  \"description\": \"Marginally below acceptance threshold\" },\n  { \"value\": 4,  \"description\": \"Ok but not good enough - rejection\" },\n  { \"value\": 3,  \"description\": \"Clear rejection\" },\n  { \"value\": 2,  \"description\": \"Strong rejection\" },\n  { \"value\": 1,  \"description\": \"Trivial or wrong\" }\n]\n\n### Possible Values for Confidence (1–5)\n[\n  { \"5\": \"The reviewer is absolutely certain and very familiar with the topic\" },\n  { \"4\": \"The reviewer is confident but not absolutely certain\" },\n  { \"3\": \"The reviewer is fairly confident\" },\n  { \"2\": \"The reviewer is willing to defend the evaluation, but might be wrong\" },\n  { \"1\": \"The reviewer's evaluation is an educated guess\" }\n]\n\n--- OUTPUT FORMAT ---\n\nPlease produce **only** a JSON object with this exact structure:\n\n{\n  \"Soundness\": (integer between 1 and 4),\n  \"Presentation\": (integer between 1 and 4),\n  \"Contribution\": (integer between 1 and 4),\n  \"Rating\": (integer between 1 and 10),\n  \"Confidence\": (integer between 1 and 5),\n  \"Strengths\": \"(string)\",\n  \"Weaknesses\": \"(string)\",\n  \"Questions\": \"(string)\"\n}\n\nNo other fields should be included. Do not add explanations or markdown formatting.\n"
}


