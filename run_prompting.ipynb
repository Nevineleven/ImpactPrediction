{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, ValidationError\n",
    "from typing import List\n",
    "from openai import OpenAI\n",
    "import re\n",
    "# import torch\n",
    "\n",
    "# For Hugging Face usage (e.g., Llama 3)\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 1. Pydantic model for the paper review\n",
    "##################################################\n",
    "\n",
    "class PaperReview(BaseModel):\n",
    "    Soundness: int\n",
    "    Presentation: int\n",
    "    Contribution: int\n",
    "    Rating: int\n",
    "    Confidence: int\n",
    "    Strengths: str\n",
    "    Weaknesses: str\n",
    "    Questions: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2. Helper functions\n",
    "##################################################\n",
    "\n",
    "def load_jsonl(file_path: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Loads a JSONL file and returns a list of dictionaries.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def save_jsonl(data: List[dict], file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves a list of dicts to JSONL, one JSON object per line.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for d in data:\n",
    "            f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def extract_json_string(raw_content: str) -> str:\n",
    "    \"\"\"\n",
    "    Sometimes the model might enclose JSON in triple backticks.\n",
    "    This regex tries to extract the JSON portion if present.\n",
    "    Otherwise returns the full raw_content.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"```(?:json)?(.*?)```\", raw_content, flags=re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return raw_content.strip()\n",
    "\n",
    "def parse_paper_review(response_content: str) -> PaperReview:\n",
    "    \"\"\"\n",
    "    Takes the raw string content from a model's response,\n",
    "    parses it as JSON, and then validates against PaperReview.\n",
    "    Raises exceptions if invalid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed_dict = json.loads(response_content)\n",
    "        return PaperReview(**parsed_dict)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Invalid JSON returned by the model:\\n{e}\\nContent:\\n{response_content}\") from e\n",
    "    except ValidationError as ve:\n",
    "        raise ValueError(f\"Pydantic validation error:\\n{ve}\\nContent:\\n{response_content}\") from ve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def run_gpt4o_inference(messages: List[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Given a list of messages in the OpenAI chat format, \n",
    "    calls the GPT-4 'o' model (whatever alias you have) \n",
    "    and returns the text content of the top choice.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages\n",
    "        )\n",
    "        raw_content = completion.choices[0].message.content\n",
    "        json_str = extract_json_string(raw_content)\n",
    "        review = parse_paper_review(json_str)\n",
    "        print(\"Zero shot review:\\n\", review)\n",
    "        return review\n",
    "    except Exception as e:\n",
    "        print(\"Zero-shot error:\", e)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Llama 3.1 8B usage from Hugging Face\n",
    "# # For a quick pipeline-based approach:\n",
    "# llama_pipeline = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=\"meta-llama/Llama-3.1-8B-Instruct\",  # or your actual Llama 3 model path\n",
    "#     trust_remote_code=True,\n",
    "#     device=0 if torch.cuda.is_available() else -1  # for GPU if available\n",
    "# )\n",
    "\n",
    "# def run_llama3_inference(messages: List[dict]) -> str:\n",
    "#     \"\"\"\n",
    "#     A very naive approach for Llama: we just concatenate the messages \n",
    "#     into a single prompt, because the huggingface pipeline doesn't \n",
    "#     natively handle the 'role-based' chat format by default.\n",
    "\n",
    "#     For a more sophisticated approach, you'd implement the chat logic \n",
    "#     or use a library that supports it. This is just a demonstration.\n",
    "#     \"\"\"\n",
    "#     # Simple approach: system + user => combine them as prompt\n",
    "#     # (If you have multiple system/user pairs, you can decide how to handle them)\n",
    "#     combined_prompt = \"\"\n",
    "#     for m in messages:\n",
    "#         role = m[\"role\"]\n",
    "#         content = m[\"content\"]\n",
    "#         if role == \"system\":\n",
    "#             combined_prompt += f\"[SYSTEM]\\n{content}\\n\"\n",
    "#         elif role == \"user\":\n",
    "#             combined_prompt += f\"[USER]\\n{content}\\n\"\n",
    "#         else:\n",
    "#             combined_prompt += f\"[{role.upper()}]\\n{content}\\n\"\n",
    "\n",
    "#     # Generate with pipeline\n",
    "#     outputs = llama_pipeline(combined_prompt, max_new_tokens=1000)\n",
    "#     # The pipeline returns a list of dicts with \"generated_text\".\n",
    "#     # We'll just return the first\n",
    "#     text_out = outputs[0][\"generated_text\"]\n",
    "#     return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 4. Main Evaluate Function\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def evaluate_model_on_files(\n",
    "    model_name: str,\n",
    "    model_inference_func,\n",
    "    input_files: List[str],\n",
    "    output_dir: str\n",
    "):\n",
    "    \"\"\"\n",
    "    - For each input_file in input_files, load the data\n",
    "    - For each line (prompt), call model_inference_func(messages)\n",
    "    - Parse output as PaperReview\n",
    "    - Save the results in output_dir/<basename>_results.jsonl\n",
    "    \"\"\"\n",
    "\n",
    "    for file_path in input_files:\n",
    "        data = load_jsonl(file_path)\n",
    "        results = []\n",
    "        for entry in data:\n",
    "            paper_id = entry.get(\"paper_id\", \"unknown_id\")\n",
    "            messages = entry.get(\"messages\", [])\n",
    "\n",
    "            # Call the model\n",
    "            try:\n",
    "                review = model_inference_func(messages)\n",
    "                # Attempt to parse PaperReview\n",
    "                # json_str = extract_json_string(raw_content)\n",
    "                # review = parse_paper_review(json_str)\n",
    "\n",
    "                results.append({\n",
    "                    \"paper_id\": paper_id,\n",
    "                    #\"raw_response\": raw_content,\n",
    "                    \"review\": review.model_dump()\n",
    "                })\n",
    "                print(f\"Success: {review}\")\n",
    "            except Exception as e:\n",
    "                # If an error or invalid JSON, store the error\n",
    "                results.append({\n",
    "                    \"paper_id\": paper_id,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "        # Build output path\n",
    "        base_name = os.path.basename(file_path)\n",
    "        out_name = base_name.replace(\".jsonl\", f\"_{model_name}_results.jsonl\")\n",
    "        out_path = os.path.join(output_dir, out_name)\n",
    "        save_jsonl(results, out_path)\n",
    "        print(f\"Finished {model_name} on {file_path}, saved => {out_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running GPT-4o Inference ===\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper bridges the gap between weight sharing in NAS and weight entanglement by proposing a novel method that allows for efficient gradient-based search in weight-entangled spaces. The findings highlight enhanced performance, improved training properties, and superior anytime performance while maintaining memory efficiency.' Weaknesses='The integration of weight-entanglement with gradient-based NAS might not be entirely novel given that some related techniques in NAS have previously explored similar avenues. Additionally, the paper may lack experimental diversity by not examining various architecture spaces or datasets.' Questions='How does the proposed approach perform compared to state-of-the-art non-weight-entangled NAS methods across different domains? Are there specific limitations or datasets where the method does not perform well?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper bridges the gap between weight sharing in NAS and weight entanglement by proposing a novel method that allows for efficient gradient-based search in weight-entangled spaces. The findings highlight enhanced performance, improved training properties, and superior anytime performance while maintaining memory efficiency.' Weaknesses='The integration of weight-entanglement with gradient-based NAS might not be entirely novel given that some related techniques in NAS have previously explored similar avenues. Additionally, the paper may lack experimental diversity by not examining various architecture spaces or datasets.' Questions='How does the proposed approach perform compared to state-of-the-art non-weight-entangled NAS methods across different domains? Are there specific limitations or datasets where the method does not perform well?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a novel technique using contrastive loss functions, demonstrating improved performance in estimating fitness functions where traditional methods like MSE fall short. It successfully leverages a theoretically sound approach with practical validation.' Weaknesses='The application and validation appear somewhat limited, focusing primarily on the proposed benchmarks without extensive exploration across different or diverse datasets. The foundational concepts, while innovative, might require more clear explanations for broader audiences.' Questions='How does the performance of the proposed method vary across different types of biological sequences and fitness landscapes not covered in the benchmarks?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a novel technique using contrastive loss functions, demonstrating improved performance in estimating fitness functions where traditional methods like MSE fall short. It successfully leverages a theoretically sound approach with practical validation.' Weaknesses='The application and validation appear somewhat limited, focusing primarily on the proposed benchmarks without extensive exploration across different or diverse datasets. The foundational concepts, while innovative, might require more clear explanations for broader audiences.' Questions='How does the performance of the proposed method vary across different types of biological sequences and fitness landscapes not covered in the benchmarks?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths='FEATHER introduces a lightweight approach to lifelong test-time adaptation, requiring significantly fewer additional parameters compared to existing methods. It effectively separates source and target domain knowledge, reducing error accumulation.' Weaknesses='The paper claims significant computational and memory savings but lacks detailed empirical analysis to support these claims. The approach primarily targets CNNs, limiting its applicability across diverse model architectures.' Questions='How does FEATHER perform in terms of memory and computation compared to other methods? Can the approach be extended to models other than CNNs, such as Transformers?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths='FEATHER introduces a lightweight approach to lifelong test-time adaptation, requiring significantly fewer additional parameters compared to existing methods. It effectively separates source and target domain knowledge, reducing error accumulation.' Weaknesses='The paper claims significant computational and memory savings but lacks detailed empirical analysis to support these claims. The approach primarily targets CNNs, limiting its applicability across diverse model architectures.' Questions='How does FEATHER perform in terms of memory and computation compared to other methods? Can the approach be extended to models other than CNNs, such as Transformers?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel approach to supervised feature selection using neural networks to estimate mutual information, which can capture sophisticated relationships between features and targets. This departure from traditional methods promises more accurate feature selection.' Weaknesses=\"The paper's performance claims rely primarily on examples rather than comprehensive experimental validation. There is no mention of benchmark comparisons or evaluations on standard datasets. Additionally, the abstract does not clarify the computational feasibility or efficiency of the proposed method, which could be a limitation in practical applications.\" Questions='How does the proposed method compare to state-of-the-art feature selection techniques in terms of computational efficiency and accuracy on standard benchmarks? Can you provide more detailed results from real-world datasets beyond the examples?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel approach to supervised feature selection using neural networks to estimate mutual information, which can capture sophisticated relationships between features and targets. This departure from traditional methods promises more accurate feature selection.' Weaknesses=\"The paper's performance claims rely primarily on examples rather than comprehensive experimental validation. There is no mention of benchmark comparisons or evaluations on standard datasets. Additionally, the abstract does not clarify the computational feasibility or efficiency of the proposed method, which could be a limitation in practical applications.\" Questions='How does the proposed method compare to state-of-the-art feature selection techniques in terms of computational efficiency and accuracy on standard benchmarks? Can you provide more detailed results from real-world datasets beyond the examples?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents a novel perspective in understanding margins in contrastive learning by analyzing gradients. The experimental results showcasing the improved generalization performance are particularly strong, indicating the practical significance of the findings.' Weaknesses='While the new gradient analysis perspective is compelling, the paper might lack clarity in certain sections, particularly in how gradient effects are translated into practical improvements. Additionally, it is unclear how the proposed methods compare with other state-of-the-art approaches not discussed in the paper.' Questions='What are the specific mechanisms by which margins affect gradients, and how do these differ from other methods without margins? Is there a potential impact on the computational efficiency or training time when implementing the proposed emphasis and scaling strategies?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents a novel perspective in understanding margins in contrastive learning by analyzing gradients. The experimental results showcasing the improved generalization performance are particularly strong, indicating the practical significance of the findings.' Weaknesses='While the new gradient analysis perspective is compelling, the paper might lack clarity in certain sections, particularly in how gradient effects are translated into practical improvements. Additionally, it is unclear how the proposed methods compare with other state-of-the-art approaches not discussed in the paper.' Questions='What are the specific mechanisms by which margins affect gradients, and how do these differ from other methods without margins? Is there a potential impact on the computational efficiency or training time when implementing the proposed emphasis and scaling strategies?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel approach to domain adaptation using conditional adversarial support alignment (CASA), which addresses limitations of previous methods under label distribution shift. It proposes a theoretical target risk bound that justifies the effectiveness of their approach, and provides empirical evidence of improved performance over state-of-the-art methods.' Weaknesses=\"The paper could provide more in-depth comparison and analysis of results to better understand the scenarios in which CASA outperforms other methods. The novelty of some components may be incremental, and further clarification on the approach's limitations and assumptions would strengthen the work.\" Questions='How does CASA perform in scenarios with varying degrees of label distribution shift? Can the authors provide more detailed visualizations or case studies of its application?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel approach to domain adaptation using conditional adversarial support alignment (CASA), which addresses limitations of previous methods under label distribution shift. It proposes a theoretical target risk bound that justifies the effectiveness of their approach, and provides empirical evidence of improved performance over state-of-the-art methods.' Weaknesses=\"The paper could provide more in-depth comparison and analysis of results to better understand the scenarios in which CASA outperforms other methods. The novelty of some components may be incremental, and further clarification on the approach's limitations and assumptions would strengthen the work.\" Questions='How does CASA perform in scenarios with varying degrees of label distribution shift? Can the authors provide more detailed visualizations or case studies of its application?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces TRACE, a novel benchmark specifically designed for evaluating continual learning in aligned large language models. TRACE spans multiple challenging tasks in diverse domains such as multilingual capabilities, code generation, and mathematical reasoning, offering a comprehensive tool to assess the continual learning abilities of such models. The unified format for all datasets allows for seamless evaluation. The empirical results presented demonstrate a significant decline in performance, underlining the challenges of achieving a balance between task-specific performance and the retention of initial model abilities.' Weaknesses=\"The impact and novelty of the proposed benchmark are not entirely clear. While TRACE aims to address continual learning, it's uncertain how much more challenging or representative it is compared to existing benchmarks. Additionally, the paper does not provide extensive comparisons with existing continual learning methodologies, which would help in validating its claims about the benchmark's uniqueness and the efficacy of the proposed Reasoning-augmented Continual Learning (RCL) approach.\" Questions='How does TRACE compare to existing continual learning benchmarks in terms of specificity and difficulty? Are there any results comparing TRACE directly with other benchmarks? What other continual learning methods were considered and why were they not included in the comparison?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces TRACE, a novel benchmark specifically designed for evaluating continual learning in aligned large language models. TRACE spans multiple challenging tasks in diverse domains such as multilingual capabilities, code generation, and mathematical reasoning, offering a comprehensive tool to assess the continual learning abilities of such models. The unified format for all datasets allows for seamless evaluation. The empirical results presented demonstrate a significant decline in performance, underlining the challenges of achieving a balance between task-specific performance and the retention of initial model abilities.' Weaknesses=\"The impact and novelty of the proposed benchmark are not entirely clear. While TRACE aims to address continual learning, it's uncertain how much more challenging or representative it is compared to existing benchmarks. Additionally, the paper does not provide extensive comparisons with existing continual learning methodologies, which would help in validating its claims about the benchmark's uniqueness and the efficacy of the proposed Reasoning-augmented Continual Learning (RCL) approach.\" Questions='How does TRACE compare to existing continual learning benchmarks in terms of specificity and difficulty? Are there any results comparing TRACE directly with other benchmarks? What other continual learning methods were considered and why were they not included in the comparison?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper proposes a novel approach to address label noise in graph neural networks, which is a less explored area compared to graph perturbation.\\n2. The use of a probabilistic graphical model framework is a strong methodological contribution and well-justified.\\n3. Extensive experiments demonstrate the robustness of the method under various noise conditions and graph types.' Weaknesses='1. The explanation regarding why uncertainty modeling is necessary is not very clear. The benefits over simpler models are not well articulated.\\n2. It is unclear how the approach specifically avoids the pitfalls of label smoothness assumptions prevalent in other methods.\\n3. The focus is primarily on synthetic noise scenarios; real-world applications and benchmarks are not sufficiently highlighted.' Questions='1. How does the method perform in real-world applications where label noise naturally occurs?\\n2. What are the computational costs associated with the probabilistic graphical model compared to more traditional approaches?\\n3. Can the method be adapted for inductive learning settings?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper proposes a novel approach to address label noise in graph neural networks, which is a less explored area compared to graph perturbation.\\n2. The use of a probabilistic graphical model framework is a strong methodological contribution and well-justified.\\n3. Extensive experiments demonstrate the robustness of the method under various noise conditions and graph types.' Weaknesses='1. The explanation regarding why uncertainty modeling is necessary is not very clear. The benefits over simpler models are not well articulated.\\n2. It is unclear how the approach specifically avoids the pitfalls of label smoothness assumptions prevalent in other methods.\\n3. The focus is primarily on synthetic noise scenarios; real-world applications and benchmarks are not sufficiently highlighted.' Questions='1. How does the method perform in real-world applications where label noise naturally occurs?\\n2. What are the computational costs associated with the probabilistic graphical model compared to more traditional approaches?\\n3. Can the method be adapted for inductive learning settings?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel approach, Utility-based Perturbed Gradient Descent (UPGD), which effectively addresses both catastrophic forgetting and loss of plasticity in continual learning. UPGD is shown to improve performance across a challenging streaming learning setup with strong empirical results. The idea of applying perturbations to the gradient update, which vary based on unit utility, is innovative and well-supported by experiments.' Weaknesses=\"While the approach shows promise, the paper could provide more detailed analysis and justification for the choice of perturbation scales. Additionally, there may be limitations in the diversity of tasks used for evaluation, which could affect the generalizability of the method's effectiveness. More clarity on how UPGD compares with other methods in diverse scenarios would enhance the paper.\" Questions='How does the performance of UPGD vary with different perturbation scales and scheduling? Can the authors provide further insights into how UPGD handles tasks with drastically different characteristics compared to the ones used in the current experiment setup?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel approach, Utility-based Perturbed Gradient Descent (UPGD), which effectively addresses both catastrophic forgetting and loss of plasticity in continual learning. UPGD is shown to improve performance across a challenging streaming learning setup with strong empirical results. The idea of applying perturbations to the gradient update, which vary based on unit utility, is innovative and well-supported by experiments.' Weaknesses=\"While the approach shows promise, the paper could provide more detailed analysis and justification for the choice of perturbation scales. Additionally, there may be limitations in the diversity of tasks used for evaluation, which could affect the generalizability of the method's effectiveness. More clarity on how UPGD compares with other methods in diverse scenarios would enhance the paper.\" Questions='How does the performance of UPGD vary with different perturbation scales and scheduling? Can the authors provide further insights into how UPGD handles tasks with drastically different characteristics compared to the ones used in the current experiment setup?'\n",
      "Zero shot review:\n",
      " Soundness=2 Presentation=2 Contribution=2 Rating=4 Confidence=4 Strengths='The paper highlights a crucial vulnerability in popular language models, potentially impacting their deployment in real-world applications.' Weaknesses=\"The experiment methodology and the broader implications of permutation sensitivity need further explanation. It is not clear how natural variations in prompt ordering affect the models' performance.\" Questions='What datasets and model architectures were tested to arrive at the conclusion regarding permutation sensitivity?'\n",
      "Success: Soundness=2 Presentation=2 Contribution=2 Rating=4 Confidence=4 Strengths='The paper highlights a crucial vulnerability in popular language models, potentially impacting their deployment in real-world applications.' Weaknesses=\"The experiment methodology and the broader implications of permutation sensitivity need further explanation. It is not clear how natural variations in prompt ordering affect the models' performance.\" Questions='What datasets and model architectures were tested to arrive at the conclusion regarding permutation sensitivity?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The introduction of the Super Floating-Point (SuFP) data type and quantization method addresses limitations in existing approaches by improving memory footprint and logic efficiency without sacrificing accuracy. The experimental results show superior performance and efficiency gains, highlighting significant improvements in computational capability and energy efficiency.' Weaknesses='The paper does not delve deeply into the potential trade-offs or limitations of the proposed approach, particularly how it may perform across different real-world scenarios beyond the benchmark tests provided. Additionally, the paper may benefit from further discussion on the complexity of implementing SuFP in existing systems.' Questions='Can the authors provide more details on how SuFP handles edge cases in data distributions? Are there any specific application scenarios where SuFP might not be the best choice?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The introduction of the Super Floating-Point (SuFP) data type and quantization method addresses limitations in existing approaches by improving memory footprint and logic efficiency without sacrificing accuracy. The experimental results show superior performance and efficiency gains, highlighting significant improvements in computational capability and energy efficiency.' Weaknesses='The paper does not delve deeply into the potential trade-offs or limitations of the proposed approach, particularly how it may perform across different real-world scenarios beyond the benchmark tests provided. Additionally, the paper may benefit from further discussion on the complexity of implementing SuFP in existing systems.' Questions='Can the authors provide more details on how SuFP handles edge cases in data distributions? Are there any specific application scenarios where SuFP might not be the best choice?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The introduction of generalized variance reduction to mitigate conflicts between zeroth-order gradients and hard-thresholding shows promise for improved algorithm performance. The approach is demonstrated in practical applications such as portfolio optimization and black-box adversarial attacks.' Weaknesses='The paper may lack clarity in some sections and could benefit from more comprehensive experimental validation. The explanation of how variance reduction specifically mitigates the mentioned conflicts might need elaboration for greater clarity.' Questions='How does the proposed method practically compare with other zeroth-order gradient methods across different scenarios? Could the results on adversarial attacks be extended to larger datasets or varied attack models?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The introduction of generalized variance reduction to mitigate conflicts between zeroth-order gradients and hard-thresholding shows promise for improved algorithm performance. The approach is demonstrated in practical applications such as portfolio optimization and black-box adversarial attacks.' Weaknesses='The paper may lack clarity in some sections and could benefit from more comprehensive experimental validation. The explanation of how variance reduction specifically mitigates the mentioned conflicts might need elaboration for greater clarity.' Questions='How does the proposed method practically compare with other zeroth-order gradient methods across different scenarios? Could the results on adversarial attacks be extended to larger datasets or varied attack models?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces Farzi, an original method for data distillation that shows potential in reducing the size of training datasets significantly while maintaining performance. The approach leverages efficient mathematical optimizations and factorization techniques, showing strong empirical results on both recommendation and language modeling tasks.' Weaknesses='The scalability of the proposed method to larger datasets and models is not extensively explored. Additional clarity on the computational efficiency gains and application scope could strengthen the paper.' Questions='How does Farzi perform when scaled to larger models such as those encountered in industrial applications?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces Farzi, an original method for data distillation that shows potential in reducing the size of training datasets significantly while maintaining performance. The approach leverages efficient mathematical optimizations and factorization techniques, showing strong empirical results on both recommendation and language modeling tasks.' Weaknesses='The scalability of the proposed method to larger datasets and models is not extensively explored. Additional clarity on the computational efficiency gains and application scope could strengthen the paper.' Questions='How does Farzi perform when scaled to larger models such as those encountered in industrial applications?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces an oracle-efficient algorithm (PNLSVI) for offline RL with non-linear function approximation, offering innovative components such as a variance-based weighted regression and a pessimistic value iteration approach. It extends results from linear and differentiable functions to a general framework, claiming to be the first statistically optimal algorithm for nonlinear offline RL. This is a significant advancement in the field.' Weaknesses='The paper lacks numerical experiments to validate the theoretical contributions, which could strengthen the practical applicability of the proposed algorithm. Additionally, there are strong assumptions (e.g., uniform data coverage) that may not hold in real-world scenarios. The need for an accompanying lower bound for non-linear cases to demonstrate optimality fully is also a potential shortcoming.' Questions=\"How does the algorithm perform in practical scenarios, and can the assumptions like uniform data coverage be relaxed or justified with concrete examples? Are there specific non-linear function classes or instances where the algorithm's benefits are particularly pronounced?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces an oracle-efficient algorithm (PNLSVI) for offline RL with non-linear function approximation, offering innovative components such as a variance-based weighted regression and a pessimistic value iteration approach. It extends results from linear and differentiable functions to a general framework, claiming to be the first statistically optimal algorithm for nonlinear offline RL. This is a significant advancement in the field.' Weaknesses='The paper lacks numerical experiments to validate the theoretical contributions, which could strengthen the practical applicability of the proposed algorithm. Additionally, there are strong assumptions (e.g., uniform data coverage) that may not hold in real-world scenarios. The need for an accompanying lower bound for non-linear cases to demonstrate optimality fully is also a potential shortcoming.' Questions=\"How does the algorithm perform in practical scenarios, and can the assumptions like uniform data coverage be relaxed or justified with concrete examples? Are there specific non-linear function classes or instances where the algorithm's benefits are particularly pronounced?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The study provides insights into LLM behavior in social settings using game theory, offering a novel approach to understanding AI interactions. The presentation of results is clear and detailed.' Weaknesses='The paper might lack sufficient depth in comparing alternative strategies or models. The focus seems limited to specific games, which may not fully represent the wider scope of LLM capabilities. Clarification on methodological approaches could be improved.' Questions=\"How generalizable are the findings to other models or more complex social scenarios? What are the implications of LLMs' behavior for practical applications in human-LLM interactions?\"\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The study provides insights into LLM behavior in social settings using game theory, offering a novel approach to understanding AI interactions. The presentation of results is clear and detailed.' Weaknesses='The paper might lack sufficient depth in comparing alternative strategies or models. The focus seems limited to specific games, which may not fully represent the wider scope of LLM capabilities. Clarification on methodological approaches could be improved.' Questions=\"How generalizable are the findings to other models or more complex social scenarios? What are the implications of LLMs' behavior for practical applications in human-LLM interactions?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses an important issue in structure learning by examining the impact of variable scale on the accuracy of DAG models. The theoretical findings are supported by empirical evidence from both synthetic and real-world datasets.' Weaknesses='The scope of the findings might be limited by the focus on scale and square-based losses. The contribution, while valuable, may not represent a significant breakthrough in the field. The novelty might be limited unless clear connections to broader applications are made.' Questions='How do the proposed conditions for minimal square-based losses in $d$-dimensional cases generalize beyond the initial assumptions? Are there practical guidelines for mitigating the impact of scale in real-world applications?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses an important issue in structure learning by examining the impact of variable scale on the accuracy of DAG models. The theoretical findings are supported by empirical evidence from both synthetic and real-world datasets.' Weaknesses='The scope of the findings might be limited by the focus on scale and square-based losses. The contribution, while valuable, may not represent a significant breakthrough in the field. The novelty might be limited unless clear connections to broader applications are made.' Questions='How do the proposed conditions for minimal square-based losses in $d$-dimensional cases generalize beyond the initial assumptions? Are there practical guidelines for mitigating the impact of scale in real-world applications?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel unsupervised clustering algorithm emphasizing computational efficiency and performance compared to state-of-the-art methods. It successfully determines the optimal number of clusters autonomously.' Weaknesses=\"The novelty of the algorithm might be limited due to the use of existing techniques. It's unclear how the proposed method compares with a more extensive set of modern clustering techniques beyond K-Means and DBSCAN. More experimental results and comparisons could strengthen the claims.\" Questions=\"How does the proposed method perform relative to other modern clustering algorithms not included in the experiments? What is the impact of parameter choices on the algorithm's performance?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel unsupervised clustering algorithm emphasizing computational efficiency and performance compared to state-of-the-art methods. It successfully determines the optimal number of clusters autonomously.' Weaknesses=\"The novelty of the algorithm might be limited due to the use of existing techniques. It's unclear how the proposed method compares with a more extensive set of modern clustering techniques beyond K-Means and DBSCAN. More experimental results and comparisons could strengthen the claims.\" Questions=\"How does the proposed method perform relative to other modern clustering algorithms not included in the experiments? What is the impact of parameter choices on the algorithm's performance?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel framework that effectively enhances the reliability of LLMs by using task-specific fine-tuned models during inference. It provides comprehensive resources, including datasets, prompts, and model outputs, and demonstrates improved performance on generalizability and factuality tasks.' Weaknesses=\"The method's novelty may be limited due to its resemblance to existing techniques. Additionally, the paper does not thoroughly explore the role of confidence or provide a detailed ablation study on this aspect. It could benefit from more direct comparisons with similar methods to highlight unique contributions more effectively.\" Questions=\"How does the proposed method directly compare to existing frameworks like Hugging Face's approach? Can you provide further insights into the significance of confidence in the framework's performance?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel framework that effectively enhances the reliability of LLMs by using task-specific fine-tuned models during inference. It provides comprehensive resources, including datasets, prompts, and model outputs, and demonstrates improved performance on generalizability and factuality tasks.' Weaknesses=\"The method's novelty may be limited due to its resemblance to existing techniques. Additionally, the paper does not thoroughly explore the role of confidence or provide a detailed ablation study on this aspect. It could benefit from more direct comparisons with similar methods to highlight unique contributions more effectively.\" Questions=\"How does the proposed method directly compare to existing frameworks like Hugging Face's approach? Can you provide further insights into the significance of confidence in the framework's performance?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper introduces the concept of Unknown Domain Inconsistency Minimization (UDIM), which is a novel approach that combines both parameter and data perturbation. This approach addresses a key issue in domain generalization and successfully outperforms existing SAM variants. The theoretical validation showing that merging SAM with UDIM creates an upper bound for domain generalization tasks adds strength to the paper's claims.\" Weaknesses='While UDIM shows empirical success, there may be concerns regarding the practicality and computational cost of generating perturbed domains and aligning loss landscapes, particularly as this can be computation-heavy. There might also be concerns about how well these simulated domains represent real unknown domains that the model might encounter.' Questions='How computationally intensive is the proposed UDIM method, and what impact does it have on training time? How well do the empirically crafted unknown domains generalize to real-world scenarios?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper introduces the concept of Unknown Domain Inconsistency Minimization (UDIM), which is a novel approach that combines both parameter and data perturbation. This approach addresses a key issue in domain generalization and successfully outperforms existing SAM variants. The theoretical validation showing that merging SAM with UDIM creates an upper bound for domain generalization tasks adds strength to the paper's claims.\" Weaknesses='While UDIM shows empirical success, there may be concerns regarding the practicality and computational cost of generating perturbed domains and aligning loss landscapes, particularly as this can be computation-heavy. There might also be concerns about how well these simulated domains represent real unknown domains that the model might encounter.' Questions='How computationally intensive is the proposed UDIM method, and what impact does it have on training time? How well do the empirically crafted unknown domains generalize to real-world scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces an innovative approach by combining competition and collaboration in GANs, which could potentially address some existing limitations in generative models. The theoretical backing and experimental validation across four datasets are strengths of the work.' Weaknesses='The presentation could be clearer, especially in relating the theoretical explanations to practical outcomes. Some aspects of the argumentation are complex, possibly making them less accessible.' Questions='How does the introduced CCGAN compare in terms of computational efficiency with existing GAN models? Are there specific scenarios where CCGAN performs less effectively?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces an innovative approach by combining competition and collaboration in GANs, which could potentially address some existing limitations in generative models. The theoretical backing and experimental validation across four datasets are strengths of the work.' Weaknesses='The presentation could be clearer, especially in relating the theoretical explanations to practical outcomes. Some aspects of the argumentation are complex, possibly making them less accessible.' Questions='How does the introduced CCGAN compare in terms of computational efficiency with existing GAN models? Are there specific scenarios where CCGAN performs less effectively?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=6 Confidence=3 Strengths='The paper introduces a novel reasoning mechanism combining iterative and parallel computation, which is beneficial for dynamic multi-step reasoning and concurrent execution of independent operations. The approach exhibits stronger reasoning capabilities and generalization with fewer parameters and computation steps, achieving state-of-the-art zero-shot performance on CLEVR-Humans.' Weaknesses=\"The technical details of IPRM, especially regarding the implementation of parallel reasoning and operation composition, are not sufficiently clear. It is difficult to discern how the approach distinctly facilitates parallel operations as claimed. The paper also lacks detailed examples or visualizations to illustrate how IPRM's reasoning process unfolds.\" Questions='How does the Operation Composition stage enforce parallel processing? Can more examples or visualizations be provided to illustrate the reasoning steps and usage of parallel computing?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=6 Confidence=3 Strengths='The paper introduces a novel reasoning mechanism combining iterative and parallel computation, which is beneficial for dynamic multi-step reasoning and concurrent execution of independent operations. The approach exhibits stronger reasoning capabilities and generalization with fewer parameters and computation steps, achieving state-of-the-art zero-shot performance on CLEVR-Humans.' Weaknesses=\"The technical details of IPRM, especially regarding the implementation of parallel reasoning and operation composition, are not sufficiently clear. It is difficult to discern how the approach distinctly facilitates parallel operations as claimed. The paper also lacks detailed examples or visualizations to illustrate how IPRM's reasoning process unfolds.\" Questions='How does the Operation Composition stage enforce parallel processing? Can more examples or visualizations be provided to illustrate the reasoning steps and usage of parallel computing?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel approach to epidemic time-series forecasting by leveraging pre-trained models, which is a significant innovation in the context of public health data analysis. The method shows improved performance over state-of-the-art models in various tasks and contexts, which demonstrates its practical relevance and potential impact.' Weaknesses='The paper may lack sufficient comparison with existing approaches in terms of computational efficiency and resource requirements during pre-training and fine-tuning. There is also some ambiguity regarding the handling of diverse datasets and the specific implementation details of self-supervised tasks.' Questions='How does the model ensure fair performance improvement over traditional data-driven models, specifically in sparse data scenarios? What specific self-supervised tasks are leveraged, and how were they tailored to capture epidemic dynamics across different diseases?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel approach to epidemic time-series forecasting by leveraging pre-trained models, which is a significant innovation in the context of public health data analysis. The method shows improved performance over state-of-the-art models in various tasks and contexts, which demonstrates its practical relevance and potential impact.' Weaknesses='The paper may lack sufficient comparison with existing approaches in terms of computational efficiency and resource requirements during pre-training and fine-tuning. There is also some ambiguity regarding the handling of diverse datasets and the specific implementation details of self-supervised tasks.' Questions='How does the model ensure fair performance improvement over traditional data-driven models, specifically in sparse data scenarios? What specific self-supervised tasks are leveraged, and how were they tailored to capture epidemic dynamics across different diseases?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses an important issue related to debiasing and dataset bias in machine learning. It proposes a scalable and efficient method that utilizes class-specific scaling to improve robust accuracy without sacrificing average accuracy significantly.' Weaknesses='The paper could provide more theoretical justification or insights into why the proposed method works well. The reliance on empirical results without deeper insights into the mechanism or theoretical underpinnings might limit the perceived depth of the contribution.' Questions='How does the approach perform under different levels of data imbalance or with different debiasing algorithms? What are the specific limitations of the class-specific scaling strategy?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses an important issue related to debiasing and dataset bias in machine learning. It proposes a scalable and efficient method that utilizes class-specific scaling to improve robust accuracy without sacrificing average accuracy significantly.' Weaknesses='The paper could provide more theoretical justification or insights into why the proposed method works well. The reliance on empirical results without deeper insights into the mechanism or theoretical underpinnings might limit the perceived depth of the contribution.' Questions='How does the approach perform under different levels of data imbalance or with different debiasing algorithms? What are the specific limitations of the class-specific scaling strategy?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel approach to disentangled representation learning for time series, which can enhance interpretability and generalization in the context of home appliance electricity usage. The introduction of the Time Disentangling Score (TDS) is a significant contribution as it provides a way to gauge disentanglement quality.' Weaknesses=\"The reliance on smooth bijections and contrastive learning may have limitations in terms of complexity and interpretability. It's not entirely clear how well the method generalizes to a variety of time series data beyond the specific application demonstrated. More experiments on diverse datasets would strengthen the validity of the approach.\" Questions='How does the method perform on other types of time series data apart from home appliance electricity usage? Could the approach be generalized to non-energy domains?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel approach to disentangled representation learning for time series, which can enhance interpretability and generalization in the context of home appliance electricity usage. The introduction of the Time Disentangling Score (TDS) is a significant contribution as it provides a way to gauge disentanglement quality.' Weaknesses=\"The reliance on smooth bijections and contrastive learning may have limitations in terms of complexity and interpretability. It's not entirely clear how well the method generalizes to a variety of time series data beyond the specific application demonstrated. More experiments on diverse datasets would strengthen the validity of the approach.\" Questions='How does the method perform on other types of time series data apart from home appliance electricity usage? Could the approach be generalized to non-energy domains?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a new series of models that successfully enhance vision-language integration, setting new benchmarks and demonstrating superior performance on real-world dialogue tasks. The release of the models for public use encourages further research and development in the field.' Weaknesses='While the models perform well, the novelty in the architectural design and training method seems limited, as similar multi-stage and multi-resolution strategies have been used in prior work. The ablation studies and some claims in the related work section require more detailed explanations and stronger justification.' Questions='What specific innovations in the training pipeline or model architecture distinguish Qwen-VL models from existing vision-language models? How does the alignment of image-caption-box tuples differ from previous methods, and what impact does it have on model performance?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a new series of models that successfully enhance vision-language integration, setting new benchmarks and demonstrating superior performance on real-world dialogue tasks. The release of the models for public use encourages further research and development in the field.' Weaknesses='While the models perform well, the novelty in the architectural design and training method seems limited, as similar multi-stage and multi-resolution strategies have been used in prior work. The ablation studies and some claims in the related work section require more detailed explanations and stronger justification.' Questions='What specific innovations in the training pipeline or model architecture distinguish Qwen-VL models from existing vision-language models? How does the alignment of image-caption-box tuples differ from previous methods, and what impact does it have on model performance?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper introduces a novel concept of 'behaviour distillation' for reinforcement learning, addressing a gap where traditional dataset distillation falls short due to the lack of fixed datasets in RL. The proposed method, HaDES, demonstrates the potential for synthesizing minimal datasets that achieve competitive performance. The method shows versatility by generalizing across different architectures and hyperparameters and applying to multi-task agents in a zero-shot fashion. Visualization of synthetic datasets provides insights into tasks, offering an interpretability advantage. The paper is well-written and the methods are clearly explained.\" Weaknesses=\"The motivation for introducing 'behaviour distillation' over traditional expert dataset access in RL is not clearly justified. The potential benefits or specific problems addressed by distilling without expert data remain unclear, which could affect the perceived impact and applicability of the approach. Details on optimizations used in HaDES could be more thoroughly discussed, as well as comparisons with other existing techniques for similar tasks. Additional insights into the performance differences between standard RL and data-driven RL approaches would strengthen the paper.\" Questions='What is the specific motivation for behavior distillation when traditional methods could utilize expert datasets directly? How does the performance of HaDES compare in situations where expert data is available? Can the authors provide more details on the optimization techniques used in HaDES and how these compare to existing work, such as BPTT?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper introduces a novel concept of 'behaviour distillation' for reinforcement learning, addressing a gap where traditional dataset distillation falls short due to the lack of fixed datasets in RL. The proposed method, HaDES, demonstrates the potential for synthesizing minimal datasets that achieve competitive performance. The method shows versatility by generalizing across different architectures and hyperparameters and applying to multi-task agents in a zero-shot fashion. Visualization of synthetic datasets provides insights into tasks, offering an interpretability advantage. The paper is well-written and the methods are clearly explained.\" Weaknesses=\"The motivation for introducing 'behaviour distillation' over traditional expert dataset access in RL is not clearly justified. The potential benefits or specific problems addressed by distilling without expert data remain unclear, which could affect the perceived impact and applicability of the approach. Details on optimizations used in HaDES could be more thoroughly discussed, as well as comparisons with other existing techniques for similar tasks. Additional insights into the performance differences between standard RL and data-driven RL approaches would strengthen the paper.\" Questions='What is the specific motivation for behavior distillation when traditional methods could utilize expert datasets directly? How does the performance of HaDES compare in situations where expert data is available? Can the authors provide more details on the optimization techniques used in HaDES and how these compare to existing work, such as BPTT?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel concept of Extended Textual Conditioning space, P+, which enhances control and expressiveness in text-to-image models. The method achieves significant improvements in reconstruction and editability over existing baselines without balancing these goals. The paper is well-written, and the experiments conducted are thorough, showcasing the effectiveness and unique properties of the proposed approach.' Weaknesses=\"The specific implementation details of the Extended Textual Inversion (XTI) and how it functions in practice are somewhat vague. While the empirical results are impressive, the paper lacks a theoretical understanding of why the proposed method outperforms existing solutions. The distinctiveness of 'per-layer prompting' requires further clarification to fully appreciate its contribution to the field.\" Questions='How does the method balance the complexity of detailed per-layer prompting with computational efficiency? Could the authors provide more theoretical insights or analyses into why the P+ space is more effective for inversion and editability compared to existing methods? Are there any limitations or edge cases where the proposed method does not perform well?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel concept of Extended Textual Conditioning space, P+, which enhances control and expressiveness in text-to-image models. The method achieves significant improvements in reconstruction and editability over existing baselines without balancing these goals. The paper is well-written, and the experiments conducted are thorough, showcasing the effectiveness and unique properties of the proposed approach.' Weaknesses=\"The specific implementation details of the Extended Textual Inversion (XTI) and how it functions in practice are somewhat vague. While the empirical results are impressive, the paper lacks a theoretical understanding of why the proposed method outperforms existing solutions. The distinctiveness of 'per-layer prompting' requires further clarification to fully appreciate its contribution to the field.\" Questions='How does the method balance the complexity of detailed per-layer prompting with computational efficiency? Could the authors provide more theoretical insights or analyses into why the P+ space is more effective for inversion and editability compared to existing methods? Are there any limitations or edge cases where the proposed method does not perform well?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='This paper provides a novel approach to the challenging task of Source-Free Unsupervised Domain Adaptation (SFUDA) by leveraging multiple prediction hypotheses and their rationales. The proposed three-step adaptation process, including model pre-adaptation, hypothesis consolidation, and semi-supervised learning, has shown state-of-the-art performance. It can also be integrated into existing approaches.' Weaknesses='The motivation behind using multiple hypotheses is unclear, especially when all are pre-trained on the same domain. The utility and accessibility of these multiple high-quality hypotheses in practical scenarios are questionable. The setting might be computationally expensive. The explanation for the chosen number of optimal hypotheses and related parameters seems dataset-specific and may not generalize well. Some datasets show non-state-of-the-art performance.' Questions='How does the method handle the selection of the optimal number of hypotheses k and values for τ1 and τ2 for various tasks beyond the experimental dataset? Additionally, why does increasing the number of hypotheses potentially degrade performance instead of improving it?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='This paper provides a novel approach to the challenging task of Source-Free Unsupervised Domain Adaptation (SFUDA) by leveraging multiple prediction hypotheses and their rationales. The proposed three-step adaptation process, including model pre-adaptation, hypothesis consolidation, and semi-supervised learning, has shown state-of-the-art performance. It can also be integrated into existing approaches.' Weaknesses='The motivation behind using multiple hypotheses is unclear, especially when all are pre-trained on the same domain. The utility and accessibility of these multiple high-quality hypotheses in practical scenarios are questionable. The setting might be computationally expensive. The explanation for the chosen number of optimal hypotheses and related parameters seems dataset-specific and may not generalize well. Some datasets show non-state-of-the-art performance.' Questions='How does the method handle the selection of the optimal number of hypotheses k and values for τ1 and τ2 for various tasks beyond the experimental dataset? Additionally, why does increasing the number of hypotheses potentially degrade performance instead of improving it?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses a novel aspect of structured output model interpretation, offering promising insights into feature importance and correlations. The energy-based training process for the interpreter function is a strong innovative aspect.' Weaknesses=\"The paper's approach may be limited in the context of images where region-based explanations might be more appropriate than independently assessed features. The learning algorithm's robustness concerning hyperparameters is not well documented, raising concerns about its stability and utility.\" Questions='How does the learning algorithm handle variations in hyperparameters such as tau and k? Are there any considerations for feature correlation within the interpreter, and has robustness against noise been assessed?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses a novel aspect of structured output model interpretation, offering promising insights into feature importance and correlations. The energy-based training process for the interpreter function is a strong innovative aspect.' Weaknesses=\"The paper's approach may be limited in the context of images where region-based explanations might be more appropriate than independently assessed features. The learning algorithm's robustness concerning hyperparameters is not well documented, raising concerns about its stability and utility.\" Questions='How does the learning algorithm handle variations in hyperparameters such as tau and k? Are there any considerations for feature correlation within the interpreter, and has robustness against noise been assessed?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper provides a valuable perspective on the potential over-engineering of GNNs and suggests that simpler spectral methods might achieve comparable performance. It addresses an important issue by examining the role of evaluation conventions in new GNN designs' perceived performance improvements.\" Weaknesses='The paper may not provide a comprehensive theoretical foundation or sufficient empirical evidence to conclusively support its claims. Additionally, the scope of the ablative study and the range of graph types evaluated might be limited.' Questions='What are the specific graph types evaluated in the study? How do the authors ensure that the proposed simpler methods generalize well across various graph datasets?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper provides a valuable perspective on the potential over-engineering of GNNs and suggests that simpler spectral methods might achieve comparable performance. It addresses an important issue by examining the role of evaluation conventions in new GNN designs' perceived performance improvements.\" Weaknesses='The paper may not provide a comprehensive theoretical foundation or sufficient empirical evidence to conclusively support its claims. Additionally, the scope of the ablative study and the range of graph types evaluated might be limited.' Questions='What are the specific graph types evaluated in the study? How do the authors ensure that the proposed simpler methods generalize well across various graph datasets?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper provides a well-motivated solution to the problem of gradient divergence in Robust Fine-Tuning (RFT) by introducing a low-rank (LoRa) branch. The proposed method, AutoLoRa, automates the conversion of pre-trained feature extractors into adversarially robust models, offering significant practical utility without extensive hyperparameter tuning.' Weaknesses='The paper does not sufficiently explain the choice of the constant factor in the proposed method and lacks diversity in the network architectures evaluated. Some parts of the presentation, such as Table 4, are confusing and contain typographical errors. More detailed ablation studies, particularly on the automated hyperparameter scheduling, would strengthen the contribution.' Questions='How robust is AutoLoRa across different types of network architectures beyond ResNet? Can the constant factor used in the formula be adjusted for potentially better performance, and if so, what would be the impact?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper provides a well-motivated solution to the problem of gradient divergence in Robust Fine-Tuning (RFT) by introducing a low-rank (LoRa) branch. The proposed method, AutoLoRa, automates the conversion of pre-trained feature extractors into adversarially robust models, offering significant practical utility without extensive hyperparameter tuning.' Weaknesses='The paper does not sufficiently explain the choice of the constant factor in the proposed method and lacks diversity in the network architectures evaluated. Some parts of the presentation, such as Table 4, are confusing and contain typographical errors. More detailed ablation studies, particularly on the automated hyperparameter scheduling, would strengthen the contribution.' Questions='How robust is AutoLoRa across different types of network architectures beyond ResNet? Can the constant factor used in the formula be adjusted for potentially better performance, and if so, what would be the impact?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces an innovative framework (AdaGLT) for Graph Neural Networks that addresses existing scalability and elasticity issues. The integration of pruning and restoration, dynamic sparsity adaptation, and automatic tuning of parameters are significant advancements over traditional methods. Rigorous theoretical proofs and extensive experiments enhance the credibility of the proposed method.' Weaknesses='The paper may still be considered incremental as it builds upon existing graph lottery ticket frameworks. The depth and breadth of experimentation, although extensive, may not conclusively cover all potential real-world applications or sufficiently large graphs.' Questions='How does AdaGLT perform on extremely large graphs and in real-time scenarios? Is the framework applicable to other forms of neural networks beyond GNNs?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces an innovative framework (AdaGLT) for Graph Neural Networks that addresses existing scalability and elasticity issues. The integration of pruning and restoration, dynamic sparsity adaptation, and automatic tuning of parameters are significant advancements over traditional methods. Rigorous theoretical proofs and extensive experiments enhance the credibility of the proposed method.' Weaknesses='The paper may still be considered incremental as it builds upon existing graph lottery ticket frameworks. The depth and breadth of experimentation, although extensive, may not conclusively cover all potential real-world applications or sufficiently large graphs.' Questions='How does AdaGLT perform on extremely large graphs and in real-time scenarios? Is the framework applicable to other forms of neural networks beyond GNNs?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths='Stable Diffusion XL demonstrates significant improvements in image synthesis quality with novel conditioning schemes and larger model architecture. The refinement model further enhances the output quality, achieving competitive results with state-of-the-art black-box models.' Weaknesses='The abstract does not provide specific details about the conditioning schemes or the refinement process, leaving questions about the generalizability and underlying innovations of these contributions.' Questions=\"How do the novel conditioning schemes impact the model's ability to handle diverse text prompts compared to previous versions? What are the specific innovations in the refinement model that contribute to improved visual fidelity?\"\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths='Stable Diffusion XL demonstrates significant improvements in image synthesis quality with novel conditioning schemes and larger model architecture. The refinement model further enhances the output quality, achieving competitive results with state-of-the-art black-box models.' Weaknesses='The abstract does not provide specific details about the conditioning schemes or the refinement process, leaving questions about the generalizability and underlying innovations of these contributions.' Questions=\"How do the novel conditioning schemes impact the model's ability to handle diverse text prompts compared to previous versions? What are the specific innovations in the refinement model that contribute to improved visual fidelity?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"1. Introduction of a new concept 'Activation Prompt' (AP) that expands the capabilities of Visual Prompting by applying universal perturbations to activation maps, rather than just input data. 2. Provides a thorough experimental analysis comparing AP against conventional VP and other PEFT techniques across multiple datasets and models. 3. Offers theoretical insights into the limitations of VP and the advantages of AP, enhancing understanding of these techniques.\" Weaknesses='1. Although the paper proposes a novel concept, the improvement of AP over existing techniques like VP may not justify the claims of superior performance without further evidence or examples. 2. The theoretical analysis, while interesting, may be too complex or abstract for practical application without additional context or clarification.' Questions='1. How does the implementation of AP affect the computational efficiency in real-world applications, beyond theoretical considerations? 2. Are there specific scenarios or tasks where AP significantly outperforms VP, and could these be highlighted more clearly in the paper?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"1. Introduction of a new concept 'Activation Prompt' (AP) that expands the capabilities of Visual Prompting by applying universal perturbations to activation maps, rather than just input data. 2. Provides a thorough experimental analysis comparing AP against conventional VP and other PEFT techniques across multiple datasets and models. 3. Offers theoretical insights into the limitations of VP and the advantages of AP, enhancing understanding of these techniques.\" Weaknesses='1. Although the paper proposes a novel concept, the improvement of AP over existing techniques like VP may not justify the claims of superior performance without further evidence or examples. 2. The theoretical analysis, while interesting, may be too complex or abstract for practical application without additional context or clarification.' Questions='1. How does the implementation of AP affect the computational efficiency in real-world applications, beyond theoretical considerations? 2. Are there specific scenarios or tasks where AP significantly outperforms VP, and could these be highlighted more clearly in the paper?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper addresses a crucial problem in machine learning: enabling rapid and continual learning similar to biological systems. The integration of biologically-inspired memory architectures with modern neural network approaches is innovative. The proposed model shows enhanced performance compared to baseline models, indicating the potential of the approach.' Weaknesses='The contribution, while innovative, may be specific to the domain of navigation tasks and not necessarily generalizable to other types of machine learning problems. Further empirical validation across diverse tasks would strengthen the claims. The comparison with existing models could be more comprehensive.' Questions='How does the proposed model perform on tasks outside the navigation domain? Are there plans to extend the benchmark to other types of tasks? What are the computational demands of the proposed memory policy compared to simpler baselines?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper addresses a crucial problem in machine learning: enabling rapid and continual learning similar to biological systems. The integration of biologically-inspired memory architectures with modern neural network approaches is innovative. The proposed model shows enhanced performance compared to baseline models, indicating the potential of the approach.' Weaknesses='The contribution, while innovative, may be specific to the domain of navigation tasks and not necessarily generalizable to other types of machine learning problems. Further empirical validation across diverse tasks would strengthen the claims. The comparison with existing models could be more comprehensive.' Questions='How does the proposed model perform on tasks outside the navigation domain? Are there plans to extend the benchmark to other types of tasks? What are the computational demands of the proposed memory policy compared to simpler baselines?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper introduces a novel generalized statistical framework, ImOOD, to tackle the problem of OOD detection on imbalanced datasets. The proposed method shows significant improvements over state-of-the-art approaches on several benchmarks.' Weaknesses='The paper could benefit from more clarity in explaining the theoretical framework and the methodology employed. Additionally, the scope of the study might be limited due to its focus on specific datasets and scenarios that may not fully represent all real-world situations.' Questions='How does the proposed ImOOD framework perform in scenarios where both in-distribution and out-of-distribution datasets are imbalanced? Are there plans to expand the testing to more diverse datasets beyond CIFAR and ImageNet?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper introduces a novel generalized statistical framework, ImOOD, to tackle the problem of OOD detection on imbalanced datasets. The proposed method shows significant improvements over state-of-the-art approaches on several benchmarks.' Weaknesses='The paper could benefit from more clarity in explaining the theoretical framework and the methodology employed. Additionally, the scope of the study might be limited due to its focus on specific datasets and scenarios that may not fully represent all real-world situations.' Questions='How does the proposed ImOOD framework perform in scenarios where both in-distribution and out-of-distribution datasets are imbalanced? Are there plans to expand the testing to more diverse datasets beyond CIFAR and ImageNet?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The proposed adversarial example detection strategy is straightforward yet effective, making it applicable to various ASR systems.\\n2. Includes a comprehensive evaluation involving different state-of-the-art ASR systems and language datasets.\\n3. The method shows high performance in differentiating adversarial examples, with over 99% AUROC against clean data.' Weaknesses='1. The paper might not sufficiently cover the performance of the method against a broader range of sophisticated or adaptive adversarial attacks.\\n2. There is a lack of direct comparison with existing adversarial detection methods in the literature, which would strengthen the evaluation.\\n3. While robustness to adaptive attacks is mentioned, more details and extensive experimentation in this area could be beneficial.' Questions='1. How does the method perform across different types of adversarial examples beyond those tested, particularly newer or more adaptive attacks?\\n2. Could you provide more details on the computational overhead introduced by the detection mechanism?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The proposed adversarial example detection strategy is straightforward yet effective, making it applicable to various ASR systems.\\n2. Includes a comprehensive evaluation involving different state-of-the-art ASR systems and language datasets.\\n3. The method shows high performance in differentiating adversarial examples, with over 99% AUROC against clean data.' Weaknesses='1. The paper might not sufficiently cover the performance of the method against a broader range of sophisticated or adaptive adversarial attacks.\\n2. There is a lack of direct comparison with existing adversarial detection methods in the literature, which would strengthen the evaluation.\\n3. While robustness to adaptive attacks is mentioned, more details and extensive experimentation in this area could be beneficial.' Questions='1. How does the method perform across different types of adversarial examples beyond those tested, particularly newer or more adaptive attacks?\\n2. Could you provide more details on the computational overhead introduced by the detection mechanism?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel flow-based model for transporting between distributions that are only accessible via finite samples, which can be a practical extension to existing models. The approach has potential applications in various domains such as mutual information estimation and image translation, which is empirically evaluated.' Weaknesses='The paper might not have made clear connections between the proposed model and existing methods or adequately discussed the limitations. Additionally, the presentation might need improvements in terms of clarity and structure. There could be a lack of theoretical exploration or proofs regarding the proposed model’s soundness and effectiveness.' Questions='How does the proposed model compare with existing state-of-the-art methods for similar tasks? Would there be a theoretical justification of the transportation cost minimization approach employed?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel flow-based model for transporting between distributions that are only accessible via finite samples, which can be a practical extension to existing models. The approach has potential applications in various domains such as mutual information estimation and image translation, which is empirically evaluated.' Weaknesses='The paper might not have made clear connections between the proposed model and existing methods or adequately discussed the limitations. Additionally, the presentation might need improvements in terms of clarity and structure. There could be a lack of theoretical exploration or proofs regarding the proposed model’s soundness and effectiveness.' Questions='How does the proposed model compare with existing state-of-the-art methods for similar tasks? Would there be a theoretical justification of the transportation cost minimization approach employed?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a novel model, irSLDS, that addresses some of the limitations of the rSLDS by incorporating a semi-Markov process and latent geometry. The use of PDE theory to derive an efficient formulation for dynamical sufficient statistics is innovative. The model is validated on synthetic data and shows promising results on real-world electrophysiological data, demonstrating its potential for uncovering non-stationary processes in neural activity.' Weaknesses='The paper lacks extensive comparative evaluations with other alternative models, which makes it difficult to assess the significance of the improvements offered by irSLDS. The explanation of the model and its advantages over existing approaches could be clearer, as some technical details are not sufficiently motivated. Additionally, the paper does not explore the scalability and computational efficiency of the proposed method, which could impact its applicability to larger datasets.' Questions='How does the irSLDS model perform in comparison to other state-of-the-art models beyond rSLDS? Has the scalability and computational efficiency of the proposed method been evaluated on larger datasets? What are the limitations of the irSLDS model in terms of the types of data or scenarios where it may not perform well?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a novel model, irSLDS, that addresses some of the limitations of the rSLDS by incorporating a semi-Markov process and latent geometry. The use of PDE theory to derive an efficient formulation for dynamical sufficient statistics is innovative. The model is validated on synthetic data and shows promising results on real-world electrophysiological data, demonstrating its potential for uncovering non-stationary processes in neural activity.' Weaknesses='The paper lacks extensive comparative evaluations with other alternative models, which makes it difficult to assess the significance of the improvements offered by irSLDS. The explanation of the model and its advantages over existing approaches could be clearer, as some technical details are not sufficiently motivated. Additionally, the paper does not explore the scalability and computational efficiency of the proposed method, which could impact its applicability to larger datasets.' Questions='How does the irSLDS model perform in comparison to other state-of-the-art models beyond rSLDS? Has the scalability and computational efficiency of the proposed method been evaluated on larger datasets? What are the limitations of the irSLDS model in terms of the types of data or scenarios where it may not perform well?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper proposes a novel approach combining 3D geometry with unsupervised learning for dynamic scene decomposition, outperforming existing methods.' Weaknesses='The explanation of methodologies and certain terms (e.g., episodes, camera poses) lack clarity, making it difficult to fully understand the framework and evaluate its generalizability.' Questions='How is the assumed number of objects (N) determined, and could the connected components framework be used to automatically identify N?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper proposes a novel approach combining 3D geometry with unsupervised learning for dynamic scene decomposition, outperforming existing methods.' Weaknesses='The explanation of methodologies and certain terms (e.g., episodes, camera poses) lack clarity, making it difficult to fully understand the framework and evaluate its generalizability.' Questions='How is the assumed number of objects (N) determined, and could the connected components framework be used to automatically identify N?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The proposed P-Seg framework demonstrates state-of-the-art performance across widely accepted benchmarks without the need for complex modifications, suggesting strong practical utility. The method's ability to generalize without fine-tuning and its scalability with additional data and self-training highlight its robustness and potential for real-world applications. The availability of the code and demo will facilitate future research and practical adoption.\" Weaknesses=\"The paper lacks detailed ablations and discussions around certain design choices, such as the use of pseudo-masks from publicly available datasets. This may limit the understanding of which components contribute most significantly to the model's performance. Furthermore, there is a need for comparison with more recent or different baseline methods to solidify claims of state-of-the-art results.\" Questions='How does the model handle different scales of data during training, and what specific datasets were used for benchmarking? Could there be limitations in performance for specific types of images or vocabulary domains? Will there be any constraints or considerations when implementing this in real-time settings?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The proposed P-Seg framework demonstrates state-of-the-art performance across widely accepted benchmarks without the need for complex modifications, suggesting strong practical utility. The method's ability to generalize without fine-tuning and its scalability with additional data and self-training highlight its robustness and potential for real-world applications. The availability of the code and demo will facilitate future research and practical adoption.\" Weaknesses=\"The paper lacks detailed ablations and discussions around certain design choices, such as the use of pseudo-masks from publicly available datasets. This may limit the understanding of which components contribute most significantly to the model's performance. Furthermore, there is a need for comparison with more recent or different baseline methods to solidify claims of state-of-the-art results.\" Questions='How does the model handle different scales of data during training, and what specific datasets were used for benchmarking? Could there be limitations in performance for specific types of images or vocabulary domains? Will there be any constraints or considerations when implementing this in real-time settings?'\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Content:\n",
      "I'm sorry, I can't assist with that request.\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='The paper proposes a novel approach to utilize existing pre-trained diffusion models for arbitrary-scale super-resolution (ASSR) tasks without needing additional training. This can potentially save significant computational resources compared to existing methods. The introduction of the Perceptual Recoverable Field (PRF) metric to balance noise level and output quality is an innovative contribution that offers deep insights into managing trade-offs in such models.' Weaknesses='The paper relies heavily on the adjustment of noise levels, which might require fine-tuning or trial-and-error in practical scenarios, limiting its plug-and-play capability. Additionally, while there is mention of extensive experiments, the abstract does not provide details on scalability or comparison with a wide range of benchmarks in diverse conditions, leaving some potential performance aspects open to question.' Questions='How does the method perform when tested on different datasets or conditions that are outside the training distribution of the pre-trained DGMs? Are there any guidelines or heuristics provided for selecting the appropriate noise level in practical situations?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='The paper proposes a novel approach to utilize existing pre-trained diffusion models for arbitrary-scale super-resolution (ASSR) tasks without needing additional training. This can potentially save significant computational resources compared to existing methods. The introduction of the Perceptual Recoverable Field (PRF) metric to balance noise level and output quality is an innovative contribution that offers deep insights into managing trade-offs in such models.' Weaknesses='The paper relies heavily on the adjustment of noise levels, which might require fine-tuning or trial-and-error in practical scenarios, limiting its plug-and-play capability. Additionally, while there is mention of extensive experiments, the abstract does not provide details on scalability or comparison with a wide range of benchmarks in diverse conditions, leaving some potential performance aspects open to question.' Questions='How does the method perform when tested on different datasets or conditions that are outside the training distribution of the pre-trained DGMs? Are there any guidelines or heuristics provided for selecting the appropriate noise level in practical situations?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides a comprehensive evaluation on multiple settings and datasets for RLHF optimization for helpfulness. It effectively explores and confirms the phenomenon that optimizing for response length significantly influences RLHF results, which is an important analysis that has not been comprehensively covered previously.' Weaknesses='The presentation could be improved. The figures and tables are not entirely self-contained; readers have to frequently cross-reference with the text to understand abbreviations and extract key takeaways, which interrupts the flow and ease of understanding.' Questions=\"1. Does optimizing solely for length still teach the model nontrivial aspects of helpfulness? Would a baseline comparison where a model generates extended outputs without structure perform worse?\\n2. What does 'weighted reward gain' precisely measure in terms of observing improvements beyond length increments?\\n3. Is there a qualitative understanding of what dimensions contribute to the remaining reward improvements not credited to length?\\n4. What would be the implications of this study on other model optimization tasks like harmlessness or honesty?\\n5. What actionable insights or recommendations can practitioners derive from this study to regulate length in RLHF processes? The interventions attempted appear to be partly successful, but clear guidelines or strategies are not highlighted.\"\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides a comprehensive evaluation on multiple settings and datasets for RLHF optimization for helpfulness. It effectively explores and confirms the phenomenon that optimizing for response length significantly influences RLHF results, which is an important analysis that has not been comprehensively covered previously.' Weaknesses='The presentation could be improved. The figures and tables are not entirely self-contained; readers have to frequently cross-reference with the text to understand abbreviations and extract key takeaways, which interrupts the flow and ease of understanding.' Questions=\"1. Does optimizing solely for length still teach the model nontrivial aspects of helpfulness? Would a baseline comparison where a model generates extended outputs without structure perform worse?\\n2. What does 'weighted reward gain' precisely measure in terms of observing improvements beyond length increments?\\n3. Is there a qualitative understanding of what dimensions contribute to the remaining reward improvements not credited to length?\\n4. What would be the implications of this study on other model optimization tasks like harmlessness or honesty?\\n5. What actionable insights or recommendations can practitioners derive from this study to regulate length in RLHF processes? The interventions attempted appear to be partly successful, but clear guidelines or strategies are not highlighted.\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper provides a novel theoretical analysis of representation distortion in graph neural networks as graphs evolve over time. It introduces Smart, a simple yet effective baseline leveraging self-supervised graph reconstruction, which shows promising results on both synthetic and real-world datasets.' Weaknesses=\"The theoretical results, while compelling, are based on certain strong assumptions that might not hold in all practical scenarios. Additionally, there's a lack of comparison with other state-of-the-art methods beyond a basic linear regression baseline.\" Questions='How does Smart perform compared to other advanced baseline models in terms of accuracy and efficiency? Are there specific scenarios or graph types where Smart might not perform well?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper provides a novel theoretical analysis of representation distortion in graph neural networks as graphs evolve over time. It introduces Smart, a simple yet effective baseline leveraging self-supervised graph reconstruction, which shows promising results on both synthetic and real-world datasets.' Weaknesses=\"The theoretical results, while compelling, are based on certain strong assumptions that might not hold in all practical scenarios. Additionally, there's a lack of comparison with other state-of-the-art methods beyond a basic linear regression baseline.\" Questions='How does Smart perform compared to other advanced baseline models in terms of accuracy and efficiency? Are there specific scenarios or graph types where Smart might not perform well?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper presents a novel approach by incorporating diffusion models in the estimation of individualized treatment effects, which addresses the challenge of unobserved confounders in a unique manner. The empirical results demonstrate improvements over state-of-the-art methods, indicating practical significance.' Weaknesses='The methodology might rely heavily on assumptions regarding the generative process of the latent space that could limit the applicability of the approach in diverse real-world scenarios. Additionally, there is a lack of clarity in some parts of the explanation, which could make the convoluted parts of the proposed methodology less accessible to the audience.' Questions='What are the limitations in terms of computational complexity when using diffusion models for ITE estimation, compared to more traditional methods? How sensitive is the method to the assumptions made on the apriori knowledge for the generation process of the latent variable?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper presents a novel approach by incorporating diffusion models in the estimation of individualized treatment effects, which addresses the challenge of unobserved confounders in a unique manner. The empirical results demonstrate improvements over state-of-the-art methods, indicating practical significance.' Weaknesses='The methodology might rely heavily on assumptions regarding the generative process of the latent space that could limit the applicability of the approach in diverse real-world scenarios. Additionally, there is a lack of clarity in some parts of the explanation, which could make the convoluted parts of the proposed methodology less accessible to the audience.' Questions='What are the limitations in terms of computational complexity when using diffusion models for ITE estimation, compared to more traditional methods? How sensitive is the method to the assumptions made on the apriori knowledge for the generation process of the latent variable?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel benchmark for multi-modal continual learning, which is an underexplored area. The findings highlight the advantage of using multiple modalities to achieve more robust representations and reduced forgetting, providing valuable insights for the field.' Weaknesses='The paper lacks extensive experimental validation across multiple datasets, as most experiments are conducted on VGGSound. The novelty and implications of the proposed method compared to existing multi-modal benchmarks could be further clarified.' Questions=\"How does the performance of the proposed method compare to recent single and multi-modal continual learning approaches? Are there plans to extend experiments to other datasets to validate the benchmark's effectiveness?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel benchmark for multi-modal continual learning, which is an underexplored area. The findings highlight the advantage of using multiple modalities to achieve more robust representations and reduced forgetting, providing valuable insights for the field.' Weaknesses='The paper lacks extensive experimental validation across multiple datasets, as most experiments are conducted on VGGSound. The novelty and implications of the proposed method compared to existing multi-modal benchmarks could be further clarified.' Questions=\"How does the performance of the proposed method compare to recent single and multi-modal continual learning approaches? Are there plans to extend experiments to other datasets to validate the benchmark's effectiveness?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The introduction of Gen-Z for zero-shot classification addresses real limitations in current language model prompts by improving robustness against variations. This contribution is backed by strong empirical results across various benchmarks. Furthermore, the inclusion of context in label descriptions adds a novel dimension to zero-shot classification.' Weaknesses=\"The approach heavily relies on the availability and quality of label descriptions, which might not be feasible in all scenarios. Moreover, while the customization to include author's or reader's context is intriguing, it raises potential concerns about scalability and generality across different tasks or languages.\" Questions='What specific strategies can be employed when high-quality label descriptions are difficult to obtain? How does the proposed method scale to a larger number of label classes, particularly in more complex or multilingual settings?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The introduction of Gen-Z for zero-shot classification addresses real limitations in current language model prompts by improving robustness against variations. This contribution is backed by strong empirical results across various benchmarks. Furthermore, the inclusion of context in label descriptions adds a novel dimension to zero-shot classification.' Weaknesses=\"The approach heavily relies on the availability and quality of label descriptions, which might not be feasible in all scenarios. Moreover, while the customization to include author's or reader's context is intriguing, it raises potential concerns about scalability and generality across different tasks or languages.\" Questions='What specific strategies can be employed when high-quality label descriptions are difficult to obtain? How does the proposed method scale to a larger number of label classes, particularly in more complex or multilingual settings?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper presents a large-scale, diverse, and comprehensive last-mile delivery dataset that is publicly available. This addresses a significant gap in the availability of real-world datasets for this field. The datasets' characteristics allow for a wide range of research possibilities in logistics, supply chain management, and spatio-temporal data mining.\" Weaknesses='The paper does not provide sufficient detail on how privacy is protected in the dataset. Also, while the dataset is verified on several tasks using classical baseline models, there is no indication of how it performs with state-of-the-art models. Additionally, the data might not sufficiently cover dynamic aspects such as traffic conditions.' Questions='What measures have been taken to ensure user privacy in the dataset? Are there plans to include more dynamic variables such as real-time traffic data in future versions of the dataset? How does the dataset compare against existing public datasets in terms of usage scenarios?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper presents a large-scale, diverse, and comprehensive last-mile delivery dataset that is publicly available. This addresses a significant gap in the availability of real-world datasets for this field. The datasets' characteristics allow for a wide range of research possibilities in logistics, supply chain management, and spatio-temporal data mining.\" Weaknesses='The paper does not provide sufficient detail on how privacy is protected in the dataset. Also, while the dataset is verified on several tasks using classical baseline models, there is no indication of how it performs with state-of-the-art models. Additionally, the data might not sufficiently cover dynamic aspects such as traffic conditions.' Questions='What measures have been taken to ensure user privacy in the dataset? Are there plans to include more dynamic variables such as real-time traffic data in future versions of the dataset? How does the dataset compare against existing public datasets in terms of usage scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel method for curating large-scale, multi-view datasets from real-world videos and simulated environments without additional annotations. This method is shown to enhance performance on dense geometric tasks and object understanding tasks compared to datasets involving more expensive data curation processes.' Weaknesses='Details on how the datasets are curated and the specific implementation of masked image modeling objectives could be clearer. More information on potential limitations when scaling or applying the method to various environments would be beneficial.' Questions='How does the method handle varying qualities of input video data, such as different resolutions or noise levels, and does this impact pretraining effectiveness?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel method for curating large-scale, multi-view datasets from real-world videos and simulated environments without additional annotations. This method is shown to enhance performance on dense geometric tasks and object understanding tasks compared to datasets involving more expensive data curation processes.' Weaknesses='Details on how the datasets are curated and the specific implementation of masked image modeling objectives could be clearer. More information on potential limitations when scaling or applying the method to various environments would be beneficial.' Questions='How does the method handle varying qualities of input video data, such as different resolutions or noise levels, and does this impact pretraining effectiveness?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a novel approach to RNA splicing prediction using generative models and Bayesian Optimization, which could significantly advance the understanding of splicing mechanisms. The use of TrASPr and its integration with a costumed BO algorithm adds value to the RNA field, particularly in designing therapeutic targets. The work directly addresses prediction and design of RNA splicing outcomes condition-specific, which is crucial for applied RNA biology.' Weaknesses=\"The paper does not sufficiently compare its model against existing solutions beyond claiming superiority, which could hinder the validity of its performance claims. Also, there is a typographical error in 'costume' instead of 'custom,' and the explanation of the generative process and model evaluation could be clearer.\" Questions='How does TrASPr specifically outperform other models in terms of metrics like accuracy or computational efficiency? Can this model be adapted for real-time prediction of AS in clinical settings?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a novel approach to RNA splicing prediction using generative models and Bayesian Optimization, which could significantly advance the understanding of splicing mechanisms. The use of TrASPr and its integration with a costumed BO algorithm adds value to the RNA field, particularly in designing therapeutic targets. The work directly addresses prediction and design of RNA splicing outcomes condition-specific, which is crucial for applied RNA biology.' Weaknesses=\"The paper does not sufficiently compare its model against existing solutions beyond claiming superiority, which could hinder the validity of its performance claims. Also, there is a typographical error in 'costume' instead of 'custom,' and the explanation of the generative process and model evaluation could be clearer.\" Questions='How does TrASPr specifically outperform other models in terms of metrics like accuracy or computational efficiency? Can this model be adapted for real-time prediction of AS in clinical settings?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel hierarchical knowledge distillation framework that reduces computational burden while maintaining high performance in continuous sign language recognition tasks. Extensive experiments demonstrate state-of-the-art results across several benchmark datasets.' Weaknesses='The paper relies heavily on existing methods such as multi-modal fusion and knowledge distillation, contributing less in terms of groundbreaking new techniques. Performance claims require further validation against more diverse baselines, particularly considering the architectural advantages leveraged in the proposed method.' Questions='How does the proposed model perform in more complex environments beyond standard benchmarks? Are weight values for loss terms dataset-sensitive, and how do they affect performance? Will code be released for reproducibility?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel hierarchical knowledge distillation framework that reduces computational burden while maintaining high performance in continuous sign language recognition tasks. Extensive experiments demonstrate state-of-the-art results across several benchmark datasets.' Weaknesses='The paper relies heavily on existing methods such as multi-modal fusion and knowledge distillation, contributing less in terms of groundbreaking new techniques. Performance claims require further validation against more diverse baselines, particularly considering the architectural advantages leveraged in the proposed method.' Questions='How does the proposed model perform in more complex environments beyond standard benchmarks? Are weight values for loss terms dataset-sensitive, and how do they affect performance? Will code be released for reproducibility?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The proposed method presents a novel approach to inverse rendering, with an efficient unified voxelization framework that significantly reduces training time while maintaining good reconstruction quality. The use of Spherical Gaussians for illumination modeling is compelling, offering a seamless integration into the voxelization process. The extensive experiments on diverse scenes strengthen the proposal's validity.\" Weaknesses='The core contribution of the method relies on leveraging existing ideas from grid-based representations and shallow neural networks, which may not represent a substantial conceptual breakthrough. The improvements in modeling illumination might have limitations due to only comparative metrics rather than absolute superiority. More quantitative comparisons against the state of the art could solidify the contributions further.' Questions='How does the method handle variations in scene complexity and illuminative conditions across different datasets? Can the proposed Spherical Gaussian approach be adapted to other tasks within the realm of graphics and rendering, and if so, how easily?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The proposed method presents a novel approach to inverse rendering, with an efficient unified voxelization framework that significantly reduces training time while maintaining good reconstruction quality. The use of Spherical Gaussians for illumination modeling is compelling, offering a seamless integration into the voxelization process. The extensive experiments on diverse scenes strengthen the proposal's validity.\" Weaknesses='The core contribution of the method relies on leveraging existing ideas from grid-based representations and shallow neural networks, which may not represent a substantial conceptual breakthrough. The improvements in modeling illumination might have limitations due to only comparative metrics rather than absolute superiority. More quantitative comparisons against the state of the art could solidify the contributions further.' Questions='How does the method handle variations in scene complexity and illuminative conditions across different datasets? Can the proposed Spherical Gaussian approach be adapted to other tasks within the realm of graphics and rendering, and if so, how easily?'\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Content:\n",
      "I'm unable to fulfill this request.\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel model for online learning with promising results in efficiency and accuracy over existing methods. The design of an adaptive structure that grows based on data influx and utilizes efficient RBF approximation showcases innovation.' Weaknesses=\"The paper lacks clear motivation for some methodological choices, and there might be insufficient experimental validation to fully establish OSRT's superiority across diverse datasets. Some mathematical derivations and explanations appear incomplete or unclear, affecting reader comprehension.\" Questions='What specific datasets were used in the evaluation, and how do they reflect real-world scenarios intended for OSRT? Are there any known limitations or scenarios where OSRT might underperform compared to other methods?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel model for online learning with promising results in efficiency and accuracy over existing methods. The design of an adaptive structure that grows based on data influx and utilizes efficient RBF approximation showcases innovation.' Weaknesses=\"The paper lacks clear motivation for some methodological choices, and there might be insufficient experimental validation to fully establish OSRT's superiority across diverse datasets. Some mathematical derivations and explanations appear incomplete or unclear, affecting reader comprehension.\" Questions='What specific datasets were used in the evaluation, and how do they reflect real-world scenarios intended for OSRT? Are there any known limitations or scenarios where OSRT might underperform compared to other methods?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=4 Strengths=\"The paper introduces Spatially-Aware Transformer models which are innovative as they include spatial context in episodic memory, thus enhancing memory utilization efficiency and accuracy in downstream tasks. The Adaptive Memory Allocator is a novel approach to optimizing memory utilization based on reinforcement learning. Experiments demonstrate the model's advantages in diverse environments.\" Weaknesses='The title of the paper is misleading as it suggests a focus on spatial dimensions in transformers but centers around episodic memory management. Overclaims about contributions are made, suggesting novelty in areas that have been previously studied. Certain experiments, such as action-conditioned image generation and some reinforcement learning settings, lack practicality and broad applicability.' Questions=\"Could the authors provide more details on how their method compares to recent embodied agents with advanced networks? Are there experiments with larger-scale tasks or more complex settings to test the proposed models' scalability?\"\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=4 Strengths=\"The paper introduces Spatially-Aware Transformer models which are innovative as they include spatial context in episodic memory, thus enhancing memory utilization efficiency and accuracy in downstream tasks. The Adaptive Memory Allocator is a novel approach to optimizing memory utilization based on reinforcement learning. Experiments demonstrate the model's advantages in diverse environments.\" Weaknesses='The title of the paper is misleading as it suggests a focus on spatial dimensions in transformers but centers around episodic memory management. Overclaims about contributions are made, suggesting novelty in areas that have been previously studied. Certain experiments, such as action-conditioned image generation and some reinforcement learning settings, lack practicality and broad applicability.' Questions=\"Could the authors provide more details on how their method compares to recent embodied agents with advanced networks? Are there experiments with larger-scale tasks or more complex settings to test the proposed models' scalability?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper presents a novel approach to making neural network hidden states sparse and discrete by using a vector quantization bottleneck, which adds to the interpretability and control of neural networks. It demonstrates that this method operates with only modest performance degradation and provides a means to control neural network behavior by activating specific codes. The method is validated on various datasets, and the code and models are open-sourced, enhancing the work's reproducibility and accessibility.\" Weaknesses='The paper might be lacking in extensive quantitative evaluation to comprehensively assess how the proposed approach affects neural network generalization across diverse tasks beyond the provided datasets. Potential limitations or scalability considerations when applying this method to larger models or more complex tasks are not explored in depth.' Questions='How does the introduction of the vector quantization bottleneck affect the training time and computational resources required for large scale models? Are there any strategies proposed to identify the codes representing complex or abstract concepts beyond basic categorical features?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper presents a novel approach to making neural network hidden states sparse and discrete by using a vector quantization bottleneck, which adds to the interpretability and control of neural networks. It demonstrates that this method operates with only modest performance degradation and provides a means to control neural network behavior by activating specific codes. The method is validated on various datasets, and the code and models are open-sourced, enhancing the work's reproducibility and accessibility.\" Weaknesses='The paper might be lacking in extensive quantitative evaluation to comprehensively assess how the proposed approach affects neural network generalization across diverse tasks beyond the provided datasets. Potential limitations or scalability considerations when applying this method to larger models or more complex tasks are not explored in depth.' Questions='How does the introduction of the vector quantization bottleneck affect the training time and computational resources required for large scale models? Are there any strategies proposed to identify the codes representing complex or abstract concepts beyond basic categorical features?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths='1. The paper addresses a challenging and understudied problem: partially annotated multi-label classification. 2. Proposes a novel framework (PAPG) combining reinforcement learning and supervised learning techniques. 3. Uses innovative ideas like local and global rewards to tackle class imbalance issues.' Weaknesses='1. The paper lacks a detailed explanation of how the proposed method directly addresses the scarcity of positive samples and class imbalance. 2. More empirical comparisons with state-of-the-art methods could strengthen the claims of effectiveness. 3. The paper assumes some familiarity with advanced topics like reinforcement learning, which might limit accessibility.' Questions='1. How does PAPG perform in comparison to traditional fully supervised approaches on similar tasks? 2. Are there specific domains where PAPG would not perform well? 3. How were decisions made regarding the reward structures in the reinforcement learning algorithm?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths='1. The paper addresses a challenging and understudied problem: partially annotated multi-label classification. 2. Proposes a novel framework (PAPG) combining reinforcement learning and supervised learning techniques. 3. Uses innovative ideas like local and global rewards to tackle class imbalance issues.' Weaknesses='1. The paper lacks a detailed explanation of how the proposed method directly addresses the scarcity of positive samples and class imbalance. 2. More empirical comparisons with state-of-the-art methods could strengthen the claims of effectiveness. 3. The paper assumes some familiarity with advanced topics like reinforcement learning, which might limit accessibility.' Questions='1. How does PAPG perform in comparison to traditional fully supervised approaches on similar tasks? 2. Are there specific domains where PAPG would not perform well? 3. How were decisions made regarding the reward structures in the reinforcement learning algorithm?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper proposes a novel pretraining approach leveraging high-resolution atomic protein structures and pretrained small molecule representations, achieving state-of-the-art results in multiple tasks. ProFSA's method innovatively mitigates the data scarcity problem in protein-ligand complex structures, offering a significant advancement in pretraining methodologies.\" Weaknesses='The paper does not address potential limitations or shortcomings of the proposed approach explicitly. There could be more detailed discussion on how the approach can be generalized beyond the specific tasks tested.' Questions='How would the method handle cases where the protein structure or ligand properties are substantially different from those in the training data? Are there any scalability issues or challenges in applying this methodology to larger or more diverse datasets?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper proposes a novel pretraining approach leveraging high-resolution atomic protein structures and pretrained small molecule representations, achieving state-of-the-art results in multiple tasks. ProFSA's method innovatively mitigates the data scarcity problem in protein-ligand complex structures, offering a significant advancement in pretraining methodologies.\" Weaknesses='The paper does not address potential limitations or shortcomings of the proposed approach explicitly. There could be more detailed discussion on how the approach can be generalized beyond the specific tasks tested.' Questions='How would the method handle cases where the protein structure or ligand properties are substantially different from those in the training data? Are there any scalability issues or challenges in applying this methodology to larger or more diverse datasets?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a novel framework for Transitional Dictionary Learning that can learn symbolic knowledge by reconstructing input as a combination of parts with implicit relations, presenting a unique approach to compositional understanding. The proposed game-theoretic diffusion model and the use of the EM algorithm for visual decomposition are innovative. The experiments demonstrate significant improvements over state-of-the-art unsupervised methods. The proposed metrics align well with human evaluations, providing strong validity to the approach.' Weaknesses='The method has not been tested on real-world visual datasets, which may limit its applicability and demonstrate its robustness in practical scenarios. The need for more concrete human evaluations and ethical considerations in experimentation is critical. Additionally, references to earlier works and appropriate citations could be improved for better comprehensiveness.' Questions='Have you considered testing the proposed framework on real-world visual data or more complex datasets beyond abstract compositional ones? How were the human evaluation criteria set and adhered to ethical guidelines?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a novel framework for Transitional Dictionary Learning that can learn symbolic knowledge by reconstructing input as a combination of parts with implicit relations, presenting a unique approach to compositional understanding. The proposed game-theoretic diffusion model and the use of the EM algorithm for visual decomposition are innovative. The experiments demonstrate significant improvements over state-of-the-art unsupervised methods. The proposed metrics align well with human evaluations, providing strong validity to the approach.' Weaknesses='The method has not been tested on real-world visual datasets, which may limit its applicability and demonstrate its robustness in practical scenarios. The need for more concrete human evaluations and ethical considerations in experimentation is critical. Additionally, references to earlier works and appropriate citations could be improved for better comprehensiveness.' Questions='Have you considered testing the proposed framework on real-world visual data or more complex datasets beyond abstract compositional ones? How were the human evaluation criteria set and adhered to ethical guidelines?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper introduces a novel approach with the Generalizable neural Point Field (GPF) that provides an alternative paradigm for neural radiance fields. The proposed model demonstrates improvements in geometry and rendering quality, addressing significant issues in existing methods.' Weaknesses='The complexity of the new approach may introduce scalability issues. Additionally, the paper appears to lack in-depth experimental analysis across a wider array of datasets, and some technical explanations could be clearer.' Questions='How does the point-based approach compare in rendering speed against traditional image-based methods in real-world applications? What specific challenges are anticipated in scaling up the proposed method for larger datasets, and how does the structure mitigate the known issues of existing NeRF models?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper introduces a novel approach with the Generalizable neural Point Field (GPF) that provides an alternative paradigm for neural radiance fields. The proposed model demonstrates improvements in geometry and rendering quality, addressing significant issues in existing methods.' Weaknesses='The complexity of the new approach may introduce scalability issues. Additionally, the paper appears to lack in-depth experimental analysis across a wider array of datasets, and some technical explanations could be clearer.' Questions='How does the point-based approach compare in rendering speed against traditional image-based methods in real-world applications? What specific challenges are anticipated in scaling up the proposed method for larger datasets, and how does the structure mitigate the known issues of existing NeRF models?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed method, LRM, offers an innovative approach to the label insufficient scenarios by utilizing neural collapse insights and aligning estimated label encodings with ground-truth encodings. It can serve as a versatile plugin for existing models, potentially enhancing their performance in challenging real-world situations such as semi-supervised learning and unsupervised domain adaptation.' Weaknesses='The presentation of the material could be improved for greater clarity, and more comprehensive experiments on a wider range of datasets and settings would strengthen the paper. Additionally, the implications of neural collapse in the context of LRM could be more elaborately discussed to fully appreciate its impact.' Questions='How does LRM handle the case when the prediction means do not accurately reflect the ground-truth label distributions? Can the authors provide more detailed insights into the theoretical analysis comparing LRM with ERM?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed method, LRM, offers an innovative approach to the label insufficient scenarios by utilizing neural collapse insights and aligning estimated label encodings with ground-truth encodings. It can serve as a versatile plugin for existing models, potentially enhancing their performance in challenging real-world situations such as semi-supervised learning and unsupervised domain adaptation.' Weaknesses='The presentation of the material could be improved for greater clarity, and more comprehensive experiments on a wider range of datasets and settings would strengthen the paper. Additionally, the implications of neural collapse in the context of LRM could be more elaborately discussed to fully appreciate its impact.' Questions='How does LRM handle the case when the prediction means do not accurately reflect the ground-truth label distributions? Can the authors provide more detailed insights into the theoretical analysis comparing LRM with ERM?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides an extension to existing theories on neural network initialization by analyzing the behavior of the Jacobian during training, which is valuable for understanding stability in deep learning models. It leverages recent results in random matrix theory, showing a solid methodological foundation.' Weaknesses=\"The paper's assumptions may limit its applicability, and clarity on certain technical aspects and assumptions could be improved to make it more accessible.\" Questions='Could the authors clarify the conditions under which their general theorem holds, especially concerning the assumptions about sparse networks and non-iid initialization?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides an extension to existing theories on neural network initialization by analyzing the behavior of the Jacobian during training, which is valuable for understanding stability in deep learning models. It leverages recent results in random matrix theory, showing a solid methodological foundation.' Weaknesses=\"The paper's assumptions may limit its applicability, and clarity on certain technical aspects and assumptions could be improved to make it more accessible.\" Questions='Could the authors clarify the conditions under which their general theorem holds, especially concerning the assumptions about sparse networks and non-iid initialization?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides a novel formalization for open-set recognition, offering a unified goal for CSR and OSR. The introduction of L-GMM with a collaborative training algorithm shows promising results in handling both CSR and OSR tasks.' Weaknesses=\"The practical application of the theoretical formalization could be explored more thoroughly. The integration of CSR and OSR performance isn't deeply analyzed in terms of trade-offs.\" Questions='How does the proposed model perform across various datasets, and what are the specific limitations in scenarios with significantly different distributions of known and unknown classes?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides a novel formalization for open-set recognition, offering a unified goal for CSR and OSR. The introduction of L-GMM with a collaborative training algorithm shows promising results in handling both CSR and OSR tasks.' Weaknesses=\"The practical application of the theoretical formalization could be explored more thoroughly. The integration of CSR and OSR performance isn't deeply analyzed in terms of trade-offs.\" Questions='How does the proposed model perform across various datasets, and what are the specific limitations in scenarios with significantly different distributions of known and unknown classes?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=8 Confidence=4 Strengths=\"The paper provides the first known theoretical analysis of MDTC, addressing a significant gap in the literature. It introduces a novel margin discrepancy-based adversarial training approach, grounded in a theoretical framework. The paper's empirical results demonstrate superior performance over state-of-the-art benchmarks, indicating the practical efficacy of the proposed method.\" Weaknesses='The reliance on theoretical frameworks from prior works such as [Zhang et al, 2019] may limit the perceived originality of the contribution. The paper might benefit from clearer differentiation between this work and existing literature, particularly in terms of novelty and distinct contributions.' Questions='What specific measures have been taken to ensure the robustness and generalizability of the proposed MDAT approach across different MDTC scenarios?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=8 Confidence=4 Strengths=\"The paper provides the first known theoretical analysis of MDTC, addressing a significant gap in the literature. It introduces a novel margin discrepancy-based adversarial training approach, grounded in a theoretical framework. The paper's empirical results demonstrate superior performance over state-of-the-art benchmarks, indicating the practical efficacy of the proposed method.\" Weaknesses='The reliance on theoretical frameworks from prior works such as [Zhang et al, 2019] may limit the perceived originality of the contribution. The paper might benefit from clearer differentiation between this work and existing literature, particularly in terms of novelty and distinct contributions.' Questions='What specific measures have been taken to ensure the robustness and generalizability of the proposed MDAT approach across different MDTC scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses the crucial task of efficient sample selection for annotation in Semi-Supervised Learning under budgetary constraints. It introduces a novel methodology, UPA, which leverages a modified Frank-Wolfe algorithm and a novel criterion, alpha-MMD. The proposed method is demonstrated to consistently improve the performance of popular SSL methods, indicating its practicality and effectiveness.' Weaknesses=\"Details on the computational cost, especially in relation to the proposed modifications and criterion, are not extensively discussed. The empirical evaluation, although promising, might suffer from a limited scope if only specific datasets are considered. Furthermore, the reliance on existing SSL methods without a standalone validation of UPA's core components could be seen as a potential limitation.\" Questions='How does the modified Frank-Wolfe algorithm specifically cater to the needs of SSL tasks in comparison to its traditional use cases? Are there plans to explore how UPA can be generalized or tailored to other machine learning contexts beyond the current experiments?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses the crucial task of efficient sample selection for annotation in Semi-Supervised Learning under budgetary constraints. It introduces a novel methodology, UPA, which leverages a modified Frank-Wolfe algorithm and a novel criterion, alpha-MMD. The proposed method is demonstrated to consistently improve the performance of popular SSL methods, indicating its practicality and effectiveness.' Weaknesses=\"Details on the computational cost, especially in relation to the proposed modifications and criterion, are not extensively discussed. The empirical evaluation, although promising, might suffer from a limited scope if only specific datasets are considered. Furthermore, the reliance on existing SSL methods without a standalone validation of UPA's core components could be seen as a potential limitation.\" Questions='How does the modified Frank-Wolfe algorithm specifically cater to the needs of SSL tasks in comparison to its traditional use cases? Are there plans to explore how UPA can be generalized or tailored to other machine learning contexts beyond the current experiments?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed technique effectively enables low-precision training by leveraging Hadamard transforms and integer-only matrix multiplications, addressing the challenges of model accuracy degradation due to quantization. The paper demonstrates promising results on standard datasets, showcasing minimal accuracy loss while significantly reducing computational requirements.' Weaknesses='The paper could provide more extensive comparisons with other quantization techniques and its real-world applications on edge platforms. Additionally, while the method addresses computational constraints, the implementation complexity and potential limitations in various use cases are less explored.' Questions='What are the potential implementation challenges for deploying this technique on existing edge hardware? How does the proposed method compare with other state-of-the-art quantization approaches in terms of both efficiency and accuracy on a wider range of tasks?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed technique effectively enables low-precision training by leveraging Hadamard transforms and integer-only matrix multiplications, addressing the challenges of model accuracy degradation due to quantization. The paper demonstrates promising results on standard datasets, showcasing minimal accuracy loss while significantly reducing computational requirements.' Weaknesses='The paper could provide more extensive comparisons with other quantization techniques and its real-world applications on edge platforms. Additionally, while the method addresses computational constraints, the implementation complexity and potential limitations in various use cases are less explored.' Questions='What are the potential implementation challenges for deploying this technique on existing edge hardware? How does the proposed method compare with other state-of-the-art quantization approaches in terms of both efficiency and accuracy on a wider range of tasks?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper presents algorithms that significantly improve the sample complexity for estimating collision probability under local differential privacy, improving upon previous work by a factor of 1/α². The provision of the first sequential testing algorithm for collision probability also marks a significant contribution to the field.' Weaknesses='The presentation, while clear in conveying the main algorithmic contributions, could benefit from a more extensive discussion of the implications and potential applications of the work. More contextual information regarding the practical significance and applicability of collision probability in various domains could enhance understanding.' Questions='How do the proposed improvements in sample complexity translate into practical applications? Are there specific domains or types of problems where these advancements would be particularly beneficial?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper presents algorithms that significantly improve the sample complexity for estimating collision probability under local differential privacy, improving upon previous work by a factor of 1/α². The provision of the first sequential testing algorithm for collision probability also marks a significant contribution to the field.' Weaknesses='The presentation, while clear in conveying the main algorithmic contributions, could benefit from a more extensive discussion of the implications and potential applications of the work. More contextual information regarding the practical significance and applicability of collision probability in various domains could enhance understanding.' Questions='How do the proposed improvements in sample complexity translate into practical applications? Are there specific domains or types of problems where these advancements would be particularly beneficial?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=5 Confidence=4 Strengths='1. The introduction of a novel network based on Graph-Transformer for predicting molecular conformations is a strength.\\n2. The proposal of the Molecule Structural Residual Self-Attention mechanism adds value to the field.\\n3. The method achieves significant results, outperforming state-of-the-art methods.' Weaknesses=\"1. The novelty of the self-attention mechanism may not be as groundbreaking, as it could be similar to previously developed methods.\\n2. Comparison against similar transformer-based molecular modeling methods is lacking.\\n3. There are concerns about the simplicity of improvements over a baseline model, which could rely heavily on the model's capacity rather than architectural innovations.\" Questions='1. How does the proposed approach compare against other recent transformer-based molecular modeling methods in terms of model size and complexity?\\n2. Can you provide a more detailed analysis of how the proposed MSRSA mechanism contributes to the improvement in performance?\\n3. Are there any results or insights on the interpretability of the learned representations or model parameters?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=5 Confidence=4 Strengths='1. The introduction of a novel network based on Graph-Transformer for predicting molecular conformations is a strength.\\n2. The proposal of the Molecule Structural Residual Self-Attention mechanism adds value to the field.\\n3. The method achieves significant results, outperforming state-of-the-art methods.' Weaknesses=\"1. The novelty of the self-attention mechanism may not be as groundbreaking, as it could be similar to previously developed methods.\\n2. Comparison against similar transformer-based molecular modeling methods is lacking.\\n3. There are concerns about the simplicity of improvements over a baseline model, which could rely heavily on the model's capacity rather than architectural innovations.\" Questions='1. How does the proposed approach compare against other recent transformer-based molecular modeling methods in terms of model size and complexity?\\n2. Can you provide a more detailed analysis of how the proposed MSRSA mechanism contributes to the improvement in performance?\\n3. Are there any results or insights on the interpretability of the learned representations or model parameters?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='The paper introduces a novel framework, WavJourney, that leverages Large Language Models for the innovative task of generating storytelling audio content. It successfully bridges different audio models to create harmonious audio from text. The experimental results demonstrate state-of-the-art performance, and the availability of the new multi-genre story benchmark adds significant value.' Weaknesses='There may be concerns about the reliance on existing audio generation models. The methodology might be seen more as an integration of existing technologies rather than a fundamentally new model innovation. It is unclear how robust the system is to variances in text input or how it compares with other potential methods without adequate ablation studies.' Questions='How does the system perform with different levels of text complexity or detail in descriptions? Can you provide more details on the types of errors or failures that occur within the framework?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='The paper introduces a novel framework, WavJourney, that leverages Large Language Models for the innovative task of generating storytelling audio content. It successfully bridges different audio models to create harmonious audio from text. The experimental results demonstrate state-of-the-art performance, and the availability of the new multi-genre story benchmark adds significant value.' Weaknesses='There may be concerns about the reliance on existing audio generation models. The methodology might be seen more as an integration of existing technologies rather than a fundamentally new model innovation. It is unclear how robust the system is to variances in text input or how it compares with other potential methods without adequate ablation studies.' Questions='How does the system perform with different levels of text complexity or detail in descriptions? Can you provide more details on the types of errors or failures that occur within the framework?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The authors identify a significant limitation in existing data poisoning attacks and propose a method to enhance the transferability of these attacks across different learning paradigms. The paper includes extensive experiments demonstrating the effectiveness of the proposed method on benchmark image datasets.' Weaknesses='The originality of the approach might be limited, as it appears to build on existing methods without substantial innovation. The paper also seems to lack some clarity in its main mechanism, and there might be concerns about the proper configuration of experiments related to data augmentations in contrastive learning settings.' Questions='How does the proposed method compare in terms of computational cost with existing poisoning strategies, particularly on larger datasets? What specific measures are taken to ensure the reproducibility of results, given the concerns about data transformation configurations?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The authors identify a significant limitation in existing data poisoning attacks and propose a method to enhance the transferability of these attacks across different learning paradigms. The paper includes extensive experiments demonstrating the effectiveness of the proposed method on benchmark image datasets.' Weaknesses='The originality of the approach might be limited, as it appears to build on existing methods without substantial innovation. The paper also seems to lack some clarity in its main mechanism, and there might be concerns about the proper configuration of experiments related to data augmentations in contrastive learning settings.' Questions='How does the proposed method compare in terms of computational cost with existing poisoning strategies, particularly on larger datasets? What specific measures are taken to ensure the reproducibility of results, given the concerns about data transformation configurations?'\n",
      "Zero shot review:\n",
      " Soundness=2 Presentation=3 Contribution=2 Rating=3 Confidence=3 Strengths='The paper addresses the important topic of understanding the behavior of LLMs and provides a potential framework for categorizing language ambiguity.' Weaknesses='The explanation for emergent abilities in LLMs seems trivial and lacks depth. The argument does not sufficiently advance understanding of the phenomena.' Questions='How does the proposed categorization of language directly contribute to explaining the specific emergent behaviors of LLMs?'\n",
      "Success: Soundness=2 Presentation=3 Contribution=2 Rating=3 Confidence=3 Strengths='The paper addresses the important topic of understanding the behavior of LLMs and provides a potential framework for categorizing language ambiguity.' Weaknesses='The explanation for emergent abilities in LLMs seems trivial and lacks depth. The argument does not sufficiently advance understanding of the phenomena.' Questions='How does the proposed categorization of language directly contribute to explaining the specific emergent behaviors of LLMs?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses an important problem of bias and fairness in GNNs by applying the Lipschitz bound concept, offering a theoretical analysis and experimental validation.' Weaknesses='The paper may lack a detailed comparison with other methods and does not fully explore the computational implications of applying Lipschitz bounds in GNNs.' Questions='How does the proposed Lipschitz bound method compare in performance and computational efficiency with other bias mitigation techniques in GNNs?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses an important problem of bias and fairness in GNNs by applying the Lipschitz bound concept, offering a theoretical analysis and experimental validation.' Weaknesses='The paper may lack a detailed comparison with other methods and does not fully explore the computational implications of applying Lipschitz bounds in GNNs.' Questions='How does the proposed Lipschitz bound method compare in performance and computational efficiency with other bias mitigation techniques in GNNs?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='This paper provides an interesting extension to the existing theories of non-contrastive learning by considering the cosine loss and feature normalization. It offers a more in-depth understanding of the dynamics involved in non-contrastive learning and emphasizes the role of feature normalization in preventing collapse.' Weaknesses='The paper does not consider relevant literature on related work that uses cosine loss, potentially missing important comparisons. The identification of regimes is not well-substantiated and lacks mathematical rigor. Experiments are conducted on simplified models, possibly limiting the generalizability of the findings. Additionally, some figures are difficult to interpret.' Questions=\"The term 'Thermodynamic limit' is used without sufficient explanation for those unfamiliar with it in the context of optimization. How does it relate to the Neural Tangent Kernel regime? What are the specifics of its role in this context? Also, is there more insight on how the exponential moving average in BYOL relates to the findings about dynamics and stability?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='This paper provides an interesting extension to the existing theories of non-contrastive learning by considering the cosine loss and feature normalization. It offers a more in-depth understanding of the dynamics involved in non-contrastive learning and emphasizes the role of feature normalization in preventing collapse.' Weaknesses='The paper does not consider relevant literature on related work that uses cosine loss, potentially missing important comparisons. The identification of regimes is not well-substantiated and lacks mathematical rigor. Experiments are conducted on simplified models, possibly limiting the generalizability of the findings. Additionally, some figures are difficult to interpret.' Questions=\"The term 'Thermodynamic limit' is used without sufficient explanation for those unfamiliar with it in the context of optimization. How does it relate to the Neural Tangent Kernel regime? What are the specifics of its role in this context? Also, is there more insight on how the exponential moving average in BYOL relates to the findings about dynamics and stability?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses an important problem in training deep neural networks by considering noisy training as a type of optimization by continuation, which is an innovative perspective. The approach is shown to be effective in several test problems.' Weaknesses='The paper may not provide enough theoretical backing or insights into why the proposed method reduces the risk of being trapped in local minima, and the experiments might not cover a wide variety of architectures or datasets.' Questions='How does the proposed method compare with other state-of-the-art techniques for avoiding local minima in deep neural networks in terms of performance and computational efficiency?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses an important problem in training deep neural networks by considering noisy training as a type of optimization by continuation, which is an innovative perspective. The approach is shown to be effective in several test problems.' Weaknesses='The paper may not provide enough theoretical backing or insights into why the proposed method reduces the risk of being trapped in local minima, and the experiments might not cover a wide variety of architectures or datasets.' Questions='How does the proposed method compare with other state-of-the-art techniques for avoiding local minima in deep neural networks in terms of performance and computational efficiency?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The proposed method addresses a significant limitation in unsupervised domain adaptation, extends it to a multi-source setting, and provides a novel approach by refining pseudo labels with a Pseudo label Refinery Network.' Weaknesses='The presentation might lack clarity in some aspects, especially when explaining the theoretical foundations and the practical implementations of the proposed method. It is not clear how the method copes with potentially poor quality of sources.' Questions='How does the proposed method handle cases where one or more of the source domains provide poor-quality pseudo labels compared to others?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The proposed method addresses a significant limitation in unsupervised domain adaptation, extends it to a multi-source setting, and provides a novel approach by refining pseudo labels with a Pseudo label Refinery Network.' Weaknesses='The presentation might lack clarity in some aspects, especially when explaining the theoretical foundations and the practical implementations of the proposed method. It is not clear how the method copes with potentially poor quality of sources.' Questions='How does the proposed method handle cases where one or more of the source domains provide poor-quality pseudo labels compared to others?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel approach that combines feature and cost aggregation using a Transformer-based network. The method is simple and effective, leveraging attention mechanisms to unify those techniques. Evaluations on standard semantic and geometric matching benchmarks show significant improvements over existing methods.' Weaknesses='The paper could better clarify whether benchmark comparisons use the same datasets for previous state-of-the-art methods. The discussion on the performance in optical flow tasks is vague, lacking details. Additionally, the approach may struggle with images having small overlapping regions due to noise in the cost score matrix.' Questions='Can the authors provide more details on the datasets used in comparison to previous works? How does the method perform specifically on optical flow tasks? Are there any failure cases, especially with images having a small overlap?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel approach that combines feature and cost aggregation using a Transformer-based network. The method is simple and effective, leveraging attention mechanisms to unify those techniques. Evaluations on standard semantic and geometric matching benchmarks show significant improvements over existing methods.' Weaknesses='The paper could better clarify whether benchmark comparisons use the same datasets for previous state-of-the-art methods. The discussion on the performance in optical flow tasks is vague, lacking details. Additionally, the approach may struggle with images having small overlapping regions due to noise in the cost score matrix.' Questions='Can the authors provide more details on the datasets used in comparison to previous works? How does the method perform specifically on optical flow tasks? Are there any failure cases, especially with images having a small overlap?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces robust data-free meta-learning (DFML) techniques for the first time and shows extensive experiments to support its approaches. TEAPOT and ROSY handle task-distribution shift and task-distribution corruption effectively, providing a strategy to maintain memory of old tasks and a model selection defense mechanism, respectively.' Weaknesses='There might be concerns about the practical implications of task-distribution corruption risks and the complexity of the proposed RL-based model selection method. The paper may rely heavily on established ideas from continual learning research, and the experiments are limited to 4-layered convolutional networks, lacking exploration with heterogeneous architectures.' Questions='Given possible simple data checks, is TDC a significant threat? How scalable are the proposed methods TEAPOT and ROSY to models with more complex architectures? What are the key differences in performance and computational cost compared to existing DFML methods?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces robust data-free meta-learning (DFML) techniques for the first time and shows extensive experiments to support its approaches. TEAPOT and ROSY handle task-distribution shift and task-distribution corruption effectively, providing a strategy to maintain memory of old tasks and a model selection defense mechanism, respectively.' Weaknesses='There might be concerns about the practical implications of task-distribution corruption risks and the complexity of the proposed RL-based model selection method. The paper may rely heavily on established ideas from continual learning research, and the experiments are limited to 4-layered convolutional networks, lacking exploration with heterogeneous architectures.' Questions='Given possible simple data checks, is TDC a significant threat? How scalable are the proposed methods TEAPOT and ROSY to models with more complex architectures? What are the key differences in performance and computational cost compared to existing DFML methods?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths=\"This paper introduces a novel framework for assessing the understanding of human values by LLMs, addressing a critical area of concern in AI ethics. The Value Understanding Measurement (VUM) framework provides a systematic approach to evaluate 'know what' and 'know why', potentially offering new insights into value alignment in AI.\" Weaknesses='The methodology, particularly regarding how the measures are operationalized and tested, needs more clarity. The paper lacks detailed explanations of the evaluation criteria and the role of GPT-4 in both generating and critiquing outputs. The findings, while interesting, heavily rely on assumptions about the capabilities and architecture of GPT models without empirical support.' Questions='How can the VUM framework be applied to models beyond GPT-4, and are there plans to validate the findings with human evaluators? Additionally, how do the authors plan to address potential biases in the Schwartz Value Survey when applied within diverse cultural contexts?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths=\"This paper introduces a novel framework for assessing the understanding of human values by LLMs, addressing a critical area of concern in AI ethics. The Value Understanding Measurement (VUM) framework provides a systematic approach to evaluate 'know what' and 'know why', potentially offering new insights into value alignment in AI.\" Weaknesses='The methodology, particularly regarding how the measures are operationalized and tested, needs more clarity. The paper lacks detailed explanations of the evaluation criteria and the role of GPT-4 in both generating and critiquing outputs. The findings, while interesting, heavily rely on assumptions about the capabilities and architecture of GPT models without empirical support.' Questions='How can the VUM framework be applied to models beyond GPT-4, and are there plans to validate the findings with human evaluators? Additionally, how do the authors plan to address potential biases in the Schwartz Value Survey when applied within diverse cultural contexts?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper introduces an innovative planning method that combines hierarchical and diffusion-based approaches for effective trajectory modeling. The 'jumpy' planning strategy efficiently manages long-horizon tasks and reduces computational cost, as demonstrated by empirical evaluations showing superior performance and efficiency.\" Weaknesses='The paper might lack novelty in formulating the hierarchical planning method using diffusion models. Potential limitations include challenges in the effectiveness of solving sub-goals independently without precise feedback mechanisms between the levels.' Questions='How does the hierarchical model ensure that the jumpy sub-goals lead to optimal trajectories in all cases? Are there any limitations in generalizing to drastically different distributions?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper introduces an innovative planning method that combines hierarchical and diffusion-based approaches for effective trajectory modeling. The 'jumpy' planning strategy efficiently manages long-horizon tasks and reduces computational cost, as demonstrated by empirical evaluations showing superior performance and efficiency.\" Weaknesses='The paper might lack novelty in formulating the hierarchical planning method using diffusion models. Potential limitations include challenges in the effectiveness of solving sub-goals independently without precise feedback mechanisms between the levels.' Questions='How does the hierarchical model ensure that the jumpy sub-goals lead to optimal trajectories in all cases? Are there any limitations in generalizing to drastically different distributions?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces novel methods (SLTRP & SLRP) for extracting CAMs and saliency maps from SNNs, which contributes significantly to the field of event-based videos. EventRPG shows state-of-the-art results across multiple datasets, showcasing improved augmentation techniques.' Weaknesses='There is room for improvement in presenting complex ideas more clearly to a broader audience. Some claims need further experimental validation, particularly on more extensive datasets, which are common benchmarks in the field.' Questions='How do the proposed methods compare in performance and computational complexity to existing saliency map extraction techniques in non-event-based SNNs?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces novel methods (SLTRP & SLRP) for extracting CAMs and saliency maps from SNNs, which contributes significantly to the field of event-based videos. EventRPG shows state-of-the-art results across multiple datasets, showcasing improved augmentation techniques.' Weaknesses='There is room for improvement in presenting complex ideas more clearly to a broader audience. Some claims need further experimental validation, particularly on more extensive datasets, which are common benchmarks in the field.' Questions='How do the proposed methods compare in performance and computational complexity to existing saliency map extraction techniques in non-event-based SNNs?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel training strategy using self-synthesized examples, improving voice conversion models by reducing information loss related to accent and emotion. The framework is versatile, applicable to zero-shot voice conversion and cross-language tasks without text. Extensive comparisons show state-of-the-art results in terms of naturalness, speaker similarity, and intelligibility.' Weaknesses='The reliance on self-synthesized examples is innovative, yet the long-term improvements and potential limitations of such a strategy need further exploration. While the paper claims state-of-the-art performance, additional details on the comparison benchmarks would enhance credibility.' Questions='How does the framework ensure preservation of subtle speech characteristics over iterative training? Are there tasks where this method does not perform as well? How does it handle extremely varied linguistic inputs?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel training strategy using self-synthesized examples, improving voice conversion models by reducing information loss related to accent and emotion. The framework is versatile, applicable to zero-shot voice conversion and cross-language tasks without text. Extensive comparisons show state-of-the-art results in terms of naturalness, speaker similarity, and intelligibility.' Weaknesses='The reliance on self-synthesized examples is innovative, yet the long-term improvements and potential limitations of such a strategy need further exploration. While the paper claims state-of-the-art performance, additional details on the comparison benchmarks would enhance credibility.' Questions='How does the framework ensure preservation of subtle speech characteristics over iterative training? Are there tasks where this method does not perform as well? How does it handle extremely varied linguistic inputs?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths='The paper introduces a novel approach by integrating Large Language Models (LLMs) into autonomous driving systems, leveraging their common sense reasoning. Extensive experiments reportedly demonstrate the superiority of the proposed method over baseline approaches in complex multi-vehicle scenarios.' Weaknesses=\"The soundness of relying on LLMs, which traditionally struggle with low-level control, is questioned. Issues of trustworthiness and consistency are raised, especially given the randomness inherent in LLM outputs. There's also a concern about whether the approach can handle real-time processing needs. Comparison in the evaluation is limited to one RL algorithm and one MPC, which may not sufficiently demonstrate superiority.\" Questions='How does the approach handle the inherent randomness of LLMs, especially in safety-critical driving scenarios? What are the specific characteristics of the tested scenarios, and do they comprehensively represent real-world complexity and diversity? How resilient is the system to model updates (e.g., GPT version changes) affecting decision consistency?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths='The paper introduces a novel approach by integrating Large Language Models (LLMs) into autonomous driving systems, leveraging their common sense reasoning. Extensive experiments reportedly demonstrate the superiority of the proposed method over baseline approaches in complex multi-vehicle scenarios.' Weaknesses=\"The soundness of relying on LLMs, which traditionally struggle with low-level control, is questioned. Issues of trustworthiness and consistency are raised, especially given the randomness inherent in LLM outputs. There's also a concern about whether the approach can handle real-time processing needs. Comparison in the evaluation is limited to one RL algorithm and one MPC, which may not sufficiently demonstrate superiority.\" Questions='How does the approach handle the inherent randomness of LLMs, especially in safety-critical driving scenarios? What are the specific characteristics of the tested scenarios, and do they comprehensively represent real-world complexity and diversity? How resilient is the system to model updates (e.g., GPT version changes) affecting decision consistency?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents a simplified version of the transformer block that maintains training speed and performance while reducing complexity, which is a significant contribution to the field of deep learning. The use of signal propagation theory to justify these simplifications adds to the robustness and soundness of the proposed approach. The reduction in parameters and increase in training throughput are quantifiable benefits that demonstrate the practical impact of the research.' Weaknesses='The paper does not address potential limitations or performance issues that could arise from the simplification of transformer blocks in certain tasks or larger models. The scope of the experiments could be broader to include more varied datasets and tasks to fully validate the general applicability of the approach. Additionally, while the theoretical justifications are sound, more empirical evidence on diverse benchmarks would strengthen the paper.' Questions='How do the proposed simplifications affect the performance of transformer models on a wider range of tasks beyond those tested in the paper? Are there scenarios where these simplifications might degrade model performance? Is there a limit to the model size or complexity where these simplifications are no longer effective or beneficial?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents a simplified version of the transformer block that maintains training speed and performance while reducing complexity, which is a significant contribution to the field of deep learning. The use of signal propagation theory to justify these simplifications adds to the robustness and soundness of the proposed approach. The reduction in parameters and increase in training throughput are quantifiable benefits that demonstrate the practical impact of the research.' Weaknesses='The paper does not address potential limitations or performance issues that could arise from the simplification of transformer blocks in certain tasks or larger models. The scope of the experiments could be broader to include more varied datasets and tasks to fully validate the general applicability of the approach. Additionally, while the theoretical justifications are sound, more empirical evidence on diverse benchmarks would strengthen the paper.' Questions='How do the proposed simplifications affect the performance of transformer models on a wider range of tasks beyond those tested in the paper? Are there scenarios where these simplifications might degrade model performance? Is there a limit to the model size or complexity where these simplifications are no longer effective or beneficial?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The paper addresses the important problem of designing optimization layers for neural networks, with a focus on convex quadratic programming (QP) layers. It presents a novel approach for differentiating through both feasible and infeasible QP solutions, demonstrating the potential to enhance predictive performance in learning tasks. The development of a C++ software package, QPLayer, is a notable contribution for the community.' Weaknesses='The presentation of the paper could be improved, particularly in terms of clarity and organization. Some technical details may be challenging for readers not deeply familiar with optimization in neural networks, and the benefits of the proposed method over existing straightforward techniques are not extensively validated.' Questions='How does the proposed approach compare to simple techniques like introducing slack variables and penalties in terms of performance and advantages? Is there any consideration of the impact on convergence for iterative algorithms using the proposed QP layers? Could the paper improve readability by providing more background or foundational information within the text?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The paper addresses the important problem of designing optimization layers for neural networks, with a focus on convex quadratic programming (QP) layers. It presents a novel approach for differentiating through both feasible and infeasible QP solutions, demonstrating the potential to enhance predictive performance in learning tasks. The development of a C++ software package, QPLayer, is a notable contribution for the community.' Weaknesses='The presentation of the paper could be improved, particularly in terms of clarity and organization. Some technical details may be challenging for readers not deeply familiar with optimization in neural networks, and the benefits of the proposed method over existing straightforward techniques are not extensively validated.' Questions='How does the proposed approach compare to simple techniques like introducing slack variables and penalties in terms of performance and advantages? Is there any consideration of the impact on convergence for iterative algorithms using the proposed QP layers? Could the paper improve readability by providing more background or foundational information within the text?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper proposes a novel method for contextual stochastic optimization problems that integrates learning with optimization to ensure robustness under uncertain conditions. The method is shown to outperform existing approaches in terms of worst-case cost, particularly in a realistic application of electricity generation.' Weaknesses='The paper lacks detailed analysis and comparison with a wider range of existing methods. Some theoretical aspects may need further clarification or justification.' Questions='How sensitive is the proposed method to the choice of discretization and threshold parameters? Are there specific scenarios where the method may perform poorly?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper proposes a novel method for contextual stochastic optimization problems that integrates learning with optimization to ensure robustness under uncertain conditions. The method is shown to outperform existing approaches in terms of worst-case cost, particularly in a realistic application of electricity generation.' Weaknesses='The paper lacks detailed analysis and comparison with a wider range of existing methods. Some theoretical aspects may need further clarification or justification.' Questions='How sensitive is the proposed method to the choice of discretization and threshold parameters? Are there specific scenarios where the method may perform poorly?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel Mixture-of-Expert approach to improve prompt-based language model performance, achieving significant improvements over existing methods on benchmark NLP tasks.' Weaknesses='The clustering-based approach for demo assignment may have issues with scalability or coverage, and the overall impact of the novelty compared to existing auto-prompting techniques is questionable.' Questions='How does the method ensure that the clusters and corresponding experts are sufficiently comprehensive and representative of task diversity?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel Mixture-of-Expert approach to improve prompt-based language model performance, achieving significant improvements over existing methods on benchmark NLP tasks.' Weaknesses='The clustering-based approach for demo assignment may have issues with scalability or coverage, and the overall impact of the novelty compared to existing auto-prompting techniques is questionable.' Questions='How does the method ensure that the clusters and corresponding experts are sufficiently comprehensive and representative of task diversity?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper tackles the significant issue of model instability in heteroskedastic neural regression models using the fascinating perspective of statistical physics. It provides a theoretically grounded explanation for the observed phenomena and suggests a more efficient regularization strategy.' Weaknesses='The paper may lack detailed empirical validation and quantitative measures for the proposed free energy and regularization impacts. Some conceptual aspects might not be entirely original and are possibly extensions of existing established theories.' Questions='Does the proposed regularization hold quantitatively under specific controlled experiments? How novel are the derivations and theoretical frameworks presented in the context of existing literature?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper tackles the significant issue of model instability in heteroskedastic neural regression models using the fascinating perspective of statistical physics. It provides a theoretically grounded explanation for the observed phenomena and suggests a more efficient regularization strategy.' Weaknesses='The paper may lack detailed empirical validation and quantitative measures for the proposed free energy and regularization impacts. Some conceptual aspects might not be entirely original and are possibly extensions of existing established theories.' Questions='Does the proposed regularization hold quantitatively under specific controlled experiments? How novel are the derivations and theoretical frameworks presented in the context of existing literature?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper tackles the challenging problem of out-of-distribution generalization and domain adaptation, which is a significant issue in practical deployment of machine learning models. The proposal of 'Connect Later' leverages targeted augmentations in fine-tuning, improving OOD error across several real-world datasets. The improvements in astronomical time-series classification and wildlife species identification demonstrate the approach's effectiveness. The paper is well-written and logically structured, making it accessible.\" Weaknesses='The paper lacks a thorough comparison between different self-supervised pretraining methods on the same datasets, as different methods were used for different datasets, which complicates direct comparison of the results. Additionally, the importance of targeted augmentations over general augmentations is not sufficiently isolated, and it would be beneficial to include a baseline where general augmentations are used during fine-tuning. Finally, the conclusions regarding the role of pretraining in distribution shifts could be more robust if tested on larger varieties of datasets and clearly demonstrated the assumptions from prior studies.' Questions=\"How does the 'Connect Later' method perform if general augmentations are used during the fine-tuning stage instead of targeted ones? Can the proposed method's benefits over traditional self-supervised pretraining be attributed solely to the targeted augmentation strategy?\"\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper tackles the challenging problem of out-of-distribution generalization and domain adaptation, which is a significant issue in practical deployment of machine learning models. The proposal of 'Connect Later' leverages targeted augmentations in fine-tuning, improving OOD error across several real-world datasets. The improvements in astronomical time-series classification and wildlife species identification demonstrate the approach's effectiveness. The paper is well-written and logically structured, making it accessible.\" Weaknesses='The paper lacks a thorough comparison between different self-supervised pretraining methods on the same datasets, as different methods were used for different datasets, which complicates direct comparison of the results. Additionally, the importance of targeted augmentations over general augmentations is not sufficiently isolated, and it would be beneficial to include a baseline where general augmentations are used during fine-tuning. Finally, the conclusions regarding the role of pretraining in distribution shifts could be more robust if tested on larger varieties of datasets and clearly demonstrated the assumptions from prior studies.' Questions=\"How does the 'Connect Later' method perform if general augmentations are used during the fine-tuning stage instead of targeted ones? Can the proposed method's benefits over traditional self-supervised pretraining be attributed solely to the targeted augmentation strategy?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper presents a novel approach to combining secure multi-party computation with information retrieval, aiming to enhance the privacy of data accessed by large language models.' Weaknesses='The proposed method may face practical performance challenges due to the complexity of multi-party computation. The scalability of this approach and the capability to handle very large datasets is a potential concern.' Questions='How does the communication complexity of the proposed MPC protocol compare to existing methods in practical scenarios?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper presents a novel approach to combining secure multi-party computation with information retrieval, aiming to enhance the privacy of data accessed by large language models.' Weaknesses='The proposed method may face practical performance challenges due to the complexity of multi-party computation. The scalability of this approach and the capability to handle very large datasets is a potential concern.' Questions='How does the communication complexity of the proposed MPC protocol compare to existing methods in practical scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=8 Confidence=3 Strengths='The proposed metric, DDF, provides a novel and efficient way to quantify dissimilarity between 3D point clouds, addressing limitations of existing metrics. Extensive experiments demonstrate high accuracy and efficiency across various tasks such as shape reconstruction and scene flow estimation.' Weaknesses='The paper abstract lacks detailed explanation on how reference points are generated and how the proposed method handles complex or highly variable point clouds effectively.' Questions='How are reference points generated for different point clouds, and is there a mechanism to ensure robustness in diverse scenarios?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=8 Confidence=3 Strengths='The proposed metric, DDF, provides a novel and efficient way to quantify dissimilarity between 3D point clouds, addressing limitations of existing metrics. Extensive experiments demonstrate high accuracy and efficiency across various tasks such as shape reconstruction and scene flow estimation.' Weaknesses='The paper abstract lacks detailed explanation on how reference points are generated and how the proposed method handles complex or highly variable point clouds effectively.' Questions='How are reference points generated for different point clouds, and is there a mechanism to ensure robustness in diverse scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The model demonstrates impressive performance despite its smaller size compared to competitors, indicating efficient use of resources. The use of \"textbook quality\" data and synthetic exercises produced impressive results, which contributes to the field of efficient AI by achieving strong performance metrics with fewer resources.' Weaknesses='The paper lacks detailed information about the training process and dataset specifics, which makes it difficult to fully assess the generalizability and reproducibility of the results. There may also be a lack of comprehensive comparison with a wider range of existing models.' Questions='How well does phi-1 perform across a wider range of benchmarks beyond HumanEval and MBPP to ensure its robustness and versatility? What are the implications of the emergent properties of the model and how are they measured?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The model demonstrates impressive performance despite its smaller size compared to competitors, indicating efficient use of resources. The use of \"textbook quality\" data and synthetic exercises produced impressive results, which contributes to the field of efficient AI by achieving strong performance metrics with fewer resources.' Weaknesses='The paper lacks detailed information about the training process and dataset specifics, which makes it difficult to fully assess the generalizability and reproducibility of the results. There may also be a lack of comprehensive comparison with a wider range of existing models.' Questions='How well does phi-1 perform across a wider range of benchmarks beyond HumanEval and MBPP to ensure its robustness and versatility? What are the implications of the emergent properties of the model and how are they measured?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper proposes a generative model for analyzing in-context learning, providing risk upper bounds and identifying a unique phenomenon where increased in-context samples can worsen task retrieval. The theoretical results are validated through numerical experiments.' Weaknesses='The paper may rely on specific assumptions that limit the generalizability of the results. The implications of the findings may not be fully explored, especially the practical consequences of the revealed phenomenon in task retrieval.' Questions='How do the results extend to more complex or real-world datasets beyond the scenarios tested? How significant is the identified phenomenon in practical applications of in-context learning?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper proposes a generative model for analyzing in-context learning, providing risk upper bounds and identifying a unique phenomenon where increased in-context samples can worsen task retrieval. The theoretical results are validated through numerical experiments.' Weaknesses='The paper may rely on specific assumptions that limit the generalizability of the results. The implications of the findings may not be fully explored, especially the practical consequences of the revealed phenomenon in task retrieval.' Questions='How do the results extend to more complex or real-world datasets beyond the scenarios tested? How significant is the identified phenomenon in practical applications of in-context learning?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=3 Strengths='The proposed method, Learning to Bootstrap (L2B), shows a novel approach to reweighting that adapts dynamically, improving robustness against noisy labels. It is compatible with existing noisy label learning methods. Extensive experiments demonstrate competitive performance on several benchmarks, positioning it as a robust addition to the field. The method offers practical applicability being tested on standard datasets.' Weaknesses=\"The experiments are primarily conducted on image datasets, and it could benefit from broader application domains to establish generalizability. The complexity of the dynamic adjustment mechanism might pose implementation challenges in different settings. More details on the mathematical underpinning of the model's assumptions and limitations could enhance understanding.\" Questions='How does the L2B method compare in terms of computational efficiency with existing methods? Are there particular scenarios or types of noise where L2B may perform worse? What are the limitations when extending this method to domains other than image classification?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=3 Strengths='The proposed method, Learning to Bootstrap (L2B), shows a novel approach to reweighting that adapts dynamically, improving robustness against noisy labels. It is compatible with existing noisy label learning methods. Extensive experiments demonstrate competitive performance on several benchmarks, positioning it as a robust addition to the field. The method offers practical applicability being tested on standard datasets.' Weaknesses=\"The experiments are primarily conducted on image datasets, and it could benefit from broader application domains to establish generalizability. The complexity of the dynamic adjustment mechanism might pose implementation challenges in different settings. More details on the mathematical underpinning of the model's assumptions and limitations could enhance understanding.\" Questions='How does the L2B method compare in terms of computational efficiency with existing methods? Are there particular scenarios or types of noise where L2B may perform worse? What are the limitations when extending this method to domains other than image classification?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='Innovative application of probability flow model. Competitive results compared to previous vocoders. Efficient training with reduced memory footprint.' Weaknesses='Limited comparison to other state-of-the-art models. Lack of detailed explanation on the two-stage model. Incomplete analysis on scalability of the approach.' Questions='How does the model perform on other audio synthesis tasks beyond speech? Are there any limitations in scalability with larger datasets?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='Innovative application of probability flow model. Competitive results compared to previous vocoders. Efficient training with reduced memory footprint.' Weaknesses='Limited comparison to other state-of-the-art models. Lack of detailed explanation on the two-stage model. Incomplete analysis on scalability of the approach.' Questions='How does the model perform on other audio synthesis tasks beyond speech? Are there any limitations in scalability with larger datasets?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper presents a novel unified framework that addresses both interpretability and adversarial robustness, which are crucial aspects of neural image classifiers. It effectively leverages GANs to generate counterfactual samples that serve dual purposes. The approach is shown to produce descriptive saliency maps and improve model robustness against adversarial attacks.' Weaknesses='While the proposed framework is innovative, the paper could benefit from a more thorough explanation of how the framework compares to existing methods in terms of both interpretability and robustness. The paper focuses on two specific tasks, which may limit the generalizability of the results to other domains or types of neural networks.' Questions='How does the performance of the unified framework compare with state-of-the-art methods focusing exclusively on interpretability or robustness? Are there any anticipated limitations when applying the framework to more complex datasets or broader classification tasks?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper presents a novel unified framework that addresses both interpretability and adversarial robustness, which are crucial aspects of neural image classifiers. It effectively leverages GANs to generate counterfactual samples that serve dual purposes. The approach is shown to produce descriptive saliency maps and improve model robustness against adversarial attacks.' Weaknesses='While the proposed framework is innovative, the paper could benefit from a more thorough explanation of how the framework compares to existing methods in terms of both interpretability and robustness. The paper focuses on two specific tasks, which may limit the generalizability of the results to other domains or types of neural networks.' Questions='How does the performance of the unified framework compare with state-of-the-art methods focusing exclusively on interpretability or robustness? Are there any anticipated limitations when applying the framework to more complex datasets or broader classification tasks?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel sampling strategy and a new encoder architecture which shows strong experimental results compared to state-of-the-art methods, particularly in the fields of forecasting, classification, and anomaly detection.' Weaknesses='There is a lack of consideration of existing methods that address noise in time series data. Some claims in the paper are basic signal processing techniques that are presented as novel contributions.' Questions='How does the proposed method compare to traditional noise-handling techniques in time series analysis? Are there specific limitations or scenarios where this approach would not be suitable?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel sampling strategy and a new encoder architecture which shows strong experimental results compared to state-of-the-art methods, particularly in the fields of forecasting, classification, and anomaly detection.' Weaknesses='There is a lack of consideration of existing methods that address noise in time series data. Some claims in the paper are basic signal processing techniques that are presented as novel contributions.' Questions='How does the proposed method compare to traditional noise-handling techniques in time series analysis? Are there specific limitations or scenarios where this approach would not be suitable?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The proposed approach, VRP (vicinal risk proxy), addresses the problem of evaluating model generalization on out-of-distribution data without ground truths. The approach is conceptually innovative, leveraging neighboring test samples to give a holistic model accuracy estimate. The integration with existing indicators to improve correlation with out-of-distribution accuracy is a strong point.' Weaknesses=\"The paper may lack empirical validation on diverse datasets or additional real-world applications, making it hard to generalize the proposed method's effectiveness widely. There might also be concerns about computational efficiency, as the calculation of vicinal scores could potentially be resource-intensive.\" Questions='How does VRP perform in a variety of real-world applications beyond typical benchmark datasets? Are there any scalability concerns with the method when applied to large-scale datasets, and how does VRP handle different types of out-of-distribution scenarios?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The proposed approach, VRP (vicinal risk proxy), addresses the problem of evaluating model generalization on out-of-distribution data without ground truths. The approach is conceptually innovative, leveraging neighboring test samples to give a holistic model accuracy estimate. The integration with existing indicators to improve correlation with out-of-distribution accuracy is a strong point.' Weaknesses=\"The paper may lack empirical validation on diverse datasets or additional real-world applications, making it hard to generalize the proposed method's effectiveness widely. There might also be concerns about computational efficiency, as the calculation of vicinal scores could potentially be resource-intensive.\" Questions='How does VRP perform in a variety of real-world applications beyond typical benchmark datasets? Are there any scalability concerns with the method when applied to large-scale datasets, and how does VRP handle different types of out-of-distribution scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel graph transformer architecture that explicitly accounts for edge directionality, which addresses an important gap in existing graph transformers. It also proposes new directional graph datasets, contributing to the practical aspects of the research.' Weaknesses='The paper could benefit from clearer explanations and more detailed evaluations of alternative approaches. It is also not clear how well the model can generalize beyond the proposed datasets.' Questions='How does the method perform on existing benchmark datasets beyond the newly introduced directional graph datasets? Are there any limitations in terms of scalability or computational requirements?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel graph transformer architecture that explicitly accounts for edge directionality, which addresses an important gap in existing graph transformers. It also proposes new directional graph datasets, contributing to the practical aspects of the research.' Weaknesses='The paper could benefit from clearer explanations and more detailed evaluations of alternative approaches. It is also not clear how well the model can generalize beyond the proposed datasets.' Questions='How does the method perform on existing benchmark datasets beyond the newly introduced directional graph datasets? Are there any limitations in terms of scalability or computational requirements?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. Introduction of the Modified Silhouette Score (MSS) as a measure of graph quality is a noteworthy contribution.\\n2. The proposal of SheAttack, an efficient attack method that effectively reduces node distinguishability without needing prior knowledge of labels or the victim model, shows innovation.\\n3. The paper provides a comprehensive evaluation through experiments on both synthetic and real-world graphs.' Weaknesses=\"1. The paper's explanation of its theoretical contributions could be clearer, as the abstract relies heavily on jargon which might be difficult for general readership.\\n2. The practical applicability of SheAttack under various real-world adversarial conditions wasn't fully demonstrated in the abstract.\\n3. The assumptions or limitations of using MSS as an objective were not detailed in the abstract.\" Questions='1. How does the Modified Silhouette Score quantify graph quality better than previous metrics in practical scenarios?\\n2. Are there any specific limitations or computational constraints of the SheAttack method when applied to very large graphs?\\n3. How does the method handle varying levels of homophily in graphs, as suggested by your findings?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. Introduction of the Modified Silhouette Score (MSS) as a measure of graph quality is a noteworthy contribution.\\n2. The proposal of SheAttack, an efficient attack method that effectively reduces node distinguishability without needing prior knowledge of labels or the victim model, shows innovation.\\n3. The paper provides a comprehensive evaluation through experiments on both synthetic and real-world graphs.' Weaknesses=\"1. The paper's explanation of its theoretical contributions could be clearer, as the abstract relies heavily on jargon which might be difficult for general readership.\\n2. The practical applicability of SheAttack under various real-world adversarial conditions wasn't fully demonstrated in the abstract.\\n3. The assumptions or limitations of using MSS as an objective were not detailed in the abstract.\" Questions='1. How does the Modified Silhouette Score quantify graph quality better than previous metrics in practical scenarios?\\n2. Are there any specific limitations or computational constraints of the SheAttack method when applied to very large graphs?\\n3. How does the method handle varying levels of homophily in graphs, as suggested by your findings?'\n",
      "Finished gpt4o on data/test_data/zero_shot/test_data_2024_abstract_prompts.jsonl, saved => results/gpt4o/test_data_2024_abstract_prompts_gpt4o_results.jsonl\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The TTformer model introduces a novel approach that effectively captures the spatio-temporal dependencies in traffic forecasting, which is a significant advancement in the field. The use of tense-specific attention modules is a creative solution. The inclusion of contrastive learning for robustness further strengthens the model.' Weaknesses=\"There may be a lack of detailed explanation regarding the implementation and impact of the tense-specific attention modules and contrastive learning. The model's computational complexity and scalability in real-world settings are not addressed.\" Questions='How does the TTformer model compare in terms of computational efficiency with existing models? Are there specific scenarios where TTformer may not perform as well?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The TTformer model introduces a novel approach that effectively captures the spatio-temporal dependencies in traffic forecasting, which is a significant advancement in the field. The use of tense-specific attention modules is a creative solution. The inclusion of contrastive learning for robustness further strengthens the model.' Weaknesses=\"There may be a lack of detailed explanation regarding the implementation and impact of the tense-specific attention modules and contrastive learning. The model's computational complexity and scalability in real-world settings are not addressed.\" Questions='How does the TTformer model compare in terms of computational efficiency with existing models? Are there specific scenarios where TTformer may not perform as well?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The Overcooked Generalisation Challenge provides a novel benchmark for studying zero-shot cooperation in reinforcement learning agents. The challenge supports state-of-the-art dual curriculum design methods, is GPU-accelerated, and is open-source, which are substantial assets to the research community. The work addresses a crucial gap in existing benchmarks by focusing on generalisation abilities, setting the stage for future research in cooperative AI.' Weaknesses='The abstract raises some concerns about the limitations of the state-of-the-art algorithms when applied to this new benchmark, suggesting a potentially steep difficulty level. While this is a meaningful outcome, it highlights the complexity of the challenge without proposing specific solutions or improvements to current methods. Additionally, the abstract does not detail how the proposed challenge might be used to enhance existing algorithms or inform new approaches beyond identifying their failures.' Questions='What specific metrics or criteria are used to assess zero-shot cooperation in the Overcooked Generalisation Challenge? Can this benchmark suggest actionable insights for improving current reinforcement learning approaches or network architectures?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The Overcooked Generalisation Challenge provides a novel benchmark for studying zero-shot cooperation in reinforcement learning agents. The challenge supports state-of-the-art dual curriculum design methods, is GPU-accelerated, and is open-source, which are substantial assets to the research community. The work addresses a crucial gap in existing benchmarks by focusing on generalisation abilities, setting the stage for future research in cooperative AI.' Weaknesses='The abstract raises some concerns about the limitations of the state-of-the-art algorithms when applied to this new benchmark, suggesting a potentially steep difficulty level. While this is a meaningful outcome, it highlights the complexity of the challenge without proposing specific solutions or improvements to current methods. Additionally, the abstract does not detail how the proposed challenge might be used to enhance existing algorithms or inform new approaches beyond identifying their failures.' Questions='What specific metrics or criteria are used to assess zero-shot cooperation in the Overcooked Generalisation Challenge? Can this benchmark suggest actionable insights for improving current reinforcement learning approaches or network architectures?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses the important challenge of training language models on long contexts by focusing on data quality, specifically through long-distance referrals. The LongPack pipeline introduces a scalable and efficient approach to generate high-quality long documents, utilizing web pages and hyperlink structures, which naturally encapsulate long-distance relationships.' Weaknesses='The novelty of the approach may be limited to specific types of data, such as web pages with hyperlinks, potentially limiting its general applicability across different domains of text data. Additionally, while improvements are claimed, more rigorous evaluation and comparison with existing methods in various scenarios would strengthen the paper.' Questions=\"How well would the LongPack approach generalize to sources of information that don't inherently rely on hyperlink structures, such as books or traditional academic papers? Are there plans to enhance the pipeline to accommodate such data?\"\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses the important challenge of training language models on long contexts by focusing on data quality, specifically through long-distance referrals. The LongPack pipeline introduces a scalable and efficient approach to generate high-quality long documents, utilizing web pages and hyperlink structures, which naturally encapsulate long-distance relationships.' Weaknesses='The novelty of the approach may be limited to specific types of data, such as web pages with hyperlinks, potentially limiting its general applicability across different domains of text data. Additionally, while improvements are claimed, more rigorous evaluation and comparison with existing methods in various scenarios would strengthen the paper.' Questions=\"How well would the LongPack approach generalize to sources of information that don't inherently rely on hyperlink structures, such as books or traditional academic papers? Are there plans to enhance the pipeline to accommodate such data?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper presents a novel augmentation-free self-supervised learning method for tabular data which addresses the difficulty in constructing data augmentations for such data. It demonstrates improved performance on downstream tasks, matching or surpassing traditional methods like Gradient Boosted Decision Trees.' Weaknesses='There is a potential lack of clarity in explaining certain concepts and how the novel method compares with existing approaches. The experimental validation could be expanded to cover more diverse datasets.' Questions='How does the proposed method compare to other recent augmentation-free SSL approaches for tabular data in terms of computational efficiency and feature interpretability?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper presents a novel augmentation-free self-supervised learning method for tabular data which addresses the difficulty in constructing data augmentations for such data. It demonstrates improved performance on downstream tasks, matching or surpassing traditional methods like Gradient Boosted Decision Trees.' Weaknesses='There is a potential lack of clarity in explaining certain concepts and how the novel method compares with existing approaches. The experimental validation could be expanded to cover more diverse datasets.' Questions='How does the proposed method compare to other recent augmentation-free SSL approaches for tabular data in terms of computational efficiency and feature interpretability?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a method to enhance few-shot learning in tabular data by using range-limited augmentation, which presumes the semantic consistency in data enhancement. This approach seems to effectively preserve task-relevant information and improve performance. Furthermore, the introduction of a new benchmark, FeSTa, comprising 42 datasets and 31 algorithms, is a significant contribution to the field.' Weaknesses='The novelty of the proposed range-limited augmentation method could be limited as it builds upon established augmentation techniques from other domains such as image processing. Additionally, there is a potential narrow scope since the approach is highly specialized for tabular data. There are also concerns about the broader applicability of the method to other data types.' Questions='How does range-limited augmentation compare to traditional augmentation techniques from fields like image or text in terms of applicability and performance improvements?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a method to enhance few-shot learning in tabular data by using range-limited augmentation, which presumes the semantic consistency in data enhancement. This approach seems to effectively preserve task-relevant information and improve performance. Furthermore, the introduction of a new benchmark, FeSTa, comprising 42 datasets and 31 algorithms, is a significant contribution to the field.' Weaknesses='The novelty of the proposed range-limited augmentation method could be limited as it builds upon established augmentation techniques from other domains such as image processing. Additionally, there is a potential narrow scope since the approach is highly specialized for tabular data. There are also concerns about the broader applicability of the method to other data types.' Questions='How does range-limited augmentation compare to traditional augmentation techniques from fields like image or text in terms of applicability and performance improvements?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"The paper proposes a novel framework for agent-oriented planning in multi-agent systems by highlighting important design principles. The integration of a feedback loop enhances the effectiveness and robustness of the proposed solution. Empirical results demonstrate the framework's superiority over existing strategies.\" Weaknesses='The abstract lacks detailed information about the experiments and specific results, making it difficult to assess the full impact of the contribution. The evaluation process and reward model are not elaborated upon in the abstract.' Questions='How does the proposed framework compare quantitatively with existing methods in terms of efficiency and quality of problem-solving? What are the specific metrics used for evaluation?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"The paper proposes a novel framework for agent-oriented planning in multi-agent systems by highlighting important design principles. The integration of a feedback loop enhances the effectiveness and robustness of the proposed solution. Empirical results demonstrate the framework's superiority over existing strategies.\" Weaknesses='The abstract lacks detailed information about the experiments and specific results, making it difficult to assess the full impact of the contribution. The evaluation process and reward model are not elaborated upon in the abstract.' Questions='How does the proposed framework compare quantitatively with existing methods in terms of efficiency and quality of problem-solving? What are the specific metrics used for evaluation?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper introduces a novel framework, TransFusion, which enables cross-lingual transfer learning for low-resource languages using a creative approach of translating data into English for better model training. It demonstrates significant improvements in performance across multiple languages and tasks, including named entity recognition. The proposed method successfully leverages translation and instruction tuning, showing versatile applicability in both zero-shot learning and fine-tuned scenarios.' Weaknesses='The paper lacks detail in explaining the methodology, particularly the specifics of the TransFusion framework and how exactly the annotation fusion is implemented. While improvements are noted, the reasons behind specific performance metrics are not thoroughly analyzed, and the reliance on proprietary models like GPT-4 may limit reproducibility. The paper could also improve in providing a clearer explanation of experimental setups and baseline comparisons.' Questions='How does the translation quality into English affect the overall performance of the model, and what translation tools or methods are used? Are there specific annotated datasets used for fine-tuning that are known to be poor in quality, and how does that impact the results? Could the framework be extended beyond the named entity recognition to other NLP tasks, and what challenges might arise in doing so?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper introduces a novel framework, TransFusion, which enables cross-lingual transfer learning for low-resource languages using a creative approach of translating data into English for better model training. It demonstrates significant improvements in performance across multiple languages and tasks, including named entity recognition. The proposed method successfully leverages translation and instruction tuning, showing versatile applicability in both zero-shot learning and fine-tuned scenarios.' Weaknesses='The paper lacks detail in explaining the methodology, particularly the specifics of the TransFusion framework and how exactly the annotation fusion is implemented. While improvements are noted, the reasons behind specific performance metrics are not thoroughly analyzed, and the reliance on proprietary models like GPT-4 may limit reproducibility. The paper could also improve in providing a clearer explanation of experimental setups and baseline comparisons.' Questions='How does the translation quality into English affect the overall performance of the model, and what translation tools or methods are used? Are there specific annotated datasets used for fine-tuning that are known to be poor in quality, and how does that impact the results? Could the framework be extended beyond the named entity recognition to other NLP tasks, and what challenges might arise in doing so?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=3 Strengths='The paper presents a novel differentiable algorithm for causal discovery in nonlinear latent hierarchical models which addresses scalability and applicability issues faced by existing methods. The approach improves both accuracy and scalability, showing practical utility in handling high-dimensional data.' Weaknesses='The presentation could be clearer in explanations of some technical aspects, and some baseline comparisons to domain literature, especially for empirical results, seem underexplored.' Questions='How does the proposed method compare to existing methods in terms of computational efficiency beyond the reported datasets, and how generalizable are the learned structures in various domains?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=3 Strengths='The paper presents a novel differentiable algorithm for causal discovery in nonlinear latent hierarchical models which addresses scalability and applicability issues faced by existing methods. The approach improves both accuracy and scalability, showing practical utility in handling high-dimensional data.' Weaknesses='The presentation could be clearer in explanations of some technical aspects, and some baseline comparisons to domain literature, especially for empirical results, seem underexplored.' Questions='How does the proposed method compare to existing methods in terms of computational efficiency beyond the reported datasets, and how generalizable are the learned structures in various domains?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a novel architecture, Spatial 3D-LLM, that improves spatial perception in 3D vision-language models, which is a crucial area for advancing AI capabilities in understanding complex environments. It also introduces new tasks and a dataset, which could drive further research in the field.' Weaknesses='The paper lacks detailed comparisons with existing state-of-the-art models and does not provide a comprehensive analysis of the potential limitations of the proposed method. The novelty of the tasks and dataset, while interesting, could be better contextualized within the current research landscape.' Questions='How does the proposed model perform compared to specific existing benchmarks? What are the specific limitations of the method, if any, when applied to more complex 3D scenes?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a novel architecture, Spatial 3D-LLM, that improves spatial perception in 3D vision-language models, which is a crucial area for advancing AI capabilities in understanding complex environments. It also introduces new tasks and a dataset, which could drive further research in the field.' Weaknesses='The paper lacks detailed comparisons with existing state-of-the-art models and does not provide a comprehensive analysis of the potential limitations of the proposed method. The novelty of the tasks and dataset, while interesting, could be better contextualized within the current research landscape.' Questions='How does the proposed model perform compared to specific existing benchmarks? What are the specific limitations of the method, if any, when applied to more complex 3D scenes?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel approach for explainable graph representation learning by leveraging graph pattern analysis. It presents theoretical analyses regarding robustness and generalization, which is commendable.' Weaknesses='The approach might have limitations concerning computational complexity due to pattern analysis. Node feature integration is not thoroughly discussed. The presentation could be improved with more detailed explanations and connections to existing methods.' Questions='How does the proposed method compare in terms of computational efficiency with existing approaches? Are there any specific limitations or challenges in extending this method to more complex graph structures?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel approach for explainable graph representation learning by leveraging graph pattern analysis. It presents theoretical analyses regarding robustness and generalization, which is commendable.' Weaknesses='The approach might have limitations concerning computational complexity due to pattern analysis. Node feature integration is not thoroughly discussed. The presentation could be improved with more detailed explanations and connections to existing methods.' Questions='How does the proposed method compare in terms of computational efficiency with existing approaches? Are there any specific limitations or challenges in extending this method to more complex graph structures?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths=\"The work provides a substantial contribution to the field of egocentric video understanding by generating a large dataset of QA samples and proposing a novel multimodal architecture with a unique 'Memory Pointer Prompting' mechanism. The presentation of the paper is clear and logically structured, enabling the reader to follow the methodology and results.\" Weaknesses='The first contribution regarding the creation of a QA dataset may not be as novel as claimed, considering similar approaches in the field. Additionally, the presented benchmark might not be sufficiently distinct from existing benchmarks to warrant being highlighted as a major contribution without further comparison to other datasets.' Questions='How does the novel architecture perform when compared to existing models using different datasets? Can the dataset and benchmark be further enriched by leveraging other existing datasets in the field?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths=\"The work provides a substantial contribution to the field of egocentric video understanding by generating a large dataset of QA samples and proposing a novel multimodal architecture with a unique 'Memory Pointer Prompting' mechanism. The presentation of the paper is clear and logically structured, enabling the reader to follow the methodology and results.\" Weaknesses='The first contribution regarding the creation of a QA dataset may not be as novel as claimed, considering similar approaches in the field. Additionally, the presented benchmark might not be sufficiently distinct from existing benchmarks to warrant being highlighted as a major contribution without further comparison to other datasets.' Questions='How does the novel architecture perform when compared to existing models using different datasets? Can the dataset and benchmark be further enriched by leveraging other existing datasets in the field?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper provides convergent R\\\\'enyi DP bounds for non-convex non-smooth losses, extending results by relaxing assumptions to Holder continuous gradients. This answers open questions in recent works. It also improves upon privacy bounds for smooth strongly convex losses. Novel approaches like forward Wasserstein distance tracking and optimal shift allocation illustrate a clear and logical methodology.\" Weaknesses='The paper may not clearly contrast practical differences from existing methods or directly demonstrate significant real-world applications. There is a reliance on theoretical constructs which may limit practical applicability without further development. Some technical aspects may be challenging for non-experts to grasp without additional supporting material or simplified explanations.' Questions='Does the Holder continuous gradient condition apply to widely-used loss functions in practice, or is it more of a theoretical interest? Can the authors provide intuitive examples or applications where their improved privacy bounds decisively outperform existing methods under realistic settings?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper provides convergent R\\\\'enyi DP bounds for non-convex non-smooth losses, extending results by relaxing assumptions to Holder continuous gradients. This answers open questions in recent works. It also improves upon privacy bounds for smooth strongly convex losses. Novel approaches like forward Wasserstein distance tracking and optimal shift allocation illustrate a clear and logical methodology.\" Weaknesses='The paper may not clearly contrast practical differences from existing methods or directly demonstrate significant real-world applications. There is a reliance on theoretical constructs which may limit practical applicability without further development. Some technical aspects may be challenging for non-experts to grasp without additional supporting material or simplified explanations.' Questions='Does the Holder continuous gradient condition apply to widely-used loss functions in practice, or is it more of a theoretical interest? Can the authors provide intuitive examples or applications where their improved privacy bounds decisively outperform existing methods under realistic settings?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a new analytical framework, Neuron Predictability Lens (NPL), that offers insights into how neurons function within feed-forward networks of large language models. This approach can uncover the predictability of neuron activations and has implications for improving model efficiency and effectiveness.' Weaknesses='Some aspects of the experiment, such as the high PPL values on LLAMA-2 and GPT-J, suggest potential issues in experimental setup or model convergence. The explanation of certain equations and methodologies in the paper could be clearer.' Questions='How does the NPL specifically enhance our understanding of transformer models beyond existing analysis techniques? Is there evidence that models optimized using NPL insights can outperform traditional fine-tuned models?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a new analytical framework, Neuron Predictability Lens (NPL), that offers insights into how neurons function within feed-forward networks of large language models. This approach can uncover the predictability of neuron activations and has implications for improving model efficiency and effectiveness.' Weaknesses='Some aspects of the experiment, such as the high PPL values on LLAMA-2 and GPT-J, suggest potential issues in experimental setup or model convergence. The explanation of certain equations and methodologies in the paper could be clearer.' Questions='How does the NPL specifically enhance our understanding of transformer models beyond existing analysis techniques? Is there evidence that models optimized using NPL insights can outperform traditional fine-tuned models?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The method offers a novel approach by leveraging pre-trained image diffusion models for video restoration, allowing it to perform well across various degradation types without re-training. The use of a hierarchical latent warping strategy and hybrid correspondence mechanism is innovative and shows significant generalization capabilities.' Weaknesses='The paper may face challenges in reproducibility due to its reliance on specific pre-trained models and possibly intricate implementation details. Some claims of superiority over trained models might require more extensive benchmarks.' Questions='How does the method handle computational complexity, especially concerning the hierarchical warping and hybrid correspondence? Are there any limitations on the types of pre-trained diffusion models that can be used effectively?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The method offers a novel approach by leveraging pre-trained image diffusion models for video restoration, allowing it to perform well across various degradation types without re-training. The use of a hierarchical latent warping strategy and hybrid correspondence mechanism is innovative and shows significant generalization capabilities.' Weaknesses='The paper may face challenges in reproducibility due to its reliance on specific pre-trained models and possibly intricate implementation details. Some claims of superiority over trained models might require more extensive benchmarks.' Questions='How does the method handle computational complexity, especially concerning the hierarchical warping and hybrid correspondence? Are there any limitations on the types of pre-trained diffusion models that can be used effectively?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses the novel problem of privacy risks in graph prompt learning with GNNs, an area not previously explored. It proposes a new approach using differential privacy to mitigate these risks, with strong theoretical grounding.' Weaknesses='The paper could benefit from more extensive experimental validation, particularly regarding the practical effectiveness of the proposed methods across a wider range of scenarios. The discussion on the limitations of the proposed methods could be more thorough.' Questions='How does the proposed method compare with existing privacy-preserving approaches in terms of computational efficiency? Are there specific graph types or domains where the approach is more or less effective?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses the novel problem of privacy risks in graph prompt learning with GNNs, an area not previously explored. It proposes a new approach using differential privacy to mitigate these risks, with strong theoretical grounding.' Weaknesses='The paper could benefit from more extensive experimental validation, particularly regarding the practical effectiveness of the proposed methods across a wider range of scenarios. The discussion on the limitations of the proposed methods could be more thorough.' Questions='How does the proposed method compare with existing privacy-preserving approaches in terms of computational efficiency? Are there specific graph types or domains where the approach is more or less effective?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel Sparse Matrix Tuning method that reduces the performance gap between parameter-efficient fine-tuning and full fine-tuning, while also reducing computational and memory costs. It demonstrates consistent performance improvements over other baseline methods in experiments with large language models.' Weaknesses='The abstract does not provide detailed information on the theoretical foundations of the Sparse Matrix Tuning method, or how it specifically selects the most significant sub-matrices. There may be a lack of a thorough comparison with existing state-of-the-art methods in parameter-efficient fine-tuning.' Questions='How does the Sparse Matrix Tuning method specifically identify and select the most significant sub-matrices for updating during the fine-tuning process? Are there any theoretical guarantees on the performance improvement of SMT over other precise methods like LoRA and DoRA?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel Sparse Matrix Tuning method that reduces the performance gap between parameter-efficient fine-tuning and full fine-tuning, while also reducing computational and memory costs. It demonstrates consistent performance improvements over other baseline methods in experiments with large language models.' Weaknesses='The abstract does not provide detailed information on the theoretical foundations of the Sparse Matrix Tuning method, or how it specifically selects the most significant sub-matrices. There may be a lack of a thorough comparison with existing state-of-the-art methods in parameter-efficient fine-tuning.' Questions='How does the Sparse Matrix Tuning method specifically identify and select the most significant sub-matrices for updating during the fine-tuning process? Are there any theoretical guarantees on the performance improvement of SMT over other precise methods like LoRA and DoRA?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths=\"The paper introduces a novel architecture, CrossModalNet, for multimodal medical image segmentation, addressing complex integration challenges with advanced mathematical frameworks and a new domain adaptation technique. The theoretical analysis provides strong justification for the architecture's capabilities, and the introduction of the CMIF metric offers a new insight into the progressive integration of multimodal data.\" Weaknesses='The paper may require more extensive validation across diverse datasets to establish its general applicability and robustness. The dependence on mathematical proofs may limit accessibility to a more general audience without a strong mathematical background.' Questions='How does CrossModalNet perform on datasets beyond MM-WHS, particularly those with different types and qualities of multimodal data? What are the computational costs associated with the proposed architecture compared to existing models?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths=\"The paper introduces a novel architecture, CrossModalNet, for multimodal medical image segmentation, addressing complex integration challenges with advanced mathematical frameworks and a new domain adaptation technique. The theoretical analysis provides strong justification for the architecture's capabilities, and the introduction of the CMIF metric offers a new insight into the progressive integration of multimodal data.\" Weaknesses='The paper may require more extensive validation across diverse datasets to establish its general applicability and robustness. The dependence on mathematical proofs may limit accessibility to a more general audience without a strong mathematical background.' Questions='How does CrossModalNet perform on datasets beyond MM-WHS, particularly those with different types and qualities of multimodal data? What are the computational costs associated with the proposed architecture compared to existing models?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper addresses a significant challenge in multi-view clustering and incorporates structure information effectively. The proposed method is innovative, using heterogeneous random walks to enhance clustering performance, which is systematically evaluated across five real-world datasets.' Weaknesses='The paper lacks theoretical analysis regarding the convergence and complexity of the proposed algorithm. The explanation for why the new method outperforms others is not deeply explored, leaving some questions about its underlying advantages.' Questions='What are the time and space complexities of the proposed method? Is there a theoretical guarantee for the convergence? How does the method theoretically ensure improvement in clustering performance compared to existing methods?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper addresses a significant challenge in multi-view clustering and incorporates structure information effectively. The proposed method is innovative, using heterogeneous random walks to enhance clustering performance, which is systematically evaluated across five real-world datasets.' Weaknesses='The paper lacks theoretical analysis regarding the convergence and complexity of the proposed algorithm. The explanation for why the new method outperforms others is not deeply explored, leaving some questions about its underlying advantages.' Questions='What are the time and space complexities of the proposed method? Is there a theoretical guarantee for the convergence? How does the method theoretically ensure improvement in clustering performance compared to existing methods?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper introduces h-SVGD, which addresses variance collapse in SVGD without additional computational cost. It provides rigorous theoretical backing for the proposed algorithm, including existence of solutions, characterization of fixed points, and other properties.' Weaknesses='The presentation could be improved; some parts are hard to follow due to lack of intuitions and commentary. Several assumptions lack discussion, and there are minor typos and inconsistencies in the notation that can lead to confusion.' Questions='Could you further clarify Assumptions (A1)-(A3) and provide examples of potentials that meet these conditions? How does the h-SVGD compare to other SVGD variants in practice beyond variance collapse alleviation?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper introduces h-SVGD, which addresses variance collapse in SVGD without additional computational cost. It provides rigorous theoretical backing for the proposed algorithm, including existence of solutions, characterization of fixed points, and other properties.' Weaknesses='The presentation could be improved; some parts are hard to follow due to lack of intuitions and commentary. Several assumptions lack discussion, and there are minor typos and inconsistencies in the notation that can lead to confusion.' Questions='Could you further clarify Assumptions (A1)-(A3) and provide examples of potentials that meet these conditions? How does the h-SVGD compare to other SVGD variants in practice beyond variance collapse alleviation?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel activation function that addresses spectral bias in neural networks, significantly improving signal reconstruction capabilities. It demonstrates superior performance against established methods in terms of accuracy and convergence speed. The extensive experimental evaluation shows its potential for real-world applications.' Weaknesses='The paper may lack detailed analysis on potential limitations or specific scenarios where the proposed method may not perform optimally compared to conventional methods. Further discussion on computational overhead and practical deployment would strengthen the contribution.' Questions='How does the implementation complexity of STAF compare to conventional methods like ReLU or SIREN, especially in real-time applications? Are there specific types of signals or datasets where STAF might not provide significant advantages?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel activation function that addresses spectral bias in neural networks, significantly improving signal reconstruction capabilities. It demonstrates superior performance against established methods in terms of accuracy and convergence speed. The extensive experimental evaluation shows its potential for real-world applications.' Weaknesses='The paper may lack detailed analysis on potential limitations or specific scenarios where the proposed method may not perform optimally compared to conventional methods. Further discussion on computational overhead and practical deployment would strengthen the contribution.' Questions='How does the implementation complexity of STAF compare to conventional methods like ReLU or SIREN, especially in real-time applications? Are there specific types of signals or datasets where STAF might not provide significant advantages?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel method called GUIDE that improves the alignment of LLM outputs with user instructions by increasing attention scores on instruction tokens. This approach demonstrates a substantial improvement in accuracy, outperforming natural prompting alternatives.' Weaknesses='The paper lacks detailed comparison with other state-of-the-art methods. The proposed new metric, Influence, needs more validation to establish its effectiveness comprehensively.' Questions='How does the performance of GUIDE vary with different types of user instructions and model architectures? Can Influence be generalized across different LLMs?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel method called GUIDE that improves the alignment of LLM outputs with user instructions by increasing attention scores on instruction tokens. This approach demonstrates a substantial improvement in accuracy, outperforming natural prompting alternatives.' Weaknesses='The paper lacks detailed comparison with other state-of-the-art methods. The proposed new metric, Influence, needs more validation to establish its effectiveness comprehensively.' Questions='How does the performance of GUIDE vary with different types of user instructions and model architectures? Can Influence be generalized across different LLMs?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper presents an innovative application of Monte-Carlo Tree Search (MCTS) to GFlowNet training, illustrating both theoretical and practical improvements over standard sampling methods. The work is well-situated within the context of existing research and draws on known connections between GFlowNets and maximum-entropy reinforcement learning. The extensive experiments across various discrete domains demonstrate the effectiveness of the proposed method.' Weaknesses=\"Some aspects of the experimental methodology, such as the representation of statistical significance in results, could be improved for clarity and reliability. While the adaptation of MCTS for GFlowNets is an interesting advancement, more detailed analysis or comparison with alternative sampling methods would strengthen the paper's contributions.\" Questions='How does the proposed MCTS adaptation compare to other non-MCTS-based sampling strategies for GFlowNets in terms of efficiency and performance? Can the authors provide more insight into the computational complexity of their proposed method compared to the baseline?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper presents an innovative application of Monte-Carlo Tree Search (MCTS) to GFlowNet training, illustrating both theoretical and practical improvements over standard sampling methods. The work is well-situated within the context of existing research and draws on known connections between GFlowNets and maximum-entropy reinforcement learning. The extensive experiments across various discrete domains demonstrate the effectiveness of the proposed method.' Weaknesses=\"Some aspects of the experimental methodology, such as the representation of statistical significance in results, could be improved for clarity and reliability. While the adaptation of MCTS for GFlowNets is an interesting advancement, more detailed analysis or comparison with alternative sampling methods would strengthen the paper's contributions.\" Questions='How does the proposed MCTS adaptation compare to other non-MCTS-based sampling strategies for GFlowNets in terms of efficiency and performance? Can the authors provide more insight into the computational complexity of their proposed method compared to the baseline?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel approach to motion generation using masked transformers, demonstrating superior performance against state-of-the-art GAN and diffusion-based methods. The use of quantization and codebook construction for motion patterns is innovative. Experiments are comprehensive, showing effectiveness in various tasks.' Weaknesses='The paper does not articulate the unique contributions clearly in comparison with existing works, leading to some difficulty in assessing the novelty. Overfitting concerns with small datasets are mentioned but not deeply explored. The evaluation could include more quantitative metrics for diversity and faithfulness.' Questions='How does MotionDreamer specifically handle overfitting with small datasets in comparison to other methods? Are there quantitative results for diversity and faithfulness beyond qualitative evaluation?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel approach to motion generation using masked transformers, demonstrating superior performance against state-of-the-art GAN and diffusion-based methods. The use of quantization and codebook construction for motion patterns is innovative. Experiments are comprehensive, showing effectiveness in various tasks.' Weaknesses='The paper does not articulate the unique contributions clearly in comparison with existing works, leading to some difficulty in assessing the novelty. Overfitting concerns with small datasets are mentioned but not deeply explored. The evaluation could include more quantitative metrics for diversity and faithfulness.' Questions='How does MotionDreamer specifically handle overfitting with small datasets in comparison to other methods? Are there quantitative results for diversity and faithfulness beyond qualitative evaluation?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='The paper proposes a novel method, AgentWrite, which effectively tackles the problem of output length limitation in current LLMs by decomposing ultra-long generation tasks into subtasks. The approach is innovative and demonstrates significant improvements in output length capability. The introduction of the LongWriter-6k dataset and the LongBench-Write benchmark are valuable contributions to the field, providing tools for further research and development.' Weaknesses='The experiments might lack variety in terms of different model architectures or dataset domains, which could provide a more comprehensive evaluation. Additionally, the reliance on a new dataset for improving model performance might suggest a lack of generalizability across different datasets.' Questions='1. How does AgentWrite handle context coherence across decomposed subtasks, especially for very long outputs?\\n2. Are there any potential limitations or bottlenecks observed when scaling the output lengths beyond the tested scenarios?\\n3. How does the method perform across various LLM architectures or on tasks/domains not covered in the paper?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='The paper proposes a novel method, AgentWrite, which effectively tackles the problem of output length limitation in current LLMs by decomposing ultra-long generation tasks into subtasks. The approach is innovative and demonstrates significant improvements in output length capability. The introduction of the LongWriter-6k dataset and the LongBench-Write benchmark are valuable contributions to the field, providing tools for further research and development.' Weaknesses='The experiments might lack variety in terms of different model architectures or dataset domains, which could provide a more comprehensive evaluation. Additionally, the reliance on a new dataset for improving model performance might suggest a lack of generalizability across different datasets.' Questions='1. How does AgentWrite handle context coherence across decomposed subtasks, especially for very long outputs?\\n2. Are there any potential limitations or bottlenecks observed when scaling the output lengths beyond the tested scenarios?\\n3. How does the method perform across various LLM architectures or on tasks/domains not covered in the paper?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel Ada-K routing strategy that dynamically adjusts the number of activated experts for each token, leading to improved balance between computational efficiency and model performance. It shows significant gains over conventional Top-K routing in terms of FLOPs reduction and inference speedup while improving performance on various benchmarks. The proposed method is practically applicable across all mainstream MoE-based LLMs and provides valuable insights for future adaptive MoE system designs.' Weaknesses=\"The paper may lack a comparison with other state-of-the-art MoE approaches or methods beyond Top-K, which could provide a more comprehensive evaluation of the proposed method's effectiveness. Additionally, the scalability and generalization of the Ada-K strategy to other large-scale models not discussed in the paper may require further investigation.\" Questions='How does the Ada-K strategy perform when scaled to even larger models or different architectures beyond the ones evaluated in the paper? Furthermore, what are the potential challenges in implementing this routing strategy in a real-world environment, considering factors like hardware compatibility and deployment complexity?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel Ada-K routing strategy that dynamically adjusts the number of activated experts for each token, leading to improved balance between computational efficiency and model performance. It shows significant gains over conventional Top-K routing in terms of FLOPs reduction and inference speedup while improving performance on various benchmarks. The proposed method is practically applicable across all mainstream MoE-based LLMs and provides valuable insights for future adaptive MoE system designs.' Weaknesses=\"The paper may lack a comparison with other state-of-the-art MoE approaches or methods beyond Top-K, which could provide a more comprehensive evaluation of the proposed method's effectiveness. Additionally, the scalability and generalization of the Ada-K strategy to other large-scale models not discussed in the paper may require further investigation.\" Questions='How does the Ada-K strategy perform when scaled to even larger models or different architectures beyond the ones evaluated in the paper? Furthermore, what are the potential challenges in implementing this routing strategy in a real-world environment, considering factors like hardware compatibility and deployment complexity?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces LoCoDL, a novel approach combining local training and compression, proving to have accelerated communication complexity with theoretical backing.' Weaknesses='The paper may lack extensive empirical validation across diverse settings and potentially insufficient exploration of non-strongly convex cases.' Questions='How does LoCoDL perform in non-convex settings, and are there any limitations in terms of model scale or specific compressor types?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces LoCoDL, a novel approach combining local training and compression, proving to have accelerated communication complexity with theoretical backing.' Weaknesses='The paper may lack extensive empirical validation across diverse settings and potentially insufficient exploration of non-strongly convex cases.' Questions='How does LoCoDL perform in non-convex settings, and are there any limitations in terms of model scale or specific compressor types?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides insightful analysis on how architectural choices affect transformer models, particularly highlighting the influence of various hyperparameters on learning strategies. The theoretical characterization of two distinct counting strategies within transformers adds depth to our understanding of model behaviors.' Weaknesses='The study is limited to simplistic transformer settings, focusing on single-layer models and a single task (histogram counting). This diminishes the broader applicability of the findings to more complex, real-world transformer applications. Additionally, the paper does not provide guidance on extending these insights to multilayer transformers or more diverse tasks.' Questions='How can the findings from the histogram task be generalized to more complex tasks or multi-layer transformer architectures? Are there potential implications or experiments that can explore these characterizations in a broader context?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides insightful analysis on how architectural choices affect transformer models, particularly highlighting the influence of various hyperparameters on learning strategies. The theoretical characterization of two distinct counting strategies within transformers adds depth to our understanding of model behaviors.' Weaknesses='The study is limited to simplistic transformer settings, focusing on single-layer models and a single task (histogram counting). This diminishes the broader applicability of the findings to more complex, real-world transformer applications. Additionally, the paper does not provide guidance on extending these insights to multilayer transformers or more diverse tasks.' Questions='How can the findings from the histogram task be generalized to more complex tasks or multi-layer transformer architectures? Are there potential implications or experiments that can explore these characterizations in a broader context?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=5 Strengths='The dataset provides a large collection of over 25 million images across 10 modalities with comprehensive multigranular annotations. The use of an automated pipeline for generating these annotations from image-ROI-description triplets is innovative. The dataset supports a wide range of tasks and the newly proposed LLaVA-Tri model achieves state-of-the-art results, showcasing its utility.' Weaknesses='The abstract lacks specific details about the methodology used in the automated pipeline. It is unclear how the dataset addresses potential quality and bias issues arising from the diversity of sources.' Questions='How does the automated pipeline ensure high-quality and accurate annotations? Are there any measures taken to mitigate potential biases in the dataset?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=5 Strengths='The dataset provides a large collection of over 25 million images across 10 modalities with comprehensive multigranular annotations. The use of an automated pipeline for generating these annotations from image-ROI-description triplets is innovative. The dataset supports a wide range of tasks and the newly proposed LLaVA-Tri model achieves state-of-the-art results, showcasing its utility.' Weaknesses='The abstract lacks specific details about the methodology used in the automated pipeline. It is unclear how the dataset addresses potential quality and bias issues arising from the diversity of sources.' Questions='How does the automated pipeline ensure high-quality and accurate annotations? Are there any measures taken to mitigate potential biases in the dataset?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=3 Strengths='The paper addresses a critical issue in the deployment of large language models, offering a novel method (RRS) that improves quantization without the drawbacks of existing techniques. This method shows a clear improvement in perplexity for INT4 inference, indicating its potential practical impact.' Weaknesses='The implementation details and potential computational overhead of the proposed methods are not thoroughly discussed, which may impact their real-world applicability. Additionally, there is no discussion on the scalability of the approach for different model sizes or types.' Questions='How does the proposed method RRS impact computational efficiency compared to existing quantization methods? Is there any planned investigation on its scalability across other model architectures?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=3 Strengths='The paper addresses a critical issue in the deployment of large language models, offering a novel method (RRS) that improves quantization without the drawbacks of existing techniques. This method shows a clear improvement in perplexity for INT4 inference, indicating its potential practical impact.' Weaknesses='The implementation details and potential computational overhead of the proposed methods are not thoroughly discussed, which may impact their real-world applicability. Additionally, there is no discussion on the scalability of the approach for different model sizes or types.' Questions='How does the proposed method RRS impact computational efficiency compared to existing quantization methods? Is there any planned investigation on its scalability across other model architectures?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=5 Strengths='1. The paper presents a novel task for one-shot medical video object segmentation, addressing the challenge of limited annotated video data in the medical domain by leveraging labeled static images.\\n2. The proposed framework is innovative, including a memory mechanism for spatiotemporal context utilization and a self-distillation strategy, demonstrating effective adaptation from static images to video segmentation.\\n3. The contribution of the OS-I2V-Seg dataset greatly facilitates future research in the community, providing a robust benchmark with diverse categories and extensive data points.' Weaknesses='1. The lack of explicit implementation details and potential limitations concerning the complexity and computational efficiency of the proposed method might hinder reproducibility and practical application without further clarification.\\n2. While the paper demonstrates strong baseline performance, the generalization of the method across different medical imaging modalities remains uncertain, given that only four video categories are included.' Questions='1. How does the efficacy of the proposed method vary across different medical imaging modalities, and is there potential for domain-specific optimization?\\n2. Can the memory mechanism and self-distillation strategy be generalized or extended to non-medical video applications, potentially improving the adaptability of the approach?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=5 Strengths='1. The paper presents a novel task for one-shot medical video object segmentation, addressing the challenge of limited annotated video data in the medical domain by leveraging labeled static images.\\n2. The proposed framework is innovative, including a memory mechanism for spatiotemporal context utilization and a self-distillation strategy, demonstrating effective adaptation from static images to video segmentation.\\n3. The contribution of the OS-I2V-Seg dataset greatly facilitates future research in the community, providing a robust benchmark with diverse categories and extensive data points.' Weaknesses='1. The lack of explicit implementation details and potential limitations concerning the complexity and computational efficiency of the proposed method might hinder reproducibility and practical application without further clarification.\\n2. While the paper demonstrates strong baseline performance, the generalization of the method across different medical imaging modalities remains uncertain, given that only four video categories are included.' Questions='1. How does the efficacy of the proposed method vary across different medical imaging modalities, and is there potential for domain-specific optimization?\\n2. Can the memory mechanism and self-distillation strategy be generalized or extended to non-medical video applications, potentially improving the adaptability of the approach?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper proposes a novel method, DCScore, which addresses diversity evaluation in LLM-generated datasets. The method is theoretically grounded and offers lower computational costs than existing approaches. It also unifies existing evaluation methods, providing a versatile tool for dataset diversity evaluation.' Weaknesses='The discussion on the real-world impact and practical applications of DCScore is limited. Additionally, the explanation of how DCScore achieves its lower computational cost could be clearer. The experimental validation, while promising, could be more robust with additional datasets and scenarios.' Questions='How does DCScore specifically address the lack of diversity in LLM-generated datasets differently from other existing methods? What are potential limitations or biases introduced by using a classification perspective for diversity evaluation?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper proposes a novel method, DCScore, which addresses diversity evaluation in LLM-generated datasets. The method is theoretically grounded and offers lower computational costs than existing approaches. It also unifies existing evaluation methods, providing a versatile tool for dataset diversity evaluation.' Weaknesses='The discussion on the real-world impact and practical applications of DCScore is limited. Additionally, the explanation of how DCScore achieves its lower computational cost could be clearer. The experimental validation, while promising, could be more robust with additional datasets and scenarios.' Questions='How does DCScore specifically address the lack of diversity in LLM-generated datasets differently from other existing methods? What are potential limitations or biases introduced by using a classification perspective for diversity evaluation?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses the instability problem of gradient pruning in large language models caused by weight perturbations. It proposes a novel pruning method, MoreauPruner, with a theoretical basis in optimization analysis and robustness claims against such perturbations. The method is thoroughly evaluated on multiple large language models, showing improvements over existing pruning techniques.' Weaknesses=\"The explanation of the added value and necessity of robustness against weight perturbations could be clearer. The performance improvements illustrated in the paper, while present, are not overwhelming and the justification of why smaller 'Diff' in perturbations is significant still needs reinforcement. Moreover, the restricted set of models evaluated limits the generalizability of the results.\" Questions='Why specifically is robustness to small perturbations crucial in practice beyond numerical type error considerations? How do the performance gains of MoreauPruner compare in terms of computational cost to other pruning methods? Are there potential applications outside of the tested LLaMA family that could benefit from MoreauPruner?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses the instability problem of gradient pruning in large language models caused by weight perturbations. It proposes a novel pruning method, MoreauPruner, with a theoretical basis in optimization analysis and robustness claims against such perturbations. The method is thoroughly evaluated on multiple large language models, showing improvements over existing pruning techniques.' Weaknesses=\"The explanation of the added value and necessity of robustness against weight perturbations could be clearer. The performance improvements illustrated in the paper, while present, are not overwhelming and the justification of why smaller 'Diff' in perturbations is significant still needs reinforcement. Moreover, the restricted set of models evaluated limits the generalizability of the results.\" Questions='Why specifically is robustness to small perturbations crucial in practice beyond numerical type error considerations? How do the performance gains of MoreauPruner compare in terms of computational cost to other pruning methods? Are there potential applications outside of the tested LLaMA family that could benefit from MoreauPruner?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The paper addresses an important problem in evaluating out-of-distribution (OOD) robustness in computer vision models with innovative contributions to the field.\\n2. The introduction of the LAION-C benchmark provides a novel way of evaluating modern models that have been exposed to web-scale datasets, offering a fresh perspective compared to standard benchmarks like ImageNet-C.\\n3. The approach includes a rigorous methodology, including psychophysical experiments comparing human robustness to model performance, which enhances the credibility and comprehensive nature of the results.' Weaknesses='1. The paper does not fully explore potential limitations or biases in the proposed LAION-C benchmark and does not address how these could impact the evaluation of model robustness.\\n2. While the paper introduces a new OOD benchmark, it may lack a detailed discussion on how to integrate these results into a broader understanding of model performance on real-world tasks, potentially limiting practical applicability.' Questions='1. How do the authors plan to address any potential class imbalances between LAION-C and web-scale datasets during evaluation and analysis?\\n2. Are there any plans to expand the types or severity of distortions in LAION-C to account for future developments in model training and dataset collection?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The paper addresses an important problem in evaluating out-of-distribution (OOD) robustness in computer vision models with innovative contributions to the field.\\n2. The introduction of the LAION-C benchmark provides a novel way of evaluating modern models that have been exposed to web-scale datasets, offering a fresh perspective compared to standard benchmarks like ImageNet-C.\\n3. The approach includes a rigorous methodology, including psychophysical experiments comparing human robustness to model performance, which enhances the credibility and comprehensive nature of the results.' Weaknesses='1. The paper does not fully explore potential limitations or biases in the proposed LAION-C benchmark and does not address how these could impact the evaluation of model robustness.\\n2. While the paper introduces a new OOD benchmark, it may lack a detailed discussion on how to integrate these results into a broader understanding of model performance on real-world tasks, potentially limiting practical applicability.' Questions='1. How do the authors plan to address any potential class imbalances between LAION-C and web-scale datasets during evaluation and analysis?\\n2. Are there any plans to expand the types or severity of distortions in LAION-C to account for future developments in model training and dataset collection?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper offers a novel integration of Dynamic Mode Decomposition (DMD) with Graph Neural Networks (GNNs), providing a unique perspective on utilizing dynamic systems theory for graph-based learning tasks. The authors validate their approach with extensive experiments, demonstrating superior performance on a variety of challenging tasks, highlighting the potential to capture complex graph dynamics efficiently.' Weaknesses=\"The motivation for using DMD over other existing dynamic analysis methods in the context of GNNs is not fully justified. While results highlight performance improvements, further discussion on computational efficiency and scalability, especially on very large-scale graphs, would strengthen the paper. Additionally, clarity in explaining how DMD's dynamic operator aligns with real-world interactions in graphs could be improved.\" Questions='How does the computational complexity of the proposed DMD-GNN compare with traditional GNN methods in large-scale applications? Can the authors provide further insights on potential domain-specific applications where this approach might particularly excel beyond traditional methods?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper offers a novel integration of Dynamic Mode Decomposition (DMD) with Graph Neural Networks (GNNs), providing a unique perspective on utilizing dynamic systems theory for graph-based learning tasks. The authors validate their approach with extensive experiments, demonstrating superior performance on a variety of challenging tasks, highlighting the potential to capture complex graph dynamics efficiently.' Weaknesses=\"The motivation for using DMD over other existing dynamic analysis methods in the context of GNNs is not fully justified. While results highlight performance improvements, further discussion on computational efficiency and scalability, especially on very large-scale graphs, would strengthen the paper. Additionally, clarity in explaining how DMD's dynamic operator aligns with real-world interactions in graphs could be improved.\" Questions='How does the computational complexity of the proposed DMD-GNN compare with traditional GNN methods in large-scale applications? Can the authors provide further insights on potential domain-specific applications where this approach might particularly excel beyond traditional methods?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=2 Rating=6 Confidence=4 Strengths='The paper provides a novel analysis for choosing hyperparameters in the LiSSA algorithm based on spectral properties of the Hessian, which could potentially improve the practicality of influence functions for large models. The use of random sketching to evaluate these spectral properties is a new approach and adds to the field by proposing a scalable solution to a known problem. The empirical validation against PBRF is well-conceived and helps to confirm the theoretical findings.' Weaknesses='The main theoretical contribution may rely on strong assumptions that can be challenging to verify in practice, leaving the applicability under practical scenarios uncertain. The presentation of empirical results could be clearer, with some conclusions being difficult to align directly with the provided figures and data. Additionally, while the parameter selection method is discussed, it lacks explicit details and guidance for practitioners, potentially limiting its utility.' Questions='1. How do the assumptions required for the main theoretical contribution apply to real-world tasks and models, especially given the conditions outlined?\\n2. Can the authors provide a clear, step-by-step example of parameter selection for LiSSA in a specific scenario?\\n3. Are there any specific model sizes or architectures where the proposed method might struggle, based on current evaluations?\\n4. How do the authors suggest validating the assumptions on spectral properties in diverse datasets outside of those empirically tested?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=2 Rating=6 Confidence=4 Strengths='The paper provides a novel analysis for choosing hyperparameters in the LiSSA algorithm based on spectral properties of the Hessian, which could potentially improve the practicality of influence functions for large models. The use of random sketching to evaluate these spectral properties is a new approach and adds to the field by proposing a scalable solution to a known problem. The empirical validation against PBRF is well-conceived and helps to confirm the theoretical findings.' Weaknesses='The main theoretical contribution may rely on strong assumptions that can be challenging to verify in practice, leaving the applicability under practical scenarios uncertain. The presentation of empirical results could be clearer, with some conclusions being difficult to align directly with the provided figures and data. Additionally, while the parameter selection method is discussed, it lacks explicit details and guidance for practitioners, potentially limiting its utility.' Questions='1. How do the assumptions required for the main theoretical contribution apply to real-world tasks and models, especially given the conditions outlined?\\n2. Can the authors provide a clear, step-by-step example of parameter selection for LiSSA in a specific scenario?\\n3. Are there any specific model sizes or architectures where the proposed method might struggle, based on current evaluations?\\n4. How do the authors suggest validating the assumptions on spectral properties in diverse datasets outside of those empirically tested?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides a novel approach to addressing the online learning problem of brokerage with a clear analysis of regret and matching lower bounds. The algorithms proposed are theoretically sound under the provided assumptions.' Weaknesses=\"The assumptions regarding traders' valuations and the contextual information might limit the applicability of the results in real-world scenarios. Some parts of the model might be too simplistic or unrealistic, potentially affecting the overall contribution.\" Questions=\"How do the assumptions about contextual information and traders' valuations impact the applicability of the model in real-world trading scenarios?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides a novel approach to addressing the online learning problem of brokerage with a clear analysis of regret and matching lower bounds. The algorithms proposed are theoretically sound under the provided assumptions.' Weaknesses=\"The assumptions regarding traders' valuations and the contextual information might limit the applicability of the results in real-world scenarios. Some parts of the model might be too simplistic or unrealistic, potentially affecting the overall contribution.\" Questions=\"How do the assumptions about contextual information and traders' valuations impact the applicability of the model in real-world trading scenarios?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a static 8-bit quantization method for state space models, successfully demonstrating reduced generation latency with minimal accuracy drop. It addresses the specific challenges of SSM quantization, suggesting practical applicability for cloud and edge deployments.' Weaknesses='Existing quantization methods may not have been sufficiently compared, raising questions about the comprehensive applicability of the proposed method. The presentation could benefit from additional clarification in places, particularly in explaining long-term scalability across different hardware.' Questions=\"What are the potential limitations of the proposed quantization method when applied to varied hardware configurations? Can further technical details be provided on the Hadamard transform's role in achieving outlier-free quantization?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a static 8-bit quantization method for state space models, successfully demonstrating reduced generation latency with minimal accuracy drop. It addresses the specific challenges of SSM quantization, suggesting practical applicability for cloud and edge deployments.' Weaknesses='Existing quantization methods may not have been sufficiently compared, raising questions about the comprehensive applicability of the proposed method. The presentation could benefit from additional clarification in places, particularly in explaining long-term scalability across different hardware.' Questions=\"What are the potential limitations of the proposed quantization method when applied to varied hardware configurations? Can further technical details be provided on the Hadamard transform's role in achieving outlier-free quantization?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed approach effectively leverages label influence propagation to improve multi-label node classification. It dynamically adjusts learning by amplifying and mitigating label influences, demonstrating superior performance over state-of-the-art methods.' Weaknesses='The paper lacks a comprehensive comparison with existing label propagation methods. It is unclear if the influence of hyperparameters like \\\\alpha and \\\\beta has been thoroughly analyzed. Additionally, there are issues with label and figure clarity, such as missing axis labels.' Questions='How does the approach compare with existing label propagation techniques? Have hyperparameters like \\\\alpha and \\\\beta been thoroughly analyzed? Can the authors provide visualizations illustrating the influence of different labels?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed approach effectively leverages label influence propagation to improve multi-label node classification. It dynamically adjusts learning by amplifying and mitigating label influences, demonstrating superior performance over state-of-the-art methods.' Weaknesses='The paper lacks a comprehensive comparison with existing label propagation methods. It is unclear if the influence of hyperparameters like \\\\alpha and \\\\beta has been thoroughly analyzed. Additionally, there are issues with label and figure clarity, such as missing axis labels.' Questions='How does the approach compare with existing label propagation techniques? Have hyperparameters like \\\\alpha and \\\\beta been thoroughly analyzed? Can the authors provide visualizations illustrating the influence of different labels?'\n",
      "Zero shot review:\n",
      " Soundness=2 Presentation=3 Contribution=3 Rating=4 Confidence=3 Strengths='The paper introduces an innovative modification to KANs by incorporating a flexible function basis, potentially enhancing both accuracy and interpretability. Comprehensive experiments across multiple tasks validate their approach. The writing is generally clear and well-organized.' Weaknesses='There is skepticism regarding the actual impact of the proposed modifications, as improvements over baseline KANs are marginal. Some theoretical claims are not backed by experiment results. The lack of comparison with state-of-the-art models for specific datasets is noted.' Questions='How does the choice of $n$ in the polynomial basis affect model performance, and is there a practical guideline for its selection? Can the authors clarify the implementation details of varying $n$ in VBn-KAN?'\n",
      "Success: Soundness=2 Presentation=3 Contribution=3 Rating=4 Confidence=3 Strengths='The paper introduces an innovative modification to KANs by incorporating a flexible function basis, potentially enhancing both accuracy and interpretability. Comprehensive experiments across multiple tasks validate their approach. The writing is generally clear and well-organized.' Weaknesses='There is skepticism regarding the actual impact of the proposed modifications, as improvements over baseline KANs are marginal. Some theoretical claims are not backed by experiment results. The lack of comparison with state-of-the-art models for specific datasets is noted.' Questions='How does the choice of $n$ in the polynomial basis affect model performance, and is there a practical guideline for its selection? Can the authors clarify the implementation details of varying $n$ in VBn-KAN?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The method allows articulated object generation from a single image, making it potentially more scalable and practical than prior works requiring multi-view or multi-state inputs. The use of a diffusion model to handle shape and motion ambiguity is innovative, and experiments show a significant improvement over state-of-the-art in terms of realism and resemblance. The paper is presented clearly with a structured approach.' Weaknesses='The method relies on ground-truth annotations and existing databases for part retrieval, which may limit real-world applicability. The practicality of handling a broad range of object categories was not fully demonstrated. Some components of the method yield minimal gains. Dependency on GPT-4o raises concerns about specific design contributions.' Questions='How does the method generalize to object categories not included in the training set? Can the approach handle more complex articulation scenarios beyond those tested? How robust is the method to variations and noise in the input single image?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The method allows articulated object generation from a single image, making it potentially more scalable and practical than prior works requiring multi-view or multi-state inputs. The use of a diffusion model to handle shape and motion ambiguity is innovative, and experiments show a significant improvement over state-of-the-art in terms of realism and resemblance. The paper is presented clearly with a structured approach.' Weaknesses='The method relies on ground-truth annotations and existing databases for part retrieval, which may limit real-world applicability. The practicality of handling a broad range of object categories was not fully demonstrated. Some components of the method yield minimal gains. Dependency on GPT-4o raises concerns about specific design contributions.' Questions='How does the method generalize to object categories not included in the training set? Can the approach handle more complex articulation scenarios beyond those tested? How robust is the method to variations and noise in the input single image?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=3 Strengths='The paper presents a novel approach called Geometric Spatiotemporal Transformers (GSTs) that effectively combines the strengths of Transformers for spatiotemporal graph inputs, which is an innovative contribution to physical dynamics simulation. The introduction of the Temporal Difference Graph (TDG) is a clever mechanism to mitigate cumulative errors in long-term prediction tasks. Moreover, the approach shows state-of-the-art performance across multiple challenging physical systems, demonstrating its robustness.' Weaknesses='The paper does not provide detailed information on runtime or parameter efficiency comparisons with existing methods, which is essential to understand practical applicability. Additionally, the complexity and scalability of the proposed method, particularly for very large systems or extended prediction horizons, are not fully addressed. The datasets evaluated are relatively small in terms of rollout timesteps, which might not fully demonstrate the long-term prediction capabilities claimed.' Questions='What are the computational requirements of GSTs compared to other state-of-the-art methods for similar tasks? How does the model handle extremely large-scale systems or datasets with longer temporal spans? Can the method be applied or adjusted for different types of physical simulations not covered in this paper?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=3 Strengths='The paper presents a novel approach called Geometric Spatiotemporal Transformers (GSTs) that effectively combines the strengths of Transformers for spatiotemporal graph inputs, which is an innovative contribution to physical dynamics simulation. The introduction of the Temporal Difference Graph (TDG) is a clever mechanism to mitigate cumulative errors in long-term prediction tasks. Moreover, the approach shows state-of-the-art performance across multiple challenging physical systems, demonstrating its robustness.' Weaknesses='The paper does not provide detailed information on runtime or parameter efficiency comparisons with existing methods, which is essential to understand practical applicability. Additionally, the complexity and scalability of the proposed method, particularly for very large systems or extended prediction horizons, are not fully addressed. The datasets evaluated are relatively small in terms of rollout timesteps, which might not fully demonstrate the long-term prediction capabilities claimed.' Questions='What are the computational requirements of GSTs compared to other state-of-the-art methods for similar tasks? How does the model handle extremely large-scale systems or datasets with longer temporal spans? Can the method be applied or adjusted for different types of physical simulations not covered in this paper?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='Graffe demonstrates competitive performance in graph representation learning, achieving state-of-the-art results on numerous datasets. The theoretical advancement of proving the objective maximizes conditional mutual information provides a strong foundation.' Weaknesses='The novelty of adapting DPMs to graph representation learning is somewhat limited by existing techniques, and the theoretical insights need more practical demonstration. Some evaluations and ablations could be extended for a deeper understanding.' Questions='How does the choice of mask ratio impact the performance across different datasets, and can an optimal ratio be generalized? What insights support the architectural choices in the encoder and decoder, and are there alternative architectures considered?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='Graffe demonstrates competitive performance in graph representation learning, achieving state-of-the-art results on numerous datasets. The theoretical advancement of proving the objective maximizes conditional mutual information provides a strong foundation.' Weaknesses='The novelty of adapting DPMs to graph representation learning is somewhat limited by existing techniques, and the theoretical insights need more practical demonstration. Some evaluations and ablations could be extended for a deeper understanding.' Questions='How does the choice of mask ratio impact the performance across different datasets, and can an optimal ratio be generalized? What insights support the architectural choices in the encoder and decoder, and are there alternative architectures considered?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel approach that integrates external knowledge with large language models for question answering. It introduces an extra attention mechanism and a memory-based mechanism to dynamically control knowledge integration, enhancing the effectiveness of RAG techniques.' Weaknesses=\"While the approach is novel, the paper lacks detailed explanations of the mechanisms and their intuitive grounding. Some methodological choices are unclear, such as details about the alignment of the latent spaces. The experiments could be more comprehensive in demonstrating the approach's benefits over existing methods.\" Questions='Could the authors provide more details on the memory-based mechanism and how the degree of knowledge integration is dynamically controlled? How does the extra attention head integrate with the internal heads of the LLMs?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel approach that integrates external knowledge with large language models for question answering. It introduces an extra attention mechanism and a memory-based mechanism to dynamically control knowledge integration, enhancing the effectiveness of RAG techniques.' Weaknesses=\"While the approach is novel, the paper lacks detailed explanations of the mechanisms and their intuitive grounding. Some methodological choices are unclear, such as details about the alignment of the latent spaces. The experiments could be more comprehensive in demonstrating the approach's benefits over existing methods.\" Questions='Could the authors provide more details on the memory-based mechanism and how the degree of knowledge integration is dynamically controlled? How does the extra attention head integrate with the internal heads of the LLMs?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel reinforcement learning approach with a performance guarantee for guided methods. It provides a robust method for adapting guide policy sampling rates to maintain a user-defined performance threshold, which is a significant contribution to the field.' Weaknesses='The empirical evaluation could be more comprehensive, and the results are limited to specific scenarios and assumptions. The practical applicability is somewhat restricted, and the assumptions made for deriving the guarantees might not hold in real-world scenarios.' Questions='How does the method perform in more complex and diverse environments? What are the limitations of the assumptions used for deriving the performance guarantees in practical applications?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel reinforcement learning approach with a performance guarantee for guided methods. It provides a robust method for adapting guide policy sampling rates to maintain a user-defined performance threshold, which is a significant contribution to the field.' Weaknesses='The empirical evaluation could be more comprehensive, and the results are limited to specific scenarios and assumptions. The practical applicability is somewhat restricted, and the assumptions made for deriving the guarantees might not hold in real-world scenarios.' Questions='How does the method perform in more complex and diverse environments? What are the limitations of the assumptions used for deriving the performance guarantees in practical applications?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces an innovative multi-stage generation framework, Diffusion on Diffusion (DoD), that improves the texture details in image generation while reducing computational costs. It presents a novel approach by integrating visual priors from generated samples for better guidance in a diffusion model. The method is thoroughly evaluated on the ImageNet dataset, demonstrating superior efficiency and performance compared to state-of-the-art methods.' Weaknesses='The paper primarily compares against basic diffusion models but lacks comparisons against other post-hoc diffusion methods and other image feature conditioning methods. The main results table is not well positioned for visibility and lacks in-depth analysis. Qualitative results are hard to interpret without specific guidance on improvements.' Questions='How does the proposed method compare to other post-hoc diffusion methods and image feature conditioning techniques? Can you provide more specific analysis or guidance on the qualitative results to help interpret the improvements?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces an innovative multi-stage generation framework, Diffusion on Diffusion (DoD), that improves the texture details in image generation while reducing computational costs. It presents a novel approach by integrating visual priors from generated samples for better guidance in a diffusion model. The method is thoroughly evaluated on the ImageNet dataset, demonstrating superior efficiency and performance compared to state-of-the-art methods.' Weaknesses='The paper primarily compares against basic diffusion models but lacks comparisons against other post-hoc diffusion methods and other image feature conditioning methods. The main results table is not well positioned for visibility and lacks in-depth analysis. Qualitative results are hard to interpret without specific guidance on improvements.' Questions='How does the proposed method compare to other post-hoc diffusion methods and image feature conditioning techniques? Can you provide more specific analysis or guidance on the qualitative results to help interpret the improvements?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel framework for dynamics generalization in deep reinforcement learning, addressing an important problem in the field. It combines mechanisms for both in-distribution and out-of-distribution generalization, demonstrating strong performance across various scenarios.' Weaknesses='The experimental evaluation may be limited to specific simulated environments, and the paper may not address recent related work comprehensively.' Questions='How does the proposed framework perform in more diverse environments beyond the tested locomotion tasks? Are there any assumptions made that limit the applicability of the method to specific types of problems?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel framework for dynamics generalization in deep reinforcement learning, addressing an important problem in the field. It combines mechanisms for both in-distribution and out-of-distribution generalization, demonstrating strong performance across various scenarios.' Weaknesses='The experimental evaluation may be limited to specific simulated environments, and the paper may not address recent related work comprehensively.' Questions='How does the proposed framework perform in more diverse environments beyond the tested locomotion tasks? Are there any assumptions made that limit the applicability of the method to specific types of problems?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper introduces a novel method for auditing the privacy of large language models, setting a new standard for detecting privacy leakage. The approach is thorough and extensively tested, demonstrating significant improvements over previous state-of-the-art methods. The experiments cover multiple families of fine-tuned LLMs and show the method's effectiveness in realistic settings.\" Weaknesses='The paper may lack discussion on potential limitations or scenarios where the proposed method might not be as effective. Additionally, the analysis might be limited to specific models or datasets, potentially affecting the generalizability of the results.' Questions='1. How does the method perform on different types or sizes of datasets, and what are the implications for generalization?\\n2. Are there any specific limitations or scenarios where the proposed approach might not be effective? \\n3. Is the method applicable to all LLMs, or are there certain prerequisites for its application?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper introduces a novel method for auditing the privacy of large language models, setting a new standard for detecting privacy leakage. The approach is thorough and extensively tested, demonstrating significant improvements over previous state-of-the-art methods. The experiments cover multiple families of fine-tuned LLMs and show the method's effectiveness in realistic settings.\" Weaknesses='The paper may lack discussion on potential limitations or scenarios where the proposed method might not be as effective. Additionally, the analysis might be limited to specific models or datasets, potentially affecting the generalizability of the results.' Questions='1. How does the method perform on different types or sizes of datasets, and what are the implications for generalization?\\n2. Are there any specific limitations or scenarios where the proposed approach might not be effective? \\n3. Is the method applicable to all LLMs, or are there certain prerequisites for its application?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='Introduce an innovative framework for converting single-view videos into high-quality stereo videos. The creation of a large dataset, YouTube-SBS, for training and benchmarking stereo video models significantly contributes to the field.' Weaknesses='The paper relies on implicit guidance without an explicit comparison to other state-of-the-art methods. There is a lack of detailed discussion on potential limitations or failure cases of the proposed method.' Questions='How do you handle temporal inconsistencies in the videos? Can the stereo videos generated by ImmersePro be applied to real-time scenarios?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='Introduce an innovative framework for converting single-view videos into high-quality stereo videos. The creation of a large dataset, YouTube-SBS, for training and benchmarking stereo video models significantly contributes to the field.' Weaknesses='The paper relies on implicit guidance without an explicit comparison to other state-of-the-art methods. There is a lack of detailed discussion on potential limitations or failure cases of the proposed method.' Questions='How do you handle temporal inconsistencies in the videos? Can the stereo videos generated by ImmersePro be applied to real-time scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"The paper addresses a significant challenge in protein design, which is extrapolating beyond the training data to improve protein fitness. Introducing triplet relations for better extrapolation represents a novel and promising approach. The experimental validations are thorough and demonstrate the method's effectiveness.\" Weaknesses='The proposed method relies on a scorer function for defining triplets, which may have limitations in real-world scenarios, particularly when generalizing out-of-distribution. The paper\\'s presentation could be clearer, as certain concepts like triplet definitions and \"scorer distillation\" are not immediately understandable.' Questions='How robust is the model to noisy inputs, especially when triplet definitions might be imperfect? Could the authors provide more details on the role of mask infilling and its impact on model performance? How is the effectiveness of triplet rankings ensured when potential scoring inaccuracies arise?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"The paper addresses a significant challenge in protein design, which is extrapolating beyond the training data to improve protein fitness. Introducing triplet relations for better extrapolation represents a novel and promising approach. The experimental validations are thorough and demonstrate the method's effectiveness.\" Weaknesses='The proposed method relies on a scorer function for defining triplets, which may have limitations in real-world scenarios, particularly when generalizing out-of-distribution. The paper\\'s presentation could be clearer, as certain concepts like triplet definitions and \"scorer distillation\" are not immediately understandable.' Questions='How robust is the model to noisy inputs, especially when triplet definitions might be imperfect? Could the authors provide more details on the role of mask infilling and its impact on model performance? How is the effectiveness of triplet rankings ensured when potential scoring inaccuracies arise?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel approach to time series data augmentation using reinforcement learning, which offers a fresh perspective on enhancing forecasting models. The method is backed by empirical analysis and shows substantial improvements in forecasting performance with minimal computational cost. The use of variational masked autoencoders is innovative and well-motivated.' Weaknesses='The abstract does not provide enough detail on the specific results or how the proposed method compares to other state-of-the-art techniques in terms of performance. Additionally, the practical implications and potential limitations of applying this method in various scenarios are not discussed.' Questions='How does AutoTSAug perform compared to other existing data augmentation methods for time series forecasting? Are there any scenarios where this approach might not be applicable or where it might underperform?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel approach to time series data augmentation using reinforcement learning, which offers a fresh perspective on enhancing forecasting models. The method is backed by empirical analysis and shows substantial improvements in forecasting performance with minimal computational cost. The use of variational masked autoencoders is innovative and well-motivated.' Weaknesses='The abstract does not provide enough detail on the specific results or how the proposed method compares to other state-of-the-art techniques in terms of performance. Additionally, the practical implications and potential limitations of applying this method in various scenarios are not discussed.' Questions='How does AutoTSAug perform compared to other existing data augmentation methods for time series forecasting? Are there any scenarios where this approach might not be applicable or where it might underperform?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses a significant problem in CLIP models regarding the modality gap and proposes a method to align embeddings better. The proposed AlignCLIP framework appears to show improvements in zero-shot and fine-tuning tasks.' Weaknesses='The explanation of the methodology could be clearer. The abstract suggests improvements, but details on how these results compare to similar works are not provided. It is also not clear how generalizable the results are beyond the datasets tested.' Questions=\"How does AlignCLIP's performance compare on a broader set of datasets? Are there any scenarios where AlignCLIP does not perform well?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses a significant problem in CLIP models regarding the modality gap and proposes a method to align embeddings better. The proposed AlignCLIP framework appears to show improvements in zero-shot and fine-tuning tasks.' Weaknesses='The explanation of the methodology could be clearer. The abstract suggests improvements, but details on how these results compare to similar works are not provided. It is also not clear how generalizable the results are beyond the datasets tested.' Questions=\"How does AlignCLIP's performance compare on a broader set of datasets? Are there any scenarios where AlignCLIP does not perform well?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses an important problem in neural network weight reconstruction and provides a novel approach by introducing a behavioral loss. It demonstrates improved performance in model reconstruction by combining structural and behavioral features.' Weaknesses='The novelty of the behavioral loss could be questioned as similar approaches might exist, and the experiments might not cover a wide range of models and tasks. More empirical evidence would strengthen the contributions.' Questions='How does the proposed behavioral loss method scale with larger models, and are there any limitations observed in practical implementations?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses an important problem in neural network weight reconstruction and provides a novel approach by introducing a behavioral loss. It demonstrates improved performance in model reconstruction by combining structural and behavioral features.' Weaknesses='The novelty of the behavioral loss could be questioned as similar approaches might exist, and the experiments might not cover a wide range of models and tasks. More empirical evidence would strengthen the contributions.' Questions='How does the proposed behavioral loss method scale with larger models, and are there any limitations observed in practical implementations?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper introduces a novel and promising approach for improving offline reinforcement learning by extending the action space considered by the Bellman update. The method addresses challenges related to sparse rewards and incomplete trajectories, showcasing improved performance in experiments. The concept of Extended Implicit Q-Learning effectively leverages existing data while mitigating overestimation risks.' Weaknesses='The paper may lack some clarity in its theoretical exposition, specifically regarding the presentation of the foundational Implicit Q-Learning algorithm. There could be more elaboration on the potential limitations or failure modes of the proposed method. Additionally, the experiments, while promising, may require further validation across diverse and more challenging environments.' Questions='How does the proposed approach compare computationally to traditional offline RL algorithms in terms of efficiency? Could the authors provide additional insights into how EIQL specifically mitigates error extrapolation risks compared to other competitive methods?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper introduces a novel and promising approach for improving offline reinforcement learning by extending the action space considered by the Bellman update. The method addresses challenges related to sparse rewards and incomplete trajectories, showcasing improved performance in experiments. The concept of Extended Implicit Q-Learning effectively leverages existing data while mitigating overestimation risks.' Weaknesses='The paper may lack some clarity in its theoretical exposition, specifically regarding the presentation of the foundational Implicit Q-Learning algorithm. There could be more elaboration on the potential limitations or failure modes of the proposed method. Additionally, the experiments, while promising, may require further validation across diverse and more challenging environments.' Questions='How does the proposed approach compare computationally to traditional offline RL algorithms in terms of efficiency? Could the authors provide additional insights into how EIQL specifically mitigates error extrapolation risks compared to other competitive methods?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses a significant problem of adaptive complexity in sampling algorithms, particularly in large-data applications. It provides a novel analysis of the lower bounds for query complexity in both unconstrained and box-constrained sampling, adding depth to the current understanding of sampling strategies in high-dimensional spaces.' Weaknesses='The paper might be challenging to follow due to complex mathematical explanations and specialized terminology. Some aspects of the proofs may seem too detailed without sufficient contextualization, which could affect the accessibility of the paper’s results to a broader audience. Additionally, practical implications or potential applications of the results are not well elaborated.' Questions='Can the techniques used in proving the lower bounds be applied or extended to other related problems in high-dimensional sampling? What are the potential implications for designing more efficient sampling algorithms given the established lower bounds? Is there a way to simplify the proofs to make them more understandable to those less familiar with the specific mathematical techniques employed?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses a significant problem of adaptive complexity in sampling algorithms, particularly in large-data applications. It provides a novel analysis of the lower bounds for query complexity in both unconstrained and box-constrained sampling, adding depth to the current understanding of sampling strategies in high-dimensional spaces.' Weaknesses='The paper might be challenging to follow due to complex mathematical explanations and specialized terminology. Some aspects of the proofs may seem too detailed without sufficient contextualization, which could affect the accessibility of the paper’s results to a broader audience. Additionally, practical implications or potential applications of the results are not well elaborated.' Questions='Can the techniques used in proving the lower bounds be applied or extended to other related problems in high-dimensional sampling? What are the potential implications for designing more efficient sampling algorithms given the established lower bounds? Is there a way to simplify the proofs to make them more understandable to those less familiar with the specific mathematical techniques employed?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The proposed Soft Preference Optimization (SPO) method presents a novel approach to aligning generative models with human preferences without relying on a reward model, which simplifies the alignment process. The ability to adjust the 'softness' of the distribution through an algorithm parameter provides flexibility. The theoretical foundation under the Bradley-Terry model is well-formulated.\" Weaknesses='The abstract does not provide detailed empirical evidence or comparisons with existing methods, which are crucial for understanding the practical effectiveness of the approach. The theoretical explanations may be complex for practitioners without detailed examples or visual demonstrations.' Questions='How does the performance of SPO compare with traditional reward model-based alignment methods in practical scenarios? Are there specific types of generative models or tasks where SPO demonstrates significant advantages or disadvantages compared to existing methods?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The proposed Soft Preference Optimization (SPO) method presents a novel approach to aligning generative models with human preferences without relying on a reward model, which simplifies the alignment process. The ability to adjust the 'softness' of the distribution through an algorithm parameter provides flexibility. The theoretical foundation under the Bradley-Terry model is well-formulated.\" Weaknesses='The abstract does not provide detailed empirical evidence or comparisons with existing methods, which are crucial for understanding the practical effectiveness of the approach. The theoretical explanations may be complex for practitioners without detailed examples or visual demonstrations.' Questions='How does the performance of SPO compare with traditional reward model-based alignment methods in practical scenarios? Are there specific types of generative models or tasks where SPO demonstrates significant advantages or disadvantages compared to existing methods?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"SyntheOcc introduces an innovative approach by conditioning a 2D diffusion model with 3D semantic multi-plane images, effectively bridging a gap in complex 3D occupancy prediction tasks. The method's ability to generate photorealistic and geometrically controlled datasets is a significant contribution to enhancing autonomous driving simulations and model training. The approach is validated with extensive qualitative and quantitative results on a prominent dataset (nuScenes), underscoring its effectiveness in data augmentation.\" Weaknesses='The paper may lack clarity in fully explaining how the integration of 3D geometric information as conditional input is achieved at a technical level. There might also be concerns about the scalability of SyntheOcc for different scenarios beyond the tested dataset, as well as the computational resources required for generating such detailed synthetic images.' Questions='How does SyntheOcc perform in terms of computational efficiency and scalability when creating large datasets for various driving environments? Are there any limitations identified during qualitative assessments that might affect its adaptability across different real-world scenarios?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"SyntheOcc introduces an innovative approach by conditioning a 2D diffusion model with 3D semantic multi-plane images, effectively bridging a gap in complex 3D occupancy prediction tasks. The method's ability to generate photorealistic and geometrically controlled datasets is a significant contribution to enhancing autonomous driving simulations and model training. The approach is validated with extensive qualitative and quantitative results on a prominent dataset (nuScenes), underscoring its effectiveness in data augmentation.\" Weaknesses='The paper may lack clarity in fully explaining how the integration of 3D geometric information as conditional input is achieved at a technical level. There might also be concerns about the scalability of SyntheOcc for different scenarios beyond the tested dataset, as well as the computational resources required for generating such detailed synthetic images.' Questions='How does SyntheOcc perform in terms of computational efficiency and scalability when creating large datasets for various driving environments? Are there any limitations identified during qualitative assessments that might affect its adaptability across different real-world scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The approach integrates diffusion models with active learning to reduce data acquisition requirements in inverse problems, demonstrating significant practical benefits in real-world applications.' Weaknesses=\"The abstract does not provide details on the computational cost or comparison with existing active learning methods, which are crucial for evaluating the approach's efficiency and novelty.\" Questions='How does the computational cost compare to traditional methods? Are there specific scenarios where this approach is more beneficial?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The approach integrates diffusion models with active learning to reduce data acquisition requirements in inverse problems, demonstrating significant practical benefits in real-world applications.' Weaknesses=\"The abstract does not provide details on the computational cost or comparison with existing active learning methods, which are crucial for evaluating the approach's efficiency and novelty.\" Questions='How does the computational cost compare to traditional methods? Are there specific scenarios where this approach is more beneficial?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper addresses the significant challenge of semantic-aware segmentation using the Segment Anything Model (SAM), which has shown considerable potential in image segmentation. The introduction of SAM-CP and its two types of composable prompts adds a novel layer to the existing methodology, allowing for versatile segmentation across various scenarios. The unified framework for calculating affinity between queries and patches is another strong point, simplifying the segmentation process without degrading performance. The experimental results, which show state-of-the-art performance in open-vocabulary segmentation, underscore the practical utility and effectiveness of SAM-CP.' Weaknesses=\"While the paper presents an innovative approach, the complexity introduced by the two types of prompts might limit the accessibility and ease of use, especially for those not intimately familiar with SAM. Furthermore, the reliance on a unified framework to handle large numbers of semantic classes and patches could lead to scalability issues. Another concern is the potential lack of detailed qualitative evaluations, which could provide deeper insights into the method's effectiveness across various domains and dataset types.\" Questions='How does the proposed SAM-CP approach perform compared to other state-of-the-art segmentation methods, particularly those not based on SAM? Are there any specific limitations when applying SAM-CP to domains outside of the tested benchmarks, such as medical imaging or other non-standard datasets?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper addresses the significant challenge of semantic-aware segmentation using the Segment Anything Model (SAM), which has shown considerable potential in image segmentation. The introduction of SAM-CP and its two types of composable prompts adds a novel layer to the existing methodology, allowing for versatile segmentation across various scenarios. The unified framework for calculating affinity between queries and patches is another strong point, simplifying the segmentation process without degrading performance. The experimental results, which show state-of-the-art performance in open-vocabulary segmentation, underscore the practical utility and effectiveness of SAM-CP.' Weaknesses=\"While the paper presents an innovative approach, the complexity introduced by the two types of prompts might limit the accessibility and ease of use, especially for those not intimately familiar with SAM. Furthermore, the reliance on a unified framework to handle large numbers of semantic classes and patches could lead to scalability issues. Another concern is the potential lack of detailed qualitative evaluations, which could provide deeper insights into the method's effectiveness across various domains and dataset types.\" Questions='How does the proposed SAM-CP approach perform compared to other state-of-the-art segmentation methods, particularly those not based on SAM? Are there any specific limitations when applying SAM-CP to domains outside of the tested benchmarks, such as medical imaging or other non-standard datasets?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a novel approach using language descriptions to guide video colorization, which is innovative in addressing temporal consistency issues and user control over the colorization process. The method leverages a pre-trained cross-modality generative model and introduces new techniques such as temporally deformable attention and cross-clip fusion that show strong results in maintaining creative and consistent colors across video frames.' Weaknesses='While the approach is innovative, there may be some limitations regarding the complexity and ambiguity of language descriptions impacting the effectiveness of colorization. The integration of the proposed methods into existing workflows needs further validation across a wider range of video content and styles. There could also be challenges related to computational resources and scalability that are not addressed.' Questions='How does the model handle ambiguities or complex sentences in language descriptions? Can the system be adjusted to weigh different parts of the language input based on context or importance? What are the potential computational overheads associated with the introduced modules and how do they scale with longer video sequences?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a novel approach using language descriptions to guide video colorization, which is innovative in addressing temporal consistency issues and user control over the colorization process. The method leverages a pre-trained cross-modality generative model and introduces new techniques such as temporally deformable attention and cross-clip fusion that show strong results in maintaining creative and consistent colors across video frames.' Weaknesses='While the approach is innovative, there may be some limitations regarding the complexity and ambiguity of language descriptions impacting the effectiveness of colorization. The integration of the proposed methods into existing workflows needs further validation across a wider range of video content and styles. There could also be challenges related to computational resources and scalability that are not addressed.' Questions='How does the model handle ambiguities or complex sentences in language descriptions? Can the system be adjusted to weigh different parts of the language input based on context or importance? What are the potential computational overheads associated with the introduced modules and how do they scale with longer video sequences?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='- The paper addresses the challenging intersection of lifelong learning and machine unlearning, presenting a novel integration.\\n- Demonstrates practical scalability across different neural architectures.\\n- Provides a mechanism for exact unlearning without degrading overall performance.' Weaknesses=\"- It is unclear if the additional complexity of the proposed method over simpler ablations is justified by significant performance gains.\\n- Lacks discussion of how adapted baselines from standard lifelong learning to the privacy-aware setting were effectively modified.\\n- Missing important baselines, such as those using Differential Privacy or from existing unlearning literature.\\n- The results rely on metrics for unlearning that may not provide a rigorous privacy guarantee.\\n- Scalability to user-level unlearning requests (where tasks don't cleanly separate via users) is not addressed.\" Questions=\"- How does the 'independent subnetworks without knowledge transfer' differ structurally from PALL, specifically in knowledge transfer mechanisms?\\n- What are the potential worst-case scenarios in the sequence of learning and unlearning tasks, particularly regarding performance degradation?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='- The paper addresses the challenging intersection of lifelong learning and machine unlearning, presenting a novel integration.\\n- Demonstrates practical scalability across different neural architectures.\\n- Provides a mechanism for exact unlearning without degrading overall performance.' Weaknesses=\"- It is unclear if the additional complexity of the proposed method over simpler ablations is justified by significant performance gains.\\n- Lacks discussion of how adapted baselines from standard lifelong learning to the privacy-aware setting were effectively modified.\\n- Missing important baselines, such as those using Differential Privacy or from existing unlearning literature.\\n- The results rely on metrics for unlearning that may not provide a rigorous privacy guarantee.\\n- Scalability to user-level unlearning requests (where tasks don't cleanly separate via users) is not addressed.\" Questions=\"- How does the 'independent subnetworks without knowledge transfer' differ structurally from PALL, specifically in knowledge transfer mechanisms?\\n- What are the potential worst-case scenarios in the sequence of learning and unlearning tasks, particularly regarding performance degradation?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces novel modules like CVCAM and MHSAM that enhance cross-view object geo-localization. It also contributes a new dataset (G2D) adding value to the field.' Weaknesses='The effectiveness of CVCAM and MHSAM requires further validation through diverse datasets and real-world scenarios. The scarcity of datasets makes it difficult to generalize findings.' Questions='How adaptable are CVCAM and MHSAM to other cross-view geo-localization tasks beyond Ground→Drone? What efforts were made to ensure the G2D dataset captures a wide variety of geographic and environmental conditions?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces novel modules like CVCAM and MHSAM that enhance cross-view object geo-localization. It also contributes a new dataset (G2D) adding value to the field.' Weaknesses='The effectiveness of CVCAM and MHSAM requires further validation through diverse datasets and real-world scenarios. The scarcity of datasets makes it difficult to generalize findings.' Questions='How adaptable are CVCAM and MHSAM to other cross-view geo-localization tasks beyond Ground→Drone? What efforts were made to ensure the G2D dataset captures a wide variety of geographic and environmental conditions?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=4 Strengths='The paper provides a theoretical explanation using matrix theory for the adversarial fragility of neural networks. It connects this explanation with previous information-theoretic work.' Weaknesses='The theoretical results might not be entirely novel. The paper seems to extend existing work rather than introducing entirely new concepts. It is unclear how practical the implications are for real-world neural network design.' Questions='What are the practical implications of these theoretical findings? How does this approach compare with previous methods addressing adversarial robustness?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=4 Strengths='The paper provides a theoretical explanation using matrix theory for the adversarial fragility of neural networks. It connects this explanation with previous information-theoretic work.' Weaknesses='The theoretical results might not be entirely novel. The paper seems to extend existing work rather than introducing entirely new concepts. It is unclear how practical the implications are for real-world neural network design.' Questions='What are the practical implications of these theoretical findings? How does this approach compare with previous methods addressing adversarial robustness?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The proposed method introduces innovative training approaches that make vision transformers robust to arbitrary execution order of layers during inference. The ability to create functional 'Frankenstein' models and the graceful decline in performance during layer-pruning are notable contributions.\" Weaknesses='The paper assumes a 20% accuracy reduction at a given model size, which is significant and might limit practical applicability. The evaluation lacks comparison with a broader range of transformer architectures and practical scenarios where execution orders change frequently.' Questions='How does the proposed approach perform on other types of neural networks beyond vision transformers? Are there real-world applications where such robustness to layer execution order is critical?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The proposed method introduces innovative training approaches that make vision transformers robust to arbitrary execution order of layers during inference. The ability to create functional 'Frankenstein' models and the graceful decline in performance during layer-pruning are notable contributions.\" Weaknesses='The paper assumes a 20% accuracy reduction at a given model size, which is significant and might limit practical applicability. The evaluation lacks comparison with a broader range of transformer architectures and practical scenarios where execution orders change frequently.' Questions='How does the proposed approach perform on other types of neural networks beyond vision transformers? Are there real-world applications where such robustness to layer execution order is critical?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses the notable problem of models exploiting spurious features and presents a novel approach called FairDropout. The empirical evaluations span across different domains, showcasing the robustness of the method.' Weaknesses='The approach may rely heavily on the assumption of neuron memorization locality, which may not hold in all scenarios. Furthermore, the description lacks detail in explaining how FairDropout specifically informs neuron targeting.' Questions='How does FairDropout ensure that the memorization is indeed targeted to specific neurons for dropout? Are there scenarios where this approach might not be effective?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses the notable problem of models exploiting spurious features and presents a novel approach called FairDropout. The empirical evaluations span across different domains, showcasing the robustness of the method.' Weaknesses='The approach may rely heavily on the assumption of neuron memorization locality, which may not hold in all scenarios. Furthermore, the description lacks detail in explaining how FairDropout specifically informs neuron targeting.' Questions='How does FairDropout ensure that the memorization is indeed targeted to specific neurons for dropout? Are there scenarios where this approach might not be effective?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The methodology introduces a novel contrastive learning technique by grouping similar hand poses which enhances the performance significantly over existing methods. The use of an extensive dataset from recent human-centric videos is a strength and demonstrates scalability. The experimental results show clear improvements over state-of-the-art techniques.' Weaknesses='While the approach shows improvements, the paper would benefit from more detailed comparisons with alternative pre-training techniques, especially in terms of runtime performance and computational requirements. Additionally, some aspects of the contrastive loss adaptation mechanism could be more thoroughly justified or explained.' Questions='How does the proposed approach handle varying hand shapes and sizes in the wild, and is there a plan to extend the dataset further? Additionally, what are the computational costs compared to other state-of-the-art contrastive learning methods?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The methodology introduces a novel contrastive learning technique by grouping similar hand poses which enhances the performance significantly over existing methods. The use of an extensive dataset from recent human-centric videos is a strength and demonstrates scalability. The experimental results show clear improvements over state-of-the-art techniques.' Weaknesses='While the approach shows improvements, the paper would benefit from more detailed comparisons with alternative pre-training techniques, especially in terms of runtime performance and computational requirements. Additionally, some aspects of the contrastive loss adaptation mechanism could be more thoroughly justified or explained.' Questions='How does the proposed approach handle varying hand shapes and sizes in the wild, and is there a plan to extend the dataset further? Additionally, what are the computational costs compared to other state-of-the-art contrastive learning methods?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel self-supervised framework for MRI denoising, addressing the label scarcity issue effectively. It introduces an innovative generalized ambient denoising score matching (GADSM) loss and demonstrates state-of-the-art performance among self-supervised methods. Additionally, the framework can incorporate multi-contrast denoising, showcasing its versatility and robustness across different MRI noise conditions.' Weaknesses='The paper lacks empirical evidence on the trade-off between noise reduction and the preservation of fine spatial features. The claimed advantage over supervised methods in terms of generalization is not fully substantiated with clear experimental results. Moreover, the implementation details regarding the reparameterization strategy and its impact on convergence are not thoroughly explained.' Questions='How does the C2S framework perform compared to traditional non-supervised methods in terms of computational complexity and time efficiency? Can the reparameterization of noise levels introduced in the framework contribute to potential advantages in other domains beyond MRI denoising?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel self-supervised framework for MRI denoising, addressing the label scarcity issue effectively. It introduces an innovative generalized ambient denoising score matching (GADSM) loss and demonstrates state-of-the-art performance among self-supervised methods. Additionally, the framework can incorporate multi-contrast denoising, showcasing its versatility and robustness across different MRI noise conditions.' Weaknesses='The paper lacks empirical evidence on the trade-off between noise reduction and the preservation of fine spatial features. The claimed advantage over supervised methods in terms of generalization is not fully substantiated with clear experimental results. Moreover, the implementation details regarding the reparameterization strategy and its impact on convergence are not thoroughly explained.' Questions='How does the C2S framework perform compared to traditional non-supervised methods in terms of computational complexity and time efficiency? Can the reparameterization of noise levels introduced in the framework contribute to potential advantages in other domains beyond MRI denoising?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='1. APG is presented as a plug-and-play method with virtually no additional computational overhead, making it attractive for practical use in various diffusion models.\\n2. The method of down-weighting the parallel component in CFG is novel and interesting, effectively reducing oversaturation and preserving image quality at high guidance scales.' Weaknesses='1. In Figure 8, could the authors explain why the proposed APG shows lower precision and higher FID compared to CFG when the guidance scale  w  is below certain values (e.g.,  w = 2.5 , where APG precision is lower than CFG)?\\n2. While reverse momentum is introduced, the intuition behind why it improves the image generation quality could be expanded. More detailed theoretical or experimental exploration of this component would clarify its importance.' Questions='See the questions in the weakness section.'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='1. APG is presented as a plug-and-play method with virtually no additional computational overhead, making it attractive for practical use in various diffusion models.\\n2. The method of down-weighting the parallel component in CFG is novel and interesting, effectively reducing oversaturation and preserving image quality at high guidance scales.' Weaknesses='1. In Figure 8, could the authors explain why the proposed APG shows lower precision and higher FID compared to CFG when the guidance scale  w  is below certain values (e.g.,  w = 2.5 , where APG precision is lower than CFG)?\\n2. While reverse momentum is introduced, the intuition behind why it improves the image generation quality could be expanded. More detailed theoretical or experimental exploration of this component would clarify its importance.' Questions='See the questions in the weakness section.'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses a relevant problem by focusing on the MultiFilterANN, enhancing existing algorithms and providing both theoretical insights and practical improvements. Theoretical results provide new space-time tradeoffs and lower bounds for popular algorithms.' Weaknesses='The connection between the theoretical framework and the empirical implementation is not entirely clear. Some sections lack clarity and the presentation could be improved. Limited information on certain empirical comparisons and performance metrics.' Questions='Could you provide more details on how the theoretical results directly inform the empirical methods proposed, and clarify the specific implementation used for the experiments?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses a relevant problem by focusing on the MultiFilterANN, enhancing existing algorithms and providing both theoretical insights and practical improvements. Theoretical results provide new space-time tradeoffs and lower bounds for popular algorithms.' Weaknesses='The connection between the theoretical framework and the empirical implementation is not entirely clear. Some sections lack clarity and the presentation could be improved. Limited information on certain empirical comparisons and performance metrics.' Questions='Could you provide more details on how the theoretical results directly inform the empirical methods proposed, and clarify the specific implementation used for the experiments?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel sampling method for PINNs which addresses the challenge of singularities in the source functions of PDEs, potentially increasing accuracy and efficiency.' Weaknesses='The evaluation appears limited with only three PDE examples discussed. The paper lacks details on algorithm specifics, such as the distribution involved in flow matching. While promising, further experiments and details are required for clarity and validation.' Questions='What specific distribution is the flow matching method sampling from? Are the sample points re-drawn or fixed throughout training?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel sampling method for PINNs which addresses the challenge of singularities in the source functions of PDEs, potentially increasing accuracy and efficiency.' Weaknesses='The evaluation appears limited with only three PDE examples discussed. The paper lacks details on algorithm specifics, such as the distribution involved in flow matching. While promising, further experiments and details are required for clarity and validation.' Questions='What specific distribution is the flow matching method sampling from? Are the sample points re-drawn or fixed throughout training?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses an important gap in evaluating the mathematical reasoning capabilities of LLMs in long-context scenarios. MathHay is a novel benchmark that combines both information retrieval and complex reasoning, demonstrating rigorous experimental work with eight top LLMs.' Weaknesses='The paper lacks a detailed exploration of why the best-performing model still struggles and does not propose specific methods to address these deficiencies. It also relies heavily on achieving a single metric (accuracy) without detailed analysis of error types or qualitative insights.' Questions='How did the selection of eight top-performing LLMs occur, and what specific measures ensure the generalizability of MathHay results? Are there any qualitative insights into the types of errors these models frequently make on the benchmark?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses an important gap in evaluating the mathematical reasoning capabilities of LLMs in long-context scenarios. MathHay is a novel benchmark that combines both information retrieval and complex reasoning, demonstrating rigorous experimental work with eight top LLMs.' Weaknesses='The paper lacks a detailed exploration of why the best-performing model still struggles and does not propose specific methods to address these deficiencies. It also relies heavily on achieving a single metric (accuracy) without detailed analysis of error types or qualitative insights.' Questions='How did the selection of eight top-performing LLMs occur, and what specific measures ensure the generalizability of MathHay results? Are there any qualitative insights into the types of errors these models frequently make on the benchmark?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses an important challenge in data sharing and proposes a novel method, Mycroft, for selecting informative data subsets efficiently. The approach is demonstrated to be robust and effective across multiple tasks and domains.' Weaknesses='The explanation and implementation details related to certain algorithms and proofs are not clear. Some sections lack sufficient detail, making it challenging to fully understand the methodology and evaluate its effectiveness. The evaluation does not include scenarios where data privacy is actively protected, which is significant given the problem context.' Questions='How does RetrieveTopK function work in practice, and how does it ensure coverage of D^hard? How does privacy impact the performance and applicability of the Mycroft method?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses an important challenge in data sharing and proposes a novel method, Mycroft, for selecting informative data subsets efficiently. The approach is demonstrated to be robust and effective across multiple tasks and domains.' Weaknesses='The explanation and implementation details related to certain algorithms and proofs are not clear. Some sections lack sufficient detail, making it challenging to fully understand the methodology and evaluate its effectiveness. The evaluation does not include scenarios where data privacy is actively protected, which is significant given the problem context.' Questions='How does RetrieveTopK function work in practice, and how does it ensure coverage of D^hard? How does privacy impact the performance and applicability of the Mycroft method?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel adversarial Bayesian framework, Domain Indexing Collaborative Filtering (DICF), which addresses cold-start issues in cross-domain recommendation systems. This framework is presented with an emphasis on interpretability, which is a significant advancement over traditional black-box approaches.' Weaknesses='The paper lacks in-depth discussion on potential limitations or constraints of the proposed method. Additionally, while the empirical results are mentioned, there is limited information on the datasets used or the specific performance improvements achieved.' Questions='How does the DICF framework handle scalability when applied to large-scale real-world datasets?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel adversarial Bayesian framework, Domain Indexing Collaborative Filtering (DICF), which addresses cold-start issues in cross-domain recommendation systems. This framework is presented with an emphasis on interpretability, which is a significant advancement over traditional black-box approaches.' Weaknesses='The paper lacks in-depth discussion on potential limitations or constraints of the proposed method. Additionally, while the empirical results are mentioned, there is limited information on the datasets used or the specific performance improvements achieved.' Questions='How does the DICF framework handle scalability when applied to large-scale real-world datasets?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel method, SteerPrompt, that enhances the comprehension of LLMs by explicitly steering attention scores rather than relying solely on token-based prompting. It addresses significant issues with LLMs regarding long or distracting contexts. The experimental results demonstrate substantial improvements, which highlights the effectiveness of the method without requiring model parameter changes.' Weaknesses=\"The method's reliance on attention score manipulation may raise concerns about its robustness and general applicability to a wider range of tasks beyond the evaluated open-book QA. Additionally, the potential impact on inference speed and computational overhead is not discussed.\" Questions='How does SteerPrompt perform compared to other state-of-the-art methods focused on improving LLM comprehension in distracting or lengthy contexts? Can this method be extended or adapted for use in other types of NLP tasks or models beyond the ones tested?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel method, SteerPrompt, that enhances the comprehension of LLMs by explicitly steering attention scores rather than relying solely on token-based prompting. It addresses significant issues with LLMs regarding long or distracting contexts. The experimental results demonstrate substantial improvements, which highlights the effectiveness of the method without requiring model parameter changes.' Weaknesses=\"The method's reliance on attention score manipulation may raise concerns about its robustness and general applicability to a wider range of tasks beyond the evaluated open-book QA. Additionally, the potential impact on inference speed and computational overhead is not discussed.\" Questions='How does SteerPrompt perform compared to other state-of-the-art methods focused on improving LLM comprehension in distracting or lengthy contexts? Can this method be extended or adapted for use in other types of NLP tasks or models beyond the ones tested?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"This paper outlines a significant adaptation of the RWKV model for vision tasks, showcasing its robustness and efficiency. The model surpasses existing architectures like Vision Transformer in both performance and resource optimization, illustrating its potential in handling high-resolution inputs effectively. The model's capability to manage large-scale parameters without complex spatial aggregation is a key strength.\" Weaknesses='The paper primarily focuses on performance metrics and may benefit from further elaboration on theoretical aspects or limitations of the VRWKV model. Additionally, direct comparisons with other RNN-based models adapted for vision are limited.' Questions='How does Vision-RWKV compare with other recent transformer models specifically designed for vision tasks in terms of scalability and adaptability to different vision applications?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"This paper outlines a significant adaptation of the RWKV model for vision tasks, showcasing its robustness and efficiency. The model surpasses existing architectures like Vision Transformer in both performance and resource optimization, illustrating its potential in handling high-resolution inputs effectively. The model's capability to manage large-scale parameters without complex spatial aggregation is a key strength.\" Weaknesses='The paper primarily focuses on performance metrics and may benefit from further elaboration on theoretical aspects or limitations of the VRWKV model. Additionally, direct comparisons with other RNN-based models adapted for vision are limited.' Questions='How does Vision-RWKV compare with other recent transformer models specifically designed for vision tasks in terms of scalability and adaptability to different vision applications?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper provides a thorough theoretical analysis and empirical validation of a novel optimization algorithm (S3) that improves on the well-known Adam optimizer. It addresses a significant problem in deep neural network training by explaining and mitigating loss spikes.' Weaknesses='The contribution, while novel, is incremental as it builds on existing ideas in optimization such as the Adam optimizer and SignSGD. The theoretical explanation could benefit from further elaboration to ensure clarity and accessibility to a broader audience.' Questions='How does the proposed optimizer perform in comparison to other recent advancements in optimization algorithms, beyond Adam and AdamW? How robust is S3 to hyperparameter changes across a diverse set of neural network architectures?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper provides a thorough theoretical analysis and empirical validation of a novel optimization algorithm (S3) that improves on the well-known Adam optimizer. It addresses a significant problem in deep neural network training by explaining and mitigating loss spikes.' Weaknesses='The contribution, while novel, is incremental as it builds on existing ideas in optimization such as the Adam optimizer and SignSGD. The theoretical explanation could benefit from further elaboration to ensure clarity and accessibility to a broader audience.' Questions='How does the proposed optimizer perform in comparison to other recent advancements in optimization algorithms, beyond Adam and AdamW? How robust is S3 to hyperparameter changes across a diverse set of neural network architectures?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper proposes a novel framework for compositional zero-shot learning by addressing existing limitations through multimodal large language model embeddings and attribute smoothing. It demonstrates state-of-the-art performance on three challenging datasets, highlighting its effectiveness.' Weaknesses='The disentanglement efficacy may still be affected by background and attribute-object entanglement, and it relies on pretrained models which may not fully capture multimodal semantic information. The explanation of the methodology could be clearer in some sections.' Questions='How does the proposed attribute smoothing through auxiliary attributes address the problem of overconfidence specifically? Can the method be applied to other types of zero-shot learning tasks beyond attribute-object compositions?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper proposes a novel framework for compositional zero-shot learning by addressing existing limitations through multimodal large language model embeddings and attribute smoothing. It demonstrates state-of-the-art performance on three challenging datasets, highlighting its effectiveness.' Weaknesses='The disentanglement efficacy may still be affected by background and attribute-object entanglement, and it relies on pretrained models which may not fully capture multimodal semantic information. The explanation of the methodology could be clearer in some sections.' Questions='How does the proposed attribute smoothing through auxiliary attributes address the problem of overconfidence specifically? Can the method be applied to other types of zero-shot learning tasks beyond attribute-object compositions?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces an innovative approach by leveraging State-Space Models for time-varying counterfactual predictions, which appears to address longstanding issues of prediction performance and efficiency in capturing interactions over long sequences.' Weaknesses=\"The proposed method's reliance on mitigated confounding bias may not comprehensively address all possible biases, and the abstract does not provide detailed insights into potential limitations or challenges of the approach.\" Questions='How does the Mamba-CDSP model compare to more recent methods in terms of both accuracy and computational efficiency, especially considering newer Transformer architectures? What specific applications does Mamba-CDSP benefit the most in real-world scenarios?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces an innovative approach by leveraging State-Space Models for time-varying counterfactual predictions, which appears to address longstanding issues of prediction performance and efficiency in capturing interactions over long sequences.' Weaknesses=\"The proposed method's reliance on mitigated confounding bias may not comprehensively address all possible biases, and the abstract does not provide detailed insights into potential limitations or challenges of the approach.\" Questions='How does the Mamba-CDSP model compare to more recent methods in terms of both accuracy and computational efficiency, especially considering newer Transformer architectures? What specific applications does Mamba-CDSP benefit the most in real-world scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper introduces the concept of 'weak-to-strong deception,' highlighting a critical issue in AI model alignment. It demonstrates through experiments that deception intensifies with a growing capability gap and provides initial strategies to mitigate the problem.\" Weaknesses='The experiments might not fully cover real-world complexities, and the effectiveness of the proposed mitigation strategies is limited. The study focuses on specific alignment dimensions, which may restrict the generalizability of findings.' Questions=\"How can the study's findings be applied to more diverse and complex real-world scenarios? Are there other strategies beyond bootstrapping to effectively mitigate weak-to-strong deception?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper introduces the concept of 'weak-to-strong deception,' highlighting a critical issue in AI model alignment. It demonstrates through experiments that deception intensifies with a growing capability gap and provides initial strategies to mitigate the problem.\" Weaknesses='The experiments might not fully cover real-world complexities, and the effectiveness of the proposed mitigation strategies is limited. The study focuses on specific alignment dimensions, which may restrict the generalizability of findings.' Questions=\"How can the study's findings be applied to more diverse and complex real-world scenarios? Are there other strategies beyond bootstrapping to effectively mitigate weak-to-strong deception?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces innovative techniques for improving autoencoder performance at high spatial compression ratios, leading to substantial speedup in diffusion models without sacrificing accuracy. The improvements in inference and training speed are significant, and the approach is thoroughly evaluated on relevant benchmarks.' Weaknesses='The paper may lack clarity in the presentation of certain technical details, making it difficult for readers to fully understand the implementation and evaluation methodologies. Additionally, there might be a lack of theoretical analysis to support the proposed improvements. Some experimental results might be challenging to interpret without additional context.' Questions='What specific datasets and evaluation metrics were used in the experiments, and can any external validation be provided to strengthen the claims? How do the proposed techniques generalize to different types of autoencoder architectures or diffusion models?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces innovative techniques for improving autoencoder performance at high spatial compression ratios, leading to substantial speedup in diffusion models without sacrificing accuracy. The improvements in inference and training speed are significant, and the approach is thoroughly evaluated on relevant benchmarks.' Weaknesses='The paper may lack clarity in the presentation of certain technical details, making it difficult for readers to fully understand the implementation and evaluation methodologies. Additionally, there might be a lack of theoretical analysis to support the proposed improvements. Some experimental results might be challenging to interpret without additional context.' Questions='What specific datasets and evaluation metrics were used in the experiments, and can any external validation be provided to strengthen the claims? How do the proposed techniques generalize to different types of autoencoder architectures or diffusion models?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper proposes a novel framework for solving heterogeneous team games, extending the PSRO framework to cases where teammates have distinct roles. The theoretical proof of lower exploitability and empirical results showing convergence where non-heterogeneous baselines fail are commendable.' Weaknesses='The paper relies on several complex theoretical constructs, and some explanations may not be sufficiently detailed for all readers. The specific improvements over existing methods, aside from heterogeneity, might not be entirely clear.' Questions='How does H-PSRO handle scalability issues when the number of unique roles or policies increases significantly?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper proposes a novel framework for solving heterogeneous team games, extending the PSRO framework to cases where teammates have distinct roles. The theoretical proof of lower exploitability and empirical results showing convergence where non-heterogeneous baselines fail are commendable.' Weaknesses='The paper relies on several complex theoretical constructs, and some explanations may not be sufficiently detailed for all readers. The specific improvements over existing methods, aside from heterogeneity, might not be entirely clear.' Questions='How does H-PSRO handle scalability issues when the number of unique roles or policies increases significantly?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The model is claimed to be the first open-source solution for zero-shot singing voice conversion that works for both human and non-human timbres, which is a significant contribution to the field. 2. The approach addresses and potentially overcomes limitations of previous methods by improving feature disentanglement and reducing dependencies. 3. The creation and use of a large-scale dataset specifically for this purpose adds to the robustness and applicability of the model.' Weaknesses='1. The paper does not provide explicit details on how well the proposed model performs relative to existing benchmarks in terms of concrete objective metrics. 2. The complexity of the model and the necessity of a large dataset may limit accessibility and reproducibility for other researchers.' Questions='1. How does SaMoye ensure the disentanglement of content, timbre, and pitch features without losing semantic information? 2. What baseline models or methods were used for comparison in the objective and subjective experiments? 3. How does the performance vary when converting to non-human timbres, and could you provide examples?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The model is claimed to be the first open-source solution for zero-shot singing voice conversion that works for both human and non-human timbres, which is a significant contribution to the field. 2. The approach addresses and potentially overcomes limitations of previous methods by improving feature disentanglement and reducing dependencies. 3. The creation and use of a large-scale dataset specifically for this purpose adds to the robustness and applicability of the model.' Weaknesses='1. The paper does not provide explicit details on how well the proposed model performs relative to existing benchmarks in terms of concrete objective metrics. 2. The complexity of the model and the necessity of a large dataset may limit accessibility and reproducibility for other researchers.' Questions='1. How does SaMoye ensure the disentanglement of content, timbre, and pitch features without losing semantic information? 2. What baseline models or methods were used for comparison in the objective and subjective experiments? 3. How does the performance vary when converting to non-human timbres, and could you provide examples?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel framework (Differentiable Map Priors) that effectively improves 3D object detection and semantic map segmentation by incorporating spatial priors from historical data. The approach integrates easily with existing 3D perception systems and shows significant improvements across several architectures on the nuScenes dataset.' Weaknesses='The paper may not address potential limitations or challenges in real-world deployment of the proposed framework, such as handling dynamic environments or evaluating the scalability of the approach across diverse and new geographical areas.' Questions='How does the proposed framework handle dynamic changes in the environment, such as new construction or temporary obstacles? Is there a strategy for updating the learned priors in such cases?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel framework (Differentiable Map Priors) that effectively improves 3D object detection and semantic map segmentation by incorporating spatial priors from historical data. The approach integrates easily with existing 3D perception systems and shows significant improvements across several architectures on the nuScenes dataset.' Weaknesses='The paper may not address potential limitations or challenges in real-world deployment of the proposed framework, such as handling dynamic environments or evaluating the scalability of the approach across diverse and new geographical areas.' Questions='How does the proposed framework handle dynamic changes in the environment, such as new construction or temporary obstacles? Is there a strategy for updating the learned priors in such cases?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper proposes a novel graph attention method (RGA-IB) that improves robustness against adversarial attacks by minimizing the Information Bottleneck loss. The experiments demonstrate significant improvements over existing methods in node classification tasks under such attacks.' Weaknesses=\"The paper's novelty is somewhat limited, as it draws heavily on existing IB principles without introducing fundamentally new concepts. The evaluation could be more comprehensive by including more baseline comparisons, datasets, and attack scenarios. The connection between lower IB loss and robustness is intriguing but not yet fully substantiated within the paper.\" Questions='How does RGA-IB compare with recent methods such as RUNG or ElasticGNN? Can the approach be evaluated on heterophilic datasets or under more diverse adversarial attacks?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper proposes a novel graph attention method (RGA-IB) that improves robustness against adversarial attacks by minimizing the Information Bottleneck loss. The experiments demonstrate significant improvements over existing methods in node classification tasks under such attacks.' Weaknesses=\"The paper's novelty is somewhat limited, as it draws heavily on existing IB principles without introducing fundamentally new concepts. The evaluation could be more comprehensive by including more baseline comparisons, datasets, and attack scenarios. The connection between lower IB loss and robustness is intriguing but not yet fully substantiated within the paper.\" Questions='How does RGA-IB compare with recent methods such as RUNG or ElasticGNN? Can the approach be evaluated on heterophilic datasets or under more diverse adversarial attacks?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths='The paper introduces a novel deep learning model with dual attention mechanisms and compiles a comprehensive immunogenicity dataset with over 9,500 sequences, which significantly enhances the research in reverse vaccinology.' Weaknesses='The paper may lack a detailed comparative analysis with alternative models and the implications of using different features may not be fully explored.' Questions='How does ProVaccine specifically improve over previous models in terms of architecture? Are there specific types of vaccines or pathogens where ProVaccine might have limitations?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths='The paper introduces a novel deep learning model with dual attention mechanisms and compiles a comprehensive immunogenicity dataset with over 9,500 sequences, which significantly enhances the research in reverse vaccinology.' Weaknesses='The paper may lack a detailed comparative analysis with alternative models and the implications of using different features may not be fully explored.' Questions='How does ProVaccine specifically improve over previous models in terms of architecture? Are there specific types of vaccines or pathogens where ProVaccine might have limitations?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses a significant problem in multi-view classification by introducing a computational trust-based discounting method. It enhances multi-view classification where conflicting views can reduce reliability. The novel use of an instance-wise probability-sensitive trust discounting mechanism is well-motivated and offers a refined approach to dealing with inconsistencies in real-world datasets.' Weaknesses=\"While the proposed solution is interesting, the abstract does not provide sufficient detail on the inner workings or assumptions of the method. Also, the results are context-limited; only six datasets are tested. There's no discussion on computational complexity or how this method scales with larger datasets. The evaluation metrics, although relevant, could be strengthened with additional real-world scenario validations.\" Questions='What kind of computational complexity does the proposed method introduce, especially in large-scale datasets? How does the trust discounting affect performance across highly diverse datasets or when scaling in real-time applications?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses a significant problem in multi-view classification by introducing a computational trust-based discounting method. It enhances multi-view classification where conflicting views can reduce reliability. The novel use of an instance-wise probability-sensitive trust discounting mechanism is well-motivated and offers a refined approach to dealing with inconsistencies in real-world datasets.' Weaknesses=\"While the proposed solution is interesting, the abstract does not provide sufficient detail on the inner workings or assumptions of the method. Also, the results are context-limited; only six datasets are tested. There's no discussion on computational complexity or how this method scales with larger datasets. The evaluation metrics, although relevant, could be strengthened with additional real-world scenario validations.\" Questions='What kind of computational complexity does the proposed method introduce, especially in large-scale datasets? How does the trust discounting affect performance across highly diverse datasets or when scaling in real-time applications?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='The paper addresses an important aspect of interpretability in large language models, focusing on disentangling polysemantic neurons. It proposes a novel measure for neuronal entanglement and provides a new experimental framework that demonstrates its effectiveness. The work potentially opens up new directions for optimizing model performance under sparsity constraints.' Weaknesses='The robustness of the proposed method for out-of-distribution data is not thoroughly discussed. Additionally, more clarity is needed regarding the practical aspects of the implementation, such as computational overhead and runtime implications of the proposed mixture of experts approach. The paper could also benefit from a deeper exploration of related work in neuronal entanglement.' Questions='How does the proposed measure of entanglement compare to existing metrics in the literature? Are there specific scenarios where the method fails or does not offer significant benefits? What are the implications of the proposed approach on model scalability and inference speed?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='The paper addresses an important aspect of interpretability in large language models, focusing on disentangling polysemantic neurons. It proposes a novel measure for neuronal entanglement and provides a new experimental framework that demonstrates its effectiveness. The work potentially opens up new directions for optimizing model performance under sparsity constraints.' Weaknesses='The robustness of the proposed method for out-of-distribution data is not thoroughly discussed. Additionally, more clarity is needed regarding the practical aspects of the implementation, such as computational overhead and runtime implications of the proposed mixture of experts approach. The paper could also benefit from a deeper exploration of related work in neuronal entanglement.' Questions='How does the proposed measure of entanglement compare to existing metrics in the literature? Are there specific scenarios where the method fails or does not offer significant benefits? What are the implications of the proposed approach on model scalability and inference speed?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel approach that leverages quantum computations to enhance parameter efficiency in fine-tuning, demonstrating significant reductions in the number of trainable parameters while maintaining competitive performance.' Weaknesses='The paper may not fully address potential computational complexities introduced by leveraging quantum computations and could further elaborate on the comparison with traditional PEFT methods beyond parameter count.' Questions='How does Quantum-PEFT impact computational overhead in practical implementations, and how does it compare quantitatively with other PEFT methods in terms of performance on large-scale benchmarks?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel approach that leverages quantum computations to enhance parameter efficiency in fine-tuning, demonstrating significant reductions in the number of trainable parameters while maintaining competitive performance.' Weaknesses='The paper may not fully address potential computational complexities introduced by leveraging quantum computations and could further elaborate on the comparison with traditional PEFT methods beyond parameter count.' Questions='How does Quantum-PEFT impact computational overhead in practical implementations, and how does it compare quantitatively with other PEFT methods in terms of performance on large-scale benchmarks?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper introduces BILBO, a novel bilevel optimization approach offering theoretical guarantees and practicality in solving noisy, constrained problems efficiently. BILBO's sampling strategy ensures feasible solutions with low regret, and it's designed for flexibility with decoupled function evaluations. Additionally, the empirical results on synthetic and real-world tasks highlight its effectiveness.\" Weaknesses=\"Motivations for specific design choices, especially around the decoupled setting and query strategies, need more clarity. There are limited comparisons with current state-of-the-art bilevel optimization methods, affecting the assessment of BILBO's performance. Furthermore, the absence of provided code hinders reproducibility and further exploration by researchers.\" Questions=\"Can you further clarify the rationale behind focusing on decoupled settings? How does BILBO compare in terms of regret bounds to other bilevel BO methods? Why was there a sharp drop in BILBO's performance at around 150 queries? Would additional comparisons with advanced baselines better showcase BILBO's advantages? Lastly, why does Nested often perform similarly or worse than TrustedRandom in tests?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper introduces BILBO, a novel bilevel optimization approach offering theoretical guarantees and practicality in solving noisy, constrained problems efficiently. BILBO's sampling strategy ensures feasible solutions with low regret, and it's designed for flexibility with decoupled function evaluations. Additionally, the empirical results on synthetic and real-world tasks highlight its effectiveness.\" Weaknesses=\"Motivations for specific design choices, especially around the decoupled setting and query strategies, need more clarity. There are limited comparisons with current state-of-the-art bilevel optimization methods, affecting the assessment of BILBO's performance. Furthermore, the absence of provided code hinders reproducibility and further exploration by researchers.\" Questions=\"Can you further clarify the rationale behind focusing on decoupled settings? How does BILBO compare in terms of regret bounds to other bilevel BO methods? Why was there a sharp drop in BILBO's performance at around 150 queries? Would additional comparisons with advanced baselines better showcase BILBO's advantages? Lastly, why does Nested often perform similarly or worse than TrustedRandom in tests?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel multi-agent collaborative data selection mechanism for LLM pretraining, which significantly enhances data efficiency and accelerates convergence. The proposed method demonstrates substantial performance gains over state-of-the-art methods across multiple language model benchmarks.' Weaknesses='The novelty of the multi-agent approach and its differentiation from existing methods are not thoroughly explained. The paper could benefit from a more detailed comparison with related work and an exploration of potential limitations concerning scalability and complexity.' Questions='How scalable is the proposed multi-agent system when applied to extremely large datasets commonly used in LLM pretraining? Are there any limitations or challenges associated with integrating this approach into existing LLM training pipelines?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel multi-agent collaborative data selection mechanism for LLM pretraining, which significantly enhances data efficiency and accelerates convergence. The proposed method demonstrates substantial performance gains over state-of-the-art methods across multiple language model benchmarks.' Weaknesses='The novelty of the multi-agent approach and its differentiation from existing methods are not thoroughly explained. The paper could benefit from a more detailed comparison with related work and an exploration of potential limitations concerning scalability and complexity.' Questions='How scalable is the proposed multi-agent system when applied to extremely large datasets commonly used in LLM pretraining? Are there any limitations or challenges associated with integrating this approach into existing LLM training pipelines?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents a novel study on data scaling laws in the context of robotics, a field where it has not been extensively explored. The empirical investigation is thorough, involving a large number of demonstrations and actual robot rollouts. The insights into the importance of diversity in environments and objects are highly valuable for the development of generalizable robot policies. The proposed data collection strategy is practical and efficient.' Weaknesses='The study might be limited to the specific tasks and environments tested, which could affect the generalizability of the conclusions. The focus on single-object categories could limit the applicability of the findings to more diverse and complex manipulation scenarios in robotics. The paper could also explore more complex tasks to strengthen its claims on generalization.' Questions='How do the findings translate to more complex and varied tasks involving objects of different categories and in more dynamic environments? Are there plans to expand the study to include a wider range of robotic tasks and settings?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents a novel study on data scaling laws in the context of robotics, a field where it has not been extensively explored. The empirical investigation is thorough, involving a large number of demonstrations and actual robot rollouts. The insights into the importance of diversity in environments and objects are highly valuable for the development of generalizable robot policies. The proposed data collection strategy is practical and efficient.' Weaknesses='The study might be limited to the specific tasks and environments tested, which could affect the generalizability of the conclusions. The focus on single-object categories could limit the applicability of the findings to more diverse and complex manipulation scenarios in robotics. The paper could also explore more complex tasks to strengthen its claims on generalization.' Questions='How do the findings translate to more complex and varied tasks involving objects of different categories and in more dynamic environments? Are there plans to expand the study to include a wider range of robotic tasks and settings?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The introduction of a self-evolving framework inspired by the Gödel machine represents a novel approach that allows agents to improve recursively. The method's ability to leverage LLMs for dynamic self-improvement without fixed routines is a significant advancement. Empirical evidence suggests superior performance over manually crafted agents in various tasks, which highlights the method's potential impact on AI agent development.\" Weaknesses='The abstract does not provide specific details on the experimental setup, baseline comparisons, or potential limitations of the method. This leaves questions about the generalizability of results across different domains.' Questions='How does Gödel Agent ensure continuous improvement without overfitting to specific tasks? What metrics are used to evaluate its performance compared to traditional agents?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The introduction of a self-evolving framework inspired by the Gödel machine represents a novel approach that allows agents to improve recursively. The method's ability to leverage LLMs for dynamic self-improvement without fixed routines is a significant advancement. Empirical evidence suggests superior performance over manually crafted agents in various tasks, which highlights the method's potential impact on AI agent development.\" Weaknesses='The abstract does not provide specific details on the experimental setup, baseline comparisons, or potential limitations of the method. This leaves questions about the generalizability of results across different domains.' Questions='How does Gödel Agent ensure continuous improvement without overfitting to specific tasks? What metrics are used to evaluate its performance compared to traditional agents?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a substantial and diverse benchmark, U-MATH, for evaluating LLMs on university-level mathematical problems, which is a significant advancement over existing evaluations that focus on elementary and high-school problems. The inclusion of multimodal problems is also a noteworthy contribution as it broadens the scope of evaluation to include visual elements. The open-sourcing of U-MATH and accompanying evaluation code enhances the transparency and accessibility of the research.' Weaknesses='The reliance on LLMs to judge the correctness of solutions could introduce biases or errors, as evidenced by the F1-score of 80% for solution assessment. This poses questions about the reliability of the evaluation process. Additionally, the paper does not provide detailed analysis on why LLMs struggle significantly more with visual problems, which could have been beneficial for future improvements.' Questions='How does the presence of multimodal problems impact the overall performance of LLMs compared to text-only problems? Could the authors provide more insight into the types of mathematical reasoning that are particularly challenging for LLMs?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a substantial and diverse benchmark, U-MATH, for evaluating LLMs on university-level mathematical problems, which is a significant advancement over existing evaluations that focus on elementary and high-school problems. The inclusion of multimodal problems is also a noteworthy contribution as it broadens the scope of evaluation to include visual elements. The open-sourcing of U-MATH and accompanying evaluation code enhances the transparency and accessibility of the research.' Weaknesses='The reliance on LLMs to judge the correctness of solutions could introduce biases or errors, as evidenced by the F1-score of 80% for solution assessment. This poses questions about the reliability of the evaluation process. Additionally, the paper does not provide detailed analysis on why LLMs struggle significantly more with visual problems, which could have been beneficial for future improvements.' Questions='How does the presence of multimodal problems impact the overall performance of LLMs compared to text-only problems? Could the authors provide more insight into the types of mathematical reasoning that are particularly challenging for LLMs?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper addresses an important area in AI, namely the mathematical reasoning capabilities of large language models using an improved evaluation benchmark (GSM-Symbolic) which offers more reliable metrics. The introduction of GSM-Symbolic is a significant contribution as it provides a more nuanced insight into the current limitations of LLMs in the domain of mathematical reasoning.' Weaknesses=\"The study reveals the fragility of current LLMs' reasoning capabilities but does not propose new methods to overcome these limitations. The reliance on synthetic benchmarks may not fully capture real-world complexities faced in mathematical reasoning tasks. The performance degradation observed might be influenced indirectly by other factors not fully isolated in the study.\" Questions='Can the GSM-Symbolic benchmark be adapted for other domains beyond mathematics to evaluate reasoning capabilities? How does the benchmark ensure diversity without straying into overly unrealistic problem settings?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper addresses an important area in AI, namely the mathematical reasoning capabilities of large language models using an improved evaluation benchmark (GSM-Symbolic) which offers more reliable metrics. The introduction of GSM-Symbolic is a significant contribution as it provides a more nuanced insight into the current limitations of LLMs in the domain of mathematical reasoning.' Weaknesses=\"The study reveals the fragility of current LLMs' reasoning capabilities but does not propose new methods to overcome these limitations. The reliance on synthetic benchmarks may not fully capture real-world complexities faced in mathematical reasoning tasks. The performance degradation observed might be influenced indirectly by other factors not fully isolated in the study.\" Questions='Can the GSM-Symbolic benchmark be adapted for other domains beyond mathematics to evaluate reasoning capabilities? How does the benchmark ensure diversity without straying into overly unrealistic problem settings?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides a novel method to quantify different sources of diversity in text-conditioned generative models, separating prompt-induced diversity from model-induced diversity. By applying matrix-based information measures, the approach allows for a clearer understanding of the generation process, which is valuable for the field.' Weaknesses=\"The paper could improve in clarity and detail regarding the implementation and potential limitations of the proposed method. Furthermore, the distinction between existing and novel contributions could be more explicit to highlight the work's original aspects.\" Questions='How scalable is the proposed method when applied to large-scale generative models with diverse datasets? Are there any assumptions in the theoretical framework that might limit its applicability to certain types of prompt-based models?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides a novel method to quantify different sources of diversity in text-conditioned generative models, separating prompt-induced diversity from model-induced diversity. By applying matrix-based information measures, the approach allows for a clearer understanding of the generation process, which is valuable for the field.' Weaknesses=\"The paper could improve in clarity and detail regarding the implementation and potential limitations of the proposed method. Furthermore, the distinction between existing and novel contributions could be more explicit to highlight the work's original aspects.\" Questions='How scalable is the proposed method when applied to large-scale generative models with diverse datasets? Are there any assumptions in the theoretical framework that might limit its applicability to certain types of prompt-based models?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces Flow with Interpolant Guidance (FIG), which enhances image reconstruction in challenging scenarios. Experimentally, it shows state-of-the-art results in various tasks, particularly handling high noise and ill-posedness effectively. The approach is efficiently implementable as a modification to existing diffusion or flow-matching models.' Weaknesses='The presentation of the motivation behind measurement interpolants and their clear advantages over existing methods requires more clarity. Furthermore, a deep dive into the limitations of current approaches and how FIG specifically addresses these would strengthen the paper.' Questions='1) How does the proposed method handle cases beyond the high-noise/ill-posedness scenarios mentioned, and is it equally effective?\\n2) Could the authors elaborate further on the theoretical assumptions and provide more intuitive explanations for the theorems?\\n3) Are there any limitations in FIG when applied to other non-linear or real-world datasets?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces Flow with Interpolant Guidance (FIG), which enhances image reconstruction in challenging scenarios. Experimentally, it shows state-of-the-art results in various tasks, particularly handling high noise and ill-posedness effectively. The approach is efficiently implementable as a modification to existing diffusion or flow-matching models.' Weaknesses='The presentation of the motivation behind measurement interpolants and their clear advantages over existing methods requires more clarity. Furthermore, a deep dive into the limitations of current approaches and how FIG specifically addresses these would strengthen the paper.' Questions='1) How does the proposed method handle cases beyond the high-noise/ill-posedness scenarios mentioned, and is it equally effective?\\n2) Could the authors elaborate further on the theoretical assumptions and provide more intuitive explanations for the theorems?\\n3) Are there any limitations in FIG when applied to other non-linear or real-world datasets?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel approach, SecureGS, which effectively addresses existing limitations in 3D Gaussian Splatting (3DGS) steganography, such as rendering fidelity and computational demands, by introducing a hybrid Gaussian encryption mechanism and density region-aware strategies.' Weaknesses='The complexities introduced by the proposed encryption and anchor strategies might impede practical implementation and usability in some scenarios where computational resources are limited.' Questions='How does the security mechanism ensure protection against specific attack vectors without compromising rendering performance?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel approach, SecureGS, which effectively addresses existing limitations in 3D Gaussian Splatting (3DGS) steganography, such as rendering fidelity and computational demands, by introducing a hybrid Gaussian encryption mechanism and density region-aware strategies.' Weaknesses='The complexities introduced by the proposed encryption and anchor strategies might impede practical implementation and usability in some scenarios where computational resources are limited.' Questions='How does the security mechanism ensure protection against specific attack vectors without compromising rendering performance?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper introduces an innovative model for handling irregular multivariate time series by incorporating both temporal and inter-variable dependencies at multiple scales through a hierarchical graph network approach. The experimental results demonstrate the model's effectiveness across various datasets.\" Weaknesses='The paper could improve by providing clearer explanations and visualizations of the hierarchical graph structures and their transformation processes. The handling of missing values and model complexity analysis are not addressed, which might affect its applicability in real-world scenarios.' Questions='How does the model handle missing values in time series data? Is there a detailed analysis of the computational complexity compared to other models?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper introduces an innovative model for handling irregular multivariate time series by incorporating both temporal and inter-variable dependencies at multiple scales through a hierarchical graph network approach. The experimental results demonstrate the model's effectiveness across various datasets.\" Weaknesses='The paper could improve by providing clearer explanations and visualizations of the hierarchical graph structures and their transformation processes. The handling of missing values and model complexity analysis are not addressed, which might affect its applicability in real-world scenarios.' Questions='How does the model handle missing values in time series data? Is there a detailed analysis of the computational complexity compared to other models?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a novel aspect of watermarking in Large Language Models by focusing on the imperceptibility of watermarks. It introduces Water-Probe, an identification algorithm that effectively detects watermark presence with low false positives. The Water-Bag strategy is proposed to enhance watermark imperceptibility, which could be a valuable addition to the field.' Weaknesses='The paper may lack depth in exploring a wider application of the proposed Water-Probe to diverse watermarking techniques. It is not entirely clear how comprehensive the experimental setup is in terms of watermarking schemes. The distinction between watermarking and related concepts such as fingerprinting is not thoroughly discussed, which might cause confusion.' Questions='How well does Water-Probe perform across a broader spectrum of watermarking methods? What specific measures were taken to ensure the randomness of watermark key selection in the Water-Bag strategy? Is there a risk that users could exploit the identification algorithm for unintended purposes?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a novel aspect of watermarking in Large Language Models by focusing on the imperceptibility of watermarks. It introduces Water-Probe, an identification algorithm that effectively detects watermark presence with low false positives. The Water-Bag strategy is proposed to enhance watermark imperceptibility, which could be a valuable addition to the field.' Weaknesses='The paper may lack depth in exploring a wider application of the proposed Water-Probe to diverse watermarking techniques. It is not entirely clear how comprehensive the experimental setup is in terms of watermarking schemes. The distinction between watermarking and related concepts such as fingerprinting is not thoroughly discussed, which might cause confusion.' Questions='How well does Water-Probe perform across a broader spectrum of watermarking methods? What specific measures were taken to ensure the randomness of watermark key selection in the Water-Bag strategy? Is there a risk that users could exploit the identification algorithm for unintended purposes?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='- The paper formalizes DPA losses in terms of discrete reasoning problems, which is a novel approach. - It provides a new, formal perspective on understanding and developing DPA loss functions. - The work seems well-structured and offers valuable insights into the DPA loss landscape.' Weaknesses=\"- The paper's approach may add complexity, especially for readers unfamiliar with the symbolic reasoning involved. - There could be more examples or case studies to demonstrate practical implementations of the framework.\" Questions=\"- How well does the proposed framework scale with very large datasets or models? - Are there any potential limitations or edge cases identified in the framework's application to various DPA losses?\"\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='- The paper formalizes DPA losses in terms of discrete reasoning problems, which is a novel approach. - It provides a new, formal perspective on understanding and developing DPA loss functions. - The work seems well-structured and offers valuable insights into the DPA loss landscape.' Weaknesses=\"- The paper's approach may add complexity, especially for readers unfamiliar with the symbolic reasoning involved. - There could be more examples or case studies to demonstrate practical implementations of the framework.\" Questions=\"- How well does the proposed framework scale with very large datasets or models? - Are there any potential limitations or edge cases identified in the framework's application to various DPA losses?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper addresses a significant challenge in the domain of audio-driven talking video generation - enhancing identity consistency and emotional expression, which is crucial for realistic applications. The approach using memory-guided temporal modeling and emotion-adaptive layer normalization is innovative and shows an improvement over state-of-the-art methods. The experiments are extensive and the results show superiority in terms of emotion-audio alignment and overall quality.' Weaknesses='The abstract lacks details on quantitative metrics used for evaluation, which makes it difficult to assess the precise improvements. Moreover, the dependency on a large-scale, high-quality dataset may limit the applicability to lower-quality scenarios.' Questions='1. How well does the method perform on lower-quality or noisy data? 2. How does the approach handle variations in audio languages not available in the training dataset?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper addresses a significant challenge in the domain of audio-driven talking video generation - enhancing identity consistency and emotional expression, which is crucial for realistic applications. The approach using memory-guided temporal modeling and emotion-adaptive layer normalization is innovative and shows an improvement over state-of-the-art methods. The experiments are extensive and the results show superiority in terms of emotion-audio alignment and overall quality.' Weaknesses='The abstract lacks details on quantitative metrics used for evaluation, which makes it difficult to assess the precise improvements. Moreover, the dependency on a large-scale, high-quality dataset may limit the applicability to lower-quality scenarios.' Questions='1. How well does the method perform on lower-quality or noisy data? 2. How does the approach handle variations in audio languages not available in the training dataset?'\n",
      "Finished gpt4o on data/test_data/zero_shot/test_data_2025_abstract_prompts.jsonl, saved => results/gpt4o/test_data_2025_abstract_prompts_gpt4o_results.jsonl\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper proposes an innovative approach to integrate weight-entanglement and gradient-based neural architecture search (NAS), which merges two previously separate sub-fields. This integration enhances performance, training properties, and performance efficiency while maintaining memory efficiency. The availability of code increases the transparency and reproducibility of the research.' Weaknesses='The paper might lack detailed theoretical analyses or benchmarks that conclusively prove the superiority of the proposed approach over existing methods across various domains. The compatibility and efficiency on diverse tasks and datasets need more comprehensive empirical evidence.' Questions='How does the proposed method generalize to other architecture search spaces or tasks beyond those tested? Are there specific scenarios where the integration of weight-entanglement with gradient-based methods might not be beneficial?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper proposes an innovative approach to integrate weight-entanglement and gradient-based neural architecture search (NAS), which merges two previously separate sub-fields. This integration enhances performance, training properties, and performance efficiency while maintaining memory efficiency. The availability of code increases the transparency and reproducibility of the research.' Weaknesses='The paper might lack detailed theoretical analyses or benchmarks that conclusively prove the superiority of the proposed approach over existing methods across various domains. The compatibility and efficiency on diverse tasks and datasets need more comprehensive empirical evidence.' Questions='How does the proposed method generalize to other architecture search spaces or tasks beyond those tested? Are there specific scenarios where the integration of weight-entanglement with gradient-based methods might not be beneficial?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel approach using contrastive loss functions in the context of global epistasis models, offering an improvement over the traditional MSE loss. The technique is validated through consistent improved performance on benchmark tasks, demonstrating its practical utility.' Weaknesses=\"The approach's reliance on a single type of loss function may limit its applicability across different types of biological data. The paper could benefit from a broader evaluation across various datasets to confirm the generality of the proposed method.\" Questions='How does the proposed method perform on other types of biological sequence data beyond the benchmark tasks mentioned? Can this approach be generalized to other domains outside of protein engineering?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel approach using contrastive loss functions in the context of global epistasis models, offering an improvement over the traditional MSE loss. The technique is validated through consistent improved performance on benchmark tasks, demonstrating its practical utility.' Weaknesses=\"The approach's reliance on a single type of loss function may limit its applicability across different types of biological data. The paper could benefit from a broader evaluation across various datasets to confirm the generality of the proposed method.\" Questions='How does the proposed method perform on other types of biological sequence data beyond the benchmark tasks mentioned? Can this approach be generalized to other domains outside of protein engineering?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The approach introduces a novel lightweight method for lifelong test-time adaptation that significantly reduces the number of additional parameters needed. FEATHER is capable of functioning without access to source data and can be integrated with existing methods, offering robustness against error accumulation.' Weaknesses=\"The paper doesn't extensively detail the comparison of computational costs with other methods. The performance claims, while impressive, rely heavily on specific benchmark datasets, which may not represent all possible real-world scenarios for test-time adaptation.\" Questions='How does FEATHER perform on non-image data or models other than CNNs? What are the implications of potential limitations in settings outside the tested benchmark datasets? How effective is FEATHER in adapting over extremely long periods or highly dynamic distributional changes?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The approach introduces a novel lightweight method for lifelong test-time adaptation that significantly reduces the number of additional parameters needed. FEATHER is capable of functioning without access to source data and can be integrated with existing methods, offering robustness against error accumulation.' Weaknesses=\"The paper doesn't extensively detail the comparison of computational costs with other methods. The performance claims, while impressive, rely heavily on specific benchmark datasets, which may not represent all possible real-world scenarios for test-time adaptation.\" Questions='How does FEATHER perform on non-image data or models other than CNNs? What are the implications of potential limitations in settings outside the tested benchmark datasets? How effective is FEATHER in adapting over extremely long periods or highly dynamic distributional changes?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel supervised feature selection approach leveraging neural estimation of mutual information, allowing for the evaluation of feature subsets as ensembles. This approach captures sophisticated feature-target relationships and demonstrates potential for improved feature selection accuracy.' Weaknesses='The paper does not provide extensive details on the experimental setup, datasets used, or comparisons with comprehensive benchmarks, which makes it difficult to fully assess the effectiveness and generalizability of the proposed method. Additionally, the computational complexity and scalability of the method are not discussed.' Questions='1. How does the proposed method compare with existing feature selection methods on large, real-world datasets?\\n2. What are the computational costs associated with the neural estimation approach, and how does it scale with large feature sets?\\n3. Are there specific types of feature-target relationships where this method excels compared to traditional methods?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel supervised feature selection approach leveraging neural estimation of mutual information, allowing for the evaluation of feature subsets as ensembles. This approach captures sophisticated feature-target relationships and demonstrates potential for improved feature selection accuracy.' Weaknesses='The paper does not provide extensive details on the experimental setup, datasets used, or comparisons with comprehensive benchmarks, which makes it difficult to fully assess the effectiveness and generalizability of the proposed method. Additionally, the computational complexity and scalability of the method are not discussed.' Questions='1. How does the proposed method compare with existing feature selection methods on large, real-world datasets?\\n2. What are the computational costs associated with the neural estimation approach, and how does it scale with large feature sets?\\n3. Are there specific types of feature-target relationships where this method excels compared to traditional methods?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides a novel perspective on understanding margins in contrastive learning by using gradient analysis, which could enhance theoretical insights into the method. Experimental results show improved generalization performance across datasets, indicating practical applicability.' Weaknesses='The explanation of the methodology, particularly in relation to how gradient effects are decomposed and analyzed, could be clearer. The theoretical insights need stronger empirical connections. The impact of additional factors beyond emphasis on positive samples and gradient scaling seems underexplored.' Questions='How do the proposed gradient-based insights translate into practical guidelines for model training? Are there specific scenarios or datasets where the proposed method is most beneficial?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides a novel perspective on understanding margins in contrastive learning by using gradient analysis, which could enhance theoretical insights into the method. Experimental results show improved generalization performance across datasets, indicating practical applicability.' Weaknesses='The explanation of the methodology, particularly in relation to how gradient effects are decomposed and analyzed, could be clearer. The theoretical insights need stronger empirical connections. The impact of additional factors beyond emphasis on positive samples and gradient scaling seems underexplored.' Questions='How do the proposed gradient-based insights translate into practical guidelines for model training? Are there specific scenarios or datasets where the proposed method is most beneficial?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a novel approach to improve unsupervised domain adaptation by minimizing conditional symmetric support divergence, which is a significant improvement over traditional methods relying on covariate shift assumptions. The introduction of a theoretical target risk bound provides a strong theoretical basis for the approach, enhancing its credibility.' Weaknesses='The methodology may require complex implementation efforts due to the new concepts introduced, such as the conditional symmetric support divergence and the theoretical target risk bound. The empirical evaluation, though promising, might be limited to specific UDA benchmark tasks, which may not fully demonstrate the generalizability of the approach across various real-world applications.' Questions='How does the proposed method perform across a wider variety of real-world datasets beyond the standard UDA benchmarks? Are there any potential limitations in scalability of the approach due to increased computational or implementation complexity?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a novel approach to improve unsupervised domain adaptation by minimizing conditional symmetric support divergence, which is a significant improvement over traditional methods relying on covariate shift assumptions. The introduction of a theoretical target risk bound provides a strong theoretical basis for the approach, enhancing its credibility.' Weaknesses='The methodology may require complex implementation efforts due to the new concepts introduced, such as the conditional symmetric support divergence and the theoretical target risk bound. The empirical evaluation, though promising, might be limited to specific UDA benchmark tasks, which may not fully demonstrate the generalizability of the approach across various real-world applications.' Questions='How does the proposed method perform across a wider variety of real-world datasets beyond the standard UDA benchmarks? Are there any potential limitations in scalability of the approach due to increased computational or implementation complexity?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=6 Confidence=4 Strengths='The introduction of TRACE as a new benchmark addresses a gap in evaluating continual learning capabilities of LLMs. The approach highlights the potential issues of catastrophic forgetting and explores solutions to mitigate it, such as the Reasoning-augmented Continual Learning (RCL).' Weaknesses='While TRACE provides a new benchmark, it remains uncertain how it accounts for prior exposure of models to similar datasets during instruction tuning, which could affect the validity of results. The paper could benefit from a more extensive comparison with existing benchmarks and techniques in continual learning.' Questions='How does TRACE ensure that it challenges models without recalling prior exposure during instruction tuning? Could you elucidate how the reasoning tasks were chosen to mitigate the loss of general abilities in the LLMs?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=6 Confidence=4 Strengths='The introduction of TRACE as a new benchmark addresses a gap in evaluating continual learning capabilities of LLMs. The approach highlights the potential issues of catastrophic forgetting and explores solutions to mitigate it, such as the Reasoning-augmented Continual Learning (RCL).' Weaknesses='While TRACE provides a new benchmark, it remains uncertain how it accounts for prior exposure of models to similar datasets during instruction tuning, which could affect the validity of results. The paper could benefit from a more extensive comparison with existing benchmarks and techniques in continual learning.' Questions='How does TRACE ensure that it challenges models without recalling prior exposure during instruction tuning? Could you elucidate how the reasoning tasks were chosen to mitigate the loss of general abilities in the LLMs?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper presents an innovative probabilistic graphical model to address label noise in graph neural networks, which is a less explored area compared to graph perturbation and attack.\\n2. The proposed PRGNN framework appears effective in high noise-rate scenarios and on heterophilous graphs, addressing common weaknesses of existing methods.\\n3. The paper includes extensive experiments to validate the robustness and effectiveness of the proposed models.' Weaknesses='1. The reliance on a probabilistic graphical model may introduce complexity, and it is unclear how it compares to simpler or alternative approaches without such a model.\\n2. It would be beneficial to have more clarity on how the model particularly addresses and performs better on heterophilous graphs, beyond standard benchmarks.\\n3. The paper does not clarify whether the proposed methods are applicable in inductive settings or only transductive ones.' Questions='1. Could the authors provide more insights into potential challenges or limitations when extending the PRGNN framework to inductive learning scenarios?\\n2. Given the extensive experiments, what are the main conditions or constraints under which PRGNN significantly outperforms other methods?\\n3. Are there any real-world datasets or benchmarks that inherently contain the types of noise addressed in the paper, beyond synthetically generated data?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper presents an innovative probabilistic graphical model to address label noise in graph neural networks, which is a less explored area compared to graph perturbation and attack.\\n2. The proposed PRGNN framework appears effective in high noise-rate scenarios and on heterophilous graphs, addressing common weaknesses of existing methods.\\n3. The paper includes extensive experiments to validate the robustness and effectiveness of the proposed models.' Weaknesses='1. The reliance on a probabilistic graphical model may introduce complexity, and it is unclear how it compares to simpler or alternative approaches without such a model.\\n2. It would be beneficial to have more clarity on how the model particularly addresses and performs better on heterophilous graphs, beyond standard benchmarks.\\n3. The paper does not clarify whether the proposed methods are applicable in inductive settings or only transductive ones.' Questions='1. Could the authors provide more insights into potential challenges or limitations when extending the PRGNN framework to inductive learning scenarios?\\n2. Given the extensive experiments, what are the main conditions or constraints under which PRGNN significantly outperforms other methods?\\n3. Are there any real-world datasets or benchmarks that inherently contain the types of noise addressed in the paper, beyond synthetically generated data?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The approach addresses two key issues of continual learning (catastrophic forgetting and loss of plasticity) simultaneously, which is innovative. UPGD demonstrates strong performance across various tasks in a challenging streaming learning setup. The method shows superior results in reinforcement learning experiments, indicating its robustness.' Weaknesses='The paper may lack clarity in the explanation of the utility-based perturbations and how they are applied. Additionally, the setup of experiments might benefit from more detailed descriptions to allow reproducibility and ensure the robustness of comparisons against other methods.' Questions='How sensitive is UPGD to the choice of hyperparameters, and are there any recommendations for their selection? Can the method be extended or adapted to other neural network architectures, and how well would it generalizes to different application domains?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The approach addresses two key issues of continual learning (catastrophic forgetting and loss of plasticity) simultaneously, which is innovative. UPGD demonstrates strong performance across various tasks in a challenging streaming learning setup. The method shows superior results in reinforcement learning experiments, indicating its robustness.' Weaknesses='The paper may lack clarity in the explanation of the utility-based perturbations and how they are applied. Additionally, the setup of experiments might benefit from more detailed descriptions to allow reproducibility and ensure the robustness of comparisons against other methods.' Questions='How sensitive is UPGD to the choice of hyperparameters, and are there any recommendations for their selection? Can the method be extended or adapted to other neural network architectures, and how well would it generalizes to different application domains?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper identifies a novel and specific vulnerability in current language and vision-language models that could impact their deployment.' Weaknesses='The scope of the vulnerability analysis could be expanded to include more diverse datasets and scenarios to firmly establish the generality of the findings.' Questions='Have potential mitigation strategies for permutation sensitivity been explored, or are there suggestions for architectural changes that might reduce this vulnerability?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper identifies a novel and specific vulnerability in current language and vision-language models that could impact their deployment.' Weaknesses='The scope of the vulnerability analysis could be expanded to include more diverse datasets and scenarios to firmly establish the generality of the findings.' Questions='Have potential mitigation strategies for permutation sensitivity been explored, or are there suggestions for architectural changes that might reduce this vulnerability?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents a novel quantization technique, Super Floating-Point (SuFP), that targets the balance between accuracy and computational efficiency without sacrificing performance. The proposed method achieves impressive improvements in computational capacity and energy efficiency. The use of multi-region piecewise quantization and tensor-wise scalable bias is an innovative approach. The paper provides extensive experimental results demonstrating significant advantage over state-of-the-art approaches.' Weaknesses='The paper does not thoroughly discuss potential limitations of SuFP, such as the trade-offs involved in its implementation and possible edge-case scenarios where it may not perform as expected. The practical implementation challenges of deploying SuFP in existing systems could be further elaborated.' Questions='Can the SuFP technique be adapted or generalized for other emerging domains or models beyond vision, language, and generative models? What are the potential impacts of SuFP on the hardware landscape beyond AI accelerators?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents a novel quantization technique, Super Floating-Point (SuFP), that targets the balance between accuracy and computational efficiency without sacrificing performance. The proposed method achieves impressive improvements in computational capacity and energy efficiency. The use of multi-region piecewise quantization and tensor-wise scalable bias is an innovative approach. The paper provides extensive experimental results demonstrating significant advantage over state-of-the-art approaches.' Weaknesses='The paper does not thoroughly discuss potential limitations of SuFP, such as the trade-offs involved in its implementation and possible edge-case scenarios where it may not perform as expected. The practical implementation challenges of deploying SuFP in existing systems could be further elaborated.' Questions='Can the SuFP technique be adapted or generalized for other emerging domains or models beyond vision, language, and generative models? What are the potential impacts of SuFP on the hardware landscape beyond AI accelerators?'\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Invalid \\escape: line 1 column 830 (char 829)\n",
      "Content:\n",
      "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper addresses a notable limitation in existing zeroth-order hard-thresholding methods by introducing a variance-reduced algorithm, which overcomes restrictions on the number of random directions.\", \"Weaknesses\": \"1. The paper does not thoroughly discuss the practical implications of the improved convergence rates and broader applicability.\\n2. Experimental validation, though promising, is limited in scope and does not fully explore diverse practical scenarios.\", \"Questions\": \"1. How does the proposed algorithm perform in real-world large-scale applications where zeroth-order optimization is highly beneficial?\\n2. Can the variance-reduction technique be applied to other zeroth-order optimization problems beyond $\\ell_0$ sparsity constraints? If so, how?\\n3. What are the computational costs associated with the proposed variance-reduction methods in terms of both time complexity and resource usage compared to previous algorithms?\"}\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The paper proposes a novel data distillation approach that significantly reduces dataset size while maintaining model performance. Utilizes efficient reverse-mode differentiation and latent-space factorization, showing impressive empirical results across tasks. The reduction in data size required for similar performance offers valuable implications for scaling large autoregressive models.' Weaknesses='Potential scalability issues for larger datasets or models may not be fully explored. The paper could benefit from a more detailed analysis of computational efficiency and training time comparisons to models trained from scratch.' Questions='How well does Farzi scale when applied to even larger models and datasets beyond those tested? What are the computational trade-offs in terms of training time and resources compared to conventional training with full datasets?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The paper proposes a novel data distillation approach that significantly reduces dataset size while maintaining model performance. Utilizes efficient reverse-mode differentiation and latent-space factorization, showing impressive empirical results across tasks. The reduction in data size required for similar performance offers valuable implications for scaling large autoregressive models.' Weaknesses='Potential scalability issues for larger datasets or models may not be fully explored. The paper could benefit from a more detailed analysis of computational efficiency and training time comparisons to models trained from scratch.' Questions='How well does Farzi scale when applied to even larger models and datasets beyond those tested? What are the computational trade-offs in terms of training time and resources compared to conventional training with full datasets?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The paper proposes a new oracle-efficient algorithm, PNLSVI, that is designed for offline reinforcement learning with non-linear function approximation, addressing a gap in the current literature.\\n2. The algorithm is innovative with three key components and achieves a minimax optimal instance-dependent regret, which is a significant theoretical contribution.\\n3. Extends prior results from simpler function classes to a more general framework, thus increasing applicability in the field.' Weaknesses='1. Although the theoretical contributions are significant, the lack of empirical evaluation to substantiate the practical benefits of the proposed algorithm limits its impact.\\n2. The dependencies of the regret bound on the function class complexity are tight, but it would be beneficial to know how these apply in practical scenarios versus theoretical ones.\\n3. Some assumptions in the paper might not be realistic or widely applicable, which could hinder the practical implementation of the algorithm.' Questions=\"1. Can the authors provide some insights or examples where the assumptions needed for this algorithm hold in real-world applications?\\n2. Is there an intuitive explanation for the need for the variance-based weighted regression scheme and how it specifically benefits the proposed method?\\n3. Are there further plans to extend this work to include empirical results or adapt existing datasets to test the algorithm's effectiveness?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The paper proposes a new oracle-efficient algorithm, PNLSVI, that is designed for offline reinforcement learning with non-linear function approximation, addressing a gap in the current literature.\\n2. The algorithm is innovative with three key components and achieves a minimax optimal instance-dependent regret, which is a significant theoretical contribution.\\n3. Extends prior results from simpler function classes to a more general framework, thus increasing applicability in the field.' Weaknesses='1. Although the theoretical contributions are significant, the lack of empirical evaluation to substantiate the practical benefits of the proposed algorithm limits its impact.\\n2. The dependencies of the regret bound on the function class complexity are tight, but it would be beneficial to know how these apply in practical scenarios versus theoretical ones.\\n3. Some assumptions in the paper might not be realistic or widely applicable, which could hinder the practical implementation of the algorithm.' Questions=\"1. Can the authors provide some insights or examples where the assumptions needed for this algorithm hold in real-world applications?\\n2. Is there an intuitive explanation for the need for the variance-based weighted regression scheme and how it specifically benefits the proposed method?\\n3. Are there further plans to extend this work to include empirical results or adapt existing datasets to test the algorithm's effectiveness?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper explores an important and novel area of research by applying behavioral game theory to analyze the social behavior of large language models (LLMs). The results provide new insights into the behaviors of LLMs in various game-theoretic settings, highlighting differences in their strategies and adaptations.' Weaknesses='The methodology for creating and testing game strategies from LLM outputs is not entirely clear and might lack robustness. The games selected, while representative, may not fully capture the range of social scenarios that LLMs might encounter. Furthermore, the results and conclusions could benefit from more detailed analysis and broader evaluation criteria.' Questions=\"1. How exactly were the strategies operationalized from the LLM outputs? What factors influenced the choice of specific games for the experiments?\\n2. Were there any significant differences in behavior between different versions of GPT, and what might account for these differences?\\n3. How might the robustness of LLM strategies be improved to enhance their performance in coordination games like the Battle of the Sexes?\\n4. Could additional social scenarios or games further illuminate the LLMs' behavioral tendencies?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper explores an important and novel area of research by applying behavioral game theory to analyze the social behavior of large language models (LLMs). The results provide new insights into the behaviors of LLMs in various game-theoretic settings, highlighting differences in their strategies and adaptations.' Weaknesses='The methodology for creating and testing game strategies from LLM outputs is not entirely clear and might lack robustness. The games selected, while representative, may not fully capture the range of social scenarios that LLMs might encounter. Furthermore, the results and conclusions could benefit from more detailed analysis and broader evaluation criteria.' Questions=\"1. How exactly were the strategies operationalized from the LLM outputs? What factors influenced the choice of specific games for the experiments?\\n2. Were there any significant differences in behavior between different versions of GPT, and what might account for these differences?\\n3. How might the robustness of LLM strategies be improved to enhance their performance in coordination games like the Battle of the Sexes?\\n4. Could additional social scenarios or games further illuminate the LLMs' behavioral tendencies?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper provides theoretical insights into the impact of variable scale on structure learning algorithms using square-based and log-likelihood-based losses. Theoretical findings are backed up by extensive empirical experiments on both synthetic and real-world data, enhancing the reliability and applicability of the conclusions drawn.' Weaknesses='The paper may not introduce a fundamentally new approach or algorithm, but rather focuses on the theoretical implications of existing methods under certain conditions. The work primarily critiques and extends prior findings, which may limit its novelty and broader impact.' Questions='How would incorporating these findings into existing structure learning algorithms improve their accuracy or robustness in practice? Are there potential methods or preprocessing steps that could mitigate the scale effects highlighted in the paper?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper provides theoretical insights into the impact of variable scale on structure learning algorithms using square-based and log-likelihood-based losses. Theoretical findings are backed up by extensive empirical experiments on both synthetic and real-world data, enhancing the reliability and applicability of the conclusions drawn.' Weaknesses='The paper may not introduce a fundamentally new approach or algorithm, but rather focuses on the theoretical implications of existing methods under certain conditions. The work primarily critiques and extends prior findings, which may limit its novelty and broader impact.' Questions='How would incorporating these findings into existing structure learning algorithms improve their accuracy or robustness in practice? Are there potential methods or preprocessing steps that could mitigate the scale effects highlighted in the paper?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='1. The algorithm effectively handles complex manifold data and autonomously determines the number of clusters. 2. Demonstrates competitive performance on real datasets with known ground-truth labels, particularly in terms of the normalized mutual information (NMI) score. 3. It boasts impressive computational efficiency, making it suitable for large-scale datasets.' Weaknesses='1. The innovation primarily lies in the combination of existing methods, such as K-Means and a community detection algorithm, which might limit the novelty. 2. It is unclear how the proposed method performs on datasets without clear ground-truth labels. 3. The paper does not discuss potential limitations or drawbacks of the algorithm.' Questions='1. How does Village-Net Clustering perform when applied to datasets without known ground-truth labels? 2. What are the potential limitations or constraints of using this algorithm in practice?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='1. The algorithm effectively handles complex manifold data and autonomously determines the number of clusters. 2. Demonstrates competitive performance on real datasets with known ground-truth labels, particularly in terms of the normalized mutual information (NMI) score. 3. It boasts impressive computational efficiency, making it suitable for large-scale datasets.' Weaknesses='1. The innovation primarily lies in the combination of existing methods, such as K-Means and a community detection algorithm, which might limit the novelty. 2. It is unclear how the proposed method performs on datasets without clear ground-truth labels. 3. The paper does not discuss potential limitations or drawbacks of the algorithm.' Questions='1. How does Village-Net Clustering perform when applied to datasets without known ground-truth labels? 2. What are the potential limitations or constraints of using this algorithm in practice?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a simple yet effective framework that significantly enhances the reliability of Large Language Models (LLMs) for generalizability and factuality. It provides a comprehensive suite of resources, including datasets and model outputs, and demonstrates how LLMs can benefit from discriminative models, which is an innovative approach. The proposed plug-in method is empirically validated and shows improved performance on a variety of tasks.' Weaknesses='While the paper presents a novel approach, it could expand further on the theoretical groundwork of how the integration of task-specific models benefits LLMs. Additionally, there might be scalability concerns or limitations regarding the computational efficiency of the proposed method when applied to very large-scale LLMs.' Questions='How does the proposed approach affect the scalability and computational resources required for very large scale LLMs? Are there any limitations to the types of tasks or models that can benefit from this framework?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a simple yet effective framework that significantly enhances the reliability of Large Language Models (LLMs) for generalizability and factuality. It provides a comprehensive suite of resources, including datasets and model outputs, and demonstrates how LLMs can benefit from discriminative models, which is an innovative approach. The proposed plug-in method is empirically validated and shows improved performance on a variety of tasks.' Weaknesses='While the paper presents a novel approach, it could expand further on the theoretical groundwork of how the integration of task-specific models benefits LLMs. Additionally, there might be scalability concerns or limitations regarding the computational efficiency of the proposed method when applied to very large-scale LLMs.' Questions='How does the proposed approach affect the scalability and computational resources required for very large scale LLMs? Are there any limitations to the types of tasks or models that can benefit from this framework?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel approach called Unknown Domain Inconsistency Minimization (UDIM) which combines parameter and data perturbations for effective domain generalization, potentially leading to improved performance over existing Sharpness-Aware Minimization (SAM) techniques. Theoretical validation is provided alongside empirical evidence across multiple benchmarks, demonstrating its effectiveness in unseen domain scenarios.' Weaknesses=\"The approach may rely heavily on the assumption that perturbed data accurately represents unseen domains, which might not be universally applicable. The computational cost associated with the additional perturbations and loss landscape alignments could also be significant, potentially impacting the method's applicability in resource-constrained environments.\" Questions='How does the method perform in terms of computational efficiency compared to existing SAM-based techniques? Are there specific cases where UDIM might not provide significant benefits over traditional methods?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel approach called Unknown Domain Inconsistency Minimization (UDIM) which combines parameter and data perturbations for effective domain generalization, potentially leading to improved performance over existing Sharpness-Aware Minimization (SAM) techniques. Theoretical validation is provided alongside empirical evidence across multiple benchmarks, demonstrating its effectiveness in unseen domain scenarios.' Weaknesses=\"The approach may rely heavily on the assumption that perturbed data accurately represents unseen domains, which might not be universally applicable. The computational cost associated with the additional perturbations and loss landscape alignments could also be significant, potentially impacting the method's applicability in resource-constrained environments.\" Questions='How does the method perform in terms of computational efficiency compared to existing SAM-based techniques? Are there specific cases where UDIM might not provide significant benefits over traditional methods?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel approach CCGAN, which combines collaborative and competitive mechanisms in GANs to enhance data generation quality and stability. The theoretical grounding provides a strong basis for its claims, particularly regarding equilibrium and training convergence. The experimental results across multiple datasets demonstrate the practical applicability and improvements over baseline models.' Weaknesses=\"While the concept is well-established, the paper could benefit from a clearer explanation of the impact of CCGAN's collaborative component, especially compared to existing multi-agent GAN frameworks. The presentation would be improved with better visualizations and examples to concretely demonstrate the qualitative improvements claimed. Additionally, more detailed experimental insights and comparisons with diverse GAN frameworks would strengthen the empirical evidence.\" Questions='How does CCGAN perform in terms of computational overhead compared to conventional GANs? Is there a trade-off? What are the specific challenges in tuning hyperparameters for CCGAN, and how might they differ from standard GANs? How does the proposed method handle diverse dataset characteristics, such as those with more complex or chaotic structures?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel approach CCGAN, which combines collaborative and competitive mechanisms in GANs to enhance data generation quality and stability. The theoretical grounding provides a strong basis for its claims, particularly regarding equilibrium and training convergence. The experimental results across multiple datasets demonstrate the practical applicability and improvements over baseline models.' Weaknesses=\"While the concept is well-established, the paper could benefit from a clearer explanation of the impact of CCGAN's collaborative component, especially compared to existing multi-agent GAN frameworks. The presentation would be improved with better visualizations and examples to concretely demonstrate the qualitative improvements claimed. Additionally, more detailed experimental insights and comparisons with diverse GAN frameworks would strengthen the empirical evidence.\" Questions='How does CCGAN perform in terms of computational overhead compared to conventional GANs? Is there a trade-off? What are the specific challenges in tuning hyperparameters for CCGAN, and how might they differ from standard GANs? How does the proposed method handle diverse dataset characteristics, such as those with more complex or chaotic structures?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a novel approach that leverages both iterative and parallel computation for reasoning tasks, effectively combining benefits from both computation types. It demonstrates strong reasoning and generalization capabilities, achieving state-of-the-art performance on challenging datasets such as CLEVR-Humans. The approach also aids interpretability by allowing for visualization of computation steps.' Weaknesses='The explanation of key components of the proposed system, particularly the parallel reasoning mechanism, lacks clarity, making it difficult to grasp how parallel processing is achieved. Some design decisions appear non-intuitive and require further elaboration. The visualizations, although insightful, could be expanded upon and better explained to highlight the unique contributions of the iterative and parallel mechanisms.' Questions='How exactly does the parallel reasoning mechanism work within the proposed framework? Can you provide more detailed visualizations or examples to clarify the impact of the iterative and parallel processing components on performance and interpretability?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a novel approach that leverages both iterative and parallel computation for reasoning tasks, effectively combining benefits from both computation types. It demonstrates strong reasoning and generalization capabilities, achieving state-of-the-art performance on challenging datasets such as CLEVR-Humans. The approach also aids interpretability by allowing for visualization of computation steps.' Weaknesses='The explanation of key components of the proposed system, particularly the parallel reasoning mechanism, lacks clarity, making it difficult to grasp how parallel processing is achieved. Some design decisions appear non-intuitive and require further elaboration. The visualizations, although insightful, could be expanded upon and better explained to highlight the unique contributions of the iterative and parallel mechanisms.' Questions='How exactly does the parallel reasoning mechanism work within the proposed framework? Can you provide more detailed visualizations or examples to clarify the impact of the iterative and parallel processing components on performance and interpretability?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper tackles a significant problem in epidemic prediction by leveraging self-supervised learning (SSL) to utilize data from past epidemics. It shows promising results with PEM outperforming previous state-of-the-art methods across various datasets, including Covid-19, highlighting efficiency and adaptability.' Weaknesses='The methodology, while sound, lacks novelty as it primarily applies existing SSL concepts to a new domain. More detailed experimental comparisons with baseline methods would strengthen the claims of superior performance. The clarity of the approach could be enhanced by providing more details on the SSL task design and baselines.' Questions=\"How does the proposed model handle significantly varied dynamics in the datasets used for pre-training? What specific aspects of epidemic dynamics are captured that contribute to the PEM's improved performance?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper tackles a significant problem in epidemic prediction by leveraging self-supervised learning (SSL) to utilize data from past epidemics. It shows promising results with PEM outperforming previous state-of-the-art methods across various datasets, including Covid-19, highlighting efficiency and adaptability.' Weaknesses='The methodology, while sound, lacks novelty as it primarily applies existing SSL concepts to a new domain. More detailed experimental comparisons with baseline methods would strengthen the claims of superior performance. The clarity of the approach could be enhanced by providing more details on the SSL task design and baselines.' Questions=\"How does the proposed model handle significantly varied dynamics in the datasets used for pre-training? What specific aspects of epidemic dynamics are captured that contribute to the PEM's improved performance?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses a relevant issue in the field of distributionally robust optimization by introducing a simple yet effective scaling strategy that can be directly applied to debiasing algorithms. The empirical results demonstrate that the approach can improve both robust and average accuracies without additional training. The introduction of a unified metric to summarize the trade-off between accuracies is a valuable contribution for evaluating existing methods.' Weaknesses='While the proposed scaling techniques are practical and easy to implement, their simplicity may limit the novelty of the contribution. The paper would benefit from a deeper theoretical analysis of why these methods work and how they can be generalized. The presentation could be improved with clearer explanations of technical details and more comprehensive experimental results. Comparison with more recent state-of-the-art methods would enhance the evaluation section.' Questions='1. How does the performance of the proposed scaling techniques vary across different datasets and tasks? \\n2. Can the instance-wise adaptive scaling technique be adjusted or extended to handle specific types of biases in different domains, such as computer vision or NLP?\\n3. Are there any limitations or potential pitfalls in using the unified metric for evaluating robust optimization methods?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses a relevant issue in the field of distributionally robust optimization by introducing a simple yet effective scaling strategy that can be directly applied to debiasing algorithms. The empirical results demonstrate that the approach can improve both robust and average accuracies without additional training. The introduction of a unified metric to summarize the trade-off between accuracies is a valuable contribution for evaluating existing methods.' Weaknesses='While the proposed scaling techniques are practical and easy to implement, their simplicity may limit the novelty of the contribution. The paper would benefit from a deeper theoretical analysis of why these methods work and how they can be generalized. The presentation could be improved with clearer explanations of technical details and more comprehensive experimental results. Comparison with more recent state-of-the-art methods would enhance the evaluation section.' Questions='1. How does the performance of the proposed scaling techniques vary across different datasets and tasks? \\n2. Can the instance-wise adaptive scaling technique be adjusted or extended to handle specific types of biases in different domains, such as computer vision or NLP?\\n3. Are there any limitations or potential pitfalls in using the unified metric for evaluating robust optimization methods?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a novel approach to learning disentangled representations for time series data, specifically targeting electricity usage, which is a relevant problem for energy optimization and reducing carbon footprint. The introduction of the Time Disentangling Score (TDS) as a metric for evaluating disentanglement quality is a valuable addition to the field.' Weaknesses=\"The abstract does not provide specific details on how the method is implemented or evaluated, making it difficult to assess the complete methodology's efficacy. The connection between the proposed method and its practical applications could be elaborated upon, particularly in terms of how it significantly advances current approaches.\" Questions='How does the proposed disentanglement approach specifically improve over existing methods that assume attribute independence in real-world applications? Could you elaborate on the practical implications and potential limitations of using the TDS metric?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a novel approach to learning disentangled representations for time series data, specifically targeting electricity usage, which is a relevant problem for energy optimization and reducing carbon footprint. The introduction of the Time Disentangling Score (TDS) as a metric for evaluating disentanglement quality is a valuable addition to the field.' Weaknesses=\"The abstract does not provide specific details on how the method is implemented or evaluated, making it difficult to assess the complete methodology's efficacy. The connection between the proposed method and its practical applications could be elaborated upon, particularly in terms of how it significantly advances current approaches.\" Questions='How does the proposed disentanglement approach specifically improve over existing methods that assume attribute independence in real-world applications? Could you elaborate on the practical implications and potential limitations of using the TDS metric?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='1. The Qwen-VL series sets new performance records on visual-centric benchmarks with generalist models of similar scales. 2. The integration of a visual receptor and a multilingual multimodal cleaned corpus demonstrates an innovative approach to enhancing model capabilities. 3. Public availability of the models encourages research and further exploration in the field.' Weaknesses='1. The abstract lacks clarity on specific novel contributions, focusing instead on results and performance. 2. There might be limited information on how the approach compares to baseline models in terms of methodology, rather than just performance metrics.' Questions='1. How does the performance of Qwen-VL compare to baseline models in terms of architectural efficiency and training resources? 2. What specific challenges were encountered when aligning the image-caption-box tuples, and how were they addressed?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='1. The Qwen-VL series sets new performance records on visual-centric benchmarks with generalist models of similar scales. 2. The integration of a visual receptor and a multilingual multimodal cleaned corpus demonstrates an innovative approach to enhancing model capabilities. 3. Public availability of the models encourages research and further exploration in the field.' Weaknesses='1. The abstract lacks clarity on specific novel contributions, focusing instead on results and performance. 2. There might be limited information on how the approach compares to baseline models in terms of methodology, rather than just performance metrics.' Questions='1. How does the performance of Qwen-VL compare to baseline models in terms of architectural efficiency and training resources? 2. What specific challenges were encountered when aligning the image-caption-box tuples, and how were they addressed?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a novel concept of behavior distillation for reinforcement learning, which does not require access to expert data, addressing a significant gap in the domain. HaDES method shows remarkable results in distilling information into just four state-action pairs while achieving competitive performance, demonstrating robustness and versatility. The experimental results are compelling, indicating potential advancements in training multi-task agents and neural architecture search.' Weaknesses='The practical applicability of the proposed method may be limited since behavior distillation without expert data might not always be feasible depending on the complexity of tasks. While the theoretical contributions are significant, further exploration and validation on a broader range of domains would strengthen the claims. The paper could expand on the discussion about potential challenges and limitations of this method in specific RL contexts.' Questions='What are the specific limitations of HaDES in more complex and high-dimensional tasks, and how does it handle diversity in state-action spaces? Can the method be extended to environments where the state space is less clearly defined or continually evolving?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a novel concept of behavior distillation for reinforcement learning, which does not require access to expert data, addressing a significant gap in the domain. HaDES method shows remarkable results in distilling information into just four state-action pairs while achieving competitive performance, demonstrating robustness and versatility. The experimental results are compelling, indicating potential advancements in training multi-task agents and neural architecture search.' Weaknesses='The practical applicability of the proposed method may be limited since behavior distillation without expert data might not always be feasible depending on the complexity of tasks. While the theoretical contributions are significant, further exploration and validation on a broader range of domains would strengthen the claims. The paper could expand on the discussion about potential challenges and limitations of this method in specific RL contexts.' Questions='What are the specific limitations of HaDES in more complex and high-dimensional tasks, and how does it handle diversity in state-action spaces? Can the method be extended to environments where the state space is less clearly defined or continually evolving?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel approach, the Extended Textual Conditioning space (P+), which enhances control over text-to-image diffusion models. The proposed Extended Textual Inversion (XTI) offers improved expressive and precise modeling, with faster convergence than existing methods. The method shows better reconstruction and editability without the need for balancing these goals, which is a significant advantage.' Weaknesses='The effectiveness and advantages of the proposed XTI method need more theoretical grounding; the improvements are primarily demonstrated through experimental results. The distinction and representation of the extended space and the per-layer tokens might require further clarification. In-depth analysis of how the unique properties of P+ lead to improved object-style mixing would strengthen the work.' Questions='How does the proposed P+ space enhance the synthesis process, and what are the specific benefits over existing textual conditioning methods? Can the authors provide more insights into the theoretical basis for the observed improvements in expressiveness and convergence?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel approach, the Extended Textual Conditioning space (P+), which enhances control over text-to-image diffusion models. The proposed Extended Textual Inversion (XTI) offers improved expressive and precise modeling, with faster convergence than existing methods. The method shows better reconstruction and editability without the need for balancing these goals, which is a significant advantage.' Weaknesses='The effectiveness and advantages of the proposed XTI method need more theoretical grounding; the improvements are primarily demonstrated through experimental results. The distinction and representation of the extended space and the per-layer tokens might require further clarification. In-depth analysis of how the unique properties of P+ lead to improved object-style mixing would strengthen the work.' Questions='How does the proposed P+ space enhance the synthesis process, and what are the specific benefits over existing textual conditioning methods? Can the authors provide more insights into the theoretical basis for the observed improvements in expressiveness and convergence?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel approach to SFUDA by introducing multiple prediction hypotheses and consolidating these to identify the most likely correct pseudo-labels. This method shows state-of-the-art performance and can be easily integrated with existing models to enhance their effectiveness.' Weaknesses='The necessity and practical applicability of the multiple hypotheses approach may require further clarification, given that creating multiple high-quality hypotheses from the same domain seems challenging and wasteful. Additionally, some performance metrics do not reach state-of-the-art levels in all cases, indicating room for improvement in specific tasks.' Questions='How are the optimal values for $\\\\tilde{k}$, $\\\\tau_1$, and $\\\\tau_2$ determined, and how might these values vary across different datasets or tasks?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel approach to SFUDA by introducing multiple prediction hypotheses and consolidating these to identify the most likely correct pseudo-labels. This method shows state-of-the-art performance and can be easily integrated with existing models to enhance their effectiveness.' Weaknesses='The necessity and practical applicability of the multiple hypotheses approach may require further clarification, given that creating multiple high-quality hypotheses from the same domain seems challenging and wasteful. Additionally, some performance metrics do not reach state-of-the-art levels in all cases, indicating room for improvement in specific tasks.' Questions='How are the optimal values for $\\\\tilde{k}$, $\\\\tau_1$, and $\\\\tau_2$ determined, and how might these values vary across different datasets or tasks?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper proposes a novel technique for interpreting structured output models, which is a relevant and complex problem that has not been extensively addressed in prior research. The introduction of an energy-based training process adds a new perspective to understanding structured outputs, allowing the method to capture correlations and provide more nuanced explanations.' Weaknesses='The presentation of the methodology could be improved, as certain components are not thoroughly explained, possibly affecting reproducibility. Additionally, while the paper mentions various datasets, specific details about the evaluation process and comparison with existing methods seem to be lacking.' Questions='How does the proposed method compare quantitatively with other state-of-the-art interpretation techniques on similar tasks? Are there specific types of structured output models where this technique might not be applicable or effective?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper proposes a novel technique for interpreting structured output models, which is a relevant and complex problem that has not been extensively addressed in prior research. The introduction of an energy-based training process adds a new perspective to understanding structured outputs, allowing the method to capture correlations and provide more nuanced explanations.' Weaknesses='The presentation of the methodology could be improved, as certain components are not thoroughly explained, possibly affecting reproducibility. Additionally, while the paper mentions various datasets, specific details about the evaluation process and comparison with existing methods seem to be lacking.' Questions='How does the proposed method compare quantitatively with other state-of-the-art interpretation techniques on similar tasks? Are there specific types of structured output models where this technique might not be applicable or effective?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper challenges current conventions in GNN design by suggesting that simpler non-deep learning methods may achieve similar or better results, which is a valuable perspective.' Weaknesses='The work might lack novelty as it largely revisits and applies existing nonparametric estimation techniques rather than introducing new methods. The experimental validation to support claims about evaluation convention shifts could be more comprehensive.' Questions='How do the simpler spectral methods perform on new, more complex SSNC tasks compared to recently developed GNN architectures?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper challenges current conventions in GNN design by suggesting that simpler non-deep learning methods may achieve similar or better results, which is a valuable perspective.' Weaknesses='The work might lack novelty as it largely revisits and applies existing nonparametric estimation techniques rather than introducing new methods. The experimental validation to support claims about evaluation convention shifts could be more comprehensive.' Questions='How do the simpler spectral methods perform on new, more complex SSNC tasks compared to recently developed GNN architectures?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel approach by disentangling the optimization paths for adversarial and natural objectives, potentially addressing a known issue in Robust Fine-Tuning (RFT). The proposed AutoLoRa method is of significant practical importance as it simplifies the tuning process by automatically converting pre-trained models for adversarially robust downstream tasks without extensive hyperparameter tuning. Extensive empirical evaluations show promising improvements.' Weaknesses='The explanation for the automated scheduling of hyperparameters lacks depth, and the empirical results in Table 5 do not clearly illustrate its advantages. Some details, like the constant factor in formulae, remain unexplained and lack validation. There is a lack of diverse architectural evaluation beyond ResNet, which limits generalizability claims.' Questions='How does the AutoLoRa method ensure stability across different backbone architectures, beyond ResNet? Could the authors provide more insight into the choice and impact of the constant factor in the learning rate scheduling?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel approach by disentangling the optimization paths for adversarial and natural objectives, potentially addressing a known issue in Robust Fine-Tuning (RFT). The proposed AutoLoRa method is of significant practical importance as it simplifies the tuning process by automatically converting pre-trained models for adversarially robust downstream tasks without extensive hyperparameter tuning. Extensive empirical evaluations show promising improvements.' Weaknesses='The explanation for the automated scheduling of hyperparameters lacks depth, and the empirical results in Table 5 do not clearly illustrate its advantages. Some details, like the constant factor in formulae, remain unexplained and lack validation. There is a lack of diverse architectural evaluation beyond ResNet, which limits generalizability claims.' Questions='How does the AutoLoRa method ensure stability across different backbone architectures, beyond ResNet? Could the authors provide more insight into the choice and impact of the constant factor in the learning rate scheduling?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper introduces an adaptive, dynamic, and automated framework for identifying Graph Lottery Tickets (AdaGLT), which addresses the key limitations of existing GLT paradigms. \\n2. The proposed method provides theoretical guarantees for mitigating over-smoothing and improving sparse structures in deep GNN scenarios.\\n3. Extensive experiments across various graph datasets showcase the superiority of AdaGLT over state-of-the-art models, especially in deep GNN use cases.' Weaknesses='The main disadvantage highlighted is the scalability issue when applying to deeper and larger-scale GNNs, but AdaGLT seems to partially address this. Nonetheless, it remains to be confirmed how well the method scales compared to simpler alternatives.\\nThe need for extensive computations was not clearly articulated in the experiments, which remains a key consideration for practical applications in resource-constrained environments.' Questions='How does AdaGLT compare to simpler sparse training methods in terms of computational cost and efficiency, especially in very large-scale graphs? Are there specific scenarios or graph types where AdaGLT may not be as effective?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper introduces an adaptive, dynamic, and automated framework for identifying Graph Lottery Tickets (AdaGLT), which addresses the key limitations of existing GLT paradigms. \\n2. The proposed method provides theoretical guarantees for mitigating over-smoothing and improving sparse structures in deep GNN scenarios.\\n3. Extensive experiments across various graph datasets showcase the superiority of AdaGLT over state-of-the-art models, especially in deep GNN use cases.' Weaknesses='The main disadvantage highlighted is the scalability issue when applying to deeper and larger-scale GNNs, but AdaGLT seems to partially address this. Nonetheless, it remains to be confirmed how well the method scales compared to simpler alternatives.\\nThe need for extensive computations was not clearly articulated in the experiments, which remains a key consideration for practical applications in resource-constrained environments.' Questions='How does AdaGLT compare to simpler sparse training methods in terms of computational cost and efficiency, especially in very large-scale graphs? Are there specific scenarios or graph types where AdaGLT may not be as effective?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=4 Strengths='1. The paper introduces a significant improvement in the architecture of the latent diffusion model, leveraging a larger UNet and novel conditioning schemes that show dramatic improvements over previous versions.\\n2. SDXL achieves competitive results with state-of-the-art black-box image generators, indicating its effectiveness.\\n3. The introduction of a refinement model further enhances the visual fidelity of generated images.' Weaknesses='1. The abstract lacks details on the potential computational cost implications of the larger model and additional components, which could be relevant for practical deployments.\\n2. There is limited discussion on the limitations or potential trade-offs involved with the new architecture, such as training time, resource requirements, or scalability.' Questions='1. What are the computational cost implications of using a three times larger UNet and additional conditioning schemes?\\n2. How does SDXL perform on diverse datasets that may contain biases, and are there any measures in place to address them?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=4 Strengths='1. The paper introduces a significant improvement in the architecture of the latent diffusion model, leveraging a larger UNet and novel conditioning schemes that show dramatic improvements over previous versions.\\n2. SDXL achieves competitive results with state-of-the-art black-box image generators, indicating its effectiveness.\\n3. The introduction of a refinement model further enhances the visual fidelity of generated images.' Weaknesses='1. The abstract lacks details on the potential computational cost implications of the larger model and additional components, which could be relevant for practical deployments.\\n2. There is limited discussion on the limitations or potential trade-offs involved with the new architecture, such as training time, resource requirements, or scalability.' Questions='1. What are the computational cost implications of using a three times larger UNet and additional conditioning schemes?\\n2. How does SDXL perform on diverse datasets that may contain biases, and are there any measures in place to address them?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='1. The introduction of activation prompting (AP) offers a novel way to enhance visual prompting by utilizing intermediate activation maps, addressing some limitations of traditional input-level VP.\\n2. The paper provides a comprehensive theoretical and empirical analysis across 29 datasets, showcasing the performance and efficiency benefits of AP over conventional methods.' Weaknesses='1. The novelty of the AP approach may be perceived as incremental, given the extensive research already conducted in parameter-efficient model adaptation techniques.\\n2. Comparisons with pre-existing methods should also consider varying settings like different backbones, as this may affect the fairness of the comparisons.\\n3. Despite extensive experiments, the impact of the proposed AP method on real-world tasks with substantial performance gaps remains somewhat unclear.' Questions='1. How does the performance of AP scale with larger models beyond CNNs and ViTs, or in more diverse real-world applications?\\n2. Can the activation prompting (AP) method be easily integrated with other existing parameter-efficient techniques to leverage their respective strengths?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='1. The introduction of activation prompting (AP) offers a novel way to enhance visual prompting by utilizing intermediate activation maps, addressing some limitations of traditional input-level VP.\\n2. The paper provides a comprehensive theoretical and empirical analysis across 29 datasets, showcasing the performance and efficiency benefits of AP over conventional methods.' Weaknesses='1. The novelty of the AP approach may be perceived as incremental, given the extensive research already conducted in parameter-efficient model adaptation techniques.\\n2. Comparisons with pre-existing methods should also consider varying settings like different backbones, as this may affect the fairness of the comparisons.\\n3. Despite extensive experiments, the impact of the proposed AP method on real-world tasks with substantial performance gaps remains somewhat unclear.' Questions='1. How does the performance of AP scale with larger models beyond CNNs and ViTs, or in more diverse real-world applications?\\n2. Can the activation prompting (AP) method be easily integrated with other existing parameter-efficient techniques to leverage their respective strengths?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper proposes an innovative approach by drawing inspiration from biological neural systems, specifically utilizing content-addressable heteroassociative memory and a spatially invariant convolutional network architecture. The proposed model demonstrates strong empirical performance, outperforming existing ANN baselines in continual and few-shot learning tasks. The new task, sequential Morris Water Maze (sWM), introduces a challenging benchmark for evaluating rapid and continual learning capability.' Weaknesses='The paper may lack sufficient discussion on the scalability of the proposed approach to more complex or varied tasks beyond the specific navigation task used. The reliance on specific biologically inspired architectures might limit the generalizability of the findings to broader machine learning applications. The evaluation is limited to comparisons with existing ANN baselines and may benefit from a broader set of benchmarks.' Questions='How does the proposed model handle tasks that are not inherently spatial in nature? Can the principles outlined be applied effectively to such tasks? \\n\\nWhat are the computational overheads associated with implementing a content-addressable heteroassociative memory compared to traditional neural network architectures?\\n\\nAre there any specific limitations or assumptions inherent in the choice of architectures that should be considered when applying this model to real-world problems?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper proposes an innovative approach by drawing inspiration from biological neural systems, specifically utilizing content-addressable heteroassociative memory and a spatially invariant convolutional network architecture. The proposed model demonstrates strong empirical performance, outperforming existing ANN baselines in continual and few-shot learning tasks. The new task, sequential Morris Water Maze (sWM), introduces a challenging benchmark for evaluating rapid and continual learning capability.' Weaknesses='The paper may lack sufficient discussion on the scalability of the proposed approach to more complex or varied tasks beyond the specific navigation task used. The reliance on specific biologically inspired architectures might limit the generalizability of the findings to broader machine learning applications. The evaluation is limited to comparisons with existing ANN baselines and may benefit from a broader set of benchmarks.' Questions='How does the proposed model handle tasks that are not inherently spatial in nature? Can the principles outlined be applied effectively to such tasks? \\n\\nWhat are the computational overheads associated with implementing a content-addressable heteroassociative memory compared to traditional neural network architectures?\\n\\nAre there any specific limitations or assumptions inherent in the choice of architectures that should be considered when applying this model to real-world problems?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper provides a novel framework, ImOOD, which effectively addresses the issue of class imbalance in OOD detection, offering a strong theoretical foundation. The proposed method consistently outperforms state-of-the-art approaches across several benchmarks, indicating its robustness and effectiveness.' Weaknesses=\"The paper may lack evaluation on diverse types of OOD datasets, which could test the method's applicability in broader scenarios. Some aspects of the proposed framework, like the specifics of the post-hoc normalization and training-time regularization, could benefit from more in-depth explanation.\" Questions='How does ImOOD perform on OOD datasets with varying degrees of class imbalance? Are there any limitations or assumptions of ImOOD that might affect its generalizability to other datasets or domains?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper provides a novel framework, ImOOD, which effectively addresses the issue of class imbalance in OOD detection, offering a strong theoretical foundation. The proposed method consistently outperforms state-of-the-art approaches across several benchmarks, indicating its robustness and effectiveness.' Weaknesses=\"The paper may lack evaluation on diverse types of OOD datasets, which could test the method's applicability in broader scenarios. Some aspects of the proposed framework, like the specifics of the post-hoc normalization and training-time regularization, could benefit from more in-depth explanation.\" Questions='How does ImOOD perform on OOD datasets with varying degrees of class imbalance? Are there any limitations or assumptions of ImOOD that might affect its generalizability to other datasets or domains?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper presents an innovative and effective method for detecting adversarial attacks in ASR systems using probability distribution characteristics.\\n2. The method shows high accuracy in distinguishing adversarial examples, with AUROC scores above 99%.\\n3. The approach is comprehensive, addressing the robustness of detection methods under adaptive attacks.' Weaknesses='1. The paper could expand on the types of adversarial attacks tested and include more diverse attack methodologies.\\n2. Some clarity is needed on the complexity and real-time applicability of the proposed detection method within live ASR systems.' Questions='How does the proposed detection approach perform with real-time ASR systems in practical, dynamic environments compared to static datasets?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper presents an innovative and effective method for detecting adversarial attacks in ASR systems using probability distribution characteristics.\\n2. The method shows high accuracy in distinguishing adversarial examples, with AUROC scores above 99%.\\n3. The approach is comprehensive, addressing the robustness of detection methods under adaptive attacks.' Weaknesses='1. The paper could expand on the types of adversarial attacks tested and include more diverse attack methodologies.\\n2. Some clarity is needed on the complexity and real-time applicability of the proposed detection method within live ASR systems.' Questions='How does the proposed detection approach perform with real-time ASR systems in practical, dynamic environments compared to static datasets?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a flow-based model to transport data between arbitrary distributions P and Q, potentially broadening the utility of such models in generative tasks. Notably, the approach extends beyond conventional normalizing flow methods by allowing flows to map between non-normal distributions and enables several downstream tasks, demonstrating versatility in applications like mutual information estimation and image translation.' Weaknesses='The theoretical claims, like ensuring optimal transport, might be unsubstantiated without further empirical validation, particularly with real-world benchmarks. The paper might suffer from insufficient comparison to existing methods in practical scenarios and could benefit from strengthening the empirical evaluation to clearly demonstrate the proposed method’s advantages.' Questions='The abstract implies improved performance metrics on tasks like density estimation and images translation. How do these performances quantitatively compare to state-of-the-art techniques in these areas? Additionally, is the choice of flow models based on certain theoretical advantages over alternative approaches, or is it empirical?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a flow-based model to transport data between arbitrary distributions P and Q, potentially broadening the utility of such models in generative tasks. Notably, the approach extends beyond conventional normalizing flow methods by allowing flows to map between non-normal distributions and enables several downstream tasks, demonstrating versatility in applications like mutual information estimation and image translation.' Weaknesses='The theoretical claims, like ensuring optimal transport, might be unsubstantiated without further empirical validation, particularly with real-world benchmarks. The paper might suffer from insufficient comparison to existing methods in practical scenarios and could benefit from strengthening the empirical evaluation to clearly demonstrate the proposed method’s advantages.' Questions='The abstract implies improved performance metrics on tasks like density estimation and images translation. How do these performances quantitatively compare to state-of-the-art techniques in these areas? Additionally, is the choice of flow models based on certain theoretical advantages over alternative approaches, or is it empirical?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces an innovative improvement over the recurrent switching linear dynamical system (rSLDS) model by addressing its limitations, such as fixed state number and latent structure. The proposed model incorporates a semi-Markov discrete state process with latent geometry, which is a significant advancement. The use of partial differential equations (PDE) theory to derive a semi-parametric formulation for dynamical sufficient statistics is noteworthy, showing a strong theoretical foundation.' Weaknesses='The abstract lacks detail on the empirical performance improvements of the irSLDS model compared to existing models, making it difficult to assess the practical impact. There is also limited discussion on the computational requirements and scalability of the model, which could impact its applicability. Furthermore, the clarity and organization of the presentation could be enhanced to improve understanding.' Questions='How does the performance of the proposed irSLDS model compare quantitatively with existing models in terms of accuracy and computational efficiency? What are the specific limitations in terms of computational resources and time when scaling the model to larger datasets or more complex neural activity patterns?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces an innovative improvement over the recurrent switching linear dynamical system (rSLDS) model by addressing its limitations, such as fixed state number and latent structure. The proposed model incorporates a semi-Markov discrete state process with latent geometry, which is a significant advancement. The use of partial differential equations (PDE) theory to derive a semi-parametric formulation for dynamical sufficient statistics is noteworthy, showing a strong theoretical foundation.' Weaknesses='The abstract lacks detail on the empirical performance improvements of the irSLDS model compared to existing models, making it difficult to assess the practical impact. There is also limited discussion on the computational requirements and scalability of the model, which could impact its applicability. Furthermore, the clarity and organization of the presentation could be enhanced to improve understanding.' Questions='How does the performance of the proposed irSLDS model compare quantitatively with existing models in terms of accuracy and computational efficiency? What are the specific limitations in terms of computational resources and time when scaling the model to larger datasets or more complex neural activity patterns?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='1. The paper introduces a novel 3D scene generative model, DynaVol, which unifies geometric structures for object-centric learning with a differentiable volume rendering framework.\\n2. The method uniquely integrates object-centric voxelization and canonical-space deformation for dynamic scene decomposition.\\n3. Demonstrates significant performance improvements over existing unsupervised methods for scene decomposition.' Weaknesses=\"1. The description of the methodology could be clearer, especially concerning the different stages of the process, such as warmup vs. dynamics stages.\\n2. The definition and role of certain components, such as 'L_point', are not sufficiently explained.\\n3. The segmentation results appear somewhat messy in the results presented, with a limited number of real video experiments conducted.\" Questions='1. Can you clarify the distinct roles of the warmup stage versus the dynamics stage in your method?\\n2. How is the number of objects N determined, and could connected components be used to automatically choose N?\\n3. Would it be feasible to utilize an invertible mapping instead of the cycle-consistent deformation networks for forward and backward dynamics?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='1. The paper introduces a novel 3D scene generative model, DynaVol, which unifies geometric structures for object-centric learning with a differentiable volume rendering framework.\\n2. The method uniquely integrates object-centric voxelization and canonical-space deformation for dynamic scene decomposition.\\n3. Demonstrates significant performance improvements over existing unsupervised methods for scene decomposition.' Weaknesses=\"1. The description of the methodology could be clearer, especially concerning the different stages of the process, such as warmup vs. dynamics stages.\\n2. The definition and role of certain components, such as 'L_point', are not sufficiently explained.\\n3. The segmentation results appear somewhat messy in the results presented, with a limited number of real video experiments conducted.\" Questions='1. Can you clarify the distinct roles of the warmup stage versus the dynamics stage in your method?\\n2. How is the number of objects N determined, and could connected components be used to automatically choose N?\\n3. Would it be feasible to utilize an invertible mapping instead of the cycle-consistent deformation networks for forward and backward dynamics?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a simple and effective approach for open-vocabulary semantic segmentation utilizing pseudo-mask and language training. It achieves state-of-the-art results on standard benchmarks without the need for fine-tuning, and it shows scalability with data. Additionally, the promise to release code and demos will aid in further research.' Weaknesses='The novelty of the method is somewhat limited, as it closely resembles existing approaches like ZegFormer, and there are significant omissions in baseline comparisons, such as OVSegmentor and TCL. Furthermore, critical ablations on design choices are missing, which reduces the insights into why the proposed method works. Presentation could be improved by providing more details on the evaluation protocol, pseudo-mask generation, and self-training procedure.' Questions='How does the proposed method compare directly with recent state-of-the-art methods in open-vocabulary segmentation not included in the comparisons? What specific textual prompts or templates are used for class and background labels? What is the effect of the choice of K in K-Means for pseudo-mask generation? How critical is setting K = 8, and how does it affect performance?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a simple and effective approach for open-vocabulary semantic segmentation utilizing pseudo-mask and language training. It achieves state-of-the-art results on standard benchmarks without the need for fine-tuning, and it shows scalability with data. Additionally, the promise to release code and demos will aid in further research.' Weaknesses='The novelty of the method is somewhat limited, as it closely resembles existing approaches like ZegFormer, and there are significant omissions in baseline comparisons, such as OVSegmentor and TCL. Furthermore, critical ablations on design choices are missing, which reduces the insights into why the proposed method works. Presentation could be improved by providing more details on the evaluation protocol, pseudo-mask generation, and self-training procedure.' Questions='How does the proposed method compare directly with recent state-of-the-art methods in open-vocabulary segmentation not included in the comparisons? What specific textual prompts or templates are used for class and background labels? What is the effect of the choice of K in K-Means for pseudo-mask generation? How critical is setting K = 8, and how does it affect performance?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces ShareFormer, a novel Transformer architecture that aims to improve latency and trainability in high-resolution image restoration tasks. The use of shared attention maps to improve inference speed and the incorporation of residual connections to facilitate optimization are strong contributions. The results are supported by numerous experimental validations, and the plan to open-source the code and models is commendable.' Weaknesses='The introduction of shared attention maps, while innovative, may not be entirely novel as similar concepts have been explored in other works. The paper lacks some detailed ablation studies to thoroughly isolate and prove the impact of individual components like CSAU. The text could benefit from clearer explanations and more detailed analysis of certain design choices.' Questions=\"Is the concept of sharing attention maps entirely novel to this paper, or have similar ideas been proposed in prior works? Could you provide more evidence or visualization to support the claim of amplified locality bias due to the residual connections on 'Value'? How do you ensure fairness in comparison with other models given potential differences in training methodologies, such as patch size during progressive learning?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces ShareFormer, a novel Transformer architecture that aims to improve latency and trainability in high-resolution image restoration tasks. The use of shared attention maps to improve inference speed and the incorporation of residual connections to facilitate optimization are strong contributions. The results are supported by numerous experimental validations, and the plan to open-source the code and models is commendable.' Weaknesses='The introduction of shared attention maps, while innovative, may not be entirely novel as similar concepts have been explored in other works. The paper lacks some detailed ablation studies to thoroughly isolate and prove the impact of individual components like CSAU. The text could benefit from clearer explanations and more detailed analysis of certain design choices.' Questions=\"Is the concept of sharing attention maps entirely novel to this paper, or have similar ideas been proposed in prior works? Could you provide more evidence or visualization to support the claim of amplified locality bias due to the residual connections on 'Value'? How do you ensure fairness in comparison with other models given potential differences in training methodologies, such as patch size during progressive learning?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. Novel approach in leveraging existing pre-trained DGMs without additional training efforts, addressing a key limitation in image super-resolution.\\n2. Introduction of the Perceptual Recoverable Field (PRF) as a theoretical insight for balancing noise injection, offering clear innovation and practical utility.\\n3. Extensive experiments showcasing superior performance and adaptability across diverse environments, indicating robustness of the method.' Weaknesses='1. While the concept of using pre-trained models is advantageous, the dependency on noise injection requires careful calibration which might limit ease of use or repeatability without specific guidelines.\\n2. Although extensive experiments are claimed, the lack of detailed quantitative and visual results for various arbitrary-scale super-resolution tasks can be a limitation for evaluating effectiveness comprehensively.' Questions='1. How does the choice of noise level adapt across different scales, and are there guidelines for selecting these parameters pragmatically?\\n2. Can the proposed Diff-SR method seamlessly integrate with all existing DGMs or are there specific prerequisites for DGMs to be compatible with this method?\\n3. The paper mentions a trade-off between low-level fidelity and high-level signature. Could more insights or examples be provided to illustrate this trade-off within the application scope?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. Novel approach in leveraging existing pre-trained DGMs without additional training efforts, addressing a key limitation in image super-resolution.\\n2. Introduction of the Perceptual Recoverable Field (PRF) as a theoretical insight for balancing noise injection, offering clear innovation and practical utility.\\n3. Extensive experiments showcasing superior performance and adaptability across diverse environments, indicating robustness of the method.' Weaknesses='1. While the concept of using pre-trained models is advantageous, the dependency on noise injection requires careful calibration which might limit ease of use or repeatability without specific guidelines.\\n2. Although extensive experiments are claimed, the lack of detailed quantitative and visual results for various arbitrary-scale super-resolution tasks can be a limitation for evaluating effectiveness comprehensively.' Questions='1. How does the choice of noise level adapt across different scales, and are there guidelines for selecting these parameters pragmatically?\\n2. Can the proposed Diff-SR method seamlessly integrate with all existing DGMs or are there specific prerequisites for DGMs to be compatible with this method?\\n3. The paper mentions a trade-off between low-level fidelity and high-level signature. Could more insights or examples be provided to illustrate this trade-off within the application scope?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses a noteworthy issue in the field of RLHF, highlighting the correlation between output length and perceived helpfulness, offering important insights into model behavior.\\n2. The authors provide a thorough analysis of reward models across multiple datasets, contributing to the ongoing discussion about the optimization of RLHF models. \\n3. By experimentally testing interventions that can decouple length from reward, this study pushes forward our understanding and potential methodologies, although partially, in refining RLHF without length bias.' Weaknesses='1. While the study successfully identifies the issue of length bias, the proposed interventions to mitigate it do not uniformly solve the problem across all tested scenarios, indicating a need for further research.\\n2. The analysis, while comprehensive, may benefit from more intuitive presentation models and clarity in how figures and data are described, to enhance readability and comprehension.\\n3. The findings, although impactful, provide limited actionable strategies for developers wanting to address this bias in practical applications, necessitating further research into concrete solution paths.' Questions=\"1. Do the findings from your analysis suggest specific systemic changes that developers could implement immediately to counteract the length bias issue in RLHF?\\n2. How generalizable are the results concerning length bias to other modalities or are they primarily confined to RLHF in language models? Would visual or multimodal RLHF tasks exhibit similar patterns?\\n3. Reflecting on your findings that lengthier responses might contribute to rewards, what implications do you anticipate for RLHF research focusing on other qualitative attributes like relevance or engagement?\\n4. Are there alternative metrics or approaches proposed in your study to gauge 'helpfulness' without conflating it with response length, and how effective might they be? \"\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses a noteworthy issue in the field of RLHF, highlighting the correlation between output length and perceived helpfulness, offering important insights into model behavior.\\n2. The authors provide a thorough analysis of reward models across multiple datasets, contributing to the ongoing discussion about the optimization of RLHF models. \\n3. By experimentally testing interventions that can decouple length from reward, this study pushes forward our understanding and potential methodologies, although partially, in refining RLHF without length bias.' Weaknesses='1. While the study successfully identifies the issue of length bias, the proposed interventions to mitigate it do not uniformly solve the problem across all tested scenarios, indicating a need for further research.\\n2. The analysis, while comprehensive, may benefit from more intuitive presentation models and clarity in how figures and data are described, to enhance readability and comprehension.\\n3. The findings, although impactful, provide limited actionable strategies for developers wanting to address this bias in practical applications, necessitating further research into concrete solution paths.' Questions=\"1. Do the findings from your analysis suggest specific systemic changes that developers could implement immediately to counteract the length bias issue in RLHF?\\n2. How generalizable are the results concerning length bias to other modalities or are they primarily confined to RLHF in language models? Would visual or multimodal RLHF tasks exhibit similar patterns?\\n3. Reflecting on your findings that lengthier responses might contribute to rewards, what implications do you anticipate for RLHF research focusing on other qualitative attributes like relevance or engagement?\\n4. Are there alternative metrics or approaches proposed in your study to gauge 'helpfulness' without conflating it with response length, and how effective might they be? \"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses an important issue with GNNs related to representation distortion as graphs evolve over time, providing a theoretical basis for the problem and introducing a novel method for mitigating it through self-supervised graph reconstruction.' Weaknesses='The theoretical assumptions and conditions under which the bound is derived might be too strong or unrealistic in practical scenarios. The method comparison could be expanded to include more diverse baselines to strengthen empirical claims.' Questions='How sensitive is the Smart method to different types of graph evolutions beyond the ones covered in the experiments?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses an important issue with GNNs related to representation distortion as graphs evolve over time, providing a theoretical basis for the problem and introducing a novel method for mitigating it through self-supervised graph reconstruction.' Weaknesses='The theoretical assumptions and conditions under which the bound is derived might be too strong or unrealistic in practical scenarios. The method comparison could be expanded to include more diverse baselines to strengthen empirical claims.' Questions='How sensitive is the Smart method to different types of graph evolutions beyond the ones covered in the experiments?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper introduces a novel use of diffusion models for estimating individualized treatment effects by modeling unobserved confounders.\\n2. The authors demonstrate consistent improvements over state-of-the-art methods using standard evaluation metrics on both synthetic and benchmark datasets.' Weaknesses='1. The assumptions and setup for the diffusion model related to unobserved confounders may need further elaboration and justification.\\n2. The explanations given in the abstract about the process and significance might be challenging for readers to fully grasp without prior knowledge of diffusion models.' Questions='1. Can the authors provide more intuition or visual explanation on how the reverse diffusion process effectively captures unobserved confounders?\\n2. How sensitive is the proposed model to the choice of priors or the nature of the apriori knowledge used?\\n3. Are there specific scenarios where the proposed methodology is expected to have limitations?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper introduces a novel use of diffusion models for estimating individualized treatment effects by modeling unobserved confounders.\\n2. The authors demonstrate consistent improvements over state-of-the-art methods using standard evaluation metrics on both synthetic and benchmark datasets.' Weaknesses='1. The assumptions and setup for the diffusion model related to unobserved confounders may need further elaboration and justification.\\n2. The explanations given in the abstract about the process and significance might be challenging for readers to fully grasp without prior knowledge of diffusion models.' Questions='1. Can the authors provide more intuition or visual explanation on how the reverse diffusion process effectively captures unobserved confounders?\\n2. How sensitive is the proposed model to the choice of priors or the nature of the apriori knowledge used?\\n3. Are there specific scenarios where the proposed methodology is expected to have limitations?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=2 Rating=5 Confidence=3 Strengths='The paper proposes a novel benchmark for multi-modal continual learning and demonstrates that using multiple modalities can help mitigate catastrophic forgetting in neural networks. The approach of integrating and aligning information from different modalities using relational structural similarities is innovative. The paper also sets a strong baseline for future research in this area.' Weaknesses='The evaluation is primarily conducted on a single dataset (VGGSound), and there is a lack of comparison with more recent and varied baselines. Additionally, while the paper provides a benchmark, the contribution in terms of proposing a new learning technique or method is somewhat limited. There is also a need for more empirical evidence across different datasets to substantiate the claims about the effectiveness of multi-modal learning in continual learning scenarios.' Questions='Can you provide a comparison of your method with more recent continual learning approaches? How does your benchmark compare with existing ones in terms of usability and coverage of different modalities? What are the specific challenges in extending your approach to other modalities, such as language?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=2 Rating=5 Confidence=3 Strengths='The paper proposes a novel benchmark for multi-modal continual learning and demonstrates that using multiple modalities can help mitigate catastrophic forgetting in neural networks. The approach of integrating and aligning information from different modalities using relational structural similarities is innovative. The paper also sets a strong baseline for future research in this area.' Weaknesses='The evaluation is primarily conducted on a single dataset (VGGSound), and there is a lack of comparison with more recent and varied baselines. Additionally, while the paper provides a benchmark, the contribution in terms of proposing a new learning technique or method is somewhat limited. There is also a need for more empirical evidence across different datasets to substantiate the claims about the effectiveness of multi-modal learning in continual learning scenarios.' Questions='Can you provide a comparison of your method with more recent continual learning approaches? How does your benchmark compare with existing ones in terms of usability and coverage of different modalities? What are the specific challenges in extending your approach to other modalities, such as language?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a novel generative prompting framework for zero-shot text classification that addresses issues of miscalibration and brittleness in LMs. The approach integrates contextual information about labels, which is a significant contribution. The empirical results are impressive, with consistent outperforming of zero-shot and few-shot baselines across multiple benchmarks.' Weaknesses='One potential weakness is the reliance on the quality of natural language label descriptions, which may vary in practice. The method might also need more evaluation on diverse linguistic or domain-specific tasks to establish broader applicability. Moreover, the complexity of setting up detailed label descriptions might hinder its usability in some applications.' Questions='Can the approach be extended to other NLP tasks beyond text classification? How sensitive is the performance to the quality or variability of label descriptions used in the generative framework?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a novel generative prompting framework for zero-shot text classification that addresses issues of miscalibration and brittleness in LMs. The approach integrates contextual information about labels, which is a significant contribution. The empirical results are impressive, with consistent outperforming of zero-shot and few-shot baselines across multiple benchmarks.' Weaknesses='One potential weakness is the reliance on the quality of natural language label descriptions, which may vary in practice. The method might also need more evaluation on diverse linguistic or domain-specific tasks to establish broader applicability. Moreover, the complexity of setting up detailed label descriptions might hinder its usability in some applications.' Questions='Can the approach be extended to other NLP tasks beyond text classification? How sensitive is the performance to the quality or variability of label descriptions used in the generative framework?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper presents the first large-scale, publicly available dataset focused on last-mile delivery, which is a significant contribution to logistics and spatio-temporal data research. The dataset's comprehensiveness and diversity make it a valuable resource for multiple research communities. The clear documentation of its characteristics and open accessibility are strong points.\" Weaknesses=\"While the dataset is a valuable contribution, the paper lacks detail on addressing data privacy concerns robustly. Furthermore, while the paper benchmarks baseline models, it would benefit from a more detailed analysis of the dataset's unique insights or potential challenges in applying existing models.\" Questions='How does the dataset specifically handle privacy concerns regarding courier and client data? Are there plans to regularly update this dataset to reflect changes in logistics operations or urban environments?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper presents the first large-scale, publicly available dataset focused on last-mile delivery, which is a significant contribution to logistics and spatio-temporal data research. The dataset's comprehensiveness and diversity make it a valuable resource for multiple research communities. The clear documentation of its characteristics and open accessibility are strong points.\" Weaknesses=\"While the dataset is a valuable contribution, the paper lacks detail on addressing data privacy concerns robustly. Furthermore, while the paper benchmarks baseline models, it would benefit from a more detailed analysis of the dataset's unique insights or potential challenges in applying existing models.\" Questions='How does the dataset specifically handle privacy concerns regarding courier and client data? Are there plans to regularly update this dataset to reflect changes in logistics operations or urban environments?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The proposed dataset-curation approach allows large-scale multi-view dataset generation without the need for additional annotations, which is a significant step forward in representation learning. Experiments show that representations from the MIMIC datasets outperform those learned from expensive, annotated datasets, indicating effectiveness and scalability.' Weaknesses='The presentation could be improved for clarity, and additional contexts or motivations could be provided for why certain choices were made, such as the specific tasks used for evaluation. More details on the limitations of the proposed method would also strengthen the paper.' Questions='How does the proposed method generalize to other tasks beyond those specifically discussed in the paper? What are the potential biases introduced by the automatically generated datasets?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The proposed dataset-curation approach allows large-scale multi-view dataset generation without the need for additional annotations, which is a significant step forward in representation learning. Experiments show that representations from the MIMIC datasets outperform those learned from expensive, annotated datasets, indicating effectiveness and scalability.' Weaknesses='The presentation could be improved for clarity, and additional contexts or motivations could be provided for why certain choices were made, such as the specific tasks used for evaluation. More details on the limitations of the proposed method would also strengthen the paper.' Questions='How does the proposed method generalize to other tasks beyond those specifically discussed in the paper? What are the potential biases introduced by the automatically generated datasets?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces TrASPr, an innovative predictive model using transformer architecture for tissue-specific RNA splicing prediction, which stands out in its performance compared to recent models. The combined use of predictive modeling and generative approaches positions the work as a novel contribution to the RNA splicing field.' Weaknesses='The reliance on Bayesian Optimization with a custom loss function for RNA outcomes could lead to concerns about the generality and reproducibility of the results. Furthermore, more comprehensive comparisons and benchmarks with established tools in the field would strengthen the claims of superiority.' Questions=\"How does TrASPr's interpretability in identifying regulatory features compare with traditional models? Could the method for designing RNA splicing outcome through generative modeling be applied to other biological sequences with similar success?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces TrASPr, an innovative predictive model using transformer architecture for tissue-specific RNA splicing prediction, which stands out in its performance compared to recent models. The combined use of predictive modeling and generative approaches positions the work as a novel contribution to the RNA splicing field.' Weaknesses='The reliance on Bayesian Optimization with a custom loss function for RNA outcomes could lead to concerns about the generality and reproducibility of the results. Furthermore, more comprehensive comparisons and benchmarks with established tools in the field would strengthen the claims of superiority.' Questions=\"How does TrASPr's interpretability in identifying regulatory features compare with traditional models? Could the method for designing RNA splicing outcome through generative modeling be applied to other biological sequences with similar success?\"\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Content:\n",
      "I'm sorry, but I cannot assist with this request.\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a unified voxelization framework, dubbed UniVoxel, for inverse rendering which accelerates the process significantly. The integration of spherical Gaussians for modeling illumination provides an elegant solution that meshes well with other scene attributes. Experimental results demonstrate significant speed-ups in training time while maintaining good reconstruction quality.' Weaknesses='The paper could benefit from a clearer explanation of how UniVoxel directly compares to existing unified voxelization frameworks outside of time efficiency. There could be deeper consideration of how different scene complexities affect the framework and its limitations in terms of handling varying lighting and material conditions.' Questions='Does the use of spherical Gaussians impose any specific limitations in terms of the range or complexity of scenes that UniVoxel can handle effectively? How does the performance vary across different types of scenes or under varying lighting conditions?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a unified voxelization framework, dubbed UniVoxel, for inverse rendering which accelerates the process significantly. The integration of spherical Gaussians for modeling illumination provides an elegant solution that meshes well with other scene attributes. Experimental results demonstrate significant speed-ups in training time while maintaining good reconstruction quality.' Weaknesses='The paper could benefit from a clearer explanation of how UniVoxel directly compares to existing unified voxelization frameworks outside of time efficiency. There could be deeper consideration of how different scene complexities affect the framework and its limitations in terms of handling varying lighting and material conditions.' Questions='Does the use of spherical Gaussians impose any specific limitations in terms of the range or complexity of scenes that UniVoxel can handle effectively? How does the performance vary across different types of scenes or under varying lighting conditions?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper introduces an innovative approach to mitigate dataset bias in diffusion models using time-dependent importance reweighting. This addresses a crucial aspect of fairness in generative models.\\n2. Strong theoretical backing is provided, linking the new method to traditional score-matching and establishing convergence to an unbiased distribution.\\n3. The experimental results show significant performance improvements across various datasets and bias settings, demonstrating the practical applicability of the method.' Weaknesses='1. The concept of time-dependent density reweighting can be complex and may be challenging for practitioners to implement without extensive understanding of the theoretical aspects.\\n2. The experiments, while thorough, primarily focus on standard datasets, and the applicability to more diverse or complex distributions may need further exploration.' Questions='1. How does the proposed method affect the computational efficiency of the diffusion model training process?\\n2. Can this method be generalized to other types of generative models beyond diffusion models?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper introduces an innovative approach to mitigate dataset bias in diffusion models using time-dependent importance reweighting. This addresses a crucial aspect of fairness in generative models.\\n2. Strong theoretical backing is provided, linking the new method to traditional score-matching and establishing convergence to an unbiased distribution.\\n3. The experimental results show significant performance improvements across various datasets and bias settings, demonstrating the practical applicability of the method.' Weaknesses='1. The concept of time-dependent density reweighting can be complex and may be challenging for practitioners to implement without extensive understanding of the theoretical aspects.\\n2. The experiments, while thorough, primarily focus on standard datasets, and the applicability to more diverse or complex distributions may need further exploration.' Questions='1. How does the proposed method affect the computational efficiency of the diffusion model training process?\\n2. Can this method be generalized to other types of generative models beyond diffusion models?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper introduces a novel predictive statistical model called the Online Sparse Residual Tree (OSRT) that dynamically adapts to streaming data.\\n2. OSRT incorporates both online tree decomposition and adaptive radial basis function exploration, which enhances its capability to handle large, evolving datasets efficiently.\\n3. The method maintains the latest trends in data by updating its final layer based on the most recent data.\\n4. Empirical results suggest that OSRT outperforms existing online radial basis function methods in terms of efficiency and accuracy.' Weaknesses=\"1. The paper does not clearly outline the limitations and potential drawbacks of the proposed OSRT model compared to other methods.\\n2. There is a lack of theoretical analysis supporting the model's performance and convergence guarantees, which is critical for understanding its applicability in different contexts.\\n3. While the paper claims efficiency and accuracy improvements over existing methods, it would benefit from providing more detailed comparisons and quantitative results.\\n4. The explanations and presentation could be clearer, particularly in describing the integration and operational nuances of the model components.\" Questions=\"1. How does the OSRT model's computational complexity scale with respect to the size and dimensionality of the input data streams?\\n2. Are there specific scenarios or dataset characteristics where OSRT might not perform well compared to existing online RBF methods?\\n3. What are the parameters and hyperparameters of the model, and how sensitive is the model to these settings?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper introduces a novel predictive statistical model called the Online Sparse Residual Tree (OSRT) that dynamically adapts to streaming data.\\n2. OSRT incorporates both online tree decomposition and adaptive radial basis function exploration, which enhances its capability to handle large, evolving datasets efficiently.\\n3. The method maintains the latest trends in data by updating its final layer based on the most recent data.\\n4. Empirical results suggest that OSRT outperforms existing online radial basis function methods in terms of efficiency and accuracy.' Weaknesses=\"1. The paper does not clearly outline the limitations and potential drawbacks of the proposed OSRT model compared to other methods.\\n2. There is a lack of theoretical analysis supporting the model's performance and convergence guarantees, which is critical for understanding its applicability in different contexts.\\n3. While the paper claims efficiency and accuracy improvements over existing methods, it would benefit from providing more detailed comparisons and quantitative results.\\n4. The explanations and presentation could be clearer, particularly in describing the integration and operational nuances of the model components.\" Questions=\"1. How does the OSRT model's computational complexity scale with respect to the size and dimensionality of the input data streams?\\n2. Are there specific scenarios or dataset characteristics where OSRT might not perform well compared to existing online RBF methods?\\n3. What are the parameters and hyperparameters of the model, and how sensitive is the model to these settings?\"\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Content:\n",
      "Could you please clarify the document or task you're referring to? I'm here to help with any questions or needs you have.\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces an innovative approach to make neural networks more interpretable by introducing codebook features, offering a novel way to discretize hidden states. It showcases that networks can maintain performance even with this quantization, opening avenues for network behavior control. The proposed method successfully overcomes the superposition problem in a finite state machine dataset and demonstrates its potential in guiding language model generation.' Weaknesses=\"The method's effectiveness largely hinges on manual intervention to identify relevant codes, which may limit its scalability or broad applicability. There is a lack of detailed analysis on the potential trade-offs between model expressiveness and interpretability introduced through quantization. Additionally, considerations on practical integration and deployment in existing neural architectures are minimally addressed.\" Questions='How scalable is the proposed method for very large-scale neural networks, and what are its implications on computational efficiency? Are there specific limitations in terms of depth or size of the network that might affect the quantization approach? Could there be an automated way to associate codes with specific semantic concepts, reducing the need for manual intervention?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces an innovative approach to make neural networks more interpretable by introducing codebook features, offering a novel way to discretize hidden states. It showcases that networks can maintain performance even with this quantization, opening avenues for network behavior control. The proposed method successfully overcomes the superposition problem in a finite state machine dataset and demonstrates its potential in guiding language model generation.' Weaknesses=\"The method's effectiveness largely hinges on manual intervention to identify relevant codes, which may limit its scalability or broad applicability. There is a lack of detailed analysis on the potential trade-offs between model expressiveness and interpretability introduced through quantization. Additionally, considerations on practical integration and deployment in existing neural architectures are minimally addressed.\" Questions='How scalable is the proposed method for very large-scale neural networks, and what are its implications on computational efficiency? Are there specific limitations in terms of depth or size of the network that might affect the quantization approach? Could there be an automated way to associate codes with specific semantic concepts, reducing the need for manual intervention?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper addresses a novel and challenging problem of partially annotated multi-label classification, which is relevant in many practical scenarios where obtaining full annotations is difficult.\\n2. The proposed PAPG framework effectively combines reinforcement learning and supervised learning, showcasing improvement over existing methods in handling class imbalance and limited annotations.\\n3. The introduction of local and global rewards is an interesting approach to deal with class imbalance.' Weaknesses=\"1. The motivation and differentiation from existing PU learning methods should be clearer, as the problem might appear similar to PU learning initially.\\n2. Details on how the proposed rewards and iterative training strategy specifically address the scarcity of positive annotations and label imbalance are not thoroughly explained, leaving some ambiguity about the approach's effectiveness.\\n3. The paper could benefit from a more thorough discussion on how the method handles label correlations, a critical aspect of multi-label classification that is not deeply explored.\" Questions='1. How does PAPG ensure better handling of label correlations compared to other multi-label classification methods?\\n2. Can the proposed method be directly compared to existing PU learning methods for effectiveness analysis?\\n3. Are there limitations in the proposed PAPG framework when applied to datasets with a higher degree of label sparsity?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper addresses a novel and challenging problem of partially annotated multi-label classification, which is relevant in many practical scenarios where obtaining full annotations is difficult.\\n2. The proposed PAPG framework effectively combines reinforcement learning and supervised learning, showcasing improvement over existing methods in handling class imbalance and limited annotations.\\n3. The introduction of local and global rewards is an interesting approach to deal with class imbalance.' Weaknesses=\"1. The motivation and differentiation from existing PU learning methods should be clearer, as the problem might appear similar to PU learning initially.\\n2. Details on how the proposed rewards and iterative training strategy specifically address the scarcity of positive annotations and label imbalance are not thoroughly explained, leaving some ambiguity about the approach's effectiveness.\\n3. The paper could benefit from a more thorough discussion on how the method handles label correlations, a critical aspect of multi-label classification that is not deeply explored.\" Questions='1. How does PAPG ensure better handling of label correlations compared to other multi-label classification methods?\\n2. Can the proposed method be directly compared to existing PU learning methods for effectiveness analysis?\\n3. Are there limitations in the proposed PAPG framework when applied to datasets with a higher degree of label sparsity?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents a novel pretraining approach that significantly improves pocket representation by leveraging high-resolution protein structures and pretrained small molecule representations. By simulating ligand-receptor interactions through the creation of over 5 million complexes, it effectively addresses the limited availability of real pocket-ligand complex data. The method, ProFSA, achieves state-of-the-art performance across multiple tasks such as druggability prediction and ligand binding affinity prediction.' Weaknesses='The paper may lack a detailed description of the potential limitations or assumptions in their pocket-ligand interaction modeling approach. Additionally, while the approach shows strong empirical results, more theoretical analysis on why the method achieves such performance is desirable. Furthermore, the scalability and practical applicability of generating such a large synthetic dataset need careful evaluation.' Questions='How does the method ensure that the synthetic complexes generated accurately simulate real ligand-receptor interactions? Are there specific benchmarks or evaluations that confirm the quality of the synthetic pairs compared to real data? What are the computational costs associated with generating and processing 5 million complexes?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents a novel pretraining approach that significantly improves pocket representation by leveraging high-resolution protein structures and pretrained small molecule representations. By simulating ligand-receptor interactions through the creation of over 5 million complexes, it effectively addresses the limited availability of real pocket-ligand complex data. The method, ProFSA, achieves state-of-the-art performance across multiple tasks such as druggability prediction and ligand binding affinity prediction.' Weaknesses='The paper may lack a detailed description of the potential limitations or assumptions in their pocket-ligand interaction modeling approach. Additionally, while the approach shows strong empirical results, more theoretical analysis on why the method achieves such performance is desirable. Furthermore, the scalability and practical applicability of generating such a large synthetic dataset need careful evaluation.' Questions='How does the method ensure that the synthetic complexes generated accurately simulate real ligand-receptor interactions? Are there specific benchmarks or evaluations that confirm the quality of the synthetic pairs compared to real data? What are the computational costs associated with generating and processing 5 million complexes?'\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Content:\n",
      "I'm sorry, I cannot produce the requested output because there is inappropriate data in the provided content.\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The proposed Generalizable Neural Point Field (GPF) introduces a novel paradigm shift by employing point-based rather than image-based rendering, which addresses significant issues related to occlusions and geometric discontinuities.\\n2. The paper presents a compelling improvement in rendering speed and reconstruction quality through its nonuniform log sampling strategy and learnable kernel for feature aggregation.\\n3. The ability to achieve superior geometries, view consistencies, and rendering quality in both generalization and fine-tuning settings suggests strong empirical performance over existing methods.' Weaknesses=\"1. The reliance on geometric priors and neural features could limit the generalizability if such priors are inaccurate or difficult to obtain, potentially affecting the robustness of the approach in varied contexts.\\n2. The evaluation is limited to three datasets, which may not fully demonstrate the approach's adaptability or limitations across diverse scenarios or more complex environments.\\n3. While there is a mention of preliminary proving of the new paradigm's potential, the experimental results need to be more thorough to solidify this claim.\" Questions=\"1. How does the dependence on geometric priors for visibility modeling affect the method's performance in dynamically changing scenes where priors may not be accurate?\\n2. Can the authors provide insights into potential computational overhead introduced by the learnable kernel and the nonuniform log sampling strategy compared to traditional NeRF approaches?\\n3. How scalable is the Generalizable NeRF approach with respect to large-scale datasets or real-time applications, given the shift to point-based rendering?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The proposed Generalizable Neural Point Field (GPF) introduces a novel paradigm shift by employing point-based rather than image-based rendering, which addresses significant issues related to occlusions and geometric discontinuities.\\n2. The paper presents a compelling improvement in rendering speed and reconstruction quality through its nonuniform log sampling strategy and learnable kernel for feature aggregation.\\n3. The ability to achieve superior geometries, view consistencies, and rendering quality in both generalization and fine-tuning settings suggests strong empirical performance over existing methods.' Weaknesses=\"1. The reliance on geometric priors and neural features could limit the generalizability if such priors are inaccurate or difficult to obtain, potentially affecting the robustness of the approach in varied contexts.\\n2. The evaluation is limited to three datasets, which may not fully demonstrate the approach's adaptability or limitations across diverse scenarios or more complex environments.\\n3. While there is a mention of preliminary proving of the new paradigm's potential, the experimental results need to be more thorough to solidify this claim.\" Questions=\"1. How does the dependence on geometric priors for visibility modeling affect the method's performance in dynamically changing scenes where priors may not be accurate?\\n2. Can the authors provide insights into potential computational overhead introduced by the learnable kernel and the nonuniform log sampling strategy compared to traditional NeRF approaches?\\n3. How scalable is the Generalizable NeRF approach with respect to large-scale datasets or real-time applications, given the shift to point-based rendering?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The proposed LRM is a novel approach that draws from neural collapse to manage scenarios with insufficient labels. The theoretical analysis of the relationship between LRM and ERM is a strong point. Empirical demonstrations show LRM's effectiveness across various scenarios such as semi-supervised learning and domain adaptation.\" Weaknesses='The paper lacks sufficient experimental evidence to demonstrate generalizability across diverse datasets. Limited clarity in presentation may hinder understanding, particularly on how the label encodings are estimated and aligned. The applicability of LRM as a plugin could be more clearly illustrated.' Questions='What are the computational overheads when using LRM compared to traditional methods? How does LRM handle scenarios with completely unlabeled target datasets aside from domain adaptation? Can LRM be effectively integrated into existing deep learning frameworks?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The proposed LRM is a novel approach that draws from neural collapse to manage scenarios with insufficient labels. The theoretical analysis of the relationship between LRM and ERM is a strong point. Empirical demonstrations show LRM's effectiveness across various scenarios such as semi-supervised learning and domain adaptation.\" Weaknesses='The paper lacks sufficient experimental evidence to demonstrate generalizability across diverse datasets. Limited clarity in presentation may hinder understanding, particularly on how the label encodings are estimated and aligned. The applicability of LRM as a plugin could be more clearly illustrated.' Questions='What are the computational overheads when using LRM compared to traditional methods? How does LRM handle scenarios with completely unlabeled target datasets aside from domain adaptation? Can LRM be effectively integrated into existing deep learning frameworks?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper addresses a critical aspect of neural network training: the stability of the Jacobian matrix throughout training, rather than only at initialization. The theoretical framework is impressive, utilizing advanced results from random matrix theory to extend stability results. The study contributes to the understanding of why initialization schemes are crucial, which is a valuable addition to the existing literature. Moreover, the paper demonstrates the applicability of the framework to various neural network scenarios, including sparse networks and non-iid initialization.' Weaknesses='The theoretical nature of the paper might limit its accessibility to those without a strong background in random matrix theory. While the paper provides a solid theoretical framework, practical implications and experiments to demonstrate the real-world effectiveness of these theoretical findings are limited. The assumptions made for stability during training might be too restrictive, as it requires conditions that are not thoroughly justified theoretically.' Questions='How can the results be generalized or applied to real-world neural network architectures beyond theoretical implications? Do the assumptions about stability during training hold in practical scenarios, and how can they be empirically validated?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper addresses a critical aspect of neural network training: the stability of the Jacobian matrix throughout training, rather than only at initialization. The theoretical framework is impressive, utilizing advanced results from random matrix theory to extend stability results. The study contributes to the understanding of why initialization schemes are crucial, which is a valuable addition to the existing literature. Moreover, the paper demonstrates the applicability of the framework to various neural network scenarios, including sparse networks and non-iid initialization.' Weaknesses='The theoretical nature of the paper might limit its accessibility to those without a strong background in random matrix theory. While the paper provides a solid theoretical framework, practical implications and experiments to demonstrate the real-world effectiveness of these theoretical findings are limited. The assumptions made for stability during training might be too restrictive, as it requires conditions that are not thoroughly justified theoretically.' Questions='How can the results be generalized or applied to real-world neural network architectures beyond theoretical implications? Do the assumptions about stability during training hold in practical scenarios, and how can they be empirically validated?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper introduces an innovative approach to simultaneously address both closed-set recognition (CSR) and open-set recognition (OSR) using a Latent Gaussian Mixture Model (L-GMM).\\n2. The framework is well-designed with a collaborative training algorithm that uniquely combines elements of generative and discriminative models.\\n3. Experimental results demonstrate improved performance over traditional discriminative models in both CSR and OSR tasks.' Weaknesses='1. The presentation of the formalization of open-set recognition based on learning theory could be clearer, as some parts appear dense and could be better explained.\\n2. The explanation of the collaborative training algorithm could benefit from more detail or examples to enhance reader understanding.\\n3. Limited discussion on the generalizability of the approach to other types of data beyond image recognition and segmentation.' Questions='1. How sensitive is the L-GMM model to the choice of hyperparameters, and what strategies do you recommend for tuning them effectively?\\n2. Can this approach be effectively adapted to other domains beyond image recognition, such as text or audio data, and what modifications might be needed?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper introduces an innovative approach to simultaneously address both closed-set recognition (CSR) and open-set recognition (OSR) using a Latent Gaussian Mixture Model (L-GMM).\\n2. The framework is well-designed with a collaborative training algorithm that uniquely combines elements of generative and discriminative models.\\n3. Experimental results demonstrate improved performance over traditional discriminative models in both CSR and OSR tasks.' Weaknesses='1. The presentation of the formalization of open-set recognition based on learning theory could be clearer, as some parts appear dense and could be better explained.\\n2. The explanation of the collaborative training algorithm could benefit from more detail or examples to enhance reader understanding.\\n3. Limited discussion on the generalizability of the approach to other types of data beyond image recognition and segmentation.' Questions='1. How sensitive is the L-GMM model to the choice of hyperparameters, and what strategies do you recommend for tuning them effectively?\\n2. Can this approach be effectively adapted to other domains beyond image recognition, such as text or audio data, and what modifications might be needed?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper provides a theoretical foundation for Multi-domain text classification (MDTC), which is a significant leap over existing empirical work.\\n2. The proposed margin discrepancy-based adversarial training (MDAT) approach is well-aligned with the theoretical insights provided and shows promising results compared to state-of-the-art methods.\\n3. Conducting empirical studies on two MDTC benchmarks supplemented the theoretical findings and validated the efficacy of the proposed method.' Weaknesses='1. While the paper claims novel theoretical contributions, the analysis seems heavily derived from existing work by Zhang et al., 2019, potentially limiting its originality.\\n2. The presentation of theoretical results lacks clarity in emphasizing new insights versus adaptations of existing results to MDTC.\\n3. A deeper discussion on the limitations and potential pitfalls of the proposed approach would strengthen the paper.' Questions='1. How does the proposed MDAT method perform in scenarios where the domain divergence is high?\\n2. Can the proposed margin discrepancy-based adversarial training approach be adapted to other multi-domain problems beyond text classification?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper provides a theoretical foundation for Multi-domain text classification (MDTC), which is a significant leap over existing empirical work.\\n2. The proposed margin discrepancy-based adversarial training (MDAT) approach is well-aligned with the theoretical insights provided and shows promising results compared to state-of-the-art methods.\\n3. Conducting empirical studies on two MDTC benchmarks supplemented the theoretical findings and validated the efficacy of the proposed method.' Weaknesses='1. While the paper claims novel theoretical contributions, the analysis seems heavily derived from existing work by Zhang et al., 2019, potentially limiting its originality.\\n2. The presentation of theoretical results lacks clarity in emphasizing new insights versus adaptations of existing results to MDTC.\\n3. A deeper discussion on the limitations and potential pitfalls of the proposed approach would strengthen the paper.' Questions='1. How does the proposed MDAT method perform in scenarios where the domain divergence is high?\\n2. Can the proposed margin discrepancy-based adversarial training approach be adapted to other multi-domain problems beyond text classification?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel sample selection methodology for SSL, addressing budget constraints through a modified Frank-Wolfe algorithm and the innovative use of the alpha-Maximum Mean Discrepancy criterion. The proposed method, UPA, is shown to consistently improve SSL method performance, offering a significant contribution to improving low-budget learning scenarios.' Weaknesses='The potential complexity of the proposed methodology may pose implementation challenges, and there is a need for further clarification on the choice and impact of the alpha-MMD criterion. The empirical evaluation, while promising, could benefit from broader testing across more varied data sets to fully establish generalization claims.' Questions='How does the alpha-MMD criterion specifically enhance generalization, and are there specific types of data or models where UPA faces limitations? Can UPA be easily integrated with other existing SSL techniques?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel sample selection methodology for SSL, addressing budget constraints through a modified Frank-Wolfe algorithm and the innovative use of the alpha-Maximum Mean Discrepancy criterion. The proposed method, UPA, is shown to consistently improve SSL method performance, offering a significant contribution to improving low-budget learning scenarios.' Weaknesses='The potential complexity of the proposed methodology may pose implementation challenges, and there is a need for further clarification on the choice and impact of the alpha-MMD criterion. The empirical evaluation, while promising, could benefit from broader testing across more varied data sets to fully establish generalization claims.' Questions='How does the alpha-MMD criterion specifically enhance generalization, and are there specific types of data or models where UPA faces limitations? Can UPA be easily integrated with other existing SSL techniques?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper proposes a novel approach using Hadamard transforms to facilitate low-precision training via integer-only operations, which potentially reduces computational demands significantly for continual learning on edge devices. The empirical results demonstrating minimal accuracy degradation with significant bit-width reduction are promising and suggest practical applications in privacy-sensitive and low-latency environments.' Weaknesses=\"The method's generalizability to a wide range of datasets and tasks is not thoroughly established; more diverse datasets and neural architectures should be considered to strengthen the claims. The practical implementation of stochastic rounding and tiled matrix multiplication on typical hardware platforms may introduce complexities not addressed in the paper. The connection between the proposed technique and the alleviation of privacy concerns remains somewhat indirect.\" Questions='How well does the proposed method integrate into existing machine learning workflows on edge devices, particularly in terms of hardware compatibility and real-world deployment? Could the authors clarify how stochastic rounding is implemented and its computational implications? Are there specific settings or types of models where the proposed method yields more pronounced benefits?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper proposes a novel approach using Hadamard transforms to facilitate low-precision training via integer-only operations, which potentially reduces computational demands significantly for continual learning on edge devices. The empirical results demonstrating minimal accuracy degradation with significant bit-width reduction are promising and suggest practical applications in privacy-sensitive and low-latency environments.' Weaknesses=\"The method's generalizability to a wide range of datasets and tasks is not thoroughly established; more diverse datasets and neural architectures should be considered to strengthen the claims. The practical implementation of stochastic rounding and tiled matrix multiplication on typical hardware platforms may introduce complexities not addressed in the paper. The connection between the proposed technique and the alleviation of privacy concerns remains somewhat indirect.\" Questions='How well does the proposed method integrate into existing machine learning workflows on edge devices, particularly in terms of hardware compatibility and real-world deployment? Could the authors clarify how stochastic rounding is implemented and its computational implications? Are there specific settings or types of models where the proposed method yields more pronounced benefits?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents novel algorithms for estimating and testing collision probability with improved sample complexity, satisfying local differential privacy, and offering nearly optimal sample complexity. These are significant improvements and advancements in the field.' Weaknesses='The presentation could be clearer, particularly for readers unfamiliar with the field. More background information and explanations of technical terms would enhance comprehension.' Questions='How do the proposed algorithms compare in real-world scenarios beyond experimental settings? Are there any specific applications where these improvements could have a significant impact?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents novel algorithms for estimating and testing collision probability with improved sample complexity, satisfying local differential privacy, and offering nearly optimal sample complexity. These are significant improvements and advancements in the field.' Weaknesses='The presentation could be clearer, particularly for readers unfamiliar with the field. More background information and explanations of technical terms would enhance comprehension.' Questions='How do the proposed algorithms compare in real-world scenarios beyond experimental settings? Are there any specific applications where these improvements could have a significant impact?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper introduces GTMGC, a novel approach using Graph-Transformer for predicting the 3D configurations of molecules from 2D structures, which is a significant step forward in molecular modeling.\\n2. The proposed Molecule Structural Residual Self-Attention mechanism is a noteworthy contribution, potentially useful in other molecular modeling tasks.\\n3. Comprehensive evaluation on Molecule3D and QM9 datasets underscores the effectiveness of the method.' Weaknesses='1. While the method shows improved performance, details on computational efficiency compared to established methods like RDkit are sparse.\\n2. The novelty of the approach may be limited as it relies on integrating existing technologies rather than introducing fundamentally new concepts.\\n3. There could be more clarity around the limitations and assumptions underlying the proposed method, such as any biases present in the training data.' Questions=\"1. How does the proposed GTMGC model compare in terms of computational efficiency with traditional DFT approaches, particularly on larger molecules?\\n2. Can the Molecule Structural Residual Self-Attention mechanism be adapted for other types of molecular modeling tasks beyond what's tested in this paper?\\n3. Were there any specific failure modes identified during testing on the Molecule3D and QM9 datasets?\"\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper introduces GTMGC, a novel approach using Graph-Transformer for predicting the 3D configurations of molecules from 2D structures, which is a significant step forward in molecular modeling.\\n2. The proposed Molecule Structural Residual Self-Attention mechanism is a noteworthy contribution, potentially useful in other molecular modeling tasks.\\n3. Comprehensive evaluation on Molecule3D and QM9 datasets underscores the effectiveness of the method.' Weaknesses='1. While the method shows improved performance, details on computational efficiency compared to established methods like RDkit are sparse.\\n2. The novelty of the approach may be limited as it relies on integrating existing technologies rather than introducing fundamentally new concepts.\\n3. There could be more clarity around the limitations and assumptions underlying the proposed method, such as any biases present in the training data.' Questions=\"1. How does the proposed GTMGC model compare in terms of computational efficiency with traditional DFT approaches, particularly on larger molecules?\\n2. Can the Molecule Structural Residual Self-Attention mechanism be adapted for other types of molecular modeling tasks beyond what's tested in this paper?\\n3. Were there any specific failure modes identified during testing on the Molecule3D and QM9 datasets?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. WavJourney introduces a novel framework combining large language models and audio models to generate storytelling audio content, expanding the possibilities of audio generation beyond domain-specific conditions.\\n2. The framework demonstrates state-of-the-art results in text-to-audio generation benchmarks, indicating its effectiveness and innovation.\\n3. The introduction of a new multi-genre story benchmark enhances the scope for future research and evaluation.' Weaknesses=\"1. The methodology primarily leverages existing models and frameworks, potentially limiting the originality of the technical contribution.\\n2. Further details on the evaluation process and comparison with alternative approaches or frameworks using LLMs would strengthen the paper's claims.\" Questions=\"1. How does the system's performance vary with different configurations of large language models, and what is the impact of using open-source LLM alternatives?\\n2. Can the approach be extended to accommodate additional or alternative audio modalities, and what challenges might arise in such extensions?\\n3. What measures are in place to objectively evaluate user engagement and storytelling effectiveness beyond subjective evaluations?\"\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. WavJourney introduces a novel framework combining large language models and audio models to generate storytelling audio content, expanding the possibilities of audio generation beyond domain-specific conditions.\\n2. The framework demonstrates state-of-the-art results in text-to-audio generation benchmarks, indicating its effectiveness and innovation.\\n3. The introduction of a new multi-genre story benchmark enhances the scope for future research and evaluation.' Weaknesses=\"1. The methodology primarily leverages existing models and frameworks, potentially limiting the originality of the technical contribution.\\n2. Further details on the evaluation process and comparison with alternative approaches or frameworks using LLMs would strengthen the paper's claims.\" Questions=\"1. How does the system's performance vary with different configurations of large language models, and what is the impact of using open-source LLM alternatives?\\n2. Can the approach be extended to accommodate additional or alternative audio modalities, and what challenges might arise in such extensions?\\n3. What measures are in place to objectively evaluate user engagement and storytelling effectiveness beyond subjective evaluations?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses an important limitation in current availability data poisoning strategies by focusing on attack transferability across different learning paradigms, which is less explored. It provides a novel algorithm that leverages supervised and unsupervised learning paradigms to enhance transferability, showing promising results across various learning algorithms and extensive experiments on benchmark datasets.' Weaknesses='The assumption that frequency-level characteristics are indicative of successful transferability is intriguing but lacks a deeper theoretical explanation. The paper could also benefit from a stronger justification for the choice of specific algorithms in generating poisoning attacks. Additionally, while extensive experiments are conducted, the scalability of the method to larger datasets or more complex learning paradigms needs further exploration.' Questions='1. How does the proposed method perform on larger scale datasets or more complex models? 2. Could the approach be adapted to other modalities beyond image datasets? 3. What are the computational costs associated with using both supervised and unsupervised paradigms in generating poisoning perturbations?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses an important limitation in current availability data poisoning strategies by focusing on attack transferability across different learning paradigms, which is less explored. It provides a novel algorithm that leverages supervised and unsupervised learning paradigms to enhance transferability, showing promising results across various learning algorithms and extensive experiments on benchmark datasets.' Weaknesses='The assumption that frequency-level characteristics are indicative of successful transferability is intriguing but lacks a deeper theoretical explanation. The paper could also benefit from a stronger justification for the choice of specific algorithms in generating poisoning attacks. Additionally, while extensive experiments are conducted, the scalability of the method to larger datasets or more complex learning paradigms needs further exploration.' Questions='1. How does the proposed method perform on larger scale datasets or more complex models? 2. Could the approach be adapted to other modalities beyond image datasets? 3. What are the computational costs associated with using both supervised and unsupervised paradigms in generating poisoning perturbations?'\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Invalid \\escape: line 1 column 261 (char 260)\n",
      "Content:\n",
      "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"The paper presents an intriguing perspective on language structure and the relationship with LLM capabilities.\", \"Weaknesses\": \"The concepts of unambiguous and {\\epsilon}-ambiguous languages as presented may require more rigorous definitions and deeper empirical evaluation to substantiate claims.\", \"Questions\": \"How do the categorization of languages into unambiguous or {\\epsilon}-ambiguous impact practical applications using LLMs?\"}\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses an important challenge in graph neural networks related to biases and fairness, offering a theoretical exploration of Lipschitz bounds as a stabilizing factor. It effectively combines robust statistics with deep learning, potentially leading to more fair and stable GNNs. The experiments conducted support the theoretical claims made.' Weaknesses='The complexity of calculating Lipschitz bounds for non-Euclidean spaces such as graphs is not entirely resolved, and the methodology may be limited to theoretical analyses without practical implementation details. The paper may lack comprehensive evaluations or considerations of alternative approaches.' Questions='How does the proposed Lipschitz bound calculation compare to existing methods in terms of efficiency and accuracy in practice? What are the potential trade-offs when applying these bounds in real-world scenarios?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses an important challenge in graph neural networks related to biases and fairness, offering a theoretical exploration of Lipschitz bounds as a stabilizing factor. It effectively combines robust statistics with deep learning, potentially leading to more fair and stable GNNs. The experiments conducted support the theoretical claims made.' Weaknesses='The complexity of calculating Lipschitz bounds for non-Euclidean spaces such as graphs is not entirely resolved, and the methodology may be limited to theoretical analyses without practical implementation details. The paper may lack comprehensive evaluations or considerations of alternative approaches.' Questions='How does the proposed Lipschitz bound calculation compare to existing methods in terms of efficiency and accuracy in practice? What are the potential trade-offs when applying these bounds in real-world scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a novel analysis of non-contrastive learning dynamics by incorporating feature normalization. The introduction of cosine loss to understand the role of feature normalization is an interesting addition that sheds light on preventing representation collapse in non-contrastive learning frameworks.' Weaknesses='The paper could benefit from a more comprehensive comparison with existing literature, especially with recent contributions that also consider the role of normalization techniques. The theoretical exploration appears sound but lacks empirical validation on more complex models, which limits the practical impact of the findings.' Questions='How does the proposed analysis compare with other derivations using implicit regularization techniques in similar frameworks? Are there practical experiments beyond linear networks to validate the claims better?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a novel analysis of non-contrastive learning dynamics by incorporating feature normalization. The introduction of cosine loss to understand the role of feature normalization is an interesting addition that sheds light on preventing representation collapse in non-contrastive learning frameworks.' Weaknesses='The paper could benefit from a more comprehensive comparison with existing literature, especially with recent contributions that also consider the role of normalization techniques. The theoretical exploration appears sound but lacks empirical validation on more complex models, which limits the practical impact of the findings.' Questions='How does the proposed analysis compare with other derivations using implicit regularization techniques in similar frameworks? Are there practical experiments beyond linear networks to validate the claims better?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses an important problem in deep learning, namely the challenges posed by local minima and saddle points during training.\\n2. The concept of optimization by continuation and its theoretical insights are valuable contributions that enhance understanding of training dynamics in deep learning models.\\n3. The proposed regularization strategy appears to be a novel way to improve optimization process.' Weaknesses='1. The paper might lack sufficiently rigorous empirical validation on large-scale or diverse datasets to demonstrate generalizability.\\n2. The treatment of saddle points might not fully account for complexities found in high-dimensional neural network settings, potentially limiting applicability.\\n3. The paper does not seem to adequately compare its approach to other established methods for overcoming optimization challenges in deep learning.' Questions='1. How does the proposed continuation method compare with other common optimization techniques like stochastic gradient descent momentum (SGDM) or Adam optimizer in terms of computational efficiency and convergence?\\n2. Could the authors provide more detailed experimental results on different architectures, such as CNNs or transformers, to evaluate the general applicability of their method?\\n3. What are the specific assumptions made about the loss landscapes of neural networks when applying this continuation approach, and how might they be relaxed or modified for broader applicability?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses an important problem in deep learning, namely the challenges posed by local minima and saddle points during training.\\n2. The concept of optimization by continuation and its theoretical insights are valuable contributions that enhance understanding of training dynamics in deep learning models.\\n3. The proposed regularization strategy appears to be a novel way to improve optimization process.' Weaknesses='1. The paper might lack sufficiently rigorous empirical validation on large-scale or diverse datasets to demonstrate generalizability.\\n2. The treatment of saddle points might not fully account for complexities found in high-dimensional neural network settings, potentially limiting applicability.\\n3. The paper does not seem to adequately compare its approach to other established methods for overcoming optimization challenges in deep learning.' Questions='1. How does the proposed continuation method compare with other common optimization techniques like stochastic gradient descent momentum (SGDM) or Adam optimizer in terms of computational efficiency and convergence?\\n2. Could the authors provide more detailed experimental results on different architectures, such as CNNs or transformers, to evaluate the general applicability of their method?\\n3. What are the specific assumptions made about the loss landscapes of neural networks when applying this continuation approach, and how might they be relaxed or modified for broader applicability?'\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Content:\n",
      "I'm sorry, the previous output is incorrect and was generated in error. Let me generate the correct JSON object for you.\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel Transformer-based architecture integrating feature and cost aggregation for dense matching tasks. It effectively exploits self- and cross-attention mechanisms to improve accuracy, demonstrating significant improvements on standard benchmarks.' Weaknesses='The abstract lacks detail on the specific results and comparisons. It also does not mention how the proposed method compares in terms of computational complexity or efficiency against existing approaches.' Questions=\"How does the proposed method's computational complexity compare to existing state-of-the-art methods? Are there specific types of dense matching tasks where the approach does not perform well?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel Transformer-based architecture integrating feature and cost aggregation for dense matching tasks. It effectively exploits self- and cross-attention mechanisms to improve accuracy, demonstrating significant improvements on standard benchmarks.' Weaknesses='The abstract lacks detail on the specific results and comparisons. It also does not mention how the proposed method compares in terms of computational complexity or efficiency against existing approaches.' Questions=\"How does the proposed method's computational complexity compare to existing state-of-the-art methods? Are there specific types of dense matching tasks where the approach does not perform well?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses a significant gap in the DFML literature by proposing methods to handle Task-Distribution Shift (TDS) and Task-Distribution Corruption (TDC), crucial aspects for improving robustness.\\n2. The introduction of TEAPOT and ROSY represents innovative approaches that diversify pseudo task distributions and adaptively manage model selection, respectively.\\n3. The experimental validation confirms the advantages of TEAPOT and ROSY for enhancing robustness against distributional challenges.' Weaknesses=\"1. The paper relies heavily on assumptions that may not hold in all practical scenarios, such as the risk of TDC through model poisoning attacks being representative without simple preventative measures.\\n2. The computational complexity associated with the reinforcement learning based ROSY method may limit its applicability to real-world situations.\\n3. While experiments showcase the method's effectiveness, the focus on 4-layered convolutional networks raises questions about generalizability to more complex architectures and diverse real-world datasets.\" Questions='1. How does the proposed TEAPOT method compare to existing continual learning methodologies in terms of computational overhead and model accuracy?\\n2. Can the ROSY framework be efficiently scaled to accommodate larger and more varied deep learning models, considering its use of reinforcement learning?\\n3. How does the defense strategy of ROSY handle cases where model poisoning is less apparent or disguised through subtle, incremental changes?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses a significant gap in the DFML literature by proposing methods to handle Task-Distribution Shift (TDS) and Task-Distribution Corruption (TDC), crucial aspects for improving robustness.\\n2. The introduction of TEAPOT and ROSY represents innovative approaches that diversify pseudo task distributions and adaptively manage model selection, respectively.\\n3. The experimental validation confirms the advantages of TEAPOT and ROSY for enhancing robustness against distributional challenges.' Weaknesses=\"1. The paper relies heavily on assumptions that may not hold in all practical scenarios, such as the risk of TDC through model poisoning attacks being representative without simple preventative measures.\\n2. The computational complexity associated with the reinforcement learning based ROSY method may limit its applicability to real-world situations.\\n3. While experiments showcase the method's effectiveness, the focus on 4-layered convolutional networks raises questions about generalizability to more complex architectures and diverse real-world datasets.\" Questions='1. How does the proposed TEAPOT method compare to existing continual learning methodologies in terms of computational overhead and model accuracy?\\n2. Can the ROSY framework be efficiently scaled to accommodate larger and more varied deep learning models, considering its use of reinforcement learning?\\n3. How does the defense strategy of ROSY handle cases where model poisoning is less apparent or disguised through subtle, incremental changes?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper addresses a crucial issue of value alignment in LLMs, introducing the Value Understanding Measurement (VUM) framework, which is an innovative approach to quantitatively assessing 'know what' and 'know why' aspects of LLM understanding of human values. The development of a dialogue dataset based on the Schwartz Value Survey and the evaluation of five representative LLMs provide solid empirical grounding for their claims.\" Weaknesses=\"There is a lack of clarity on certain methodological aspects, such as the inputs for models in different steps of the evaluation process, and the specifics of prompts used in certain experiments. The evaluation might be limited by the unknown parameters and training details of the LLMs considered, which might affect the robustness of the claims related to scaling laws and model capabilities. The presented human study lacks details, which raises concerns about the validation of the critique model's ability to distinguish outputs beyond superficial explanations.\" Questions='1. Can you clarify what specific inputs were used for the models at each step of the VUM framework evaluation? 2. What were the specific prompts used in the experiment involving relevance to values, and where are the results discussed? 3. Could you provide more detailed insights into the human study methodology and results you mention in the appendix?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper addresses a crucial issue of value alignment in LLMs, introducing the Value Understanding Measurement (VUM) framework, which is an innovative approach to quantitatively assessing 'know what' and 'know why' aspects of LLM understanding of human values. The development of a dialogue dataset based on the Schwartz Value Survey and the evaluation of five representative LLMs provide solid empirical grounding for their claims.\" Weaknesses=\"There is a lack of clarity on certain methodological aspects, such as the inputs for models in different steps of the evaluation process, and the specifics of prompts used in certain experiments. The evaluation might be limited by the unknown parameters and training details of the LLMs considered, which might affect the robustness of the claims related to scaling laws and model capabilities. The presented human study lacks details, which raises concerns about the validation of the critique model's ability to distinguish outputs beyond superficial explanations.\" Questions='1. Can you clarify what specific inputs were used for the models at each step of the VUM framework evaluation? 2. What were the specific prompts used in the experiment involving relevance to values, and where are the results discussed? 3. Could you provide more detailed insights into the human study methodology and results you mention in the appendix?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The proposed Hierarchical Diffuser demonstrates an effective solution to combine hierarchical planning strategies with diffusion-based methods, showing superior performance on challenging offline reinforcement learning tasks.\\nThe model successfully addresses the computational inefficiencies typically associated with diffusion-based planners by adopting a 'jumpy' planning strategy, leading to faster training and planning stages as verified through empirical evaluations.\\nThe paper provides an interesting exploration of the generalization capabilities of their method on out-of-distribution tasks, illustrating its potential beyond standard benchmarks.\" Weaknesses='The concept of integrating hierarchical methods with diffusion-based models, while effective, may lack substantial novelty, as elements of this approach draw from existing methods within the field. \\nFurther details on the implementation specifics, such as parameter settings or computational overhead in real-world scenarios, are needed to thoroughly evaluate practical applicability and consistency across different environments.\\nThe evaluation could be enhanced by comparing against a broader range of state-of-the-art methodologies, particularly those employing recent advancements in hierarchical and diffusion models.' Questions=\"How does the Hierarchical Diffuser handle environments with significantly different distributions or noise levels? Are there assumptions about the underlying dynamics or noise in order for the jumpy planning strategy to be effective?\\nCan the method be extended to handle multi-agent environments, or is it inherently limited to single-agent settings due to the architectural choices made in the model's design?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The proposed Hierarchical Diffuser demonstrates an effective solution to combine hierarchical planning strategies with diffusion-based methods, showing superior performance on challenging offline reinforcement learning tasks.\\nThe model successfully addresses the computational inefficiencies typically associated with diffusion-based planners by adopting a 'jumpy' planning strategy, leading to faster training and planning stages as verified through empirical evaluations.\\nThe paper provides an interesting exploration of the generalization capabilities of their method on out-of-distribution tasks, illustrating its potential beyond standard benchmarks.\" Weaknesses='The concept of integrating hierarchical methods with diffusion-based models, while effective, may lack substantial novelty, as elements of this approach draw from existing methods within the field. \\nFurther details on the implementation specifics, such as parameter settings or computational overhead in real-world scenarios, are needed to thoroughly evaluate practical applicability and consistency across different environments.\\nThe evaluation could be enhanced by comparing against a broader range of state-of-the-art methodologies, particularly those employing recent advancements in hierarchical and diffusion models.' Questions=\"How does the Hierarchical Diffuser handle environments with significantly different distributions or noise levels? Are there assumptions about the underlying dynamics or noise in order for the jumpy planning strategy to be effective?\\nCan the method be extended to handle multi-agent environments, or is it inherently limited to single-agent settings due to the architectural choices made in the model's design?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='This paper presents a novel approach to generating saliency maps for Spiking Neural Networks (SNNs), addressing a notable gap in event-based vision using a bio-inspired sensor. The introduction of Spiking Layer-Time-wise Relevance Propagation and Spiking Layer-wise Relevance Propagation demonstrates a methodological innovation that contributes to improved performance in object and action recognition tasks. The results show state-of-the-art performance, highlighting the effectiveness of the proposed method.' Weaknesses='While the proposed method and results are promising, the lack of comparison with pre-existing event datasets such as N-ImageNet is a limitation. The method could also benefit from broader comparative studies with other saliency-based augmentation techniques besides conventional vision methods like Grad-CAM.' Questions='How does the proposed method compare to other saliency-based augmentation techniques specifically designed for event camera applications? Additionally, could the method be applied to larger datasets such as N-ImageNet for a more comprehensive performance evaluation?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='This paper presents a novel approach to generating saliency maps for Spiking Neural Networks (SNNs), addressing a notable gap in event-based vision using a bio-inspired sensor. The introduction of Spiking Layer-Time-wise Relevance Propagation and Spiking Layer-wise Relevance Propagation demonstrates a methodological innovation that contributes to improved performance in object and action recognition tasks. The results show state-of-the-art performance, highlighting the effectiveness of the proposed method.' Weaknesses='While the proposed method and results are promising, the lack of comparison with pre-existing event datasets such as N-ImageNet is a limitation. The method could also benefit from broader comparative studies with other saliency-based augmentation techniques besides conventional vision methods like Grad-CAM.' Questions='How does the proposed method compare to other saliency-based augmentation techniques specifically designed for event camera applications? Additionally, could the method be applied to larger datasets such as N-ImageNet for a more comprehensive performance evaluation?'\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Content:\n",
      "I'm sorry, but I cannot proceed with the completion or output of this request.\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The integration of LLMs into autonomous driving represents an innovative approach, addressing the challenge of rare event generalization and interpretability through commonsense reasoning.\\n2. The paper is well-structured, with clear illustrations that enhance understanding of the proposed methodology.\\n3. Extensive experimentation validates the approach, showing improvements over baseline methods.' Weaknesses='1. The paper lacks a detailed analysis of the potential safety risks and consistency issues associated with using LLMs in real-time safety-critical systems like autonomous driving.\\n2. The scalability and real-time applicability of the proposed method are not thoroughly evaluated, raising concerns about the efficiency of the approach under practical constraints.\\n3. Comparisons with only one other RL algorithm limit the strength of demonstrated improvements over existing methods.' Questions='1. How does the proposed method ensure the robustness of LLM outputs given the inherent randomness in LLMs, especially in critical driving scenarios?\\n2. Can the authors elaborate on the real-time computational demands of integrating LLMs into the decision-making process of autonomous vehicles?\\n3. What mechanisms are in place to ensure consistent behavior across different versions of LLMs used for decision-making?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The integration of LLMs into autonomous driving represents an innovative approach, addressing the challenge of rare event generalization and interpretability through commonsense reasoning.\\n2. The paper is well-structured, with clear illustrations that enhance understanding of the proposed methodology.\\n3. Extensive experimentation validates the approach, showing improvements over baseline methods.' Weaknesses='1. The paper lacks a detailed analysis of the potential safety risks and consistency issues associated with using LLMs in real-time safety-critical systems like autonomous driving.\\n2. The scalability and real-time applicability of the proposed method are not thoroughly evaluated, raising concerns about the efficiency of the approach under practical constraints.\\n3. Comparisons with only one other RL algorithm limit the strength of demonstrated improvements over existing methods.' Questions='1. How does the proposed method ensure the robustness of LLM outputs given the inherent randomness in LLMs, especially in critical driving scenarios?\\n2. Can the authors elaborate on the real-time computational demands of integrating LLMs into the decision-making process of autonomous vehicles?\\n3. What mechanisms are in place to ensure consistent behavior across different versions of LLMs used for decision-making?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The research presents a novel approach to simplifying Transformer architecture, which leads to faster training throughput and reduced parameter use without loss of performance or training speed. The combination of signal propagation theory and empirical observations provides a solid foundation to the proposed modifications, making it an insightful contribution to the field.' Weaknesses='The evaluation was conducted on relatively smaller models (~100M parameters) and a limited number of tasks. This raises questions about the generalizability of the results to larger, more complex models and a broader set of tasks. Moreover, the practical implications of removing certain components (e.g., skip connections) might not be fully addressed, particularly regarding long-term stability and scalability.' Questions='1. How does the simplified model perform on larger, more complex transformer architectures commonly used in industry? 2. What are the potential long-term effects of removing components like skip connections on model stability during training? 3. Would the simplifications proposed be effective across various types of tasks, such as those requiring complex reasoning or multi-modal integration?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The research presents a novel approach to simplifying Transformer architecture, which leads to faster training throughput and reduced parameter use without loss of performance or training speed. The combination of signal propagation theory and empirical observations provides a solid foundation to the proposed modifications, making it an insightful contribution to the field.' Weaknesses='The evaluation was conducted on relatively smaller models (~100M parameters) and a limited number of tasks. This raises questions about the generalizability of the results to larger, more complex models and a broader set of tasks. Moreover, the practical implications of removing certain components (e.g., skip connections) might not be fully addressed, particularly regarding long-term stability and scalability.' Questions='1. How does the simplified model perform on larger, more complex transformer architectures commonly used in industry? 2. What are the potential long-term effects of removing components like skip connections on model stability during training? 3. Would the simplifications proposed be effective across various types of tasks, such as those requiring complex reasoning or multi-modal integration?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper addresses an important problem of differentiating through infeasible QPs, which is crucial for optimization layers in neural networks. The approach is innovative and leverages primal-dual augmented Lagrangian techniques effectively. The provision of open-source software, QPLayer, adds significant practical value to the research, allowing easy integration into modern learning frameworks.' Weaknesses='The presentation could be clearer, especially for readers not familiar with optimization layers, as some sections are complex and could be expanded with more explanations. The feasibility analysis might benefit from additional empirical validations across diverse datasets and practical scenarios, which would further substantiate the claims.' Questions='How does the proposed method compare in performance with traditional optimization techniques in extensive real-world applications? Are there limitations to the differentiability approach that might affect computational efficiency or stability under certain conditions? Could the method be generalized for non-convex QP layers?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper addresses an important problem of differentiating through infeasible QPs, which is crucial for optimization layers in neural networks. The approach is innovative and leverages primal-dual augmented Lagrangian techniques effectively. The provision of open-source software, QPLayer, adds significant practical value to the research, allowing easy integration into modern learning frameworks.' Weaknesses='The presentation could be clearer, especially for readers not familiar with optimization layers, as some sections are complex and could be expanded with more explanations. The feasibility analysis might benefit from additional empirical validations across diverse datasets and practical scenarios, which would further substantiate the claims.' Questions='How does the proposed method compare in performance with traditional optimization techniques in extensive real-world applications? Are there limitations to the differentiability approach that might affect computational efficiency or stability under certain conditions? Could the method be generalized for non-convex QP layers?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a novel method for addressing contextual stochastic optimization problems, focusing on balancing expected cost minimization with robustness against worst-case scenarios. The approach involves discretizing the feasible region and employing a secondary optimization problem, providing theoretical guarantees for bounding regret. The method shows competitive results and significantly reduces worst-case costs in experiments.' Weaknesses=\"The paper's presentation could be clearer, particularly in explaining the discretization and secondary optimization processes in a more accessible manner. Further experimental validation on a broader set of problems would strengthen the claims.\" Questions='How sensitive is the method to the choice of discretization and the specific parameters controlling the user-defined cost threshold? Are there specific contexts where the method may not perform as well?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a novel method for addressing contextual stochastic optimization problems, focusing on balancing expected cost minimization with robustness against worst-case scenarios. The approach involves discretizing the feasible region and employing a secondary optimization problem, providing theoretical guarantees for bounding regret. The method shows competitive results and significantly reduces worst-case costs in experiments.' Weaknesses=\"The paper's presentation could be clearer, particularly in explaining the discretization and secondary optimization processes in a more accessible manner. Further experimental validation on a broader set of problems would strengthen the claims.\" Questions='How sensitive is the method to the choice of discretization and the specific parameters controlling the user-defined cost threshold? Are there specific contexts where the method may not perform as well?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel approach to prompt design by employing a Mixture-of-Expert paradigm, which successfully addresses the limitation of demo-free instruction models. The proposed Mixture-of-Prompts (MoP) method significantly enhances performance on benchmark NLP tasks, suggesting its practical applicability and effectiveness. The incorporation of semantic similarity in demo assignment and the use of a joint search strategy further strengthen their approach.' Weaknesses='The dependency on the initial clustering method and its potential impact on the performance requires further discussion and analysis. There is a need for more comprehensive experimentation across a wider variety of datasets and tasks to ensure the generality of the approach.' Questions='How does the choice of clustering algorithm impact the overall performance of the MoP approach? Is there a significant variation in results based on different clustering techniques?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel approach to prompt design by employing a Mixture-of-Expert paradigm, which successfully addresses the limitation of demo-free instruction models. The proposed Mixture-of-Prompts (MoP) method significantly enhances performance on benchmark NLP tasks, suggesting its practical applicability and effectiveness. The incorporation of semantic similarity in demo assignment and the use of a joint search strategy further strengthen their approach.' Weaknesses='The dependency on the initial clustering method and its potential impact on the performance requires further discussion and analysis. There is a need for more comprehensive experimentation across a wider variety of datasets and tasks to ensure the generality of the approach.' Questions='How does the choice of clustering algorithm impact the overall performance of the MoP approach? Is there a significant variation in results based on different clustering techniques?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. This paper provides a novel theoretical perspective on the instability issues observed in heteroskedastic neural regression models, particularly highlighting the importance of regularization. \\n2. The derivation of the nonparametric free energy and the identification of phase transitions provide a deeper understanding of the underlying phenomena in overparameterized models.' Weaknesses='1. The paper may rely heavily on theoretical concepts that could be difficult for practitioners not familiar with statistical physics to grasp fully. \\n2. More extensive empirical validation could strengthen the presented theoretical claims, especially in diverse real-world scenarios.' Questions='1. How might the proposed regularization optimization scheme be applied in practice in current machine learning frameworks?\\n2. Are there specific types of data or problem domains where the described phase transitions are more critical to consider?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. This paper provides a novel theoretical perspective on the instability issues observed in heteroskedastic neural regression models, particularly highlighting the importance of regularization. \\n2. The derivation of the nonparametric free energy and the identification of phase transitions provide a deeper understanding of the underlying phenomena in overparameterized models.' Weaknesses='1. The paper may rely heavily on theoretical concepts that could be difficult for practitioners not familiar with statistical physics to grasp fully. \\n2. More extensive empirical validation could strengthen the presented theoretical claims, especially in diverse real-world scenarios.' Questions='1. How might the proposed regularization optimization scheme be applied in practice in current machine learning frameworks?\\n2. Are there specific types of data or problem domains where the described phase transitions are more critical to consider?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses an important problem of domain adaptation, particularly in scenarios where unlabeled target data is available but not effectively utilized through standard approaches. The proposed method, Connect Later, is innovative in combining generic pretraining with fine-tuning using targeted augmentations to effectively reduce out-of-distribution (OOD) errors across multiple real-world datasets. The results are significant, achieving state-of-the-art in certain cases, and the paper is well-structured, making it easy to follow the proposed methodology and experiments.' Weaknesses='One potential limitation is the dependency on the specific domain knowledge to design targeted augmentations, which might limit the applicability of the proposed method to domains where such knowledge is not readily available. Additionally, the paper could benefit from a broader comparison with other domain adaptation or transfer learning techniques to situate its contributions more clearly within the field.' Questions='Can the Connect Later framework be applied in cases where there is minimal or no domain knowledge available to inform the design of targeted augmentations?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses an important problem of domain adaptation, particularly in scenarios where unlabeled target data is available but not effectively utilized through standard approaches. The proposed method, Connect Later, is innovative in combining generic pretraining with fine-tuning using targeted augmentations to effectively reduce out-of-distribution (OOD) errors across multiple real-world datasets. The results are significant, achieving state-of-the-art in certain cases, and the paper is well-structured, making it easy to follow the proposed methodology and experiments.' Weaknesses='One potential limitation is the dependency on the specific domain knowledge to design targeted augmentations, which might limit the applicability of the proposed method to domains where such knowledge is not readily available. Additionally, the paper could benefit from a broader comparison with other domain adaptation or transfer learning techniques to situate its contributions more clearly within the field.' Questions='Can the Connect Later framework be applied in cases where there is minimal or no domain knowledge available to inform the design of targeted augmentations?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper introduces a novel approach using multi-party computation for secure information retrieval that maintains privacy.\\n2. It effectively addresses the issue of performing retrieval-augmented generation without exposing queries or database contents, which is crucial for sensitive data handling.\\n3. The introduction of a new MPC protocol for inverted file approximate search is a significant technical contribution.' Weaknesses='1. The practicality of the approach in real-world large-scale applications could be questioned, given the potential for high computational and communication costs.\\n2. There is limited information on performance benchmarks or comparisons with existing solutions, which makes it challenging to assess efficiency.' Questions='1. How does the proposed system scale with very large datasets, and what are the computational and time overheads involved?\\n2. Can the approach be integrated easily with existing large language models and data pipelines without significant modifications?\\n3. What is the impact on accuracy and relevance of the retrieved documents when using the MPC-based retrieval method?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper introduces a novel approach using multi-party computation for secure information retrieval that maintains privacy.\\n2. It effectively addresses the issue of performing retrieval-augmented generation without exposing queries or database contents, which is crucial for sensitive data handling.\\n3. The introduction of a new MPC protocol for inverted file approximate search is a significant technical contribution.' Weaknesses='1. The practicality of the approach in real-world large-scale applications could be questioned, given the potential for high computational and communication costs.\\n2. There is limited information on performance benchmarks or comparisons with existing solutions, which makes it challenging to assess efficiency.' Questions='1. How does the proposed system scale with very large datasets, and what are the computational and time overheads involved?\\n2. Can the approach be integrated easily with existing large language models and data pipelines without significant modifications?\\n3. What is the impact on accuracy and relevance of the retrieved documents when using the MPC-based retrieval method?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The proposal of a new distance metric (DDF) for comparing 3D point clouds is a novel idea that can advance existing methodologies significantly. \\n2. The experiments demonstrate that DDF outperforms existing metrics across multiple tasks, indicating its versatility and effectiveness.\\n3. The proposed method is computationally and memory efficient, making it practical for real-world applications.' Weaknesses='1. The generation process of reference points and some technical details related to the shared identical weight operation are not thoroughly explained in the paper. \\n2. While the experimental results are promising, more insights into the limitations or potential downsides of DDF compared with other metrics would strengthen the paper.' Questions='1. How are the reference points determined or generated, and how does this affect the results?\\n2. Can you elaborate on how the identical weights are shared in the equation related to directional distances? Is there any scenario where this might not hold effectively?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The proposal of a new distance metric (DDF) for comparing 3D point clouds is a novel idea that can advance existing methodologies significantly. \\n2. The experiments demonstrate that DDF outperforms existing metrics across multiple tasks, indicating its versatility and effectiveness.\\n3. The proposed method is computationally and memory efficient, making it practical for real-world applications.' Weaknesses='1. The generation process of reference points and some technical details related to the shared identical weight operation are not thoroughly explained in the paper. \\n2. While the experimental results are promising, more insights into the limitations or potential downsides of DDF compared with other metrics would strengthen the paper.' Questions='1. How are the reference points determined or generated, and how does this affect the results?\\n2. Can you elaborate on how the identical weights are shared in the equation related to directional distances? Is there any scenario where this might not hold effectively?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The model phi-1 demonstrates impressive performance on coding tasks like HumanEval and MBPP despite its relatively small size. It showcases the potential of training efficiency by achieving reasonable accuracy with constrained computational resources and a small parameter count.' Weaknesses=\"The paper lacks information on the specific strategies or novel techniques used in refining phi-1, which are crucial to fully understanding its success. There is also limited explanation on how 'textbook quality' data significantly improves performance and how synthetic data was generated to enhance the learning process.\" Questions=\"How was the 'textbook quality' dataset curated, and what criteria were used to ensure its quality? What specific role did synthetically generated textbooks and exercises play in the model's training, and could more detail be provided on the GPT-3.5 synthetic data generation process?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The model phi-1 demonstrates impressive performance on coding tasks like HumanEval and MBPP despite its relatively small size. It showcases the potential of training efficiency by achieving reasonable accuracy with constrained computational resources and a small parameter count.' Weaknesses=\"The paper lacks information on the specific strategies or novel techniques used in refining phi-1, which are crucial to fully understanding its success. There is also limited explanation on how 'textbook quality' data significantly improves performance and how synthetic data was generated to enhance the learning process.\" Questions=\"How was the 'textbook quality' dataset curated, and what criteria were used to ensure its quality? What specific role did synthetically generated textbooks and exercises play in the model's training, and could more detail be provided on the GPT-3.5 synthetic data generation process?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides a novel and rigorous analysis of in-context learning focusing on both task retrieval and task learning. It reveals an interesting phenomenon regarding the risk bounds in task retrieval, which could have significant implications for the design of in-context learning systems.' Weaknesses='The paper’s generative models may be too simplified to fully capture the complexities of real-world in-context learning scenarios. Additionally, the paper may not sufficiently address potential limitations or challenges in applying these findings to practical implementations.' Questions='How do the theoretical findings translate to practical improvements in Transformer model implementations? Are there any specific conditions under which the unique risk bound behavior is most pronounced?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides a novel and rigorous analysis of in-context learning focusing on both task retrieval and task learning. It reveals an interesting phenomenon regarding the risk bounds in task retrieval, which could have significant implications for the design of in-context learning systems.' Weaknesses='The paper’s generative models may be too simplified to fully capture the complexities of real-world in-context learning scenarios. Additionally, the paper may not sufficiently address potential limitations or challenges in applying these findings to practical implementations.' Questions='How do the theoretical findings translate to practical improvements in Transformer model implementations? Are there any specific conditions under which the unique risk bound behavior is most pronounced?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"1. The proposed Learning to Bootstrap (L2B) method provides a novel approach to handling noisy labels by leveraging joint reweighting and implicit relabeling. 2. The method is both innovative and practical, offering compatibility with existing noisy label learning methods and leading to significant improvements without additional computational costs. 3. Extensive experimental results demonstrate the method's efficacy across multiple benchmark datasets, highlighting its robustness and practical applicability.\" Weaknesses=\"1. Although the paper demonstrates improvements on benchmark datasets, the novelty of the approach might be limited since the concept of bootstrapping and reweighting has been explored in different contexts before. 2. The reliance on a clean validation set assumption, as mentioned in section 3.1, contradicts the claims of not needing validation samples. This inconsistency could raise questions about the practical application of the method. 3. It's unclear how the method generalizes across diverse real-world datasets beyond those tested, given potential variations in the nature of label noise.\" Questions='1. How does the method handle systematic label noise, particularly given that much of the focus is on random noise scenarios? Are there specific adaptations required? 2. Can more clarifications be provided regarding the training process when alpha and beta do not necessarily sum to one? How does this affect the stability of the learned weightings?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"1. The proposed Learning to Bootstrap (L2B) method provides a novel approach to handling noisy labels by leveraging joint reweighting and implicit relabeling. 2. The method is both innovative and practical, offering compatibility with existing noisy label learning methods and leading to significant improvements without additional computational costs. 3. Extensive experimental results demonstrate the method's efficacy across multiple benchmark datasets, highlighting its robustness and practical applicability.\" Weaknesses=\"1. Although the paper demonstrates improvements on benchmark datasets, the novelty of the approach might be limited since the concept of bootstrapping and reweighting has been explored in different contexts before. 2. The reliance on a clean validation set assumption, as mentioned in section 3.1, contradicts the claims of not needing validation samples. This inconsistency could raise questions about the practical application of the method. 3. It's unclear how the method generalizes across diverse real-world datasets beyond those tested, given potential variations in the nature of label noise.\" Questions='1. How does the method handle systematic label noise, particularly given that much of the focus is on random noise scenarios? Are there specific adaptations required? 2. Can more clarifications be provided regarding the training process when alpha and beta do not necessarily sum to one? How does this affect the stability of the learned weightings?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper introduces an innovative approach to speech synthesis using the WaveFluid model, which directly learns a velocity field through adversarial training.\\n2. By leveraging reparameterization techniques, the proposed method effectively reduces memory footprint and improves training efficiency.\\n3. The competitive performance of the model is demonstrated, showing improved sample quality compared to previous vocoders within limited inference steps.' Weaknesses=\"1. The evaluation seems limited to specific settings, and more comparative analysis with state-of-the-art models across various benchmarks could strengthen the claims.\\n2. The paper's focus on specific architectural improvements may limit its broader applicability to other domains of generative modeling.\\n3. Details on training and hyperparameter settings, as well as potential limitations of the proposed approach, are not sufficiently discussed.\" Questions='How does the proposed WaveFluid model compare with cutting-edge vocoders in terms of computational resources required during inference? Are there plans to expand this approach to broader generative modeling tasks beyond speech synthesis?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper introduces an innovative approach to speech synthesis using the WaveFluid model, which directly learns a velocity field through adversarial training.\\n2. By leveraging reparameterization techniques, the proposed method effectively reduces memory footprint and improves training efficiency.\\n3. The competitive performance of the model is demonstrated, showing improved sample quality compared to previous vocoders within limited inference steps.' Weaknesses=\"1. The evaluation seems limited to specific settings, and more comparative analysis with state-of-the-art models across various benchmarks could strengthen the claims.\\n2. The paper's focus on specific architectural improvements may limit its broader applicability to other domains of generative modeling.\\n3. Details on training and hyperparameter settings, as well as potential limitations of the proposed approach, are not sufficiently discussed.\" Questions='How does the proposed WaveFluid model compare with cutting-edge vocoders in terms of computational resources required during inference? Are there plans to expand this approach to broader generative modeling tasks beyond speech synthesis?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper presents a novel unified framework using GANs to generate counterfactual examples that simultaneously improve interpretability and robustness of neural image classifiers. The framework is innovative in its dual focus, addressing two critical issues in image classification tasks. The method demonstrates effectiveness in producing descriptive saliency maps and improved adversarial robustness, which are validated through semantic segmentation and adversarial attack scenarios.' Weaknesses='The approach may require further validation across a broader range of datasets and tasks to establish its general applicability. The explanation of how the unified model works, particularly the integration of the classifier and discriminator, could benefit from additional clarity. There is also a need for comparative studies with other state-of-the-art methods addressing interpretability or adversarial robustness to thoroughly evaluate the competitive edge of the proposed framework.' Questions='How does the framework perform on multiclass classification tasks beyond the binary tasks evaluated? What are the computational costs associated with training the unified model compared to traditional classifier and adversarial training methods? Can the method be extended to other domains or application areas beyond image classification?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper presents a novel unified framework using GANs to generate counterfactual examples that simultaneously improve interpretability and robustness of neural image classifiers. The framework is innovative in its dual focus, addressing two critical issues in image classification tasks. The method demonstrates effectiveness in producing descriptive saliency maps and improved adversarial robustness, which are validated through semantic segmentation and adversarial attack scenarios.' Weaknesses='The approach may require further validation across a broader range of datasets and tasks to establish its general applicability. The explanation of how the unified model works, particularly the integration of the classifier and discriminator, could benefit from additional clarity. There is also a need for comparative studies with other state-of-the-art methods addressing interpretability or adversarial robustness to thoroughly evaluate the competitive edge of the proposed framework.' Questions='How does the framework perform on multiclass classification tasks beyond the binary tasks evaluated? What are the computational costs associated with training the unified model compared to traditional classifier and adversarial training methods? Can the method be extended to other domains or application areas beyond image classification?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper introduces a novel sampling strategy for noise-resilient representation learning, and proposes an efficient encoder architecture using dilated convolution within an Inception block.' Weaknesses=\"The lack of previous work explicitly addressing noise in time series is claimed, but not well corroborated with comparative analysis. The proposed method's novelty could be overstated given basic concepts like low-pass filtering for noise reduction.\" Questions='How does the proposed encoder with dilated convolution compare to other lightweight architectures in terms of computational efficiency? Are there specific scenarios or datasets where this method significantly underperforms?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper introduces a novel sampling strategy for noise-resilient representation learning, and proposes an efficient encoder architecture using dilated convolution within an Inception block.' Weaknesses=\"The lack of previous work explicitly addressing noise in time series is claimed, but not well corroborated with comparative analysis. The proposed method's novelty could be overstated given basic concepts like low-pass filtering for noise reduction.\" Questions='How does the proposed encoder with dilated convolution compare to other lightweight architectures in terms of computational efficiency? Are there specific scenarios or datasets where this method significantly underperforms?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=4 Strengths='The paper presents a novel approach to improve out-of-distribution generalization assessment by integrating neighboring test samples into correctness indications.' Weaknesses='The approach may add complexity and computational overhead due to the reliance on neighboring sample analysis. Additionally, it might not sufficiently address challenges involving severe distribution shifts.' Questions=\"How does the method's performance compare with other state-of-the-art techniques when applied to real-world datasets showing severe distributional shifts? Additionally, can VRP be optimized to reduce potential computational overhead?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=4 Strengths='The paper presents a novel approach to improve out-of-distribution generalization assessment by integrating neighboring test samples into correctness indications.' Weaknesses='The approach may add complexity and computational overhead due to the reliance on neighboring sample analysis. Additionally, it might not sufficiently address challenges involving severe distribution shifts.' Questions=\"How does the method's performance compare with other state-of-the-art techniques when applied to real-world datasets showing severe distributional shifts? Additionally, can VRP be optimized to reduce potential computational overhead?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel graph transformer architecture that explicitly considers edge directionality, addressing a significant gap in existing graph learning models. The use of dual encodings and a directional attention module demonstrates an innovative approach to learning with directed graphs. The empirical results on newly proposed directional graph datasets show state-of-the-art performance, underscoring the effectiveness of the method.' Weaknesses='While the approach is sound and presents a novel perspective, the paper would benefit from a more thorough exploration of potential limitations or challenges in applying the approach to a wide range of graph types. The introduction of new datasets is useful, but there could be a potential bias in designing datasets that naturally favor the proposed method.' Questions=\"How does the proposed method perform on non-directional graphs, or when directionality is only mildly present? Are there any plans to test the method's applicability to real-world datasets where directionality might not be as pronounced?\"\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel graph transformer architecture that explicitly considers edge directionality, addressing a significant gap in existing graph learning models. The use of dual encodings and a directional attention module demonstrates an innovative approach to learning with directed graphs. The empirical results on newly proposed directional graph datasets show state-of-the-art performance, underscoring the effectiveness of the method.' Weaknesses='While the approach is sound and presents a novel perspective, the paper would benefit from a more thorough exploration of potential limitations or challenges in applying the approach to a wide range of graph types. The introduction of new datasets is useful, but there could be a potential bias in designing datasets that naturally favor the proposed method.' Questions=\"How does the proposed method perform on non-directional graphs, or when directionality is only mildly present? Are there any plans to test the method's applicability to real-world datasets where directionality might not be as pronounced?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses a relevant and practical problem of restricted black-box attacks on Graph Neural Networks and proposes a novel attack method, SheAttack, using the Modified Silhouette Score (MSS) as an objective measurement. The approach is theoretically underpinned and shows promising results in experimental validation across both synthetic and real-world datasets.' Weaknesses=\"The method relies on several hyperparameters, the impact of which isn't fully explored or justified, potentially affecting the robustness and general applicability of the proposed solution. Moreover, while theoretically informed, the innovative aspects of the MSS and its implementation might require further elaboration to fully appreciate its significance over existing methods.\" Questions='How sensitive is the SheAttack performance to the choice of hyperparameters? Are there particular settings or initialization strategies that mitigate performance variability? Further, could the modified silhouette approach be generalized to attack other types of networks beyond GNNs?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses a relevant and practical problem of restricted black-box attacks on Graph Neural Networks and proposes a novel attack method, SheAttack, using the Modified Silhouette Score (MSS) as an objective measurement. The approach is theoretically underpinned and shows promising results in experimental validation across both synthetic and real-world datasets.' Weaknesses=\"The method relies on several hyperparameters, the impact of which isn't fully explored or justified, potentially affecting the robustness and general applicability of the proposed solution. Moreover, while theoretically informed, the innovative aspects of the MSS and its implementation might require further elaboration to fully appreciate its significance over existing methods.\" Questions='How sensitive is the SheAttack performance to the choice of hyperparameters? Are there particular settings or initialization strategies that mitigate performance variability? Further, could the modified silhouette approach be generalized to attack other types of networks beyond GNNs?'\n",
      "Finished gpt4o on data/test_data/one_shot/test_data_2024_abstract_prompts_oneshot.jsonl, saved => results/gpt4o/test_data_2024_abstract_prompts_oneshot_gpt4o_results.jsonl\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The integration between gradient-based NAS and weight-entangled spaces is novel and addresses a key gap in the field. TangleNAS provides a more efficient approach by leveraging the strengths of both paradigms, which results in improved performance, reduced memory usage, and high accuracy across various search spaces.' Weaknesses='The paper may lack details on potential limitations or edge cases where the methodology may not perform as expected. Additionally, the complexity of implementing the proposed approach could be a barrier for some practitioners.' Questions='How does TangleNAS handle edge cases where weight entanglement might introduce conflicts or marginal improvements? Can it be adapted for other configurations beyond the tested search spaces?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The integration between gradient-based NAS and weight-entangled spaces is novel and addresses a key gap in the field. TangleNAS provides a more efficient approach by leveraging the strengths of both paradigms, which results in improved performance, reduced memory usage, and high accuracy across various search spaces.' Weaknesses='The paper may lack details on potential limitations or edge cases where the methodology may not perform as expected. Additionally, the complexity of implementing the proposed approach could be a barrier for some practitioners.' Questions='How does TangleNAS handle edge cases where weight entanglement might introduce conflicts or marginal improvements? Can it be adapted for other configurations beyond the tested search spaces?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper effectively applies the concept of contrastive loss to a novel domain, offering a potentially impactful alternative to traditional MSE loss for protein fitness prediction. The method is demonstrated to be more data-efficient and robust, even in noisy conditions, which is essential for real-world applications.' Weaknesses='The reliance on simulations and limited real-world validations (e.g., only using FLIP benchmarks) may not fully capture the versatility of the approach across diverse biological datasets. Further empirical evaluation on a broader set of datasets would enhance the validity of the conclusions.' Questions='Could the proposed method be applied to different biological contexts beyond protein fitness prediction, or extend to other types of biological data analysis? Are there limitations in the types of nonlinearities that can be tackled using contrastive loss?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper effectively applies the concept of contrastive loss to a novel domain, offering a potentially impactful alternative to traditional MSE loss for protein fitness prediction. The method is demonstrated to be more data-efficient and robust, even in noisy conditions, which is essential for real-world applications.' Weaknesses='The reliance on simulations and limited real-world validations (e.g., only using FLIP benchmarks) may not fully capture the versatility of the approach across diverse biological datasets. Further empirical evaluation on a broader set of datasets would enhance the validity of the conclusions.' Questions='Could the proposed method be applied to different biological contexts beyond protein fitness prediction, or extend to other types of biological data analysis? Are there limitations in the types of nonlinearities that can be tackled using contrastive loss?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='FEATHER introduces a novel lightweight adapter framework capable of efficient lifelong test-time adaptation with reduced computational requirements. Its approach of separating source and target domain knowledge is innovative and practical for hardware-constrained environments. The method does not require access to source data, which is advantageous in real-world scenarios.' Weaknesses='The performance compared to full adaptation methods like CoTTA could be further clarified. The applicability of the proposed adapter solution beyond CNNs is not addressed, potentially limiting its generalizability. The parameter efficiency claim, while strong, could benefit from a more detailed analysis of the precise computational savings and any possible trade-offs in terms of performance.' Questions=\"Can the adapter mechanism be effectively extended to transformer architectures or other neural network structures beyond CNNs? How does FEATHER's adaptation performance compare to methods requiring full model adaptation, particularly in highly dynamic test environments? What specific scenarios or datasets would most benefit from the proposed parameter reduction without a performance compromise?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='FEATHER introduces a novel lightweight adapter framework capable of efficient lifelong test-time adaptation with reduced computational requirements. Its approach of separating source and target domain knowledge is innovative and practical for hardware-constrained environments. The method does not require access to source data, which is advantageous in real-world scenarios.' Weaknesses='The performance compared to full adaptation methods like CoTTA could be further clarified. The applicability of the proposed adapter solution beyond CNNs is not addressed, potentially limiting its generalizability. The parameter efficiency claim, while strong, could benefit from a more detailed analysis of the precise computational savings and any possible trade-offs in terms of performance.' Questions=\"Can the adapter mechanism be effectively extended to transformer architectures or other neural network structures beyond CNNs? How does FEATHER's adaptation performance compare to methods requiring full model adaptation, particularly in highly dynamic test environments? What specific scenarios or datasets would most benefit from the proposed parameter reduction without a performance compromise?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces MINERVA, a novel feature selection filter, which effectively captures complex non-linear relationships between features and targets using neural estimation of mutual information. The methodological approach is clearly presented and demonstrates improved accuracy on synthetic benchmarks.' Weaknesses='While the method shows promise on synthetic datasets, it remains to be seen how it performs on real-world data. The reliance on neural networks may introduce complexity in tuning parameters and the potential for overfitting, which is not fully explored in the experiments.' Questions='1. How does MINERVA perform on real-world datasets compared to synthetic ones?\\n2. What strategies are in place to prevent overfitting of the neural network model used in feature selection?\\n3. How sensitive is MINERVA to different neural network architectures or hyperparameter settings?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces MINERVA, a novel feature selection filter, which effectively captures complex non-linear relationships between features and targets using neural estimation of mutual information. The methodological approach is clearly presented and demonstrates improved accuracy on synthetic benchmarks.' Weaknesses='While the method shows promise on synthetic datasets, it remains to be seen how it performs on real-world data. The reliance on neural networks may introduce complexity in tuning parameters and the potential for overfitting, which is not fully explored in the experiments.' Questions='1. How does MINERVA perform on real-world datasets compared to synthetic ones?\\n2. What strategies are in place to prevent overfitting of the neural network model used in feature selection?\\n3. How sensitive is MINERVA to different neural network architectures or hyperparameter settings?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides a new perspective on understanding the role of margins in contrastive learning through gradient-level analysis, which is novel and could lead to further insights in improving contrastive learning methods. The experimental results are robust, using multiple datasets to validate the findings.' Weaknesses='The connection between the gradient-level analysis and practical improvements in contrastive learning is not fully explored. The theoretical exposition could be clearer, as some parts may be too dense for readers not already familiar with the intricacies of contrastive learning and the role of margins.' Questions='How do the proposed gradient-level effects of margins translate into practical guidelines for optimizing contrastive learning? Could the insights gained from the gradient analysis be applied to other types of self-supervised learning tasks beyond the ones considered?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides a new perspective on understanding the role of margins in contrastive learning through gradient-level analysis, which is novel and could lead to further insights in improving contrastive learning methods. The experimental results are robust, using multiple datasets to validate the findings.' Weaknesses='The connection between the gradient-level analysis and practical improvements in contrastive learning is not fully explored. The theoretical exposition could be clearer, as some parts may be too dense for readers not already familiar with the intricacies of contrastive learning and the role of margins.' Questions='How do the proposed gradient-level effects of margins translate into practical guidelines for optimizing contrastive learning? Could the insights gained from the gradient analysis be applied to other types of self-supervised learning tasks beyond the ones considered?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel approach, CASA, for addressing unsupervised domain adaptation under label shift by focusing on conditional support alignment. The method is theoretically grounded with a new risk bound, which adds strength to its empirical success across multiple benchmark datasets.' Weaknesses='The paper primarily focuses on datasets with clear label shifts, leaving questions on its performance in more complex real-world scenarios or with more diverse distributions unaddressed. Additionally, the reliance on pseudo-labels may introduce biases, particularly when initial pseudo-labels are incorrect.' Questions='How does the CASA framework perform in real-world, complex domains beyond the benchmark datasets tested? Is the method robust to initial pseudo-label errors, and if not, how can this be mitigated in practice?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel approach, CASA, for addressing unsupervised domain adaptation under label shift by focusing on conditional support alignment. The method is theoretically grounded with a new risk bound, which adds strength to its empirical success across multiple benchmark datasets.' Weaknesses='The paper primarily focuses on datasets with clear label shifts, leaving questions on its performance in more complex real-world scenarios or with more diverse distributions unaddressed. Additionally, the reliance on pseudo-labels may introduce biases, particularly when initial pseudo-labels are incorrect.' Questions='How does the CASA framework perform in real-world, complex domains beyond the benchmark datasets tested? Is the method robust to initial pseudo-label errors, and if not, how can this be mitigated in practice?'\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Content:\n",
      "I'm sorry, but I can only provide valid JSON object based on the given input summary. Can you please clarify if you need any help with that?\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The PRGNN framework effectively addresses the challenge of label noise in GNNs, a relatively underexplored area, using a probabilistic graphical model without relying on label smoothness.\\n2. The method is robust, performing well across both homophilous and heterophilous graphs and in scenarios with high noise levels.\\n3. The use of variational inference and contrastive loss provides a strong methodological contribution to handling noisy labels in GNNs.' Weaknesses='1. The paper may lack clarity in explaining the practical implementation of PRGNN, particularly the steps involved in the variational inference process and the integration of Bayesian networks in the model.\\n2. Although the approach is novel, the evaluation datasets might not be diverse enough to fully demonstrate its robustness across different real-world scenarios.\\n3. The paper does not explore the potential limitations or failure cases of the method.' Questions='1. Can PRGNN be effectively extended to inductive learning scenarios where new nodes appear after the initial training phase?\\n2. How sensitive is the PRGNN framework to the choice of hyperparameters, especially in heterophilous graphs?\\n3. Are there any real-world datasets or applications where PRGNN has been tested beyond synthetic noise settings?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The PRGNN framework effectively addresses the challenge of label noise in GNNs, a relatively underexplored area, using a probabilistic graphical model without relying on label smoothness.\\n2. The method is robust, performing well across both homophilous and heterophilous graphs and in scenarios with high noise levels.\\n3. The use of variational inference and contrastive loss provides a strong methodological contribution to handling noisy labels in GNNs.' Weaknesses='1. The paper may lack clarity in explaining the practical implementation of PRGNN, particularly the steps involved in the variational inference process and the integration of Bayesian networks in the model.\\n2. Although the approach is novel, the evaluation datasets might not be diverse enough to fully demonstrate its robustness across different real-world scenarios.\\n3. The paper does not explore the potential limitations or failure cases of the method.' Questions='1. Can PRGNN be effectively extended to inductive learning scenarios where new nodes appear after the initial training phase?\\n2. How sensitive is the PRGNN framework to the choice of hyperparameters, especially in heterophilous graphs?\\n3. Are there any real-world datasets or applications where PRGNN has been tested beyond synthetic noise settings?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The paper addresses an important gap in continual learning by proposing a novel approach that simultaneously tackles catastrophic forgetting and loss of plasticity, making it highly relevant for real-world streaming learning applications.\\n2. The proposed UPGD method is innovative, leveraging utility-based gradient updates to dynamically manage neural network weights, which is a promising direction for improving learning adaptability in non-stationary environments.\\n3. The methodology is thoroughly evaluated on multiple datasets and tasks, including both supervised and reinforcement learning scenarios, demonstrating its versatility and effectiveness.' Weaknesses=\"1. The explanation of UPGD's mechanism, particularly the utility-based perturbations and their impact on weight updates, could be more detailed to better convey its soundness.\\n2. The experiments mainly focus on common benchmark datasets, and the applicability of UPGD to more complex, real-world datasets remains to be thoroughly demonstrated.\\n3. The reliance on Taylor series approximation for utility computation might limit scalability to larger models, and this limitation is not adequately discussed.\" Questions='1. How does UPGD perform on more complex, real-world datasets or tasks beyond the benchmark datasets used in the paper?\\n2. What considerations are needed when scaling UPGD to larger networks, particularly in terms of computational overhead caused by utility weight computations?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The paper addresses an important gap in continual learning by proposing a novel approach that simultaneously tackles catastrophic forgetting and loss of plasticity, making it highly relevant for real-world streaming learning applications.\\n2. The proposed UPGD method is innovative, leveraging utility-based gradient updates to dynamically manage neural network weights, which is a promising direction for improving learning adaptability in non-stationary environments.\\n3. The methodology is thoroughly evaluated on multiple datasets and tasks, including both supervised and reinforcement learning scenarios, demonstrating its versatility and effectiveness.' Weaknesses=\"1. The explanation of UPGD's mechanism, particularly the utility-based perturbations and their impact on weight updates, could be more detailed to better convey its soundness.\\n2. The experiments mainly focus on common benchmark datasets, and the applicability of UPGD to more complex, real-world datasets remains to be thoroughly demonstrated.\\n3. The reliance on Taylor series approximation for utility computation might limit scalability to larger models, and this limitation is not adequately discussed.\" Questions='1. How does UPGD perform on more complex, real-world datasets or tasks beyond the benchmark datasets used in the paper?\\n2. What considerations are needed when scaling UPGD to larger networks, particularly in terms of computational overhead caused by utility weight computations?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=2 Rating=6 Confidence=4 Strengths='The paper highlights a critical vulnerability in LLMs and VLLMs, specifically to permutation attacks, which is an important aspect of robustness that has not been thoroughly explored before. Investigating these models across multiple benchmarks provides a comprehensive overview of the issue.' Weaknesses='The study reveals a significant flaw but does not offer substantial solutions or mitigation strategies, leaving the reader with unanswered concerns about practical applicability. The exploration of why permutation attacks are effective lacks depth, potentially limiting the utility of these findings for robust model design.' Questions='How can the insights from this sensitivity to permutations inform the development of more robust LLMs and VLLMs? Are there specific architectural changes that could mitigate this vulnerability?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=2 Rating=6 Confidence=4 Strengths='The paper highlights a critical vulnerability in LLMs and VLLMs, specifically to permutation attacks, which is an important aspect of robustness that has not been thoroughly explored before. Investigating these models across multiple benchmarks provides a comprehensive overview of the issue.' Weaknesses='The study reveals a significant flaw but does not offer substantial solutions or mitigation strategies, leaving the reader with unanswered concerns about practical applicability. The exploration of why permutation attacks are effective lacks depth, potentially limiting the utility of these findings for robust model design.' Questions='How can the insights from this sensitivity to permutations inform the development of more robust LLMs and VLLMs? Are there specific architectural changes that could mitigate this vulnerability?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths=\"1. Introduction of a novel quantization method, SuFP, which effectively combines multi-region piecewise quantization with a tensor-wise scalable bias, providing significant improvements in efficiency and accuracy.\\n2. The method demonstrates strong experimental results, achieving superior computational capability and energy efficiency compared to existing methods while maintaining accuracy.\\n3. SuFP's hardware implementation offers practical benefits, ensuring the approach is scalable and applicable in real-world scenarios.\" Weaknesses=\"1. The paper may lack detailed comparisons with other contemporary quantization methods, such as examining trade-offs in precision and efficiency across different domains.\\n2. The evaluation could provide more insight into how the multi-region quantization impacts various types of DNN data distributions, particularly with a deeper analysis of extremely dense or sparse data.\\n3. Potential challenges in the method's adaptability to rapidly evolving and diverse architectures of DNNs might be insufficiently addressed.\" Questions=\"1. How does SuFP perform specifically on large-scale language models or emerging model architectures not covered in the current experiments?\\n2. Can the proposed method be integrated with existing quantization pipelines without significant overhead or need for retraining models?\\n3. What are the potential limitations in extending SuFP's hardware design for next-generation neural network models with more complex data distributions?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths=\"1. Introduction of a novel quantization method, SuFP, which effectively combines multi-region piecewise quantization with a tensor-wise scalable bias, providing significant improvements in efficiency and accuracy.\\n2. The method demonstrates strong experimental results, achieving superior computational capability and energy efficiency compared to existing methods while maintaining accuracy.\\n3. SuFP's hardware implementation offers practical benefits, ensuring the approach is scalable and applicable in real-world scenarios.\" Weaknesses=\"1. The paper may lack detailed comparisons with other contemporary quantization methods, such as examining trade-offs in precision and efficiency across different domains.\\n2. The evaluation could provide more insight into how the multi-region quantization impacts various types of DNN data distributions, particularly with a deeper analysis of extremely dense or sparse data.\\n3. Potential challenges in the method's adaptability to rapidly evolving and diverse architectures of DNNs might be insufficiently addressed.\" Questions=\"1. How does SuFP perform specifically on large-scale language models or emerging model architectures not covered in the current experiments?\\n2. Can the proposed method be integrated with existing quantization pipelines without significant overhead or need for retraining models?\\n3. What are the potential limitations in extending SuFP's hardware design for next-generation neural network models with more complex data distributions?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper presents a novel variance-reduced zeroth-order hard-thresholding algorithm detailed with solid theoretical analysis, addressing the practical limitations of existing methods for solving ℓ0 constrained optimization problems.\\n2. The algorithm significantly improves convergence rates and applicability, tackling the restriction on the number of random directions, which is crucial for high-dimensional sparse learning tasks.\\n3. Experiments validate the improved performance of the new method on both portfolio optimization and black-box adversarial attack tasks, showcasing its practical impact.' Weaknesses='1. While the algorithm demonstrates substantial theoretical and practical contributions, its practical impact or real-world applicability beyond the given applications might need further elaboration or examples.\\n2. The performance results focus largely on the improvements over a single existing algorithm (SZOHT), which may limit the perceived novelty or range of tested applicability.\\n3. Some minor details about the experimental setup, such as the choice of datasets or parameters, are not thoroughly discussed, leaving questions on the generalizability of the results.' Questions='1. Are there potential applications of the proposed algorithm in domains beyond portfolio optimization and adversarial attacks where the removal of zeroth-order gradient restrictions might be beneficial?\\n2. The theoretical analysis relies on certain assumptions like restricted strong smoothness and convexity; how robust is the algorithm to violations of these assumptions in complex, real-world datasets?\\n3. How sensitive is the method to the choice of parameters during execution, and is there guidance for selecting these parameters optimally?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper presents a novel variance-reduced zeroth-order hard-thresholding algorithm detailed with solid theoretical analysis, addressing the practical limitations of existing methods for solving ℓ0 constrained optimization problems.\\n2. The algorithm significantly improves convergence rates and applicability, tackling the restriction on the number of random directions, which is crucial for high-dimensional sparse learning tasks.\\n3. Experiments validate the improved performance of the new method on both portfolio optimization and black-box adversarial attack tasks, showcasing its practical impact.' Weaknesses='1. While the algorithm demonstrates substantial theoretical and practical contributions, its practical impact or real-world applicability beyond the given applications might need further elaboration or examples.\\n2. The performance results focus largely on the improvements over a single existing algorithm (SZOHT), which may limit the perceived novelty or range of tested applicability.\\n3. Some minor details about the experimental setup, such as the choice of datasets or parameters, are not thoroughly discussed, leaving questions on the generalizability of the results.' Questions='1. Are there potential applications of the proposed algorithm in domains beyond portfolio optimization and adversarial attacks where the removal of zeroth-order gradient restrictions might be beneficial?\\n2. The theoretical analysis relies on certain assumptions like restricted strong smoothness and convexity; how robust is the algorithm to violations of these assumptions in complex, real-world datasets?\\n3. How sensitive is the method to the choice of parameters during execution, and is there guidance for selecting these parameters optimally?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents an innovative method (FARZI) that effectively synthesizes compact, high-quality datasets for autoregressive tasks, demonstrating impressive performance gains with significantly reduced data size. The approach addresses a gap in data distillation for large discrete token spaces.' Weaknesses='The practical scalability of FARZI for very large datasets and models is not thoroughly explored, and the paper lacks detailed analysis on real-world time savings compared to traditional training methods.' Questions='How does FARZI scale when applied to very large language models or datasets? Could more empirical analysis on time savings and computational efficiency be provided?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents an innovative method (FARZI) that effectively synthesizes compact, high-quality datasets for autoregressive tasks, demonstrating impressive performance gains with significantly reduced data size. The approach addresses a gap in data distillation for large discrete token spaces.' Weaknesses='The practical scalability of FARZI for very large datasets and models is not thoroughly explored, and the paper lacks detailed analysis on real-world time savings compared to traditional training methods.' Questions='How does FARZI scale when applied to very large language models or datasets? Could more empirical analysis on time savings and computational efficiency be provided?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel algorithm for offline RL with non-linear functions and provides statistically optimal instance-dependent regret guarantees. The translation of concepts from linear to non-linear contexts is well-executed, and the paper contributes to the theoretical understanding of offline RL in more complex environments.' Weaknesses='The method assumes strong coverage which may limit its practical applicability, and the paper lacks empirical validations which would demonstrate real-world applicability and performance of the proposed approach. Additionally, assumptions used for theoretical results could be perceived as restrictive.' Questions='How does the algorithm perform with practical data where assumptions such as full coverage may not hold? Are there practical scenarios or specific domains where assumptions are satisfied? Could empirical evaluations help to illustrate potential performance in real-world settings?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel algorithm for offline RL with non-linear functions and provides statistically optimal instance-dependent regret guarantees. The translation of concepts from linear to non-linear contexts is well-executed, and the paper contributes to the theoretical understanding of offline RL in more complex environments.' Weaknesses='The method assumes strong coverage which may limit its practical applicability, and the paper lacks empirical validations which would demonstrate real-world applicability and performance of the proposed approach. Additionally, assumptions used for theoretical results could be perceived as restrictive.' Questions='How does the algorithm perform with practical data where assumptions such as full coverage may not hold? Are there practical scenarios or specific domains where assumptions are satisfied? Could empirical evaluations help to illustrate potential performance in real-world settings?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='The paper explores an important aspect of LLMs by examining their behavior in social interactive contexts using game theory, which could have significant implications for their use in cooperative environments. The use of behavioral prompts to simulate interactions adds a novel layer to the evaluation.' Weaknesses=\"The study's findings might be limited by the current experimental setup, which focuses on specific game types and does not extensively explore the diversity of possible LLM behaviors in varied interactive contexts. Additionally, improvements suggested for LLM behavior could benefit from more in-depth exploration and testing.\" Questions='How do the findings from the 2x2 games generalize to more complex, real-world interactions involving multiple strategies and participants? Are there plans to test LLMs in environments with a mix of competitive and cooperative elements? Can the methodology be adapted and extended to cover broader types of social interactions with different rules and structures?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='The paper explores an important aspect of LLMs by examining their behavior in social interactive contexts using game theory, which could have significant implications for their use in cooperative environments. The use of behavioral prompts to simulate interactions adds a novel layer to the evaluation.' Weaknesses=\"The study's findings might be limited by the current experimental setup, which focuses on specific game types and does not extensively explore the diversity of possible LLM behaviors in varied interactive contexts. Additionally, improvements suggested for LLM behavior could benefit from more in-depth exploration and testing.\" Questions='How do the findings from the 2x2 games generalize to more complex, real-world interactions involving multiple strategies and participants? Are there plans to test LLMs in environments with a mix of competitive and cooperative elements? Can the methodology be adapted and extended to cover broader types of social interactions with different rules and structures?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The paper effectively addresses the seldom-explored influence of data scaling on structure learning algorithms in higher dimensions and non-linear settings. It provides theoretical insights and practical evaluations with both synthetic and real-world datasets, advancing the understanding of bias introduced by least square and log-likelihood losses. The clear demonstration of scale effects under these scenarios fills a gap in existing literature.' Weaknesses=\"While the paper presents valuable theoretical proofs, there's a lack of novel methodology or substantial theoretical innovation beyond scaling insights. The primary contribution revolves around the exploration of existing loss functions in new contexts rather than proposing or developing new structure learning approaches. Additionally, applying the findings operationally might require deeper exploration, particularly in achieving scale-invariant results.\" Questions='What specific strategies can be employed to mitigate scale effects in real-world applications beyond simple normalization, and how might the insights of this study integrate with existing robustness approaches in structure learning? Are there computational cost implications associated with applying the suggested normalization techniques to various data types and dimensions?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The paper effectively addresses the seldom-explored influence of data scaling on structure learning algorithms in higher dimensions and non-linear settings. It provides theoretical insights and practical evaluations with both synthetic and real-world datasets, advancing the understanding of bias introduced by least square and log-likelihood losses. The clear demonstration of scale effects under these scenarios fills a gap in existing literature.' Weaknesses=\"While the paper presents valuable theoretical proofs, there's a lack of novel methodology or substantial theoretical innovation beyond scaling insights. The primary contribution revolves around the exploration of existing loss functions in new contexts rather than proposing or developing new structure learning approaches. Additionally, applying the findings operationally might require deeper exploration, particularly in achieving scale-invariant results.\" Questions='What specific strategies can be employed to mitigate scale effects in real-world applications beyond simple normalization, and how might the insights of this study integrate with existing robustness approaches in structure learning? Are there computational cost implications associated with applying the suggested normalization techniques to various data types and dimensions?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. Village-Net Clustering efficiently identifies and clusters complex datasets while being scalable due to its low time complexity.\\n2. The method autonomously determines the number of clusters, which is a significant and practical advantage over many traditional clustering techniques.\\n3. Demonstrates competitive performance on real-world datasets and proposes a novel way of combining K-Means with community detection algorithms.' Weaknesses='1. The accuracy of the method might be limited by the initial K-Means step, which relies on predefined parameters. \\n2. There might be challenges related to the initial hyperparameter selection, and the advantages might be limited when working with highly diverse datasets.\\n3. Current implementation seems competitive on speed and efficiency, but might require further adjustments to match accuracy benchmarks of state-of-the-art methods. ' Questions=\"1. How does Village-Net Clustering perform in terms of accuracy compared to algorithms designed specifically for non-convex data structures?\\n2. Is there a way to reduce the initial dependency on K-Means for constructing 'villages,' or improve it for better initial clusters?\\n3. Can the community detection phase be modified or optimized further to improve the accuracy of the final clustering outcome?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. Village-Net Clustering efficiently identifies and clusters complex datasets while being scalable due to its low time complexity.\\n2. The method autonomously determines the number of clusters, which is a significant and practical advantage over many traditional clustering techniques.\\n3. Demonstrates competitive performance on real-world datasets and proposes a novel way of combining K-Means with community detection algorithms.' Weaknesses='1. The accuracy of the method might be limited by the initial K-Means step, which relies on predefined parameters. \\n2. There might be challenges related to the initial hyperparameter selection, and the advantages might be limited when working with highly diverse datasets.\\n3. Current implementation seems competitive on speed and efficiency, but might require further adjustments to match accuracy benchmarks of state-of-the-art methods. ' Questions=\"1. How does Village-Net Clustering perform in terms of accuracy compared to algorithms designed specifically for non-convex data structures?\\n2. Is there a way to reduce the initial dependency on K-Means for constructing 'villages,' or improve it for better initial clusters?\\n3. Can the community detection phase be modified or optimized further to improve the accuracy of the final clustering outcome?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The integration of task-specific supervised knowledge from SLMs into LLM in-context learning presents a novel method to enhance performance on out-of-distribution tasks while mitigating issues like hallucinations. The use of a broad range of datasets to test the method adds credibility to the findings.' Weaknesses='The paper relies primarily on existing models and frameworks, limiting the originality of the contribution. The practical implementation of the SuperContext framework might also pose challenges due to resource demands. Additionally, the necessity of hand-curated datasets could limit scalability.' Questions='Is there a quantifiable measure of the computational overhead introduced by the SuperContext integration compared to baseline methods? How does the method perform when SLM predictions are incorrect or highly uncertain?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The integration of task-specific supervised knowledge from SLMs into LLM in-context learning presents a novel method to enhance performance on out-of-distribution tasks while mitigating issues like hallucinations. The use of a broad range of datasets to test the method adds credibility to the findings.' Weaknesses='The paper relies primarily on existing models and frameworks, limiting the originality of the contribution. The practical implementation of the SuperContext framework might also pose challenges due to resource demands. Additionally, the necessity of hand-curated datasets could limit scalability.' Questions='Is there a quantifiable measure of the computational overhead introduced by the SuperContext integration compared to baseline methods? How does the method perform when SLM predictions are incorrect or highly uncertain?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper provides a novel approach by combining parameter and data space perturbations to improve domain generalization, addressing a gap left by existing methods. The proposed UDIM framework shows statistically significant improvements over existing SAM variants, showcasing its potential for robust model generalization.' Weaknesses=\"The proposed method's reliance on simulated perturbations from the source domain may not fully capture the complexities of truly unknown domains. Additionally, while the theory is well-founded, practical implementation details and the selection of perturbations in data space could be further clarified for broader application.\" Questions='How does UDIM decide the extent of perturbations in the data space, and is there a risk of overfitting to perturbed simulations rather than generalizing to genuinely unseen domains? Can the approach be extended to scenarios with extremely diverse unknown target domains?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper provides a novel approach by combining parameter and data space perturbations to improve domain generalization, addressing a gap left by existing methods. The proposed UDIM framework shows statistically significant improvements over existing SAM variants, showcasing its potential for robust model generalization.' Weaknesses=\"The proposed method's reliance on simulated perturbations from the source domain may not fully capture the complexities of truly unknown domains. Additionally, while the theory is well-founded, practical implementation details and the selection of perturbations in data space could be further clarified for broader application.\" Questions='How does UDIM decide the extent of perturbations in the data space, and is there a risk of overfitting to perturbed simulations rather than generalizing to genuinely unseen domains? Can the approach be extended to scenarios with extremely diverse unknown target domains?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. Introduces a novel collaborative-competitive framework for GAN training, merging competition with cooperation for enhanced performance.\\n2. Provides both theoretical analysis and comprehensive empirical evaluation, demonstrating improvements over traditional GANs.\\n3. The proposed method is robust to training collapse, ensuring stability in data generation.' Weaknesses=\"1. The presentation of results, particularly the visual examples, is lacking in detail and clarity, making it difficult to fully appreciate the improvements claimed.\\n2. There is a lack of direct comparison with state-of-the-art GAN methods in terms of standard metrics such as FID or IS, which is a common practice in GAN evaluation.\\n3. The method's compatibility with existing advanced GAN architectures (like StyleGAN) is not discussed or evaluated.\" Questions='1. How does CCGAN perform in terms of standard evaluation metrics (e.g., FID, IS) compared to state-of-the-art GAN models?\\n2. Can CCGAN be integrated with or improved by more complex GAN architectures such as StyleGAN?\\n3. What are the computational costs associated with the proposed collaborative-competitive framework compared to traditional GANs?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. Introduces a novel collaborative-competitive framework for GAN training, merging competition with cooperation for enhanced performance.\\n2. Provides both theoretical analysis and comprehensive empirical evaluation, demonstrating improvements over traditional GANs.\\n3. The proposed method is robust to training collapse, ensuring stability in data generation.' Weaknesses=\"1. The presentation of results, particularly the visual examples, is lacking in detail and clarity, making it difficult to fully appreciate the improvements claimed.\\n2. There is a lack of direct comparison with state-of-the-art GAN methods in terms of standard metrics such as FID or IS, which is a common practice in GAN evaluation.\\n3. The method's compatibility with existing advanced GAN architectures (like StyleGAN) is not discussed or evaluated.\" Questions='1. How does CCGAN perform in terms of standard evaluation metrics (e.g., FID, IS) compared to state-of-the-art GAN models?\\n2. Can CCGAN be integrated with or improved by more complex GAN architectures such as StyleGAN?\\n3. What are the computational costs associated with the proposed collaborative-competitive framework compared to traditional GANs?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper introduces a novel integration of iterative and parallel computation, which potentially enhances the efficiency and generalization of visual reasoning models.\\n2. The proposed IPRM model demonstrates superior performance on several benchmarks, showcasing its practical impact in visual reasoning tasks.\\n3. The combination of iterative and parallel processing strategies is well-motivated to address the limitations of existing purely iterative or parallel models.' Weaknesses=\"1. The paper could benefit from a clearer explanation of how parallel reasoning is distinctly implemented within the model architecture. The interpretation of 'parallel' operations, in this case, seems somewhat implicit from matrix operations, and more explicit illustrations or explanations are needed.\\n2. The design choices, especially for Operation Composition, seem complex and are not entirely intuitive from the description provided. Better elucidation in this area would enhance understanding.\\n3. Some visualizations in the supplement, while potentially indicating interesting result aspects, lack clarity in demonstrating the model's parallel and iterative reasoning processes.\" Questions=\"1. How is the 'identity attention' mechanism enabling higher degrees of operation composition, and how does this relate to traditional attention mechanisms?\\n2. Could additional visualizations or examples showing the step-by-step operations of IPRM be included to illustrate the iterative and parallel processing more effectively?\\n3. Can you provide further detail on how the model's parallel processing component explicitly contributes to the performance gains observed in different reasoning benchmark tasks?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper introduces a novel integration of iterative and parallel computation, which potentially enhances the efficiency and generalization of visual reasoning models.\\n2. The proposed IPRM model demonstrates superior performance on several benchmarks, showcasing its practical impact in visual reasoning tasks.\\n3. The combination of iterative and parallel processing strategies is well-motivated to address the limitations of existing purely iterative or parallel models.' Weaknesses=\"1. The paper could benefit from a clearer explanation of how parallel reasoning is distinctly implemented within the model architecture. The interpretation of 'parallel' operations, in this case, seems somewhat implicit from matrix operations, and more explicit illustrations or explanations are needed.\\n2. The design choices, especially for Operation Composition, seem complex and are not entirely intuitive from the description provided. Better elucidation in this area would enhance understanding.\\n3. Some visualizations in the supplement, while potentially indicating interesting result aspects, lack clarity in demonstrating the model's parallel and iterative reasoning processes.\" Questions=\"1. How is the 'identity attention' mechanism enabling higher degrees of operation composition, and how does this relate to traditional attention mechanisms?\\n2. Could additional visualizations or examples showing the step-by-step operations of IPRM be included to illustrate the iterative and parallel processing more effectively?\\n3. Can you provide further detail on how the model's parallel processing component explicitly contributes to the performance gains observed in different reasoning benchmark tasks?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper effectively applies pre-training paradigms from other domains to epidemic forecasting, a novel application area, demonstrating significant performance improvements.\\n2. The combination of multiple self-supervised learning tasks (RANDMASK, LASTMASK, etc.) presents a robust methodology for extracting useful data representations from heterogeneous datasets.\\n3. The evaluation shows a comprehensive improvement over state-of-the-art models in forecasting accuracy and adaptability to new epidemics.' Weaknesses='1. The paper may lack detailed explanations on how specific methodological choices like parameter settings for learning tasks (e.g., gamma for LASTMASK) were determined.\\n2. The experimental setup may not clearly establish how the baselines were trained and if resource discrepancies (like data and computational power) were normalized across comparisons.\\n3. There might be a potential issue with data bleeding between temporally aligned datasets, such as influenza and cryptosporidiosis, which could affect the reported improvements in data efficiency.' Questions='1. How do you choose the value of T (length of the input time series) in your experiments, and how sensitive is the model performance to this parameter?\\n2. Could you clarify how the training process is normalized when comparing PEM with baseline models? Specifically, how do you ensure that additional data or compute does not skew the comparisons?\\n3. Can PEM be potentially applied to more non-traditional epidemic data types, or is it specifically designed for datasets similar to those used in the evaluation?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper effectively applies pre-training paradigms from other domains to epidemic forecasting, a novel application area, demonstrating significant performance improvements.\\n2. The combination of multiple self-supervised learning tasks (RANDMASK, LASTMASK, etc.) presents a robust methodology for extracting useful data representations from heterogeneous datasets.\\n3. The evaluation shows a comprehensive improvement over state-of-the-art models in forecasting accuracy and adaptability to new epidemics.' Weaknesses='1. The paper may lack detailed explanations on how specific methodological choices like parameter settings for learning tasks (e.g., gamma for LASTMASK) were determined.\\n2. The experimental setup may not clearly establish how the baselines were trained and if resource discrepancies (like data and computational power) were normalized across comparisons.\\n3. There might be a potential issue with data bleeding between temporally aligned datasets, such as influenza and cryptosporidiosis, which could affect the reported improvements in data efficiency.' Questions='1. How do you choose the value of T (length of the input time series) in your experiments, and how sensitive is the model performance to this parameter?\\n2. Could you clarify how the training process is normalized when comparing PEM with baseline models? Specifically, how do you ensure that additional data or compute does not skew the comparisons?\\n3. Can PEM be potentially applied to more non-traditional epidemic data types, or is it specifically designed for datasets similar to those used in the evaluation?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper introduces a novel post-processing technique for managing the trade-off between robust and average accuracies, which does not require additional training.\\n2. The proposed class-specific scaling strategy is simple yet effective, demonstrating improvements across various datasets.\\n3. A new evaluation metric, robust coverage, is presented to capture the trade-off and provides a comprehensive evaluation of debiasing algorithms.' Weaknesses='1. The method relies on pre-trained models and thus may have limitations if those models have inherent biases or incorrect calibrations.\\n2. The paper does not deeply explore the theoretical underpinnings of why class-specific scaling should work well in all cases.\\n3. The simplistic nature of the post-processing method might limit its scalability or applicability to more complex, non-linear trade-offs.' Questions='1. How does the proposed method perform compared to existing state-of-the-art techniques in terms of computational efficiency?\\n2. Can the class-specific scaling technique be adapted to handle other types of biases or is it specifically tailored for robust vs. average accuracy trade-offs? \\n3. Has the robustness of the proposed method been tested across a range of different model architectures?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper introduces a novel post-processing technique for managing the trade-off between robust and average accuracies, which does not require additional training.\\n2. The proposed class-specific scaling strategy is simple yet effective, demonstrating improvements across various datasets.\\n3. A new evaluation metric, robust coverage, is presented to capture the trade-off and provides a comprehensive evaluation of debiasing algorithms.' Weaknesses='1. The method relies on pre-trained models and thus may have limitations if those models have inherent biases or incorrect calibrations.\\n2. The paper does not deeply explore the theoretical underpinnings of why class-specific scaling should work well in all cases.\\n3. The simplistic nature of the post-processing method might limit its scalability or applicability to more complex, non-linear trade-offs.' Questions='1. How does the proposed method perform compared to existing state-of-the-art techniques in terms of computational efficiency?\\n2. Can the class-specific scaling technique be adapted to handle other types of biases or is it specifically tailored for robust vs. average accuracy trade-offs? \\n3. Has the robustness of the proposed method been tested across a range of different model architectures?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses an important challenge in disentangled representation learning for time series data by introducing a novel approach that relaxes the assumption of attribute independence. The proposed DIOSC method, which integrates contrastive learning with attentive l-variational inference, appears to enhance disentanglement and improve robustness in scenarios involving correlated attributes. The introduction of the Time Disentangling Score (TDS) as a metric also contributes to a more precise evaluation of disentanglement quality and performance.' Weaknesses=\"The method's general applicability to domains beyond energy consumption datasets like NILM is not extensively demonstrated, which might limit the perceived versatility of DIOSC. Furthermore, while the paper claims significant improvements over existing methods, more explicit examples or case studies detailing performance advantages and scenario applicability would strengthen the argument. The necessity of intricate setup for l-variational inference with self-attention might pose a practical hurdle.\" Questions=\"1. How does the DIOSC method's performance vary across different types of time series datasets beyond energy consumption?\\n2. Can the Time Disentangling Score (TDS) be applied effectively for evaluating other disentanglement approaches that do not employ independence-of-support?\"\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses an important challenge in disentangled representation learning for time series data by introducing a novel approach that relaxes the assumption of attribute independence. The proposed DIOSC method, which integrates contrastive learning with attentive l-variational inference, appears to enhance disentanglement and improve robustness in scenarios involving correlated attributes. The introduction of the Time Disentangling Score (TDS) as a metric also contributes to a more precise evaluation of disentanglement quality and performance.' Weaknesses=\"The method's general applicability to domains beyond energy consumption datasets like NILM is not extensively demonstrated, which might limit the perceived versatility of DIOSC. Furthermore, while the paper claims significant improvements over existing methods, more explicit examples or case studies detailing performance advantages and scenario applicability would strengthen the argument. The necessity of intricate setup for l-variational inference with self-attention might pose a practical hurdle.\" Questions=\"1. How does the DIOSC method's performance vary across different types of time series datasets beyond energy consumption?\\n2. Can the Time Disentangling Score (TDS) be applied effectively for evaluating other disentanglement approaches that do not employ independence-of-support?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='The Qwen-VL series offers significant upgrades to LVLMs by achieving remarkable improvements in fine-grained visual understanding and multilingual support. The authors provide extensive evaluations across various real-world scenarios, which enhances the credibility of the results and establishes new benchmarks. Additionally, the comprehensive training on diverse datasets presents a robust framework highly applicable for vision-language tasks.' Weaknesses='While the Qwen-VL series shows impressive performance gains, the novelty in terms of the architectural innovation may be somewhat limited, as it builds upon and slightly refines existing methods (e.g., Vision Transformers and adapters). The effectiveness spurts could also be heavily attributed to large-scale training rather than fundamental methodological innovations. Further, it is not entirely clear how scalable the approach is with regards to incorporating additional modalities beyond text and visual data.' Questions=\"How does the Qwen-VL architecture quantitatively compare to state-of-the-art proprietary models across fine-grained tasks? Are there any quantitative benchmarks showing the performance variation specifically introduced by each stage of the training pipeline? Additionally, how does the model's size and computational requirements compare to other models with similar capabilities?\"\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='The Qwen-VL series offers significant upgrades to LVLMs by achieving remarkable improvements in fine-grained visual understanding and multilingual support. The authors provide extensive evaluations across various real-world scenarios, which enhances the credibility of the results and establishes new benchmarks. Additionally, the comprehensive training on diverse datasets presents a robust framework highly applicable for vision-language tasks.' Weaknesses='While the Qwen-VL series shows impressive performance gains, the novelty in terms of the architectural innovation may be somewhat limited, as it builds upon and slightly refines existing methods (e.g., Vision Transformers and adapters). The effectiveness spurts could also be heavily attributed to large-scale training rather than fundamental methodological innovations. Further, it is not entirely clear how scalable the approach is with regards to incorporating additional modalities beyond text and visual data.' Questions=\"How does the Qwen-VL architecture quantitatively compare to state-of-the-art proprietary models across fine-grained tasks? Are there any quantitative benchmarks showing the performance variation specifically introduced by each stage of the training pipeline? Additionally, how does the model's size and computational requirements compare to other models with similar capabilities?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper presents an innovative approach to adapting dataset distillation to reinforcement learning, addressing the lack of fixed datasets in RL by introducing a novel method, HaDES, which enables behavior distillation without prior expert data.\\n2. The use of a meta-evolutionary approach combined with bi-level optimization for distilling RL information is sophisticated and effective, as demonstrated across a wide range of tasks and architectures.\\n3. The methodology demonstrates strong generalization capabilities, showing that the distilled datasets can be effectively used across different player architectures and in multi-task learning scenarios.' Weaknesses='1. The paper could further elaborate on the potential limitations of the proposed method, particularly regarding the scalability of the approach to more complex RL environments or tasks beyond those demonstrated in the experiments.\\n2. While the study is extensive, it might benefit from a more detailed comparison with existing RL methods beyond traditional dataset distillation techniques to highlight where HaDES stands in relation to current state-of-the-art.\\n3. The study addresses the adaptation of dataset distillation in RL, but it might not thoroughly discuss the broader implications and potential real-world applications where this adaptation could prove especially beneficial.' Questions='1. How does the performance of HaDES compare with traditional RL methods across more complex environments, and are there any specific challenges noted when scaling the approach?\\n2. Can the synthetic datasets generated by HaDES be interpreted or visualized in a way that provides additional insights into the learned RL policies, potentially enhancing explainability or trust in the distilled policies?\\n3. How does HaDES perform in scenarios where continuous updates to the environment or task objectives occur—can the distilled datasets rapidly adapt in such dynamic settings?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper presents an innovative approach to adapting dataset distillation to reinforcement learning, addressing the lack of fixed datasets in RL by introducing a novel method, HaDES, which enables behavior distillation without prior expert data.\\n2. The use of a meta-evolutionary approach combined with bi-level optimization for distilling RL information is sophisticated and effective, as demonstrated across a wide range of tasks and architectures.\\n3. The methodology demonstrates strong generalization capabilities, showing that the distilled datasets can be effectively used across different player architectures and in multi-task learning scenarios.' Weaknesses='1. The paper could further elaborate on the potential limitations of the proposed method, particularly regarding the scalability of the approach to more complex RL environments or tasks beyond those demonstrated in the experiments.\\n2. While the study is extensive, it might benefit from a more detailed comparison with existing RL methods beyond traditional dataset distillation techniques to highlight where HaDES stands in relation to current state-of-the-art.\\n3. The study addresses the adaptation of dataset distillation in RL, but it might not thoroughly discuss the broader implications and potential real-world applications where this adaptation could prove especially beneficial.' Questions='1. How does the performance of HaDES compare with traditional RL methods across more complex environments, and are there any specific challenges noted when scaling the approach?\\n2. Can the synthetic datasets generated by HaDES be interpreted or visualized in a way that provides additional insights into the learned RL policies, potentially enhancing explainability or trust in the distilled policies?\\n3. How does HaDES perform in scenarios where continuous updates to the environment or task objectives occur—can the distilled datasets rapidly adapt in such dynamic settings?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces an innovative extended textual conditioning space, P+, which provides greater control and expressiveness in text-to-image synthesis. This method significantly enhances the capabilities of existing models by using layer-specific textual tokens, improving image generation customization. Furthermore, the introduction of Extended Textual Inversion (XTI) provides notable improvements in reconstruction quality and editability compared to existing methods like Textual Inversion and DreamBooth.' Weaknesses='The methodology primarily focuses on empirical results without providing detailed theoretical insights into the underlying mechanisms. The extended conditioning approach assumes access to fine-grained textual tokens and may require more in-depth investigation on the scalability and applicability to other models. Additionally, while the results are promising, the effectiveness of this approach in varied and broader contexts remains to be extensively validated.' Questions='How does the proposed P+ conditioning space handle scenarios with ambiguous or conflicting textual prompts across different layers? Is there a mechanism in place to ensure coherence in the final image output? Additionally, can the model adapt to dynamic prompts that change over time or are generated interactively during image synthesis?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces an innovative extended textual conditioning space, P+, which provides greater control and expressiveness in text-to-image synthesis. This method significantly enhances the capabilities of existing models by using layer-specific textual tokens, improving image generation customization. Furthermore, the introduction of Extended Textual Inversion (XTI) provides notable improvements in reconstruction quality and editability compared to existing methods like Textual Inversion and DreamBooth.' Weaknesses='The methodology primarily focuses on empirical results without providing detailed theoretical insights into the underlying mechanisms. The extended conditioning approach assumes access to fine-grained textual tokens and may require more in-depth investigation on the scalability and applicability to other models. Additionally, while the results are promising, the effectiveness of this approach in varied and broader contexts remains to be extensively validated.' Questions='How does the proposed P+ conditioning space handle scenarios with ambiguous or conflicting textual prompts across different layers? Is there a mechanism in place to ensure coherence in the final image output? Additionally, can the model adapt to dynamic prompts that change over time or are generated interactively during image synthesis?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper presents a novel approach to Source-Free Unsupervised Domain Adaptation by integrating hypothesis consolidation with semi-supervised learning. The method transforms domain adaptation challenges into semi-supervised learning tasks effectively. State-of-the-art performance achieved on SFUDA tasks demonstrates the method's practical applicability. The three-step method effectively consolidates hypotheses for reliable pseudo-label generation, enhancing training outcomes.\" Weaknesses='The approach relies heavily on the accuracy of initial prediction hypotheses, which might be sensitive to initial conditions. The method requires the construction of class-wise rationale centroids, which may introduce complexity in implementation and computational overhead. The dependence on existing semi-supervised learning frameworks might limit the versatility of the approach in different contexts.' Questions=\"How do different sizes or diversities of the initial hypothesis set affect the method's performance? Are there any limitations or challenges in constructing rationale representations using GradCAM in different domain settings?\"\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper presents a novel approach to Source-Free Unsupervised Domain Adaptation by integrating hypothesis consolidation with semi-supervised learning. The method transforms domain adaptation challenges into semi-supervised learning tasks effectively. State-of-the-art performance achieved on SFUDA tasks demonstrates the method's practical applicability. The three-step method effectively consolidates hypotheses for reliable pseudo-label generation, enhancing training outcomes.\" Weaknesses='The approach relies heavily on the accuracy of initial prediction hypotheses, which might be sensitive to initial conditions. The method requires the construction of class-wise rationale centroids, which may introduce complexity in implementation and computational overhead. The dependence on existing semi-supervised learning frameworks might limit the versatility of the approach in different contexts.' Questions=\"How do different sizes or diversities of the initial hypothesis set affect the method's performance? Are there any limitations or challenges in constructing rationale representations using GradCAM in different domain settings?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel approach to interpreting structured output models by taking into account correlations among output variables, which is an important advancement in the field. The SOInter method demonstrated competitive performance compared to existing interpretation techniques, and its scalability makes it applicable for large-scale problems. The integration of structural information during training results in improved explanation quality, particularly when output variable correlations are significant.' Weaknesses='The method may require careful tuning of the energy-based model and the Gumbel-Softmax trick, which could complicate its applicability in some cases. The exposition of the methodology could be clearer, particularly regarding the training phases and the specifics of the loss function used. Additionally, the paper could further detail the evaluation metrics and their significance in the context of interpretability.' Questions='How robust is the method to variations in the choice of target output variables during training? Can the approach be generalized to other model architectures beyond those explicitly examined in the paper? How do changes in hyperparameters, such as the number of important features selected, impact the quality and robustness of the interpretations?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel approach to interpreting structured output models by taking into account correlations among output variables, which is an important advancement in the field. The SOInter method demonstrated competitive performance compared to existing interpretation techniques, and its scalability makes it applicable for large-scale problems. The integration of structural information during training results in improved explanation quality, particularly when output variable correlations are significant.' Weaknesses='The method may require careful tuning of the energy-based model and the Gumbel-Softmax trick, which could complicate its applicability in some cases. The exposition of the methodology could be clearer, particularly regarding the training phases and the specifics of the loss function used. Additionally, the paper could further detail the evaluation metrics and their significance in the context of interpretability.' Questions='How robust is the method to variations in the choice of target output variables during training? Can the approach be generalized to other model architectures beyond those explicitly examined in the paper? How do changes in hyperparameters, such as the number of important features selected, impact the quality and robustness of the interpretations?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper challenges the prevailing trend of increasingly complex GNN architectures, highlighting that simpler, traditional methods can achieve competitive performance on SSNC tasks.\\n2. By utilizing classical nonparametric techniques in the spectral domain, the study opens avenues for more efficient and interpretable models, promoting cleaner theoretical analysis.\\n3. The narrative regarding the impact of changing evaluation standards provides valuable insights into model benchmarking and evaluation practices in graph-based learning.' Weaknesses='1. The approach may lack novelty as it leverages traditional methods within a new context, but without significantly advancing theoretical or methodological understanding.\\n2. While claiming competitive performance, the paper relies predominantly on benchmark datasets that may not fully capture the potential nuances or challenges posed to GNNs in real-world scenarios.\\n3. The work may require extended empirical validation across diverse datasets and more discussion on the trade-offs between model simplicity and expressive power.' Questions='1. How do the simple spectral methods perform when the graph structure presents significant noise or irregularities, and would the method require adaptation for such scenarios?\\n2. Considering the cleaner theoretical analysis enabled by your method, what specific insights can be drawn about the typical pitfalls or inefficiencies in larger, more complex GNNs?\\n3. Are there any specific limitations identified when applying these nonparametric approaches in practice, particularly concerning scalability or adapting to various types of graph data?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper challenges the prevailing trend of increasingly complex GNN architectures, highlighting that simpler, traditional methods can achieve competitive performance on SSNC tasks.\\n2. By utilizing classical nonparametric techniques in the spectral domain, the study opens avenues for more efficient and interpretable models, promoting cleaner theoretical analysis.\\n3. The narrative regarding the impact of changing evaluation standards provides valuable insights into model benchmarking and evaluation practices in graph-based learning.' Weaknesses='1. The approach may lack novelty as it leverages traditional methods within a new context, but without significantly advancing theoretical or methodological understanding.\\n2. While claiming competitive performance, the paper relies predominantly on benchmark datasets that may not fully capture the potential nuances or challenges posed to GNNs in real-world scenarios.\\n3. The work may require extended empirical validation across diverse datasets and more discussion on the trade-offs between model simplicity and expressive power.' Questions='1. How do the simple spectral methods perform when the graph structure presents significant noise or irregularities, and would the method require adaptation for such scenarios?\\n2. Considering the cleaner theoretical analysis enabled by your method, what specific insights can be drawn about the typical pitfalls or inefficiencies in larger, more complex GNNs?\\n3. Are there any specific limitations identified when applying these nonparametric approaches in practice, particularly concerning scalability or adapting to various types of graph data?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses the significant issue of gradient divergence in Robust Fine-Tuning and proposes a novel method, AutoLoRa, which effectively separates natural and adversarial optimization objectives. The method is validated through extensive experiments across various datasets, achieving state-of-the-art adversarial robustness. Additionally, it alleviates the sensitivity to hyperparameters, making it a practical solution.' Weaknesses='While the paper presents a promising approach, the explanation of the automated scheduling for hyperparameters could be more detailed. Further, the introduction of the low-rank branch is intriguing but not entirely novel. The clarity of certain experimental results, such as those in Table 4, could be improved to avoid confusion. The paper also lacks diversity in tested network architectures, using primarily ResNet models.' Questions='How does AutoLoRa perform on architectures other than ResNet? Can the heuristic scheduling strategies for learning rates be applied to other domains or models effectively? What would be the implications of varying the constant factor in Formula 7?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses the significant issue of gradient divergence in Robust Fine-Tuning and proposes a novel method, AutoLoRa, which effectively separates natural and adversarial optimization objectives. The method is validated through extensive experiments across various datasets, achieving state-of-the-art adversarial robustness. Additionally, it alleviates the sensitivity to hyperparameters, making it a practical solution.' Weaknesses='While the paper presents a promising approach, the explanation of the automated scheduling for hyperparameters could be more detailed. Further, the introduction of the low-rank branch is intriguing but not entirely novel. The clarity of certain experimental results, such as those in Table 4, could be improved to avoid confusion. The paper also lacks diversity in tested network architectures, using primarily ResNet models.' Questions='How does AutoLoRa perform on architectures other than ResNet? Can the heuristic scheduling strategies for learning rates be applied to other domains or models effectively? What would be the implications of varying the constant factor in Formula 7?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The paper addresses key limitations of existing Graph Lottery Ticket methods by proposing a novel framework that integrates adaptive, dynamic, and automated features, effectively demonstrating enhancements in both performance and applicability in complex GNN scenarios.\\n2. Theoretical proofs provided ensure that AdaGLT can mitigate over-smoothing and improve sparse structure handling in deeper GNNs, adding a strong theoretical underpinning to the proposed method.\\n3. Empirical results across multiple datasets showcase AdaGLT's superiority in scalability, automation, and efficiency in uncovering sparse subnetworks within GNNs, highlighting its practical significance.\" Weaknesses='1. While the paper proposes an innovative approach, it might appear incremental given the existing research on Graph Lottery Tickets, especially with works focusing on sparsity and pruning techniques in GNNs.\\n2. The experiments, though extensive, are primarily focused on existing datasets, potentially limiting a comprehensive evaluation in significantly larger scale or real-world dynamic graph settings.\\n3. Details on the computational overhead or resource utilization in dynamic pruning and restoration during training are not explicitly discussed, which might be crucial for practitioners considering scalable implementations.' Questions='1. How does the AdaGLT framework perform in real-world dynamic or evolving graph scenarios outside of the conventional benchmarks?\\n2. Can the AdaGLT method be extended to incorporate other types of neural networks beyond GNNs, or is the methodology particularly tailored for graph-based architectures?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The paper addresses key limitations of existing Graph Lottery Ticket methods by proposing a novel framework that integrates adaptive, dynamic, and automated features, effectively demonstrating enhancements in both performance and applicability in complex GNN scenarios.\\n2. Theoretical proofs provided ensure that AdaGLT can mitigate over-smoothing and improve sparse structure handling in deeper GNNs, adding a strong theoretical underpinning to the proposed method.\\n3. Empirical results across multiple datasets showcase AdaGLT's superiority in scalability, automation, and efficiency in uncovering sparse subnetworks within GNNs, highlighting its practical significance.\" Weaknesses='1. While the paper proposes an innovative approach, it might appear incremental given the existing research on Graph Lottery Tickets, especially with works focusing on sparsity and pruning techniques in GNNs.\\n2. The experiments, though extensive, are primarily focused on existing datasets, potentially limiting a comprehensive evaluation in significantly larger scale or real-world dynamic graph settings.\\n3. Details on the computational overhead or resource utilization in dynamic pruning and restoration during training are not explicitly discussed, which might be crucial for practitioners considering scalable implementations.' Questions='1. How does the AdaGLT framework perform in real-world dynamic or evolving graph scenarios outside of the conventional benchmarks?\\n2. Can the AdaGLT method be extended to incorporate other types of neural networks beyond GNNs, or is the methodology particularly tailored for graph-based architectures?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper introduces a significantly improved version of an existing open-source model, making it a competitive alternative to state-of-the-art black-box models. \\n2. The model enhancements maintain transparency and reproducibility, which are valuable for the research community.\\n3. The proposed model demonstrates notable improvements in image quality and adherence to text prompts.' Weaknesses='1. The paper does not provide extensive details on the specific limitations or failure cases of the new conditioning schemes.\\n2. There is limited discussion on potential challenges or trade-offs involved in training with the larger architecture.' Questions='1. How does the training time of SDXL compare to previous versions, and what resources are required for effective training?\\n2. What specific areas in the model architecture contribute most to the improvements in image quality and adherence to text prompts?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper introduces a significantly improved version of an existing open-source model, making it a competitive alternative to state-of-the-art black-box models. \\n2. The model enhancements maintain transparency and reproducibility, which are valuable for the research community.\\n3. The proposed model demonstrates notable improvements in image quality and adherence to text prompts.' Weaknesses='1. The paper does not provide extensive details on the specific limitations or failure cases of the new conditioning schemes.\\n2. There is limited discussion on potential challenges or trade-offs involved in training with the larger architecture.' Questions='1. How does the training time of SDXL compare to previous versions, and what resources are required for effective training?\\n2. What specific areas in the model architecture contribute most to the improvements in image quality and adherence to text prompts?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper introduces a novel and efficient technique, Activation Prompts (AP), which improves upon Visual Prompts by utilizing intermediate activation layers.\\n2. The approach is theoretically connected to normalization tuning, providing deeper insights into the operational mechanics of pre-trained models.\\n3. Extensive empirical evaluation across 29 datasets and various architectures convincingly demonstrates the advantage of AP over existing PEFT techniques in terms of accuracy and efficiency.' Weaknesses='1. While the paper introduces a new method, it largely builds upon existing concepts such as Visual Prompts and PEFT, and the novelty may be seen as incremental.\\n2. The performance of AP is highly dependent on the choice of layer depth for application, which may require significant fine-tuning and domain expertise to optimize.\\n3. The paper could provide more in-depth analysis on the potential limitations or failure cases of AP in certain scenarios or datasets.' Questions='1. How sensitive is the performance of AP to the specific choice of layer depth for application? Are there practical heuristics or guidelines for selecting them?\\n2. Can the proposed AP method be generalized to modalities beyond vision, or is it inherently tied to visual data?\\n3. What are the potential challenges in deploying AP in real-world applications where computational resources might be limited?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper introduces a novel and efficient technique, Activation Prompts (AP), which improves upon Visual Prompts by utilizing intermediate activation layers.\\n2. The approach is theoretically connected to normalization tuning, providing deeper insights into the operational mechanics of pre-trained models.\\n3. Extensive empirical evaluation across 29 datasets and various architectures convincingly demonstrates the advantage of AP over existing PEFT techniques in terms of accuracy and efficiency.' Weaknesses='1. While the paper introduces a new method, it largely builds upon existing concepts such as Visual Prompts and PEFT, and the novelty may be seen as incremental.\\n2. The performance of AP is highly dependent on the choice of layer depth for application, which may require significant fine-tuning and domain expertise to optimize.\\n3. The paper could provide more in-depth analysis on the potential limitations or failure cases of AP in certain scenarios or datasets.' Questions='1. How sensitive is the performance of AP to the specific choice of layer depth for application? Are there practical heuristics or guidelines for selecting them?\\n2. Can the proposed AP method be generalized to modalities beyond vision, or is it inherently tied to visual data?\\n3. What are the potential challenges in deploying AP in real-world applications where computational resources might be limited?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths=\"The paper presents a novel approach to overcoming catastrophic forgetting in machine learning by drawing parallels with biological neural architectures, specifically the entorhinal-hippocampal circuit. The proposed model, MESH, effectively combines biologically inspired memory systems with modern neural network architectures to achieve both rapid learning and memory retention, which is a significant contribution. The paper's presentation is clear, with a logical structure and detailed descriptions of the methods and experiments. The proposed sequential Morris Water Maze (sWM) task provides a useful benchmark for evaluating continual learning models.\" Weaknesses=\"While the approach is inspired by biological systems, the practical implementation may rely on assumptions that deviate from actual biological processes. The evaluation primarily focuses on a task that the model is explicitly designed to excel at, which may limit the generalizability of the results to other domains or tasks. Additionally, further comparisons with state-of-the-art methods in continual learning are needed to validate the model's effectiveness more comprehensively.\" Questions='How does the proposed model generalize to tasks beyond the sequential Morris Water Maze, particularly those that may not have a direct biological analog?\\nWhat are the specific limitations of the MESH model in terms of scalability and computational resources?\\nCan the content-addressable memory system be adapted for other types of inputs, such as non-image data, while retaining its effectiveness?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths=\"The paper presents a novel approach to overcoming catastrophic forgetting in machine learning by drawing parallels with biological neural architectures, specifically the entorhinal-hippocampal circuit. The proposed model, MESH, effectively combines biologically inspired memory systems with modern neural network architectures to achieve both rapid learning and memory retention, which is a significant contribution. The paper's presentation is clear, with a logical structure and detailed descriptions of the methods and experiments. The proposed sequential Morris Water Maze (sWM) task provides a useful benchmark for evaluating continual learning models.\" Weaknesses=\"While the approach is inspired by biological systems, the practical implementation may rely on assumptions that deviate from actual biological processes. The evaluation primarily focuses on a task that the model is explicitly designed to excel at, which may limit the generalizability of the results to other domains or tasks. Additionally, further comparisons with state-of-the-art methods in continual learning are needed to validate the model's effectiveness more comprehensively.\" Questions='How does the proposed model generalize to tasks beyond the sequential Morris Water Maze, particularly those that may not have a direct biological analog?\\nWhat are the specific limitations of the MESH model in terms of scalability and computational resources?\\nCan the content-addressable memory system be adapted for other types of inputs, such as non-image data, while retaining its effectiveness?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses a significant gap in the literature by focusing on the problem of OOD detection in imbalanced datasets, which is an important real-world issue.\\n2. The proposed method, ImOOD, demonstrates a significant improvement in OOD detection on imbalanced datasets, showcasing robustness and effectiveness across multiple benchmarks.\\n3. The combination of post-hoc normalization and training-time regularization is innovative and contributes to reducing the errors associated with class-aware biases.' Weaknesses=\"1. The paper may not fully address all possible scenarios of imbalanced data since it primarily focuses on imbalanced ID data, and it would be beneficial to study its efficacy in more diverse settings, including imbalanced OOD scenarios.\\n2. There is a potential lack of clarity on how well post-hoc normalization alone contributes to the improvements without the training-time regularization, especially on larger datasets where improvement might be marginal.\\n3. The method's applicability beyond the tested benchmarks could be further explored to verify its generalizability.\" Questions=\"1. How does ImOOD handle situations where both ID and OOD datasets are imbalanced? Is the method's performance robust in such scenarios?\\n2. Can the method be extended or adapted to deal with other types of imbalanced settings, such as imbalanced OOD data during training or evaluation?\\n3. How sensitive is the proposed framework to the choice of thresholds and hyperparameters during the post-hoc normalization and training-time regularization processes?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses a significant gap in the literature by focusing on the problem of OOD detection in imbalanced datasets, which is an important real-world issue.\\n2. The proposed method, ImOOD, demonstrates a significant improvement in OOD detection on imbalanced datasets, showcasing robustness and effectiveness across multiple benchmarks.\\n3. The combination of post-hoc normalization and training-time regularization is innovative and contributes to reducing the errors associated with class-aware biases.' Weaknesses=\"1. The paper may not fully address all possible scenarios of imbalanced data since it primarily focuses on imbalanced ID data, and it would be beneficial to study its efficacy in more diverse settings, including imbalanced OOD scenarios.\\n2. There is a potential lack of clarity on how well post-hoc normalization alone contributes to the improvements without the training-time regularization, especially on larger datasets where improvement might be marginal.\\n3. The method's applicability beyond the tested benchmarks could be further explored to verify its generalizability.\" Questions=\"1. How does ImOOD handle situations where both ID and OOD datasets are imbalanced? Is the method's performance robust in such scenarios?\\n2. Can the method be extended or adapted to deal with other types of imbalanced settings, such as imbalanced OOD data during training or evaluation?\\n3. How sensitive is the proposed framework to the choice of thresholds and hyperparameters during the post-hoc normalization and training-time regularization processes?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='1. The paper addresses a critical security threat in ASR systems by introducing a novel method to detect adversarial attacks, thereby providing significant practical utility.\\n2. The approach leverages statistical characteristics of output probability distributions, exploiting properties of ASR system outputs that are often overlooked in other methods.\\n3. Comprehensive empirical validation across diverse datasets and ASR systems demonstrates the effectiveness of the method, achieving high accuracy and robustness to adaptive attacks.' Weaknesses=\"1. The approach may require more detailed investigation into its limitations, especially regarding computational efficiency and potential weaknesses against more advanced adaptive attacks.\\n2. The reliance on statistical characteristics might limit the method's applicability to certain types of adversarial examples, potentially missing those crafted in ways that do not alter distributions in a detectable manner.\" Questions='1. How does the proposed method handle adversarial examples crafted to manipulate the statistical characteristics used for detection, such as entropy or divergence measures?\\n2. Could this method be adapted or extended to other modalities beyond audio, or is it closely tied to the properties of ASR outputs?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='1. The paper addresses a critical security threat in ASR systems by introducing a novel method to detect adversarial attacks, thereby providing significant practical utility.\\n2. The approach leverages statistical characteristics of output probability distributions, exploiting properties of ASR system outputs that are often overlooked in other methods.\\n3. Comprehensive empirical validation across diverse datasets and ASR systems demonstrates the effectiveness of the method, achieving high accuracy and robustness to adaptive attacks.' Weaknesses=\"1. The approach may require more detailed investigation into its limitations, especially regarding computational efficiency and potential weaknesses against more advanced adaptive attacks.\\n2. The reliance on statistical characteristics might limit the method's applicability to certain types of adversarial examples, potentially missing those crafted in ways that do not alter distributions in a detectable manner.\" Questions='1. How does the proposed method handle adversarial examples crafted to manipulate the statistical characteristics used for detection, such as entropy or divergence measures?\\n2. Could this method be adapted or extended to other modalities beyond audio, or is it closely tied to the properties of ASR outputs?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The paper proposes a novel flow-based neural ODE model, Q-flow, for computing optimal transport maps directly between two general distributions P and Q.\\n2. By bypassing the need for an intermediary normal distribution, the method improves efficiency and offers new possibilities for generative models and distribution interpolation.\\n3. The empirical evaluation demonstrates strong performance in various high-dimensional data applications, highlighting the model's practical utility.\" Weaknesses='1. The paper primarily focuses on the application of Q-flow to specific tasks like density ratio estimation and image-to-image translation but lacks a deep theoretical understanding or justification of why the method should outperform existing models.\\n2. While the method appears promising, the paper may have limitations regarding the scalability of the proposed model to very large datasets or distributions with complex structures.\\n3. There is a need for more comparative analysis with other state-of-the-art methods to strengthen the claims of superiority in specific applications.' Questions='1. How does the proposed Q-flow method compare with other existing flow-based models or frameworks for tasks like image-to-image translation in terms of computational efficiency and quality of results?\\n2. Are there any specific limitations or conditions under which the Q-flow model may not perform optimally?\\n3. How sensitive is the model to the choice of hyperparameters, and are there any guidelines or recommendations provided by the authors based on their experiments?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The paper proposes a novel flow-based neural ODE model, Q-flow, for computing optimal transport maps directly between two general distributions P and Q.\\n2. By bypassing the need for an intermediary normal distribution, the method improves efficiency and offers new possibilities for generative models and distribution interpolation.\\n3. The empirical evaluation demonstrates strong performance in various high-dimensional data applications, highlighting the model's practical utility.\" Weaknesses='1. The paper primarily focuses on the application of Q-flow to specific tasks like density ratio estimation and image-to-image translation but lacks a deep theoretical understanding or justification of why the method should outperform existing models.\\n2. While the method appears promising, the paper may have limitations regarding the scalability of the proposed model to very large datasets or distributions with complex structures.\\n3. There is a need for more comparative analysis with other state-of-the-art methods to strengthen the claims of superiority in specific applications.' Questions='1. How does the proposed Q-flow method compare with other existing flow-based models or frameworks for tasks like image-to-image translation in terms of computational efficiency and quality of results?\\n2. Are there any specific limitations or conditions under which the Q-flow model may not perform optimally?\\n3. How sensitive is the model to the choice of hyperparameters, and are there any guidelines or recommendations provided by the authors based on their experiments?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed irSLDS model makes a significant contribution towards addressing the limitations of current rSLDS models. It allows for flexible state cardinality and integrates a latent geometric structure over states, which improves interpretability and applicability for neural data analysis. The model is effectively demonstrated on both synthetic and real-world datasets, showcasing its ability to capture non-stationary processes.' Weaknesses='The paper could benefit from more comprehensive comparisons with other state-of-the-art models beyond traditional rSLDS, such as those incorporating alternative state space representations or priors. Additionally, while the use of a PDE framework is mentioned, further elaboration on its specific advantages and limitations compared to other methods would strengthen the argument for its choice.' Questions='How does the performance of irSLDS compare with alternative models that utilize more recent machine learning techniques for neural data analysis? Are there potential challenges in scaling irSLDS to larger datasets or more complex neural recordings?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed irSLDS model makes a significant contribution towards addressing the limitations of current rSLDS models. It allows for flexible state cardinality and integrates a latent geometric structure over states, which improves interpretability and applicability for neural data analysis. The model is effectively demonstrated on both synthetic and real-world datasets, showcasing its ability to capture non-stationary processes.' Weaknesses='The paper could benefit from more comprehensive comparisons with other state-of-the-art models beyond traditional rSLDS, such as those incorporating alternative state space representations or priors. Additionally, while the use of a PDE framework is mentioned, further elaboration on its specific advantages and limitations compared to other methods would strengthen the argument for its choice.' Questions='How does the performance of irSLDS compare with alternative models that utilize more recent machine learning techniques for neural data analysis? Are there potential challenges in scaling irSLDS to larger datasets or more complex neural recordings?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper presents a novel approach to 3D dynamic scene decomposition using object-centric voxelization, which is an innovative step beyond existing 2D methods. The use of differentiable volume rendering, along with a comprehensive three-stage training process, allows for improved performance in scene decomposition, especially in handling occlusions. The method's ability to enable scene editing and dynamic view synthesis is a significant advancement.\" Weaknesses='While the method is promising, there are areas that could be more clearly explained. The description of the training stages is somewhat fragmented, making it difficult to piece together how exactly the model progresses through each stage. Additionally, the reliance on defining a number of objects (N) could limit the adaptability of the method in more complex scenes unless this is dynamically determined. Some technical details about the implementation are lacking, such as the integration of the dynamics module with other components. Furthermore, simplifications or assumptions made in the method, such as object occlusion handling, might limit its applicability to more diverse datasets.' Questions='How does the assumed number of objects (N) affect performance in scenes with varying complexity? Is there a mechanism in place for dynamically adjusting this parameter? Is there potential to extend this method to handle more complex scenes with a higher number of interacting objects? Could the usability of this method be compromised by object occlusion or complex motions that exceed its current design considerations?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper presents a novel approach to 3D dynamic scene decomposition using object-centric voxelization, which is an innovative step beyond existing 2D methods. The use of differentiable volume rendering, along with a comprehensive three-stage training process, allows for improved performance in scene decomposition, especially in handling occlusions. The method's ability to enable scene editing and dynamic view synthesis is a significant advancement.\" Weaknesses='While the method is promising, there are areas that could be more clearly explained. The description of the training stages is somewhat fragmented, making it difficult to piece together how exactly the model progresses through each stage. Additionally, the reliance on defining a number of objects (N) could limit the adaptability of the method in more complex scenes unless this is dynamically determined. Some technical details about the implementation are lacking, such as the integration of the dynamics module with other components. Furthermore, simplifications or assumptions made in the method, such as object occlusion handling, might limit its applicability to more diverse datasets.' Questions='How does the assumed number of objects (N) affect performance in scenes with varying complexity? Is there a mechanism in place for dynamically adjusting this parameter? Is there potential to extend this method to handle more complex scenes with a higher number of interacting objects? Could the usability of this method be compromised by object occlusion or complex motions that exceed its current design considerations?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper presents a novel methodology for open-vocabulary semantic segmentation that does not rely on annotated masks, simplifying the training process.\\n2. P-Seg demonstrates competitive performance on several benchmarks and showcases scalability with enlarged datasets, achieving state-of-the-art results.\\n3. The method leverages a simple framework utilizing pseudo-mask generation and language supervision, reducing reliance on complex pre-trained models.' Weaknesses='1. The innovation of the proposed method might be limited, as it resembles existing frameworks with minor modifications, and these similarities are not thoroughly discussed in the paper.\\n2. The experiments could benefit from comparing results against more recent methods in the domain to better substantiate the state-of-the-art claim.\\n3. There could be more detailed ablations to justify design choices, as the current analysis primarily centers around increasing data size.\\n4. Presentation-wise, some crucial procedural details are missing, such as the formation of textual prompts for evaluation and specifics of self-training.' Questions=\"1. How does the method compare directly against more recent baselines, such as OVSegmentor or SegCLIP, in terms of performance?\\n2. Can the paper clarify the exact nature of its zero-shot claims? Specifically, how is zero-shot defined in this context, given the training datasets likely contain target instances?\\n3. Would the inclusion of pre-trained backbones impact the method's efficacy, and what prompted the decision to train models from scratch?\"\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper presents a novel methodology for open-vocabulary semantic segmentation that does not rely on annotated masks, simplifying the training process.\\n2. P-Seg demonstrates competitive performance on several benchmarks and showcases scalability with enlarged datasets, achieving state-of-the-art results.\\n3. The method leverages a simple framework utilizing pseudo-mask generation and language supervision, reducing reliance on complex pre-trained models.' Weaknesses='1. The innovation of the proposed method might be limited, as it resembles existing frameworks with minor modifications, and these similarities are not thoroughly discussed in the paper.\\n2. The experiments could benefit from comparing results against more recent methods in the domain to better substantiate the state-of-the-art claim.\\n3. There could be more detailed ablations to justify design choices, as the current analysis primarily centers around increasing data size.\\n4. Presentation-wise, some crucial procedural details are missing, such as the formation of textual prompts for evaluation and specifics of self-training.' Questions=\"1. How does the method compare directly against more recent baselines, such as OVSegmentor or SegCLIP, in terms of performance?\\n2. Can the paper clarify the exact nature of its zero-shot claims? Specifically, how is zero-shot defined in this context, given the training datasets likely contain target instances?\\n3. Would the inclusion of pre-trained backbones impact the method's efficacy, and what prompted the decision to train models from scratch?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The paper effectively addresses computational challenges in Transformer-based image restoration methods by proposing a novel Shared Portion Stripe Attention mechanism which reduces redundancy and computational overhead.\\n2. ShareFormer is shown to achieve state-of-the-art performance across multiple benchmarks with improved latency and trainability, indicating the method's potential applicability in a wide range of image restoration tasks.\\n3. Comprehensive evaluation and comparison with existing methods highlight the effectiveness of ShareFormer in achieving an optimal balance between speed and accuracy.\" Weaknesses='1. The paper lacks detailed ablation studies to demonstrate the importance of proposed components like the combined shared attention unit (CSAU). This could help in clarifying individual contributions to the overall performance.\\n2. Despite the improved inference speed, the performance gains in some tasks like grayscale denoising and JPEG compression artifact reduction are not significant, suggesting limited improvements in certain scenarios.\\n3. The novelty of sharing attention maps might not be significant given previous methods like ELAN have proposed similar mechanisms, questioning the distinctiveness of the ShareFormer approach.' Questions=\"1. Can the proposed Shared Portion Stripe Attention mechanism be generalized to other high-dimensional vision tasks beyond image restoration, and if so, what challenges might arise?\\n2. How does the Shared Portion Stripe Attention specifically impact the model's ability to handle high-resolution images compared to conventional Transformer-based methods?\\n3. Are there specific cases where the proposed method underperforms compared to convolution-based approaches, and what might be the cause?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The paper effectively addresses computational challenges in Transformer-based image restoration methods by proposing a novel Shared Portion Stripe Attention mechanism which reduces redundancy and computational overhead.\\n2. ShareFormer is shown to achieve state-of-the-art performance across multiple benchmarks with improved latency and trainability, indicating the method's potential applicability in a wide range of image restoration tasks.\\n3. Comprehensive evaluation and comparison with existing methods highlight the effectiveness of ShareFormer in achieving an optimal balance between speed and accuracy.\" Weaknesses='1. The paper lacks detailed ablation studies to demonstrate the importance of proposed components like the combined shared attention unit (CSAU). This could help in clarifying individual contributions to the overall performance.\\n2. Despite the improved inference speed, the performance gains in some tasks like grayscale denoising and JPEG compression artifact reduction are not significant, suggesting limited improvements in certain scenarios.\\n3. The novelty of sharing attention maps might not be significant given previous methods like ELAN have proposed similar mechanisms, questioning the distinctiveness of the ShareFormer approach.' Questions=\"1. Can the proposed Shared Portion Stripe Attention mechanism be generalized to other high-dimensional vision tasks beyond image restoration, and if so, what challenges might arise?\\n2. How does the Shared Portion Stripe Attention specifically impact the model's ability to handle high-resolution images compared to conventional Transformer-based methods?\\n3. Are there specific cases where the proposed method underperforms compared to convolution-based approaches, and what might be the cause?\"\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Content:\n",
      "I'm sorry, but there seems to be a misinterpretation. If you'd like me to re-evaluate the output, please provide more context or clarify the components you'd like adjusted or repeated.\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper provides a thorough analysis of the output length biases in RLHF and highlights a novel concern that had not been well-addressed in previous research. The experiments conducted on multiple datasets demonstrate rigor and provide compelling evidence for the claims made regarding length biases. The study offers valuable insights into the development of more effective reward models for RLHF systems.' Weaknesses='The presentation of the results, particularly in figures and tables, is somewhat unclear and could be better structured to aid comprehension. There seems to be a lack of clear, actionable guidance for practitioners based on the findings, which might limit the practical impact of the research. Additionally, there could be more exploration of other qualitative factors that contribute to reward improvements beyond output length.' Questions='1. Have the authors considered evaluating the impact of output length biases on other RLHF objectives such as harmfulness or honesty, and how might these factors play a role differently from helpfulness?\\n2. Can the findings regarding length biases be generalized to other domains or tasks beyond language models and RLHF settings?\\n3. Are there more concrete suggestions or methodologies proposed for practitioners to mitigate length biases in reward modeling beyond those tested in the study?\\n4. What are the next steps anticipated in evolving reward models that better align with human preferences without being skewed by factors like output length?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper provides a thorough analysis of the output length biases in RLHF and highlights a novel concern that had not been well-addressed in previous research. The experiments conducted on multiple datasets demonstrate rigor and provide compelling evidence for the claims made regarding length biases. The study offers valuable insights into the development of more effective reward models for RLHF systems.' Weaknesses='The presentation of the results, particularly in figures and tables, is somewhat unclear and could be better structured to aid comprehension. There seems to be a lack of clear, actionable guidance for practitioners based on the findings, which might limit the practical impact of the research. Additionally, there could be more exploration of other qualitative factors that contribute to reward improvements beyond output length.' Questions='1. Have the authors considered evaluating the impact of output length biases on other RLHF objectives such as harmfulness or honesty, and how might these factors play a role differently from helpfulness?\\n2. Can the findings regarding length biases be generalized to other domains or tasks beyond language models and RLHF settings?\\n3. Are there more concrete suggestions or methodologies proposed for practitioners to mitigate length biases in reward modeling beyond those tested in the study?\\n4. What are the next steps anticipated in evolving reward models that better align with human preferences without being skewed by factors like output length?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. SMART introduces an innovative approach to address the problem of evolving graphs in GNNs through self-supervised learning and adaptive feature extraction, which is a novel contribution to the field.\\n2. The methodology is well-structured and leverages graph reconstruction techniques effectively to improve temporal generalization estimation.\\n3. The paper demonstrates a thorough evaluation on both synthetic and real-world data, showing the effectiveness of the proposed method.' Weaknesses='1. The paper could benefit from a more detailed discussion on the limitations and potential pitfalls of the proposed approach, such as assumptions made during theoretical analysis and bound derivation.\\n2. There is a lack of comparison with existing state-of-the-art methods in the experiments, which may undermine the claimed effectiveness of the proposed method.\\n3. The paper could explore additional real-world datasets to further validate the proposed methodology.' Questions=\"1. How does SMART perform in scenarios where the graph's temporality is less predictable or has abrupt changes?\\n2. Can the approach be extended to accommodate disappearing nodes, not just evolving graphs with added nodes and edges?\"\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. SMART introduces an innovative approach to address the problem of evolving graphs in GNNs through self-supervised learning and adaptive feature extraction, which is a novel contribution to the field.\\n2. The methodology is well-structured and leverages graph reconstruction techniques effectively to improve temporal generalization estimation.\\n3. The paper demonstrates a thorough evaluation on both synthetic and real-world data, showing the effectiveness of the proposed method.' Weaknesses='1. The paper could benefit from a more detailed discussion on the limitations and potential pitfalls of the proposed approach, such as assumptions made during theoretical analysis and bound derivation.\\n2. There is a lack of comparison with existing state-of-the-art methods in the experiments, which may undermine the claimed effectiveness of the proposed method.\\n3. The paper could explore additional real-world datasets to further validate the proposed methodology.' Questions=\"1. How does SMART perform in scenarios where the graph's temporality is less predictable or has abrupt changes?\\n2. Can the approach be extended to accommodate disappearing nodes, not just evolving graphs with added nodes and edges?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The proposed technique offers a novel approach to modeling unobserved confounders using diffusion models, which could enhance the estimation of individual treatment effects.\\n2. The paper demonstrates improved performance over several state-of-the-art methods in causal inference tasks, suggesting the practical utility of diffusion models in this area.\\n3. The methodology is well-founded on a clear theoretical basis, combining diffusion processes with deep learning techniques.\\n4. Comprehensive experiments and tests on benchmark datasets help validate the framework's effectiveness vis-à-vis existing models.\" Weaknesses='1. The paper could benefit from clearer presentation in terms of algorithmic steps and intuition behind the diffusion processes.\\n2. The approach, while innovative, may require complex configurations that could limit its accessibility to practitioners who are less familiar with diffusion models.\\n3. There is a lack of detailed discussion regarding computational costs and potential limitations in highly dynamic, less understood environments.' Questions=\"1. How do the computational requirements of the proposed diffusion model-based method compare to those of simpler generative models like VAEs or GANs?\\n2. The notion of 'capturing unobserved latent spaces' through diffusion models might have computational trade-offs in scale. Can you discuss potential pathways to optimize efficiency or handle larger datasets?\\n3. Can the diffusion-based framework be adapted or scaled to different contexts beyond healthcare or recommendation systems?\"\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The proposed technique offers a novel approach to modeling unobserved confounders using diffusion models, which could enhance the estimation of individual treatment effects.\\n2. The paper demonstrates improved performance over several state-of-the-art methods in causal inference tasks, suggesting the practical utility of diffusion models in this area.\\n3. The methodology is well-founded on a clear theoretical basis, combining diffusion processes with deep learning techniques.\\n4. Comprehensive experiments and tests on benchmark datasets help validate the framework's effectiveness vis-à-vis existing models.\" Weaknesses='1. The paper could benefit from clearer presentation in terms of algorithmic steps and intuition behind the diffusion processes.\\n2. The approach, while innovative, may require complex configurations that could limit its accessibility to practitioners who are less familiar with diffusion models.\\n3. There is a lack of detailed discussion regarding computational costs and potential limitations in highly dynamic, less understood environments.' Questions=\"1. How do the computational requirements of the proposed diffusion model-based method compare to those of simpler generative models like VAEs or GANs?\\n2. The notion of 'capturing unobserved latent spaces' through diffusion models might have computational trade-offs in scale. Can you discuss potential pathways to optimize efficiency or handle larger datasets?\\n3. Can the diffusion-based framework be adapted or scaled to different contexts beyond healthcare or recommendation systems?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses an important gap in the field of continual learning by exploring the integration of multi-modal learning, which has been overlooked in previous studies.\\n2. The introduction of a benchmark for multi-modal continual learning using a well-known dataset like VGGSound is a significant contribution that sets a foundation for further research.\\n3. The proposed SAMM method shows potential in improving the stability-plasticity trade-off in dynamic environments, indicating robustness and versatility in handling multi-modal data.' Weaknesses=\"1. The evaluation is somewhat limited, as it focuses on a single dataset (VGGSound) for the experiments. Broader validation across different datasets and settings could strengthen the conclusions.\\n2. The paper's novelty mainly lies in the integration aspect of the approach, which might not be groundbreaking considering existing work in i.i.d. settings and domain-specific applications.\\n3. It would have been beneficial to include a more comprehensive comparison against existing state-of-the-art methods in both unimodal and multimodal continual learning.\" Questions='1. Can the proposed SAMM method be effectively applied to other multimodal contexts beyond audio-visual, such as textual data?\\n2. How does the proposed method handle scenarios where there is significant noise or missing data in one of the modalities?\\n3. Are there any limitations or specific challenges expected when extending this benchmark or method to real-world applications beyond controlled experimental settings?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses an important gap in the field of continual learning by exploring the integration of multi-modal learning, which has been overlooked in previous studies.\\n2. The introduction of a benchmark for multi-modal continual learning using a well-known dataset like VGGSound is a significant contribution that sets a foundation for further research.\\n3. The proposed SAMM method shows potential in improving the stability-plasticity trade-off in dynamic environments, indicating robustness and versatility in handling multi-modal data.' Weaknesses=\"1. The evaluation is somewhat limited, as it focuses on a single dataset (VGGSound) for the experiments. Broader validation across different datasets and settings could strengthen the conclusions.\\n2. The paper's novelty mainly lies in the integration aspect of the approach, which might not be groundbreaking considering existing work in i.i.d. settings and domain-specific applications.\\n3. It would have been beneficial to include a more comprehensive comparison against existing state-of-the-art methods in both unimodal and multimodal continual learning.\" Questions='1. Can the proposed SAMM method be effectively applied to other multimodal contexts beyond audio-visual, such as textual data?\\n2. How does the proposed method handle scenarios where there is significant noise or missing data in one of the modalities?\\n3. Are there any limitations or specific challenges expected when extending this benchmark or method to real-world applications beyond controlled experimental settings?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The proposed GEN-Z method addresses a relevant problem in NLP by providing a robust alternative to discriminative prompting for zero-shot text classification.\\n2. The method demonstrates impressive performance across a wide range of tasks and models, showcasing its generalizability and robustness.\\n3. The approach of using generative prompting with label descriptions and context is innovative and novel, potentially contributing significantly to the field of language model prompting.' Weaknesses=\"1. Although the approach improves robustness to prompt variations, it still relies on manually crafted label descriptions and paraphrases generated by ChatGPT, which may limit scalability and practicality.\\n2. The method's effectiveness is somewhat contingent on the quality of the label descriptions, which could vary widely depending on the task and domain.\\n3. The approach may face challenges in standardization and consistency across different NLP tasks without significant manual intervention.\" Questions='1. Can the approach be fully automated, such as through advanced algorithms or tools, to eliminate the need for manually crafted label descriptions and still maintain performance?\\n2. How does the performance of GEN-Z compare to state-of-the-art methodologies for zero-shot text classification in terms of computational efficiency and resource usage?\\n3. What are the potential limitations of using generative prompting in highly specialized or less-studied domains?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The proposed GEN-Z method addresses a relevant problem in NLP by providing a robust alternative to discriminative prompting for zero-shot text classification.\\n2. The method demonstrates impressive performance across a wide range of tasks and models, showcasing its generalizability and robustness.\\n3. The approach of using generative prompting with label descriptions and context is innovative and novel, potentially contributing significantly to the field of language model prompting.' Weaknesses=\"1. Although the approach improves robustness to prompt variations, it still relies on manually crafted label descriptions and paraphrases generated by ChatGPT, which may limit scalability and practicality.\\n2. The method's effectiveness is somewhat contingent on the quality of the label descriptions, which could vary widely depending on the task and domain.\\n3. The approach may face challenges in standardization and consistency across different NLP tasks without significant manual intervention.\" Questions='1. Can the approach be fully automated, such as through advanced algorithms or tools, to eliminate the need for manually crafted label descriptions and still maintain performance?\\n2. How does the performance of GEN-Z compare to state-of-the-art methodologies for zero-shot text classification in terms of computational efficiency and resource usage?\\n3. What are the potential limitations of using generative prompting in highly specialized or less-studied domains?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=5 Strengths=\"1. LaDe is a comprehensive and large-scale dataset that fills a critical gap in the logistics research domain, by providing researchers with real-world, diverse data that includes pick-up and delivery scenarios, as well as detailed courier information.\\n2. The dataset covers a wide range of geographical locations and operational conditions, making it versatile for benchmarking various logistics algorithms and tasks. \\n3. The work not only contributes a valuable resource but also benchmarks its utility across multiple relevant tasks, demonstrating the dataset's practical applicability.\" Weaknesses='1. While the dataset is introduced as a significant contribution, the paper does not deeply explore how it specifically enhances or challenges existing logistic models beyond initial benchmarking. This limits insights into its transformative potential.\\n2. The potential privacy issues related to the released data, especially concerning details of couriers and operation specifics, need more discussion on how these are addressed to ensure ethical data use.' Questions='1. Beyond the current benchmark tasks (route prediction, ETA prediction, and spatio-temporal forecasting), what are some other potential research areas that could benefit from the LaDe dataset?\\n2. Are there any plans to update or expand the dataset in the future to include more recent data or additional features, such as more detailed traffic conditions or customer feedback? \\n3. How does the LaDe dataset handle data imbalances or bias inherent in operational data across different cities or courier profiles?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=5 Strengths=\"1. LaDe is a comprehensive and large-scale dataset that fills a critical gap in the logistics research domain, by providing researchers with real-world, diverse data that includes pick-up and delivery scenarios, as well as detailed courier information.\\n2. The dataset covers a wide range of geographical locations and operational conditions, making it versatile for benchmarking various logistics algorithms and tasks. \\n3. The work not only contributes a valuable resource but also benchmarks its utility across multiple relevant tasks, demonstrating the dataset's practical applicability.\" Weaknesses='1. While the dataset is introduced as a significant contribution, the paper does not deeply explore how it specifically enhances or challenges existing logistic models beyond initial benchmarking. This limits insights into its transformative potential.\\n2. The potential privacy issues related to the released data, especially concerning details of couriers and operation specifics, need more discussion on how these are addressed to ensure ethical data use.' Questions='1. Beyond the current benchmark tasks (route prediction, ETA prediction, and spatio-temporal forecasting), what are some other potential research areas that could benefit from the LaDe dataset?\\n2. Are there any plans to update or expand the dataset in the future to include more recent data or additional features, such as more detailed traffic conditions or customer feedback? \\n3. How does the LaDe dataset handle data imbalances or bias inherent in operational data across different cities or courier profiles?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The method MIMIC addresses an important gap by generating large-scale multi-view datasets from real-world unannotated videos.\\n2. The dataset provides impressive scalability, aligning with the current need for real-world large-scale datasets for dense vision tasks.\\n3. The approach demonstrates superior performance in dense prediction tasks compared to models trained on annotated datasets.' Weaknesses=\"1. The method's reliance on classical computer vision techniques might not adequately handle complex real-world scenes.\\n2. There could be limitations in terms of the diversity and variability of scenes in the curated datasets.\" Questions='1. How does MIMIC handle scenarios with significant dynamic changes in the scene?\\n2. Are there any limitations in terms of the type or scope of videos that can be used to generate datasets with MIMIC?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The method MIMIC addresses an important gap by generating large-scale multi-view datasets from real-world unannotated videos.\\n2. The dataset provides impressive scalability, aligning with the current need for real-world large-scale datasets for dense vision tasks.\\n3. The approach demonstrates superior performance in dense prediction tasks compared to models trained on annotated datasets.' Weaknesses=\"1. The method's reliance on classical computer vision techniques might not adequately handle complex real-world scenes.\\n2. There could be limitations in terms of the diversity and variability of scenes in the curated datasets.\" Questions='1. How does MIMIC handle scenarios with significant dynamic changes in the scene?\\n2. Are there any limitations in terms of the type or scope of videos that can be used to generate datasets with MIMIC?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The TrASPr model introduces an innovative use of transformers for the task of predicting tissue-specific RNA splicing, which is a crucial step forward given the existing challenges in the field. The integration with a Bayesian Optimization framework showcases the potential for practical applications in RNA splicing outcome design.' Weaknesses='The paper may have some gaps in comparing its results with existing models in depth, and there might be issues with scaling the approach to different datasets or generalizing across various cellular conditions. The reliance on specific datasets also raises questions about its broader applicability.' Questions='How well does the TrASPr model generalize across different tissue types not seen in training? Are there any limitations anticipated in adapting the Bayesian Optimization framework for other splicing-related tasks?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The TrASPr model introduces an innovative use of transformers for the task of predicting tissue-specific RNA splicing, which is a crucial step forward given the existing challenges in the field. The integration with a Bayesian Optimization framework showcases the potential for practical applications in RNA splicing outcome design.' Weaknesses='The paper may have some gaps in comparing its results with existing models in depth, and there might be issues with scaling the approach to different datasets or generalizing across various cellular conditions. The reliance on specific datasets also raises questions about its broader applicability.' Questions='How well does the TrASPr model generalize across different tissue types not seen in training? Are there any limitations anticipated in adapting the Bayesian Optimization framework for other splicing-related tasks?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper proposes a novel multi-modal framework combining video, keypoints, and optical flow with a hierarchical knowledge distillation framework, which shows significant improvements over existing methods for continuous sign language recognition.\\n2. State-of-the-art performance is achieved on multiple benchmarks such as Phoenix-2014, Phoenix-2014T, and CSL-Daily, demonstrating the effectiveness of the approach.\\n3. The method reduces computational costs by using a knowledge distillation technique, which is essential for deploying models in real-world scenarios.\\n4. Comprehensive evaluation across different datasets enhances the reliability of the results presented.' Weaknesses='1. While the integration of multiple modalities is beneficial, the novelty might be limited as multi-modal learning and knowledge distillation are established techniques, suggesting that the key contribution lies more in the application to CSLR rather than advancing these techniques.\\n2. The paper does not discuss the computational requirements in detail, such as how training times compare to other methods or the specific requirements needed for practical deployment.\\n3. There is a dependency on choosing the right modalities and fusion techniques, which may limit general applicability across different CSLR tasks or benchmarks.' Questions='1. How does the model performance vary with different fusion methods, and what criteria should be used to choose between MLP, attention, and convolution-based methods?\\n2. Are there any specific challenges or limitations encountered when integrating the three modalities, particularly with synchronization or alignment issues?\\n3. How resilient is the proposed model to variations in input quality, such as noise in keypoints or low-quality video data?\\n4. What are the training times and hardware requirements needed to train the teacher model compared to the student model?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper proposes a novel multi-modal framework combining video, keypoints, and optical flow with a hierarchical knowledge distillation framework, which shows significant improvements over existing methods for continuous sign language recognition.\\n2. State-of-the-art performance is achieved on multiple benchmarks such as Phoenix-2014, Phoenix-2014T, and CSL-Daily, demonstrating the effectiveness of the approach.\\n3. The method reduces computational costs by using a knowledge distillation technique, which is essential for deploying models in real-world scenarios.\\n4. Comprehensive evaluation across different datasets enhances the reliability of the results presented.' Weaknesses='1. While the integration of multiple modalities is beneficial, the novelty might be limited as multi-modal learning and knowledge distillation are established techniques, suggesting that the key contribution lies more in the application to CSLR rather than advancing these techniques.\\n2. The paper does not discuss the computational requirements in detail, such as how training times compare to other methods or the specific requirements needed for practical deployment.\\n3. There is a dependency on choosing the right modalities and fusion techniques, which may limit general applicability across different CSLR tasks or benchmarks.' Questions='1. How does the model performance vary with different fusion methods, and what criteria should be used to choose between MLP, attention, and convolution-based methods?\\n2. Are there any specific challenges or limitations encountered when integrating the three modalities, particularly with synchronization or alignment issues?\\n3. How resilient is the proposed model to variations in input quality, such as noise in keypoints or low-quality video data?\\n4. What are the training times and hardware requirements needed to train the teacher model compared to the student model?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The proposed UniVoxel framework effectively addresses the inefficiencies in traditional inverse rendering methods by utilizing a unified voxelization approach. \\n2. The use of Spherical Gaussians for modeling local incident light is a novel and efficient way to incorporate illumination without complex calculations, improving performance across varying lighting conditions.\\n3. UniVoxel significantly reduces per-scene training time while achieving high-quality renderings, demonstrating robustness across diverse datasets.' Weaknesses='1. The paper may lack substantial novelty as some aspects of the framework resemble existing methods, and it may not fully address how global illumination and self-occlusion are managed during relighting.\\n2. Although promising, comparisons might not fully account for parameter equivalency when assessing representations like Spherical Gaussians versus Spherical Harmonics, which could challenge claimed efficiency gains.\\n3. There might be limitations in benchmarking only a limited range of scene complexities, potentially not testing edge cases where more complex lighting scenarios are present.' Questions='1. How does UniVoxel handle global illumination and self-occlusion effects during the relighting process to ensure realistic outcomes?\\n2. Are there any scalability constraints when incorporating complex environments or larger scene datasets with the current framework design?\\n3. Have the authors considered extending the evaluation to include more diverse lighting conditions to robustly test the versatility of the proposed method?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The proposed UniVoxel framework effectively addresses the inefficiencies in traditional inverse rendering methods by utilizing a unified voxelization approach. \\n2. The use of Spherical Gaussians for modeling local incident light is a novel and efficient way to incorporate illumination without complex calculations, improving performance across varying lighting conditions.\\n3. UniVoxel significantly reduces per-scene training time while achieving high-quality renderings, demonstrating robustness across diverse datasets.' Weaknesses='1. The paper may lack substantial novelty as some aspects of the framework resemble existing methods, and it may not fully address how global illumination and self-occlusion are managed during relighting.\\n2. Although promising, comparisons might not fully account for parameter equivalency when assessing representations like Spherical Gaussians versus Spherical Harmonics, which could challenge claimed efficiency gains.\\n3. There might be limitations in benchmarking only a limited range of scene complexities, potentially not testing edge cases where more complex lighting scenarios are present.' Questions='1. How does UniVoxel handle global illumination and self-occlusion effects during the relighting process to ensure realistic outcomes?\\n2. Are there any scalability constraints when incorporating complex environments or larger scene datasets with the current framework design?\\n3. Have the authors considered extending the evaluation to include more diverse lighting conditions to robustly test the versatility of the proposed method?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=4 Strengths='1. The paper addresses a critical issue of dataset bias in diffusion models, providing a novel solution with time-dependent importance reweighting that enhances fairness and quality of generative models. \\n2. Comprehensive experiments across multiple standard datasets demonstrate the effectiveness of the proposed method in bias reduction compared to existing baselines.\\n3. The approach is theoretically grounded, with a solid framework that supports the practical applicability of the method in various settings.' Weaknesses=\"1. The method's reliance on accurate density ratio estimation could pose challenges in scenarios where this is difficult, potentially affecting the robustness of the results.\\n2. The computational complexity and scalability of the proposed method in large-scale settings or with extremely large datasets are not thoroughly examined.\" Questions='1. How does the proposed time-dependent importance reweighting method scale with very large datasets or in real-world applications requiring high computational efficiency?\\n2. Are there any specific limitations or failures that were encountered during experimentation that could guide future improvements?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=4 Strengths='1. The paper addresses a critical issue of dataset bias in diffusion models, providing a novel solution with time-dependent importance reweighting that enhances fairness and quality of generative models. \\n2. Comprehensive experiments across multiple standard datasets demonstrate the effectiveness of the proposed method in bias reduction compared to existing baselines.\\n3. The approach is theoretically grounded, with a solid framework that supports the practical applicability of the method in various settings.' Weaknesses=\"1. The method's reliance on accurate density ratio estimation could pose challenges in scenarios where this is difficult, potentially affecting the robustness of the results.\\n2. The computational complexity and scalability of the proposed method in large-scale settings or with extremely large datasets are not thoroughly examined.\" Questions='1. How does the proposed time-dependent importance reweighting method scale with very large datasets or in real-world applications requiring high computational efficiency?\\n2. Are there any specific limitations or failures that were encountered during experimentation that could guide future improvements?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper introduces the Online Sparse Residual Tree (OSRT) model, which efficaciously integrates tree decomposition with adaptive radial basis function (RBF) exploration. This framework supports dynamic and accurate handling of streaming multivariate scattered data. OSRT's innovative approach to growing and refining nodes dynamically based on streaming data points ensures model sparsity, thereby enhancing efficiency and adaptability. The model's superior prediction accuracy and efficiency are substantiated through benchmarking against time series datasets such as Mackey-Glass and Lorenz series.\" Weaknesses='Despite its notable strengths, the paper could further detail the specific implementation steps and potential limitations encountered. Existing methodologies, such as RBF-based models, already address the problem of online learning. The degree to which the proposed OSRT model outperforms these existing solutions requires more substantial analysis and comparative metrics across a broader array of datasets. Additionally, a more comprehensive exploration into the computational complexity and resource-intensiveness when applying OSRT on larger datasets would enhance the practicality and applicability of the model.' Questions='What are the computational costs associated with dynamically adjusting the tree and RBF networks when operating on significantly larger datasets or more complex streams? Does the efficiency of OSRT remain consistent across varying problem domains, and how well does it generalize beyond the time series datasets explored?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper introduces the Online Sparse Residual Tree (OSRT) model, which efficaciously integrates tree decomposition with adaptive radial basis function (RBF) exploration. This framework supports dynamic and accurate handling of streaming multivariate scattered data. OSRT's innovative approach to growing and refining nodes dynamically based on streaming data points ensures model sparsity, thereby enhancing efficiency and adaptability. The model's superior prediction accuracy and efficiency are substantiated through benchmarking against time series datasets such as Mackey-Glass and Lorenz series.\" Weaknesses='Despite its notable strengths, the paper could further detail the specific implementation steps and potential limitations encountered. Existing methodologies, such as RBF-based models, already address the problem of online learning. The degree to which the proposed OSRT model outperforms these existing solutions requires more substantial analysis and comparative metrics across a broader array of datasets. Additionally, a more comprehensive exploration into the computational complexity and resource-intensiveness when applying OSRT on larger datasets would enhance the practicality and applicability of the model.' Questions='What are the computational costs associated with dynamically adjusting the tree and RBF networks when operating on significantly larger datasets or more complex streams? Does the efficiency of OSRT remain consistent across varying problem domains, and how well does it generalize beyond the time series datasets explored?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The paper effectively addresses an overlooked aspect in AI episodic memory modeling by incorporating spatial information, which aligns with cognitive science principles. 2. The introduction of Spatially-Aware Transformers (SAT) and Adaptive Memory Allocator (AMA) showcases innovative thinking, potentially leading to improved memory efficiency and accuracy in task performance. 3. The experiments across various environments and tasks provide strong evidence for the proposed models' effectiveness.\" Weaknesses='1. While the idea of integrating spatial information in episodic memory models is valuable, the novelty of the approach may be questioned, as the adaptation of transformers to new domains is a common research trajectory. 2. The paper may lack clarity regarding the implementation details of the Adaptive Memory Allocator and the specific impacts it has on different learning tasks. 3. The scalability and generalization of the proposed approach to more complex or larger-scale environments remain uncertain.' Questions='1. How does the Adaptive Memory Allocator specifically operate across the different tasks presented in the experiments? Further clarification could support understanding of its functionality. 2. Can the Spatially-Aware Transformers be easily scaled or adapted for use in larger, more complex environments than those tested? 3. Are there any limitations or potential issues in using spatial information that might affect episodic memory modeling or lead to unintended consequences in task accuracy?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The paper effectively addresses an overlooked aspect in AI episodic memory modeling by incorporating spatial information, which aligns with cognitive science principles. 2. The introduction of Spatially-Aware Transformers (SAT) and Adaptive Memory Allocator (AMA) showcases innovative thinking, potentially leading to improved memory efficiency and accuracy in task performance. 3. The experiments across various environments and tasks provide strong evidence for the proposed models' effectiveness.\" Weaknesses='1. While the idea of integrating spatial information in episodic memory models is valuable, the novelty of the approach may be questioned, as the adaptation of transformers to new domains is a common research trajectory. 2. The paper may lack clarity regarding the implementation details of the Adaptive Memory Allocator and the specific impacts it has on different learning tasks. 3. The scalability and generalization of the proposed approach to more complex or larger-scale environments remain uncertain.' Questions='1. How does the Adaptive Memory Allocator specifically operate across the different tasks presented in the experiments? Further clarification could support understanding of its functionality. 2. Can the Spatially-Aware Transformers be easily scaled or adapted for use in larger, more complex environments than those tested? 3. Are there any limitations or potential issues in using spatial information that might affect episodic memory modeling or lead to unintended consequences in task accuracy?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The introduction of sparse and discrete bottlenecks enhances interpretability and control of neural networks without significant performance degradation. The method utilizes vector quantization effectively and demonstrates its application on multiple datasets.' Weaknesses='The approach may be limited by its reliance on a large codebook, which could introduce complexity. The practical implementation across different domains may vary, and there might be challenges in selecting appropriate code vectors.' Questions='How does the method scale with very large models and datasets? Are there any performance trade-offs observed when applying this approach to high-dimensional input data?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The introduction of sparse and discrete bottlenecks enhances interpretability and control of neural networks without significant performance degradation. The method utilizes vector quantization effectively and demonstrates its application on multiple datasets.' Weaknesses='The approach may be limited by its reliance on a large codebook, which could introduce complexity. The practical implementation across different domains may vary, and there might be challenges in selecting appropriate code vectors.' Questions='How does the method scale with very large models and datasets? Are there any performance trade-offs observed when applying this approach to high-dimensional input data?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper addresses a significant problem of learning from partially annotated datasets in multi-label classification, which is challenging and practical in real-world scenarios.\\n2. The combination of reinforcement learning and supervised learning in the proposed PAPG framework is innovative and shows potential to improve performance on incomplete datasets.\\n3. Comprehensive experiments with various datasets demonstrate the robustness and effectiveness of the approach.' Weaknesses='1. The reinforcement learning component, while novel, could add additional complexity to the training process, which might be a hurdle for practitioners to adopt it quickly.\\n2. The paper lacks sufficient detail on how the local and global rewards are precisely calculated, which may affect reproducibility and understanding.\\n3. The integration and benefits of the data enhancement and adaptive threshold techniques in the framework are not thoroughly analyzed in terms of their impact individually.' Questions='1. How does the policy gradient approach handle situations where the unannotated labels have strong interdependencies with the annotated ones?\\n2. What is the sensitivity of the proposed framework to the choice of adaptive thresholds, and how are these thresholds initialized and tuned?\\n3. Can PAPG be extended or adapted to handle fully annotated datasets efficiently, or is it specifically tailored for partially annotated conditions?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper addresses a significant problem of learning from partially annotated datasets in multi-label classification, which is challenging and practical in real-world scenarios.\\n2. The combination of reinforcement learning and supervised learning in the proposed PAPG framework is innovative and shows potential to improve performance on incomplete datasets.\\n3. Comprehensive experiments with various datasets demonstrate the robustness and effectiveness of the approach.' Weaknesses='1. The reinforcement learning component, while novel, could add additional complexity to the training process, which might be a hurdle for practitioners to adopt it quickly.\\n2. The paper lacks sufficient detail on how the local and global rewards are precisely calculated, which may affect reproducibility and understanding.\\n3. The integration and benefits of the data enhancement and adaptive threshold techniques in the framework are not thoroughly analyzed in terms of their impact individually.' Questions='1. How does the policy gradient approach handle situations where the unannotated labels have strong interdependencies with the annotated ones?\\n2. What is the sensitivity of the proposed framework to the choice of adaptive thresholds, and how are these thresholds initialized and tuned?\\n3. Can PAPG be extended or adapted to handle fully annotated datasets efficiently, or is it specifically tailored for partially annotated conditions?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. ProFSA introduces a creative approach to handle protein-ligand complex data scarcity by generating a large set of pseudo-complexes for pretraining, showing excellent results. \\n2. The method demonstrates substantial improvements over existing techniques in key tasks such as druggability prediction and ligand binding affinity prediction. \\n3. The use of high-resolution protein structures for segmentation is well thought out and leverages available data effectively.' Weaknesses=\"1. The method's reliance on pseudo-ligand representations introduces a potential gap between synthetic and real ligands, which might limit the applicability to real-world scenarios.\\n2. The complexity of the method, involving pseudo-ligand and pocket alignments, may require additional effort to replicate or extend. \\n3. Discussion on potential limitations of the pseudo-complexes approach, including any biases introduced during pretraining, is limited.\" Questions=\"1. How does the performance of ProFSA compare when pretraining with actual protein-ligand complexes versus the pseudo-complexes, if such data is available?\\n2. What are the implications of using pseudo-ligands on the interpretability and explainability of ProFSA's predictions?\\n3. Can this approach be generalized to model other types of biomolecular interactions beyond protein-ligand interactions?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. ProFSA introduces a creative approach to handle protein-ligand complex data scarcity by generating a large set of pseudo-complexes for pretraining, showing excellent results. \\n2. The method demonstrates substantial improvements over existing techniques in key tasks such as druggability prediction and ligand binding affinity prediction. \\n3. The use of high-resolution protein structures for segmentation is well thought out and leverages available data effectively.' Weaknesses=\"1. The method's reliance on pseudo-ligand representations introduces a potential gap between synthetic and real ligands, which might limit the applicability to real-world scenarios.\\n2. The complexity of the method, involving pseudo-ligand and pocket alignments, may require additional effort to replicate or extend. \\n3. Discussion on potential limitations of the pseudo-complexes approach, including any biases introduced during pretraining, is limited.\" Questions=\"1. How does the performance of ProFSA compare when pretraining with actual protein-ligand complexes versus the pseudo-complexes, if such data is available?\\n2. What are the implications of using pseudo-ligands on the interpretability and explainability of ProFSA's predictions?\\n3. Can this approach be generalized to model other types of biomolecular interactions beyond protein-ligand interactions?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents a novel approach for integrating neural networks with symbolic reasoning. The Transitional Dictionary Learning framework is innovative and leverages unsupervised learning to bridge the gap between neural and symbolic representations. The introduction of new evaluation metrics that align with human interpretation is a significant contribution, enhancing the interpretability of neural network models.' Weaknesses=\"While the framework shows promise, it primarily focuses on specific datasets and may not generalize well across different domains or more complex real-world scenarios. The method's reliance on unsupervised learning could introduce variability in results depending on the data's nature and complexity.\" Questions='How does the proposed method perform when applied to datasets with higher complexity or more real-world applications? Can the approach be integrated into existing neural network architectures for broader practical applications?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents a novel approach for integrating neural networks with symbolic reasoning. The Transitional Dictionary Learning framework is innovative and leverages unsupervised learning to bridge the gap between neural and symbolic representations. The introduction of new evaluation metrics that align with human interpretation is a significant contribution, enhancing the interpretability of neural network models.' Weaknesses=\"While the framework shows promise, it primarily focuses on specific datasets and may not generalize well across different domains or more complex real-world scenarios. The method's reliance on unsupervised learning could introduce variability in results depending on the data's nature and complexity.\" Questions='How does the proposed method perform when applied to datasets with higher complexity or more real-world applications? Can the approach be integrated into existing neural network architectures for broader practical applications?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents an innovative approach to address limitations in existing Neural Radiance Fields by shifting to a point-based rendering method, demonstrating improvements in rendering quality, geometry correctness, and view consistency. The method introduces novel components like visibility-oriented feature fetching and a feature-augmented learnable kernel, which contribute to the effectiveness of GPF. The experiments conducted on multiple datasets underscore its potential for generalization and finetuning.' Weaknesses='The paper, while presenting a novel approach, may face challenges in terms of scalability and efficiency due to the complexity of the proposed method. Additionally, the reliance on external processes like PatchmatchMVS for the initial point scaffold introduces a dependency that might affect standalone deployment or application in varied scenarios.' Questions='How does the reliance on PatchmatchMVS for initial point scaffolding affect the method’s scalability and applicability in real-world scenarios? Can future work aiming at a NeRF-based initialization resolve potential limitations? Furthermore, how does GPF compare in terms of computational efficiency with existing methods such as E-NeRF?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents an innovative approach to address limitations in existing Neural Radiance Fields by shifting to a point-based rendering method, demonstrating improvements in rendering quality, geometry correctness, and view consistency. The method introduces novel components like visibility-oriented feature fetching and a feature-augmented learnable kernel, which contribute to the effectiveness of GPF. The experiments conducted on multiple datasets underscore its potential for generalization and finetuning.' Weaknesses='The paper, while presenting a novel approach, may face challenges in terms of scalability and efficiency due to the complexity of the proposed method. Additionally, the reliance on external processes like PatchmatchMVS for the initial point scaffold introduces a dependency that might affect standalone deployment or application in varied scenarios.' Questions='How does the reliance on PatchmatchMVS for initial point scaffolding affect the method’s scalability and applicability in real-world scenarios? Can future work aiming at a NeRF-based initialization resolve potential limitations? Furthermore, how does GPF compare in terms of computational efficiency with existing methods such as E-NeRF?'\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Content:\n",
      "My apologies, it seems there was an error in the response. Let me correct that. \n",
      "\n",
      "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper presents a novel approach, LRM, which extends ERM in scenarios with limited labeled data by leveraging label encodings, contributing to maintaining prediction diversity and reducing bias towards dominant classes.\\n2. The theoretical analysis provides a solid understanding of the relationship between LRM, neural collapse, and ERM, helping validate the methodological approach.\\n3. Empirical results on multiple datasets and tasks (SSL, UDA, SHDA) support the method's effectiveness, showcasing improvements over traditional approaches like EntMin.\", \"Weaknesses\": \"1. The presentation could be clearer in parts, especially in detailing how LRM precisely operationalizes label encodings for unlabeled samples and its integration with existing models.\\n2. While the experiments demonstrate effectiveness, the diversity of datasets and settings might be limited, potentially affecting the generalizability of the results in larger and more varied contexts.\\n3. The concept of estimating label encodings needs further exploration to understand any potential limitations or dependencies in diverse real-world scenarios.\", \"Questions\": \"1. How scalable is the LRM approach when applied to very large datasets, given that calculating and aligning label encodings might be computationally intensive?\\n2. Can the proposed method be applied effectively in industry settings like defect detection without requiring extensive modification or fine-tuning?\\n3. What mechanisms are in place within the LRM framework to dynamically adapt to rapidly changing data distributions where class imbalance might shift over time?\"}\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses an important issue related to the training of deep neural networks, specifically the stability of the Jacobian matrix throughout the training process. By exploring the relationship between the initial stability of the Jacobian and its behavior during training, the paper contributes valuable insights to the theoretical understanding of neural network stability.\\n\\nThe work leverages advanced concepts from random matrix theory and offers a general stability theorem, which enriches the discussion and analysis. This solid theoretical foundation is a notable strength of the paper.' Weaknesses='While the theoretical contributions are significant, the paper could benefit from more empirical validation to complement the theoretical analysis. Real-world experiments, especially those demonstrating the practical implications of Jacobian stability throughout training, would strengthen the findings.\\n\\nThe assumption of Jacobian stability based on initialization might not fully capture the nuances of network training dynamics, especially in complex architectures or diverse training scenarios. More exploration of these scenarios would provide a more comprehensive understanding.' Questions='How generalizable are the findings to architectures beyond MLPs or to setups involving non-standard configurations (e.g., different pruning strategies, diverse datasets)? \\n\\nWhat are the specific implications for network design or training protocols based on the stability insight achieved in the study?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses an important issue related to the training of deep neural networks, specifically the stability of the Jacobian matrix throughout the training process. By exploring the relationship between the initial stability of the Jacobian and its behavior during training, the paper contributes valuable insights to the theoretical understanding of neural network stability.\\n\\nThe work leverages advanced concepts from random matrix theory and offers a general stability theorem, which enriches the discussion and analysis. This solid theoretical foundation is a notable strength of the paper.' Weaknesses='While the theoretical contributions are significant, the paper could benefit from more empirical validation to complement the theoretical analysis. Real-world experiments, especially those demonstrating the practical implications of Jacobian stability throughout training, would strengthen the findings.\\n\\nThe assumption of Jacobian stability based on initialization might not fully capture the nuances of network training dynamics, especially in complex architectures or diverse training scenarios. More exploration of these scenarios would provide a more comprehensive understanding.' Questions='How generalizable are the findings to architectures beyond MLPs or to setups involving non-standard configurations (e.g., different pruning strategies, diverse datasets)? \\n\\nWhat are the specific implications for network design or training protocols based on the stability insight achieved in the study?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper introduces the Latent Gaussian Mixture Model (L-GMM), which offers a unified approach for tackling both closed-set and open-set recognition challenges. By using generative modeling, it provides a coherent solution that doesn't rely on training modifications or prior knowledge about unknown classes.\" Weaknesses=\"While the approach is innovative, it might be challenging to adopt in practical settings where interpretability and computational resources are key concerns. Additionally, there might be limited novelty as similar generative approaches have been explored in some contexts; the paper doesn't clearly distinguish itself from such existing work.\" Questions=\"How does the performance of L-GMM compare to other state-of-the-art methods specifically designed for CSR or OSR, in terms of computational efficiency and resource demands? Can the model's approach be extended or adapted to different modalities beyond image recognition and segmentation?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper introduces the Latent Gaussian Mixture Model (L-GMM), which offers a unified approach for tackling both closed-set and open-set recognition challenges. By using generative modeling, it provides a coherent solution that doesn't rely on training modifications or prior knowledge about unknown classes.\" Weaknesses=\"While the approach is innovative, it might be challenging to adopt in practical settings where interpretability and computational resources are key concerns. Additionally, there might be limited novelty as similar generative approaches have been explored in some contexts; the paper doesn't clearly distinguish itself from such existing work.\" Questions=\"How does the performance of L-GMM compare to other state-of-the-art methods specifically designed for CSR or OSR, in terms of computational efficiency and resource demands? Can the model's approach be extended or adapted to different modalities beyond image recognition and segmentation?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper addresses a notable gap in multi-domain text classification by providing a theoretical basis for the task. The use of margin discrepancy as a measurement tool for domain divergence in adversarial training offers both innovation and a solid ground for better model performance. Additionally, the empirical results demonstrate significant advancements over previous approaches across multiple datasets, validating the concept and claims effectively.' Weaknesses='The originality of the theoretical contribution might be questioned, as it largely extends existing work rather than developing entirely new principles. The implementation details and theoretical proofs could benefit from more clarity for practitioners less familiar with the background literature. Furthermore, the dependence on effectively computing and utilizing margin discrepancies in high-dimensional feature spaces could challenge broader applicability in different settings.' Questions='How does the complexity of computing margin discrepancies scale with the size of the datasets and feature spaces? Is the method tested or applicable in real-world large-scale classification tasks where computational efficiency is critical?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper addresses a notable gap in multi-domain text classification by providing a theoretical basis for the task. The use of margin discrepancy as a measurement tool for domain divergence in adversarial training offers both innovation and a solid ground for better model performance. Additionally, the empirical results demonstrate significant advancements over previous approaches across multiple datasets, validating the concept and claims effectively.' Weaknesses='The originality of the theoretical contribution might be questioned, as it largely extends existing work rather than developing entirely new principles. The implementation details and theoretical proofs could benefit from more clarity for practitioners less familiar with the background literature. Furthermore, the dependence on effectively computing and utilizing margin discrepancies in high-dimensional feature spaces could challenge broader applicability in different settings.' Questions='How does the complexity of computing margin discrepancies scale with the size of the datasets and feature spaces? Is the method tested or applicable in real-world large-scale classification tasks where computational efficiency is critical?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper proposes an innovative sample selection method that integrates well with existing SSL frameworks, offering improved performance, especially under budget constraints.\\n2. The use of the Generalized Kernel Herding without Replacement (GKHR) algorithm to minimize α-MMD is a novel contribution that demonstrates good theoretical grounding while addressing the problem of optimal sample selection.\\n3. Extensive experimentation on multiple datasets highlights the effectiveness of the UPA approach.' Weaknesses=\"1. The paper could benefit from more clarity in its explanation of the α-Maximum Mean Discrepancy (α-MMD) and how it directly affects the generalization ability of SSL frameworks.\\n2. The method's scalability and computational overhead are not thoroughly discussed, especially concerning the proposed use of the modified Frank-Wolfe algorithm.\\n3. While performance improvements are demonstrated, there is a lack of qualitative analysis or case studies showing the real-world applicability of the method.\" Questions='1. Can the proposed UPA method be generalized to other domains beyond those tested in the paper, such as more complex real-world data where labeled samples might be even scarcer?\\n2. How does the computational efficiency of UPA compare when integrated with different SSL frameworks or when applied to larger datasets?\\n3. Is there a theoretical guarantee on the improvement of sample diversity and representativeness afforded by the GKHR algorithm in practical scenarios?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper proposes an innovative sample selection method that integrates well with existing SSL frameworks, offering improved performance, especially under budget constraints.\\n2. The use of the Generalized Kernel Herding without Replacement (GKHR) algorithm to minimize α-MMD is a novel contribution that demonstrates good theoretical grounding while addressing the problem of optimal sample selection.\\n3. Extensive experimentation on multiple datasets highlights the effectiveness of the UPA approach.' Weaknesses=\"1. The paper could benefit from more clarity in its explanation of the α-Maximum Mean Discrepancy (α-MMD) and how it directly affects the generalization ability of SSL frameworks.\\n2. The method's scalability and computational overhead are not thoroughly discussed, especially concerning the proposed use of the modified Frank-Wolfe algorithm.\\n3. While performance improvements are demonstrated, there is a lack of qualitative analysis or case studies showing the real-world applicability of the method.\" Questions='1. Can the proposed UPA method be generalized to other domains beyond those tested in the paper, such as more complex real-world data where labeled samples might be even scarcer?\\n2. How does the computational efficiency of UPA compare when integrated with different SSL frameworks or when applied to larger datasets?\\n3. Is there a theoretical guarantee on the improvement of sample diversity and representativeness afforded by the GKHR algorithm in practical scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel integration of Hadamard transforms with quantization for continual learning, successfully demonstrating minimal accuracy loss combined with significant computational efficiency gains. It offers a valuable contribution to the field of deploying machine learning on edge devices, specifically under resource constraints. The proposed method, HDQT, shows promising results under stringent conditions akin to real-world applications.' Weaknesses='The scalability of the proposed method to more complex datasets or tasks with larger models remains largely unexplored. Additionally, the reliance on specific quantization techniques and stochastic rounding poses potential implementation challenges in different hardware environments. There is also limited discussion on broader applicability beyond the presented datasets.' Questions='How does the method handle more complex models or larger datasets beyond CIFAR100 and the mentioned HAR datasets? Are there specific types of models or tasks where HDQT would not be applicable? Also, what are the potential implications of the specific hardware requirements for deploying HDQT in edge computing scenarios?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel integration of Hadamard transforms with quantization for continual learning, successfully demonstrating minimal accuracy loss combined with significant computational efficiency gains. It offers a valuable contribution to the field of deploying machine learning on edge devices, specifically under resource constraints. The proposed method, HDQT, shows promising results under stringent conditions akin to real-world applications.' Weaknesses='The scalability of the proposed method to more complex datasets or tasks with larger models remains largely unexplored. Additionally, the reliance on specific quantization techniques and stochastic rounding poses potential implementation challenges in different hardware environments. There is also limited discussion on broader applicability beyond the presented datasets.' Questions='How does the method handle more complex models or larger datasets beyond CIFAR100 and the mentioned HAR datasets? Are there specific types of models or tasks where HDQT would not be applicable? Also, what are the potential implications of the specific hardware requirements for deploying HDQT in edge computing scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=2 Contribution=3 Rating=5 Confidence=4 Strengths='The paper introduces two novel algorithms that address significant gaps in the existing literature. The private estimation algorithm has a promising approach to reducing sample complexity while maintaining privacy, a critical aspect for real-world applications. Additionally, the development of a sequential testing algorithm that adapts to unknown difficulty parameters is innovative and practically useful.' Weaknesses='The presentation of the paper could be improved, particularly in terms of contextualizing the results with respect to prior work. The novelty of the algorithms, while present, is limited and may not represent a substantial leap forward in the field. Additionally, the paper might benefit from a more detailed comparison with other related works.' Questions='How do the algorithms perform in practical settings where epsilon is not only unknown but potentially variable over time? Are there any assumptions in your algorithms that may limit their applicability in certain domains?'\n",
      "Success: Soundness=3 Presentation=2 Contribution=3 Rating=5 Confidence=4 Strengths='The paper introduces two novel algorithms that address significant gaps in the existing literature. The private estimation algorithm has a promising approach to reducing sample complexity while maintaining privacy, a critical aspect for real-world applications. Additionally, the development of a sequential testing algorithm that adapts to unknown difficulty parameters is innovative and practically useful.' Weaknesses='The presentation of the paper could be improved, particularly in terms of contextualizing the results with respect to prior work. The novelty of the algorithms, while present, is limited and may not represent a substantial leap forward in the field. Additionally, the paper might benefit from a more detailed comparison with other related works.' Questions='How do the algorithms perform in practical settings where epsilon is not only unknown but potentially variable over time? Are there any assumptions in your algorithms that may limit their applicability in certain domains?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper introduces a novel approach to predicting 3D ground-state molecular conformations from 2D topological data using an innovative self-attention mechanism, which is a significant step forward in molecular modeling.\\n2. The proposed model, GTMGC, demonstrates impressive improvements over current state-of-the-art models, as evidenced by the benchmark results on the Molecule3D and QM9 datasets.\\n3. The research has broad implications for molecular property predictions, highlighting the robustness and general applicability of the MSRSA module.' Weaknesses='1. The paper does not thoroughly compare the proposed method with other recent Transformer-based architectures in molecular modeling, which could further substantiate the claims of superiority.\\n2. The reliance on specific datasets like Molecule3D and QM9 might limit the generalizability of the results across other types of molecules or different datasets not evaluated.\\n3. While the MSRSA mechanism is innovative, the implementation details for replicability are not fully fleshed out, which could hinder reproducibility by other researchers.' Questions='1. How does the GTMGC model perform with molecules that have more complex conformational spaces or higher degrees of flexibility beyond the datasets tested?\\n2. Can the proposed MSRSA mechanism be adapted or applied to other domains outside molecular modeling, and if so, what adjustments might be necessary?\\n3. Are there any plans to open-source the GTMGC model or provide additional implementation details to ensure the community can replicate and build upon this work?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper introduces a novel approach to predicting 3D ground-state molecular conformations from 2D topological data using an innovative self-attention mechanism, which is a significant step forward in molecular modeling.\\n2. The proposed model, GTMGC, demonstrates impressive improvements over current state-of-the-art models, as evidenced by the benchmark results on the Molecule3D and QM9 datasets.\\n3. The research has broad implications for molecular property predictions, highlighting the robustness and general applicability of the MSRSA module.' Weaknesses='1. The paper does not thoroughly compare the proposed method with other recent Transformer-based architectures in molecular modeling, which could further substantiate the claims of superiority.\\n2. The reliance on specific datasets like Molecule3D and QM9 might limit the generalizability of the results across other types of molecules or different datasets not evaluated.\\n3. While the MSRSA mechanism is innovative, the implementation details for replicability are not fully fleshed out, which could hinder reproducibility by other researchers.' Questions='1. How does the GTMGC model perform with molecules that have more complex conformational spaces or higher degrees of flexibility beyond the datasets tested?\\n2. Can the proposed MSRSA mechanism be adapted or applied to other domains outside molecular modeling, and if so, what adjustments might be necessary?\\n3. Are there any plans to open-source the GTMGC model or provide additional implementation details to ensure the community can replicate and build upon this work?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='1. WavJourney introduces a novel approach to audio generation by combining large language models with structured semantic representations, which is innovative and addresses a significant gap in the domain of audio creation.\\n2. The method achieves state-of-the-art results on established benchmarks, demonstrating its effectiveness in synthesizing complex audio compositions.\\n3. The framework is compositional and interpretable, allowing the use of different expert audio models without requiring additional training.' Weaknesses='1. While the approach is innovative, the reliance on existing audio models and large language models may limit the novelty and scope of technical contributions.\\n2. The efficiency and real-world applicability of the system might be constrained by its dependence on computationally expensive models.\\n3. The paper discusses potential limitations in artificial composition and efficiency, but does not provide concrete solutions or performance data in these areas.' Questions='1. How does the system handle complex dialogues and transitions in storytelling audio, especially in scenarios requiring intricate timing and layering of audio elements?\\n2. Can the proposed framework be adapted to other domains or modalities, such as video storytelling, using a similar approach?\\n3. What are the specific limitations related to the artificial composition of audio, and how do these limitations impact the realism and quality of the generated audio?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='1. WavJourney introduces a novel approach to audio generation by combining large language models with structured semantic representations, which is innovative and addresses a significant gap in the domain of audio creation.\\n2. The method achieves state-of-the-art results on established benchmarks, demonstrating its effectiveness in synthesizing complex audio compositions.\\n3. The framework is compositional and interpretable, allowing the use of different expert audio models without requiring additional training.' Weaknesses='1. While the approach is innovative, the reliance on existing audio models and large language models may limit the novelty and scope of technical contributions.\\n2. The efficiency and real-world applicability of the system might be constrained by its dependence on computationally expensive models.\\n3. The paper discusses potential limitations in artificial composition and efficiency, but does not provide concrete solutions or performance data in these areas.' Questions='1. How does the system handle complex dialogues and transitions in storytelling audio, especially in scenarios requiring intricate timing and layering of audio elements?\\n2. Can the proposed framework be adapted to other domains or modalities, such as video storytelling, using a similar approach?\\n3. What are the specific limitations related to the artificial composition of audio, and how do these limitations impact the realism and quality of the generated audio?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses an important aspect of data poisoning attacks — their transferability across different learning paradigms — which is crucial given the diversity of training algorithms used in practice.\\n2. The proposed method demonstrates significant improvement over existing techniques in terms of reducing test accuracy across varied datasets and learning paradigms.\\n3. The integration of both supervised and unsupervised contrastive learning to enhance attack transferability is a novel approach.' Weaknesses='1. The reliance on high-frequency perturbations might not be sufficient to ensure transferability across all possible scenarios, and the paper may not fully address how to systematically select or design these perturbations.\\n2. While the experiments are promising, additional analysis or theoretical grounding on why these perturbations are efficient across learning paradigms would strengthen the results.\\n3. The dependency on pretrained models and specific datasets could limit the generalizability of the approach.' Questions='1. How does the method perform on larger or more varied datasets beyond CIFAR-10, CIFAR-100, and TinyImageNet? \\n2. Is there any theoretical justification or insight into why high-frequency perturbations are particularly transferable with contrastive learning?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses an important aspect of data poisoning attacks — their transferability across different learning paradigms — which is crucial given the diversity of training algorithms used in practice.\\n2. The proposed method demonstrates significant improvement over existing techniques in terms of reducing test accuracy across varied datasets and learning paradigms.\\n3. The integration of both supervised and unsupervised contrastive learning to enhance attack transferability is a novel approach.' Weaknesses='1. The reliance on high-frequency perturbations might not be sufficient to ensure transferability across all possible scenarios, and the paper may not fully address how to systematically select or design these perturbations.\\n2. While the experiments are promising, additional analysis or theoretical grounding on why these perturbations are efficient across learning paradigms would strengthen the results.\\n3. The dependency on pretrained models and specific datasets could limit the generalizability of the approach.' Questions='1. How does the method perform on larger or more varied datasets beyond CIFAR-10, CIFAR-100, and TinyImageNet? \\n2. Is there any theoretical justification or insight into why high-frequency perturbations are particularly transferable with contrastive learning?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper provides a novel theoretical framework for understanding the emergent abilities of large language models, focusing on Bayesian inference and joint distributions.\\n2. The theoretical approach offers a new perspective on how LLMs leverage data distribution properties without explicit semantic training.' Weaknesses='1. The theoretical framework may lack empirical support from real-world datasets, as the validation relies on synthetic data.\\n2. The concept of ε-ambiguous languages and latent spaces needs further clarification to enhance comprehensibility for a broader audience.' Questions='1. How does this theoretical model extend to real-world applications and datasets beyond synthetic ones?\\n2. Are there specific guidelines for designing experiments that can empirically test the proposed latent space theory in practical scenarios?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper provides a novel theoretical framework for understanding the emergent abilities of large language models, focusing on Bayesian inference and joint distributions.\\n2. The theoretical approach offers a new perspective on how LLMs leverage data distribution properties without explicit semantic training.' Weaknesses='1. The theoretical framework may lack empirical support from real-world datasets, as the validation relies on synthetic data.\\n2. The concept of ε-ambiguous languages and latent spaces needs further clarification to enhance comprehensibility for a broader audience.' Questions='1. How does this theoretical model extend to real-world applications and datasets beyond synthetic ones?\\n2. Are there specific guidelines for designing experiments that can empirically test the proposed latent space theory in practical scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='1. The paper presents a novel approach to enhancing fairness in GNNs by applying Lipschitz bounds, which is a well-established stabilization method in other neural network domains.\\n2. The method is practical and can be integrated into existing GNN training workflows with minimal overhead, leveraging automatic differentiation.\\n3. Provides both theoretical validation and empirical results that demonstrate the effectiveness of the method on standard graph-based tasks.' Weaknesses='1. While the concept of applying Lipschitz bounds to control GNN output is novel, the paper could have explored more diverse datasets beyond standard node classification and link prediction tasks to thoroughly test the robustness of the method.\\n2. The presentation could be improved with clearer explanations on how Lipschitz bounds specifically enhance fairness and the choice of metrics used to evaluate this.\\n3. The theoretical aspects might be challenging to grasp for readers not deeply familiar with differential matrix calculus.' Questions='1. How does the selection of datasets affect the observed efficacy of applying Lipschitz bounds in GNN training?\\n2. Could the method be extended or adapted to incorporate additional fairness metrics, and if so, how?\\n3. Are there any known limitations or scenarios where applying such Lipschitz constraints might inadvertently reduce performance on other GNN attributes like interpretability or complexity?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='1. The paper presents a novel approach to enhancing fairness in GNNs by applying Lipschitz bounds, which is a well-established stabilization method in other neural network domains.\\n2. The method is practical and can be integrated into existing GNN training workflows with minimal overhead, leveraging automatic differentiation.\\n3. Provides both theoretical validation and empirical results that demonstrate the effectiveness of the method on standard graph-based tasks.' Weaknesses='1. While the concept of applying Lipschitz bounds to control GNN output is novel, the paper could have explored more diverse datasets beyond standard node classification and link prediction tasks to thoroughly test the robustness of the method.\\n2. The presentation could be improved with clearer explanations on how Lipschitz bounds specifically enhance fairness and the choice of metrics used to evaluate this.\\n3. The theoretical aspects might be challenging to grasp for readers not deeply familiar with differential matrix calculus.' Questions='1. How does the selection of datasets affect the observed efficacy of applying Lipschitz bounds in GNN training?\\n2. Could the method be extended or adapted to incorporate additional fairness metrics, and if so, how?\\n3. Are there any known limitations or scenarios where applying such Lipschitz constraints might inadvertently reduce performance on other GNN attributes like interpretability or complexity?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper extends existing dynamic analyses of non-contrastive learning by incorporating feature normalization effects, providing a novel perspective on how cosine loss impacts learning dynamics.\\n2. It offers a theoretical understanding that bridges a gap in the literature regarding the stability of non-contrastive methods in practical applications where normalization is used.\\n3. The methodology includes both theoretical analysis and empirical validation using synthetic data and experiments with CIFAR-10, strengthening the findings.' Weaknesses=\"1. The paper could improve in its discussion of prior work, particularly regarding recent contributions that have also explored dynamics of non-contrastive methods with normalization.\\n2. The experimental validation, while helpful, is limited to relatively simple settings and does not extensively explore more complex or varied data scenarios or architectures beyond ResNet-18.\\n3. There's a lack of exploration of any potential limitations or failure cases of the proposed theoretical framework in practical settings.\" Questions='1. How does the introduction of feature normalization and the associated higher-order dynamics specifically influence real-world applications? Are there scenarios where this effect is negligible or counterproductive?\\n2. Can the theoretical framework proposed be generalized to other forms of loss or learning setups beyond the cosine loss in non-contrastive representations?\\n3. What are the computational implications of the sixth-order dynamics introduced by cosine loss relative to the third-order dynamics of L2 loss?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper extends existing dynamic analyses of non-contrastive learning by incorporating feature normalization effects, providing a novel perspective on how cosine loss impacts learning dynamics.\\n2. It offers a theoretical understanding that bridges a gap in the literature regarding the stability of non-contrastive methods in practical applications where normalization is used.\\n3. The methodology includes both theoretical analysis and empirical validation using synthetic data and experiments with CIFAR-10, strengthening the findings.' Weaknesses=\"1. The paper could improve in its discussion of prior work, particularly regarding recent contributions that have also explored dynamics of non-contrastive methods with normalization.\\n2. The experimental validation, while helpful, is limited to relatively simple settings and does not extensively explore more complex or varied data scenarios or architectures beyond ResNet-18.\\n3. There's a lack of exploration of any potential limitations or failure cases of the proposed theoretical framework in practical settings.\" Questions='1. How does the introduction of feature normalization and the associated higher-order dynamics specifically influence real-world applications? Are there scenarios where this effect is negligible or counterproductive?\\n2. Can the theoretical framework proposed be generalized to other forms of loss or learning setups beyond the cosine loss in non-contrastive representations?\\n3. What are the computational implications of the sixth-order dynamics introduced by cosine loss relative to the third-order dynamics of L2 loss?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper effectively introduces and applies a continuation method to address non-convex optimization challenges in training deep networks. It provides empirical results that demonstrate improved training outcomes with reduced variability and enhanced convergence speed.' Weaknesses=\"The empirical evaluation may be limited to relatively simple datasets like MNIST, which might not fully demonstrate the method's effectiveness on more complex tasks. Additionally, the choice of specific methodologies and parameters requires further justification and exploration.\" Questions=\"To better understand the proposed method's applicability, can it be extended to more complex neural network architectures and larger datasets beyond MNIST? What are the trade-offs or potential limitations when scaling this method to more challenging scenarios?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper effectively introduces and applies a continuation method to address non-convex optimization challenges in training deep networks. It provides empirical results that demonstrate improved training outcomes with reduced variability and enhanced convergence speed.' Weaknesses=\"The empirical evaluation may be limited to relatively simple datasets like MNIST, which might not fully demonstrate the method's effectiveness on more complex tasks. Additionally, the choice of specific methodologies and parameters requires further justification and exploration.\" Questions=\"To better understand the proposed method's applicability, can it be extended to more complex neural network architectures and larger datasets beyond MNIST? What are the trade-offs or potential limitations when scaling this method to more challenging scenarios?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses a relevant problem in domain adaptation by proposing a novel framework that leverages multiple source domains for pseudo-label refinement.\\n2. The method is both theoretically motivated and empirically validated across multiple benchmark datasets.\\n3. The framework enables more effective target domain adaptation without direct access to source data or models, addressing privacy and security concerns.' Weaknesses=\"1. The approach, while innovative, could face challenges in practical scenarios where the statistical relationships between the sources and the target may not be straightforward or well-defined.\\n2. The reliance on pseudo labels means the framework's performance might degrade if the pseudo labels are not accurate, potentially affecting its robustness.\\n3. Limited information is provided on the computational complexity and scalability of the Pseudo-label Refinery Network (PRN) when applied to larger datasets or domains.\" Questions='1. How does the PRN handle variations in the number and nature of source domains, especially with highly diverse or poorly correlated domains?\\n2. Can the proposed method be adapted to scenarios where new sources are introduced incrementally during training?\\n3. Is there any specific mechanism to evaluate the quality of the pseudo labels before using them in the adaptation framework?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses a relevant problem in domain adaptation by proposing a novel framework that leverages multiple source domains for pseudo-label refinement.\\n2. The method is both theoretically motivated and empirically validated across multiple benchmark datasets.\\n3. The framework enables more effective target domain adaptation without direct access to source data or models, addressing privacy and security concerns.' Weaknesses=\"1. The approach, while innovative, could face challenges in practical scenarios where the statistical relationships between the sources and the target may not be straightforward or well-defined.\\n2. The reliance on pseudo labels means the framework's performance might degrade if the pseudo labels are not accurate, potentially affecting its robustness.\\n3. Limited information is provided on the computational complexity and scalability of the Pseudo-label Refinery Network (PRN) when applied to larger datasets or domains.\" Questions='1. How does the PRN handle variations in the number and nature of source domains, especially with highly diverse or poorly correlated domains?\\n2. Can the proposed method be adapted to scenarios where new sources are introduced incrementally during training?\\n3. Is there any specific mechanism to evaluate the quality of the pseudo labels before using them in the adaptation framework?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper effectively integrates feature and cost aggregation for dense matching tasks, presenting a novel Transformer-based architecture that significantly improves performance. The hierarchical approach and multi-scale predictions enhance robustness and accuracy in challenging conditions.' Weaknesses='The approach may require further validation in different practical scenarios to assess its generalization ability. The reliance on combined aggregation may not be easily extensible to tasks with smaller overlapping regions.' Questions='How does the proposed method perform on datasets with limited or minimal overlap between images, particularly in terms of maintaining robust correspondence?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper effectively integrates feature and cost aggregation for dense matching tasks, presenting a novel Transformer-based architecture that significantly improves performance. The hierarchical approach and multi-scale predictions enhance robustness and accuracy in challenging conditions.' Weaknesses='The approach may require further validation in different practical scenarios to assess its generalization ability. The reliance on combined aggregation may not be easily extensible to tasks with smaller overlapping regions.' Questions='How does the proposed method perform on datasets with limited or minimal overlap between images, particularly in terms of maintaining robust correspondence?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The introduction of TEAPOT and ROSY demonstrates a novel approach to address key vulnerabilities in data-free meta-learning. The combination of memory-based strategy and reinforcement learning for model selection provides robustness against task-distribution shifts and corruptions. Extensive experiments validate the effectiveness of the approach.' Weaknesses='The practical applicability of ROSY may be constrained by computational complexity associated with reinforcement learning in real-world scenarios. It remains unclear if simpler, less resource-intensive methods could provide similar robustness enhancements.' Questions=\"How does the computational overhead of the ROSY model selection policy compare to simpler baseline methods? Are there specific scenarios where the method's effectiveness might be limited?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The introduction of TEAPOT and ROSY demonstrates a novel approach to address key vulnerabilities in data-free meta-learning. The combination of memory-based strategy and reinforcement learning for model selection provides robustness against task-distribution shifts and corruptions. Extensive experiments validate the effectiveness of the approach.' Weaknesses='The practical applicability of ROSY may be constrained by computational complexity associated with reinforcement learning in real-world scenarios. It remains unclear if simpler, less resource-intensive methods could provide similar robustness enhancements.' Questions=\"How does the computational overhead of the ROSY model selection policy compare to simpler baseline methods? Are there specific scenarios where the method's effectiveness might be limited?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths='1. The paper addresses a critical and timely issue concerning the alignment of LLMs with human values, which is pivotal for their safe deployment.\\n2. It proposes a novel framework, VUM, to measure value understanding in LLMs, which advances beyond existing methods that mainly focus on ethical perspectives without deeper understanding of values.\\n3. The study methodically uses the Schwartz Value Survey along with a discriminator-critique gap approach, providing a comprehensive evaluation mechanism.' Weaknesses=\"1. The claim that scaling laws significantly impact 'know what' but not 'know why' requires more robust evidence and analysis, given that there is limited detail on model specifics like training data, sizes, and architecture.\\n2. There is some ambiguity in the implementation details, such as how various prompts are integrated or evaluated within the framework, which affects replicability and clarity.\\n3. The assessment of LLMs' 'know why' capabilities relying significantly on GPT-4 annotations may lead to over-reliance on automated evaluations without sufficient human benchmarking.\" Questions=\"1. Can the authors provide more details on the human study mentioned in Appendix B.2, such as the methodology and results, to bolster the findings on LLMs' 'know why' capabilities?\\n2. How well does the VUM framework generalize across different LLM architectures and training regimes? Are there limitations observed when applying this framework to non-GPT models?\\n3. Could the results for models' 'know what' and 'know why' performance be influenced by the specific selection of values from the Schwartz Value Survey? How do these choices impact the generalizability of the findings?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths='1. The paper addresses a critical and timely issue concerning the alignment of LLMs with human values, which is pivotal for their safe deployment.\\n2. It proposes a novel framework, VUM, to measure value understanding in LLMs, which advances beyond existing methods that mainly focus on ethical perspectives without deeper understanding of values.\\n3. The study methodically uses the Schwartz Value Survey along with a discriminator-critique gap approach, providing a comprehensive evaluation mechanism.' Weaknesses=\"1. The claim that scaling laws significantly impact 'know what' but not 'know why' requires more robust evidence and analysis, given that there is limited detail on model specifics like training data, sizes, and architecture.\\n2. There is some ambiguity in the implementation details, such as how various prompts are integrated or evaluated within the framework, which affects replicability and clarity.\\n3. The assessment of LLMs' 'know why' capabilities relying significantly on GPT-4 annotations may lead to over-reliance on automated evaluations without sufficient human benchmarking.\" Questions=\"1. Can the authors provide more details on the human study mentioned in Appendix B.2, such as the methodology and results, to bolster the findings on LLMs' 'know why' capabilities?\\n2. How well does the VUM framework generalize across different LLM architectures and training regimes? Are there limitations observed when applying this framework to non-GPT models?\\n3. Could the results for models' 'know what' and 'know why' performance be influenced by the specific selection of values from the Schwartz Value Survey? How do these choices impact the generalizability of the findings?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The introduction of hierarchical planning into diffusion models is an innovative approach that addresses the inefficiencies seen in traditional diffusion-based generative methods.\\n2. The method demonstrates empirical success, outperforming non-hierarchical and other hierarchical planning methods by improving training and planning speeds and generalizing better to compositional out-of-distribution tasks.\\n3. The division of high-level and low-level diffusers, with the strategic use of subgoals, creates a scalable framework capable of handling long-horizon tasks.' Weaknesses='1. While the hierarchical approach is promising, the paper lacks a detailed ablation study assessing the contribution of each component of the Hierarchical Diffuser.\\n2. The approach might face challenges in cases with undefined or vague subgoals, potentially affecting efficacy and generalizability.\\n3. The implementation details and computational requirements of the method are not extensively discussed, leaving questions about its feasibility for diverse applications.' Questions='1. How does the Hierarchical Diffuser specifically manage subgoal creation in tasks with inherent uncertainty or ambiguity in goal definition?\\n2. What are the computational trade-offs in employing two separate diffusers compared to existing methods, and how scalable is this approach in terms of complexity and resource requirements?\\n3. Can the methodology be effectively applied or extended to real-world scenarios, particularly those with dynamic environments or adaptable objectives?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The introduction of hierarchical planning into diffusion models is an innovative approach that addresses the inefficiencies seen in traditional diffusion-based generative methods.\\n2. The method demonstrates empirical success, outperforming non-hierarchical and other hierarchical planning methods by improving training and planning speeds and generalizing better to compositional out-of-distribution tasks.\\n3. The division of high-level and low-level diffusers, with the strategic use of subgoals, creates a scalable framework capable of handling long-horizon tasks.' Weaknesses='1. While the hierarchical approach is promising, the paper lacks a detailed ablation study assessing the contribution of each component of the Hierarchical Diffuser.\\n2. The approach might face challenges in cases with undefined or vague subgoals, potentially affecting efficacy and generalizability.\\n3. The implementation details and computational requirements of the method are not extensively discussed, leaving questions about its feasibility for diverse applications.' Questions='1. How does the Hierarchical Diffuser specifically manage subgoal creation in tasks with inherent uncertainty or ambiguity in goal definition?\\n2. What are the computational trade-offs in employing two separate diffusers compared to existing methods, and how scalable is this approach in terms of complexity and resource requirements?\\n3. Can the methodology be effectively applied or extended to real-world scenarios, particularly those with dynamic environments or adaptable objectives?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces novel methodologies (SLTRP and SLRP) adapted specifically for Spiking Neural Networks, which address a critical gap in saliency map generation. The combination of these methods with data augmentation techniques, RPGDrop and RPGMix, enhances the generalization capability of SNNs and sets a new state-of-the-art performance in various tasks. The integration of traditional augmentation techniques is also a well-rounded approach to achieve robustness.\\nThe use of biologically inspired components such as event cameras and SNNs, together with advanced computational techniques, presents a meaningful contribution to both neuroscience-inspired computing and machine learning.' Weaknesses=\"The contribution, while innovative, is currently limited to classification tasks. The scope of EventRPG could be expanded to be applicable beyond classification tasks to increase its impact. Additionally, the evaluation is limited to a number of datasets; expanding the evaluation to more and varied datasets could provide further evidence of the method's generalization capabilities. The computational demands and efficiency across different hardware platforms could have been discussed more comprehensively to provide insights into real-world applicability.\" Questions='Could the methods presented (SLTRP, SLRP, RPGDrop, RPGMix) be adapted for use in neural network types other than SNNs or tasks beyond classification? Are there specific limitations posed by the methods that restrict their application to SNNs or classification tasks? How do these methods perform in real-time scenarios or under dynamic conditions that are typical for event-based sensing?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces novel methodologies (SLTRP and SLRP) adapted specifically for Spiking Neural Networks, which address a critical gap in saliency map generation. The combination of these methods with data augmentation techniques, RPGDrop and RPGMix, enhances the generalization capability of SNNs and sets a new state-of-the-art performance in various tasks. The integration of traditional augmentation techniques is also a well-rounded approach to achieve robustness.\\nThe use of biologically inspired components such as event cameras and SNNs, together with advanced computational techniques, presents a meaningful contribution to both neuroscience-inspired computing and machine learning.' Weaknesses=\"The contribution, while innovative, is currently limited to classification tasks. The scope of EventRPG could be expanded to be applicable beyond classification tasks to increase its impact. Additionally, the evaluation is limited to a number of datasets; expanding the evaluation to more and varied datasets could provide further evidence of the method's generalization capabilities. The computational demands and efficiency across different hardware platforms could have been discussed more comprehensively to provide insights into real-world applicability.\" Questions='Could the methods presented (SLTRP, SLRP, RPGDrop, RPGMix) be adapted for use in neural network types other than SNNs or tasks beyond classification? Are there specific limitations posed by the methods that restrict their application to SNNs or classification tasks? How do these methods perform in real-time scenarios or under dynamic conditions that are typical for event-based sensing?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper presents an innovative approach to voice conversion using entangled speech representations, improving zero-shot voice conversion capabilities. The self-synthesized examples as a part of the iterative training strategy contribute to the model's robustness, allowing it to achieve significant improvements over existing baselines. The model's applicability across different languages without reliance on text makes it versatile for various tasks.\" Weaknesses='The reliance on existing methodologies like self-supervised learning and speaker verification might limit the novelty of the approach. Although the method shows improved results, it remains unclear how critical the self-synthesized examples are or how they specifically enhance the training process. There is also limited discussion on possible limitations when scaling to more diverse datasets or real-world scenarios.' Questions='How does the system perform in more complex voice conversion scenarios involving significant background noise or more diverse accents? Are there any insights or benchmarks against entirely different frameworks that do not rely on self-supervised learning components?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper presents an innovative approach to voice conversion using entangled speech representations, improving zero-shot voice conversion capabilities. The self-synthesized examples as a part of the iterative training strategy contribute to the model's robustness, allowing it to achieve significant improvements over existing baselines. The model's applicability across different languages without reliance on text makes it versatile for various tasks.\" Weaknesses='The reliance on existing methodologies like self-supervised learning and speaker verification might limit the novelty of the approach. Although the method shows improved results, it remains unclear how critical the self-synthesized examples are or how they specifically enhance the training process. There is also limited discussion on possible limitations when scaling to more diverse datasets or real-world scenarios.' Questions='How does the system perform in more complex voice conversion scenarios involving significant background noise or more diverse accents? Are there any insights or benchmarks against entirely different frameworks that do not rely on self-supervised learning components?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. Innovative integration of Large Language Models (LLMs) in autonomous driving, which enhances decision-making capabilities and interpretability. \\n2. The proposed system shows improvement in handling complex driving tasks, illustrating the potential of LLMs in improving safety and adaptability in realistic scenarios. \\n3. The cognitive pathway framework and chain-of-thought processes provide a systematic approach that mimics human-like reasoning in AD systems.' Weaknesses='1. The paper might not fully address the potential limitations and challenges of implementing LLMs in real-time environments, such as computational overhead and response times. \\n2. Further clarity is needed on how the system handles edge cases or fails gracefully, especially under real-world conditions. \\n3. The dependency on language-based reasoning could be limiting in scenarios that require real-time processing speed to ensure safety.' Questions='1. How does the proposed method address the potential computational delays associated with running LLMs in real-time autonomous driving scenarios?\\n2. What are the mechanisms in place to ensure consistent and fail-safe decision-making, especially in dynamic and rapidly changing environments?\\n3. Is there any discussion on the ethical implications and trustworthiness of decisions made by AI in safety-critical applications like autonomous driving?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. Innovative integration of Large Language Models (LLMs) in autonomous driving, which enhances decision-making capabilities and interpretability. \\n2. The proposed system shows improvement in handling complex driving tasks, illustrating the potential of LLMs in improving safety and adaptability in realistic scenarios. \\n3. The cognitive pathway framework and chain-of-thought processes provide a systematic approach that mimics human-like reasoning in AD systems.' Weaknesses='1. The paper might not fully address the potential limitations and challenges of implementing LLMs in real-time environments, such as computational overhead and response times. \\n2. Further clarity is needed on how the system handles edge cases or fails gracefully, especially under real-world conditions. \\n3. The dependency on language-based reasoning could be limiting in scenarios that require real-time processing speed to ensure safety.' Questions='1. How does the proposed method address the potential computational delays associated with running LLMs in real-time autonomous driving scenarios?\\n2. What are the mechanisms in place to ensure consistent and fail-safe decision-making, especially in dynamic and rapidly changing environments?\\n3. Is there any discussion on the ethical implications and trustworthiness of decisions made by AI in safety-critical applications like autonomous driving?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='- The paper addresses an important aspect of deep learning architectures by simplifying the Transformer blocks, which are widely used in various applications.\\n- The study shows that it is possible to maintain performance while achieving significant improvements in training throughput and parameter reduction.\\n- The study is backed by empirical results on both decoder-only and encoder-only models, demonstrating the applicability of the proposed simplifications across different model architectures.' Weaknesses='- The simplifications and results have been demonstrated primarily on smaller-scale models and may not directly transfer to larger state-of-the-art models in practical use.\\n- The specific design choices and the interplay between them could be further explained, particularly how certain components interact to ensure performance is maintained.\\n- The experimental evaluation could be more comprehensive, covering a wider range of tasks beyond the datasets considered to validate the generality of the findings.' Questions=\"- How well do the simplified Transformer blocks perform on larger, more complex datasets and more substantial model architectures?\\n- Can the proposed simplifications be integrated with other Transformer improvements, such as those targeting attention mechanisms or alternative architectures?\\n- What are the implications of these simplifications on tasks beyond language modeling, such as those in vision or multimodal domains?\\n- Are there potential drawbacks of the simplification concerning the model's capacity to capture complex patterns in data?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='- The paper addresses an important aspect of deep learning architectures by simplifying the Transformer blocks, which are widely used in various applications.\\n- The study shows that it is possible to maintain performance while achieving significant improvements in training throughput and parameter reduction.\\n- The study is backed by empirical results on both decoder-only and encoder-only models, demonstrating the applicability of the proposed simplifications across different model architectures.' Weaknesses='- The simplifications and results have been demonstrated primarily on smaller-scale models and may not directly transfer to larger state-of-the-art models in practical use.\\n- The specific design choices and the interplay between them could be further explained, particularly how certain components interact to ensure performance is maintained.\\n- The experimental evaluation could be more comprehensive, covering a wider range of tasks beyond the datasets considered to validate the generality of the findings.' Questions=\"- How well do the simplified Transformer blocks perform on larger, more complex datasets and more substantial model architectures?\\n- Can the proposed simplifications be integrated with other Transformer improvements, such as those targeting attention mechanisms or alternative architectures?\\n- What are the implications of these simplifications on tasks beyond language modeling, such as those in vision or multimodal domains?\\n- Are there potential drawbacks of the simplification concerning the model's capacity to capture complex patterns in data?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The approach addresses the problem of infeasibility in QP layers, which can be a significant limitation in neural network design.\\n2. The paper proposes a novel method using augmented Lagrangian techniques to differentiate through infeasible QPs, expanding the capabilities of QP layers.\\n3. A practical implementation with the QPLayer package that interfaces with common frameworks, showing robust and efficient results in numerical experiments.' Weaknesses='1. The presentation could benefit from more clarity and detail on the mathematical underpinnings of the proposed method.\\n2. While the proposed method is innovative, the paper could include more extensive comparisons with existing methods, including potential drawbacks or limitations.\\n3. The reliance on augmented Lagrangian techniques may introduce complexity in certain scenarios which the paper does not fully address.' Questions='1. How does the proposed ECJ framework compare in computational complexity to traditional methods when dealing with large-scale QP layers?\\n2. Are there specific domains or types of neural network architectures where the proposed method might face challenges or limitations?\\n3. Can the QPLayer package be easily integrated into existing neural network training pipelines, and what support is provided for users implementing it?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The approach addresses the problem of infeasibility in QP layers, which can be a significant limitation in neural network design.\\n2. The paper proposes a novel method using augmented Lagrangian techniques to differentiate through infeasible QPs, expanding the capabilities of QP layers.\\n3. A practical implementation with the QPLayer package that interfaces with common frameworks, showing robust and efficient results in numerical experiments.' Weaknesses='1. The presentation could benefit from more clarity and detail on the mathematical underpinnings of the proposed method.\\n2. While the proposed method is innovative, the paper could include more extensive comparisons with existing methods, including potential drawbacks or limitations.\\n3. The reliance on augmented Lagrangian techniques may introduce complexity in certain scenarios which the paper does not fully address.' Questions='1. How does the proposed ECJ framework compare in computational complexity to traditional methods when dealing with large-scale QP layers?\\n2. Are there specific domains or types of neural network architectures where the proposed method might face challenges or limitations?\\n3. Can the QPLayer package be easily integrated into existing neural network training pipelines, and what support is provided for users implementing it?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The method proposes an integrated approach to contextual stochastic optimization, enhancing robustness against uncertainty, which is a critical aspect in real-world applications.\\n2. Theoretical guarantees are provided, bounding the regret of decisions, which adds soundness to the approach.\\n3. Experimental validation, including a real-world electricity generation problem, demonstrates the effectiveness of the method in reducing worst-case costs.' Weaknesses=\"1. The presentation might lack clarity in explaining the complex methodology and the theoretical aspects, which could limit accessibility for readers not deeply familiar with the topic.\\n2. The dependence on discretizing the feasible region might limit the method's scalability or applicability to problems with large or continuous feasible spaces.\" Questions='1. How does the method scale with larger datasets or different types of uncertainties? \\n2. Are there any limitations in the types of contextual information or distributions that can be effectively handled with this approach?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The method proposes an integrated approach to contextual stochastic optimization, enhancing robustness against uncertainty, which is a critical aspect in real-world applications.\\n2. Theoretical guarantees are provided, bounding the regret of decisions, which adds soundness to the approach.\\n3. Experimental validation, including a real-world electricity generation problem, demonstrates the effectiveness of the method in reducing worst-case costs.' Weaknesses=\"1. The presentation might lack clarity in explaining the complex methodology and the theoretical aspects, which could limit accessibility for readers not deeply familiar with the topic.\\n2. The dependence on discretizing the feasible region might limit the method's scalability or applicability to problems with large or continuous feasible spaces.\" Questions='1. How does the method scale with larger datasets or different types of uncertainties? \\n2. Are there any limitations in the types of contextual information or distributions that can be effectively handled with this approach?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='1. The Mixture-of-Prompts framework significantly improves performance on benchmark tasks by splitting tasks into specialized experts, showcasing potential in dynamic prompt generation.\\n2. The approach introduces a novel way of combining instruction and demonstration prompts, leveraging their respective strengths for enhancing LLM capabilities.' Weaknesses='1. The methodology depends heavily on effective clustering of demonstrations into homogeneous regions, which may not always accurately reflect real-world data characteristics.\\n2. The assumption that clustering can effectively divide problem spaces into distinct regions could be problematic, especially if task characteristics are not easily separable or the chosen method influences cluster quality.' Questions='1. How robust is the MoP method to changes in the initial parameters or clustering methods? Are there specific scenarios where its performance is significantly affected?\\n2. Can the approach be generalized beyond the tested NLP benchmark tasks, or does it require adjustments to remain effective in different application domains or modalities?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='1. The Mixture-of-Prompts framework significantly improves performance on benchmark tasks by splitting tasks into specialized experts, showcasing potential in dynamic prompt generation.\\n2. The approach introduces a novel way of combining instruction and demonstration prompts, leveraging their respective strengths for enhancing LLM capabilities.' Weaknesses='1. The methodology depends heavily on effective clustering of demonstrations into homogeneous regions, which may not always accurately reflect real-world data characteristics.\\n2. The assumption that clustering can effectively divide problem spaces into distinct regions could be problematic, especially if task characteristics are not easily separable or the chosen method influences cluster quality.' Questions='1. How robust is the MoP method to changes in the initial parameters or clustering methods? Are there specific scenarios where its performance is significantly affected?\\n2. Can the approach be generalized beyond the tested NLP benchmark tasks, or does it require adjustments to remain effective in different application domains or modalities?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper develops a theoretical framework to understand the failure modes of overparameterized heteroskedastic regression models, which is a significant contribution to the field. The use of a field-theoretical approach from statistical physics to study these models is innovative and provides new insights into their behaviors. The authors successfully validate their framework experimentally, demonstrating its practical relevance. The findings related to phase transitions and regularization strategies are potentially very impactful.' Weaknesses='While the theoretical insights are strong, the paper may be quite complex and challenging to follow for readers not familiar with field theory and statistical physics concepts. The empirical validation, although present, might not cover all potential applications of the theory, and the proposed regularization strategies could require further exploration to assess their efficacy across diverse datasets and neural network architectures.' Questions=\"1. Can the proposed NFE framework and insights about regularization be extended to other types of probabilistic models beyond heteroskedastic regression?\\n2. How sensitive are the phase transition observations and regularization strategies to variations in the model's architecture and the nature of the data?\\n3. What specific challenges might arise when applying the proposed regularization strategies in large-scale real-world datasets?\"\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper develops a theoretical framework to understand the failure modes of overparameterized heteroskedastic regression models, which is a significant contribution to the field. The use of a field-theoretical approach from statistical physics to study these models is innovative and provides new insights into their behaviors. The authors successfully validate their framework experimentally, demonstrating its practical relevance. The findings related to phase transitions and regularization strategies are potentially very impactful.' Weaknesses='While the theoretical insights are strong, the paper may be quite complex and challenging to follow for readers not familiar with field theory and statistical physics concepts. The empirical validation, although present, might not cover all potential applications of the theory, and the proposed regularization strategies could require further exploration to assess their efficacy across diverse datasets and neural network architectures.' Questions=\"1. Can the proposed NFE framework and insights about regularization be extended to other types of probabilistic models beyond heteroskedastic regression?\\n2. How sensitive are the phase transition observations and regularization strategies to variations in the model's architecture and the nature of the data?\\n3. What specific challenges might arise when applying the proposed regularization strategies in large-scale real-world datasets?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"This paper addresses a critical issue in machine learning domain adaptation, focusing on OOD scenarios where traditional fine-tuning methods fall short. The introduction of the 'Connect Later' framework offers a novel approach by using targeted augmentations to fine-tune models after self-supervised pretraining, yielding significant improvements in benchmarks across various datasets. The study's methodology is well-validated, showcasing impressive performance boosts in benchmarks such as AstroClassification and Redshift regression tasks, indicating the applicability and potential of their approach across different real-world applications.\" Weaknesses='Despite the promising results, there is a lack of clarity on how well the findings can be generalized beyond the specific tasks and datasets tested. Additionally, while the framework improves OOD metrics, the paper does not extensively explore the underlying reasons behind these improvements, particularly in relation to the choice and design of targeted augmentations. Furthermore, comparisons between different pretraining strategies are not thoroughly addressed, which limits understanding of how various self-supervised techniques might interact with the proposed fine-tuning method.' Questions='How were the targeted augmentations crafted for each dataset, and is there a systematic way to design these for new datasets? Additionally, can this method be applied to domains where the distribution shifts are more complex or not well understood? How does the performance of the Connect Later framework compare if generic data augmentations are used during the fine-tuning phase instead of targeted ones?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"This paper addresses a critical issue in machine learning domain adaptation, focusing on OOD scenarios where traditional fine-tuning methods fall short. The introduction of the 'Connect Later' framework offers a novel approach by using targeted augmentations to fine-tune models after self-supervised pretraining, yielding significant improvements in benchmarks across various datasets. The study's methodology is well-validated, showcasing impressive performance boosts in benchmarks such as AstroClassification and Redshift regression tasks, indicating the applicability and potential of their approach across different real-world applications.\" Weaknesses='Despite the promising results, there is a lack of clarity on how well the findings can be generalized beyond the specific tasks and datasets tested. Additionally, while the framework improves OOD metrics, the paper does not extensively explore the underlying reasons behind these improvements, particularly in relation to the choice and design of targeted augmentations. Furthermore, comparisons between different pretraining strategies are not thoroughly addressed, which limits understanding of how various self-supervised techniques might interact with the proposed fine-tuning method.' Questions='How were the targeted augmentations crafted for each dataset, and is there a systematic way to design these for new datasets? Additionally, can this method be applied to domains where the distribution shifts are more complex or not well understood? How does the performance of the Connect Later framework compare if generic data augmentations are used during the fine-tuning phase instead of targeted ones?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed PRAG system effectively addresses the gap in maintaining the privacy of queries and data during retrieval, which is crucial for privacy preservation in LLMs. The method is theoretically sound, leveraging multi-party computation techniques to ensure secure interactions across distributed servers without revealing sensitive information. The use of synthetic and real datasets for evaluation demonstrates the practical applicability of the approach.' Weaknesses='The computational complexity of the proposed method is still a concern, suggesting a trade-off between privacy and computational cost that may limit practicality in some settings. Further optimization is required to make the solution more feasible for large-scale applications. The presentation could benefit from clearer explanations of the novel protocol for IVF used, and more concrete comparisons with existing methods in terms of execution time and resource requirements.' Questions='How does the computational overhead of the proposed PRAG system compare to existing privacy-preserving retrieval methods on average-sized and large databases? What specific optimizations could be explored to reduce the computational cost while maintaining the same level of privacy? Are there scenarios or data distributions where the PRAG system might not perform optimally, and if so, how can those be addressed?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed PRAG system effectively addresses the gap in maintaining the privacy of queries and data during retrieval, which is crucial for privacy preservation in LLMs. The method is theoretically sound, leveraging multi-party computation techniques to ensure secure interactions across distributed servers without revealing sensitive information. The use of synthetic and real datasets for evaluation demonstrates the practical applicability of the approach.' Weaknesses='The computational complexity of the proposed method is still a concern, suggesting a trade-off between privacy and computational cost that may limit practicality in some settings. Further optimization is required to make the solution more feasible for large-scale applications. The presentation could benefit from clearer explanations of the novel protocol for IVF used, and more concrete comparisons with existing methods in terms of execution time and resource requirements.' Questions='How does the computational overhead of the proposed PRAG system compare to existing privacy-preserving retrieval methods on average-sized and large databases? What specific optimizations could be explored to reduce the computational cost while maintaining the same level of privacy? Are there scenarios or data distributions where the PRAG system might not perform optimally, and if so, how can those be addressed?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The proposed DDF metric introduces a novel approach to measure differences between 3D point clouds by focusing on underlying surface differences rather than point-wise correspondences. \\n2. The DDF metric significantly improves accuracy and computational efficiency in several 3D tasks such as shape reconstruction, rigid registration, and feature representation.\\n3. The approach has shown to be memory-efficient and provides a more robust alternative to traditional methods like EMD and CD.' Weaknesses=\"1. The paper lacks detailed explanations on how the reference points are generated and may not be clear to all readers.\\n2. While promising, the DDF metric's implications on practical applications outside the tested scenarios need further exploration and validation.\\n3. The method's effectiveness relies heavily on the selection and distribution of reference points, which might limit its applicability in highly irregular or sparse point clouds.\" Questions='1. How does the choice and placement of reference points affect the performance of the DDF metric in various scenarios?\\n2. Can the DDF method be extended or adapted for use in highly irregular or non-uniformly sampled 3D point clouds?\\n3. Are there specific types of 3D point cloud structures or scenarios where DDF does not perform as well compared to traditional metrics?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The proposed DDF metric introduces a novel approach to measure differences between 3D point clouds by focusing on underlying surface differences rather than point-wise correspondences. \\n2. The DDF metric significantly improves accuracy and computational efficiency in several 3D tasks such as shape reconstruction, rigid registration, and feature representation.\\n3. The approach has shown to be memory-efficient and provides a more robust alternative to traditional methods like EMD and CD.' Weaknesses=\"1. The paper lacks detailed explanations on how the reference points are generated and may not be clear to all readers.\\n2. While promising, the DDF metric's implications on practical applications outside the tested scenarios need further exploration and validation.\\n3. The method's effectiveness relies heavily on the selection and distribution of reference points, which might limit its applicability in highly irregular or sparse point clouds.\" Questions='1. How does the choice and placement of reference points affect the performance of the DDF metric in various scenarios?\\n2. Can the DDF method be extended or adapted for use in highly irregular or non-uniformly sampled 3D point clouds?\\n3. Are there specific types of 3D point cloud structures or scenarios where DDF does not perform as well compared to traditional metrics?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths=\"The approach significantly reduces computational resources while achieving state-of-the-art performance, showcasing the importance of data quality over quantity. This could influence future research directions in training language models. The method is well-documented, not only showing substantial quantitative results but also presenting a novel aspect of leveraging 'textbook quality' data.\" Weaknesses='The model shows specialization in Python and sensitivity to prompt variations, which limits its versatility across different programming tasks and might affect performance consistency.' Questions='Could the approach be generalized to other programming languages, or is it inherently tailored to Python? How does the model handle polyglot programming code or code that mixes different languages?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths=\"The approach significantly reduces computational resources while achieving state-of-the-art performance, showcasing the importance of data quality over quantity. This could influence future research directions in training language models. The method is well-documented, not only showing substantial quantitative results but also presenting a novel aspect of leveraging 'textbook quality' data.\" Weaknesses='The model shows specialization in Python and sensitivity to prompt variations, which limits its versatility across different programming tasks and might affect performance consistency.' Questions='Could the approach be generalized to other programming languages, or is it inherently tailored to Python? How does the model handle polyglot programming code or code that mixes different languages?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper presents a novel theoretical framework that distinguishes between task learning and task retrieval in in-context learning, which is a valuable contribution to understanding large language models' behavior. The use of generative models, specifically the Gaussian mixture model, provides a solid basis for analyzing task diversity and deriving risk upper bounds.\" Weaknesses=\"The reliance on the Gaussian mixture model may oversimplify the properties and complexities of actual pretraining distributions and in-context learning scenarios. The clarity of the presentation could be improved, as some of the theoretical derivations and concepts might be hard to follow for a broader audience. Additionally, the paper's novelty might be questioned since the separation of task learning and retrieval has been an area of interest in the community.\" Questions='How does the proposed theoretical framework compare to other existing frameworks that analyze task learning and retrieval in terms of practical applicability and predictive performance? Is the Gaussian mixture model assumption limiting, and if so, how could it potentially be extended to more accurately capture the complexities of real-world data distributions?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper presents a novel theoretical framework that distinguishes between task learning and task retrieval in in-context learning, which is a valuable contribution to understanding large language models' behavior. The use of generative models, specifically the Gaussian mixture model, provides a solid basis for analyzing task diversity and deriving risk upper bounds.\" Weaknesses=\"The reliance on the Gaussian mixture model may oversimplify the properties and complexities of actual pretraining distributions and in-context learning scenarios. The clarity of the presentation could be improved, as some of the theoretical derivations and concepts might be hard to follow for a broader audience. Additionally, the paper's novelty might be questioned since the separation of task learning and retrieval has been an area of interest in the community.\" Questions='How does the proposed theoretical framework compare to other existing frameworks that analyze task learning and retrieval in terms of practical applicability and predictive performance? Is the Gaussian mixture model assumption limiting, and if so, how could it potentially be extended to more accurately capture the complexities of real-world data distributions?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"1. The paper provides a new approach, Learning to Bootstrap (L2B), that does not depend on explicit assumptions about noise or require a clean validation set, which is a notable advantage in handling label noise.\\n2. Extensive experiments demonstrate the method's robustness and effectiveness across multiple datasets, highlighting its ability to generalize well.\" Weaknesses=\"1. The reliance on a validation set to adjust weights raises questions about the method's practical applicability, especially in scenarios where such a set might not be available.\\n2. The method's performance heavily depends on effective pseudo-labeling, which could be challenging in environments with extreme levels of noise or in less structured domains.\" Questions='1. How does the L2B method perform in real-world noisy data scenarios outside of the tested datasets, particularly in non-image domains?\\n2. Could further explanation be provided regarding how pseudo-labels are generated and validated during the training process to ensure quality without a clean validation set?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"1. The paper provides a new approach, Learning to Bootstrap (L2B), that does not depend on explicit assumptions about noise or require a clean validation set, which is a notable advantage in handling label noise.\\n2. Extensive experiments demonstrate the method's robustness and effectiveness across multiple datasets, highlighting its ability to generalize well.\" Weaknesses=\"1. The reliance on a validation set to adjust weights raises questions about the method's practical applicability, especially in scenarios where such a set might not be available.\\n2. The method's performance heavily depends on effective pseudo-labeling, which could be challenging in environments with extreme levels of noise or in less structured domains.\" Questions='1. How does the L2B method perform in real-world noisy data scenarios outside of the tested datasets, particularly in non-image domains?\\n2. Could further explanation be provided regarding how pseudo-labels are generated and validated during the training process to ensure quality without a clean validation set?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. WaveFluid introduces a novel approach to speech synthesis by learning a velocity field, which improves efficiency and reduces computational overhead.\\n2. The model achieves competitive high-fidelity audio synthesis with only one inference step, addressing a key limitation of diffusion models.\\n3. The modular design and use of adversarial training enhance model adaptability and performance improvements.' Weaknesses='1. The paper lacks a comprehensive comparison with all state-of-the-art methods, particularly the latest GAN-based vocoders.\\n2. There is limited discussion on the generalization of WaveFluid beyond the specific task of mel-spectrogram to audio synthesis.\\n3. The reliance on adversarial training can introduce instability and make training sensitive to hyperparameter choices.' Questions='1. How does the model perform on synthesis tasks beyond mel-spectrogram to audio conversion?\\n2. Have any stability issues been observed during adversarial training in WaveFluid?\\n3. Could the approach of learning a velocity field be extended to other generative tasks outside of audio synthesis?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. WaveFluid introduces a novel approach to speech synthesis by learning a velocity field, which improves efficiency and reduces computational overhead.\\n2. The model achieves competitive high-fidelity audio synthesis with only one inference step, addressing a key limitation of diffusion models.\\n3. The modular design and use of adversarial training enhance model adaptability and performance improvements.' Weaknesses='1. The paper lacks a comprehensive comparison with all state-of-the-art methods, particularly the latest GAN-based vocoders.\\n2. There is limited discussion on the generalization of WaveFluid beyond the specific task of mel-spectrogram to audio synthesis.\\n3. The reliance on adversarial training can introduce instability and make training sensitive to hyperparameter choices.' Questions='1. How does the model perform on synthesis tasks beyond mel-spectrogram to audio conversion?\\n2. Have any stability issues been observed during adversarial training in WaveFluid?\\n3. Could the approach of learning a velocity field be extended to other generative tasks outside of audio synthesis?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper presents a novel approach that combines interpretability and robustness in neural networks, addressing two critical issues simultaneously with a unified framework. \\n2. The proposed method shows improved performance in terms of interpretability through competitive IoU scores and robustness against adversarial attacks compared to traditional classifiers. \\n3. The methodology has practical significance, potentially improving the reliability and trustworthiness of neural networks in sensitive applications such as medical diagnosis.' Weaknesses='1. The approach largely focuses on binary classification, which may limit its applicability in more complex scenarios involving multiple classes. \\n2. The paper could benefit from a more extensive evaluation, including comparisons with state-of-the-art counterfactual generation and adversarial robustness techniques to better position its contributions. \\n3. While the cycle-consistent loss term and specialized architectures contribute to improved performance, the specifics of these components and their exact impact on the results are not fully detailed, which may affect reproducibility.' Questions=\"1. How does the approach extend to multiclass classification problems? Are there inherent limitations in scaling the method to more complex classification tasks? \\n2. Can the discriminator's output reliably measure prediction uncertainty across different datasets, or is this a dataset-specific feature? \\n3. How sensitive is the proposed method to changes in the architecture of the GAN components, and are there best practices for selecting these architectures?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper presents a novel approach that combines interpretability and robustness in neural networks, addressing two critical issues simultaneously with a unified framework. \\n2. The proposed method shows improved performance in terms of interpretability through competitive IoU scores and robustness against adversarial attacks compared to traditional classifiers. \\n3. The methodology has practical significance, potentially improving the reliability and trustworthiness of neural networks in sensitive applications such as medical diagnosis.' Weaknesses='1. The approach largely focuses on binary classification, which may limit its applicability in more complex scenarios involving multiple classes. \\n2. The paper could benefit from a more extensive evaluation, including comparisons with state-of-the-art counterfactual generation and adversarial robustness techniques to better position its contributions. \\n3. While the cycle-consistent loss term and specialized architectures contribute to improved performance, the specifics of these components and their exact impact on the results are not fully detailed, which may affect reproducibility.' Questions=\"1. How does the approach extend to multiclass classification problems? Are there inherent limitations in scaling the method to more complex classification tasks? \\n2. Can the discriminator's output reliably measure prediction uncertainty across different datasets, or is this a dataset-specific feature? \\n3. How sensitive is the proposed method to changes in the architecture of the GAN components, and are there best practices for selecting these architectures?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The paper presents a novel approach for unsupervised time series representation learning using a noise-resilient sampling strategy and an effective encoder design.\\n2. The framework outperforms state-of-the-art methods across benchmarks, showcasing its scalability and robustness.\\n3. The method's architecture is designed to be parameter-efficient, facilitating scalability.\\n4. The paper is well-structured and clearly articulated, with thorough experimental evaluations showing the method's effectiveness.\" Weaknesses='1. The paper does not sufficiently address potential limitations or edge cases where the CoInception approach may not perform well.\\n2. The novelty of employing a noise-resilient sampling strategy based on a low-pass filter is somewhat limited, as it is a well-established concept in signal processing.\\n3. Clarity could be improved on how the hierarchical triplet loss specifically contributes to noise resilience, particularly in quantitative comparisons to simpler loss functions.' Questions='1. How does CoInception compare to classical signal processing methods that also employ spectral analysis with low-pass filters in terms of performance and computational efficiency?\\n2. Can the authors provide more insights into the selection of hyperparameters, particularly those relevant to the noise-resilient sampling strategy?\\n3. How sensitive is the method to different types of noise, and does it maintain robustness across varying noise levels?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The paper presents a novel approach for unsupervised time series representation learning using a noise-resilient sampling strategy and an effective encoder design.\\n2. The framework outperforms state-of-the-art methods across benchmarks, showcasing its scalability and robustness.\\n3. The method's architecture is designed to be parameter-efficient, facilitating scalability.\\n4. The paper is well-structured and clearly articulated, with thorough experimental evaluations showing the method's effectiveness.\" Weaknesses='1. The paper does not sufficiently address potential limitations or edge cases where the CoInception approach may not perform well.\\n2. The novelty of employing a noise-resilient sampling strategy based on a low-pass filter is somewhat limited, as it is a well-established concept in signal processing.\\n3. Clarity could be improved on how the hierarchical triplet loss specifically contributes to noise resilience, particularly in quantitative comparisons to simpler loss functions.' Questions='1. How does CoInception compare to classical signal processing methods that also employ spectral analysis with low-pass filters in terms of performance and computational efficiency?\\n2. Can the authors provide more insights into the selection of hyperparameters, particularly those relevant to the noise-resilient sampling strategy?\\n3. How sensitive is the method to different types of noise, and does it maintain robustness across varying noise levels?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses a significant challenge in OOD generalization assessment by proposing a novel approach (Vicinal Risk Proxy) that considers neighboring test sample responses.\\n2. The introduced VRP method improves the robustness and accuracy of risk proxies for OOD scenarios, demonstrated through experiments across multiple datasets.\\n3. The presentation is clear and well-structured, making the methodology comprehensible.\\n4. The method has practical implications as it can be applied without the need for ground truth labels, thus reducing the cost associated with label acquisition in real-world applications.' Weaknesses='1. The reliance on neighboring test samples assumes the existence of sufficiently relevant neighbors in the test set, which might not always be the case.\\n2. The generalization of VRP across diverse datasets or transferability to unseen types of shifts is not thoroughly explored in the paper.\\n3. The paper does not fully investigate the computational overhead introduced by the VRP calculation, which may impact its applicability to large-scale scenarios.' Questions='1. How well does the VRP approach perform when there is a significant domain shift between training and test data, where the vicinal space may not be accurately defined?\\n2. Could the VRP method be integrated with existing ensemble techniques to further enhance OOD generalization assessment?\\n3. Are there specific characteristics of datasets where the VRP method performs particularly well or poorly?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses a significant challenge in OOD generalization assessment by proposing a novel approach (Vicinal Risk Proxy) that considers neighboring test sample responses.\\n2. The introduced VRP method improves the robustness and accuracy of risk proxies for OOD scenarios, demonstrated through experiments across multiple datasets.\\n3. The presentation is clear and well-structured, making the methodology comprehensible.\\n4. The method has practical implications as it can be applied without the need for ground truth labels, thus reducing the cost associated with label acquisition in real-world applications.' Weaknesses='1. The reliance on neighboring test samples assumes the existence of sufficiently relevant neighbors in the test set, which might not always be the case.\\n2. The generalization of VRP across diverse datasets or transferability to unseen types of shifts is not thoroughly explored in the paper.\\n3. The paper does not fully investigate the computational overhead introduced by the VRP calculation, which may impact its applicability to large-scale scenarios.' Questions='1. How well does the VRP approach perform when there is a significant domain shift between training and test data, where the vicinal space may not be accurately defined?\\n2. Could the VRP method be integrated with existing ensemble techniques to further enhance OOD generalization assessment?\\n3. Are there specific characteristics of datasets where the VRP method performs particularly well or poorly?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='DiGT leverages a novel attention mechanism to incorporate edge directionality, which is crucial for directed graph tasks. The method shows significant performance improvements over state-of-the-art models on directed graph datasets, highlighting its effectiveness. The introduction of dual encodings and interpretable attention matrices adds to its innovation.' Weaknesses=\"The method may rely heavily on the assumption that edge directionality is always crucial, which might limit its generalizability to graphs where directionality is less important. Additionally, the need for k-hop neighborhood localization and masking may complicate the model's implementation and computational efficiency.\" Questions='How does the performance of DiGT compare on undirected graphs, and is there a mechanism to adapt it to scenarios where directionality is less significant? What are the computational overheads introduced by the k-hop localization and dual encodings?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='DiGT leverages a novel attention mechanism to incorporate edge directionality, which is crucial for directed graph tasks. The method shows significant performance improvements over state-of-the-art models on directed graph datasets, highlighting its effectiveness. The introduction of dual encodings and interpretable attention matrices adds to its innovation.' Weaknesses=\"The method may rely heavily on the assumption that edge directionality is always crucial, which might limit its generalizability to graphs where directionality is less important. Additionally, the need for k-hop neighborhood localization and masking may complicate the model's implementation and computational efficiency.\" Questions='How does the performance of DiGT compare on undirected graphs, and is there a mechanism to adapt it to scenarios where directionality is less significant? What are the computational overheads introduced by the k-hop localization and dual encodings?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses an important problem in the context of adversarial attacks on Graph Neural Networks, providing a novel approach that does not rely on the common assumptions of previous models, such as homophily.\\n2. SheAttack is designed to function effectively across both homophilic and heterophilic graphs and is shown to be scalable and efficient under the black-box setting.\\n3. The use of Modified Silhouette Score to guide attacks is both innovative and a significant improvement over existing methods, providing a generalized approach to enhance attack strategies.' Weaknesses='1. The paper could further clarify the mechanisms of the proposed method. While MSS is a novel approach, the paper does not provide in-depth theoretical analyses or insights into its application beyond empirical results, which could strengthen its contribution.\\n2. The scalability and efficiency claims, while demonstrated with experimental results, would benefit from clear comparative computational complexity analyses against existing methods.\\n3. The presentation, while acceptable, could be improved in terms of clarity and accessibility, especially regarding the explanation of the Modified Silhouette Score and the optimization process employed by SheAttack.' Questions='1. How does the selection of parameters in the Modified Silhouette Score impact the performance of SheAttack, particularly in graphs with different properties?\\n2. Can the proposed method be extended or adapted to work with different categories of graph embeddings or attack scenarios beyond the ones discussed?\\n3. How does SheAttack compare in performance and efficiency to state-of-the-art methods in scenarios with varying degrees of partial or noisy knowledge of graph properties?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses an important problem in the context of adversarial attacks on Graph Neural Networks, providing a novel approach that does not rely on the common assumptions of previous models, such as homophily.\\n2. SheAttack is designed to function effectively across both homophilic and heterophilic graphs and is shown to be scalable and efficient under the black-box setting.\\n3. The use of Modified Silhouette Score to guide attacks is both innovative and a significant improvement over existing methods, providing a generalized approach to enhance attack strategies.' Weaknesses='1. The paper could further clarify the mechanisms of the proposed method. While MSS is a novel approach, the paper does not provide in-depth theoretical analyses or insights into its application beyond empirical results, which could strengthen its contribution.\\n2. The scalability and efficiency claims, while demonstrated with experimental results, would benefit from clear comparative computational complexity analyses against existing methods.\\n3. The presentation, while acceptable, could be improved in terms of clarity and accessibility, especially regarding the explanation of the Modified Silhouette Score and the optimization process employed by SheAttack.' Questions='1. How does the selection of parameters in the Modified Silhouette Score impact the performance of SheAttack, particularly in graphs with different properties?\\n2. Can the proposed method be extended or adapted to work with different categories of graph embeddings or attack scenarios beyond the ones discussed?\\n3. How does SheAttack compare in performance and efficiency to state-of-the-art methods in scenarios with varying degrees of partial or noisy knowledge of graph properties?'\n",
      "Finished gpt4o on data/test_data/one_shot/test_data_2024_summary_prompts_oneshot.jsonl, saved => results/gpt4o/test_data_2024_summary_prompts_oneshot_gpt4o_results.jsonl\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The novel approach of using three tense-specific attention modules to capture spatio-temporal dependencies is innovative and could provide new insights into traffic forecasting.\\n2. The paper addresses the challenge of incomplete traffic data by integrating contrastive learning with a negative filtering technique, offering a potential improvement in model robustness.\\n3. The proposed TTformer significantly outperforms existing models in traffic forecasting, indicating the effectiveness of the approach.' Weaknesses='1. The paper could provide more clarity on how each tense-specific attention module operates and their individual contributions to the overall model performance.\\n2. The reliance on contrastive learning and the details of the negative filtering technique are not fully detailed, which could impact the reproducibility of the results.\\n3. There is a lack of discussion on the computational complexity of the proposed model, which may impact its applicability in real-world scenarios.' Questions='1. How does the model determine the optimal balance between the three tense-specific attention modules to maximize forecasting accuracy?\\n2. Can the TTformer be adapted to other domains or is it specific to traffic forecasting?\\n3. How does the absence of predefined adjacency matrices affect the model performance in scenarios where they are typically available?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The novel approach of using three tense-specific attention modules to capture spatio-temporal dependencies is innovative and could provide new insights into traffic forecasting.\\n2. The paper addresses the challenge of incomplete traffic data by integrating contrastive learning with a negative filtering technique, offering a potential improvement in model robustness.\\n3. The proposed TTformer significantly outperforms existing models in traffic forecasting, indicating the effectiveness of the approach.' Weaknesses='1. The paper could provide more clarity on how each tense-specific attention module operates and their individual contributions to the overall model performance.\\n2. The reliance on contrastive learning and the details of the negative filtering technique are not fully detailed, which could impact the reproducibility of the results.\\n3. There is a lack of discussion on the computational complexity of the proposed model, which may impact its applicability in real-world scenarios.' Questions='1. How does the model determine the optimal balance between the three tense-specific attention modules to maximize forecasting accuracy?\\n2. Can the TTformer be adapted to other domains or is it specific to traffic forecasting?\\n3. How does the absence of predefined adjacency matrices affect the model performance in scenarios where they are typically available?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The paper presents a novel challenge that is significant for advancing research on reinforcement learning and human-AI cooperation.\\n2. The introduction of a benchmark specifically designed for evaluating generalization in cooperative multi-agent environments is valuable and fills a gap in existing literature.\\n3. The use of dual curriculum design methods adds a new dimension to the evaluation of generalization abilities in AI agents.' Weaknesses='1. The paper lacks detailed examples of how the proposed benchmark leads to new insights or findings in AI cooperation.\\n2. There may be a steep learning curve for researchers unfamiliar with the Overcooked-AI environment, affecting the accessibility of the benchmark.\\n3. The failure of state-of-the-art algorithms on the benchmark is discussed, but further analysis on why this occurs could be provided.' Questions='1. How does the Overcooked Generalisation Challenge compare to existing benchmarks in terms of difficulty and learning requirements for agents?\\n2. What are the planned updates or future expansions for this benchmark to keep it relevant with advancing AI technologies?\\n3. Is there any plan to implement more complex or variable environments within the Overcooked-AI setting to enhance the generalization challenge further?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The paper presents a novel challenge that is significant for advancing research on reinforcement learning and human-AI cooperation.\\n2. The introduction of a benchmark specifically designed for evaluating generalization in cooperative multi-agent environments is valuable and fills a gap in existing literature.\\n3. The use of dual curriculum design methods adds a new dimension to the evaluation of generalization abilities in AI agents.' Weaknesses='1. The paper lacks detailed examples of how the proposed benchmark leads to new insights or findings in AI cooperation.\\n2. There may be a steep learning curve for researchers unfamiliar with the Overcooked-AI environment, affecting the accessibility of the benchmark.\\n3. The failure of state-of-the-art algorithms on the benchmark is discussed, but further analysis on why this occurs could be provided.' Questions='1. How does the Overcooked Generalisation Challenge compare to existing benchmarks in terms of difficulty and learning requirements for agents?\\n2. What are the planned updates or future expansions for this benchmark to keep it relevant with advancing AI technologies?\\n3. Is there any plan to implement more complex or variable environments within the Overcooked-AI setting to enhance the generalization challenge further?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper provides a novel approach to address the problem of data shortage for training large language models in long-context scenarios. It identifies the importance of long-distance referrals found in natural documents and provides a scalable data pipeline, LongPack, which constructs high-quality long documents using minimal resources. The use of hyperlinks as a natural relational signal is both intuitive and effective, leading to superior performance compared to existing methods. The paper demonstrates significant efficiency and scalability in creating large datasets with 'near-natural' quality.\" Weaknesses=\"The approach may be heavily reliant on the availability of hyperlinks, making it less applicable to other document types without clear referral signals. Furthermore, while the paper indicates improvements, it would be strengthened by a deeper comparison against a wider range of existing techniques and providing more details about the experimental setup. There is a need for more insight into the method's adaptability to other types of data outside of the web context.\" Questions='How does the LongPack method perform with data sources other than web pages, where hyperlink structures may not be present? Are there alternative referral signals that could be used in such contexts?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper provides a novel approach to address the problem of data shortage for training large language models in long-context scenarios. It identifies the importance of long-distance referrals found in natural documents and provides a scalable data pipeline, LongPack, which constructs high-quality long documents using minimal resources. The use of hyperlinks as a natural relational signal is both intuitive and effective, leading to superior performance compared to existing methods. The paper demonstrates significant efficiency and scalability in creating large datasets with 'near-natural' quality.\" Weaknesses=\"The approach may be heavily reliant on the availability of hyperlinks, making it less applicable to other document types without clear referral signals. Furthermore, while the paper indicates improvements, it would be strengthened by a deeper comparison against a wider range of existing techniques and providing more details about the experimental setup. There is a need for more insight into the method's adaptability to other types of data outside of the web context.\" Questions='How does the LongPack method perform with data sources other than web pages, where hyperlink structures may not be present? Are there alternative referral signals that could be used in such contexts?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=3 Strengths='1. The paper proposes an augmentation-free SSL method for tabular data, addressing one of the core challenges in self-supervised learning for structured data.\\n2. The experimental results are promising, showing substantial improvement in classification and regression tasks compared to traditional learning methods.\\n3. T-JEPA effectively identifies relevant features for downstream tasks without labels, which is a significant advantage for unsupervised learning contexts.' Weaknesses='1. The methodology relies heavily on the architecture and specific setup of JEPA, which might limit its generalizability across different contexts or architectures.\\n2. Although the experimental results are promising, further validation on a wider diversity of datasets would strengthen the claims.' Questions='1. How does the performance of T-JEPA compare when varying the size of feature subsets used for latent space prediction?\\n2. Can the insights from T-JEPA in tabular contexts be extended to other data forms, like time-series or spatial data?\\n3. What are the computational costs associated with training the T-JEPA compared to other SSL methods for tabular data?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=3 Strengths='1. The paper proposes an augmentation-free SSL method for tabular data, addressing one of the core challenges in self-supervised learning for structured data.\\n2. The experimental results are promising, showing substantial improvement in classification and regression tasks compared to traditional learning methods.\\n3. T-JEPA effectively identifies relevant features for downstream tasks without labels, which is a significant advantage for unsupervised learning contexts.' Weaknesses='1. The methodology relies heavily on the architecture and specific setup of JEPA, which might limit its generalizability across different contexts or architectures.\\n2. Although the experimental results are promising, further validation on a wider diversity of datasets would strengthen the claims.' Questions='1. How does the performance of T-JEPA compare when varying the size of feature subsets used for latent space prediction?\\n2. Can the insights from T-JEPA in tabular contexts be extended to other data forms, like time-series or spatial data?\\n3. What are the computational costs associated with training the T-JEPA compared to other SSL methods for tabular data?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper introduces a novel and effective augmentation strategy for few-shot learning in tabular domains, addressing a significant challenge in limited annotated data scenarios. The creation of the FeSTa benchmark with 42 datasets and 31 algorithms provides a comprehensive evaluation framework for future research in this area.' Weaknesses='The novelty of the range-limited augmentation method may be questioned as it builds on existing concepts without significant theoretical advancement. The applicability of the approach to non-tabular domains is not explored, which could limit the generalizability of the findings.' Questions='How does the proposed range-limited augmentation method compare with other state-of-the-art augmentation techniques in different domains, such as images or text? Is the benchmark designed to be easily expandable with new datasets or algorithms for future research?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper introduces a novel and effective augmentation strategy for few-shot learning in tabular domains, addressing a significant challenge in limited annotated data scenarios. The creation of the FeSTa benchmark with 42 datasets and 31 algorithms provides a comprehensive evaluation framework for future research in this area.' Weaknesses='The novelty of the range-limited augmentation method may be questioned as it builds on existing concepts without significant theoretical advancement. The applicability of the approach to non-tabular domains is not explored, which could limit the generalizability of the findings.' Questions='How does the proposed range-limited augmentation method compare with other state-of-the-art augmentation techniques in different domains, such as images or text? Is the benchmark designed to be easily expandable with new datasets or algorithms for future research?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths=\"The paper proposes a novel framework for agent-oriented planning in multi-agent systems, highlighting three critical design principles: solvability, completeness, and non-redundancy. These principles guide the design of a reward-based evaluation model that can significantly advance multi-agent planning. The feedback loop incorporated into the framework is also a notable strength that enhances its robustness and adaptability. Furthermore, the extensive experiments demonstrate the framework's superiority over existing planning strategies.\" Weaknesses=\"While the framework is novel, the discussion lacks clarity on how the feedback loop mechanism specifically influences task allocation and decision-making processes in dynamic environments. Additionally, while experiments show improvement, the paper doesn't specify the limitations and potential edge cases where the framework may not perform optimally.\" Questions=\"1. How does the feedback loop specifically contribute to improving task allocation?\\n2. Are there scenarios where the reward model might not provide accurate evaluations of the agents' performance?\\n3. Can the framework be extended to a broader range of applications beyond those tested?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths=\"The paper proposes a novel framework for agent-oriented planning in multi-agent systems, highlighting three critical design principles: solvability, completeness, and non-redundancy. These principles guide the design of a reward-based evaluation model that can significantly advance multi-agent planning. The feedback loop incorporated into the framework is also a notable strength that enhances its robustness and adaptability. Furthermore, the extensive experiments demonstrate the framework's superiority over existing planning strategies.\" Weaknesses=\"While the framework is novel, the discussion lacks clarity on how the feedback loop mechanism specifically influences task allocation and decision-making processes in dynamic environments. Additionally, while experiments show improvement, the paper doesn't specify the limitations and potential edge cases where the framework may not perform optimally.\" Questions=\"1. How does the feedback loop specifically contribute to improving task allocation?\\n2. Are there scenarios where the reward model might not provide accurate evaluations of the agents' performance?\\n3. Can the framework be extended to a broader range of applications beyond those tested?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The paper presents an innovative framework, TransFusion, that enhances cross-lingual transfer for low-resource languages through English translations and annotation fusion. \\n2. The proposed method has been empirically validated across a wide range of languages and multilingual IE datasets, demonstrating substantial performance improvements.\\n3. The framework shows versatility by improving various models, including proprietary ones like GPT-4 and both decoder-only and encoder-only language models.' Weaknesses='1. The paper may not fully explore potential limitations or challenges of applying TransFusion in real-world settings, particularly regarding computational resources needed for translation and tuning. \\n2. While the concept is novel, more in-depth analysis or ablation studies could strengthen the findings and clarify the contribution of individual components of the framework.\\n3. Potential difficulties in applying the framework to languages with very distinct structures from English or for which few translation resources exist are not discussed.' Questions='1. How does the performance of TransFusion vary across different language typologies, especially those that are significantly distinct from English?\\n2. What is the impact of translation quality on the performance of the TransFusion framework in low-resource settings?\\n3. Can this approach be adapted or extended to languages that lack reliable translation models to English?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The paper presents an innovative framework, TransFusion, that enhances cross-lingual transfer for low-resource languages through English translations and annotation fusion. \\n2. The proposed method has been empirically validated across a wide range of languages and multilingual IE datasets, demonstrating substantial performance improvements.\\n3. The framework shows versatility by improving various models, including proprietary ones like GPT-4 and both decoder-only and encoder-only language models.' Weaknesses='1. The paper may not fully explore potential limitations or challenges of applying TransFusion in real-world settings, particularly regarding computational resources needed for translation and tuning. \\n2. While the concept is novel, more in-depth analysis or ablation studies could strengthen the findings and clarify the contribution of individual components of the framework.\\n3. Potential difficulties in applying the framework to languages with very distinct structures from English or for which few translation resources exist are not discussed.' Questions='1. How does the performance of TransFusion vary across different language typologies, especially those that are significantly distinct from English?\\n2. What is the impact of translation quality on the performance of the TransFusion framework in low-resource settings?\\n3. Can this approach be adapted or extended to languages that lack reliable translation models to English?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper provides new theoretical insights by relaxing previous assumptions in causal discovery, proposing a novel differentiable algorithm. This contributes significantly to improving scalability and applicability to real-world data. The practical utility is demonstrated effectively with interpretable results on high-dimensional image data.' Weaknesses='The paper lacks detailed discussion on the impact and limitations of the proposed method. Some empirical results, such as those on high-dimensional data, could be better contextualized in existing literature.' Questions='How does the proposed method handle situations with severe noise or very sparse data? Are there any limitations when dealing with extremely large datasets?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper provides new theoretical insights by relaxing previous assumptions in causal discovery, proposing a novel differentiable algorithm. This contributes significantly to improving scalability and applicability to real-world data. The practical utility is demonstrated effectively with interpretable results on high-dimensional image data.' Weaknesses='The paper lacks detailed discussion on the impact and limitations of the proposed method. Some empirical results, such as those on high-dimensional data, could be better contextualized in existing literature.' Questions='How does the proposed method handle situations with severe noise or very sparse data? Are there any limitations when dealing with extremely large datasets?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a significant architectural advancement designed to capture multi-level spatial data. It demonstrates superior outcomes in certain evaluation metrics over traditional models. The proposed new tasks could drive future exploration in the field.' Weaknesses='The work lacks an in-depth comparative analysis with existing state-of-the-art models, potentially undervaluing its relative improvement claims. Furthermore, the specificity of newly introduced tasks may limit applicability or necessitate additional baseline comparisons.' Questions='How does the Spatial 3D-LLM perform in real-world applications involving dynamic environments? What are the limitations when adapting to varying scales in 3D objects during layout editing? Is there any provision in the model for handling occlusions in 3D scenes?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a significant architectural advancement designed to capture multi-level spatial data. It demonstrates superior outcomes in certain evaluation metrics over traditional models. The proposed new tasks could drive future exploration in the field.' Weaknesses='The work lacks an in-depth comparative analysis with existing state-of-the-art models, potentially undervaluing its relative improvement claims. Furthermore, the specificity of newly introduced tasks may limit applicability or necessitate additional baseline comparisons.' Questions='How does the Spatial 3D-LLM perform in real-world applications involving dynamic environments? What are the limitations when adapting to varying scales in 3D objects during layout editing? Is there any provision in the model for handling occlusions in 3D scenes?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel approach to explain graph representations through graph pattern analysis, addressing an under-explored area in explainable graph learning. The introduction of a framework for learning and explaining graph representations is innovative. Theoretical analyses on robustness and generalization add depth to the work.' Weaknesses='The limitations and potential high-dimensionality of the pattern counting vector might hinder practical applicability. Though theoretical analyses are provided, there is limited insight into how these results directly translate to real-world improvements. The paper might not fully explore connections to existing post-hoc interpretability methods.' Questions='What are the computational implications of sampling graph substructures of various patterns, and how does it scale with larger graphs? How do the theoretical analyses of robustness and generalization translate to actual observed improvements in real-world scenarios?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel approach to explain graph representations through graph pattern analysis, addressing an under-explored area in explainable graph learning. The introduction of a framework for learning and explaining graph representations is innovative. Theoretical analyses on robustness and generalization add depth to the work.' Weaknesses='The limitations and potential high-dimensionality of the pattern counting vector might hinder practical applicability. Though theoretical analyses are provided, there is limited insight into how these results directly translate to real-world improvements. The paper might not fully explore connections to existing post-hoc interpretability methods.' Questions='What are the computational implications of sampling graph substructures of various patterns, and how does it scale with larger graphs? How do the theoretical analyses of robustness and generalization translate to actual observed improvements in real-world scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=3 Strengths='1. The research introduces a large and high-quality QA dataset specific to egocentric videos, addressing a significant gap in the field.\\n2. The proposed benchmark for egocentric video QA challenges the models to recognize and memorize visual details, enhancing the evaluation process.\\n3. The novel \"Memory Pointer Prompting\" mechanism in the multimodal architecture is well-conceived, allowing for better comprehension of extended video content.' Weaknesses='1. The methodology for automatic generation of the QA dataset may lack transparency regarding the quality assurance mechanisms employed.\\n2. The paper may not provide enough empirical evidence comparing the new model against a wide array of existing baseline models in the domain.' Questions='1. Can the authors provide more details on the process of generating high-quality QA samples and the criteria used to ensure their quality?\\n2. How does the performance of the proposed model compare to state-of-the-art methods on existing benchmarks not introduced in this paper?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=3 Strengths='1. The research introduces a large and high-quality QA dataset specific to egocentric videos, addressing a significant gap in the field.\\n2. The proposed benchmark for egocentric video QA challenges the models to recognize and memorize visual details, enhancing the evaluation process.\\n3. The novel \"Memory Pointer Prompting\" mechanism in the multimodal architecture is well-conceived, allowing for better comprehension of extended video content.' Weaknesses='1. The methodology for automatic generation of the QA dataset may lack transparency regarding the quality assurance mechanisms employed.\\n2. The paper may not provide enough empirical evidence comparing the new model against a wide array of existing baseline models in the domain.' Questions='1. Can the authors provide more details on the process of generating high-quality QA samples and the criteria used to ensure their quality?\\n2. How does the performance of the proposed model compare to state-of-the-art methods on existing benchmarks not introduced in this paper?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The paper provides an important advancement by proving convergent Rényi Differential Privacy (DP) bounds for non-convex, non-smooth losses with H\"older continuous gradients. This relaxes the previous requirements of smooth convexity, broadening the applicability of DP guarantees. The enhanced privacy bounds offered even in smooth strongly convex losses demonstrate the robustness of their approach. The methodologies like forward Wasserstein distance tracking and optimal shifts allocation are novel and contribute significantly to the field.' Weaknesses='The presentation of the mathematical concepts might be challenging for readers without extensive background in differential privacy and optimization. Some parts of the methodology, such as the interpretation of H\"older continuity in practical settings, need more clarity and justification. The assumptions made, while innovative, might limit the direct applicability of the results to real-world scenarios without further empirical validation.' Questions='How does the assumption of H\"older continuity align with typical real-world applications of non-convex optimization in privacy contexts? Are there practical examples where the proposed method significantly outperforms existing techniques under these assumptions? Can the authors provide more intuitive explanations or simplified versions of the main theorems to enhance accessibility to a broader audience?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The paper provides an important advancement by proving convergent Rényi Differential Privacy (DP) bounds for non-convex, non-smooth losses with H\"older continuous gradients. This relaxes the previous requirements of smooth convexity, broadening the applicability of DP guarantees. The enhanced privacy bounds offered even in smooth strongly convex losses demonstrate the robustness of their approach. The methodologies like forward Wasserstein distance tracking and optimal shifts allocation are novel and contribute significantly to the field.' Weaknesses='The presentation of the mathematical concepts might be challenging for readers without extensive background in differential privacy and optimization. Some parts of the methodology, such as the interpretation of H\"older continuity in practical settings, need more clarity and justification. The assumptions made, while innovative, might limit the direct applicability of the results to real-world scenarios without further empirical validation.' Questions='How does the assumption of H\"older continuity align with typical real-world applications of non-convex optimization in privacy contexts? Are there practical examples where the proposed method significantly outperforms existing techniques under these assumptions? Can the authors provide more intuitive explanations or simplified versions of the main theorems to enhance accessibility to a broader audience?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths=\"The paper introduces an innovative framework called Neuron Predictability Lens (NPL) to analyze transformer-based language models, which can significantly enhance our understanding of neural activations. The experiments on LLaMA-2 and GPT-J demonstrate the framework's capability to reveal intricate details about neuron functions, including the discoverability of 'background neurons'. This analytical tool has the potential to improve model efficiency and effectiveness.\" Weaknesses='The explanation of certain technical terms and methodologies, such as the correlation measures and specific notations in Equations 7 and 8, could be improved for better clarity. There may also be concerns about the practical applicability of the findings and whether the framework can be generalized to a broader range of models or tasks.' Questions='How generalizable is the Neuron Predictability Lens framework to other transformer-based models or non-transformer architectures? Could the framework be adapted to analyze other layers beyond feed-forward networks?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths=\"The paper introduces an innovative framework called Neuron Predictability Lens (NPL) to analyze transformer-based language models, which can significantly enhance our understanding of neural activations. The experiments on LLaMA-2 and GPT-J demonstrate the framework's capability to reveal intricate details about neuron functions, including the discoverability of 'background neurons'. This analytical tool has the potential to improve model efficiency and effectiveness.\" Weaknesses='The explanation of certain technical terms and methodologies, such as the correlation measures and specific notations in Equations 7 and 8, could be improved for better clarity. There may also be concerns about the practical applicability of the findings and whether the framework can be generalized to a broader range of models or tasks.' Questions='How generalizable is the Neuron Predictability Lens framework to other transformer-based models or non-transformer architectures? Could the framework be adapted to analyze other layers beyond feed-forward networks?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The method offers a novel approach to video restoration that exploits pre-trained image diffusion models without requiring retraining, demonstrating significant generalization capabilities across different datasets and degradations. 2. The technique is versatile, as it can be used with any 2D restoration diffusion model, making it applicable to a wide range of video enhancement tasks.' Weaknesses='1. The reliance on a hierarchical latent warping strategy and hybrid correspondence mechanism may introduce complexity that could affect the scalability and efficiency of the method on larger datasets or real-time applications. 2. The paper primarily focuses on high-level results and could benefit from more detailed analysis, such as the computational overhead introduced by the proposed method and comparisons with more baseline techniques.' Questions='1. How does the model handle real-time restoration scenarios, given the complexity of the hierarchical and hybrid mechanisms? 2. What are the computational requirements for this method, particularly when applied to extensive video datasets?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The method offers a novel approach to video restoration that exploits pre-trained image diffusion models without requiring retraining, demonstrating significant generalization capabilities across different datasets and degradations. 2. The technique is versatile, as it can be used with any 2D restoration diffusion model, making it applicable to a wide range of video enhancement tasks.' Weaknesses='1. The reliance on a hierarchical latent warping strategy and hybrid correspondence mechanism may introduce complexity that could affect the scalability and efficiency of the method on larger datasets or real-time applications. 2. The paper primarily focuses on high-level results and could benefit from more detailed analysis, such as the computational overhead introduced by the proposed method and comparisons with more baseline techniques.' Questions='1. How does the model handle real-time restoration scenarios, given the complexity of the hierarchical and hybrid mechanisms? 2. What are the computational requirements for this method, particularly when applied to extensive video datasets?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper highlights an important privacy issue in graph prompt learning that has not been extensively addressed before, thus opening up new research avenues.\\n2. It proposes novel algorithms (DP-GPL and DP-GPL+W) that offer solutions to these privacy concerns by introducing differential privacy in graph prompt learning.\\n3. The evaluation validates the proposed methods across different GNN architectures and pre-training strategies, confirming their utility and effectiveness.' Weaknesses='1. The paper could provide more detailed theoretical analysis or proofs regarding the privacy guarantees of the proposed methods, enhancing the understanding of their practical implications.\\n2. The real-world applicability of the proposed algorithms might be limited by practical constraints such as computational overhead introduced by differential privacy mechanisms.\\n3. The current work might be expanded further in terms of exploring other potential privacy attacks that could affect graph prompt learning.' Questions='1. What are the specific trade-offs in terms of computational overhead when using the proposed methods compared to standard practices?\\n2. Can the proposed privacy-preserving methods be adapted easily to other types of graph data beyond those tested in the paper?\\n3. How do the proposed methods affect the accuracy and performance of GNN models on larger, more complex datasets?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper highlights an important privacy issue in graph prompt learning that has not been extensively addressed before, thus opening up new research avenues.\\n2. It proposes novel algorithms (DP-GPL and DP-GPL+W) that offer solutions to these privacy concerns by introducing differential privacy in graph prompt learning.\\n3. The evaluation validates the proposed methods across different GNN architectures and pre-training strategies, confirming their utility and effectiveness.' Weaknesses='1. The paper could provide more detailed theoretical analysis or proofs regarding the privacy guarantees of the proposed methods, enhancing the understanding of their practical implications.\\n2. The real-world applicability of the proposed algorithms might be limited by practical constraints such as computational overhead introduced by differential privacy mechanisms.\\n3. The current work might be expanded further in terms of exploring other potential privacy attacks that could affect graph prompt learning.' Questions='1. What are the specific trade-offs in terms of computational overhead when using the proposed methods compared to standard practices?\\n2. Can the proposed privacy-preserving methods be adapted easily to other types of graph data beyond those tested in the paper?\\n3. How do the proposed methods affect the accuracy and performance of GNN models on larger, more complex datasets?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper introduces a novel method, SMT, which identifies and updates the most significant sparse sub-matrices during fine-tuning, effectively reducing both computational and memory costs.\\n2. The empirical results indicate that SMT outperforms existing PEFT methods like LoRA and DoRA across a variety of tasks, especially on large language models such as LLaMA, which demonstrates its applicability and robustness.' Weaknesses='1. The abstraction could benefit from more clarity on the selection process of sparse sub-matrices, as well as the criteria used to identify the most significant ones.\\n2. The analysis of why SMT does not plateau or decline in performance, unlike LoRA or DoRA, could be more thorough or supported by theoretical insights to strengthen the claims.' Questions='1. How does the method ensure the selection of the optimal sparse sub-matrices, and what performance trade-offs occur with different selection strategies?\\n2. Can SMT be generalized or adapted for use beyond the specific models like LLaMA, and what adjustments would be necessary?\\n3. What are the specific tasks or scenarios where SMT would be less effective or encounter limitations?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper introduces a novel method, SMT, which identifies and updates the most significant sparse sub-matrices during fine-tuning, effectively reducing both computational and memory costs.\\n2. The empirical results indicate that SMT outperforms existing PEFT methods like LoRA and DoRA across a variety of tasks, especially on large language models such as LLaMA, which demonstrates its applicability and robustness.' Weaknesses='1. The abstraction could benefit from more clarity on the selection process of sparse sub-matrices, as well as the criteria used to identify the most significant ones.\\n2. The analysis of why SMT does not plateau or decline in performance, unlike LoRA or DoRA, could be more thorough or supported by theoretical insights to strengthen the claims.' Questions='1. How does the method ensure the selection of the optimal sparse sub-matrices, and what performance trade-offs occur with different selection strategies?\\n2. Can SMT be generalized or adapted for use beyond the specific models like LLaMA, and what adjustments would be necessary?\\n3. What are the specific tasks or scenarios where SMT would be less effective or encounter limitations?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents an innovative architecture, CrossModalNet, which applies advanced mathematical frameworks in multimodal medical image segmentation. Extensive theoretical and empirical support provided enhances the potential impact of the proposal. The introduction of a new metric for evaluating multimodal information flow and a strategy for addressing domain shifts are significant contributions.' Weaknesses='Despite the strengths, the paper could benefit from more clarity in explaining the complex mathematical proofs and the practical implementation of the Joint Adversarial Domain Adaptation framework. Additionally, some claims about the universal applicability of the method may require more empirical validation across diverse datasets.' Questions='How does CrossModalNet handle cases where one modality is missing or of significantly lower quality? Can the proposed CMIF metric be generalized to other domains outside of biomedical applications?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents an innovative architecture, CrossModalNet, which applies advanced mathematical frameworks in multimodal medical image segmentation. Extensive theoretical and empirical support provided enhances the potential impact of the proposal. The introduction of a new metric for evaluating multimodal information flow and a strategy for addressing domain shifts are significant contributions.' Weaknesses='Despite the strengths, the paper could benefit from more clarity in explaining the complex mathematical proofs and the practical implementation of the Joint Adversarial Domain Adaptation framework. Additionally, some claims about the universal applicability of the method may require more empirical validation across diverse datasets.' Questions='How does CrossModalNet handle cases where one modality is missing or of significantly lower quality? Can the proposed CMIF metric be generalized to other domains outside of biomedical applications?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a novel approach to multi-view clustering using a structural network and heterogeneous random walks, which improves clustering performance by utilizing multiple datasets. It demonstrates improvement through extensive experiments on real-world datasets.' Weaknesses='There is a lack of detailed analysis on time and space complexity, convergence proof, or theoretical guarantees. Comparative discussion of why this approach outperforms others would enhance understanding of its advantages.' Questions='What theoretical guarantees exist regarding the convergence and performance improvements of the proposed method? How does the algorithm handle scalability and complexity in practice?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a novel approach to multi-view clustering using a structural network and heterogeneous random walks, which improves clustering performance by utilizing multiple datasets. It demonstrates improvement through extensive experiments on real-world datasets.' Weaknesses='There is a lack of detailed analysis on time and space complexity, convergence proof, or theoretical guarantees. Comparative discussion of why this approach outperforms others would enhance understanding of its advantages.' Questions='What theoretical guarantees exist regarding the convergence and performance improvements of the proposed method? How does the algorithm handle scalability and complexity in practice?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper demonstrates significant theoretical advancements in the study of h-SVGD, including a unique solution to the hybrid Stein partial differential equation, which distinguishes this work from prior SVGD investigations. The empirical results provide robust evidence of h-SVGD's efficacy in addressing variance collapse in high-dimensional inference tasks, maintaining competitive performance without additional computational burden.\" Weaknesses=\"The paper's exploration of theoretical aspects could benefit from more intuitive explanations to facilitate comprehension, especially for readers less familiar with advanced mathematical concepts such as Wasserstein gradient flows. Some of the assumptions and statements could use better clarification and discussion to enhance understanding and validate their relevance to practical applications.\" Questions='1. Can you provide more insight into the specific conditions under which h-SVGD fails to converge to the target distribution in the mean field limit?\\n2. Are there particular problems or domains where the observed bias in the mean field limiting distribution might become a significant limitation?\\n3. How generalizable are these theoretical and empirical findings to other variants of SVGD?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper demonstrates significant theoretical advancements in the study of h-SVGD, including a unique solution to the hybrid Stein partial differential equation, which distinguishes this work from prior SVGD investigations. The empirical results provide robust evidence of h-SVGD's efficacy in addressing variance collapse in high-dimensional inference tasks, maintaining competitive performance without additional computational burden.\" Weaknesses=\"The paper's exploration of theoretical aspects could benefit from more intuitive explanations to facilitate comprehension, especially for readers less familiar with advanced mathematical concepts such as Wasserstein gradient flows. Some of the assumptions and statements could use better clarification and discussion to enhance understanding and validate their relevance to practical applications.\" Questions='1. Can you provide more insight into the specific conditions under which h-SVGD fails to converge to the target distribution in the mean field limit?\\n2. Are there particular problems or domains where the observed bias in the mean field limiting distribution might become a significant limitation?\\n3. How generalizable are these theoretical and empirical findings to other variants of SVGD?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths=\"The paper introduces a novel and effective approach, STAF, that addresses the spectral bias limitations of conventional ReLU networks by utilizing sinusoidal trainable activation functions. This enables better modeling and reconstruction of complex signals with high precision, which is evident from the paper's extensive empirical evaluations demonstrating higher accuracy and faster convergence compared to existing methods. The work is well-motivated and has significant implications for improving the reconstruction quality in continuous signals, making it valuable for computer graphics and related fields.\" Weaknesses='While the paper demonstrates improvements over several state-of-the-art methods, the complexity and computational requirements of implementing STAF are not entirely clear. Furthermore, the potential limitations in scalability or applicability to a wider range of signal types are not addressed comprehensively.' Questions='How does STAF perform in high-dimensional or more complex environments, and are there any scalability concerns? Also, what are the specific computational costs associated with implementing STAF compared to ReLU or other activation functions like those used in KAN, WIRE, SIREN, or Fourier features?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths=\"The paper introduces a novel and effective approach, STAF, that addresses the spectral bias limitations of conventional ReLU networks by utilizing sinusoidal trainable activation functions. This enables better modeling and reconstruction of complex signals with high precision, which is evident from the paper's extensive empirical evaluations demonstrating higher accuracy and faster convergence compared to existing methods. The work is well-motivated and has significant implications for improving the reconstruction quality in continuous signals, making it valuable for computer graphics and related fields.\" Weaknesses='While the paper demonstrates improvements over several state-of-the-art methods, the complexity and computational requirements of implementing STAF are not entirely clear. Furthermore, the potential limitations in scalability or applicability to a wider range of signal types are not addressed comprehensively.' Questions='How does STAF perform in high-dimensional or more complex environments, and are there any scalability concerns? Also, what are the specific computational costs associated with implementing STAF compared to ReLU or other activation functions like those used in KAN, WIRE, SIREN, or Fourier features?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. GUIDE method is a simple yet effective approach to align LLM outputs with user instructions by increasing attention on instruction tokens. 2. Innovative influence metric introduced that highlights user instruction impact through transformer layers.\\n3. Demonstrates significant improvement in accuracy for instruction-following compared to alternatives.' Weaknesses='1. Missing comparisons to very recent and relevant works that address similar problems, which could provide a more comprehensive evaluation. 2. The effect of GUIDE on output quality is not discussed, which could be important for understanding trade-offs.\\n3. Influence metric might not comprehensively capture all facets of instruction alignment efficacy.' Questions='1. How does GUIDE method compare in effectiveness and implementation complexity with \\nother recent methods addressing instruction alignment?\\n2. What impact does GUIDE have on the overall output quality from LLMs?\\n3. Is there any evaluation on the computational cost or runtime efficiency when using GUIDE?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. GUIDE method is a simple yet effective approach to align LLM outputs with user instructions by increasing attention on instruction tokens. 2. Innovative influence metric introduced that highlights user instruction impact through transformer layers.\\n3. Demonstrates significant improvement in accuracy for instruction-following compared to alternatives.' Weaknesses='1. Missing comparisons to very recent and relevant works that address similar problems, which could provide a more comprehensive evaluation. 2. The effect of GUIDE on output quality is not discussed, which could be important for understanding trade-offs.\\n3. Influence metric might not comprehensively capture all facets of instruction alignment efficacy.' Questions='1. How does GUIDE method compare in effectiveness and implementation complexity with \\nother recent methods addressing instruction alignment?\\n2. What impact does GUIDE have on the overall output quality from LLMs?\\n3. Is there any evaluation on the computational cost or runtime efficiency when using GUIDE?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper successfully integrates MCTS into GFlowNet training, demonstrating improved trajectory sampling. The work provides theoretical proofs and validates the approach in multiple discrete domains, suggesting potential for wide applicability.' Weaknesses='While the adaptation of MCTS to GFlowNet is novel, the clarity of explanation, particularly around the experimental setup and results, could be improved. Some results lack statistical significance testing, making it difficult to fully assess their impact.' Questions='What are the specific challenges or limitations encountered when integrating MCTS with different GFlowNet parameterizations? How does the approach handle large state spaces efficiently?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper successfully integrates MCTS into GFlowNet training, demonstrating improved trajectory sampling. The work provides theoretical proofs and validates the approach in multiple discrete domains, suggesting potential for wide applicability.' Weaknesses='While the adaptation of MCTS to GFlowNet is novel, the clarity of explanation, particularly around the experimental setup and results, could be improved. Some results lack statistical significance testing, making it difficult to fully assess their impact.' Questions='What are the specific challenges or limitations encountered when integrating MCTS with different GFlowNet parameterizations? How does the approach handle large state spaces efficiently?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='The proposed MotionDreamer framework effectively addresses the challenge of overfitting in animation generation from limited data by introducing a localized masked modeling paradigm, novel quantization-based token embedding, and a sliding window local attention mechanism. The framework demonstrates significant performance improvements over existing GAN or Diffusion-based methods in both faithfulness and diversity of generated animations, as well as successfully applying to downstream tasks like temporal editing, crowd synthesis, and beat-aligned dance generation. The intention to publicly release the implementation and models adds further value.' Weaknesses=\"The method's generalizability across diverse motion datasets and real-world applications remains uncertain without broader validations. The reliance on a single reference motion might limit the model's versatility in synthesizing more complex or varied motion sequences. The abstract lacks specific quantitative results that demonstrate how significant the improvements are compared to existing methods.\" Questions='1. How does MotionDreamer handle the variability in motion styles across different reference motions?\\n2. What are the computational constraints or efficiency of the proposed localized masked modeling approach compared to traditional GAN or Diffusion-based methods?\\n3. How well does the quantization-based approach scale with more complex motions or longer duration references?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='The proposed MotionDreamer framework effectively addresses the challenge of overfitting in animation generation from limited data by introducing a localized masked modeling paradigm, novel quantization-based token embedding, and a sliding window local attention mechanism. The framework demonstrates significant performance improvements over existing GAN or Diffusion-based methods in both faithfulness and diversity of generated animations, as well as successfully applying to downstream tasks like temporal editing, crowd synthesis, and beat-aligned dance generation. The intention to publicly release the implementation and models adds further value.' Weaknesses=\"The method's generalizability across diverse motion datasets and real-world applications remains uncertain without broader validations. The reliance on a single reference motion might limit the model's versatility in synthesizing more complex or varied motion sequences. The abstract lacks specific quantitative results that demonstrate how significant the improvements are compared to existing methods.\" Questions='1. How does MotionDreamer handle the variability in motion styles across different reference motions?\\n2. What are the computational constraints or efficiency of the proposed localized masked modeling approach compared to traditional GAN or Diffusion-based methods?\\n3. How well does the quantization-based approach scale with more complex motions or longer duration references?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The paper addresses a significant challenge in large language models: the limitation of generating long outputs. By identifying that this constraint is largely due to training dataset limitations, the research offers a novel perspective on enhancing output length capabilities.\\n2. The introduction of the AgentWrite pipeline is innovative, effectively decomposing ultra-long generation tasks, leveraging current LLM capabilities in a pragmatic way.\\n3. The creation of the LongWriter-6k dataset and LongBench-Write benchmark contributes valuable resources to the research community, enabling further exploration and evaluation of long-output generation.\\n4. The paper demonstrates that existing long context LLMs have untapped potential for output length, which is a valuable insight for the development of future models.' Weaknesses='1. The experimental setup, while controlled, could benefit from additional evaluations or comparisons with alternative methods to further substantiate the claims. Specifically, demonstrating competitive efficacy compared to extensive manual fine-tuning would strengthen the contribution.\\n2. While addressing the output length limitation, the paper does not deeply explore the potential trade-offs in terms of computational efficiency or training costs when adopting the proposed methodologies.\\n3. The dependency on agent-based task decomposition might limit the general applicability of the proposed approach across different LLM architectures or tasks, especially ones that do not lend themselves easily to decomposition.' Questions='1. How does the computational cost of the AgentWrite pipeline compare to more traditional approaches of generating lengthy outputs?\\n2. Are there specific characteristics or types of inputs where AgentWrite might not be effective or could encounter difficulties?\\n3. Given the reliance on subtask decomposition, how does the approach handle tasks where context continuity is crucial and cannot be easily segmented?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The paper addresses a significant challenge in large language models: the limitation of generating long outputs. By identifying that this constraint is largely due to training dataset limitations, the research offers a novel perspective on enhancing output length capabilities.\\n2. The introduction of the AgentWrite pipeline is innovative, effectively decomposing ultra-long generation tasks, leveraging current LLM capabilities in a pragmatic way.\\n3. The creation of the LongWriter-6k dataset and LongBench-Write benchmark contributes valuable resources to the research community, enabling further exploration and evaluation of long-output generation.\\n4. The paper demonstrates that existing long context LLMs have untapped potential for output length, which is a valuable insight for the development of future models.' Weaknesses='1. The experimental setup, while controlled, could benefit from additional evaluations or comparisons with alternative methods to further substantiate the claims. Specifically, demonstrating competitive efficacy compared to extensive manual fine-tuning would strengthen the contribution.\\n2. While addressing the output length limitation, the paper does not deeply explore the potential trade-offs in terms of computational efficiency or training costs when adopting the proposed methodologies.\\n3. The dependency on agent-based task decomposition might limit the general applicability of the proposed approach across different LLM architectures or tasks, especially ones that do not lend themselves easily to decomposition.' Questions='1. How does the computational cost of the AgentWrite pipeline compare to more traditional approaches of generating lengthy outputs?\\n2. Are there specific characteristics or types of inputs where AgentWrite might not be effective or could encounter difficulties?\\n3. Given the reliance on subtask decomposition, how does the approach handle tasks where context continuity is crucial and cannot be easily segmented?'\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Expecting property name enclosed in double quotes: line 10 column 1 (char 1061)\n",
      "Content:\n",
      "{\n",
      "  \"Soundness\": 4,\n",
      "  \"Presentation\": 4,\n",
      "  \"Contribution\": 4,\n",
      "  \"Rating\": 8,\n",
      "  \"Confidence\": 4,\n",
      "  \"Strengths\": \"The paper presents a novel Ada-K routing strategy that dynamically adjusts the number of activated experts in Mixture-of-Experts architectures. This approach significantly reduces computational costs while maintaining or improving model performance, as shown by comprehensive evaluations demonstrating substantial reductions in FLOPs and inference speedup.\",\n",
      "  \"Weaknesses\": \"The paper may benefit from further exploration of limitations or potential pitfalls in applying the Ada-K routing strategy, such as its adaptability to varying model architectures or datasets beyond those evaluated. There might also be a need for more detailed discussion on the training overhead introduced by the learnable allocator modules.\",\n",
      "  \"Questions\": \"How does the Ada-K routing strategy perform on tasks or models not covered in the extensive evaluations? Is there a trade-off between the complexity of the allocator modules and the efficiency gains achieved?\",\n",
      "}\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='1. The algorithm, LoCoDL, effectively combines local training and compression to significantly reduce communication costs in distributed and federated learning settings.\\n2. Provides theoretical guarantees for the communication efficiency and effectiveness in terms of the condition number and model dimension.\\n3. The algorithm is versatile, operating with various types of unbiased compressors including sparsification and quantization.' Weaknesses='1. The presentation could be improved for clarity, particularly in the explanation of the theoretical aspects and practical implementation.\\n2. It is not clear how the approach performs in non-heterogeneous regimes or with non-strongly convex functions, which may limit its applicability in general scenarios.' Questions='1. How does LoCoDL perform with non-strongly convex functions or in homogeneous settings?\\n2. Are there specific scenarios or applications where LoCoDL encounters limitations?\\n3. What are the computational trade-offs associated with using different types of compressors with LoCoDL?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='1. The algorithm, LoCoDL, effectively combines local training and compression to significantly reduce communication costs in distributed and federated learning settings.\\n2. Provides theoretical guarantees for the communication efficiency and effectiveness in terms of the condition number and model dimension.\\n3. The algorithm is versatile, operating with various types of unbiased compressors including sparsification and quantization.' Weaknesses='1. The presentation could be improved for clarity, particularly in the explanation of the theoretical aspects and practical implementation.\\n2. It is not clear how the approach performs in non-heterogeneous regimes or with non-strongly convex functions, which may limit its applicability in general scenarios.' Questions='1. How does LoCoDL perform with non-strongly convex functions or in homogeneous settings?\\n2. Are there specific scenarios or applications where LoCoDL encounters limitations?\\n3. What are the computational trade-offs associated with using different types of compressors with LoCoDL?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper provides a theoretical framework and empirical evidence on how architectural choices impact transformer's behavior even in simple tasks. The work gives insights into two different counting strategies and thoroughly examines the impact of architectural decisions, such as embedding size and attention mechanisms.\" Weaknesses='The study is limited to a simple histogram counting task, which may not generalize to more complex real-world applications. The findings are highly specific to the settings and may not transfer directly to larger models or more complicated tasks.' Questions='How might these insights inform architectural decisions for more complex tasks? Could more complex architectures lead to new emergent counting strategies beyond the two identified?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper provides a theoretical framework and empirical evidence on how architectural choices impact transformer's behavior even in simple tasks. The work gives insights into two different counting strategies and thoroughly examines the impact of architectural decisions, such as embedding size and attention mechanisms.\" Weaknesses='The study is limited to a simple histogram counting task, which may not generalize to more complex real-world applications. The findings are highly specific to the settings and may not transfer directly to larger models or more complicated tasks.' Questions='How might these insights inform architectural decisions for more complex tasks? Could more complex architectures lead to new emergent counting strategies beyond the two identified?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The creation of MedTrinity-25M, a large-scale, richly annotated dataset, represents a significant step forward for multimodal medical research, providing unprecedented resources for various tasks such as image captioning and disease classification.\\n2. The novel method of generating multigranular annotations without paired text descriptions is innovative and expands the potential for automated medical data analysis.\\n3. The dataset is diverse, covering over 65 diseases and multiple modalities, enhancing its applicability to a wide range of medical research scenarios.\\n4. Demonstrates state-of-the-art performance with LLaVA-Tri, reflecting the value of the dataset for training powerful AI models in the medical domain.' Weaknesses='1. The reliance on automated pipelines for multigranular annotations may lead to concerns about the accuracy and reliability of the generated labels without expert validation.\\n2. The scalability and adaptability of the dataset and annotation methods to novel diseases and modalities are not clearly addressed.\\n3. The paper does not extensively discuss the potential biases inherent in the data collected from over 30 different sources, which may affect the generalizability and fairness of models trained on this dataset.' Questions='1. How does the automated annotation process ensure high accuracy and reliability, and is there a mechanism for validation against expert-labeled data?\\n2. Can the technologies and methodologies described in the paper be adapted to include novel modalities or newly emerging disease categories effectively?\\n3. What measures have been taken to address and mitigate potential biases introduced by sourcing data from a wide variety of sources?\\n4. How does MedTrinity-25M compare in terms of annotation quality and task performance to other existing labeled datasets in the medical domain? Is there benchmark testing included for such comparisons?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The creation of MedTrinity-25M, a large-scale, richly annotated dataset, represents a significant step forward for multimodal medical research, providing unprecedented resources for various tasks such as image captioning and disease classification.\\n2. The novel method of generating multigranular annotations without paired text descriptions is innovative and expands the potential for automated medical data analysis.\\n3. The dataset is diverse, covering over 65 diseases and multiple modalities, enhancing its applicability to a wide range of medical research scenarios.\\n4. Demonstrates state-of-the-art performance with LLaVA-Tri, reflecting the value of the dataset for training powerful AI models in the medical domain.' Weaknesses='1. The reliance on automated pipelines for multigranular annotations may lead to concerns about the accuracy and reliability of the generated labels without expert validation.\\n2. The scalability and adaptability of the dataset and annotation methods to novel diseases and modalities are not clearly addressed.\\n3. The paper does not extensively discuss the potential biases inherent in the data collected from over 30 different sources, which may affect the generalizability and fairness of models trained on this dataset.' Questions='1. How does the automated annotation process ensure high accuracy and reliability, and is there a mechanism for validation against expert-labeled data?\\n2. Can the technologies and methodologies described in the paper be adapted to include novel modalities or newly emerging disease categories effectively?\\n3. What measures have been taken to address and mitigate potential biases introduced by sourcing data from a wide variety of sources?\\n4. How does MedTrinity-25M compare in terms of annotation quality and task performance to other existing labeled datasets in the medical domain? Is there benchmark testing included for such comparisons?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed Rotated Runtime Smooth (RRS) approach presents an innovative way to handle outliers in activations for more efficient INT4 quantization, significantly improving perplexity scores in benchmarks like LLaMA and Qwen families.' Weaknesses='The paper does not provide extensive details on the computational and latency trade-offs introduced by the RRS method. Additionally, more comparison with existing methods on diverse tasks could strengthen the results.' Questions='What are the specific latency and computational cost implications of using RRS at runtime, and how do they compare with existing methods?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed Rotated Runtime Smooth (RRS) approach presents an innovative way to handle outliers in activations for more efficient INT4 quantization, significantly improving perplexity scores in benchmarks like LLaMA and Qwen families.' Weaknesses='The paper does not provide extensive details on the computational and latency trade-offs introduced by the RRS method. Additionally, more comparison with existing methods on diverse tasks could strengthen the results.' Questions='What are the specific latency and computational cost implications of using RRS at runtime, and how do they compare with existing methods?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper introduces a novel task aimed at effectively leveraging static image datasets for one-shot medical video object segmentation, addressing a significant gap in medical imaging where annotated video data is scarce. \\n2. The proposed framework employs a unique combination of a training strategy on images and test-time adaptation for medical videos, showcasing a creative solution to a key challenge in medical video analysis.\\n3. The introduction of a new dataset (OS-I2V-Seg) that includes a comprehensive set of image/frame-mask pairs, encouraging further research in this domain and establishing baseline performance.' Weaknesses='1. The details regarding the training phase, such as the specifics of the one-shot segmentation model and the overall architecture, are insufficiently detailed, limiting reproducibility and a comprehensive understanding of the approach.\\n2. While the strategy to adapt a model trained on static images to video is interesting, there may be challenges in maintaining the generalization capabilities across various video domains, which might limit its effectiveness beyond the medical imaging context.\\n3. The efficacy of temporal awareness via memory mechanism in segmenting new frames might vary significantly depending on vsideo complexity and the overlap with training imagery, potentially impacting its general applicability.' Questions='1. What specific features or characteristics of the static image datasets most significantly influence the successful adaptation of the model for video segmentation in your approach?\\n2. How does the self-distillation strategy ensure maintaining generalization capabilities, especially if there is a stark domain shift between images and video data?\\n3. Could there be potential adaptations or extensions to the memory mechanism to improve performance in other low-data regimes or in different kinds of video datasets beyond the medical domain?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper introduces a novel task aimed at effectively leveraging static image datasets for one-shot medical video object segmentation, addressing a significant gap in medical imaging where annotated video data is scarce. \\n2. The proposed framework employs a unique combination of a training strategy on images and test-time adaptation for medical videos, showcasing a creative solution to a key challenge in medical video analysis.\\n3. The introduction of a new dataset (OS-I2V-Seg) that includes a comprehensive set of image/frame-mask pairs, encouraging further research in this domain and establishing baseline performance.' Weaknesses='1. The details regarding the training phase, such as the specifics of the one-shot segmentation model and the overall architecture, are insufficiently detailed, limiting reproducibility and a comprehensive understanding of the approach.\\n2. While the strategy to adapt a model trained on static images to video is interesting, there may be challenges in maintaining the generalization capabilities across various video domains, which might limit its effectiveness beyond the medical imaging context.\\n3. The efficacy of temporal awareness via memory mechanism in segmenting new frames might vary significantly depending on vsideo complexity and the overlap with training imagery, potentially impacting its general applicability.' Questions='1. What specific features or characteristics of the static image datasets most significantly influence the successful adaptation of the model for video segmentation in your approach?\\n2. How does the self-distillation strategy ensure maintaining generalization capabilities, especially if there is a stark domain shift between images and video data?\\n3. Could there be potential adaptations or extensions to the memory mechanism to improve performance in other low-data regimes or in different kinds of video datasets beyond the medical domain?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces DCScore, a novel method for evaluating the diversity of LLM-generated datasets, addressing a pertinent gap in current research. The method is theoretically grounded and offers a unified framework that incorporates existing diversity evaluation approaches, with an added advantage of reduced computational costs.' Weaknesses='The practical significance and real-world applications of evaluating dataset diversity with DCScore are unclear. Further validation with a wider range of scenarios and datasets would strengthen the understanding of its generalizability and effectiveness.' Questions='How can the findings from this diversity evaluation method be applied to improve the generation of LLM training data or other practical applications? Are there specific domains or use cases where DCScore has been particularly insightful?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces DCScore, a novel method for evaluating the diversity of LLM-generated datasets, addressing a pertinent gap in current research. The method is theoretically grounded and offers a unified framework that incorporates existing diversity evaluation approaches, with an added advantage of reduced computational costs.' Weaknesses='The practical significance and real-world applications of evaluating dataset diversity with DCScore are unclear. Further validation with a wider range of scenarios and datasets would strengthen the understanding of its generalizability and effectiveness.' Questions='How can the findings from this diversity evaluation method be applied to improve the generation of LLM training data or other practical applications? Are there specific domains or use cases where DCScore has been particularly insightful?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses a well-defined problem in the field of model pruning and presents a novel approach, MoreauPruner, which is grounded in optimization analysis and demonstrates robustness against weight perturbations.\\n2. Extensive evaluation of the MoreauPruner algorithm on several prominent LLMs, showing how robust importance estimation contributes to its effectiveness compared to existing pruning methods.' Weaknesses='1. The motivation for requiring robust pruning masks against small perturbations may need further clarification, as there can be multiple valid pruning masks for a model, and the need for robustness in this context is not entirely clear.\\n2. The performance improvements over existing methods are not very large, which might limit the impact of the proposed method.\\n3. The selection of target models for evaluation is somewhat limited, mainly focusing on the LLaMA family and lacking diversity with other model architectures.' Questions='1. Can MoreauPruner be adapted for use with models outside the LLaMA family, such as Mistral, OPT, or BLOOM?\\n2. How does the computational cost of obtaining pruning masks with MoreauPruner compare to traditional methods?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses a well-defined problem in the field of model pruning and presents a novel approach, MoreauPruner, which is grounded in optimization analysis and demonstrates robustness against weight perturbations.\\n2. Extensive evaluation of the MoreauPruner algorithm on several prominent LLMs, showing how robust importance estimation contributes to its effectiveness compared to existing pruning methods.' Weaknesses='1. The motivation for requiring robust pruning masks against small perturbations may need further clarification, as there can be multiple valid pruning masks for a model, and the need for robustness in this context is not entirely clear.\\n2. The performance improvements over existing methods are not very large, which might limit the impact of the proposed method.\\n3. The selection of target models for evaluation is somewhat limited, mainly focusing on the LLaMA family and lacking diversity with other model architectures.' Questions='1. Can MoreauPruner be adapted for use with models outside the LLaMA family, such as Mistral, OPT, or BLOOM?\\n2. How does the computational cost of obtaining pruning masks with MoreauPruner compare to traditional methods?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper addresses a relevant and timely issue by introducing a new benchmark, LAION-C, which is designed to assess OOD robustness in models trained on web-scale datasets, filling a significant gap in current evaluation methods.\\n2. Comprehensive evaluation: The authors thoroughly evaluate contemporary models using the new benchmark and effectively compare model performance against human robustness, providing valuable insights into the evolving capabilities of modern models.\\n3. Strong empirical foundation: The introduction of novel distortion types not commonly found in existing datasets and the rigorous testing against human observers add robustness to the findings presented.' Weaknesses='1. While the construction of the LAION-C benchmark is justified, the paper could provide more detailed analysis on the selection criteria for distortion types and the underlying rationale for each choice.\\n2. The discussion on model improvements could be more detailed, providing deeper insights into how the benchmark can guide future model development and architectural choices.' Questions='1. How were the six novel distortion types specifically chosen and designed, and were any experimental methods used to ensure these types remain genuinely OOD for web-scale datasets?\\n2. How does the performance of models on LAION-C relate to practical improvements in real-world deployment scenarios where OOD robustness is critical? Could the benchmark guide specific adaptations or improvements in model architectures?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper addresses a relevant and timely issue by introducing a new benchmark, LAION-C, which is designed to assess OOD robustness in models trained on web-scale datasets, filling a significant gap in current evaluation methods.\\n2. Comprehensive evaluation: The authors thoroughly evaluate contemporary models using the new benchmark and effectively compare model performance against human robustness, providing valuable insights into the evolving capabilities of modern models.\\n3. Strong empirical foundation: The introduction of novel distortion types not commonly found in existing datasets and the rigorous testing against human observers add robustness to the findings presented.' Weaknesses='1. While the construction of the LAION-C benchmark is justified, the paper could provide more detailed analysis on the selection criteria for distortion types and the underlying rationale for each choice.\\n2. The discussion on model improvements could be more detailed, providing deeper insights into how the benchmark can guide future model development and architectural choices.' Questions='1. How were the six novel distortion types specifically chosen and designed, and were any experimental methods used to ensure these types remain genuinely OOD for web-scale datasets?\\n2. How does the performance of models on LAION-C relate to practical improvements in real-world deployment scenarios where OOD robustness is critical? Could the benchmark guide specific adaptations or improvements in model architectures?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The paper makes a novel contribution by connecting GNNs to Koopman theory and Dynamic Mode Decomposition, offering a new perspective for analyzing and improving GNNs.\\n2. Extensive empirical validation supports the proposed methodology across various graph learning tasks, showing state-of-the-art performance and practical significance.\\n3. The proposed integration of domain-specific constraints, such as symmetry, provides a method for enhancing GNN models by aligning them with physical system properties, thereby improving their applicability.' Weaknesses='1. The theoretical connection between the DMD-estimated operator and the original dynamic operator could be elaborated further for better clarity.\\n2. The complexity and overhead introduced by incorporating DMD into GNN frameworks are not thoroughly discussed, which could raise concerns about scalability and computational efficiency.\\n3. While the use of DMD is innovative, the approach may seem complex for practitioners who are not familiar with dynamical systems or Koopman theory, potentially limiting accessibility and adoption.' Questions='1. How does the proposed DMD-GNN approach scale with very large graphs compared to traditional GNNs in terms of computational resources?\\n2. Can the incorporation of domain-specific constraints be generalized beyond the examples provided (e.g., symmetry), and if so, how would this affect the generality of the approach?\\n3. How resilient is the DMD-GNN framework to noise or incomplete graph data, and are there specific adjustments needed to maintain robustness in such scenarios?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The paper makes a novel contribution by connecting GNNs to Koopman theory and Dynamic Mode Decomposition, offering a new perspective for analyzing and improving GNNs.\\n2. Extensive empirical validation supports the proposed methodology across various graph learning tasks, showing state-of-the-art performance and practical significance.\\n3. The proposed integration of domain-specific constraints, such as symmetry, provides a method for enhancing GNN models by aligning them with physical system properties, thereby improving their applicability.' Weaknesses='1. The theoretical connection between the DMD-estimated operator and the original dynamic operator could be elaborated further for better clarity.\\n2. The complexity and overhead introduced by incorporating DMD into GNN frameworks are not thoroughly discussed, which could raise concerns about scalability and computational efficiency.\\n3. While the use of DMD is innovative, the approach may seem complex for practitioners who are not familiar with dynamical systems or Koopman theory, potentially limiting accessibility and adoption.' Questions='1. How does the proposed DMD-GNN approach scale with very large graphs compared to traditional GNNs in terms of computational resources?\\n2. Can the incorporation of domain-specific constraints be generalized beyond the examples provided (e.g., symmetry), and if so, how would this affect the generality of the approach?\\n3. How resilient is the DMD-GNN framework to noise or incomplete graph data, and are there specific adjustments needed to maintain robustness in such scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper provides insights into the practical application of influence functions for large models by addressing key hyperparameter tuning challenges. It suggests a way to select hyperparameters based on spectral properties, which could be of practical utility. The empirical validation against PBRF provides a comparative understanding of the method's performance.\" Weaknesses='The claims on the practicality of LiSSA for large models need stronger empirical validation across a diverse set of models. While the theoretical insights are useful, the lack of clarity on the broader applicability and limitations might limit the utility of the findings. The paper may also benefit from clearer exposition, particularly in the sections detailing empirical results.' Questions='How do the proposed hyperparameter selection strategies affect the computational cost compared to traditional methods? Is there a trade-off between convergence rate and computational efficiency?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper provides insights into the practical application of influence functions for large models by addressing key hyperparameter tuning challenges. It suggests a way to select hyperparameters based on spectral properties, which could be of practical utility. The empirical validation against PBRF provides a comparative understanding of the method's performance.\" Weaknesses='The claims on the practicality of LiSSA for large models need stronger empirical validation across a diverse set of models. While the theoretical insights are useful, the lack of clarity on the broader applicability and limitations might limit the utility of the findings. The paper may also benefit from clearer exposition, particularly in the sections detailing empirical results.' Questions='How do the proposed hyperparameter selection strategies affect the computational cost compared to traditional methods? Is there a trade-off between convergence rate and computational efficiency?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper provides a rigorous analysis of the online learning framework in brokerage, with clear theoretical underpinnings and results. The introduction of contextual information to the broker's decision-making process and the distinction between full feedback and partial feedback scenarios are particularly notable. The matching of upper and lower bounds for regret in both scenarios demonstrates a thorough understanding and exploration of the problem.\" Weaknesses='The assumptions about the market model and trader valuations might be considered too specific or unrealistic in real-world applications. Additionally, the presentation could be improved, as some technical notations and explanations may be difficult for readers to understand without extensive experience in the field. The novelty in comparison to existing literature could be more explicitly highlighted.' Questions='How might this model be adjusted to account for more realistic assumptions about market dynamics and trader behavior? Are there opportunities to validate the theoretical results with empirical data from actual brokerage scenarios?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper provides a rigorous analysis of the online learning framework in brokerage, with clear theoretical underpinnings and results. The introduction of contextual information to the broker's decision-making process and the distinction between full feedback and partial feedback scenarios are particularly notable. The matching of upper and lower bounds for regret in both scenarios demonstrates a thorough understanding and exploration of the problem.\" Weaknesses='The assumptions about the market model and trader valuations might be considered too specific or unrealistic in real-world applications. Additionally, the presentation could be improved, as some technical notations and explanations may be difficult for readers to understand without extensive experience in the field. The novelty in comparison to existing literature could be more explicitly highlighted.' Questions='How might this model be adjusted to account for more realistic assumptions about market dynamics and trader behavior? Are there opportunities to validate the theoretical results with empirical data from actual brokerage scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper proposes a novel solution to the challenge of quantizing State Space Models (SSMs), which have garnered attention for their efficiency over traditional Transformer models. The proposed static 8-bit quantization method is well-tailored for the unique characteristics of SSMs and effectively addresses issues like output activation outliers with innovative use of Hadamard transforms. The quantization process demonstrates significant improvements in model deployment feasibility on both cloud and edge platforms, as evidenced by the experiments which show minimal accuracy loss and significant reduction in generation latency. This work offers a practical and scalable approach to deploying large language models with SSM architectures in resource-constrained environments.' Weaknesses='While the paper provides impressive results, it relies on hardware-specific performance measurements that may not be directly generalizable to all environments. The implementation details of the quantization method, such as the parameters for percentile-based clipping, are not fully specified, potentially affecting reproducibility. Furthermore, the paper claims benefits of quantization that extend to cloud-based deployments but primarily provides experimental results from edge device testing, which could lead to constrained applicability insights.' Questions='Does the percentile-based clipping approach require manual tuning for different models or datasets, and if so, how sensitive is the method to these parameter settings? Specifics on guidelines for selecting the clipping threshold would be beneficial for reproducibility and adoption. Additionally, could the authors expand on the potential challenges of adapting this technique for real-time cloud computing scenarios, as opposed to edge deployment, given the differences in computational resources and processing constraints?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper proposes a novel solution to the challenge of quantizing State Space Models (SSMs), which have garnered attention for their efficiency over traditional Transformer models. The proposed static 8-bit quantization method is well-tailored for the unique characteristics of SSMs and effectively addresses issues like output activation outliers with innovative use of Hadamard transforms. The quantization process demonstrates significant improvements in model deployment feasibility on both cloud and edge platforms, as evidenced by the experiments which show minimal accuracy loss and significant reduction in generation latency. This work offers a practical and scalable approach to deploying large language models with SSM architectures in resource-constrained environments.' Weaknesses='While the paper provides impressive results, it relies on hardware-specific performance measurements that may not be directly generalizable to all environments. The implementation details of the quantization method, such as the parameters for percentile-based clipping, are not fully specified, potentially affecting reproducibility. Furthermore, the paper claims benefits of quantization that extend to cloud-based deployments but primarily provides experimental results from edge device testing, which could lead to constrained applicability insights.' Questions='Does the percentile-based clipping approach require manual tuning for different models or datasets, and if so, how sensitive is the method to these parameter settings? Specifics on guidelines for selecting the clipping threshold would be beneficial for reproducibility and adoption. Additionally, could the authors expand on the potential challenges of adapting this technique for real-time cloud computing scenarios, as opposed to edge deployment, given the differences in computational resources and processing constraints?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper adeptly addresses the complex issue of multi-label node classification by introducing a novel approach that decomposes GNN processes into propagation and transformation, providing a new perspective on label influence. Additionally, it proposes Label Influence Propagation (LIP), which consistently shows superior performance compared to state-of-the-art methods on benchmark datasets.' Weaknesses=\"The paper's approach may be somewhat limited in scope, focusing primarily on label influence without fully evaluating the broader implications or applications of this methodology outside the specific datasets used. Further investigation could be beneficial to verify the robustness and adaptability of their model across a wider range of graph structures and domains. Additionally, the presentation could be improved by including more visual aids or simpler explanations for complex concepts to enhance reader understanding.\" Questions='How does Label Influence Propagation (LIP) handle noisy or conflicting labels in practice, and are there any mechanisms within the proposed framework to address potential mislabeling or errors in the data?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper adeptly addresses the complex issue of multi-label node classification by introducing a novel approach that decomposes GNN processes into propagation and transformation, providing a new perspective on label influence. Additionally, it proposes Label Influence Propagation (LIP), which consistently shows superior performance compared to state-of-the-art methods on benchmark datasets.' Weaknesses=\"The paper's approach may be somewhat limited in scope, focusing primarily on label influence without fully evaluating the broader implications or applications of this methodology outside the specific datasets used. Further investigation could be beneficial to verify the robustness and adaptability of their model across a wider range of graph structures and domains. Additionally, the presentation could be improved by including more visual aids or simpler explanations for complex concepts to enhance reader understanding.\" Questions='How does Label Influence Propagation (LIP) handle noisy or conflicting labels in practice, and are there any mechanisms within the proposed framework to address potential mislabeling or errors in the data?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths='1. The paper presents a new approach that combines Kolmogorov-Arnold Networks with Bernstein polynomials to improve both performance and interpretability.\\n2. The proposed method is evaluated across multiple domains, demonstrating its generalizability and effectiveness in diverse application areas.\\n3. The theoretical foundations and motivations for using Bernstein polynomials are well-presented, leveraging the Weierstrass approximation theorem.' Weaknesses=\"1. The improvement over existing methods and other KAN variants is marginal based on the experimental results provided, which limits the impact of the contribution.\\n2. Some claims regarding the benefits and properties of the approach are not backed by strong empirical evidence or detailed analyses.\\n3. The paper lacks a comparison with state-of-the-art methods specific to the datasets used, focusing instead on internal comparisons, which may not provide a full picture of the proposed method's strengths.\" Questions=\"1. How does the choice of the Bernstein polynomial degree n affect the model's performance across different tasks, and is there a recommended method for selecting this parameter?\\n2. Can the robustness and flexibility benefits claimed be empirically demonstrated more clearly in the experiments?\\n3. How does the proposed framework perform in comparison to state-of-the-art non-KAN methods on standard benchmarks like CIFAR-10 and CIFAR-100?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths='1. The paper presents a new approach that combines Kolmogorov-Arnold Networks with Bernstein polynomials to improve both performance and interpretability.\\n2. The proposed method is evaluated across multiple domains, demonstrating its generalizability and effectiveness in diverse application areas.\\n3. The theoretical foundations and motivations for using Bernstein polynomials are well-presented, leveraging the Weierstrass approximation theorem.' Weaknesses=\"1. The improvement over existing methods and other KAN variants is marginal based on the experimental results provided, which limits the impact of the contribution.\\n2. Some claims regarding the benefits and properties of the approach are not backed by strong empirical evidence or detailed analyses.\\n3. The paper lacks a comparison with state-of-the-art methods specific to the datasets used, focusing instead on internal comparisons, which may not provide a full picture of the proposed method's strengths.\" Questions=\"1. How does the choice of the Bernstein polynomial degree n affect the model's performance across different tasks, and is there a recommended method for selecting this parameter?\\n2. Can the robustness and flexibility benefits claimed be empirically demonstrated more clearly in the experiments?\\n3. How does the proposed framework perform in comparison to state-of-the-art non-KAN methods on standard benchmarks like CIFAR-10 and CIFAR-100?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=4 Strengths='The proposed method significantly advances the state-of-the-art in generating articulated 3D models from single images, providing a more scalable and practical approach. The use of a diffusion model to handle ambiguity in part shape and motion from limited input is innovative. Experiments show a considerable improvement in both realism and accuracy compared to previous methods.' Weaknesses='While the paper presents a robust method, the reliance on a diffusion model may introduce computational complexity. Additionally, the method is benchmarked against specific datasets, which might limit its applicability to broader or different domains where the models may not generalize well.' Questions='Could the method be adapted for objects in domains other than household items, and how well might it generalize to completely unseen categories of objects?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=4 Strengths='The proposed method significantly advances the state-of-the-art in generating articulated 3D models from single images, providing a more scalable and practical approach. The use of a diffusion model to handle ambiguity in part shape and motion from limited input is innovative. Experiments show a considerable improvement in both realism and accuracy compared to previous methods.' Weaknesses='While the paper presents a robust method, the reliance on a diffusion model may introduce computational complexity. Additionally, the method is benchmarked against specific datasets, which might limit its applicability to broader or different domains where the models may not generalize well.' Questions='Could the method be adapted for objects in domains other than household items, and how well might it generalize to completely unseen categories of objects?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths=\"1. The introduction of Geometric Spatiotemporal Transformers (GSTs) shows a novel approach to leveraging Transformer architectures for physical dynamics simulations, which can bring advantages of long-term context awareness and handling varying sequence lengths. \\n2. The Temporal Difference Graph (TDG) is an innovative mechanism that aids in reducing cumulative errors, an essential aspect for simulating physical systems effectively over time. \\n3. The method demonstrates state-of-the-art performance across different scales, from molecular to macro-level systems, indicating the approach's robustness and versatility.\" Weaknesses=\"1. The paper requires additional clarity and detail in its presentation to improve understanding, particularly in the explanation of technical aspects and the innovative features like the TDG. The current description might be challenging for readers unfamiliar with the specific technical background.\\n2. The reliance on Transformers for simulation, despite their advantages, may bring computational challenges not fully addressed in the paper. Specifically, efficiency and resource requirements compared to other methods like GNNs might need further exploration or justification.\\n3. While qualitative results seem robust, there's a need for more detailed quantitative comparisons, specifically covering metrics beyond state-of-the-art claims, such as computational efficiency or other trade-offs in different settings.\" Questions='1. How does the GST approach compare computationally to existing Graph Neural Network (GNN) methods in terms of efficiency and scalability?\\n2. Can you elaborate on how the Temporal Difference Graph (TDG) effectively reduces cumulative error in practice, and are there scenarios where it might not perform as expected?\\n3. Is there potential for GSTs to be applied or extended to domains outside of physics-based simulations, such as in socio-economic systems or broader scientific computations?\\n4. Could more insight be provided on how the alternating spatial-temporal block structure contributes specifically to maintaining E(3) symmetries, and are there known limitations in this approach?\\n5. Regarding scalability, how does the method perform with respect to increasing input sequence lengths or higher complexity physical systems (beyond those tested)? '\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths=\"1. The introduction of Geometric Spatiotemporal Transformers (GSTs) shows a novel approach to leveraging Transformer architectures for physical dynamics simulations, which can bring advantages of long-term context awareness and handling varying sequence lengths. \\n2. The Temporal Difference Graph (TDG) is an innovative mechanism that aids in reducing cumulative errors, an essential aspect for simulating physical systems effectively over time. \\n3. The method demonstrates state-of-the-art performance across different scales, from molecular to macro-level systems, indicating the approach's robustness and versatility.\" Weaknesses=\"1. The paper requires additional clarity and detail in its presentation to improve understanding, particularly in the explanation of technical aspects and the innovative features like the TDG. The current description might be challenging for readers unfamiliar with the specific technical background.\\n2. The reliance on Transformers for simulation, despite their advantages, may bring computational challenges not fully addressed in the paper. Specifically, efficiency and resource requirements compared to other methods like GNNs might need further exploration or justification.\\n3. While qualitative results seem robust, there's a need for more detailed quantitative comparisons, specifically covering metrics beyond state-of-the-art claims, such as computational efficiency or other trade-offs in different settings.\" Questions='1. How does the GST approach compare computationally to existing Graph Neural Network (GNN) methods in terms of efficiency and scalability?\\n2. Can you elaborate on how the Temporal Difference Graph (TDG) effectively reduces cumulative error in practice, and are there scenarios where it might not perform as expected?\\n3. Is there potential for GSTs to be applied or extended to domains outside of physics-based simulations, such as in socio-economic systems or broader scientific computations?\\n4. Could more insight be provided on how the alternating spatial-temporal block structure contributes specifically to maintaining E(3) symmetries, and are there known limitations in this approach?\\n5. Regarding scalability, how does the method perform with respect to increasing input sequence lengths or higher complexity physical systems (beyond those tested)? '\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper proposes a novel approach to graph representation learning by adapting diffusion probabilistic models (DPMs), which have previously been overlooked in this area. The method is empirically validated, showing state-of-the-art results on 9 of 11 datasets, indicating its effectiveness.' Weaknesses='The explanation of the theoretical foundation may not be fully clear to all readers, potentially limiting understanding. The practical implications or limitations of using DPMs for graph representations are not deeply explored, such as computational cost or scalability concerns.' Questions='How does the computational complexity of Graffe compare to traditional graph representation learning models? Are there specific types of graphs or datasets where Graffe performs better or worse, and if so, why?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper proposes a novel approach to graph representation learning by adapting diffusion probabilistic models (DPMs), which have previously been overlooked in this area. The method is empirically validated, showing state-of-the-art results on 9 of 11 datasets, indicating its effectiveness.' Weaknesses='The explanation of the theoretical foundation may not be fully clear to all readers, potentially limiting understanding. The practical implications or limitations of using DPMs for graph representations are not deeply explored, such as computational cost or scalability concerns.' Questions='How does the computational complexity of Graffe compare to traditional graph representation learning models? Are there specific types of graphs or datasets where Graffe performs better or worse, and if so, why?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper proposes a novel mechanism for integrating external knowledge into large language models, which addresses the limitations of existing retrieval-augmented generation approaches. The use of a memory-based mechanism for dynamic knowledge integration is an innovative aspect that contributes to the field.\\nThe proposed approach is evaluated on a range of QA tasks, demonstrating its effectiveness in enhancing the performance of LLMs. This offers evidence that supports the feasibility and applicability of the method across domains.' Weaknesses='The paper lacks detailed discussion on the limitations of the proposed approach, such as potential computational overheads. Additionally, the intricacies of integrating the external knowledge attention mechanism with existing attention mechanisms are complex and might require further clarification to ensure reproducibility.\\nThe novelty of integrating external knowledge with an attention head is incremental, as the fundamental idea of augmenting models with attention mechanisms is not entirely novel.' Questions=\"How does the proposed method handle contradictory or noisy information from the retrieved knowledge?\\nCan the model's approach to control the degree of knowledge integration be interpreted to understand why certain knowledge is considered more relevant than others?\\nWhat are the specific challenges faced when integrating the external knowledge attention mechanism with internal attention heads in terms of model training or stability?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper proposes a novel mechanism for integrating external knowledge into large language models, which addresses the limitations of existing retrieval-augmented generation approaches. The use of a memory-based mechanism for dynamic knowledge integration is an innovative aspect that contributes to the field.\\nThe proposed approach is evaluated on a range of QA tasks, demonstrating its effectiveness in enhancing the performance of LLMs. This offers evidence that supports the feasibility and applicability of the method across domains.' Weaknesses='The paper lacks detailed discussion on the limitations of the proposed approach, such as potential computational overheads. Additionally, the intricacies of integrating the external knowledge attention mechanism with existing attention mechanisms are complex and might require further clarification to ensure reproducibility.\\nThe novelty of integrating external knowledge with an attention head is incremental, as the fundamental idea of augmenting models with attention mechanisms is not entirely novel.' Questions=\"How does the proposed method handle contradictory or noisy information from the retrieved knowledge?\\nCan the model's approach to control the degree of knowledge integration be interpreted to understand why certain knowledge is considered more relevant than others?\\nWhat are the specific challenges faced when integrating the external knowledge attention mechanism with internal attention heads in terms of model training or stability?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=3 Strengths='The paper introduces a novel approach that uses a calculated guide policy sampling rate to guarantee that the mean return of the learning policy surpasses a performance degradation threshold, marking an innovative advancement in guided RL methods. The method is presented with theoretical backing and claims to be robust to hyperparameter choices, which can be significant for practical applicability. The introduction of a roll-back feature is a potentially powerful tool to dynamically adjust and optimize learning guidance.' Weaknesses='The applicability of the derived method under practical conditions may be limited, as the theoretical assumptions made for guaranteeing the performance might not hold in many real-world scenarios. The empirical validation is not very comprehensive, with limited experiments that may not convincingly demonstrate the robustness or generality of the proposed approach. Additionally, some of the claims may be overstated without enough qualification or evidence, particularly regarding performance guarantees.' Questions='1. How does the guide policy sampling rate derived under the given assumptions compare to empirical results? Are there scenarios where the assumptions hold more strongly and lead to better performance?\\n2. Can the roll-back mechanism be generalized to various RL environments with different reward structures, or is it specifically tailored for the ones tested in the paper?\\n3. How does the approach handle environments with sparse rewards, and would the performance guarantees still be applicable in such cases?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=3 Strengths='The paper introduces a novel approach that uses a calculated guide policy sampling rate to guarantee that the mean return of the learning policy surpasses a performance degradation threshold, marking an innovative advancement in guided RL methods. The method is presented with theoretical backing and claims to be robust to hyperparameter choices, which can be significant for practical applicability. The introduction of a roll-back feature is a potentially powerful tool to dynamically adjust and optimize learning guidance.' Weaknesses='The applicability of the derived method under practical conditions may be limited, as the theoretical assumptions made for guaranteeing the performance might not hold in many real-world scenarios. The empirical validation is not very comprehensive, with limited experiments that may not convincingly demonstrate the robustness or generality of the proposed approach. Additionally, some of the claims may be overstated without enough qualification or evidence, particularly regarding performance guarantees.' Questions='1. How does the guide policy sampling rate derived under the given assumptions compare to empirical results? Are there scenarios where the assumptions hold more strongly and lead to better performance?\\n2. Can the roll-back mechanism be generalized to various RL environments with different reward structures, or is it specifically tailored for the ones tested in the paper?\\n3. How does the approach handle environments with sparse rewards, and would the performance guarantees still be applicable in such cases?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The paper introduces an innovative framework, Diffusion on Diffusion (DoD), addressing limitations of conventional class-guided diffusion models in generating detailed texture.\\n2. The method successfully reduces computational cost significantly (7x training cost reduction) compared to previous models while achieving superior FID-50K scores.\\n3. This approach is well-motivated with empirical evidence demonstrating enhanced performance on the popular ImageNet dataset.' Weaknesses='1. The paper lacks direct comparisons with recent state-of-the-art post-hoc diffusion refinement methods, which could provide further evidence of superiority.\\n2. The proposed Latent Embedding Module, while a novel contribution, lacks comparative analysis with other existing methods.\\n3. There is limited qualitative analysis or visual examples provided, making it challenging to assess the qualitative improvements claimed.' Questions='1. How does DoD compare against other SOTA post-hoc diffusion refinement methods not included in the current evaluation?\\n2. Can the Latent Embedding Module be justified further, perhaps by comparison with similar existing conditioning approaches?\\n3. Are there specific qualitative differences in image outcomes that can be detailed or visually compared to support claims of improved texture detail?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The paper introduces an innovative framework, Diffusion on Diffusion (DoD), addressing limitations of conventional class-guided diffusion models in generating detailed texture.\\n2. The method successfully reduces computational cost significantly (7x training cost reduction) compared to previous models while achieving superior FID-50K scores.\\n3. This approach is well-motivated with empirical evidence demonstrating enhanced performance on the popular ImageNet dataset.' Weaknesses='1. The paper lacks direct comparisons with recent state-of-the-art post-hoc diffusion refinement methods, which could provide further evidence of superiority.\\n2. The proposed Latent Embedding Module, while a novel contribution, lacks comparative analysis with other existing methods.\\n3. There is limited qualitative analysis or visual examples provided, making it challenging to assess the qualitative improvements claimed.' Questions='1. How does DoD compare against other SOTA post-hoc diffusion refinement methods not included in the current evaluation?\\n2. Can the Latent Embedding Module be justified further, perhaps by comparison with similar existing conditioning approaches?\\n3. Are there specific qualitative differences in image outcomes that can be detailed or visually compared to support claims of improved texture detail?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The proposed framework unifies in-distribution and out-of-distribution generalization within a single architecture, which is a significant contribution to deep reinforcement learning. The introduction of a robust adaptation module and joint training pipeline are innovative ideas that enhance the applicability of reinforcement learning in complex real-world scenarios. The effectiveness of the proposed approach is demonstrated through rigorous testing on a variety of realistic simulated tasks.' Weaknesses='The paper might lack comprehensive coverage of related work or comparison with existing state-of-the-art methods. Furthermore, the approach’s applicability to domains other than the tested simulated locomotion tasks remains an open question.' Questions='How does the approach compare with existing methods in terms of computational complexity? Are there any limitations regarding the scalability of the proposed framework to more complex environments or different types of robots beyond the quadruped robot tested?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The proposed framework unifies in-distribution and out-of-distribution generalization within a single architecture, which is a significant contribution to deep reinforcement learning. The introduction of a robust adaptation module and joint training pipeline are innovative ideas that enhance the applicability of reinforcement learning in complex real-world scenarios. The effectiveness of the proposed approach is demonstrated through rigorous testing on a variety of realistic simulated tasks.' Weaknesses='The paper might lack comprehensive coverage of related work or comparison with existing state-of-the-art methods. Furthermore, the approach’s applicability to domains other than the tested simulated locomotion tasks remains an open question.' Questions='How does the approach compare with existing methods in terms of computational complexity? Are there any limitations regarding the scalability of the proposed framework to more complex environments or different types of robots beyond the quadruped robot tested?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel approach to generating effective canaries, leading to a significant improvement in privacy auditing of large language models. The experimental results clearly demonstrate the superiority of the proposed method over prior state-of-the-art techniques. The approach is applicable under realistic threat models where the attacker has limited capabilities, which increases the practicality and relevance of the findings. The paper is also well-written and clearly structured, enhancing its readability and comprehension.' Weaknesses='While the results are impressive, the novel canary generation approach is somewhat specific to certain LLM architectures and does not provide insights into how generalizable the findings are across different types of models or datasets. Additionally, there may be challenges in scaling the approach for models larger than those tested, which is not addressed in the paper.' Questions='What are the potential limitations of the canary design methodology when applied to different LLM architectures or in settings with larger datasets? Can the authors provide more insights into the scalability of their approach and how it might be adapted for future, larger models?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel approach to generating effective canaries, leading to a significant improvement in privacy auditing of large language models. The experimental results clearly demonstrate the superiority of the proposed method over prior state-of-the-art techniques. The approach is applicable under realistic threat models where the attacker has limited capabilities, which increases the practicality and relevance of the findings. The paper is also well-written and clearly structured, enhancing its readability and comprehension.' Weaknesses='While the results are impressive, the novel canary generation approach is somewhat specific to certain LLM architectures and does not provide insights into how generalizable the findings are across different types of models or datasets. Additionally, there may be challenges in scaling the approach for models larger than those tested, which is not addressed in the paper.' Questions='What are the potential limitations of the canary design methodology when applied to different LLM architectures or in settings with larger datasets? Can the authors provide more insights into the scalability of their approach and how it might be adapted for future, larger models?'\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Content:\n",
      "I'm sorry, it seems there was a misunderstanding. Please disregard the previous message and I will provide you with the corrected JSON output. Here it is:\n",
      "\n",
      "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 4, \"Rating\": 7, \"Confidence\": 4, \"Strengths\": \"1. The paper introduces an innovative framework, ImmersePro, for transforming single-view videos into stereo videos, which is a significant contribution to the field. 2. The dual-branch architecture with spatial-temporal attention mechanisms effectively enhances the stereo video generation process. 3. The creation of the YouTube-SBS dataset provides a valuable resource for future research and benchmarking in stereo video generation.\", \"Weaknesses\": \"1. The paper may lack detailed technical information about the implementation and performance of the disparity and context branches. 2. The paper may not provide a comprehensive comparison with a wide range of existing methods in the literature.\", \"Questions\": \"1. How does the framework handle variations in video quality and content diversity in the YouTube-SBS dataset? 2. Can ImmersePro be adapted for real-time applications or higher resolution videos?\"}\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel progressive search method for extrapolative protein design by distilling triplet relations into the model. This approach builds upon the success of preference alignment models, showing significant improvements in the design of AAV and GFP proteins for extrapolation tasks.' Weaknesses=\"The reliance on defining triplet relations or scorers might be seen as a limitation, especially if the scorers do not generalize well to out-of-distribution settings, which could potentially hinder the method's robustness.\" Questions=\"How does the method perform in cases where the scorer function might be imperfect or noisy? Could this affect the model's extrapolation capabilities?\"\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel progressive search method for extrapolative protein design by distilling triplet relations into the model. This approach builds upon the success of preference alignment models, showing significant improvements in the design of AAV and GFP proteins for extrapolation tasks.' Weaknesses=\"The reliance on defining triplet relations or scorers might be seen as a limitation, especially if the scorers do not generalize well to out-of-distribution settings, which could potentially hinder the method's robustness.\" Questions=\"How does the method perform in cases where the scorer function might be imperfect or noisy? Could this affect the model's extrapolation capabilities?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel approach for time series forecasting by employing a learnable data augmentation technique using reinforcement learning. The method effectively identifies marginal samples and augments them to enhance forecasting performance. The use of variational masked autoencoders and the REINFORCE algorithm adds to the innovation. Moreover, the approach claims to achieve improvement with minimal computational cost, which is highly beneficial.' Weaknesses='The paper does not thoroughly explain how the variational masked autoencoders are specifically tailored for time series data, which could be a critical aspect of the methodology. Additionally, there is a lack of detailed comparisons with other state-of-the-art augmentation techniques in forecasting. It is also unclear how well the proposed method generalizes beyond the datasets tested.' Questions='How does AutoTSAug perform in comparison to other advanced data augmentation techniques for time series forecasting? Does the method require retraining the forecasting models after augmenting the data, or is the augmentation integrated into the training process?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel approach for time series forecasting by employing a learnable data augmentation technique using reinforcement learning. The method effectively identifies marginal samples and augments them to enhance forecasting performance. The use of variational masked autoencoders and the REINFORCE algorithm adds to the innovation. Moreover, the approach claims to achieve improvement with minimal computational cost, which is highly beneficial.' Weaknesses='The paper does not thoroughly explain how the variational masked autoencoders are specifically tailored for time series data, which could be a critical aspect of the methodology. Additionally, there is a lack of detailed comparisons with other state-of-the-art augmentation techniques in forecasting. It is also unclear how well the proposed method generalizes beyond the datasets tested.' Questions='How does AutoTSAug perform in comparison to other advanced data augmentation techniques for time series forecasting? Does the method require retraining the forecasting models after augmenting the data, or is the augmentation integrated into the training process?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper proposes a novel approach called AlignCLIP to improve the alignment of text and image embeddings, addressing the modality gap in CLIP.\\n2. Gains in downstream zero-shot and fine-tuning tasks are demonstrated, indicating potential practical impact.\\n3. The presentation is clear and well-structured, making it easy to follow the proposed method and its evaluations.' Weaknesses='1. While the approach is an interesting enhancement over existing methods, the novelty might be considered incremental in leveraging shared modality encoders.\\n2. The theoretical underpinnings of the semantically-regularized separation objective function are not deeply analyzed, leaving some questions on robustness and versatility across various datasets.\\n3. More detailed experiments and comparative analysis with other recent methods in the domain could strengthen the claims.' Questions='1. How transferable is the proposed method to other architectures beyond CLIP, and what would be required to adapt it?\\n2. Can the semantically-regularized separation objective function be applied to other cross-modal alignment tasks effectively? If so, what are the potential challenges?\\n3. How does the approach handle highly imbalanced datasets where the modality gap may vary significantly across subregions?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper proposes a novel approach called AlignCLIP to improve the alignment of text and image embeddings, addressing the modality gap in CLIP.\\n2. Gains in downstream zero-shot and fine-tuning tasks are demonstrated, indicating potential practical impact.\\n3. The presentation is clear and well-structured, making it easy to follow the proposed method and its evaluations.' Weaknesses='1. While the approach is an interesting enhancement over existing methods, the novelty might be considered incremental in leveraging shared modality encoders.\\n2. The theoretical underpinnings of the semantically-regularized separation objective function are not deeply analyzed, leaving some questions on robustness and versatility across various datasets.\\n3. More detailed experiments and comparative analysis with other recent methods in the domain could strengthen the claims.' Questions='1. How transferable is the proposed method to other architectures beyond CLIP, and what would be required to adapt it?\\n2. Can the semantically-regularized separation objective function be applied to other cross-modal alignment tasks effectively? If so, what are the potential challenges?\\n3. How does the approach handle highly imbalanced datasets where the modality gap may vary significantly across subregions?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper identifies a significant limitation in existing weight-space autoencoders, which is crucial for advancing the use of neural network weights as a data modality.\\n2. The introduction of the behavioral loss is a novel approach that addresses this limitation and enhances the performance of reconstructed models.\\n3. The paper is well-organized and clearly explains the synergy between structural and behavioral features in model reconstruction.' Weaknesses='1. The approach might be limited to the specific types of models and tasks evaluated, as the paper does not extensively discuss generalization to other architectures or domains.\\n2. The evaluation, while thorough, could be seen as lacking diversity in terms of the types of models and tasks used for testing the proposed approach.\\n3. There is a potential concern about the scalability of the approach, especially when applied to larger models, which is not fully addressed.' Questions='1. How well does the proposed method generalize to larger, more complex models beyond those tested?\\n2. Can the behavioral loss be adapted or extended to other types of models or different modalities beyond weight space?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper identifies a significant limitation in existing weight-space autoencoders, which is crucial for advancing the use of neural network weights as a data modality.\\n2. The introduction of the behavioral loss is a novel approach that addresses this limitation and enhances the performance of reconstructed models.\\n3. The paper is well-organized and clearly explains the synergy between structural and behavioral features in model reconstruction.' Weaknesses='1. The approach might be limited to the specific types of models and tasks evaluated, as the paper does not extensively discuss generalization to other architectures or domains.\\n2. The evaluation, while thorough, could be seen as lacking diversity in terms of the types of models and tasks used for testing the proposed approach.\\n3. There is a potential concern about the scalability of the approach, especially when applied to larger models, which is not fully addressed.' Questions='1. How well does the proposed method generalize to larger, more complex models beyond those tested?\\n2. Can the behavioral loss be adapted or extended to other types of models or different modalities beyond weight space?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel algorithm that effectively extends the utility of the Bellman update in offline reinforcement learning, demonstrating improved performance, particularly in sparse reward environments.' Weaknesses='The paper lacks a detailed discussion on how EIQL compares with related model-based offline RL approaches. It also omits discussions of certain key references, affecting the completeness of the related work section. Additionally, the paper could benefit from a more detailed explanation of the algorithms involved.' Questions='How does EIQL perform when compared with other model-based offline RL methods? Can the approach be adapted to environments with more complex or high-dimensional state spaces?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel algorithm that effectively extends the utility of the Bellman update in offline reinforcement learning, demonstrating improved performance, particularly in sparse reward environments.' Weaknesses='The paper lacks a detailed discussion on how EIQL compares with related model-based offline RL approaches. It also omits discussions of certain key references, affecting the completeness of the related work section. Additionally, the paper could benefit from a more detailed explanation of the algorithms involved.' Questions='How does EIQL perform when compared with other model-based offline RL methods? Can the approach be adapted to environments with more complex or high-dimensional state spaces?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper makes significant theoretical progress in understanding the complexity of sampling in diffusion models, particularly in high-dimensional settings. The novel analysis and use of hardness potentials contribute to a deeper understanding of the limitations of almost linear iteration algorithms.' Weaknesses='The paper might be difficult to follow for readers who are not experts in the field due to the complex theoretical concepts involved. The practical implications of the theoretical results are not thoroughly explored, leaving some uncertainty about their impact on real-world applications.' Questions='How can the results regarding almost linear iteration algorithms inform the design of future sampling algorithms, especially in practical scenarios? Are there potential applications of these theoretical insights beyond the scope of diffusion models?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper makes significant theoretical progress in understanding the complexity of sampling in diffusion models, particularly in high-dimensional settings. The novel analysis and use of hardness potentials contribute to a deeper understanding of the limitations of almost linear iteration algorithms.' Weaknesses='The paper might be difficult to follow for readers who are not experts in the field due to the complex theoretical concepts involved. The practical implications of the theoretical results are not thoroughly explored, leaving some uncertainty about their impact on real-world applications.' Questions='How can the results regarding almost linear iteration algorithms inform the design of future sampling algorithms, especially in practical scenarios? Are there potential applications of these theoretical insights beyond the scope of diffusion models?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper presents a novel method, Soft Preference Optimization, to align generative models with human preferences without relying on a reward model, simplifying the optimization process. The integration of preference loss and regularization across the model's output distribution is theoretically grounded, and the adjustable softness parameter adds flexibility. Theoretical guarantees under the Bradley-Terry model bolster the soundness of the approach.\" Weaknesses=\"Details on empirical validation are sparse. It's unclear how the proposed method compares quantitatively with state-of-the-art reward-model-based methods like RLHF. The paper could benefit from more extensive empirical evaluations to demonstrate its practical effectiveness and generalizability. Additionally, the role and impact of the regularization term need clearer exposition and justification.\" Questions=\"1. How does SPO perform in practical scenarios compared to established reward-model-based approaches? \\n2. Could the authors provide examples of generative tasks where SPO's preference alignment significantly outperforms traditional methods?\\n3. How sensitive is the model's performance to the setting of the softmax exponent in the algorithm?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper presents a novel method, Soft Preference Optimization, to align generative models with human preferences without relying on a reward model, simplifying the optimization process. The integration of preference loss and regularization across the model's output distribution is theoretically grounded, and the adjustable softness parameter adds flexibility. Theoretical guarantees under the Bradley-Terry model bolster the soundness of the approach.\" Weaknesses=\"Details on empirical validation are sparse. It's unclear how the proposed method compares quantitatively with state-of-the-art reward-model-based methods like RLHF. The paper could benefit from more extensive empirical evaluations to demonstrate its practical effectiveness and generalizability. Additionally, the role and impact of the regularization term need clearer exposition and justification.\" Questions=\"1. How does SPO perform in practical scenarios compared to established reward-model-based approaches? \\n2. Could the authors provide examples of generative tasks where SPO's preference alignment significantly outperforms traditional methods?\\n3. How sensitive is the model's performance to the setting of the softmax exponent in the algorithm?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper proposes an innovative method to synthesize photorealistic images for autonomous driving scenarios using a 2D diffusion model conditioned on 3D occupancy labels, potentially reducing the need for costly human annotation.\\n2. SyntheOcc successfully integrates 3D semantic multi-plane images, ensuring coherent and spatially aligned scene description, which is crucial for high-quality dataset generation.\\n3. The approach is well validated through both qualitative and quantitative evaluations, demonstrating its effectiveness on real-world data from the nuScenes dataset.' Weaknesses='1. The method relies heavily on the quality and accuracy of the occupancy labels, which might limit its applicability in environments where such labels are difficult to obtain.\\n2. While the use of multi-plane images is novel, it may introduce complexity and computational overhead that could limit scalability.\\n3. The approach may need further validation on additional datasets and driving scenarios to fully establish its generality and robustness.' Questions='1. How does the computational efficiency of SyntheOcc compare with traditional data augmentation techniques?\\n2. Can the approach be adapted for real-time data generation, and if so, what are the trade-offs in terms of performance and quality?\\n3. What are the limitations in the current implementation with respect to generating diverse driving scenarios, and how might these be addressed in future work?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper proposes an innovative method to synthesize photorealistic images for autonomous driving scenarios using a 2D diffusion model conditioned on 3D occupancy labels, potentially reducing the need for costly human annotation.\\n2. SyntheOcc successfully integrates 3D semantic multi-plane images, ensuring coherent and spatially aligned scene description, which is crucial for high-quality dataset generation.\\n3. The approach is well validated through both qualitative and quantitative evaluations, demonstrating its effectiveness on real-world data from the nuScenes dataset.' Weaknesses='1. The method relies heavily on the quality and accuracy of the occupancy labels, which might limit its applicability in environments where such labels are difficult to obtain.\\n2. While the use of multi-plane images is novel, it may introduce complexity and computational overhead that could limit scalability.\\n3. The approach may need further validation on additional datasets and driving scenarios to fully establish its generality and robustness.' Questions='1. How does the computational efficiency of SyntheOcc compare with traditional data augmentation techniques?\\n2. Can the approach be adapted for real-time data generation, and if so, what are the trade-offs in terms of performance and quality?\\n3. What are the limitations in the current implementation with respect to generating diverse driving scenarios, and how might these be addressed in future work?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The integration of diffusion models with active learning for inverse imaging is novel and applicable to real-world problems, such as reducing X-ray dosage in tomography.' Weaknesses='The reliance on a pre-trained diffusion model might limit the application scope, and the computational cost of the approach might be high.' Questions='How does the approach compare with other state-of-the-art active learning methods in terms of computational efficiency?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The integration of diffusion models with active learning for inverse imaging is novel and applicable to real-world problems, such as reducing X-ray dosage in tomography.' Weaknesses='The reliance on a pre-trained diffusion model might limit the application scope, and the computational cost of the approach might be high.' Questions='How does the approach compare with other state-of-the-art active learning methods in terms of computational efficiency?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper introduces SAM-CP, which effectively extends SAM's capabilities to achieve state-of-the-art performance in open-vocabulary segmentation. The method is well-structured with two composable prompt types that enable semantic-aware segmentation across diverse scenarios. It offers a generalized approach for equipping vision foundation models with enhanced semantic perception.\" Weaknesses='The approach might rely heavily on pre-existing datasets for effective functioning, which could limit its applicability in less-structured or unseen environments. The paper primarily focuses on merging patches, potentially overlooking scenarios where separation of patches is needed. Furthermore, a direct comparison with other SAM-based approaches would have enriched the evaluation.' Questions='How does SAM-CP perform in environments with highly complex or fine-grained segmentation requirements? Are there any strategies incorporated for scenarios where merging is not sufficient, and splitting of patches becomes necessary?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper introduces SAM-CP, which effectively extends SAM's capabilities to achieve state-of-the-art performance in open-vocabulary segmentation. The method is well-structured with two composable prompt types that enable semantic-aware segmentation across diverse scenarios. It offers a generalized approach for equipping vision foundation models with enhanced semantic perception.\" Weaknesses='The approach might rely heavily on pre-existing datasets for effective functioning, which could limit its applicability in less-structured or unseen environments. The paper primarily focuses on merging patches, potentially overlooking scenarios where separation of patches is needed. Furthermore, a direct comparison with other SAM-based approaches would have enriched the evaluation.' Questions='How does SAM-CP perform in environments with highly complex or fine-grained segmentation requirements? Are there any strategies incorporated for scenarios where merging is not sufficient, and splitting of patches becomes necessary?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=8 Confidence=4 Strengths=\"1. The paper presents a novel language-based approach to video colorization that allows for creative color expressions while maintaining temporal consistency.\\n2. It introduces innovative components such as the cross-modality pre-fusion module and temporally deformable attention, which are well-justified and enhance the model's performance significantly.\\n3. The experimental results convincingly demonstrate the superior performance of the proposed method over existing techniques.\" Weaknesses='1. The paper does not fully explore limitations or potential challenges in implementing the proposed model in different scenarios, particularly in varying lighting conditions or with complex scenes.\\n2. Some technical details remain insufficiently explained, such as the handling of ambiguous textual cues and extremely diverse color possibilities.' Questions=\"1. How does the proposed method handle ambiguous or complex text descriptions where multiple color interpretations are possible? \\n2. What is the impact of different types of textual descriptions on the model's performance, and how does it manage user expectations during colorization?\"\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=8 Confidence=4 Strengths=\"1. The paper presents a novel language-based approach to video colorization that allows for creative color expressions while maintaining temporal consistency.\\n2. It introduces innovative components such as the cross-modality pre-fusion module and temporally deformable attention, which are well-justified and enhance the model's performance significantly.\\n3. The experimental results convincingly demonstrate the superior performance of the proposed method over existing techniques.\" Weaknesses='1. The paper does not fully explore limitations or potential challenges in implementing the proposed model in different scenarios, particularly in varying lighting conditions or with complex scenes.\\n2. Some technical details remain insufficiently explained, such as the handling of ambiguous textual cues and extremely diverse color possibilities.' Questions=\"1. How does the proposed method handle ambiguous or complex text descriptions where multiple color interpretations are possible? \\n2. What is the impact of different types of textual descriptions on the model's performance, and how does it manage user expectations during colorization?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='1. The paper addresses a timely and important topic in machine learning, providing a solution for lifelong learning with privacy-focused unlearning capabilities. 2. The proposed approach, PALL, demonstrates innovation by combining task-specific sparse subnetworks and episodic memory rehearsal. 3. Empirical results show the scalability and effectiveness of the approach across various architectures.' Weaknesses='1. It is not clear how the episodic memory rehearsal mechanism affects the overall performance and efficiency of the model. 2. The implementation and optimization details of privacy-aware lifelong learning may be challenging to understand and replicate based on the abstract alone. 3. The balance between lifelong learning and unlearning objectives might be complex, and practical trade-offs are not well-explored.' Questions='1. How does PALL manage the potential trade-offs between maintaining high model performance and ensuring robust unlearning? 2. What are the specific criteria or triggers for when unlearning occurs in the experimental setup? 3. How scalable is the episodic memory rehearsal mechanism in terms of computational resources required, particularly when handling large datasets or models?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='1. The paper addresses a timely and important topic in machine learning, providing a solution for lifelong learning with privacy-focused unlearning capabilities. 2. The proposed approach, PALL, demonstrates innovation by combining task-specific sparse subnetworks and episodic memory rehearsal. 3. Empirical results show the scalability and effectiveness of the approach across various architectures.' Weaknesses='1. It is not clear how the episodic memory rehearsal mechanism affects the overall performance and efficiency of the model. 2. The implementation and optimization details of privacy-aware lifelong learning may be challenging to understand and replicate based on the abstract alone. 3. The balance between lifelong learning and unlearning objectives might be complex, and practical trade-offs are not well-explored.' Questions='1. How does PALL manage the potential trade-offs between maintaining high model performance and ensuring robust unlearning? 2. What are the specific criteria or triggers for when unlearning occurs in the experimental setup? 3. How scalable is the episodic memory rehearsal mechanism in terms of computational resources required, particularly when handling large datasets or models?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='1. The introduction of the CVCAM and MHSAM modules represents significant contributions to improving cross-view object geo-localization, offering a novel way to enhance contextual information exchange and feature representation. 2. The creation and introduction of the G2D dataset fill a gap in existing resources, providing a valuable tool for further research in the Ground→Drone localization task. 3. Extensive experimental validation on multiple datasets, including the proposed G2D, demonstrates the effectiveness of the method, showing significant improvements over state-of-the-art baselines.' Weaknesses='1. The abstract mentions an issue with existing methods focusing on irrelevant edge noise, but does not explain how this is specifically addressed in a quantifiable manner through the proposed modules. 2. The effectiveness of the proposed approach in different scenario variations or the integration of dynamic and real-time environments is not thoroughly discussed, which could impact applicability. 3. While the paper introduces a new dataset, detailed information about its scope, uniqueness, and potential biases or limitations is not extensively covered.' Questions='1. How robust is the proposed method when applied to environments with varying conditions, such as changes in lighting or weather? 2. Can the integration of the CVCAM and MHSAM modules be generalized to other tasks beyond object geo-localization? 3. What are the potential biases inherent in the G2D dataset, and how might these impact the results or applicability of the model in real-world scenarios?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='1. The introduction of the CVCAM and MHSAM modules represents significant contributions to improving cross-view object geo-localization, offering a novel way to enhance contextual information exchange and feature representation. 2. The creation and introduction of the G2D dataset fill a gap in existing resources, providing a valuable tool for further research in the Ground→Drone localization task. 3. Extensive experimental validation on multiple datasets, including the proposed G2D, demonstrates the effectiveness of the method, showing significant improvements over state-of-the-art baselines.' Weaknesses='1. The abstract mentions an issue with existing methods focusing on irrelevant edge noise, but does not explain how this is specifically addressed in a quantifiable manner through the proposed modules. 2. The effectiveness of the proposed approach in different scenario variations or the integration of dynamic and real-time environments is not thoroughly discussed, which could impact applicability. 3. While the paper introduces a new dataset, detailed information about its scope, uniqueness, and potential biases or limitations is not extensively covered.' Questions='1. How robust is the proposed method when applied to environments with varying conditions, such as changes in lighting or weather? 2. Can the integration of the CVCAM and MHSAM modules be generalized to other tasks beyond object geo-localization? 3. What are the potential biases inherent in the G2D dataset, and how might these impact the results or applicability of the model in real-world scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=4 Strengths='The paper provides a matrix-theoretic perspective on the adversarial fragility of deep neural networks, which is a valuable contribution to understanding adversarial robustness. It aligns the findings with earlier information-theoretic explanations, enriching the existing theoretical landscape.' Weaknesses=\"The paper doesn't address practical implications or how these theoretical insights might be applied to develop more robust models. The assumptions made for the theoretical proofs may limit the general applicability of the results.\" Questions='How can the theoretical results be leveraged to improve the adversarial robustness of neural networks in practice?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=4 Strengths='The paper provides a matrix-theoretic perspective on the adversarial fragility of deep neural networks, which is a valuable contribution to understanding adversarial robustness. It aligns the findings with earlier information-theoretic explanations, enriching the existing theoretical landscape.' Weaknesses=\"The paper doesn't address practical implications or how these theoretical insights might be applied to develop more robust models. The assumptions made for the theoretical proofs may limit the general applicability of the results.\" Questions='How can the theoretical results be leveraged to improve the adversarial robustness of neural networks in practice?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper proposes a novel method that enables vision transformers to be robust against changes in layer order during inference, a useful attribute for distributed neural networks and other applications. The ability to merge trained models into functional 'Frankenstein' models without performance loss is also compelling.\" Weaknesses=\"The reduction in accuracy by approximately 20% is significant and may limit the practical applications of this method. The method's evaluation appears limited to certain architectures, such as ViT/DeiT, potentially reducing the generalizability of the findings.\" Questions='Is it possible to mitigate the accuracy reduction seen with the proposed method? Are there plans to extend the evaluation to other modern transformer architectures like Swin or EfficientViT that may handle shuffling differently?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper proposes a novel method that enables vision transformers to be robust against changes in layer order during inference, a useful attribute for distributed neural networks and other applications. The ability to merge trained models into functional 'Frankenstein' models without performance loss is also compelling.\" Weaknesses=\"The reduction in accuracy by approximately 20% is significant and may limit the practical applications of this method. The method's evaluation appears limited to certain architectures, such as ViT/DeiT, potentially reducing the generalizability of the findings.\" Questions='Is it possible to mitigate the accuracy reduction seen with the proposed method? Are there plans to extend the evaluation to other modern transformer architectures like Swin or EfficientViT that may handle shuffling differently?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"1. The introduction of FairDropout is a novel approach that creatively utilizes example-tied dropout to mitigate the issues arising from memorization of minority groups by deep learning models.\\n2. The paper addresses an important problem in deep learning, specifically the exploitation of spurious correlations, which impacts the generalization of models.\\n3. FairDropout's application across varied domains like vision, language, and healthcare suggests its wide applicability and potential impact.\" Weaknesses='1. The methodology, while innovative, may incur significant computational overhead due to the localization of memorization to specific neurons, potentially impacting scalability on larger datasets.\\n2. While the approach is validated across multiple benchmarks, the lack of comparison with other state-of-the-art methods addressing spurious correlation makes it hard to assess the relative performance improvements.\\n3. The paper could benefit from clearer explanations of how FairDropout precisely targets neurons responsible for memorization, as well as more comprehensive ablation studies.' Questions='1. How does the performance of FairDropout compare with other existing methods that address the issue of spurious correlation, such as those focusing on domain adaptation or distributional robustness?\\n2. What are the implications of the computational cost of FairDropout for large-scale industrial applications?\\n3. Could the approach be adapted or extended to work with structural changes to neural network architectures rather than being limited to the dropout technique?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"1. The introduction of FairDropout is a novel approach that creatively utilizes example-tied dropout to mitigate the issues arising from memorization of minority groups by deep learning models.\\n2. The paper addresses an important problem in deep learning, specifically the exploitation of spurious correlations, which impacts the generalization of models.\\n3. FairDropout's application across varied domains like vision, language, and healthcare suggests its wide applicability and potential impact.\" Weaknesses='1. The methodology, while innovative, may incur significant computational overhead due to the localization of memorization to specific neurons, potentially impacting scalability on larger datasets.\\n2. While the approach is validated across multiple benchmarks, the lack of comparison with other state-of-the-art methods addressing spurious correlation makes it hard to assess the relative performance improvements.\\n3. The paper could benefit from clearer explanations of how FairDropout precisely targets neurons responsible for memorization, as well as more comprehensive ablation studies.' Questions='1. How does the performance of FairDropout compare with other existing methods that address the issue of spurious correlation, such as those focusing on domain adaptation or distributional robustness?\\n2. What are the implications of the computational cost of FairDropout for large-scale industrial applications?\\n3. Could the approach be adapted or extended to work with structural changes to neural network architectures rather than being limited to the dropout technique?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper introduces a novel contrastive learning method specifically designed for 3D hand pose estimation from in-the-wild images, which leverages the similarity between different hand poses effectively.\\n2. It utilizes a large dataset from in-the-wild videos, which is a significant asset for training robust models.\\n3. The method outperforms state-of-the-art techniques, showing substantial gains across multiple datasets, which is a strong empirical validation of its effectiveness.' Weaknesses='1. While the paper demonstrates significant empirical improvements, the novelty of adapting contrastive learning to this specific problem may be perceived as incremental.\\n2. The method relies heavily on large-scale in-the-wild data, which may limit its applicability in scenarios where such data is unavailable or difficult to obtain.\\n3. The paper could further explore the theoretical foundations or provide more insights into why the proposed method achieves superior performance.' Questions='1. How does the proposed method perform in settings where less data is available for pre-training? Can it generalize well with smaller datasets?\\n2. What specific strategies were used for collecting and pre-processing the in-the-wild datasets?\\n3. Is there a potential for extending the proposed method to other 3D pose estimation tasks beyond hands, such as full-body or object pose estimation?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper introduces a novel contrastive learning method specifically designed for 3D hand pose estimation from in-the-wild images, which leverages the similarity between different hand poses effectively.\\n2. It utilizes a large dataset from in-the-wild videos, which is a significant asset for training robust models.\\n3. The method outperforms state-of-the-art techniques, showing substantial gains across multiple datasets, which is a strong empirical validation of its effectiveness.' Weaknesses='1. While the paper demonstrates significant empirical improvements, the novelty of adapting contrastive learning to this specific problem may be perceived as incremental.\\n2. The method relies heavily on large-scale in-the-wild data, which may limit its applicability in scenarios where such data is unavailable or difficult to obtain.\\n3. The paper could further explore the theoretical foundations or provide more insights into why the proposed method achieves superior performance.' Questions='1. How does the proposed method perform in settings where less data is available for pre-training? Can it generalize well with smaller datasets?\\n2. What specific strategies were used for collecting and pre-processing the in-the-wild datasets?\\n3. Is there a potential for extending the proposed method to other 3D pose estimation tasks beyond hands, such as full-body or object pose estimation?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. C2S introduces an innovative self-supervised framework that effectively addresses the label scarcity problem in MRI denoising by employing a novel loss function tailored for the ambient noise setting.\\n2. The method achieves a good balance between noise reduction and preservation of fine spatial features, which is often a challenging trade-off in denoising tasks.\\n3. The extension to multi-contrast denoising is a strong contribution, leveraging different MRI contrasts to enhance denoising performance.\\n4. C2S shows competitive performance compared to state-of-the-art supervised methods without requiring high SNR labels, validating its effectiveness and applicability in varied noise conditions and datasets.' Weaknesses='1. While the detail refinement extension is introduced to balance noise reduction and spatial detail preservation, the extent to which this effectively mitigates oversmoothing relative to existing methods could be demonstrated with more comprehensive visual and quantitative comparisons.\\n2. The complexity of the proposed framework, particularly in terms of parameter tuning for different noise levels and configurations, may pose a practical challenge for its adoption in clinical settings.' Questions='1. Can the approach be generalized to other forms of medical imaging modalities beyond MRI, and if so, what are the potential challenges and necessary adaptations?\\n2. How sensitive is the method to variations in the estimation of noise levels, and how does this sensitivity impact actual clinical diagnostics accuracy?\\n3. Are there scenarios where supervised methods might still outperform C2S, especially when high SNR data is partially available, and how does C2S compare under these conditions?\\n4. Could the proposed reparameterization of noise levels unintentionally introduce biases, particularly when training across different datasets with varying noise characteristics?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. C2S introduces an innovative self-supervised framework that effectively addresses the label scarcity problem in MRI denoising by employing a novel loss function tailored for the ambient noise setting.\\n2. The method achieves a good balance between noise reduction and preservation of fine spatial features, which is often a challenging trade-off in denoising tasks.\\n3. The extension to multi-contrast denoising is a strong contribution, leveraging different MRI contrasts to enhance denoising performance.\\n4. C2S shows competitive performance compared to state-of-the-art supervised methods without requiring high SNR labels, validating its effectiveness and applicability in varied noise conditions and datasets.' Weaknesses='1. While the detail refinement extension is introduced to balance noise reduction and spatial detail preservation, the extent to which this effectively mitigates oversmoothing relative to existing methods could be demonstrated with more comprehensive visual and quantitative comparisons.\\n2. The complexity of the proposed framework, particularly in terms of parameter tuning for different noise levels and configurations, may pose a practical challenge for its adoption in clinical settings.' Questions='1. Can the approach be generalized to other forms of medical imaging modalities beyond MRI, and if so, what are the potential challenges and necessary adaptations?\\n2. How sensitive is the method to variations in the estimation of noise levels, and how does this sensitivity impact actual clinical diagnostics accuracy?\\n3. Are there scenarios where supervised methods might still outperform C2S, especially when high SNR data is partially available, and how does C2S compare under these conditions?\\n4. Could the proposed reparameterization of noise levels unintentionally introduce biases, particularly when training across different datasets with varying noise characteristics?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper introduces a novel method, APG, that improves diffusion models' generation quality without significant computational overhead. The approach effectively addresses oversaturation issues at high guidance scales. The connections drawn between CFG and gradient ascent offer valuable insights, and experimental results are comprehensive, showing compatibility with various models and samplers while achieving improved scores.\" Weaknesses='The paper could further expand on the theoretical underpinnings of why the orthogonal component enhances image quality and the intuitive basis for using reverse momentum. Additionally, while the computational overhead is minimal, some discussion on potential limitations or failure cases would be beneficial.' Questions='What are the potential limitations or specific scenarios where APG might not perform well compared to CFG? Can the insights from decomposing the update term be generalized to other areas in machine learning outside diffusion models?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper introduces a novel method, APG, that improves diffusion models' generation quality without significant computational overhead. The approach effectively addresses oversaturation issues at high guidance scales. The connections drawn between CFG and gradient ascent offer valuable insights, and experimental results are comprehensive, showing compatibility with various models and samplers while achieving improved scores.\" Weaknesses='The paper could further expand on the theoretical underpinnings of why the orthogonal component enhances image quality and the intuitive basis for using reverse momentum. Additionally, while the computational overhead is minimal, some discussion on potential limitations or failure cases would be beneficial.' Questions='What are the potential limitations or specific scenarios where APG might not perform well compared to CFG? Can the insights from decomposing the update term be generalized to other areas in machine learning outside diffusion models?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=4 Strengths='The paper presents a theoretical and empirical study of the MultiFilterANN problem, contributing to the understanding of search and recommendation systems that require filter constraints. It offers provable algorithms with the best-known space-time tradeoffs for large filter regimes, which is a valuable contribution to the literature.' Weaknesses=\"The paper seems to lack clarity in connecting the theoretical analysis to the empirical methods proposed. It's not entirely clear if the empirical evaluation utilizes the proposed theoretical framework or another method entirely. Additionally, the practical relevance of the problem, especially with multiple filters, may not be clear as brute force methods might suffice in some cases.\" Questions='Can you clarify the connection between the theoretical results and the empirical algorithm used in the experiments? Are the empirical results directly derived from the proposed theoretical methods?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=4 Strengths='The paper presents a theoretical and empirical study of the MultiFilterANN problem, contributing to the understanding of search and recommendation systems that require filter constraints. It offers provable algorithms with the best-known space-time tradeoffs for large filter regimes, which is a valuable contribution to the literature.' Weaknesses=\"The paper seems to lack clarity in connecting the theoretical analysis to the empirical methods proposed. It's not entirely clear if the empirical evaluation utilizes the proposed theoretical framework or another method entirely. Additionally, the practical relevance of the problem, especially with multiple filters, may not be clear as brute force methods might suffice in some cases.\" Questions='Can you clarify the connection between the theoretical results and the empirical algorithm used in the experiments? Are the empirical results directly derived from the proposed theoretical methods?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses the challenge of singularities in PDEs for PINNs, which is a significant issue in the application of these models.\\n2. The proposed method of using diffusion models and optimal transport coupling flow introduces a novel perspective for solving the sampling point distribution problem without the need for explicit probability density modeling.\\n3. Empirical results show that the method can outperform existing sampling techniques in scenarios with complex geometries, demonstrating its potential effectiveness.' Weaknesses='1. While the proposed method provides a novel approach, the paper could further elaborate on the theoretical underpinnings and provide more clarity on how flow-matching precisely contributes to the overall performance improvements.\\n2. The comparison with existing methods, though provided, could be more comprehensive to better establish the advantages and limitations of the proposed method.\\n3. The presentation of some technical details might be too brief for full comprehension without prior knowledge of related advanced concepts like diffusion models and flow-matching.' Questions='1. Can the proposed sampling method be easily adapted to other types of partial differential equations beyond the examples provided?\\n2. How does the computational complexity of the proposed method compare to existing sampling techniques for PINNs, especially in larger scale problems?\\n3. Are there scenarios where the proposed flow-matching method might perform less effectively than traditional techniques, and if so, what are the potential causes?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses the challenge of singularities in PDEs for PINNs, which is a significant issue in the application of these models.\\n2. The proposed method of using diffusion models and optimal transport coupling flow introduces a novel perspective for solving the sampling point distribution problem without the need for explicit probability density modeling.\\n3. Empirical results show that the method can outperform existing sampling techniques in scenarios with complex geometries, demonstrating its potential effectiveness.' Weaknesses='1. While the proposed method provides a novel approach, the paper could further elaborate on the theoretical underpinnings and provide more clarity on how flow-matching precisely contributes to the overall performance improvements.\\n2. The comparison with existing methods, though provided, could be more comprehensive to better establish the advantages and limitations of the proposed method.\\n3. The presentation of some technical details might be too brief for full comprehension without prior knowledge of related advanced concepts like diffusion models and flow-matching.' Questions='1. Can the proposed sampling method be easily adapted to other types of partial differential equations beyond the examples provided?\\n2. How does the computational complexity of the proposed method compare to existing sampling techniques for PINNs, especially in larger scale problems?\\n3. Are there scenarios where the proposed flow-matching method might perform less effectively than traditional techniques, and if so, what are the potential causes?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The introduction of MathHay as a benchmark for evaluating long-context mathematical reasoning capabilities in LLMs fills a critical gap in the current research landscape. The paper provides extensive experimental evaluation, highlighting the limitations of existing LLMs in handling complex mathematical reasoning over long contexts.' Weaknesses='The paper could provide more detailed explanations of the MathHay benchmark construction and the types of mathematical reasoning tasks it includes. A deeper analysis of why current LLMs struggle with mathematical reasoning in long contexts would enhance the paper.' Questions='What types of mathematical reasoning tasks are included in the MathHay benchmark? How does MathHay differ from other benchmarks besides requiring mathematical reasoning over long contexts? What are the key factors contributing to the difficulty of mathematical reasoning tasks in long contexts for LLMs?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The introduction of MathHay as a benchmark for evaluating long-context mathematical reasoning capabilities in LLMs fills a critical gap in the current research landscape. The paper provides extensive experimental evaluation, highlighting the limitations of existing LLMs in handling complex mathematical reasoning over long contexts.' Weaknesses='The paper could provide more detailed explanations of the MathHay benchmark construction and the types of mathematical reasoning tasks it includes. A deeper analysis of why current LLMs struggle with mathematical reasoning in long contexts would enhance the paper.' Questions='What types of mathematical reasoning tasks are included in the MathHay benchmark? How does MathHay differ from other benchmarks besides requiring mathematical reasoning over long contexts? What are the key factors contributing to the difficulty of mathematical reasoning tasks in long contexts for LLMs?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper addresses a significant and relevant challenge in machine learning concerning data accessibility due to privacy and proprietary concerns.\\n2. Mycroft offers a novel approach by utilizing feature space distances and gradient matching to extract informative data subsets efficiently.\\n3. Experimental results demonstrate that the method can rapidly converge to a baseline performance similar to that with full data sharing, suggesting practical utility.\\n4. The robustness to noise and efficacy in ranking data owners by utility are notable strengths, potentially allowing for more targeted and efficient collaborations.' Weaknesses='1. The methodology and algorithms, such as RetrieveTopK, could be better explained to enhances clarity and understanding.\\n2. Complexity in presentation, where some aspects (e.g., Algorithm descriptions) are not adequately supported by textual explanations, making them difficult to follow.\\n3. The proof of Theorem 3.1 contains unclear elements, such as the derivation of specific inequalities, reducing confidence in the theoretical soundness.\\n4. Evaluation relies heavily on subsets of hard data, without clearly demonstrating performance improvements on broader datasets, questioning general applicability.\\n5. Absence of experiments in actual privacy-protective scenarios makes it challenging to assess real-world feasibility.' Questions=\"1. How does the RetrieveTopK function in Algorithm 3 ensure the coverage of D^hard?\\n2. Could the authors provide a clearer explanation for the derivation of inequalities in the proof of Theorem 3.1?\\n3. Why is there a divergence in Algorithm 2 from Elenberg et al.'s Algorithm 3, and how does it account for feature-regularization gradients?\\n4. How were datasets and tasks chosen for experiments in Section 4, and are they benchmark datasets in this field?\\n5. What adjustments might be necessary for Mycroft when applied in genuinely privacy-protective contexts, and how would they impact its performance?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper addresses a significant and relevant challenge in machine learning concerning data accessibility due to privacy and proprietary concerns.\\n2. Mycroft offers a novel approach by utilizing feature space distances and gradient matching to extract informative data subsets efficiently.\\n3. Experimental results demonstrate that the method can rapidly converge to a baseline performance similar to that with full data sharing, suggesting practical utility.\\n4. The robustness to noise and efficacy in ranking data owners by utility are notable strengths, potentially allowing for more targeted and efficient collaborations.' Weaknesses='1. The methodology and algorithms, such as RetrieveTopK, could be better explained to enhances clarity and understanding.\\n2. Complexity in presentation, where some aspects (e.g., Algorithm descriptions) are not adequately supported by textual explanations, making them difficult to follow.\\n3. The proof of Theorem 3.1 contains unclear elements, such as the derivation of specific inequalities, reducing confidence in the theoretical soundness.\\n4. Evaluation relies heavily on subsets of hard data, without clearly demonstrating performance improvements on broader datasets, questioning general applicability.\\n5. Absence of experiments in actual privacy-protective scenarios makes it challenging to assess real-world feasibility.' Questions=\"1. How does the RetrieveTopK function in Algorithm 3 ensure the coverage of D^hard?\\n2. Could the authors provide a clearer explanation for the derivation of inequalities in the proof of Theorem 3.1?\\n3. Why is there a divergence in Algorithm 2 from Elenberg et al.'s Algorithm 3, and how does it account for feature-regularization gradients?\\n4. How were datasets and tasks chosen for experiments in Section 4, and are they benchmark datasets in this field?\\n5. What adjustments might be necessary for Mycroft when applied in genuinely privacy-protective contexts, and how would they impact its performance?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The development of the Domain Indexing Collaborative Filtering (DICF) framework provides both performance improvements and interpretability, addressing the important issue of cold-start items in cross-domain recommendation systems. Additionally, the incorporation of adversarial and Bayesian framework enhances the robustness of the methodology.' Weaknesses='A potential weakness is the lack of clarity on how this framework integrates with existing systems and what kind of computational overhead it introduces. Also, more detailed empirical evidence across a wider variety of real-world datasets would strengthen the claims.' Questions='How does DICF compare with other state-of-the-art cross-domain recommendation systems concerning scalability and computational efficiency? Are there specific parameters or conditions under which DICF struggles?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The development of the Domain Indexing Collaborative Filtering (DICF) framework provides both performance improvements and interpretability, addressing the important issue of cold-start items in cross-domain recommendation systems. Additionally, the incorporation of adversarial and Bayesian framework enhances the robustness of the methodology.' Weaknesses='A potential weakness is the lack of clarity on how this framework integrates with existing systems and what kind of computational overhead it introduces. Also, more detailed empirical evidence across a wider variety of real-world datasets would strengthen the claims.' Questions='How does DICF compare with other state-of-the-art cross-domain recommendation systems concerning scalability and computational efficiency? Are there specific parameters or conditions under which DICF struggles?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The proposed method, SteerPrompt, offers an innovative approach to improve LLMs' comprehension by steering their attention without altering model parameters. The method is applied at inference time, making it resource-efficient. The experiments show significant improvement in performance on open-book QA tasks.\" Weaknesses='The paper primarily demonstrates the effectiveness of SteerPrompt in open-book QA, leaving its performance in other tasks unexplored. The dependency on accurately identifying key information may limit its effectiveness in contexts with ambiguous or misleading information.' Questions='Can SteerPrompt be effectively applied to other NLP tasks beyond open-book QA? How does the method handle cases where the key contextual information is not easily identifiable or is highly ambiguous?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The proposed method, SteerPrompt, offers an innovative approach to improve LLMs' comprehension by steering their attention without altering model parameters. The method is applied at inference time, making it resource-efficient. The experiments show significant improvement in performance on open-book QA tasks.\" Weaknesses='The paper primarily demonstrates the effectiveness of SteerPrompt in open-book QA, leaving its performance in other tasks unexplored. The dependency on accurately identifying key information may limit its effectiveness in contexts with ambiguous or misleading information.' Questions='Can SteerPrompt be effectively applied to other NLP tasks beyond open-book QA? How does the method handle cases where the key contextual information is not easily identifiable or is highly ambiguous?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=3 Strengths='1. The paper presents Vision-RWKV, an adaptation for vision tasks that demonstrates superior performance over traditional models. \\n2. VRWKV is efficient in high-resolution image processing, showcasing improvements in both speed and memory usage. \\n3. The model retains robust global processing capabilities alongside reduced spatial aggregation complexity.' Weaknesses='1. The novelty of modifying RWKV for vision tasks, while valuable, may not be highly groundbreaking as it involves adaptation rather than a completely novel architecture. \\n2. The direct comparison with other state-of-the-art models in dense prediction tasks might need stronger emphasis to solidify claims on performance.' Questions='1. How does Vision-RWKV compare with other linear attention models like mamba in vision tasks?\\n2. Can the model handle ultra-high-resolution images without performance degradation, and are there specific limits observed through experiments?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=3 Strengths='1. The paper presents Vision-RWKV, an adaptation for vision tasks that demonstrates superior performance over traditional models. \\n2. VRWKV is efficient in high-resolution image processing, showcasing improvements in both speed and memory usage. \\n3. The model retains robust global processing capabilities alongside reduced spatial aggregation complexity.' Weaknesses='1. The novelty of modifying RWKV for vision tasks, while valuable, may not be highly groundbreaking as it involves adaptation rather than a completely novel architecture. \\n2. The direct comparison with other state-of-the-art models in dense prediction tasks might need stronger emphasis to solidify claims on performance.' Questions='1. How does Vision-RWKV compare with other linear attention models like mamba in vision tasks?\\n2. Can the model handle ultra-high-resolution images without performance degradation, and are there specific limits observed through experiments?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The study provides a comprehensive theoretical analysis along with empirical validation of a novel optimizer, S3, which clearly shows improvements over traditional Adam by addressing its susceptibility to loss spikes. The proposed method is innovative and particularly strong in effectively managing large updates and enhancing convergence speed with memory efficiency.' Weaknesses=\"The paper could provide more details on the specific benchmarks used for various vision and language tasks to allow for more precise evaluation of the optimizer's performance relative to standard benchmarks. Additionally, it could further elaborate on the potential limitations of the method under different problem setups or distributions.\" Questions='How does S3 perform in comparison to other recent optimizers beyond AdamW given similar experimental conditions? Is there a possibility of S3 to be adapted or customized for specific types of neural network architectures or problems?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The study provides a comprehensive theoretical analysis along with empirical validation of a novel optimizer, S3, which clearly shows improvements over traditional Adam by addressing its susceptibility to loss spikes. The proposed method is innovative and particularly strong in effectively managing large updates and enhancing convergence speed with memory efficiency.' Weaknesses=\"The paper could provide more details on the specific benchmarks used for various vision and language tasks to allow for more precise evaluation of the optimizer's performance relative to standard benchmarks. Additionally, it could further elaborate on the potential limitations of the method under different problem setups or distributions.\" Questions='How does S3 perform in comparison to other recent optimizers beyond AdamW given similar experimental conditions? Is there a possibility of S3 to be adapted or customized for specific types of neural network architectures or problems?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses key challenges in compositional zero-shot learning by proposing the TRIDENT framework, which effectively combines multimodal large language model embeddings and attribute smoothing. The proposed method shows strong state-of-the-art results on three challenging datasets, demonstrating its efficacy.' Weaknesses=\"The presentation of some elements, such as Figure 2, could be clearer to aid in understanding the framework, and there are inaccuracies in certain expressions like equation (12). Moreover, while extensive experiments are conducted, there is a lack of comparison with the most recent works in 2024, such as Troika, which may affect the paper's contemporariness.\" Questions='How does the proposed TRIDENT framework compare to the latest 2024 works like Troika in terms of performance? Could the final prediction process further benefit from the attribute-object disentanglement and feature alignment modules introduced in the method section?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses key challenges in compositional zero-shot learning by proposing the TRIDENT framework, which effectively combines multimodal large language model embeddings and attribute smoothing. The proposed method shows strong state-of-the-art results on three challenging datasets, demonstrating its efficacy.' Weaknesses=\"The presentation of some elements, such as Figure 2, could be clearer to aid in understanding the framework, and there are inaccuracies in certain expressions like equation (12). Moreover, while extensive experiments are conducted, there is a lack of comparison with the most recent works in 2024, such as Troika, which may affect the paper's contemporariness.\" Questions='How does the proposed TRIDENT framework compare to the latest 2024 works like Troika in terms of performance? Could the final prediction process further benefit from the attribute-object disentanglement and feature alignment modules introduced in the method section?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper introduces an innovative approach by merging state-space models (SSMs) with time-varying counterfactual predictions (TCP), capitalizing on the strengths of SSMs for long-sequence modeling enhancement. The proposed Mamba-CDSP framework addresses confounding bias by de-correlating treatment with covariates, enhancing TCP's effectiveness and efficiency. Extensive experiments on synthetic and real-world datasets demonstrate the model's superiority over baselines in both performance and efficiency.\" Weaknesses='The paper relies heavily on technical jargon, which may obfuscate its contributions for broader audiences. The methodological novelty could be questioned, as the integration of SSMs into TCP is not a radical departure but rather an extension leveraging existing methods. Additional clarity on comparative advantages over related methods would strengthen the contribution.' Questions='How does the proposed method compare in computational cost relative to the baseline models under similar conditions? Are there specific types of data or situations where Mamba-CDSP might underperform compared to other baselines?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper introduces an innovative approach by merging state-space models (SSMs) with time-varying counterfactual predictions (TCP), capitalizing on the strengths of SSMs for long-sequence modeling enhancement. The proposed Mamba-CDSP framework addresses confounding bias by de-correlating treatment with covariates, enhancing TCP's effectiveness and efficiency. Extensive experiments on synthetic and real-world datasets demonstrate the model's superiority over baselines in both performance and efficiency.\" Weaknesses='The paper relies heavily on technical jargon, which may obfuscate its contributions for broader audiences. The methodological novelty could be questioned, as the integration of SSMs into TCP is not a radical departure but rather an extension leveraging existing methods. Additional clarity on comparative advantages over related methods would strengthen the contribution.' Questions='How does the proposed method compare in computational cost relative to the baseline models under similar conditions? Are there specific types of data or situations where Mamba-CDSP might underperform compared to other baselines?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths=\"1. The paper identifies an important issue in AI alignment where strong models may deceive weak models by producing misaligned behaviors, contributing valuable insights into the safety and reliability of superalignment.\\n2. The experimental setup is comprehensive and systematically explores the phenomenon across various settings and model configurations, enhancing the credibility of the findings.\\n3. The research introduces the concept of 'weak-to-strong deception,' enriching the discourse on model supervision and alignment beyond traditional paradigms.\" Weaknesses='1. The paper primarily focuses on a specific case of multi-objective alignment, which may limit the generalizability of the findings to broader contexts in AI alignment.\\n2. While the concept of deception is well-explored, the proposed mitigation strategies, such as bootstrapping with intermediate models, appear to have limited effectiveness, suggesting the need for more robust solutions.\\n3. The abstract lacks a thorough explanation of the theoretical framework underpinning the concept of deception, which would enhance the understanding of the phenomena observed.' Questions='1. How might the phenomenon of weak-to-strong deception manifest in real-world applications, and what are the implications for industry practices?\\n2. What other strategies, besides bootstrapping, could be explored to mitigate deception?\\n3. Could the study be extended to explore the impact of deception in more diverse and complex alignment scenarios?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths=\"1. The paper identifies an important issue in AI alignment where strong models may deceive weak models by producing misaligned behaviors, contributing valuable insights into the safety and reliability of superalignment.\\n2. The experimental setup is comprehensive and systematically explores the phenomenon across various settings and model configurations, enhancing the credibility of the findings.\\n3. The research introduces the concept of 'weak-to-strong deception,' enriching the discourse on model supervision and alignment beyond traditional paradigms.\" Weaknesses='1. The paper primarily focuses on a specific case of multi-objective alignment, which may limit the generalizability of the findings to broader contexts in AI alignment.\\n2. While the concept of deception is well-explored, the proposed mitigation strategies, such as bootstrapping with intermediate models, appear to have limited effectiveness, suggesting the need for more robust solutions.\\n3. The abstract lacks a thorough explanation of the theoretical framework underpinning the concept of deception, which would enhance the understanding of the phenomena observed.' Questions='1. How might the phenomenon of weak-to-strong deception manifest in real-world applications, and what are the implications for industry practices?\\n2. What other strategies, besides bootstrapping, could be explored to mitigate deception?\\n3. Could the study be extended to explore the impact of deception in more diverse and complex alignment scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel approach to improving spatial compression in autoencoders, enabling higher compression ratios while maintaining quality. The proposed techniques, Residual Autoencoding and Decoupled High-Resolution Adaptation, are innovative and show significant practical implications by achieving notable speedups on large-scale datasets.' Weaknesses='While the paper shows promising results, details about the potential limitations or challenges with the proposed techniques are limited. It would be beneficial to have a more detailed comparison with existing methods in various scenarios to highlight any specific shortcomings or trade-offs.' Questions='How does the performance of the proposed DC-AE compare to traditional methods in low-compression scenarios? Are there any scenarios where this approach might not be advantageous?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel approach to improving spatial compression in autoencoders, enabling higher compression ratios while maintaining quality. The proposed techniques, Residual Autoencoding and Decoupled High-Resolution Adaptation, are innovative and show significant practical implications by achieving notable speedups on large-scale datasets.' Weaknesses='While the paper shows promising results, details about the potential limitations or challenges with the proposed techniques are limited. It would be beneficial to have a more detailed comparison with existing methods in various scenarios to highlight any specific shortcomings or trade-offs.' Questions='How does the performance of the proposed DC-AE compare to traditional methods in low-compression scenarios? Are there any scenarios where this approach might not be advantageous?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a novel framework, Heterogeneous-PSRO, that extends the capabilities of Policy Space Response Oracles to heterogeneous team games, marking a significant advancement. Theoretical proofs support the claims of improved exploitability, demonstrating substantial soundness. Empirical results indicate performance benefits over existing methods in both heterogeneous and homogeneous games.' Weaknesses=\"Despite the strong theoretical framework, the paper might require clarification in terms of algorithmic complexity and practical applicability. It's also important to benchmark H-PSRO against more diverse game settings to demonstrate robustness across a wider array of challenges. Additional clarity on handling computational constraints of larger scale applications would be valuable.\" Questions='How does the computational complexity of H-PSRO compare to Team PSRO, especially in very large or complex games? Can the advantages observed in matrix heterogeneous games be reliably extended to continuous action spaces?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a novel framework, Heterogeneous-PSRO, that extends the capabilities of Policy Space Response Oracles to heterogeneous team games, marking a significant advancement. Theoretical proofs support the claims of improved exploitability, demonstrating substantial soundness. Empirical results indicate performance benefits over existing methods in both heterogeneous and homogeneous games.' Weaknesses=\"Despite the strong theoretical framework, the paper might require clarification in terms of algorithmic complexity and practical applicability. It's also important to benchmark H-PSRO against more diverse game settings to demonstrate robustness across a wider array of challenges. Additional clarity on handling computational constraints of larger scale applications would be valuable.\" Questions='How does the computational complexity of H-PSRO compare to Team PSRO, especially in very large or complex games? Can the advantages observed in matrix heterogeneous games be reliably extended to continuous action spaces?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The model SaMoye represents a significant step forward in SVC, particularly in terms of zero-shot performance. The disentanglement of singing voice features into content, timbre, and pitch, along with the innovative use of ASR models to compress content features, reduces timbre leakages effectively. The large-scale dataset supports zero-shot capabilities and provides a solid benchmark for future research. The experiments demonstrate impressive results in both human and non-human timbre conversion.' Weaknesses='Details on the specific architecture and training strategies for SaMoye are limited, and while the large dataset is a strength, the specifics of its collection and curation are not fully described. There could be more depth regarding the objective and subjective evaluation methods used, as well as potential biases introduced by the dataset composition.' Questions='How does the SaMoye model handle highly variable input in terms of vocal techniques or background noise in singing voice recordings? Is there a detailed analysis of how different ASR models contribute to reducing timbre leakages?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The model SaMoye represents a significant step forward in SVC, particularly in terms of zero-shot performance. The disentanglement of singing voice features into content, timbre, and pitch, along with the innovative use of ASR models to compress content features, reduces timbre leakages effectively. The large-scale dataset supports zero-shot capabilities and provides a solid benchmark for future research. The experiments demonstrate impressive results in both human and non-human timbre conversion.' Weaknesses='Details on the specific architecture and training strategies for SaMoye are limited, and while the large dataset is a strength, the specifics of its collection and curation are not fully described. There could be more depth regarding the objective and subjective evaluation methods used, as well as potential biases introduced by the dataset composition.' Questions='How does the SaMoye model handle highly variable input in terms of vocal techniques or background noise in singing voice recordings? Is there a detailed analysis of how different ASR models contribute to reducing timbre leakages?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel framework, Differentiable Map Priors, that effectively integrates spatial priors into 3D perception systems at minimal computational cost, demonstrating consistent improvements across tasks such as 3D object detection and map segmentation on the nuScenes dataset. The idea is both simple and impactful, addressing a common oversight in autonomous systems by leveraging historical traversal data.' Weaknesses='While the concept of incorporating spatial priors is innovative, the actual improvements in performance metrics are relatively modest, which might limit the perceived impact. Additionally, there are unclear aspects regarding the scalability of the sparse voxel grid representation used to encode the map priors and specific details on how various resolutions and levels are managed.' Questions='1. How does the system handle dynamic objects that may not adhere to static map priors?\\n2. Can the approach be adapted or extended to handle fewer pre-existing traversal data effectively?\\n3. What are the specific memory and computational overheads introduced by using Differentiable Map Priors compared to existing approaches?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel framework, Differentiable Map Priors, that effectively integrates spatial priors into 3D perception systems at minimal computational cost, demonstrating consistent improvements across tasks such as 3D object detection and map segmentation on the nuScenes dataset. The idea is both simple and impactful, addressing a common oversight in autonomous systems by leveraging historical traversal data.' Weaknesses='While the concept of incorporating spatial priors is innovative, the actual improvements in performance metrics are relatively modest, which might limit the perceived impact. Additionally, there are unclear aspects regarding the scalability of the sparse voxel grid representation used to encode the map priors and specific details on how various resolutions and levels are managed.' Questions='1. How does the system handle dynamic objects that may not adhere to static map priors?\\n2. Can the approach be adapted or extended to handle fewer pre-existing traversal data effectively?\\n3. What are the specific memory and computational overheads introduced by using Differentiable Map Priors compared to existing approaches?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper presents a novel graph attention method, RGA-IB, based on Information Bottleneck, which helps improve robustness against adversarial attacks.\\n2. Extensive experiments demonstrate the effectiveness of the proposed method in reducing IB loss and enhancing node classification accuracy under various adversarial attacks.' Weaknesses=\"1. The connection between IB loss reduction and improved robustness could be further explored and detailed.\\n2. Comparisons to more diverse baselines, including those focused on heterophilic datasets or other types of attacks, would strengthen the evaluation.\\n3. Presentation could be improved for readability, particularly in the method's section organization.\" Questions='1. How does RGA-IB perform on heterophilic datasets compared to other robust GNN methods?\\n2. Can the authors elaborate on why RGA-IB is more effective in reducing IB loss compared to other IB-based models?\\n3. Are there specific hyperparameter settings that critically influence the performance of RGA-IB, especially under different attack scenarios?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper presents a novel graph attention method, RGA-IB, based on Information Bottleneck, which helps improve robustness against adversarial attacks.\\n2. Extensive experiments demonstrate the effectiveness of the proposed method in reducing IB loss and enhancing node classification accuracy under various adversarial attacks.' Weaknesses=\"1. The connection between IB loss reduction and improved robustness could be further explored and detailed.\\n2. Comparisons to more diverse baselines, including those focused on heterophilic datasets or other types of attacks, would strengthen the evaluation.\\n3. Presentation could be improved for readability, particularly in the method's section organization.\" Questions='1. How does RGA-IB perform on heterophilic datasets compared to other robust GNN methods?\\n2. Can the authors elaborate on why RGA-IB is more effective in reducing IB loss compared to other IB-based models?\\n3. Are there specific hyperparameter settings that critically influence the performance of RGA-IB, especially under different attack scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths='The paper presents a novel deep learning solution with a dual attention mechanism, clearly detailing how it advances the field of immunogenicity prediction. The compilation of the most comprehensive immunogenicity dataset to date is a significant contribution that will benefit future research in this area. The extensive experiments demonstrating superior performance across multiple evaluation metrics underscore the robustness of the proposed approach.' Weaknesses='While the approach and dataset compilation are strong, the paper could enhance its discussion on the limitations and potential downsides of the proposed method. Additionally, it first mentions a post-hoc validation but does not detail the specific results and findings from this evaluation, which could help in understanding its practical applicability better.' Questions='Can you elaborate on how the dual attention mechanism specifically contributes to performance improvements? Additionally, how does the model handle the variability in sequence input length and potential batch effects from different datasets within the compiled database?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths='The paper presents a novel deep learning solution with a dual attention mechanism, clearly detailing how it advances the field of immunogenicity prediction. The compilation of the most comprehensive immunogenicity dataset to date is a significant contribution that will benefit future research in this area. The extensive experiments demonstrating superior performance across multiple evaluation metrics underscore the robustness of the proposed approach.' Weaknesses='While the approach and dataset compilation are strong, the paper could enhance its discussion on the limitations and potential downsides of the proposed method. Additionally, it first mentions a post-hoc validation but does not detail the specific results and findings from this evaluation, which could help in understanding its practical applicability better.' Questions='Can you elaborate on how the dual attention mechanism specifically contributes to performance improvements? Additionally, how does the model handle the variability in sequence input length and potential batch effects from different datasets within the compiled database?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper addresses a pertinent issue in multi-view classification by considering the realistic scenario where different views may have conflicting information. The proposed computational trust-based discounting method for multi-view classification is innovative, as it introduces a nuanced way to handle conflicts by focusing on the reliability of each view through an instance-wise approach. The use of real-world datasets and multiple metrics for evaluation, including a novel metric for assessing multi-view agreement with ground truth, strengthens the results and their relevance to practical applications.' Weaknesses='The presentation of the method lacks some clarity and could benefit from a more detailed explanation of the computational trust mechanism and how it integrates into the evidential framework. The assumptions underlying the trust discounting process are not fully explored, potentially limiting understanding of when the method is most applicable. Additionally, while the evaluation is thorough, there is limited discussion of the computational complexity introduced by the trust mechanism and how it scales with larger datasets.' Questions='How does the computational trust-based discounting method perform on datasets with highly imbalanced view reliabilities? Can the proposed approach be easily integrated into existing multi-view systems, or does it require significant modifications? What are the potential limitations of using the Multi-View Agreement with Ground Truth metric, and how does it complement existing metrics like Top-1 Accuracy and Fleiss’ Kappa?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper addresses a pertinent issue in multi-view classification by considering the realistic scenario where different views may have conflicting information. The proposed computational trust-based discounting method for multi-view classification is innovative, as it introduces a nuanced way to handle conflicts by focusing on the reliability of each view through an instance-wise approach. The use of real-world datasets and multiple metrics for evaluation, including a novel metric for assessing multi-view agreement with ground truth, strengthens the results and their relevance to practical applications.' Weaknesses='The presentation of the method lacks some clarity and could benefit from a more detailed explanation of the computational trust mechanism and how it integrates into the evidential framework. The assumptions underlying the trust discounting process are not fully explored, potentially limiting understanding of when the method is most applicable. Additionally, while the evaluation is thorough, there is limited discussion of the computational complexity introduced by the trust mechanism and how it scales with larger datasets.' Questions='How does the computational trust-based discounting method perform on datasets with highly imbalanced view reliabilities? Can the proposed approach be easily integrated into existing multi-view systems, or does it require significant modifications? What are the potential limitations of using the Multi-View Agreement with Ground Truth metric, and how does it complement existing metrics like Top-1 Accuracy and Fleiss’ Kappa?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"The paper explores a novel approach to understand neuronal entanglement in large language models using Wasserstein distance, which is a relatively unexplored area. The identification of 'Wasserstein Neurons' provides insight into the impact of entangled neurons on model performance. The proposed methodology offers a new perspective on disentangling polysemantic neurons and optimizing neural networks post-training.\" Weaknesses='The methodology could benefit from a more comprehensive evaluation across diverse datasets and architectures to validate its generalizability. The experimental framework and results, while promising, may lack sufficient depth and analysis in terms of practical applications and potential limitations. The paper also might not sufficiently address how the approach compares to existing techniques in the literature.' Questions='1. How does the proposed method compare to other disentanglement techniques in terms of computational efficiency and effectiveness?\\n2. Is the approach applicable to other types of neural networks beyond large language models, and if so, what modifications would be necessary?\\n3. How does the choice of Gaussian distribution as a reference point affect the results, and are there alternative distributions that could be more appropriate?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"The paper explores a novel approach to understand neuronal entanglement in large language models using Wasserstein distance, which is a relatively unexplored area. The identification of 'Wasserstein Neurons' provides insight into the impact of entangled neurons on model performance. The proposed methodology offers a new perspective on disentangling polysemantic neurons and optimizing neural networks post-training.\" Weaknesses='The methodology could benefit from a more comprehensive evaluation across diverse datasets and architectures to validate its generalizability. The experimental framework and results, while promising, may lack sufficient depth and analysis in terms of practical applications and potential limitations. The paper also might not sufficiently address how the approach compares to existing techniques in the literature.' Questions='1. How does the proposed method compare to other disentanglement techniques in terms of computational efficiency and effectiveness?\\n2. Is the approach applicable to other types of neural networks beyond large language models, and if so, what modifications would be necessary?\\n3. How does the choice of Gaussian distribution as a reference point affect the results, and are there alternative distributions that could be more appropriate?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a novel approach leveraging quantum computations for parameter-efficient fine-tuning, offering a significant reduction in trainable parameters while maintaining competitive performance. The use of quantum unitary parameterization powered by Pauli matrices is innovative and shows promising results on transfer learning benchmarks, demonstrating the potential of quantum concepts in enhancing parameter efficiency.' Weaknesses='The technical depth and novelty of using quantum strategies in PEFT may pose challenges in practical implementation and understanding for the broader machine learning community. Further, the abstract lacks detail on how Quantum-PEFT compares quantitatively with existing methods in terms of both parameter efficiency and actual performance gains beyond brief mentions.' Questions='How does the proposed method affect the computational overhead compared to traditional PEFT methods like LoRA? How scalable is Quantum-PEFT in practice with larger models and datasets, and are there potential limitations due to the quantum computation techniques employed?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a novel approach leveraging quantum computations for parameter-efficient fine-tuning, offering a significant reduction in trainable parameters while maintaining competitive performance. The use of quantum unitary parameterization powered by Pauli matrices is innovative and shows promising results on transfer learning benchmarks, demonstrating the potential of quantum concepts in enhancing parameter efficiency.' Weaknesses='The technical depth and novelty of using quantum strategies in PEFT may pose challenges in practical implementation and understanding for the broader machine learning community. Further, the abstract lacks detail on how Quantum-PEFT compares quantitatively with existing methods in terms of both parameter efficiency and actual performance gains beyond brief mentions.' Questions='How does the proposed method affect the computational overhead compared to traditional PEFT methods like LoRA? How scalable is Quantum-PEFT in practice with larger models and datasets, and are there potential limitations due to the quantum computation techniques employed?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a novel and practical approach to solve bilevel optimization problems by utilizing Bayesian optimization techniques, effectively addressing issues in noisy, constrained, and derivative-free settings. The algorithm, BILBO, provides a strong theoretical foundation with a proven sublinear regret bound and is empirically validated across multiple data sets.' Weaknesses='The use of specific configurations and scenarios for experiments is not sufficiently justified. There is also limited discussion on the potential limitations and assumptions underlying the proposed method. Comparisons to alternative approaches in the bilevel optimization field could be more comprehensive to better highlight the improvements brought by BILBO.' Questions='How does the algorithm perform under different noise conditions or constraint settings? Are there any specific types of problems or domains where the proposed approach might struggle?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a novel and practical approach to solve bilevel optimization problems by utilizing Bayesian optimization techniques, effectively addressing issues in noisy, constrained, and derivative-free settings. The algorithm, BILBO, provides a strong theoretical foundation with a proven sublinear regret bound and is empirically validated across multiple data sets.' Weaknesses='The use of specific configurations and scenarios for experiments is not sufficiently justified. There is also limited discussion on the potential limitations and assumptions underlying the proposed method. Comparisons to alternative approaches in the bilevel optimization field could be more comprehensive to better highlight the improvements brought by BILBO.' Questions='How does the algorithm perform under different noise conditions or constraint settings? Are there any specific types of problems or domains where the proposed approach might struggle?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel multi-agent framework that integrates multiple data selection strategies to enhance the pretraining efficiency of large language models. The extensive empirical studies demonstrate a significant improvement in data efficiency and convergence speed, showcasing the practical utility of the approach.' Weaknesses='The inherent complexity of the multi-agent framework may pose challenges in terms of implementation and interpretability. Additionally, the paper does not provide detailed insights into the specific conflicts between existing data selection methods, which could strengthen the justification for the proposed solution.' Questions=\"How do the agents' prioritization criteria adapt over time, and what specific strategies are effective in resolving conflicts between agents' selections? Are there any scalability concerns when applying this multi-agent approach to larger models or datasets?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel multi-agent framework that integrates multiple data selection strategies to enhance the pretraining efficiency of large language models. The extensive empirical studies demonstrate a significant improvement in data efficiency and convergence speed, showcasing the practical utility of the approach.' Weaknesses='The inherent complexity of the multi-agent framework may pose challenges in terms of implementation and interpretability. Additionally, the paper does not provide detailed insights into the specific conflicts between existing data selection methods, which could strengthen the justification for the proposed solution.' Questions=\"How do the agents' prioritization criteria adapt over time, and what specific strategies are effective in resolving conflicts between agents' selections? Are there any scalability concerns when applying this multi-agent approach to larger models or datasets?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a thorough empirical investigation into data scaling laws in the realm of robotic manipulation, providing valuable insights into how generalization performance is influenced by environmental and object diversity. It demonstrates that a practical data collection strategy can lead to significant success rates in novel environments, showcasing its applicability.' Weaknesses=\"The paper's conclusions are primarily based on experiments with restricted tasks and object categories, potentially limiting the generalizability of the findings to more complex or varied scenarios. Additionally, while the paper claims to address the potential for zero-shot deployment, the specific details on task complexity and types of objects are not entirely clear from the presented abstract.\" Questions='What types of objects and environments were selected for the experiments, and how do they reflect real-world diversity?\\nHow do the authors intend to handle more complex manipulation tasks that may require different data scaling strategies?\\nAre there any insights on the potential limitations of the proposed data collection strategy if applied to varied or more dynamic environments?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a thorough empirical investigation into data scaling laws in the realm of robotic manipulation, providing valuable insights into how generalization performance is influenced by environmental and object diversity. It demonstrates that a practical data collection strategy can lead to significant success rates in novel environments, showcasing its applicability.' Weaknesses=\"The paper's conclusions are primarily based on experiments with restricted tasks and object categories, potentially limiting the generalizability of the findings to more complex or varied scenarios. Additionally, while the paper claims to address the potential for zero-shot deployment, the specific details on task complexity and types of objects are not entirely clear from the presented abstract.\" Questions='What types of objects and environments were selected for the experiments, and how do they reflect real-world diversity?\\nHow do the authors intend to handle more complex manipulation tasks that may require different data scaling strategies?\\nAre there any insights on the potential limitations of the proposed data collection strategy if applied to varied or more dynamic environments?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths=\"1. The paper introduces a self-evolving framework for AI agents inspired by the Gödel machine, which is a novel approach allowing agents to improve autonomously without predefined routines.\\n2. The experimental results show that Gödel Agent surpasses manually crafted agents in performance, efficiency, and generalizability.\\n3. The use of large language models to dynamically modify the agent's logic and behavior is an innovative concept and is shown to achieve continuous self-improvement.\" Weaknesses='1. The concept, while innovative, may face challenges in practical implementation and scaling across diverse applications beyond the tested mathematical reasoning and complex agent tasks.\\n2. The reliance on LLMs might pose limitations in scenarios where data availability or computational resources are restricted.\\n3. The evaluation results, while promising, may require verification across a broader set of benchmarks to validate the generalizability of the proposed framework.' Questions=\"1. How does the Gödel Agent ensure stability and prevent undesirable behavior when it modifies its logic and behavior autonomously?\\n2. Are there specific tasks where the Gödel Agent's self-improvement process becomes less effective, and how can these limitations be addressed?\\n3. How does the system handle the computational cost associated with leveraging LLMs for continual self-improvement, and is there a mechanism to optimize this process?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths=\"1. The paper introduces a self-evolving framework for AI agents inspired by the Gödel machine, which is a novel approach allowing agents to improve autonomously without predefined routines.\\n2. The experimental results show that Gödel Agent surpasses manually crafted agents in performance, efficiency, and generalizability.\\n3. The use of large language models to dynamically modify the agent's logic and behavior is an innovative concept and is shown to achieve continuous self-improvement.\" Weaknesses='1. The concept, while innovative, may face challenges in practical implementation and scaling across diverse applications beyond the tested mathematical reasoning and complex agent tasks.\\n2. The reliance on LLMs might pose limitations in scenarios where data availability or computational resources are restricted.\\n3. The evaluation results, while promising, may require verification across a broader set of benchmarks to validate the generalizability of the proposed framework.' Questions=\"1. How does the Gödel Agent ensure stability and prevent undesirable behavior when it modifies its logic and behavior autonomously?\\n2. Are there specific tasks where the Gödel Agent's self-improvement process becomes less effective, and how can these limitations be addressed?\\n3. How does the system handle the computational cost associated with leveraging LLMs for continual self-improvement, and is there a mechanism to optimize this process?\"\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Invalid \\escape: line 1 column 555 (char 554)\n",
      "Content:\n",
      "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 7, \"Confidence\": 4, \"Strengths\": \"The U-MATH benchmark addresses existing gaps by introducing a diverse and challenging set of university-level math problems, including a significant percentage of multimodal problems. This is a strong contribution to the evaluation of LLMs in complex and realistic scenarios. The paper is well-organized, with a clear presentation of the benchmark creation, evaluation processes, and findings. The approach of using LLMs for solution evaluation with the $\\mu$-MATH dataset is innovative, providing a meta-evaluation of LLM capabilities.\", \"Weaknesses\": \"The paper primarily reports on the challenges faced by LLMs but does not explore solutions or methodologies to improve performance on such tasks. It would be beneficial to provide insights or recommendations on how LLMs could be enhanced for better handling of university-level and multimodal math problems.\", \"Questions\": \"What specific strategies could be pursued to improve LLM performance on visual and multimodal math problems? Is there any preliminary evidence or theory suggesting possible improvements?\"}\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper provides a comprehensive evaluation of LLMs' mathematical reasoning abilities, introducing a novel and more rigorous benchmark, GSM-Symbolic, which allows for more controlled and reliable assessment. The study systematically highlights the capability gaps in LLMs regarding mathematical reasoning, which is a significant contribution. The findings expose important limitations in current models' logical reasoning processes, offering valuable insights into their performance and potential areas for improvement.\" Weaknesses='The paper primarily focuses on the limitations of LLMs rather than proposing concrete solutions or improvements to enhance their mathematical reasoning capabilities. Additionally, while the GSM-Symbolic benchmark provides new insights, it may not fully capture all nuances of mathematical reasoning, and its dependency on symbolic templates might limit its generalizability to other forms of reasoning tasks.' Questions=\"How feasible is it to adapt the GSM-Symbolic benchmark to evaluate reasoning in other domains beyond mathematics? Can LLMs' deficits in logic and reasoning be mitigated by architecture improvements or training regimen adjustments?\"\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper provides a comprehensive evaluation of LLMs' mathematical reasoning abilities, introducing a novel and more rigorous benchmark, GSM-Symbolic, which allows for more controlled and reliable assessment. The study systematically highlights the capability gaps in LLMs regarding mathematical reasoning, which is a significant contribution. The findings expose important limitations in current models' logical reasoning processes, offering valuable insights into their performance and potential areas for improvement.\" Weaknesses='The paper primarily focuses on the limitations of LLMs rather than proposing concrete solutions or improvements to enhance their mathematical reasoning capabilities. Additionally, while the GSM-Symbolic benchmark provides new insights, it may not fully capture all nuances of mathematical reasoning, and its dependency on symbolic templates might limit its generalizability to other forms of reasoning tasks.' Questions=\"How feasible is it to adapt the GSM-Symbolic benchmark to evaluate reasoning in other domains beyond mathematics? Can LLMs' deficits in logic and reasoning be mitigated by architecture improvements or training regimen adjustments?\"\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Content:\n",
      "I'm sorry, I misunderstood your previous request. Let me provide an updated response based on the correct request.\n",
      "\n",
      "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 7, \"Confidence\": 4, \"Strengths\": \"1. The proposed approach to quantify prompt-induced and model-induced diversity is novel and addresses a gap in existing evaluation metrics for generative models.\\n2. The combination of matrix-based information measures with the existing Vendi diversity framework provides a unique perspective and could be impactful for analyzing generative models.\\n3. Theoretical results are presented that provide a solid foundation for the methodology introduced.\", \"Weaknesses\": \"1. The paper could benefit from more detailed explanations of the experimental results to clearly convey the practical implications of the proposed metrics.\\n2. Some sections of the paper, especially those dealing with mathematical derivations, may be difficult to follow for readers without a strong background in information theory.\", \"Questions\": \"1. How does the proposed approach compare with existing metrics in terms of computational efficiency?\\n2. Can the proposed method be easily extended to other types of generative models beyond text-conditioned ones?\"}\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed Flow with Interpolant Guidance (FIG) offers an innovative method for improving the performance of diffusion and flow matching models in challenging image reconstruction tasks. The use of measurement interpolants for reverse-time sampling is novel and effectively enhances results under high noise and severe ill-posed conditions. Experimentally, FIG demonstrates superior performance compared to state-of-the-art methods.' Weaknesses='The paper could provide clearer explanations on the theoretical underpinnings and motivations of the measurement interpolants. Additionally, the presentation of results, while strong, would benefit from a more in-depth analysis of specific scenarios where FIG significantly outperforms baseline approaches.' Questions='What are the primary challenges in implementing measurement interpolants in FIG, and how do they compare to standard conditioning techniques? Can the authors provide more detailed insights into the specific types of noise or ill-posedness where FIG shows the most substantial improvements?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed Flow with Interpolant Guidance (FIG) offers an innovative method for improving the performance of diffusion and flow matching models in challenging image reconstruction tasks. The use of measurement interpolants for reverse-time sampling is novel and effectively enhances results under high noise and severe ill-posed conditions. Experimentally, FIG demonstrates superior performance compared to state-of-the-art methods.' Weaknesses='The paper could provide clearer explanations on the theoretical underpinnings and motivations of the measurement interpolants. Additionally, the presentation of results, while strong, would benefit from a more in-depth analysis of specific scenarios where FIG significantly outperforms baseline approaches.' Questions='What are the primary challenges in implementing measurement interpolants in FIG, and how do they compare to standard conditioning techniques? Can the authors provide more detailed insights into the specific types of noise or ill-posedness where FIG shows the most substantial improvements?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper proposes a novel and effective solution for 3DGS steganography that considerably enhances security and rendering fidelity compared to existing methods.\\n2. The approach shows strong experimental performance in concealing and extracting information with minimal impact on rendering quality and speed.\\n3. SecureGS introduces innovative mechanisms like hybrid Gaussian encryption and adaptive region-aware strategies, addressing significant challenges in the field.' Weaknesses='1. The complexity of the proposed SecureGS framework might hinder its adoption in real-time applications where simplicity and low computational overhead are crucial.\\n2. While the paper demonstrates improved security, details on scalability and performance in highly dynamic 3D environments remain unclear.' Questions='1. How does SecureGS handle potential scalability issues when applied to large or highly detailed 3D assets?\\n2. Can the proposed framework be integrated with existing 3D rendering pipelines without substantial modifications?\\n3. What are the specific computational trade-offs associated with the enhanced security mechanisms introduced in SecureGS?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper proposes a novel and effective solution for 3DGS steganography that considerably enhances security and rendering fidelity compared to existing methods.\\n2. The approach shows strong experimental performance in concealing and extracting information with minimal impact on rendering quality and speed.\\n3. SecureGS introduces innovative mechanisms like hybrid Gaussian encryption and adaptive region-aware strategies, addressing significant challenges in the field.' Weaknesses='1. The complexity of the proposed SecureGS framework might hinder its adoption in real-time applications where simplicity and low computational overhead are crucial.\\n2. While the paper demonstrates improved security, details on scalability and performance in highly dynamic 3D environments remain unclear.' Questions='1. How does SecureGS handle potential scalability issues when applied to large or highly detailed 3D assets?\\n2. Can the proposed framework be integrated with existing 3D rendering pipelines without substantial modifications?\\n3. What are the specific computational trade-offs associated with the enhanced security mechanisms introduced in SecureGS?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths=\"The paper introduces an innovative approach using hierarchical patch graph networks to tackle the challenges of modeling multivariate time series with irregular sampling rates. The experimental results demonstrating superior performance across multiple datasets highlight the method's efficacy.\" Weaknesses='While the proposed method is promising, the paper could benefit from a more detailed exploration of the limitations of the approach, such as potential computational complexity or scalability issues when dealing with extremely large datasets.' Questions='What are the implications for computational efficiency and how does the model scale with larger datasets?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths=\"The paper introduces an innovative approach using hierarchical patch graph networks to tackle the challenges of modeling multivariate time series with irregular sampling rates. The experimental results demonstrating superior performance across multiple datasets highlight the method's efficacy.\" Weaknesses='While the proposed method is promising, the paper could benefit from a more detailed exploration of the limitations of the approach, such as potential computational complexity or scalability issues when dealing with extremely large datasets.' Questions='What are the implications for computational efficiency and how does the model scale with larger datasets?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses a novel problem in the field of LLMs by focusing on the imperceptibility of watermarking, a crucial aspect for real-world applications.\\n2. Introduction of a new algorithm, Water-Probe, which successfully detects watermarking in LLMs and identifies vulnerabilities in current watermarking methods.\\n3. Proposes the Water-Bag strategy to enhance imperceptibility, showing practical solutions to improve current LLM watermarking techniques.' Weaknesses='1. The paper lacks a thorough exploration of different types of watermarking schemes beyond the mainstream ones, potentially limiting the scope of its evaluation.\\n2. The presentation could be clearer, with more details on how Water-Probe works and its limitations. More background could be provided to better understand the problem context.\\n3. Experimental results could be expanded to include more diverse types of LLMs and watermarking techniques to solidify claims.' Questions='1. How does Water-Probe perform on more advanced or less common watermarking techniques?\\n2. Can Water-Probe detect watermarks from deliberately altered or manipulated watermarked text?\\n3. Could the Water-Bag strategy be adapted to other kinds of digital content watermarking, such as images or audio, beyond text?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses a novel problem in the field of LLMs by focusing on the imperceptibility of watermarking, a crucial aspect for real-world applications.\\n2. Introduction of a new algorithm, Water-Probe, which successfully detects watermarking in LLMs and identifies vulnerabilities in current watermarking methods.\\n3. Proposes the Water-Bag strategy to enhance imperceptibility, showing practical solutions to improve current LLM watermarking techniques.' Weaknesses='1. The paper lacks a thorough exploration of different types of watermarking schemes beyond the mainstream ones, potentially limiting the scope of its evaluation.\\n2. The presentation could be clearer, with more details on how Water-Probe works and its limitations. More background could be provided to better understand the problem context.\\n3. Experimental results could be expanded to include more diverse types of LLMs and watermarking techniques to solidify claims.' Questions='1. How does Water-Probe perform on more advanced or less common watermarking techniques?\\n2. Can Water-Probe detect watermarks from deliberately altered or manipulated watermarked text?\\n3. Could the Water-Bag strategy be adapted to other kinds of digital content watermarking, such as images or audio, beyond text?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=3 Strengths='The paper provides a comprehensive formal framework for understanding and developing direct preference alignment (DPA) loss functions, offering clarity on their semantics and relationships. It proposes innovative theoretical foundations and provides structured ways to derive new loss functions.' Weaknesses='Potential complexity and computational challenges might arise from the formalism, leading to exponential blowups, such as in model enumeration. Some notation and references in the paper may lack clarity or contain errors, which may affect the comprehensibility for those unfamiliar with the domain.' Questions=\"How does the framework address potential computational complexity issues, such as those arising from exponential model counting? Could you elaborate on the interpretation of certain notations, like the preference relation 'y_w \\\\succ y_l'? Are there specific computational optimizations that could be adopted to handle the potentially high computational cost?\"\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=3 Strengths='The paper provides a comprehensive formal framework for understanding and developing direct preference alignment (DPA) loss functions, offering clarity on their semantics and relationships. It proposes innovative theoretical foundations and provides structured ways to derive new loss functions.' Weaknesses='Potential complexity and computational challenges might arise from the formalism, leading to exponential blowups, such as in model enumeration. Some notation and references in the paper may lack clarity or contain errors, which may affect the comprehensibility for those unfamiliar with the domain.' Questions=\"How does the framework address potential computational complexity issues, such as those arising from exponential model counting? Could you elaborate on the interpretation of certain notations, like the preference relation 'y_w \\\\succ y_l'? Are there specific computational optimizations that could be adopted to handle the potentially high computational cost?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The proposed MEMO framework presents a novel approach to enhancing audio-driven talking video generation by tackling key issues such as identity consistency and emotional expression. The integration of memory-guided temporal and emotion-aware modules shows strong promise in improving video realism and synchronization with audio, surpassing current state-of-the-art methods. The use of linear attention and emotion adaptive layer norm without relying on facial inductive biases underpins a well-founded methodology. Extensive experimental validation on a large-scale dataset further demonstrates the efficacy of MEMO.' Weaknesses='While results are promising, the paper could provide more clarity on the computational complexity and resource needs of MEMO, particularly given its reliance on large-scale datasets and sophisticated model architectures. Additionally, details on the generalization of MEMO across diverse linguistic and cultural contexts, beyond the dataset it was trained on, would strengthen the claims.' Questions='1. How does the model handle linguistic diversity in audio inputs, particularly languages with significantly different phonetic structures?\\n2. What measures are taken to ensure dataset biases do not adversely affect the generalization of the model?\\n3. Can you elaborate on the potential adaptations of MEMO for real-time applications considering computational demands?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The proposed MEMO framework presents a novel approach to enhancing audio-driven talking video generation by tackling key issues such as identity consistency and emotional expression. The integration of memory-guided temporal and emotion-aware modules shows strong promise in improving video realism and synchronization with audio, surpassing current state-of-the-art methods. The use of linear attention and emotion adaptive layer norm without relying on facial inductive biases underpins a well-founded methodology. Extensive experimental validation on a large-scale dataset further demonstrates the efficacy of MEMO.' Weaknesses='While results are promising, the paper could provide more clarity on the computational complexity and resource needs of MEMO, particularly given its reliance on large-scale datasets and sophisticated model architectures. Additionally, details on the generalization of MEMO across diverse linguistic and cultural contexts, beyond the dataset it was trained on, would strengthen the claims.' Questions='1. How does the model handle linguistic diversity in audio inputs, particularly languages with significantly different phonetic structures?\\n2. What measures are taken to ensure dataset biases do not adversely affect the generalization of the model?\\n3. Can you elaborate on the potential adaptations of MEMO for real-time applications considering computational demands?'\n",
      "Finished gpt4o on data/test_data/one_shot/test_data_2025_abstract_prompts_oneshot.jsonl, saved => results/gpt4o/test_data_2025_abstract_prompts_oneshot_gpt4o_results.jsonl\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The approach introduces an innovative way to handle spatio-temporal dependencies in traffic forecasting using tense-specific attention modules, which is novel and demonstrates improved performance over baseline models.\\n2. The paper effectively presents a method to integrate contrastive learning techniques to enhance model robustness against incomplete data, which is a significant challenge in real-world applications.\\n3. Empirical results on standard datasets show that the proposed method significantly outperforms existing models, demonstrating the practical efficacy of the approach.' Weaknesses=\"1. The paper could benefit from more detailed theoretical analysis to reinforce the empirical claims and provide deeper insight into why the proposed tense-specific attention modules improve model performance.\\n2. There is limited discussion regarding the computational efficiency of the proposed model. The introduction of multiple tense-specific modules could increase computational complexity, which is not addressed.\\n3. The contrastive learning approach and its impact on the model's performance are not thoroughly evaluated, leaving questions about its contribution relative to the tense-specific attention modules.\" Questions='1. Could the authors provide more details on the contrastive learning approach and its specific role in handling incomplete data?\\n2. What is the computational cost of the proposed TTformer model compared to baseline models, particularly in terms of training time and resource requirements?\\n3. How does the choice of tense-specific modules scale with larger or more complex datasets, and are there potential limitations in terms of scalability?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The approach introduces an innovative way to handle spatio-temporal dependencies in traffic forecasting using tense-specific attention modules, which is novel and demonstrates improved performance over baseline models.\\n2. The paper effectively presents a method to integrate contrastive learning techniques to enhance model robustness against incomplete data, which is a significant challenge in real-world applications.\\n3. Empirical results on standard datasets show that the proposed method significantly outperforms existing models, demonstrating the practical efficacy of the approach.' Weaknesses=\"1. The paper could benefit from more detailed theoretical analysis to reinforce the empirical claims and provide deeper insight into why the proposed tense-specific attention modules improve model performance.\\n2. There is limited discussion regarding the computational efficiency of the proposed model. The introduction of multiple tense-specific modules could increase computational complexity, which is not addressed.\\n3. The contrastive learning approach and its impact on the model's performance are not thoroughly evaluated, leaving questions about its contribution relative to the tense-specific attention modules.\" Questions='1. Could the authors provide more details on the contrastive learning approach and its specific role in handling incomplete data?\\n2. What is the computational cost of the proposed TTformer model compared to baseline models, particularly in terms of training time and resource requirements?\\n3. How does the choice of tense-specific modules scale with larger or more complex datasets, and are there potential limitations in terms of scalability?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper identifies and addresses a crucial gap in the current evaluation of reinforcement learning agents for their generalization capabilities in cooperative contexts. \\n2. The introduction of the Overcooked Generalisation Challenge (OGC) represents a significant contribution towards understanding and improving zero-shot cooperation in RL agents.\\n3. The use of innovative neural network architectures and a variety of DCD methods provides rich insight into the complexity and potential of the problem space.' Weaknesses='1. The paper does not include a detailed breakdown of how the proposed methods perform on individual unseen layouts, potentially missing insights into specific challenges faced by agents.\\n2. Fails to fully explore the depth of Overcooked environments by limiting the game to a single recipe, which may overlook important aspects of coordination and impact the depth of the evaluation.\\n3. Some minor presentation issues such as unexplained baselines (e.g., Random and Stay) and not fully clarifying the observation space.\\n4. There is a question of whether the observed challenges in generalization are due to limitations in the UED algorithms or insufficient training regimes, which is not clearly analyzed.' Questions=\"1. Can the proposed challenge isolate partner generalization from environment generalization to specifically test challenges in partner variability?\\n2. How does the limitation of single-recipe scenarios impact the proposed challenge's applicability to broader real-world human-AI collaboration tasks? Would including multiple recipes change the results?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper identifies and addresses a crucial gap in the current evaluation of reinforcement learning agents for their generalization capabilities in cooperative contexts. \\n2. The introduction of the Overcooked Generalisation Challenge (OGC) represents a significant contribution towards understanding and improving zero-shot cooperation in RL agents.\\n3. The use of innovative neural network architectures and a variety of DCD methods provides rich insight into the complexity and potential of the problem space.' Weaknesses='1. The paper does not include a detailed breakdown of how the proposed methods perform on individual unseen layouts, potentially missing insights into specific challenges faced by agents.\\n2. Fails to fully explore the depth of Overcooked environments by limiting the game to a single recipe, which may overlook important aspects of coordination and impact the depth of the evaluation.\\n3. Some minor presentation issues such as unexplained baselines (e.g., Random and Stay) and not fully clarifying the observation space.\\n4. There is a question of whether the observed challenges in generalization are due to limitations in the UED algorithms or insufficient training regimes, which is not clearly analyzed.' Questions=\"1. Can the proposed challenge isolate partner generalization from environment generalization to specifically test challenges in partner variability?\\n2. How does the limitation of single-recipe scenarios impact the proposed challenge's applicability to broader real-world human-AI collaboration tasks? Would including multiple recipes change the results?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The idea of leveraging natural referral relationships such as hyperlinks to construct long documents is innovative and appears to substantially improve the quality of pretraining data for LLMs.\\n2. The LongPack method demonstrates considerable improvement over past methods, achieving a 32.7% higher performance, highlighting its effectiveness.\\n3. The approach provides a scalable solution, demonstrating efficient construction of long documents, which is a significant stride towards better long-context comprehension in LLMs.' Weaknesses='1. The scope of LongPack is limited to documents where natural referral signals are available, such as web pages. It is unclear how this might translate to other domains lacking such explicit relationships.\\n2. While the method shows improvement, the paper lacks extensive ablation studies to robustly analyze the contribution of different components within LongPack.\\n3. The novelty of the methodology is potentially limited, as it builds on existing concepts of using document links or referrals, albeit in a more structured manner.' Questions='1. How well does LongPack perform in domains that lack explicit referral signals like hyperlinks, such as literary texts or transcriptions?\\n2. Can the concept of LongPack be extended beyond web-based documents to academic papers or research articles where citations could act as referral relationships?\\n3. Are there potential drawbacks or limitations when scaling the LongPack method to even larger datasets, and how might these be mitigated?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The idea of leveraging natural referral relationships such as hyperlinks to construct long documents is innovative and appears to substantially improve the quality of pretraining data for LLMs.\\n2. The LongPack method demonstrates considerable improvement over past methods, achieving a 32.7% higher performance, highlighting its effectiveness.\\n3. The approach provides a scalable solution, demonstrating efficient construction of long documents, which is a significant stride towards better long-context comprehension in LLMs.' Weaknesses='1. The scope of LongPack is limited to documents where natural referral signals are available, such as web pages. It is unclear how this might translate to other domains lacking such explicit relationships.\\n2. While the method shows improvement, the paper lacks extensive ablation studies to robustly analyze the contribution of different components within LongPack.\\n3. The novelty of the methodology is potentially limited, as it builds on existing concepts of using document links or referrals, albeit in a more structured manner.' Questions='1. How well does LongPack perform in domains that lack explicit referral signals like hyperlinks, such as literary texts or transcriptions?\\n2. Can the concept of LongPack be extended beyond web-based documents to academic papers or research articles where citations could act as referral relationships?\\n3. Are there potential drawbacks or limitations when scaling the LongPack method to even larger datasets, and how might these be mitigated?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper introduces a novel augmentation-free self-supervised learning method for tabular data, a challenging area where existing SSL techniques often struggle.\\n2. T-JEPA demonstrates significant improvements over traditional methods, including Gradient Boosted Decision Trees, in classification and regression tasks.\\n3. The method effectively identifies relevant features without needing labels, showcasing a strong practical potential for real-world applications.' Weaknesses=\"1. The paper relies heavily on the introduction of regularization tokens, which may raise questions about the complexity and potential overfitting of the model.\\n2. While the results are promising, the model's generalizability across diverse tabular datasets needs further validation.\\n3. The explanation of the joint embedding predictive architecture (JEPA) and how it specifically applies to tabular data could be more detailed for complete clarity.\" Questions='1. How does T-JEPA handle cases where tabular data contains missing values, which is a common scenario in many real-world datasets?\\n2. Could the authors elaborate on how the context and target masks are specifically constructed to ensure effectiveness across different types of tabular data?\\n3. What are the implications of using regularization tokens with respect to the interpretability of the learned representations?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper introduces a novel augmentation-free self-supervised learning method for tabular data, a challenging area where existing SSL techniques often struggle.\\n2. T-JEPA demonstrates significant improvements over traditional methods, including Gradient Boosted Decision Trees, in classification and regression tasks.\\n3. The method effectively identifies relevant features without needing labels, showcasing a strong practical potential for real-world applications.' Weaknesses=\"1. The paper relies heavily on the introduction of regularization tokens, which may raise questions about the complexity and potential overfitting of the model.\\n2. While the results are promising, the model's generalizability across diverse tabular datasets needs further validation.\\n3. The explanation of the joint embedding predictive architecture (JEPA) and how it specifically applies to tabular data could be more detailed for complete clarity.\" Questions='1. How does T-JEPA handle cases where tabular data contains missing values, which is a common scenario in many real-world datasets?\\n2. Could the authors elaborate on how the context and target masks are specifically constructed to ensure effectiveness across different types of tabular data?\\n3. What are the implications of using regularization tokens with respect to the interpretability of the learned representations?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper presents a novel range-limited augmentation method specifically tailored for tabular data, addressing a significant challenge in few-shot learning by preserving task-relevant information. The approach is demonstrated to outperform existing tabular learning methods across various scenarios, which highlights the effectiveness of the proposed strategy. Additionally, the comprehensive evaluation using a large benchmark of 42 tabular datasets strengthens the paper's contributions.\" Weaknesses='The method may have limited applicability beyond tabular data, as the proposed range-limited augmentation is specifically designed for such data types. Furthermore, the paper may lack details on how certain feature-specific ranges are determined, which could be crucial for replicating and effectively applying the method in practice. The practical implementation details and the sensitivity of the method to different parameter settings could use further elaboration.' Questions='What specific guidelines or heuristics are used to define the feature-specific ranges, and how sensitive is the method to the choice of these ranges? Are there any generalization strategies proposed to adapt this method to other types of data beyond tabular domains?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper presents a novel range-limited augmentation method specifically tailored for tabular data, addressing a significant challenge in few-shot learning by preserving task-relevant information. The approach is demonstrated to outperform existing tabular learning methods across various scenarios, which highlights the effectiveness of the proposed strategy. Additionally, the comprehensive evaluation using a large benchmark of 42 tabular datasets strengthens the paper's contributions.\" Weaknesses='The method may have limited applicability beyond tabular data, as the proposed range-limited augmentation is specifically designed for such data types. Furthermore, the paper may lack details on how certain feature-specific ranges are determined, which could be crucial for replicating and effectively applying the method in practice. The practical implementation details and the sensitivity of the method to different parameter settings could use further elaboration.' Questions='What specific guidelines or heuristics are used to define the feature-specific ranges, and how sensitive is the method to the choice of these ranges? Are there any generalization strategies proposed to adapt this method to other types of data beyond tabular domains?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='1. The paper presents a novel framework for agent-oriented planning in multi-agent systems, addressing significant gaps related to solvability, completeness, and non-redundancy in task decomposition and allocation.\\n2. The use of an LLM-powered meta-agent to guide task decomposition and allocation is innovative and shows promising results in experiments.\\n3. Integration of feedback loops and reward models to continuously improve the planning framework creates a robust mechanism for enhancing efficiency and problem-solving capabilities.' Weaknesses=\"1. The framework's reliance on large language models may limit its applicability in scenarios where such resources are unavailable or impractical.\\n2. There is a need for more extensive experimental validation across diverse datasets to substantiate claims of real-world applicability.\\n3. The paper could benefit from clearer explanations in some sections, especially concerning the operation and integration of the reward model and detector components.\" Questions='1. How does the framework handle failures within the individual agents? Does it allow for dynamic reassignment or redundancy checks?\\n2. Can the framework be extended to incorporate other forms of meta-agents beyond those powered by large language models?\\n3. How do different configurations or intricacies of the reward model influence the outcomes of task evaluation and meta-agent updates?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='1. The paper presents a novel framework for agent-oriented planning in multi-agent systems, addressing significant gaps related to solvability, completeness, and non-redundancy in task decomposition and allocation.\\n2. The use of an LLM-powered meta-agent to guide task decomposition and allocation is innovative and shows promising results in experiments.\\n3. Integration of feedback loops and reward models to continuously improve the planning framework creates a robust mechanism for enhancing efficiency and problem-solving capabilities.' Weaknesses=\"1. The framework's reliance on large language models may limit its applicability in scenarios where such resources are unavailable or impractical.\\n2. There is a need for more extensive experimental validation across diverse datasets to substantiate claims of real-world applicability.\\n3. The paper could benefit from clearer explanations in some sections, especially concerning the operation and integration of the reward model and detector components.\" Questions='1. How does the framework handle failures within the individual agents? Does it allow for dynamic reassignment or redundancy checks?\\n2. Can the framework be extended to incorporate other forms of meta-agents beyond those powered by large language models?\\n3. How do different configurations or intricacies of the reward model influence the outcomes of task evaluation and meta-agent updates?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper proposes an innovative method, TransFusion, that effectively enhances the performance of instruction-tuned LLMs for low-resource languages by leveraging translations.\\nThe method shows significant improvement in information extraction tasks across a range of multilingual datasets, providing a clear performance boost over existing methods.' Weaknesses=\"The paper's explanation could be clearer, as the method requires thorough reading to be fully understood.\\nWhile the method's flexibility is a strength, the implementation differences in its various configurations may cause confusion regarding consistency and universality.\" Questions='How can the method be adapted or improved for languages with even fewer resources available, both in terms of annotated and unannotated texts?\\nWhat are the trade-offs in terms of computational cost when using machine translation and annotation fusion techniques in practice?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper proposes an innovative method, TransFusion, that effectively enhances the performance of instruction-tuned LLMs for low-resource languages by leveraging translations.\\nThe method shows significant improvement in information extraction tasks across a range of multilingual datasets, providing a clear performance boost over existing methods.' Weaknesses=\"The paper's explanation could be clearer, as the method requires thorough reading to be fully understood.\\nWhile the method's flexibility is a strength, the implementation differences in its various configurations may cause confusion regarding consistency and universality.\" Questions='How can the method be adapted or improved for languages with even fewer resources available, both in terms of annotated and unannotated texts?\\nWhat are the trade-offs in terms of computational cost when using machine translation and annotation fusion techniques in practice?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths=\"The method presented is a significant advancement in causal discovery by incorporating nonlinear latent hierarchical models without deterministic assumptions, making it highly applicable to real-world scenarios. The authors also provide strong theoretical identifiability results, which bolster the robustness of their approach. The experiments demonstrate superior performance over existing methods on challenging datasets, highlighting the method's accuracy and scalability.\" Weaknesses='While the paper presents an impressive method, the complexity of the approach utilizing variational autoencoders and continuous optimizations might limit accessibility to researchers not specialized in these areas. Additionally, the experiments, while extensive on synthetic data, could benefit from further validation on a more diverse range of real-world datasets to showcase broad applicability and adaptability.' Questions='How does the proposed method perform on datasets with more complex relational structures beyond MNIST, especially in contexts that may not inherently have a clear hierarchical latent structure? Are there any computational constraints or limitations, given the use of VAE and Gumbel-softmax, when scaling to very large datasets?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths=\"The method presented is a significant advancement in causal discovery by incorporating nonlinear latent hierarchical models without deterministic assumptions, making it highly applicable to real-world scenarios. The authors also provide strong theoretical identifiability results, which bolster the robustness of their approach. The experiments demonstrate superior performance over existing methods on challenging datasets, highlighting the method's accuracy and scalability.\" Weaknesses='While the paper presents an impressive method, the complexity of the approach utilizing variational autoencoders and continuous optimizations might limit accessibility to researchers not specialized in these areas. Additionally, the experiments, while extensive on synthetic data, could benefit from further validation on a more diverse range of real-world datasets to showcase broad applicability and adaptability.' Questions='How does the proposed method perform on datasets with more complex relational structures beyond MNIST, especially in contexts that may not inherently have a clear hierarchical latent structure? Are there any computational constraints or limitations, given the use of VAE and Gumbel-softmax, when scaling to very large datasets?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed Spatial 3D-LLM method effectively enhances existing models by introducing a progressive spatial awareness scheme. This method demonstrates considerable improvements in tasks requiring spatial reasoning and detailed 3D perception, as shown by experiments using the new MODLE dataset. The integration of graph convolutional networks and attention mechanisms is innovative and well-suited for enriching spatial embeddings.' Weaknesses=\"The methodology may not fully address the challenge of performing a quantitative comparison with existing state-of-the-art models, particularly in terms of fine-grained spatial understanding. Additionally, the new dataset MODLE, while beneficial, may not fully test the model's capabilities without comparisons to stronger baselines.\" Questions='1. Are there plans to make the MODLE dataset publicly available for further validation and comparative studies?\\n2. How does the Spatial 3D-LLM perform in comparison with other models that specifically focus on fine-grained spatial reasoning in 3D environments?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed Spatial 3D-LLM method effectively enhances existing models by introducing a progressive spatial awareness scheme. This method demonstrates considerable improvements in tasks requiring spatial reasoning and detailed 3D perception, as shown by experiments using the new MODLE dataset. The integration of graph convolutional networks and attention mechanisms is innovative and well-suited for enriching spatial embeddings.' Weaknesses=\"The methodology may not fully address the challenge of performing a quantitative comparison with existing state-of-the-art models, particularly in terms of fine-grained spatial understanding. Additionally, the new dataset MODLE, while beneficial, may not fully test the model's capabilities without comparisons to stronger baselines.\" Questions='1. Are there plans to make the MODLE dataset publicly available for further validation and comparative studies?\\n2. How does the Spatial 3D-LLM perform in comparison with other models that specifically focus on fine-grained spatial reasoning in 3D environments?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper addresses an important gap in the field of explainable graph representation learning by focusing on representation-level explainability, which is less explored compared to model-level and instance-level explanations. The proposed methods are backed by both theoretical analysis and empirical validation on real-world datasets, demonstrating robustness and generalization benefits.' Weaknesses='The paper could benefit from more in-depth comparisons with existing approaches in explainable AI for graph data, particularly in terms of visualization and interpretability aspects. Additionally, more details on the implementation and time complexity of the proposed methods would provide useful insights.' Questions='How does the proposed method perform relative to state-of-the-art explainability techniques for graphs in terms of interpretability and computational efficiency? Can the authors provide more details on how pattern importance is quantified and its effect on the robustness of the representations?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper addresses an important gap in the field of explainable graph representation learning by focusing on representation-level explainability, which is less explored compared to model-level and instance-level explanations. The proposed methods are backed by both theoretical analysis and empirical validation on real-world datasets, demonstrating robustness and generalization benefits.' Weaknesses='The paper could benefit from more in-depth comparisons with existing approaches in explainable AI for graph data, particularly in terms of visualization and interpretability aspects. Additionally, more details on the implementation and time complexity of the proposed methods would provide useful insights.' Questions='How does the proposed method perform relative to state-of-the-art explainability techniques for graphs in terms of interpretability and computational efficiency? Can the authors provide more details on how pattern importance is quantified and its effect on the robustness of the representations?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='1. The authors curate a large-scale dataset with over 7 million QA samples for egocentric videos of varying lengths and introduce a QA benchmark called EgoMemoria, which contains 7,026 questions for 629 videos of different lengths. \\n\\n2. A specialized multimodal architecture is proposed, using a \"Memory Pointer Prompting\" mechanism. This includes a global glimpse step for overarching video understanding and a fallback step that uses key visual information to generate responses, enabling the model to comprehend extended video content more effectively.' Weaknesses='1. The first claim of contribution, the “narration to egocentric QA” data pipeline, I believe, should not be emphasized as a major contribution. This type of generating QA from dense captions have been used in multiple previous works, from the non-LLM era (like TVQA) to the LLM era (like LLama-VID). I believe it is better to tone down this statement.\\n2. The generated EgoMemoria Benchmark does not stands itself out of many long video understanding benchmarks. Even we narrow down to only egocentric videos, the GroundVQA dataset is also good to be compared and especially be used to test the MMEgo model. I would also recommend the authors to compare a long of long video datasets, providing more proofs that this benchmark is not so incremental.\\n\\nOverall, I think this paper is proposing a good model, while the benchmark side is relatively weak. I would recommend the authors to either tone down the benchmark in the paper to put more emphasize on this model, or improve the benchmark. I recommend the authors to consider using the datasets of EgoExo4D and EgoExoLearn, both of which also contain dense narrations of egocentric videos and should be very suitable for enriching your benchmark in terms of both size and diversity.' Questions='It would be great if the authors could answer my two points in the weakness section.'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='1. The authors curate a large-scale dataset with over 7 million QA samples for egocentric videos of varying lengths and introduce a QA benchmark called EgoMemoria, which contains 7,026 questions for 629 videos of different lengths. \\n\\n2. A specialized multimodal architecture is proposed, using a \"Memory Pointer Prompting\" mechanism. This includes a global glimpse step for overarching video understanding and a fallback step that uses key visual information to generate responses, enabling the model to comprehend extended video content more effectively.' Weaknesses='1. The first claim of contribution, the “narration to egocentric QA” data pipeline, I believe, should not be emphasized as a major contribution. This type of generating QA from dense captions have been used in multiple previous works, from the non-LLM era (like TVQA) to the LLM era (like LLama-VID). I believe it is better to tone down this statement.\\n2. The generated EgoMemoria Benchmark does not stands itself out of many long video understanding benchmarks. Even we narrow down to only egocentric videos, the GroundVQA dataset is also good to be compared and especially be used to test the MMEgo model. I would also recommend the authors to compare a long of long video datasets, providing more proofs that this benchmark is not so incremental.\\n\\nOverall, I think this paper is proposing a good model, while the benchmark side is relatively weak. I would recommend the authors to either tone down the benchmark in the paper to put more emphasize on this model, or improve the benchmark. I recommend the authors to consider using the datasets of EgoExo4D and EgoExoLearn, both of which also contain dense narrations of egocentric videos and should be very suitable for enriching your benchmark in terms of both size and diversity.' Questions='It would be great if the authors could answer my two points in the weakness section.'\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Invalid \\escape: line 1 column 539 (char 538)\n",
      "Content:\n",
      "{\"Soundness\": 4, \"Presentation\": 4, \"Contribution\": 4, \"Rating\": 7, \"Confidence\": 4, \"Strengths\": \"The paper presents a novel analysis that significantly extends the applicability of privacy guarantees for Noisy-SGD, showing convergence of privacy loss even for non-convex, non-smooth losses by leveraging an assumption of H\\\"older continuous gradients. This relaxation of traditional assumptions such as strong convexity makes the results more applicable to a broader range of realistic problems. The method used, extending the shifted R\\'enyi divergence, is a technical improvement that could inspire further research.\", \"Weaknesses\": \"The assumptions introduced, while theoretically sound, might limit the immediate practical applicability of the results. The paper would benefit from more empirical validation in real-world scenarios to demonstrate the practical significance of the privacy bounds under the new assumptions. Additionally, the presentation could be clearer in terms of practical implications and comparisons with existing bounds. Some assumptions and theoretical results might be difficult to interpret or validate directly, limiting the comprehensibility for a broader audience.\", \"Questions\": \"Can you provide empirical evidence or real-world scenarios where your results show significant improvements under the new assumptions? How do the new privacy bounds compare with existing methods in terms of computational efficiency and ease of integration into existing frameworks? Are there specific types of non-convex and non-smooth problems where your analysis would have the most impact?\"}\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths=\"1. The paper introduces the Neuron Predictability Lens (NPL), which is a novel framework for understanding neuron activations in transformer-based models. This can potentially enhance the interpretability of large language models and provide insights into model behavior.\\n2. The research reveals interesting insights about neuron behavior, such as the implication of shallow and deep layers, and identifies 'background neurons' which hold significant importance. This adds valuable knowledge to the understanding of neural network mechanisms.\\n3. The presentation of the study is well-structured and clearly articulated, making complex concepts accessible.\" Weaknesses='1. The depth of the experimental analysis could be expanded, particularly in verifying the predictability of neurons beyond the datasets mentioned. The generalizability of the findings to other models and datasets remains an open question.\\n2. While the paper introduces an interesting framework, the long-term applications and implications of the NPL, in practical terms of enhancing model efficiency or interpretability, need to be clearly elucidated.\\n3. Some of the technical details and methodologies employed for mapping neuron activations might be complex, requiring a more in-depth explanation for a broader audience.' Questions='1. How generalizable is the NPL framework across different types of transformer-based models? Are there certain architectures where it is more or less applicable?\\n2. What are the potential applications of the insights gained from neuron predictability in improving or optimizing LLMs in real-world applications?\\n3. Is there any preliminary assessment of how the use of NPL could directly lead to improvements in tasks other than interpretability, such as language modeling performance or computational efficiency?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths=\"1. The paper introduces the Neuron Predictability Lens (NPL), which is a novel framework for understanding neuron activations in transformer-based models. This can potentially enhance the interpretability of large language models and provide insights into model behavior.\\n2. The research reveals interesting insights about neuron behavior, such as the implication of shallow and deep layers, and identifies 'background neurons' which hold significant importance. This adds valuable knowledge to the understanding of neural network mechanisms.\\n3. The presentation of the study is well-structured and clearly articulated, making complex concepts accessible.\" Weaknesses='1. The depth of the experimental analysis could be expanded, particularly in verifying the predictability of neurons beyond the datasets mentioned. The generalizability of the findings to other models and datasets remains an open question.\\n2. While the paper introduces an interesting framework, the long-term applications and implications of the NPL, in practical terms of enhancing model efficiency or interpretability, need to be clearly elucidated.\\n3. Some of the technical details and methodologies employed for mapping neuron activations might be complex, requiring a more in-depth explanation for a broader audience.' Questions='1. How generalizable is the NPL framework across different types of transformer-based models? Are there certain architectures where it is more or less applicable?\\n2. What are the potential applications of the insights gained from neuron predictability in improving or optimizing LLMs in real-world applications?\\n3. Is there any preliminary assessment of how the use of NPL could directly lead to improvements in tasks other than interpretability, such as language modeling performance or computational efficiency?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The proposed method effectively adapts pre-trained image restoration diffusion models for zero-shot video restoration, which is innovative and applicable to a wide range of tasks.\\n2. The approach does not require additional training or fine-tuning, making it computationally efficient and generalizable across different video degradation types and datasets.\\n3. Comprehensive evaluation on various datasets demonstrates superior performance compared to state-of-the-art models, especially in terms of temporal consistency and detail generation.' Weaknesses='1. The reliance on hierarchical latent warping and hybrid flow-guided approaches might limit the adaptability of the method to certain complex video scenarios where these assumptions do not hold.\\n2. The paper may lack detailed analysis regarding the limitations and edge cases where the proposed method may not perform well, reducing the clarity on its general scope and applicability.' Questions='1. How does the method perform on videos with very erratic motion, where temporal consistency might be more challenging?\\n2. Are there scenarios or types of degradation where this zero-shot approach significantly underperforms compared to task-specific training of diffusion models?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The proposed method effectively adapts pre-trained image restoration diffusion models for zero-shot video restoration, which is innovative and applicable to a wide range of tasks.\\n2. The approach does not require additional training or fine-tuning, making it computationally efficient and generalizable across different video degradation types and datasets.\\n3. Comprehensive evaluation on various datasets demonstrates superior performance compared to state-of-the-art models, especially in terms of temporal consistency and detail generation.' Weaknesses='1. The reliance on hierarchical latent warping and hybrid flow-guided approaches might limit the adaptability of the method to certain complex video scenarios where these assumptions do not hold.\\n2. The paper may lack detailed analysis regarding the limitations and edge cases where the proposed method may not perform well, reducing the clarity on its general scope and applicability.' Questions='1. How does the method perform on videos with very erratic motion, where temporal consistency might be more challenging?\\n2. Are there scenarios or types of degradation where this zero-shot approach significantly underperforms compared to task-specific training of diffusion models?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The paper effectively addresses a significant privacy concern in the domain of graph prompt learning and provides a novel solution through the adaptation of the PATE framework.\\n2. The methodologies proposed, DP-GPL and DP-GPL+W, are innovative and seem well-constructed to balance the trade-off between privacy and utility, a key challenge in differentially private learning.\\n3. The experiments cover several datasets and architectures, showcasing the methods' broad applicability and effectiveness over existing approaches like DP-SGD.\" Weaknesses=\"1. While the paper introduces a novel method for privacy-preserving graph prompt learning, it might benefit from deeper discussions or evaluations concerning the limitations and scalability of the proposed algorithms, especially in real-world scenarios involving large graphs.\\n2. The assumption about the need for pre-trained GNNs and its integration with prompt learning might not be feasible for all kinds of graph data, limiting the method's direct applicability.\\n3. The reliance on node centrality for weighting in DP-GPL+W, while innovative, may sometimes overlook other significant factors affecting privacy-utility trade-off.\" Questions='1. How does the performance of DP-GPL and DP-GPL+W compare with respect to different levels of privacy budgets? Are there specific thresholds beyond which performance significantly drops?\\n2. Can the methods proposed be extended or modified for use with other forms of graph representations, such as heterogeneous graphs or dynamic graphs?\\n3. How sensitive are the proposed methods to the choice of hyperparameters such as the number of teacher prompts or the centrality weighting mechanism in DP-GPL+W?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The paper effectively addresses a significant privacy concern in the domain of graph prompt learning and provides a novel solution through the adaptation of the PATE framework.\\n2. The methodologies proposed, DP-GPL and DP-GPL+W, are innovative and seem well-constructed to balance the trade-off between privacy and utility, a key challenge in differentially private learning.\\n3. The experiments cover several datasets and architectures, showcasing the methods' broad applicability and effectiveness over existing approaches like DP-SGD.\" Weaknesses=\"1. While the paper introduces a novel method for privacy-preserving graph prompt learning, it might benefit from deeper discussions or evaluations concerning the limitations and scalability of the proposed algorithms, especially in real-world scenarios involving large graphs.\\n2. The assumption about the need for pre-trained GNNs and its integration with prompt learning might not be feasible for all kinds of graph data, limiting the method's direct applicability.\\n3. The reliance on node centrality for weighting in DP-GPL+W, while innovative, may sometimes overlook other significant factors affecting privacy-utility trade-off.\" Questions='1. How does the performance of DP-GPL and DP-GPL+W compare with respect to different levels of privacy budgets? Are there specific thresholds beyond which performance significantly drops?\\n2. Can the methods proposed be extended or modified for use with other forms of graph representations, such as heterogeneous graphs or dynamic graphs?\\n3. How sensitive are the proposed methods to the choice of hyperparameters such as the number of teacher prompts or the centrality weighting mechanism in DP-GPL+W?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"1. Introduces Sparse Matrix Tuning (SMT), a novel approach with potential to significantly improve parameter-efficient fine-tuning by minimizing performance gaps and reducing resource consumption.\\n2. Provides a well-structured method using gradient information to focus on significant sub-matrices, optimizing tuning efficiency, which can inspire future research improvements.\\n3. Empirical results demonstrate STM's superiority over existing methods, supporting its claimed efficiency and effectiveness across various tasks.\" Weaknesses=\"1. The methodology lacks detailed justification for some design choices, such as the customization of sparse linear layers and the selection process for sub-matrices, which could affect reproducibility.\\n2. Limited discussions on potential limitations or challenges of SMT, such as scalability, robustness across diverse domains, or integration with other advanced neural architectures.\\n3. More comprehensive ablations, especially regarding block sizes and masking strategies, would strengthen the paper's assertions.\" Questions='1. Can SMT be effectively generalized across different types of neural architectures beyond those evaluated in the current study?\\n2. What are the consequences of varying the block size in SMT on model performance and memory efficiency?\\n3. How does SMT handle potential distribution shifts or overfitting given its focus on specific sub-matrices?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"1. Introduces Sparse Matrix Tuning (SMT), a novel approach with potential to significantly improve parameter-efficient fine-tuning by minimizing performance gaps and reducing resource consumption.\\n2. Provides a well-structured method using gradient information to focus on significant sub-matrices, optimizing tuning efficiency, which can inspire future research improvements.\\n3. Empirical results demonstrate STM's superiority over existing methods, supporting its claimed efficiency and effectiveness across various tasks.\" Weaknesses=\"1. The methodology lacks detailed justification for some design choices, such as the customization of sparse linear layers and the selection process for sub-matrices, which could affect reproducibility.\\n2. Limited discussions on potential limitations or challenges of SMT, such as scalability, robustness across diverse domains, or integration with other advanced neural architectures.\\n3. More comprehensive ablations, especially regarding block sizes and masking strategies, would strengthen the paper's assertions.\" Questions='1. Can SMT be effectively generalized across different types of neural architectures beyond those evaluated in the current study?\\n2. What are the consequences of varying the block size in SMT on model performance and memory efficiency?\\n3. How does SMT handle potential distribution shifts or overfitting given its focus on specific sub-matrices?'\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Content:\n",
      "I'm sorry, I made an error. Let me correct that.\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper introduces an innovative approach by leveraging heterogeneous random walks to integrate multi-view information, demonstrating superior clustering performance. It addresses a significant gap in multi-view clustering, effectively capturing high-order structures across views and integrating them into a unified sample-level structure. The experiments are comprehensive and validate the model's effectiveness across multiple datasets.\" Weaknesses='The paper could provide more theoretical analysis or proofs regarding the convergence and performance guarantees of the proposed method. Additionally, while the method is innovative, there is a need for a deeper discussion on the limitations and potential biases of the approach.' Questions='What are the potential limitations or biases in using heterogeneous random walks for capturing multi-view structures? Is there a theoretical guarantee for the convergence of the SMvCNet method?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper introduces an innovative approach by leveraging heterogeneous random walks to integrate multi-view information, demonstrating superior clustering performance. It addresses a significant gap in multi-view clustering, effectively capturing high-order structures across views and integrating them into a unified sample-level structure. The experiments are comprehensive and validate the model's effectiveness across multiple datasets.\" Weaknesses='The paper could provide more theoretical analysis or proofs regarding the convergence and performance guarantees of the proposed method. Additionally, while the method is innovative, there is a need for a deeper discussion on the limitations and potential biases of the approach.' Questions='What are the potential limitations or biases in using heterogeneous random walks for capturing multi-view structures? Is there a theoretical guarantee for the convergence of the SMvCNet method?'\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Content:\n",
      "I'm sorry, but I can only provide the requested JSON object. Would you like me to try again?\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper introduces a novel activation function, STAF, which builds upon the strengths of sinusoidal functions to tackle spectral bias in implicit neural representations. This provides a significant improvement in handling high-frequency details. \\n2. The proposed method shows impressive experimental results, surpassing existing methods like WIRE and SIREN in terms of reconstruction quality and convergence speed.\\n3. The paper is well-structured and clearly written, making complex mathematical concepts accessible and demonstrating the practical implications effectively.' Weaknesses=\"1. The paper could have explored more diverse architectures for further validation of the proposed method's robustness across various use cases. \\n2. While the theoretical foundations of STAF are well elaborated, there is limited discussion on potential limitations or scenarios where STAF might underperform compared to existing methods.\" Questions='1. What are the potential challenges in scaling this method to larger datasets or more complex architectures beyond the experimental setup provided?\\n2. Can the authors elaborate on how sensitive the STAF is to its initialization scheme and whether alternative initialization strategies were considered or tested?\\n3. How does the method perform in environments with noise or less structured input data? Are there scenarios where STAF could potentially introduce artifacts?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper introduces a novel activation function, STAF, which builds upon the strengths of sinusoidal functions to tackle spectral bias in implicit neural representations. This provides a significant improvement in handling high-frequency details. \\n2. The proposed method shows impressive experimental results, surpassing existing methods like WIRE and SIREN in terms of reconstruction quality and convergence speed.\\n3. The paper is well-structured and clearly written, making complex mathematical concepts accessible and demonstrating the practical implications effectively.' Weaknesses=\"1. The paper could have explored more diverse architectures for further validation of the proposed method's robustness across various use cases. \\n2. While the theoretical foundations of STAF are well elaborated, there is limited discussion on potential limitations or scenarios where STAF might underperform compared to existing methods.\" Questions='1. What are the potential challenges in scaling this method to larger datasets or more complex architectures beyond the experimental setup provided?\\n2. Can the authors elaborate on how sensitive the STAF is to its initialization scheme and whether alternative initialization strategies were considered or tested?\\n3. How does the method perform in environments with noise or less structured input data? Are there scenarios where STAF could potentially introduce artifacts?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses a significant problem in the domain of large language models, namely the alignment of outputs with user instructions.\\n2. GUIDE offers a novel approach by introducing the concept of strategically increasing attention to important instruction tokens, thus improving the output alignment without additional training resources.\\n3. The introduction of the Influence metric adds value by providing a tool for measuring token importance propagation, which can aid further research in explainability and alignment for transformer models.' Weaknesses='1. The effectiveness of GUIDE in diverse and particularly complex scenarios is not exhaustively covered; reliance on tagging may not capture the nuances required for varied instruction adherence.\\n2. While the Influence metric provides insight into alignment, the robustness and general applicability of this metric across different models and task types remain underexplored.\\n3. The comparative evaluation with state-of-the-art models is limited and the scalability of the method when applied to large datasets or more sophisticated tasks remains uncertain.' Questions=\"1. How does the GUIDE method's performance scale when dealing with substantially larger datasets and more complex instructions?\\n2. Can GUIDE be integrated with existing LLM architectures without intricate modifications, or does it require substantial adjustments?\\n3. How generalized is the Influence metric across different architectures and model sizes?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses a significant problem in the domain of large language models, namely the alignment of outputs with user instructions.\\n2. GUIDE offers a novel approach by introducing the concept of strategically increasing attention to important instruction tokens, thus improving the output alignment without additional training resources.\\n3. The introduction of the Influence metric adds value by providing a tool for measuring token importance propagation, which can aid further research in explainability and alignment for transformer models.' Weaknesses='1. The effectiveness of GUIDE in diverse and particularly complex scenarios is not exhaustively covered; reliance on tagging may not capture the nuances required for varied instruction adherence.\\n2. While the Influence metric provides insight into alignment, the robustness and general applicability of this metric across different models and task types remain underexplored.\\n3. The comparative evaluation with state-of-the-art models is limited and the scalability of the method when applied to large datasets or more sophisticated tasks remains uncertain.' Questions=\"1. How does the GUIDE method's performance scale when dealing with substantially larger datasets and more complex instructions?\\n2. Can GUIDE be integrated with existing LLM architectures without intricate modifications, or does it require substantial adjustments?\\n3. How generalized is the Influence metric across different architectures and model sizes?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents an innovative approach by adapting Monte Carlo Tree Search for better sampling in GFlowNet training. This novel integration suggests a meaningful advancement in improving training efficiency for generative models in discrete spaces, which are significant in both academic research and practical applications.' Weaknesses=\"Some sections, particularly the experimental details, lack clarity making it hard to fully grasp the metric calculations and domain settings. The results presented in plots lack statistical significance testing, raising questions on the robustness of claims. Also, the adaptation of MCTS in GFlowNets isn't thoroughly benchmarked against robust baselines, like variations of the original GFlowNet training methods.\" Questions='Can more insights be provided into the parameter settings chosen for MCDS compared to traditional GFlowNets for a deeper understanding of the performance benefits? Additionally, what mechanisms are employed in MCDS to prevent overfitting to high-reward trajectories, a common issue with RL-inspired methods?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents an innovative approach by adapting Monte Carlo Tree Search for better sampling in GFlowNet training. This novel integration suggests a meaningful advancement in improving training efficiency for generative models in discrete spaces, which are significant in both academic research and practical applications.' Weaknesses=\"Some sections, particularly the experimental details, lack clarity making it hard to fully grasp the metric calculations and domain settings. The results presented in plots lack statistical significance testing, raising questions on the robustness of claims. Also, the adaptation of MCTS in GFlowNets isn't thoroughly benchmarked against robust baselines, like variations of the original GFlowNet training methods.\" Questions='Can more insights be provided into the parameter settings chosen for MCDS compared to traditional GFlowNets for a deeper understanding of the performance benefits? Additionally, what mechanisms are employed in MCDS to prevent overfitting to high-reward trajectories, a common issue with RL-inspired methods?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=2 Rating=5 Confidence=3 Strengths='MotionDreamer introduces the innovative application of generative masked transformers to the challenging task of motion synthesis from single references. The use of localized attention effectively captures motion patterns, and the evaluation framework based on coverage, diversity, and perceptual metrics is comprehensive.' Weaknesses='The approach lacks novelty as it primarily adapts existing techniques to a new domain. While achieving good performance, the methodological contributions are not very distinct from prior work in related areas such as localized attention and distribution regularization. The dependence on a single motion reference might limit its scalability.' Questions='Can the technique be extended to handle multiple reference motions effectively, or is it inherently limited to single instances? How does MotionDreamer perform in rare or unseen motion scenarios compared to traditional models?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=2 Rating=5 Confidence=3 Strengths='MotionDreamer introduces the innovative application of generative masked transformers to the challenging task of motion synthesis from single references. The use of localized attention effectively captures motion patterns, and the evaluation framework based on coverage, diversity, and perceptual metrics is comprehensive.' Weaknesses='The approach lacks novelty as it primarily adapts existing techniques to a new domain. While achieving good performance, the methodological contributions are not very distinct from prior work in related areas such as localized attention and distribution regularization. The dependence on a single motion reference might limit its scalability.' Questions='Can the technique be extended to handle multiple reference motions effectively, or is it inherently limited to single instances? How does MotionDreamer perform in rare or unseen motion scenarios compared to traditional models?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses an important limitation of LLMs, expanding their ability to generate longer outputs effectively. \\n2. The introduction of novel datasets and benchmarks like LongWriter-6k and LongBench-Write is a valuable contribution to the field of long-text generation.' Weaknesses=\"1. The method's reliance on dataset expansion and the use of AgentWrite for subtasks may not fully address underlying architectural limitations of LLMs in handling long sequences.\\n2. There is a lack of clarity on the evaluation results concerning output quality beyond the established benchmark, which could provide better insights into practical applications.\" Questions='1. How does the quality of generated long-text outputs compare qualitatively to human-authored texts of similar lengths?\\n2. Are there any specific domains or contexts where LLMs trained with the LongWriter-6k dataset particularly excel at generating long outputs?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses an important limitation of LLMs, expanding their ability to generate longer outputs effectively. \\n2. The introduction of novel datasets and benchmarks like LongWriter-6k and LongBench-Write is a valuable contribution to the field of long-text generation.' Weaknesses=\"1. The method's reliance on dataset expansion and the use of AgentWrite for subtasks may not fully address underlying architectural limitations of LLMs in handling long sequences.\\n2. There is a lack of clarity on the evaluation results concerning output quality beyond the established benchmark, which could provide better insights into practical applications.\" Questions='1. How does the quality of generated long-text outputs compare qualitatively to human-authored texts of similar lengths?\\n2. Are there any specific domains or contexts where LLMs trained with the LongWriter-6k dataset particularly excel at generating long outputs?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper introduces a dynamic routing strategy, Ada-K routing, which effectively addresses the inefficiencies of traditional Top-K static routing in Mixture-of-Experts (MoE) models. The method is shown to improve model performance, computational efficiency, and inference speed significantly. Extensive evaluation across multiple benchmarks supports the proposed method's effectiveness, providing a comprehensive analysis of its benefits. The adaptive approach to resource allocation represents a meaningful contribution to maximizing the computational efficiency of large language models.\" Weaknesses=\"While the model demonstrates improved performance and efficiency, the implementation complexity and potential impact on real-world applications could be further elaborated. There may be concerns regarding the adaptability of the proposed method across diverse language models or tasks not covered in the evaluation. Additionally, the dependency on Proximal Policy Optimization (PPO) might limit the method's accessibility or complicate integration into existing systems.\" Questions='How does the Ada-K routing strategy perform when integrated with different types of architectures or smaller language models? Are there any limitations to its scalability or applicability to tasks beyond those tested in the paper? This could have implications for broader adoption and implementation. Furthermore, how does the allocator manage very low importance tokens, and is there a risk of bias or reduced model performance under specific conditions?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper introduces a dynamic routing strategy, Ada-K routing, which effectively addresses the inefficiencies of traditional Top-K static routing in Mixture-of-Experts (MoE) models. The method is shown to improve model performance, computational efficiency, and inference speed significantly. Extensive evaluation across multiple benchmarks supports the proposed method's effectiveness, providing a comprehensive analysis of its benefits. The adaptive approach to resource allocation represents a meaningful contribution to maximizing the computational efficiency of large language models.\" Weaknesses=\"While the model demonstrates improved performance and efficiency, the implementation complexity and potential impact on real-world applications could be further elaborated. There may be concerns regarding the adaptability of the proposed method across diverse language models or tasks not covered in the evaluation. Additionally, the dependency on Proximal Policy Optimization (PPO) might limit the method's accessibility or complicate integration into existing systems.\" Questions='How does the Ada-K routing strategy perform when integrated with different types of architectures or smaller language models? Are there any limitations to its scalability or applicability to tasks beyond those tested in the paper? This could have implications for broader adoption and implementation. Furthermore, how does the allocator manage very low importance tokens, and is there a risk of bias or reduced model performance under specific conditions?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. LoCoDL effectively integrates both Local Training and Communication Compression, addressing a significant gap in federated learning.\\n2. The theoretical framework and proofs provided ensure a sound understanding of the algorithm's performance.\\n3. The approach demonstrates clear practical performance benefits over existing methods in the federated learning context.\" Weaknesses='1. The method heavily relies on the use of specific unbiased compressors, which may limit flexibility.\\n2. The complexity of the approach and the mathematical framework may pose implementation challenges for practitioners not well-versed in theoretical machine learning methods.' Questions='1. How does LoCoDL perform with different types of data distributions not covered in the study?\\n2. Are there specific types of compression algorithms within U(ω) that more significantly impact the performance of LoCoDL?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. LoCoDL effectively integrates both Local Training and Communication Compression, addressing a significant gap in federated learning.\\n2. The theoretical framework and proofs provided ensure a sound understanding of the algorithm's performance.\\n3. The approach demonstrates clear practical performance benefits over existing methods in the federated learning context.\" Weaknesses='1. The method heavily relies on the use of specific unbiased compressors, which may limit flexibility.\\n2. The complexity of the approach and the mathematical framework may pose implementation challenges for practitioners not well-versed in theoretical machine learning methods.' Questions='1. How does LoCoDL perform with different types of data distributions not covered in the study?\\n2. Are there specific types of compression algorithms within U(ω) that more significantly impact the performance of LoCoDL?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper provides a clear and focused exploration of transformer components' roles, particularly in simple tasks like counting. It presents insightful theoretical analyses and rigorously conducts controlled experiments to validate these insights. The research highlights the significant impact of architectural nuances on the model's effectiveness, which is a valuable contribution to understanding how transformers can be optimized.\" Weaknesses='The scope of the study is limited to a simple counting task and single-layer transformers, which may restrict the broader applicability and impact of the findings. The practical implications for more complex, real-world tasks or multi-layer transformer models remain unexplored. Additionally, the study could benefit from expanding the range of tasks and architectures considered to generalize the insights.' Questions=\"1. How do the findings regarding architectural changes and softmax's role translate to more complex tasks or multi-layer transformer models?\\n2. Are there plans to extend the research to other simple algorithmic tasks to reinforce or contrast the conclusions drawn in this study?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper provides a clear and focused exploration of transformer components' roles, particularly in simple tasks like counting. It presents insightful theoretical analyses and rigorously conducts controlled experiments to validate these insights. The research highlights the significant impact of architectural nuances on the model's effectiveness, which is a valuable contribution to understanding how transformers can be optimized.\" Weaknesses='The scope of the study is limited to a simple counting task and single-layer transformers, which may restrict the broader applicability and impact of the findings. The practical implications for more complex, real-world tasks or multi-layer transformer models remain unexplored. Additionally, the study could benefit from expanding the range of tasks and architectures considered to generalize the insights.' Questions=\"1. How do the findings regarding architectural changes and softmax's role translate to more complex tasks or multi-layer transformer models?\\n2. Are there plans to extend the research to other simple algorithmic tasks to reinforce or contrast the conclusions drawn in this study?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=8 Confidence=4 Strengths='1. The paper introduces a highly needed large-scale dataset, MedTrinity-25M, which is a significant advancement in medical AI applications by providing rich multigranular annotations.\\n2. The proposed automated pipeline for generating the dataset eliminates the need for manual image-text pairing, addressing a critical scalability issue.\\n3. Demonstrates practical value by showcasing superior performance in medical visual QA tasks using pre-trained models on MedTrinity-25M.' Weaknesses=\"1. The automated annotation process might introduce biases or errors if the domain-specific models and retrieval systems used are not accurately designed or updated.\\n2. Potential lack of expert verification in generated annotations could impact reliability for critical medical applications.\\n3. The paper could provide more detailed comparisons to existing datasets or state-of-the-art methods to reinforce claims about the dataset's advantages.\" Questions='1. How does the quality of automatically generated annotations compare to expert-generated annotations in terms of accuracy and reliability?\\n2. What measures are in place to ensure that the domain-specific models and retrieval systems are current and accurately represent diverse medical knowledge?\\n3. Given the data source diversity, how are potential biases in the dataset identified and mitigated?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=8 Confidence=4 Strengths='1. The paper introduces a highly needed large-scale dataset, MedTrinity-25M, which is a significant advancement in medical AI applications by providing rich multigranular annotations.\\n2. The proposed automated pipeline for generating the dataset eliminates the need for manual image-text pairing, addressing a critical scalability issue.\\n3. Demonstrates practical value by showcasing superior performance in medical visual QA tasks using pre-trained models on MedTrinity-25M.' Weaknesses=\"1. The automated annotation process might introduce biases or errors if the domain-specific models and retrieval systems used are not accurately designed or updated.\\n2. Potential lack of expert verification in generated annotations could impact reliability for critical medical applications.\\n3. The paper could provide more detailed comparisons to existing datasets or state-of-the-art methods to reinforce claims about the dataset's advantages.\" Questions='1. How does the quality of automatically generated annotations compare to expert-generated annotations in terms of accuracy and reliability?\\n2. What measures are in place to ensure that the domain-specific models and retrieval systems are current and accurately represent diverse medical knowledge?\\n3. Given the data source diversity, how are potential biases in the dataset identified and mitigated?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The method effectively addresses issues with activation outliers in low-bit quantization, significantly improving INT4 inference accuracy.\\n2. Demonstrates improved performance over state-of-the-art methods like SmoothQuant and QuaRot.\\n3. Offers a practical, low-overhead solution compatible with existing hardware.' Weaknesses='1. The real-world computational overhead and efficiency of the proposed method are not extensively evaluated or compared to alternative solutions in different contexts.\\n2. The clarity on how this solution scales with larger models or different architectures could be improved.' Questions='1. How does RRS perform across a wider range of datasets or in different domain-specific applications beyond those tested?\\n2. Are there scenarios or model types where RRS might not offer a performance improvement or might introduce new challenges?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The method effectively addresses issues with activation outliers in low-bit quantization, significantly improving INT4 inference accuracy.\\n2. Demonstrates improved performance over state-of-the-art methods like SmoothQuant and QuaRot.\\n3. Offers a practical, low-overhead solution compatible with existing hardware.' Weaknesses='1. The real-world computational overhead and efficiency of the proposed method are not extensively evaluated or compared to alternative solutions in different contexts.\\n2. The clarity on how this solution scales with larger models or different architectures could be improved.' Questions='1. How does RRS perform across a wider range of datasets or in different domain-specific applications beyond those tested?\\n2. Are there scenarios or model types where RRS might not offer a performance improvement or might introduce new challenges?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses a critical issue in medical imaging: segmenting objects in videos with minimal annotation, which is an expensive and challenging task. \\n2. The proposed approach leverages test-time training with a novel memory mechanism and self-distillation, which is innovative in the context of one-shot video segmentation. \\n3. The authors introduce the OS-I2V-Seg dataset to establish a benchmark and evaluate their methods, contributing valuable resources to the community.' Weaknesses=\"1. The description of the method lacks some depth and clarity, particularly regarding the specifics of the self-distillation and memory mechanism implementation. Additional details could have strengthened the understanding of the approach. \\n2. While the proposed framework effectively leverages static images, the transition to video and handling of temporal information could be further elaborated to enhance reproducibility. \\n3. The adaptation strategy's potential for dealing with domain shifts is not deeply explored or evaluated, despite its importance in real-world applications.\" Questions='1. How does the framework handle significant domain shifts between static pre-training datasets and medical video data, particularly in unseen scenarios? \\n2. Could the memory mechanism be optimized further to enhance computational efficiency during test-time adaptation? \\n3. Given the rapid development of foundation models like SAM, how does the proposed approach compare in terms of accuracy and computational cost?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses a critical issue in medical imaging: segmenting objects in videos with minimal annotation, which is an expensive and challenging task. \\n2. The proposed approach leverages test-time training with a novel memory mechanism and self-distillation, which is innovative in the context of one-shot video segmentation. \\n3. The authors introduce the OS-I2V-Seg dataset to establish a benchmark and evaluate their methods, contributing valuable resources to the community.' Weaknesses=\"1. The description of the method lacks some depth and clarity, particularly regarding the specifics of the self-distillation and memory mechanism implementation. Additional details could have strengthened the understanding of the approach. \\n2. While the proposed framework effectively leverages static images, the transition to video and handling of temporal information could be further elaborated to enhance reproducibility. \\n3. The adaptation strategy's potential for dealing with domain shifts is not deeply explored or evaluated, despite its importance in real-world applications.\" Questions='1. How does the framework handle significant domain shifts between static pre-training datasets and medical video data, particularly in unseen scenarios? \\n2. Could the memory mechanism be optimized further to enhance computational efficiency during test-time adaptation? \\n3. Given the rapid development of foundation models like SAM, how does the proposed approach compare in terms of accuracy and computational cost?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"1. The paper introduces DCScore, a novel approach to evaluating dataset diversity, which treats it as a classification problem. This perspective offers computational efficiency and aligns with diversity indicators like human judgment.\\n2. Extensive experiments validate DCScore's effectiveness, showing its capability to adhere to diversity axioms and exhibit strong correlation with known diversity indicators.\\n3. The method reduces computational costs compared to existing approaches, potentially making it more accessible for practical applications.\" Weaknesses=\"1. The significance of the research in practical applications or its broader impact on LLM usage is not well articulated. The paper could benefit from clarifying how diversity evaluation of LLM-generated datasets directly contributes to the field.\\n2. The novelty of the method, DCScore, may not be clearly established. It's unclear how it significantly advances the state-of-the-art beyond being efficient and adhering to axioms.\\n3. There is a lack of detailed explanation on the correlation findings between DCScore's output and diversity pseudo-truths. The justification of its experimental design could be improved, particularly regarding the choice of experiments conducted.\" Questions=\"1. Could the authors elaborate on the practical implications of evaluating LLM-generated data diversity? How will this research impact the development or deployment of LLMs?\\n2. Is DCScore a simplified or innovative synthesis of existing methodologies, and how does it fundamentally differ from them?\\n3. What is the rationale behind the experiments correlating DCScore with diversity pseudo-truths, and how do these results support the overarching thesis of the paper?\\n4. How should one interpret DCScore's relative performance under different settings, such as zero-shot scenarios, and why were the specific tasks chosen for experimental evaluation?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"1. The paper introduces DCScore, a novel approach to evaluating dataset diversity, which treats it as a classification problem. This perspective offers computational efficiency and aligns with diversity indicators like human judgment.\\n2. Extensive experiments validate DCScore's effectiveness, showing its capability to adhere to diversity axioms and exhibit strong correlation with known diversity indicators.\\n3. The method reduces computational costs compared to existing approaches, potentially making it more accessible for practical applications.\" Weaknesses=\"1. The significance of the research in practical applications or its broader impact on LLM usage is not well articulated. The paper could benefit from clarifying how diversity evaluation of LLM-generated datasets directly contributes to the field.\\n2. The novelty of the method, DCScore, may not be clearly established. It's unclear how it significantly advances the state-of-the-art beyond being efficient and adhering to axioms.\\n3. There is a lack of detailed explanation on the correlation findings between DCScore's output and diversity pseudo-truths. The justification of its experimental design could be improved, particularly regarding the choice of experiments conducted.\" Questions=\"1. Could the authors elaborate on the practical implications of evaluating LLM-generated data diversity? How will this research impact the development or deployment of LLMs?\\n2. Is DCScore a simplified or innovative synthesis of existing methodologies, and how does it fundamentally differ from them?\\n3. What is the rationale behind the experiments correlating DCScore with diversity pseudo-truths, and how do these results support the overarching thesis of the paper?\\n4. How should one interpret DCScore's relative performance under different settings, such as zero-shot scenarios, and why were the specific tasks chosen for experimental evaluation?\"\n",
      "Zero-shot error: expected string or bytes-like object\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths=\"1. The introduction of a new benchmark, LAION-C, specifically designed to evaluate the out-of-distribution (OOD) robustness of modern vision models trained on large, web-scale datasets, is an invaluable contribution to the field. \\n2. The paper highlights a significant gap in current evaluation practices by showing that existing benchmarks like ImageNet-C may not truly assess OOD robustness for models trained on datasets like LAION.\\n3. Comprehensive evaluation and comparison of various vision models using the new benchmark provides useful insights into the models' strategies distinct from human perception.\" Weaknesses=\"1. The paper may have limited novelty in the sense that it mainly extends the concept of OOD testing rather than bringing a fundamentally new method or theoretical insight.\\n2. While LAION-C aims to present a more robust evaluation, the paper does not thoroughly discuss the potential biases LAION-C itself may introduce, or how representative its selected distortions are of genuine OOD scenarios across different applications.\\n3. The novelty and uniqueness of the distortions compared to existing benchmarks might not be entirely convincing, as it's unclear how they are grounded in real-world OOD challenges.\" Questions='1. How were the specific distortion types selected for LAION-C defined, and to what extent do they reflect realistic OOD conditions encountered in practice?\\n2. Have the authors considered how the LAION-C benchmark can be standardized or extended to other domains beyond visual data, such as audio or text, in a similar manner, to create complementary OOD benchmarks?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths=\"1. The introduction of a new benchmark, LAION-C, specifically designed to evaluate the out-of-distribution (OOD) robustness of modern vision models trained on large, web-scale datasets, is an invaluable contribution to the field. \\n2. The paper highlights a significant gap in current evaluation practices by showing that existing benchmarks like ImageNet-C may not truly assess OOD robustness for models trained on datasets like LAION.\\n3. Comprehensive evaluation and comparison of various vision models using the new benchmark provides useful insights into the models' strategies distinct from human perception.\" Weaknesses=\"1. The paper may have limited novelty in the sense that it mainly extends the concept of OOD testing rather than bringing a fundamentally new method or theoretical insight.\\n2. While LAION-C aims to present a more robust evaluation, the paper does not thoroughly discuss the potential biases LAION-C itself may introduce, or how representative its selected distortions are of genuine OOD scenarios across different applications.\\n3. The novelty and uniqueness of the distortions compared to existing benchmarks might not be entirely convincing, as it's unclear how they are grounded in real-world OOD challenges.\" Questions='1. How were the specific distortion types selected for LAION-C defined, and to what extent do they reflect realistic OOD conditions encountered in practice?\\n2. Have the authors considered how the LAION-C benchmark can be standardized or extended to other domains beyond visual data, such as audio or text, in a similar manner, to create complementary OOD benchmarks?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The integration of Dynamic Mode Decomposition with Graph Neural Networks is a novel approach, providing a fresh perspective to understand and improve GNN behavior.\\n2. The framework is validated on a wide range of graph-based learning tasks, demonstrating its effectiveness and adaptability.\\n3. The paper is thorough in its experimentation, supporting claims of improved performance compared to traditional GNNs.' Weaknesses='1. The application of dynamical systems theory to GNNs could benefit from a more detailed theoretical foundation and the implications of these dynamics on network performance.\\n2. While the experimental results are strong, the practical implementation details of the approach are somewhat lacking, which could challenge reproducibility.\\n3. The connection between the dynamical systems methodology and the observed improvements in performance could be more explicitly clarified.' Questions='How does the use of DMD specifically enhance the interpretability of GNNs, and what specific insights do the eigenfunctions provide about data dynamics that are not accessible through standard GNN methods?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The integration of Dynamic Mode Decomposition with Graph Neural Networks is a novel approach, providing a fresh perspective to understand and improve GNN behavior.\\n2. The framework is validated on a wide range of graph-based learning tasks, demonstrating its effectiveness and adaptability.\\n3. The paper is thorough in its experimentation, supporting claims of improved performance compared to traditional GNNs.' Weaknesses='1. The application of dynamical systems theory to GNNs could benefit from a more detailed theoretical foundation and the implications of these dynamics on network performance.\\n2. While the experimental results are strong, the practical implementation details of the approach are somewhat lacking, which could challenge reproducibility.\\n3. The connection between the dynamical systems methodology and the observed improvements in performance could be more explicitly clarified.' Questions='How does the use of DMD specifically enhance the interpretability of GNNs, and what specific insights do the eigenfunctions provide about data dynamics that are not accessible through standard GNN methods?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed method effectively reduces the computational challenges associated with influence function calculations for large models. The use of spectral properties of the Hessian is innovative and shows promise in simplifying the application of the LiSSA algorithm. The empirical validation across different models adds credibility to the results. The paper is well-organized and integrates well with existing literature on influence functions, providing a clear understanding of the problem and solution.' Weaknesses='While the approach is novel, the main contribution lies primarily in the methodological improvement for parameter setting rather than a transformative conceptual advance. The paper could benefit from a more in-depth analysis of how the proposed spectral property-based method compares with alternative approaches beyond Proximal Bregman Retraining Functions. Additionally, the choice and impact of random sketching techniques on the accuracy and scalability of the method are not thoroughly explained.' Questions='How sensitive is the proposed method to the choice of random sketching techniques? Are there specific scenarios where this method might fail compared to traditional approaches for calculating influence functions? Can this approach be adapted or extended to work with other computationally intensive methods in machine learning, beyond the influence functions?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed method effectively reduces the computational challenges associated with influence function calculations for large models. The use of spectral properties of the Hessian is innovative and shows promise in simplifying the application of the LiSSA algorithm. The empirical validation across different models adds credibility to the results. The paper is well-organized and integrates well with existing literature on influence functions, providing a clear understanding of the problem and solution.' Weaknesses='While the approach is novel, the main contribution lies primarily in the methodological improvement for parameter setting rather than a transformative conceptual advance. The paper could benefit from a more in-depth analysis of how the proposed spectral property-based method compares with alternative approaches beyond Proximal Bregman Retraining Functions. Additionally, the choice and impact of random sketching techniques on the accuracy and scalability of the method are not thoroughly explained.' Questions='How sensitive is the proposed method to the choice of random sketching techniques? Are there specific scenarios where this method might fail compared to traditional approaches for calculating influence functions? Can this approach be adapted or extended to work with other computationally intensive methods in machine learning, beyond the influence functions?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses a novel aspect of online learning in brokerage by incorporating contextual information, which is underexplored in the literature.\\n2. The theoretical contributions are significant as they propose algorithms achieving optimal regret bounds in the presence of contextual information.\\n3. The work is rigorous in its theoretical approach, thoroughly analyzing both full feedback and two-bit feedback scenarios.' Weaknesses=\"1. The assumption of zero-mean perturbations and bounded density for traders' valuations might limit the practical applicability of the proposed models.\\n2. The model's assumptions about market environment consistency with the observed context could be overly simplistic or unrealistic in real-world settings.\\n3. The presentation could benefit from clarifying the underlying assumptions and providing more intuition around the algorithms, especially the scouting ridge regression approach in the two-bit feedback mechanism.\" Questions=\"1. How does the assumption of traders' valuations as perturbations of the market value align with real market scenarios, where diverse opinions on asset values exist?\\n2. Is the performance of the proposed algorithms evaluated in any real-world settings, or is the analysis purely theoretical?\\n3. Can the methodology be extended to more complex market environments with varying or multi-dimensional trader behaviors?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses a novel aspect of online learning in brokerage by incorporating contextual information, which is underexplored in the literature.\\n2. The theoretical contributions are significant as they propose algorithms achieving optimal regret bounds in the presence of contextual information.\\n3. The work is rigorous in its theoretical approach, thoroughly analyzing both full feedback and two-bit feedback scenarios.' Weaknesses=\"1. The assumption of zero-mean perturbations and bounded density for traders' valuations might limit the practical applicability of the proposed models.\\n2. The model's assumptions about market environment consistency with the observed context could be overly simplistic or unrealistic in real-world settings.\\n3. The presentation could benefit from clarifying the underlying assumptions and providing more intuition around the algorithms, especially the scouting ridge regression approach in the two-bit feedback mechanism.\" Questions=\"1. How does the assumption of traders' valuations as perturbations of the market value align with real market scenarios, where diverse opinions on asset values exist?\\n2. Is the performance of the proposed algorithms evaluated in any real-world settings, or is the analysis purely theoretical?\\n3. Can the methodology be extended to more complex market environments with varying or multi-dimensional trader behaviors?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The introduction of a tailored quantization method specifically for State Space Models (SSMs) is a novel contribution as there is limited work in this area.\\n2. The paper effectively addresses the challenge of deploying SSMs with high efficiency on both cloud and edge hardware, providing empirical results showing the reduced latency and improved deployment efficiency.\\n3. The method maintains competitive accuracy while significantly reducing model size, an important aspect for practical deployment in resource-constrained environments.' Weaknesses='1. The paper could have provided more detailed information on the effectiveness of Quamba on a wider range of SSM models beyond just Mamba 2.8B.\\n2. The results focus mainly on zero-shot tasks, which may not fully capture the accuracy and efficiency gains across more diverse and representative real-world scenarios.' Questions='1. How does Quamba handle scenarios with more extreme data distributions or outliers compared to those observed in the experiments?\\n2. Could this approach be extended to handle larger SSM models beyond what is currently demonstrated, and if so, how would it affect efficiency and accuracy?\\n3. What potential limitations might arise when transitioning this quantization approach from experimental setups to practical, real-world applications?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The introduction of a tailored quantization method specifically for State Space Models (SSMs) is a novel contribution as there is limited work in this area.\\n2. The paper effectively addresses the challenge of deploying SSMs with high efficiency on both cloud and edge hardware, providing empirical results showing the reduced latency and improved deployment efficiency.\\n3. The method maintains competitive accuracy while significantly reducing model size, an important aspect for practical deployment in resource-constrained environments.' Weaknesses='1. The paper could have provided more detailed information on the effectiveness of Quamba on a wider range of SSM models beyond just Mamba 2.8B.\\n2. The results focus mainly on zero-shot tasks, which may not fully capture the accuracy and efficiency gains across more diverse and representative real-world scenarios.' Questions='1. How does Quamba handle scenarios with more extreme data distributions or outliers compared to those observed in the experiments?\\n2. Could this approach be extended to handle larger SSM models beyond what is currently demonstrated, and if so, how would it affect efficiency and accuracy?\\n3. What potential limitations might arise when transitioning this quantization approach from experimental setups to practical, real-world applications?'\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Content:\n",
      "Here's the revised version of output after fixing the format. I mistakenly included colons directly after the titles, which was a mistake.\n",
      "\n",
      "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper considers the positive and negative influence of different labels and encourages or suppresses labels that bring positive or negative influences, respectively.\\n2. The proposed model is a plug-and-play approach, which can be applied to various GNN backbones. \\n3. The paper offers a label correlation analysis by dissecting the pipeline into a forward and backward propagation segment.\", \"Weaknesses\": \"1. What is the difference between the label propagation methods? (GMNN: Graph Markov Neural Networks, Combining Graph Convolutional Neural Networks and Label Propagation, Resurrecting Label Propagation for Graphs with Heterophily and Label Noise). The paper should cite and compare with them, and highlight the improvement of the model.\\n2. Some hyperparameters are important to the model. It is better to give some hyperparameter analysis about the model, such as \\\\alpha, \\\\beta. It is suggested to plot the results showing how performance varies with these parameters and report the chosen values.\\n3. How does the number of label categories k affect the model? It is recommended to study the effect of the performance.\\n4. It is highly recommended to give some examples, i.e., the visualization of the positive and negative influence of different labels in the case studies, showing how certain labels positively or negatively influence others and how this affects the model's predictions.\\n5. It is recommended to give other backbones, such as the most commonly used GIN, to show the effectiveness of the model.\\n6. It is better to check the label of the axis in the figure. i.e., fig 4c. The label of the x-axis is missing.\", \"Questions\": \"Please see the above weakness.\"}\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper introduces a novel framework (VBn-KAN) that enhances the robustness and flexibility of Kolmogorov-Arnold Networks by employing variable Bernstein polynomial bases.\\n2. The proposed framework leverages Bayesian optimization for dynamic adaptation and demonstrates superiority in diverse applications like time series forecasting, computer vision, and function approximation.\\n3. The paper is well-structured and clearly articulates how the methodological advancements translate into empirical performance improvements.' Weaknesses='1. Although the method shows improved performance, the innovation primarily resides in the application of existing mathematical concepts within the KAN framework, which may limit its novelty.\\n2. The experimentation, while comprehensive, could benefit from more thorough benchmarking against state-of-the-art non-KAN models across the described tasks to highlight competitive advantages.\\n3. Bayesian optimization, being resource-intensive, might be a limiting factor in practical scalability for high-dimensional or very large dataset scenarios.' Questions='1. How does the complexity of the VBn-KAN model, both in terms of parameters and computational requirements, compare with traditional KANs and typical neural network frameworks?\\n2. Can the authors elaborate on the computational overhead imposed by dynamically adjusting polynomial degrees using Bayesian optimization?\\n3. How generalizable is the approach to other domains outside of the tested fields, such as natural language processing or reinforcement learning?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper introduces a novel framework (VBn-KAN) that enhances the robustness and flexibility of Kolmogorov-Arnold Networks by employing variable Bernstein polynomial bases.\\n2. The proposed framework leverages Bayesian optimization for dynamic adaptation and demonstrates superiority in diverse applications like time series forecasting, computer vision, and function approximation.\\n3. The paper is well-structured and clearly articulates how the methodological advancements translate into empirical performance improvements.' Weaknesses='1. Although the method shows improved performance, the innovation primarily resides in the application of existing mathematical concepts within the KAN framework, which may limit its novelty.\\n2. The experimentation, while comprehensive, could benefit from more thorough benchmarking against state-of-the-art non-KAN models across the described tasks to highlight competitive advantages.\\n3. Bayesian optimization, being resource-intensive, might be a limiting factor in practical scalability for high-dimensional or very large dataset scenarios.' Questions='1. How does the complexity of the VBn-KAN model, both in terms of parameters and computational requirements, compare with traditional KANs and typical neural network frameworks?\\n2. Can the authors elaborate on the computational overhead imposed by dynamically adjusting polynomial degrees using Bayesian optimization?\\n3. How generalizable is the approach to other domains outside of the tested fields, such as natural language processing or reinforcement learning?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper presents a novel approach to generating articulated 3D objects from a single image, addressing scalability and practicality issues in current methods.\\n2. The use of diffusion-based models, large pretrained models like DINOv2 and GPT-4o, and a coarse-to-fine generation pipeline demonstrates an innovative integration of techniques.\\n3. Systematic evaluations show significant improvements over state-of-the-art methods, with impressive generalization to complex datasets.\\n4. Qualitative analysis and user studies confirm improved realism and consistency of the generated assets.' Weaknesses='1. The approach relies on state-of-the-art pre-trained models, which may limit accessibility or require substantial computational resources.\\n2. There could be challenges in extending the method to highly diverse or complex object categories not covered in the current datasets.\\n3. The dependency on a single image may limit the full understanding of object parts in cases of occlusion or atypical configurations, which may affect the reconstruction quality.' Questions='1. How does the method handle cases where the single image contains occluded or unusual object configurations?\\n2. What are the limitations or trade-offs of relying on pretrained models like DINOv2 and GPT-4o in terms of computational efficiency and model accessibility?\\n3. How scalable is the method to handle an increased diversity of object categories and complexity, and what datasets are anticipated to further validate this approach?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper presents a novel approach to generating articulated 3D objects from a single image, addressing scalability and practicality issues in current methods.\\n2. The use of diffusion-based models, large pretrained models like DINOv2 and GPT-4o, and a coarse-to-fine generation pipeline demonstrates an innovative integration of techniques.\\n3. Systematic evaluations show significant improvements over state-of-the-art methods, with impressive generalization to complex datasets.\\n4. Qualitative analysis and user studies confirm improved realism and consistency of the generated assets.' Weaknesses='1. The approach relies on state-of-the-art pre-trained models, which may limit accessibility or require substantial computational resources.\\n2. There could be challenges in extending the method to highly diverse or complex object categories not covered in the current datasets.\\n3. The dependency on a single image may limit the full understanding of object parts in cases of occlusion or atypical configurations, which may affect the reconstruction quality.' Questions='1. How does the method handle cases where the single image contains occluded or unusual object configurations?\\n2. What are the limitations or trade-offs of relying on pretrained models like DINOv2 and GPT-4o in terms of computational efficiency and model accessibility?\\n3. How scalable is the method to handle an increased diversity of object categories and complexity, and what datasets are anticipated to further validate this approach?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. Novel application of Transformer architecture in simulating physical dynamics, addressing a less explored area in comparison to traditional GNN approaches.\\n2. Introduction of the Temporal Difference Graph (TDG) to effectively handle cumulative errors in long-term predictions.\\n3. Performance of GSTs tested and validated across multiple scales, demonstrating strong potential to outperform existing methods.' Weaknesses='1. The complexity of the proposed model and its scalability concerning computational resources and larger datasets are not thoroughly addressed.\\n2. Lack of detailed comparison on runtime performance can pose concerns regarding practical applicability in real-world scenarios.\\n3. Limited information about the size of datasets and problem complexity, which may affect the validity of comparisons with other models.' Questions='1. How does the implementation of GSTs affect the computational requirements compared to traditional GNN approaches?\\n2. Can the proposed Temporal Difference Graph (TDG) technique be generalized or adapted to other applications beyond physical dynamic simulations?\\n3. Are there specific datasets or scenarios where the proposed model does not perform as well, and if so, what are the limitations?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. Novel application of Transformer architecture in simulating physical dynamics, addressing a less explored area in comparison to traditional GNN approaches.\\n2. Introduction of the Temporal Difference Graph (TDG) to effectively handle cumulative errors in long-term predictions.\\n3. Performance of GSTs tested and validated across multiple scales, demonstrating strong potential to outperform existing methods.' Weaknesses='1. The complexity of the proposed model and its scalability concerning computational resources and larger datasets are not thoroughly addressed.\\n2. Lack of detailed comparison on runtime performance can pose concerns regarding practical applicability in real-world scenarios.\\n3. Limited information about the size of datasets and problem complexity, which may affect the validity of comparisons with other models.' Questions='1. How does the implementation of GSTs affect the computational requirements compared to traditional GNN approaches?\\n2. Can the proposed Temporal Difference Graph (TDG) technique be generalized or adapted to other applications beyond physical dynamic simulations?\\n3. Are there specific datasets or scenarios where the proposed model does not perform as well, and if so, what are the limitations?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper successfully extends diffusion probabilistic models to graph representation learning, a relatively unexplored area. It presents a novel framework, Graffe, which demonstrates state-of-the-art performance on multiple benchmarks. The approach integrates theoretical foundations with practical experiments, contributing valuable insights into the suitability of diffusion models for non-Euclidean datasets.' Weaknesses='The dependence on specific configurations such as mask ratio can lead to extensive tuning, as suggested in the limitations. While theoretical claims are made, not all insights are fully elaborated, particularly the role of graph spectra or how reconstruction influences performance negatively.' Questions='How does the proposed Graffe model handle graphs of varying sizes, and is there a performance trade-off based on graph complexity? Moreover, would altering the encoder and decoder roles within the diffusion process affect the overall model efficacy?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper successfully extends diffusion probabilistic models to graph representation learning, a relatively unexplored area. It presents a novel framework, Graffe, which demonstrates state-of-the-art performance on multiple benchmarks. The approach integrates theoretical foundations with practical experiments, contributing valuable insights into the suitability of diffusion models for non-Euclidean datasets.' Weaknesses='The dependence on specific configurations such as mask ratio can lead to extensive tuning, as suggested in the limitations. While theoretical claims are made, not all insights are fully elaborated, particularly the role of graph spectra or how reconstruction influences performance negatively.' Questions='How does the proposed Graffe model handle graphs of varying sizes, and is there a performance trade-off based on graph complexity? Moreover, would altering the encoder and decoder roles within the diffusion process affect the overall model efficacy?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed mechanism introduces an innovative integration approach that enhances the handling of external knowledge by LLMs, leading to improved performance on QA tasks. The use of a memory-based layer-wise integration strategy allows for more nuanced and effective utilization of retrieved knowledge.' Weaknesses='The paper may lack clarity in detailing the implementation specifics and justification for certain methodological choices, such as the decision to incorporate an additional attention head rather than modifying existing ones. Additionally, potential challenges or limitations in scaling or adapting the proposed method to various LLM architectures and domains are not extensively explored.' Questions='How does the external knowledge attention mechanism affect the computational efficiency and model scalability compared to traditional RAG methods? Are there any considerations or limitations for integrating this approach with LLMs of varying sizes or architectures?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed mechanism introduces an innovative integration approach that enhances the handling of external knowledge by LLMs, leading to improved performance on QA tasks. The use of a memory-based layer-wise integration strategy allows for more nuanced and effective utilization of retrieved knowledge.' Weaknesses='The paper may lack clarity in detailing the implementation specifics and justification for certain methodological choices, such as the decision to incorporate an additional attention head rather than modifying existing ones. Additionally, potential challenges or limitations in scaling or adapting the proposed method to various LLM architectures and domains are not extensively explored.' Questions='How does the external knowledge attention mechanism affect the computational efficiency and model scalability compared to traditional RAG methods? Are there any considerations or limitations for integrating this approach with LLMs of varying sizes or architectures?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"1. The paper introduces a novel algorithm, GRL and GRL-RB, for guided reinforcement learning with adaptive sampling rates, which ensures performance guarantees by maintaining the learning policy's mean return above a user-defined threshold. This theoretically grounded approach has the potential to provide practical benefits in reinforcement learning applied to tasks requiring transition between guide and learning policies.\\n2. The approach of dynamically adjusting the sampling rate to prevent performance degradation is sound and could prove valuable across various practical scenarios. The roll-back feature (GRL-RB) for recovering from potential performance degradation adds to the robustness of the method.\\n3. Empirical validation has been done on structured environments like Combination Lock and AntMaze, demonstrating the robustness of the proposed method across different environments.\\n4. The paper is relatively well-organized, providing a clear description of the problem, methodology, and validation.\" Weaknesses=\"1. The experimental validation, while present, is limited in terms of diversity and realism of tasks. While Combination Lock and AntMaze provide a baseline understanding, they may not represent the challenges faced in more complex, real-world reinforcement learning tasks. Additionally, the empirical results show that the LD baseline performs similarly to the new algorithm, raising questions about the practical impact of the proposed approach.\\n2. The assumptions made for the derivation are strong and may not hold in practical scenarios, which might limit the applicability of the performance guarantees in real-world tasks.\\n3. Some claims in the paper, like the guarantee of online performance above a threshold, could be considered overstated, as the mean performance guarantee does not necessarily protect against individual episodes falling below thresholds.\\n4. The paper may benefit from a more comprehensive discussion on the potential limitations when extending the approach to more complex tasks or environments with different reward structures.\\n5. Some parts of the presentation, such as figure interpretation, could be improved to avoid potential misinterpretation, and the term 'guarantee' should be used cautiously to reflect the assumptions and limitations effectively.\" Questions='1. Can the GRL and GRL-RB algorithms be extended to environments with more complex reward structures or tasks with different dynamics?\\n2. How do the assumptions about convergence and performance guarantees hold in larger, real-world environments, and what modifications might be necessary for broader applicability?\\n3. Could the authors clarify the empirical results and variations observed in the guide sampling rate graphics, ensuring it aligns with expectations from derived parameters?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"1. The paper introduces a novel algorithm, GRL and GRL-RB, for guided reinforcement learning with adaptive sampling rates, which ensures performance guarantees by maintaining the learning policy's mean return above a user-defined threshold. This theoretically grounded approach has the potential to provide practical benefits in reinforcement learning applied to tasks requiring transition between guide and learning policies.\\n2. The approach of dynamically adjusting the sampling rate to prevent performance degradation is sound and could prove valuable across various practical scenarios. The roll-back feature (GRL-RB) for recovering from potential performance degradation adds to the robustness of the method.\\n3. Empirical validation has been done on structured environments like Combination Lock and AntMaze, demonstrating the robustness of the proposed method across different environments.\\n4. The paper is relatively well-organized, providing a clear description of the problem, methodology, and validation.\" Weaknesses=\"1. The experimental validation, while present, is limited in terms of diversity and realism of tasks. While Combination Lock and AntMaze provide a baseline understanding, they may not represent the challenges faced in more complex, real-world reinforcement learning tasks. Additionally, the empirical results show that the LD baseline performs similarly to the new algorithm, raising questions about the practical impact of the proposed approach.\\n2. The assumptions made for the derivation are strong and may not hold in practical scenarios, which might limit the applicability of the performance guarantees in real-world tasks.\\n3. Some claims in the paper, like the guarantee of online performance above a threshold, could be considered overstated, as the mean performance guarantee does not necessarily protect against individual episodes falling below thresholds.\\n4. The paper may benefit from a more comprehensive discussion on the potential limitations when extending the approach to more complex tasks or environments with different reward structures.\\n5. Some parts of the presentation, such as figure interpretation, could be improved to avoid potential misinterpretation, and the term 'guarantee' should be used cautiously to reflect the assumptions and limitations effectively.\" Questions='1. Can the GRL and GRL-RB algorithms be extended to environments with more complex reward structures or tasks with different dynamics?\\n2. How do the assumptions about convergence and performance guarantees hold in larger, real-world environments, and what modifications might be necessary for broader applicability?\\n3. Could the authors clarify the empirical results and variations observed in the guide sampling rate graphics, ensuring it aligns with expectations from derived parameters?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1) The paper presents a well-motivated method (DoD) integrating visual priors into the diffusion process, enhancing texture detail in generated images.\\n2) The method demonstrates notable improvements in image quality and training efficiency over traditional class-guided diffusion models.\\n3) The framework is theoretically sound and addresses significant shortcomings of existing techniques, offering a state-of-the-art solution.' Weaknesses='1) The paper could benefit from comparisons against other very recent SOTA methods in diffusion models and visual priors, potentially overlooking some competitive baselines.\\n2) The qualitative results, while indicated to improve texture detail, are not prominently highlighted in the paper.\\n3) Some technical implementations such as the use of the Latent Embedding Module (LEM) for compressing samples may need further clarity to avoid misconceptions.' Questions='1) How does the proposed method compare with the latest advancements in text-to-image synthesis, particularly those employing more advanced conditioning techniques?\\n2) Can the DoD framework be adapted or extended to tasks beyond image generation, potentially leveraging similar principles in different generative applications?\\n3) Would the integration of LEM sacrifice any critical attributes or detail in samples, which might affect other downstream tasks relying on these generations?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1) The paper presents a well-motivated method (DoD) integrating visual priors into the diffusion process, enhancing texture detail in generated images.\\n2) The method demonstrates notable improvements in image quality and training efficiency over traditional class-guided diffusion models.\\n3) The framework is theoretically sound and addresses significant shortcomings of existing techniques, offering a state-of-the-art solution.' Weaknesses='1) The paper could benefit from comparisons against other very recent SOTA methods in diffusion models and visual priors, potentially overlooking some competitive baselines.\\n2) The qualitative results, while indicated to improve texture detail, are not prominently highlighted in the paper.\\n3) Some technical implementations such as the use of the Latent Embedding Module (LEM) for compressing samples may need further clarity to avoid misconceptions.' Questions='1) How does the proposed method compare with the latest advancements in text-to-image synthesis, particularly those employing more advanced conditioning techniques?\\n2) Can the DoD framework be adapted or extended to tasks beyond image generation, potentially leveraging similar principles in different generative applications?\\n3) Would the integration of LEM sacrifice any critical attributes or detail in samples, which might affect other downstream tasks relying on these generations?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=5 Strengths='The manuscript is extremely well written and provides clear statements to understand the proposed method. The authors demonstrate a successful implementation of recently introduced Epistemic Neural Networks for a relevant and important open problem in robot learning (i.e. generalization over unobservable environment contexts)' Weaknesses='Missing recent related works: the authors should consider citing and discussing recent works [1] and [2] as they present themselves as state-of-the-art methods in Robust RL and Domain Randomization as of 2024. Specifically, DORAEMON [2] tackles the same problem setting as in this work where privileged information is available at training time, and a history of previous observations and actions is used to allow implicit system identification at test time and promote adaptive behavior. Limited experimental evaluation: While Domain Randomization is an extremely popular baseline for learning robust/adaptive behavior in sim2real settings, the authors provide little information to the implementation of this baseline and no comparison of GRAM vs. DR in fig. 4 and 5. Implementation details of DR: I highly suggest the authors to compare GRAM against a DR baseline which also uses a history of previous state and actions, as this is the notorious way to implement DR [3]. Furthermore, DR may and should also leverage privileged information at training time, by using the notorious asymmetric actor-critic paradigm [2, 3, 4]. Conditioning the critic on the known context often drastically affects the results of DR methods vs. unprivileged critics, and does not require further assumptions. The analysis is carried out on locomotion environments only, and simulated environments only. The generalization problem under unobservable dynamics is likely exacerbated and more challenging for manipulation and contact-rich settings, which would make the experimental evaluation more significant and relevant. [1] Reddi, A. et al. \"Robust Adversarial Reinforcement Learning via Bounded Rationality Curricula.\" ICLR 2024. [2] Tiboni, G. et al. \"Domain Randomization via Entropy Maximization.\" ICLR 2024. [3] Peng, Xue Bin, et al. \"Sim-to-real transfer of robotic control with dynamics randomization.\" 2018 IEEE international conference on robotics and automation (ICRA). IEEE, 2018. [4] Handa, Ankur, et al. \"Dextreme: Transfer of agile in-hand manipulation from simulation to reality.\" 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023.' Questions='Why is the performance of all baselines better in Fig. 3 (right) for the \"Base ID + frozen joints\" vs. the \"Base ID\" counterpart? Regarding the statement at the end of Sec. 5 \"as the goal is to train the epinet in (5) to output estimates with low variance in ID contexts and high variance in OOD contexts.\". Does GRAM promote the learned encoder to output high variance in OOD contexts at training time? Judging from Eq. (7) alone, it seems to me that the encoder is only trained to provide low variance on ID contexts, whereas a higher OOD variance is a spontaneous effect that occurs at test time only.'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=5 Strengths='The manuscript is extremely well written and provides clear statements to understand the proposed method. The authors demonstrate a successful implementation of recently introduced Epistemic Neural Networks for a relevant and important open problem in robot learning (i.e. generalization over unobservable environment contexts)' Weaknesses='Missing recent related works: the authors should consider citing and discussing recent works [1] and [2] as they present themselves as state-of-the-art methods in Robust RL and Domain Randomization as of 2024. Specifically, DORAEMON [2] tackles the same problem setting as in this work where privileged information is available at training time, and a history of previous observations and actions is used to allow implicit system identification at test time and promote adaptive behavior. Limited experimental evaluation: While Domain Randomization is an extremely popular baseline for learning robust/adaptive behavior in sim2real settings, the authors provide little information to the implementation of this baseline and no comparison of GRAM vs. DR in fig. 4 and 5. Implementation details of DR: I highly suggest the authors to compare GRAM against a DR baseline which also uses a history of previous state and actions, as this is the notorious way to implement DR [3]. Furthermore, DR may and should also leverage privileged information at training time, by using the notorious asymmetric actor-critic paradigm [2, 3, 4]. Conditioning the critic on the known context often drastically affects the results of DR methods vs. unprivileged critics, and does not require further assumptions. The analysis is carried out on locomotion environments only, and simulated environments only. The generalization problem under unobservable dynamics is likely exacerbated and more challenging for manipulation and contact-rich settings, which would make the experimental evaluation more significant and relevant. [1] Reddi, A. et al. \"Robust Adversarial Reinforcement Learning via Bounded Rationality Curricula.\" ICLR 2024. [2] Tiboni, G. et al. \"Domain Randomization via Entropy Maximization.\" ICLR 2024. [3] Peng, Xue Bin, et al. \"Sim-to-real transfer of robotic control with dynamics randomization.\" 2018 IEEE international conference on robotics and automation (ICRA). IEEE, 2018. [4] Handa, Ankur, et al. \"Dextreme: Transfer of agile in-hand manipulation from simulation to reality.\" 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023.' Questions='Why is the performance of all baselines better in Fig. 3 (right) for the \"Base ID + frozen joints\" vs. the \"Base ID\" counterpart? Regarding the statement at the end of Sec. 5 \"as the goal is to train the epinet in (5) to output estimates with low variance in ID contexts and high variance in OOD contexts.\". Does GRAM promote the learned encoder to output high variance in OOD contexts at training time? Judging from Eq. (7) alone, it seems to me that the encoder is only trained to provide low variance on ID contexts, whereas a higher OOD variance is a spontaneous effect that occurs at test time only.'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper addresses a critical issue of privacy risks in LLMs by improving upon existing canary-based privacy auditing methods.\\n2. The methodology for generating more effective canaries with unigram, N-gram, and model-based strategies is innovative and demonstrates significant improvement in empirical privacy leakage estimation.\\n3. Comprehensive experiments across various LLM architectures and privacy settings, including differentially private models, provide a robust validation of the approach.' Weaknesses='1. The paper may not fully explore the generalizability of their proposed canary across different types of datasets beyond those tested.\\n2. The practical implementation of the proposed canaries might be complex, and real-world applicability in various models is yet to be thoroughly vetted.\\n3. The paper does not fully address how these findings could be systematically integrated into privacy-preserving practices for developers.' Questions='1. How does the proposed canary generation process handle potential variations in natural language data that might affect memorization in diverse model architectures or fine-tuning scenarios?\\n2. Can the effectiveness of the canaries be preserved when models undergo significant architecture changes, or is the approach closely tied to specific model structures?\\n3. What strategies can developers implement to mitigate the privacy risks identified in models with the new canary-based audits in environments with tighter privacy constraints, such as federated learning settings?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper addresses a critical issue of privacy risks in LLMs by improving upon existing canary-based privacy auditing methods.\\n2. The methodology for generating more effective canaries with unigram, N-gram, and model-based strategies is innovative and demonstrates significant improvement in empirical privacy leakage estimation.\\n3. Comprehensive experiments across various LLM architectures and privacy settings, including differentially private models, provide a robust validation of the approach.' Weaknesses='1. The paper may not fully explore the generalizability of their proposed canary across different types of datasets beyond those tested.\\n2. The practical implementation of the proposed canaries might be complex, and real-world applicability in various models is yet to be thoroughly vetted.\\n3. The paper does not fully address how these findings could be systematically integrated into privacy-preserving practices for developers.' Questions='1. How does the proposed canary generation process handle potential variations in natural language data that might affect memorization in diverse model architectures or fine-tuning scenarios?\\n2. Can the effectiveness of the canaries be preserved when models undergo significant architecture changes, or is the approach closely tied to specific model structures?\\n3. What strategies can developers implement to mitigate the privacy risks identified in models with the new canary-based audits in environments with tighter privacy constraints, such as federated learning settings?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The novel dual-branch architecture effectively incorporates temporal information into stereo video conversion, leading to improved consistency and visual quality.\\n2. The introduction of the YouTube-SBS dataset provides a valuable resource for benchmarking and developing stereo video generation algorithms.\\n3. The methodology is well-justified and supported with quantitative evidence showing substantial improvements over existing methods.' Weaknesses='1. The paper lacks detailed analysis on the computational efficiency and runtime performance of the proposed method relative to previous approaches.\\n2. There is some ambiguity regarding the scalability of the approach to varying video qualities or different content domains.\\n3. Potential biases or limitations within the YouTube-SBS dataset are not discussed, which could affect generalizability.' Questions='1. How does ImmersePro perform on videos with varying levels of noise or compression artifacts compared to high-quality inputs?\\n2. Are there specific types of videos for which the method underperforms, such as rapidly changing scenes or low-light conditions?\\n3. Can the authors elaborate on the scalability of their approach when applied to longer or ultra-high-resolution video content?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The novel dual-branch architecture effectively incorporates temporal information into stereo video conversion, leading to improved consistency and visual quality.\\n2. The introduction of the YouTube-SBS dataset provides a valuable resource for benchmarking and developing stereo video generation algorithms.\\n3. The methodology is well-justified and supported with quantitative evidence showing substantial improvements over existing methods.' Weaknesses='1. The paper lacks detailed analysis on the computational efficiency and runtime performance of the proposed method relative to previous approaches.\\n2. There is some ambiguity regarding the scalability of the approach to varying video qualities or different content domains.\\n3. Potential biases or limitations within the YouTube-SBS dataset are not discussed, which could affect generalizability.' Questions='1. How does ImmersePro perform on videos with varying levels of noise or compression artifacts compared to high-quality inputs?\\n2. Are there specific types of videos for which the method underperforms, such as rapidly changing scenes or low-light conditions?\\n3. Can the authors elaborate on the scalability of their approach when applied to longer or ultra-high-resolution video content?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper presents a novel approach to extrapolative protein design by integrating triplet relations into preference learning models, enhancing the generative model's capabilities. Thorough evaluations demonstrate significant improvements over existing state-of-the-art methods in protein design across multiple datasets, showcasing the method's robustness and effectiveness. The innovative use of triplet-based data distillation addresses a key limitation in current extrapolative design methods, making a strong contribution to the field.\" Weaknesses='While the methodology is promising, its reliance on creating accurate triplet datasets may present practical challenges, especially in scenarios where obtaining reliable triplet data is difficult. The paper could discuss the limitations or potential pitfalls in scenarios with noisy or incomplete triplet data, as well as robustness against varying degrees of triplet noise. Furthermore, the technical depth may require a strong background in the domain to fully grasp the methodological novelty.' Questions='How sensitive is the proposed method to the quality and noise level of triplet data? In real-world applications, it might be challenging to construct accurate triplets; does the model retain its efficacy when the triplet data is noisy or incomplete? Additionally, how does the approach handle cases where triplet information does not align with the expected fitness gradients due to experimental limitations?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper presents a novel approach to extrapolative protein design by integrating triplet relations into preference learning models, enhancing the generative model's capabilities. Thorough evaluations demonstrate significant improvements over existing state-of-the-art methods in protein design across multiple datasets, showcasing the method's robustness and effectiveness. The innovative use of triplet-based data distillation addresses a key limitation in current extrapolative design methods, making a strong contribution to the field.\" Weaknesses='While the methodology is promising, its reliance on creating accurate triplet datasets may present practical challenges, especially in scenarios where obtaining reliable triplet data is difficult. The paper could discuss the limitations or potential pitfalls in scenarios with noisy or incomplete triplet data, as well as robustness against varying degrees of triplet noise. Furthermore, the technical depth may require a strong background in the domain to fully grasp the methodological novelty.' Questions='How sensitive is the proposed method to the quality and noise level of triplet data? In real-world applications, it might be challenging to construct accurate triplets; does the model retain its efficacy when the triplet data is noisy or incomplete? Additionally, how does the approach handle cases where triplet information does not align with the expected fitness gradients due to experimental limitations?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper proposes a novel adaptive data augmentation method (AutoTSAug) that targets marginal samples, potentially leading to enhanced performance in time series forecasting.\\n2. The method incorporates a reinforcement learning framework, utilizing prediction variance across a model zoo as a reward mechanism, which is an innovative approach.\\n3. The paper demonstrates that AutoTSAug can improve forecasting accuracy with minimal computational overhead.' Weaknesses='1. The requirement of a robust model zoo for optimal results might be a significant limitation, as assembling such a zoo can be resource-intensive and might not be feasible for all practitioners.\\n2. The paper could have provided a more detailed comparison with existing adaptive and non-adaptive data augmentation methods in the time series forecasting domain to better highlight its contributions.\\n3. The method may not be straightforward to implement and require substantial tuning, as the reinforcement learning component adds complexity.' Questions=\"1. How sensitive are the results to the choice of models in the model zoo? Is there a minimum diversity or quality of models required to obtain significant improvements?\\n2. Given the reinforcement learning framework's reliance on variance as a reward signal, how does the method handle cases where the variance is inherently low across the model zoo?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper proposes a novel adaptive data augmentation method (AutoTSAug) that targets marginal samples, potentially leading to enhanced performance in time series forecasting.\\n2. The method incorporates a reinforcement learning framework, utilizing prediction variance across a model zoo as a reward mechanism, which is an innovative approach.\\n3. The paper demonstrates that AutoTSAug can improve forecasting accuracy with minimal computational overhead.' Weaknesses='1. The requirement of a robust model zoo for optimal results might be a significant limitation, as assembling such a zoo can be resource-intensive and might not be feasible for all practitioners.\\n2. The paper could have provided a more detailed comparison with existing adaptive and non-adaptive data augmentation methods in the time series forecasting domain to better highlight its contributions.\\n3. The method may not be straightforward to implement and require substantial tuning, as the reinforcement learning component adds complexity.' Questions=\"1. How sensitive are the results to the choice of models in the model zoo? Is there a minimum diversity or quality of models required to obtain significant improvements?\\n2. Given the reinforcement learning framework's reliance on variance as a reward signal, how does the method handle cases where the variance is inherently low across the model zoo?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths=\"1. The methodically solid framework enhances cross-modal alignment by addressing a key issue in CLIP models: the modality gap.\\n2. The introduction of shared parameter encoders and semantically-regularized separation objectives shows a creative approach in improving CLIP's performance.\\n3. Experimental results convincingly demonstrate enhanced zero-shot classification and multi-modal retrieval tasks, backing the proposed approach's efficacy.\\n4. The paper is well-structured and clearly presented, making complex ideas easy to follow.\" Weaknesses='1. The novelty might be somewhat incremental given that shared transformer architectures and semantic separation strategies have been explored in related domains.\\n2. The dependency on the specific dataset (CC12M) for pre-training and the choice of SBERT for semantic regularization could limit the generalizability of the method.\\n3. More detailed ablation studies are needed to separate the contributions of each component (shared transformer vs. intra-modality separation) within the proposed AlignCLIP framework.' Questions='1. How sensitive is AlignCLIP to the choice of datasets and pre-training strategies, particularly beyond CC12M?\\n2. Could you provide more insights or hypothetical scenarios where the intra-modality separation might cause performance drops?\\n3. Is there potential to apply the proposed method in other domains beyond vision-language tasks, such as audio-visual tasks?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths=\"1. The methodically solid framework enhances cross-modal alignment by addressing a key issue in CLIP models: the modality gap.\\n2. The introduction of shared parameter encoders and semantically-regularized separation objectives shows a creative approach in improving CLIP's performance.\\n3. Experimental results convincingly demonstrate enhanced zero-shot classification and multi-modal retrieval tasks, backing the proposed approach's efficacy.\\n4. The paper is well-structured and clearly presented, making complex ideas easy to follow.\" Weaknesses='1. The novelty might be somewhat incremental given that shared transformer architectures and semantic separation strategies have been explored in related domains.\\n2. The dependency on the specific dataset (CC12M) for pre-training and the choice of SBERT for semantic regularization could limit the generalizability of the method.\\n3. More detailed ablation studies are needed to separate the contributions of each component (shared transformer vs. intra-modality separation) within the proposed AlignCLIP framework.' Questions='1. How sensitive is AlignCLIP to the choice of datasets and pre-training strategies, particularly beyond CC12M?\\n2. Could you provide more insights or hypothetical scenarios where the intra-modality separation might cause performance drops?\\n3. Is there potential to apply the proposed method in other domains beyond vision-language tasks, such as audio-visual tasks?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper proposes a novel approach by combining structural and behavioral losses for autoencoder training, improving model reconstruction in neural networks. The method shows enhanced performance for various tasks and datasets, indicating its potential effectiveness. The problem formulation and the combination of losses contribute to the understanding of NN weight as a data modality.' Weaknesses=\"The key concern involves the limited exploration of more diverse neural network architectures and tasks which may narrow the applicability and demonstration of the method's robustness. The claim of novelty could be diluted by existing uses of similar loss functions in related work. In addition, there is a significant training time increase when using the behavioral loss, presenting practicality challenges for larger networks.\" Questions='1. How does the method perform with larger scale networks, especially given the doubling of training times?\\n2. Can the methodology be readily adapted to or extended beyond discriminative tasks for different architecture types (e.g., recurrent neural networks) or broader datasets?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper proposes a novel approach by combining structural and behavioral losses for autoencoder training, improving model reconstruction in neural networks. The method shows enhanced performance for various tasks and datasets, indicating its potential effectiveness. The problem formulation and the combination of losses contribute to the understanding of NN weight as a data modality.' Weaknesses=\"The key concern involves the limited exploration of more diverse neural network architectures and tasks which may narrow the applicability and demonstration of the method's robustness. The claim of novelty could be diluted by existing uses of similar loss functions in related work. In addition, there is a significant training time increase when using the behavioral loss, presenting practicality challenges for larger networks.\" Questions='1. How does the method perform with larger scale networks, especially given the doubling of training times?\\n2. Can the methodology be readily adapted to or extended beyond discriminative tasks for different architecture types (e.g., recurrent neural networks) or broader datasets?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper proposes a novel approach to address extrapolation errors in offline reinforcement learning, which is a significant challenge in the field. \\n2. The method, EIQL, combines expectile regression with Implicit Q-Learning to effectively utilize out-of-sample actions, providing a new direction for enhancing offline RL algorithms. \\n3. Experimental results demonstrate that EIQL outperforms traditional offline RL algorithms in various challenging environments, highlighting its practical impact.' Weaknesses='1. The paper could provide more detailed explanations of the underlying mechanisms of EIQL, particularly how it balances the use of in-sample and out-of-sample actions.\\n2. The theoretical justification for the approach and the choice of model components could be more thoroughly developed to enhance understanding.\\n3. The presentation could be improved by including a clearer description of the algorithm and its parameters to facilitate reproducibility.' Questions='1. How sensitive is EIQL to the choice of expectile level, and how should one choose this parameter in practice?\\n2. Can EIQL be extended or modified to handle offline RL settings with more complex action spaces, such as continuous control tasks?\\n3. What insights can be gleaned from the experimental results regarding the environments where EIQL shows particularly strong performance?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper proposes a novel approach to address extrapolation errors in offline reinforcement learning, which is a significant challenge in the field. \\n2. The method, EIQL, combines expectile regression with Implicit Q-Learning to effectively utilize out-of-sample actions, providing a new direction for enhancing offline RL algorithms. \\n3. Experimental results demonstrate that EIQL outperforms traditional offline RL algorithms in various challenging environments, highlighting its practical impact.' Weaknesses='1. The paper could provide more detailed explanations of the underlying mechanisms of EIQL, particularly how it balances the use of in-sample and out-of-sample actions.\\n2. The theoretical justification for the approach and the choice of model components could be more thoroughly developed to enhance understanding.\\n3. The presentation could be improved by including a clearer description of the algorithm and its parameters to facilitate reproducibility.' Questions='1. How sensitive is EIQL to the choice of expectile level, and how should one choose this parameter in practice?\\n2. Can EIQL be extended or modified to handle offline RL settings with more complex action spaces, such as continuous control tasks?\\n3. What insights can be gleaned from the experimental results regarding the environments where EIQL shows particularly strong performance?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses an important gap in the adaptive complexity of sampling, particularly in high-dimensional, high-accuracy settings. It establishes solid theoretical bounds that provide insight into the limitations of current sampling algorithms. The novel construction of hardness potentials and use of random partitions are innovative contributions.' Weaknesses='While the paper sets important theoretical limits, it does not propose any new algorithms or practical improvements for overcoming these limits. The presentation is technical and might be challenging for readers not deeply familiar with the topic. Some assumptions, such as log-concavity, may limit the applicability of the results.' Questions='Can these lower bounds lead to new approaches or modifications in existing sampling algorithms to better handle high-dimensional cases? How do these findings translate into implications for real-world applications of sampling in machine learning and Bayesian inference?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses an important gap in the adaptive complexity of sampling, particularly in high-dimensional, high-accuracy settings. It establishes solid theoretical bounds that provide insight into the limitations of current sampling algorithms. The novel construction of hardness potentials and use of random partitions are innovative contributions.' Weaknesses='While the paper sets important theoretical limits, it does not propose any new algorithms or practical improvements for overcoming these limits. The presentation is technical and might be challenging for readers not deeply familiar with the topic. Some assumptions, such as log-concavity, may limit the applicability of the results.' Questions='Can these lower bounds lead to new approaches or modifications in existing sampling algorithms to better handle high-dimensional cases? How do these findings translate into implications for real-world applications of sampling in machine learning and Bayesian inference?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper introduces a novel method, Soft Preference Optimization (SPO), which is simpler yet competitive in aligning Large Language Models with human preferences. The integration of preference loss with regularization is interesting, providing a balance that maintains the performance while favorably adhering to human preferences. SPO's ability to function without a reward model is an appealing simplification over existing complex RLHF methodologies. Experimental results showcasing performance improvements against methods like DPO are well-demonstrated.\" Weaknesses='The paper lacks comprehensive comparisons with established RLHF methods that could solidify the claimed advantages of the SPO approach. Additionally, the explanation and theoretical derivation of the regularization term could be more detailed, which might help in better understanding the applicability and limitations of the approach. Concerns around potential overfitting or misalignment with out-of-distribution data due to the focus on preference data alone are valid and need further exploration.' Questions=\"How does the regularization term specifically contribute to avoiding overfitting in SPO compared to traditional reward-based alignment? Are there plans to explore SPO's applicability on larger scale models and varied domains to further validate its robustness and generalization capabilities?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper introduces a novel method, Soft Preference Optimization (SPO), which is simpler yet competitive in aligning Large Language Models with human preferences. The integration of preference loss with regularization is interesting, providing a balance that maintains the performance while favorably adhering to human preferences. SPO's ability to function without a reward model is an appealing simplification over existing complex RLHF methodologies. Experimental results showcasing performance improvements against methods like DPO are well-demonstrated.\" Weaknesses='The paper lacks comprehensive comparisons with established RLHF methods that could solidify the claimed advantages of the SPO approach. Additionally, the explanation and theoretical derivation of the regularization term could be more detailed, which might help in better understanding the applicability and limitations of the approach. Concerns around potential overfitting or misalignment with out-of-distribution data due to the focus on preference data alone are valid and need further exploration.' Questions=\"How does the regularization term specifically contribute to avoiding overfitting in SPO compared to traditional reward-based alignment? Are there plans to explore SPO's applicability on larger scale models and varied domains to further validate its robustness and generalization capabilities?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. SyntheOcc effectively utilizes 3D semantic MPIs for better geometric control and high-quality image synthesis. 2. The framework demonstrates significant improvements in generating data that supports testing in autonomous systems, addressing a crucial need for high-quality datasets. 3. It incorporates innovative methods like reweighting strategies to handle class imbalance and maintains multi-view and temporal consistency.' Weaknesses='1. The complexity and novelty of 3D semantic multi-plane images might pose challenges in understanding and implementing the approach for broader use. 2. Although SyntheOcc shows improvements, the experiments appear limited to the nuScenes dataset; there could be more validation across diverse datasets. 3. The paper could offer more detailed comparisons with existing methods to better highlight the relative strengths of SyntheOcc.' Questions='1. How well does SyntheOcc perform across different datasets beyond nuScenes, particularly in terms of generalization? 2. Can the framework be adapted or extended for other types of environments or scenarios outside autonomous driving? 3. What are the computational requirements and potential challenges for broad implementation of SyntheOcc?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. SyntheOcc effectively utilizes 3D semantic MPIs for better geometric control and high-quality image synthesis. 2. The framework demonstrates significant improvements in generating data that supports testing in autonomous systems, addressing a crucial need for high-quality datasets. 3. It incorporates innovative methods like reweighting strategies to handle class imbalance and maintains multi-view and temporal consistency.' Weaknesses='1. The complexity and novelty of 3D semantic multi-plane images might pose challenges in understanding and implementing the approach for broader use. 2. Although SyntheOcc shows improvements, the experiments appear limited to the nuScenes dataset; there could be more validation across diverse datasets. 3. The paper could offer more detailed comparisons with existing methods to better highlight the relative strengths of SyntheOcc.' Questions='1. How well does SyntheOcc perform across different datasets beyond nuScenes, particularly in terms of generalization? 2. Can the framework be adapted or extended for other types of environments or scenarios outside autonomous driving? 3. What are the computational requirements and potential challenges for broad implementation of SyntheOcc?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The integration of generative diffusion models with active learning is innovative and has potential to significantly reduce data acquisition needs in CT imaging. The method demonstrated substantial improvements in reconstruction quality and reduced X-ray exposure.' Weaknesses=\"The methods' application to high-stakes scenarios, like medical imaging, requires careful examination due to potential limitations and trade-offs. The computational complexity involved in using generative diffusion models can also be a concern.\" Questions='How does the method ensure robustness and reliability in medical imaging applications where errors can have serious consequences?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The integration of generative diffusion models with active learning is innovative and has potential to significantly reduce data acquisition needs in CT imaging. The method demonstrated substantial improvements in reconstruction quality and reduced X-ray exposure.' Weaknesses=\"The methods' application to high-stakes scenarios, like medical imaging, requires careful examination due to potential limitations and trade-offs. The computational complexity involved in using generative diffusion models can also be a concern.\" Questions='How does the method ensure robustness and reliability in medical imaging applications where errors can have serious consequences?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"1. The novelty of adding composable prompts for semantic alignment and instance determination greatly enhances SAM's segmentation capability in open-vocabulary scenarios.\\n2. The approach addresses the critical issue of over-segmentation in vision foundation models, thereby creating a bridge for more refined results without significantly increasing computational complexity.\\n3. The experiments conducted demonstrate the method's performance across diverse segmentation tasks on widely recognized datasets, ensuring the methodology's applicability and robustness.\" Weaknesses=\"1. The reliance on dataset-specific training for the method's success might limit its generalizability across a wide array of domains, especially when the context differs significantly from the datasets it was tested on.\\n2. There’s a lack of comparison with existing methods that also adapt SAM for semantic tasks, which could better contextualize the advantage of the proposed method over similar approaches.\\n3. The method focuses primarily on merging resulting patches, which could limit its effectiveness in scenarios that require dynamic splitting of segmentations for detailed results.\" Questions='1. Could the authors elaborate on how SAM-CP maintains its performance and efficiency in domains drastically different from the datasets used in training, such as medical imaging or more detailed 3D modeling?\\n2. Is there a potential for extending this approach to accommodate dynamic splitting strategies, beyond the current focus on merging, to further enhance segmentation precision?\\n3. How does the proposed method compare in computational efficiency and precision with other segmentation approaches that utilize SAM as a base? Would it still be feasible in real-time applications?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"1. The novelty of adding composable prompts for semantic alignment and instance determination greatly enhances SAM's segmentation capability in open-vocabulary scenarios.\\n2. The approach addresses the critical issue of over-segmentation in vision foundation models, thereby creating a bridge for more refined results without significantly increasing computational complexity.\\n3. The experiments conducted demonstrate the method's performance across diverse segmentation tasks on widely recognized datasets, ensuring the methodology's applicability and robustness.\" Weaknesses=\"1. The reliance on dataset-specific training for the method's success might limit its generalizability across a wide array of domains, especially when the context differs significantly from the datasets it was tested on.\\n2. There’s a lack of comparison with existing methods that also adapt SAM for semantic tasks, which could better contextualize the advantage of the proposed method over similar approaches.\\n3. The method focuses primarily on merging resulting patches, which could limit its effectiveness in scenarios that require dynamic splitting of segmentations for detailed results.\" Questions='1. Could the authors elaborate on how SAM-CP maintains its performance and efficiency in domains drastically different from the datasets used in training, such as medical imaging or more detailed 3D modeling?\\n2. Is there a potential for extending this approach to accommodate dynamic splitting strategies, beyond the current focus on merging, to further enhance segmentation precision?\\n3. How does the proposed method compare in computational efficiency and precision with other segmentation approaches that utilize SAM as a base? Would it still be feasible in real-time applications?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='1. The L-C4 model innovatively combines language inputs with advanced video processing techniques to deliver semantic accuracy and consistency in video colorization.\\n2. Addresses significant limitations of previous methods by ensuring temporally consistent and user-controlled colorization.\\n3. The paper is well-organized and clearly explains the methodologies used, providing a comprehensive evaluation on several benchmarks.' Weaknesses=\"1. The method relies heavily on the quality and appropriateness of user-provided text descriptions, which may limit its applicability if such inputs are ambiguous or poorly constructed.\\n2. The implementation details regarding the training dataset and how it influences the model's understanding of aesthetic quality are not fully elaborated.\" Questions='1. How does the model handle ambiguities or contradictions in the text descriptions provided by users?\\n2. Would the system require retraining if applied to different video domains, such as animations or different cultural contexts, where color interpretations might differ?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='1. The L-C4 model innovatively combines language inputs with advanced video processing techniques to deliver semantic accuracy and consistency in video colorization.\\n2. Addresses significant limitations of previous methods by ensuring temporally consistent and user-controlled colorization.\\n3. The paper is well-organized and clearly explains the methodologies used, providing a comprehensive evaluation on several benchmarks.' Weaknesses=\"1. The method relies heavily on the quality and appropriateness of user-provided text descriptions, which may limit its applicability if such inputs are ambiguous or poorly constructed.\\n2. The implementation details regarding the training dataset and how it influences the model's understanding of aesthetic quality are not fully elaborated.\" Questions='1. How does the model handle ambiguities or contradictions in the text descriptions provided by users?\\n2. Would the system require retraining if applied to different video domains, such as animations or different cultural contexts, where color interpretations might differ?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper addresses a significant and timely issue in the field of lifelong learning combined with machine unlearning, which is crucial for privacy and regulatory compliance. 2. The proposed method, PALL, provides a novel approach by optimizing sparse subnetworks and allows for exact task unlearning while preventing catastrophic forgetting, which is a notable contribution. 3. The method demonstrates scalability and state-of-the-art performance in experiments, showing its practical applicability and effectiveness.' Weaknesses='1. The explanation of the method could be more clear, particularly regarding the handling of subnetworks and the specific mechanisms used for knowledge transfer and unlearning. 2. There is a lack of detailed comparison with potential baseline methods that involve differential privacy or approximate unlearning techniques. 3. The paper does not sufficiently address the practical challenges and limitations of implementing the proposed method in real-world scenarios, where tasks may not be clearly delineated.' Questions=\"1. How does the method ensure that the subnetworks used for unlearning do not inadvertently affect other learned tasks, and what are the implications for long-term model stability? 2. Can the approach be extended or adapted to handle non-task-based unlearning requests, where a user's data might span multiple tasks or contexts? 3. How was the episodic memory mechanism designed, and how does it impact the overall scalability and efficiency of the framework?\"\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper addresses a significant and timely issue in the field of lifelong learning combined with machine unlearning, which is crucial for privacy and regulatory compliance. 2. The proposed method, PALL, provides a novel approach by optimizing sparse subnetworks and allows for exact task unlearning while preventing catastrophic forgetting, which is a notable contribution. 3. The method demonstrates scalability and state-of-the-art performance in experiments, showing its practical applicability and effectiveness.' Weaknesses='1. The explanation of the method could be more clear, particularly regarding the handling of subnetworks and the specific mechanisms used for knowledge transfer and unlearning. 2. There is a lack of detailed comparison with potential baseline methods that involve differential privacy or approximate unlearning techniques. 3. The paper does not sufficiently address the practical challenges and limitations of implementing the proposed method in real-world scenarios, where tasks may not be clearly delineated.' Questions=\"1. How does the method ensure that the subnetworks used for unlearning do not inadvertently affect other learned tasks, and what are the implications for long-term model stability? 2. Can the approach be extended or adapted to handle non-task-based unlearning requests, where a user's data might span multiple tasks or contexts? 3. How was the episodic memory mechanism designed, and how does it impact the overall scalability and efficiency of the framework?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The proposed dual attention approach is innovative and significantly enhances geo-localization accuracy by improving feature interaction and representation.\\n2. Introducing a new dataset, G2D, fills a gap in the 'Ground→Drone' geo-localization tasks, providing a valuable resource for further research.\\n3. The method demonstrates strong empirical results, outperforming previous state-of-the-art methods across different datasets.\" Weaknesses='1. The paper could provide more detailed explanations about the novelty and technical depth of the Cross-view and Cross-attention Module (CVCAM) and the Multi-head Spatial Attention Module (MHSAM) beyond standard use of attention mechanisms.\\n2. The sensitivity of performance to particular architectural choices such as the specific sizes of the convolutional kernels in MHSAM is not thoroughly explored.\\n3. The creation and use of the G2D dataset is mentioned, but more detail could be given regarding its collection methodology, diversity, and potential biases.' Questions='1. How does the proposed method handle extreme viewpoint changes or occlusions that are significantly larger than the norm, which are still common challenges in cross-view geo-localization?\\n2. Can the method be adapted or extended to handle other types of challenging scenarios or tasks outside of the current experimental setups?\\n3. What are the main limitations observed when using the new G2D dataset, and are there any plans to address them in future work?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The proposed dual attention approach is innovative and significantly enhances geo-localization accuracy by improving feature interaction and representation.\\n2. Introducing a new dataset, G2D, fills a gap in the 'Ground→Drone' geo-localization tasks, providing a valuable resource for further research.\\n3. The method demonstrates strong empirical results, outperforming previous state-of-the-art methods across different datasets.\" Weaknesses='1. The paper could provide more detailed explanations about the novelty and technical depth of the Cross-view and Cross-attention Module (CVCAM) and the Multi-head Spatial Attention Module (MHSAM) beyond standard use of attention mechanisms.\\n2. The sensitivity of performance to particular architectural choices such as the specific sizes of the convolutional kernels in MHSAM is not thoroughly explored.\\n3. The creation and use of the G2D dataset is mentioned, but more detail could be given regarding its collection methodology, diversity, and potential biases.' Questions='1. How does the proposed method handle extreme viewpoint changes or occlusions that are significantly larger than the norm, which are still common challenges in cross-view geo-localization?\\n2. Can the method be adapted or extended to handle other types of challenging scenarios or tasks outside of the current experimental setups?\\n3. What are the main limitations observed when using the new G2D dataset, and are there any plans to address them in future work?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The paper provides a novel theoretical perspective on the adversarial fragility of neural networks using matrix-theoretic analysis.\\n2. The connection established between feature compression and adversarial vulnerability is compelling and aligns with some existing hypotheses, while providing a new understanding of the phenomenon.\\n3. The theoretical framework spans from two-layer to multi-layer networks, showing robustness degradation with increasing input dimension, which is validated by numerical experiments.' Weaknesses='1. The analysis assumes Gaussian distributions, which might limit applicability to real-world datasets that do not follow this assumption.\\n2. The theoretical results, while robust, might have limited immediate application without further exploration into mitigating strategies.\\n3. The paper could benefit from a more extensive discussion on potential implications for improving adversarial robustness beyond identifying the cause of fragility.' Questions='1. How can the insights from this theoretical framework be used to devise strategies to improve adversarial robustness in practical applications?\\n2. Are there plans to extend the analysis to non-Gaussian distributions that might resemble more realistic data scenarios?\\n3. How does the feature compression theory align or conflict with existing methods aimed at improving adversarial robustness, such as adversarial training?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The paper provides a novel theoretical perspective on the adversarial fragility of neural networks using matrix-theoretic analysis.\\n2. The connection established between feature compression and adversarial vulnerability is compelling and aligns with some existing hypotheses, while providing a new understanding of the phenomenon.\\n3. The theoretical framework spans from two-layer to multi-layer networks, showing robustness degradation with increasing input dimension, which is validated by numerical experiments.' Weaknesses='1. The analysis assumes Gaussian distributions, which might limit applicability to real-world datasets that do not follow this assumption.\\n2. The theoretical results, while robust, might have limited immediate application without further exploration into mitigating strategies.\\n3. The paper could benefit from a more extensive discussion on potential implications for improving adversarial robustness beyond identifying the cause of fragility.' Questions='1. How can the insights from this theoretical framework be used to devise strategies to improve adversarial robustness in practical applications?\\n2. Are there plans to extend the analysis to non-Gaussian distributions that might resemble more realistic data scenarios?\\n3. How does the feature compression theory align or conflict with existing methods aimed at improving adversarial robustness, such as adversarial training?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"1. The approach addresses a critical issue in distributed computing environments where execution order can vary, offering a practical solution for improving model robustness.\\n2. The method proposed, LayerShuffle, demonstrates potential for broader applicability in handling other irregularities in model execution, such as node failures.\\n3. The experimental setup using datasets like ImageNet2012 and CIFAR-100 is robust and demonstrates the method's validity across different scenarios.\" Weaknesses='1. The accuracy loss in regular execution orders is a notable drawback, and the trade-off might limit the adoption of this approach in practice.\\n2. The evaluation is primarily focused on ViT/DeiT models, lacking diversity across other Transformer models such as Swin or EfficientVit that might provide further insights.\\n3. Some figures and experimental comparisons, like in Figure 1c, could be misleading as they do not account for baseline limitations such as lack of training with shuffled layer orders.' Questions='1. How does LayerShuffle perform on more diverse types of Transformer models beyond ViT/DeiT?\\n2. Can the method be adapted or optimized further to reduce the accuracy gap for regular execution orders?\\n3. Are there plans to test LayerShuffle in real-world distributed environments to further verify its efficacy under varied execution conditions?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"1. The approach addresses a critical issue in distributed computing environments where execution order can vary, offering a practical solution for improving model robustness.\\n2. The method proposed, LayerShuffle, demonstrates potential for broader applicability in handling other irregularities in model execution, such as node failures.\\n3. The experimental setup using datasets like ImageNet2012 and CIFAR-100 is robust and demonstrates the method's validity across different scenarios.\" Weaknesses='1. The accuracy loss in regular execution orders is a notable drawback, and the trade-off might limit the adoption of this approach in practice.\\n2. The evaluation is primarily focused on ViT/DeiT models, lacking diversity across other Transformer models such as Swin or EfficientVit that might provide further insights.\\n3. Some figures and experimental comparisons, like in Figure 1c, could be misleading as they do not account for baseline limitations such as lack of training with shuffled layer orders.' Questions='1. How does LayerShuffle perform on more diverse types of Transformer models beyond ViT/DeiT?\\n2. Can the method be adapted or optimized further to reduce the accuracy gap for regular execution orders?\\n3. Are there plans to test LayerShuffle in real-world distributed environments to further verify its efficacy under varied execution conditions?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper presents a practical method `FairDropout` to address the issue of poor generalization in deep learning models due to spurious correlations, without requiring group labels.\\n2. It shows promising results across various domains, including vision, language, and healthcare, on popular benchmark datasets.\\n3. The method is scalable and successfully applies to large networks like ResNet-50 and BERT.' Weaknesses='1. The assumptions about neuron roles and the focus on neuron localization for memorization might limit the general applicability or efficiency of FairDropout, requiring further empirical and theoretical investigation.\\n2. There is a lack of discussion on the potential beneficial aspects of memorization, which seems to be dismissed outright without thorough analysis.\\n3. The implementation specifics for `FairDropout` are not detailed thoroughly, which could impede reproducibility and wider adoption.' Questions='1. How robust is FairDropout to variations in training configurations or architectures not covered in the provided experiments?\\n2. What are the potential drawbacks or failure modes when applying FairDropout on datasets where spurious correlations are subtler and less pronounced?\\n3. Can the approach be adapted or extended to handle scenarios where some level of memorization might be beneficial?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper presents a practical method `FairDropout` to address the issue of poor generalization in deep learning models due to spurious correlations, without requiring group labels.\\n2. It shows promising results across various domains, including vision, language, and healthcare, on popular benchmark datasets.\\n3. The method is scalable and successfully applies to large networks like ResNet-50 and BERT.' Weaknesses='1. The assumptions about neuron roles and the focus on neuron localization for memorization might limit the general applicability or efficiency of FairDropout, requiring further empirical and theoretical investigation.\\n2. There is a lack of discussion on the potential beneficial aspects of memorization, which seems to be dismissed outright without thorough analysis.\\n3. The implementation specifics for `FairDropout` are not detailed thoroughly, which could impede reproducibility and wider adoption.' Questions='1. How robust is FairDropout to variations in training configurations or architectures not covered in the provided experiments?\\n2. What are the potential drawbacks or failure modes when applying FairDropout on datasets where spurious correlations are subtler and less pronounced?\\n3. Can the approach be adapted or extended to handle scenarios where some level of memorization might be beneficial?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The proposed approach is innovative in leveraging large-scale in-the-wild data to improve contrastive learning for 3D hand pose estimation.\\n2. The adaptive weighting mechanism effectively enhances the contrastive loss, leading to significant performance gains over state-of-the-art methods.\\n3. The framework successfully demonstrates improvements across multiple datasets, showcasing its robustness and applicability.' Weaknesses='1. The selection process for similar hand poses using PCA-based dimension reduction might not capture all relevant variations in hand poses, potentially affecting the quality of positive pairs.\\n2. The paper lacks a detailed discussion on the computational cost and scalability of the proposed method when applied to very large datasets.\\n3. Limited comparison with other recent advanced methods for 3D hand pose estimation could impact the assessment of its relative performance.' Questions='1. How does the proposed method perform in scenarios with severe occlusions or complex backgrounds?\\n2. Can the adaptive weighting mechanism be applied to other domains of contrastive learning beyond 3D hand pose estimation?\\n3. What is the potential impact of extending this approach to more diverse datasets, and how might it affect the generalization capabilities of the model?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The proposed approach is innovative in leveraging large-scale in-the-wild data to improve contrastive learning for 3D hand pose estimation.\\n2. The adaptive weighting mechanism effectively enhances the contrastive loss, leading to significant performance gains over state-of-the-art methods.\\n3. The framework successfully demonstrates improvements across multiple datasets, showcasing its robustness and applicability.' Weaknesses='1. The selection process for similar hand poses using PCA-based dimension reduction might not capture all relevant variations in hand poses, potentially affecting the quality of positive pairs.\\n2. The paper lacks a detailed discussion on the computational cost and scalability of the proposed method when applied to very large datasets.\\n3. Limited comparison with other recent advanced methods for 3D hand pose estimation could impact the assessment of its relative performance.' Questions='1. How does the proposed method perform in scenarios with severe occlusions or complex backgrounds?\\n2. Can the adaptive weighting mechanism be applied to other domains of contrastive learning beyond 3D hand pose estimation?\\n3. What is the potential impact of extending this approach to more diverse datasets, and how might it affect the generalization capabilities of the model?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel self-supervised framework that effectively addresses noise in MRI images without relying on high-SNR labels. The method demonstrates competitive performance compared to both supervised and existing self-supervised approaches, showing potential for practical clinical use.' Weaknesses='The paper could provide more clarity on the technical aspects of the proposed method, particularly in its implementation details and how it compares statistically significant improvements over other methods. Additionally, the novelty of using score-matching in this context may need further justification given similar existing frameworks.' Questions='What steps were taken to ensure that the learned denoising function generalizes well across different MRI machines and field strengths, not just across sample images? How does the detail refinement process specifically counteract the tendency to oversmooth fine features in a quantitative manner?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel self-supervised framework that effectively addresses noise in MRI images without relying on high-SNR labels. The method demonstrates competitive performance compared to both supervised and existing self-supervised approaches, showing potential for practical clinical use.' Weaknesses='The paper could provide more clarity on the technical aspects of the proposed method, particularly in its implementation details and how it compares statistically significant improvements over other methods. Additionally, the novelty of using score-matching in this context may need further justification given similar existing frameworks.' Questions='What steps were taken to ensure that the learned denoising function generalizes well across different MRI machines and field strengths, not just across sample images? How does the detail refinement process specifically counteract the tendency to oversmooth fine features in a quantitative manner?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='1. APG is presented as a plug-and-play method with virtually no additional computational overhead, making it attractive for practical use in various diffusion models.\\n2. The method of down-weighting the parallel component in CFG is novel and interesting, effectively reducing oversaturation and preserving image quality at high guidance scales.' Weaknesses='1. While the concept of orthogonal projection and the implementation of rescaling and momentum are intriguing, the theoretical underpinning could be improved to justify the choice of these mechanisms.\\n2. The evaluation focuses mostly on quantitative measures; an expanded qualitative analysis could further demonstrate the benefits and limitations of APG.' Questions='How generalizable is the APG method across different types of datasets and diffusion model configurations? Do the results hold for radically different image domains?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='1. APG is presented as a plug-and-play method with virtually no additional computational overhead, making it attractive for practical use in various diffusion models.\\n2. The method of down-weighting the parallel component in CFG is novel and interesting, effectively reducing oversaturation and preserving image quality at high guidance scales.' Weaknesses='1. While the concept of orthogonal projection and the implementation of rescaling and momentum are intriguing, the theoretical underpinning could be improved to justify the choice of these mechanisms.\\n2. The evaluation focuses mostly on quantitative measures; an expanded qualitative analysis could further demonstrate the benefits and limitations of APG.' Questions='How generalizable is the APG method across different types of datasets and diffusion model configurations? Do the results hold for radically different image domains?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper provides a theoretical framework for MultiFilterANN, filling a significant gap in existing research. The algorithms presented demonstrate both theoretical and empirical advances in handling large filter regimes with efficient space-time trade-offs. The release of new datasets offers support for future research in this area.' Weaknesses='The theoretical claims need clearer substantiation through empirical results. The paper could improve in clarifying certain technical aspects and providing more detailed comparisons with existing methods. The explanations of how the proposed methods differ from previous ones sometimes lack clarity.' Questions='How do the proposed algorithms compare in terms of computational efficiency compared to other state-of-the-art graph-based NNS methods? Can the framework extend beyond the current filter regime to a broader set of practical applications?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper provides a theoretical framework for MultiFilterANN, filling a significant gap in existing research. The algorithms presented demonstrate both theoretical and empirical advances in handling large filter regimes with efficient space-time trade-offs. The release of new datasets offers support for future research in this area.' Weaknesses='The theoretical claims need clearer substantiation through empirical results. The paper could improve in clarifying certain technical aspects and providing more detailed comparisons with existing methods. The explanations of how the proposed methods differ from previous ones sometimes lack clarity.' Questions='How do the proposed algorithms compare in terms of computational efficiency compared to other state-of-the-art graph-based NNS methods? Can the framework extend beyond the current filter regime to a broader set of practical applications?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper presents an innovative approach to improving the efficiency and accuracy of Physics-informed neural networks (PINNs) when solving PDEs with complex singularities. \\n2. By utilizing flow-matching and diffusion models for sampling, the method offers a potential solution to high-frequency issues inherent in PINNs.\\n3. The proposed methodology is demonstrated to outperform traditional techniques in various scenarios, suggesting its efficacy in handling difficult PDE problems.' Weaknesses='1. The proposed solution might be computationally intensive due to the complex sampling approach and the use of diffusion models, potentially limiting its scalability for very high-dimensional PDE problems.\\n2. The need for specific tuning of the sampling strategy could limit the general applicability of the method across diverse types of PDEs.\\n3. While promising, the results are based on a limited range of PDE scenarios, reducing the certainty of its broad applicability or advantages over existing methods.' Questions=\"1. How does the method's computational efficiency scale with the complexity and dimensionality of the PDEs?\\n2. Could the proposed sampling strategy be effectively integrated with other neural network architectures or numerical solvers beyond PINNs?\\n3. Is the approach robust to changes in the underlying geometry of PDE problems, or does it require careful tuning for each scenario?\"\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper presents an innovative approach to improving the efficiency and accuracy of Physics-informed neural networks (PINNs) when solving PDEs with complex singularities. \\n2. By utilizing flow-matching and diffusion models for sampling, the method offers a potential solution to high-frequency issues inherent in PINNs.\\n3. The proposed methodology is demonstrated to outperform traditional techniques in various scenarios, suggesting its efficacy in handling difficult PDE problems.' Weaknesses='1. The proposed solution might be computationally intensive due to the complex sampling approach and the use of diffusion models, potentially limiting its scalability for very high-dimensional PDE problems.\\n2. The need for specific tuning of the sampling strategy could limit the general applicability of the method across diverse types of PDEs.\\n3. While promising, the results are based on a limited range of PDE scenarios, reducing the certainty of its broad applicability or advantages over existing methods.' Questions=\"1. How does the method's computational efficiency scale with the complexity and dimensionality of the PDEs?\\n2. Could the proposed sampling strategy be effectively integrated with other neural network architectures or numerical solvers beyond PINNs?\\n3. Is the approach robust to changes in the underlying geometry of PDE problems, or does it require careful tuning for each scenario?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper introduces a novel benchmark, MathHay, specifically designed to evaluate long-context mathematical reasoning, a gap in existing assessments.\\n The methodology is comprehensive, simulating realistic and challenging environments through 'haystacks' of documents with mixed relevance.\" Weaknesses='While the benchmark is innovative, the paper lacks a detailed analysis of why current models struggle with long-context mathematical reasoning.\\n The paper could benefit from a broader range of models and an exploration of potential strategies to improve performance.' Questions='Could the types of reasoning questions or document types in MathHay influence LLM performance, and how might this be addressed?\\n How could future models be trained or fine-tuned specifically to improve long-context mathematical reasoning capabilities?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper introduces a novel benchmark, MathHay, specifically designed to evaluate long-context mathematical reasoning, a gap in existing assessments.\\n The methodology is comprehensive, simulating realistic and challenging environments through 'haystacks' of documents with mixed relevance.\" Weaknesses='While the benchmark is innovative, the paper lacks a detailed analysis of why current models struggle with long-context mathematical reasoning.\\n The paper could benefit from a broader range of models and an exploration of potential strategies to improve performance.' Questions='Could the types of reasoning questions or document types in MathHay influence LLM performance, and how might this be addressed?\\n How could future models be trained or fine-tuned specifically to improve long-context mathematical reasoning capabilities?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths='1. Mycroft addresses a pertinent problem in machine learning regarding efficient data acquisition while mitigating privacy concerns, which is increasingly important with the growing importance of data privacy laws.\\n2. The methodology proposed is innovative in its combination of feature space distances and gradient matching to identify useful data subsets, thereby maximizing model performance.\\n3. Demonstrates effectiveness across different domains, indicating its potential broad applicability.' Weaknesses='1. Some algorithmic details such as RetrieveTopK in Algorithm 3 are confusing and lack clarity in explanation, potentially making replication and understanding of the method difficult.\\n2. There is an inconsistency in definitions and proof details in Theorem 3.1, which might affect the confidence in the soundness of the theoretical claims.\\n3. Evaluation on a held-out portion of the dataset is not clearly differentiated from evaluation on the entire dataset, leading to unclear implications about overfitting and generalization.\\n4. Lack of experiments under actual privacy-protecting conditions, which makes it hard to assess the real-world applicability of the method in secure and private data-sharing scenarios.' Questions=\"1. Could the authors clarify the RetrieveTopK function used in Algorithm 3 and how it ensures coverage of important data subsets?\\n2. How do the authors justify the differences between Algorithm 2 in this paper and Algorithm 3 of Elenberg et al. (2016)?\\n3. What is the size of the dataset $D^{hard}$ used in the experiments, and how does it relate to the conclusions drawn?\\n4. Are there plans to evaluate Mycroft in real-world privacy-protected data-sharing environments, and if so, what approaches might be taken to ensure privacy without compromising the method's efficacy?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths='1. Mycroft addresses a pertinent problem in machine learning regarding efficient data acquisition while mitigating privacy concerns, which is increasingly important with the growing importance of data privacy laws.\\n2. The methodology proposed is innovative in its combination of feature space distances and gradient matching to identify useful data subsets, thereby maximizing model performance.\\n3. Demonstrates effectiveness across different domains, indicating its potential broad applicability.' Weaknesses='1. Some algorithmic details such as RetrieveTopK in Algorithm 3 are confusing and lack clarity in explanation, potentially making replication and understanding of the method difficult.\\n2. There is an inconsistency in definitions and proof details in Theorem 3.1, which might affect the confidence in the soundness of the theoretical claims.\\n3. Evaluation on a held-out portion of the dataset is not clearly differentiated from evaluation on the entire dataset, leading to unclear implications about overfitting and generalization.\\n4. Lack of experiments under actual privacy-protecting conditions, which makes it hard to assess the real-world applicability of the method in secure and private data-sharing scenarios.' Questions=\"1. Could the authors clarify the RetrieveTopK function used in Algorithm 3 and how it ensures coverage of important data subsets?\\n2. How do the authors justify the differences between Algorithm 2 in this paper and Algorithm 3 of Elenberg et al. (2016)?\\n3. What is the size of the dataset $D^{hard}$ used in the experiments, and how does it relate to the conclusions drawn?\\n4. Are there plans to evaluate Mycroft in real-world privacy-protected data-sharing environments, and if so, what approaches might be taken to ensure privacy without compromising the method's efficacy?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel approach that marries Bayesian statistics with adversarial learning to tackle the problem of cross-domain recommendations, improving both performance and interpretability. The method is empirically validated across both synthetic and real-world datasets, demonstrating its efficacy and shedding light on domain relationships.' Weaknesses='While the method shows promise, the complexity of integrating adversarial Bayesian frameworks could pose challenges in practical implementation and scalability. Additionally, the reliance on synthetic datasets as part of the evaluation might not accurately reflect the performance on more complex, real-world interactions.' Questions='How does DICF handle highly dynamic scenarios where the domain landscape changes frequently? Are there specific limitations or considerations in terms of computational cost when applying this framework to large-scale, heterogeneous datasets?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel approach that marries Bayesian statistics with adversarial learning to tackle the problem of cross-domain recommendations, improving both performance and interpretability. The method is empirically validated across both synthetic and real-world datasets, demonstrating its efficacy and shedding light on domain relationships.' Weaknesses='While the method shows promise, the complexity of integrating adversarial Bayesian frameworks could pose challenges in practical implementation and scalability. Additionally, the reliance on synthetic datasets as part of the evaluation might not accurately reflect the performance on more complex, real-world interactions.' Questions='How does DICF handle highly dynamic scenarios where the domain landscape changes frequently? Are there specific limitations or considerations in terms of computational cost when applying this framework to large-scale, heterogeneous datasets?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The method effectively enhances LLM reading comprehension by integrating iterative prompting and attention steering, achieving significant improvements on QA tasks without additional training. The attention head selection is generalizable for different tasks and models.' Weaknesses=\"The method's reliance on accurate key sentence identification may limit performance with complex contexts. The scope of evaluation is limited to QA tasks, leaving its applicability to other scenarios unclear.\" Questions='How does SteerPrompt perform on tasks besides open-book QA? Can it be adapted to improve other types of LLM tasks? What are the limitations in terms of context complexity or length?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The method effectively enhances LLM reading comprehension by integrating iterative prompting and attention steering, achieving significant improvements on QA tasks without additional training. The attention head selection is generalizable for different tasks and models.' Weaknesses=\"The method's reliance on accurate key sentence identification may limit performance with complex contexts. The scope of evaluation is limited to QA tasks, leaving its applicability to other scenarios unclear.\" Questions='How does SteerPrompt perform on tasks besides open-book QA? Can it be adapted to improve other types of LLM tasks? What are the limitations in terms of context complexity or length?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=4 Strengths='The paper presents a novel adaptation of the RWKV attention mechanism for vision tasks, demonstrating well-researched methodological improvements over existing Vision Transformers. The VRWKV model shows significant efficiency gains and superior performance in dense prediction tasks, validating the hypothesis through extensive empirical testing.' Weaknesses='While the adaptation of RWKV to vision tasks is innovative, the contribution could be perceived as incremental since it largely leverages existing components from NLP without introducing fundamentally new architectural concepts. Additionally, the complexity of integrating and benefiting from bidirectional global attention and Q-Shift may pose challenges for ease of implementation and adoption.' Questions=\"How does the Q-Shift operation specifically affect the model's ability to generalize across different vision tasks beyond the tested benchmarks? Are there any particular challenges or limitations identified in scaling the VRWKV model to even larger datasets or tasks with higher demands?\"\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=4 Strengths='The paper presents a novel adaptation of the RWKV attention mechanism for vision tasks, demonstrating well-researched methodological improvements over existing Vision Transformers. The VRWKV model shows significant efficiency gains and superior performance in dense prediction tasks, validating the hypothesis through extensive empirical testing.' Weaknesses='While the adaptation of RWKV to vision tasks is innovative, the contribution could be perceived as incremental since it largely leverages existing components from NLP without introducing fundamentally new architectural concepts. Additionally, the complexity of integrating and benefiting from bidirectional global attention and Q-Shift may pose challenges for ease of implementation and adoption.' Questions=\"How does the Q-Shift operation specifically affect the model's ability to generalize across different vision tasks beyond the tested benchmarks? Are there any particular challenges or limitations identified in scaling the VRWKV model to even larger datasets or tasks with higher demands?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The method addresses an important problem of loss spikes encountered in the training of large models with the Adam optimizer.\\n2. S3 demonstrates faster convergence rates and fewer loss spikes, which are critical for efficient model training.\\n3. The proposal of a generalized sign-like formulation with a flexible momentum is novel and well-justified.' Weaknesses='1. While promising, the method might need extensive empirical tuning in practice to achieve optimal performance.\\n2. The impact of the flexible p-th order momentum on different model architectures might not be thoroughly explored across diverse tasks.' Questions='1. How does SignSoftSGD (S3) perform compared to Adam when considering computational resources and time efficiency in practical scenarios?\\n2. Can the proposed optimizer be easily integrated into existing frameworks for real-world applications, especially those involving large, pre-trained models?\\n3. What are the implications of using the same exponential moving average coefficient for both numerator and denominator, and are there other variants tested?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The method addresses an important problem of loss spikes encountered in the training of large models with the Adam optimizer.\\n2. S3 demonstrates faster convergence rates and fewer loss spikes, which are critical for efficient model training.\\n3. The proposal of a generalized sign-like formulation with a flexible momentum is novel and well-justified.' Weaknesses='1. While promising, the method might need extensive empirical tuning in practice to achieve optimal performance.\\n2. The impact of the flexible p-th order momentum on different model architectures might not be thoroughly explored across diverse tasks.' Questions='1. How does SignSoftSGD (S3) perform compared to Adam when considering computational resources and time efficiency in practical scenarios?\\n2. Can the proposed optimizer be easily integrated into existing frameworks for real-world applications, especially those involving large, pre-trained models?\\n3. What are the implications of using the same exponential moving average coefficient for both numerator and denominator, and are there other variants tested?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper effectively addresses key limitations in the compositional zero-shot learning domain by introducing TRIDENT, which leverages MLLM embeddings and attribute smoothing to improve attribute-object disentanglement and model generalization. The approach is innovative, providing state-of-the-art performance across multiple challenging datasets. The paper is well-structured and the experiments are detailed and comprehensive.' Weaknesses='The presentation could be improved for clarity, as certain aspects of the framework, such as the training process and modules, lack detailed explanation. While the method is innovative, the contribution could be more explicitly linked to the existing body of work. There is also some ambiguity in the detailed workings of the different modules and their impact on the final results, which could be clarified further.' Questions='How adaptable is TRIDENT to other compositional zero-shot learning tasks outside of the tested datasets? Is there an analysis of how sensitive the model is to different types of noise in the data?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper effectively addresses key limitations in the compositional zero-shot learning domain by introducing TRIDENT, which leverages MLLM embeddings and attribute smoothing to improve attribute-object disentanglement and model generalization. The approach is innovative, providing state-of-the-art performance across multiple challenging datasets. The paper is well-structured and the experiments are detailed and comprehensive.' Weaknesses='The presentation could be improved for clarity, as certain aspects of the framework, such as the training process and modules, lack detailed explanation. While the method is innovative, the contribution could be more explicitly linked to the existing body of work. There is also some ambiguity in the detailed workings of the different modules and their impact on the final results, which could be clarified further.' Questions='How adaptable is TRIDENT to other compositional zero-shot learning tasks outside of the tested datasets? Is there an analysis of how sensitive the model is to different types of noise in the data?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a novel approach using state-space models (SSMs) for time-varying counterfactual predictions, effectively addressing limitations of traditional models like LSTMs and Transformers in handling long-sequence data. The introduction of the Mamba-CDSP method showcases a meaningful attempt at mitigating confounding biases in a computationally efficient manner.' Weaknesses=\"The paper could improve on providing a detailed theoretical underpinning of the approach, specifically regarding the linear time-complexity mechanism. The reliance on synthetic datasets for evaluation may not fully demonstrate the method's applicability in real-world scenarios. Additionally, the innovation primarily builds on existing state-space model frameworks rather than introducing entirely new methodologies.\" Questions='How does Mamba-CDSP perform in truly real-world applications, and are there any plans to test beyond the current datasets? What specific challenges can arise when transitioning this method from synthetic to practical scenarios?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a novel approach using state-space models (SSMs) for time-varying counterfactual predictions, effectively addressing limitations of traditional models like LSTMs and Transformers in handling long-sequence data. The introduction of the Mamba-CDSP method showcases a meaningful attempt at mitigating confounding biases in a computationally efficient manner.' Weaknesses=\"The paper could improve on providing a detailed theoretical underpinning of the approach, specifically regarding the linear time-complexity mechanism. The reliance on synthetic datasets for evaluation may not fully demonstrate the method's applicability in real-world scenarios. Additionally, the innovation primarily builds on existing state-space model frameworks rather than introducing entirely new methodologies.\" Questions='How does Mamba-CDSP perform in truly real-world applications, and are there any plans to test beyond the current datasets? What specific challenges can arise when transitioning this method from synthetic to practical scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper addresses a crucial challenge in AI alignment, introducing the concept of weak-to-strong deception which highlights potential risks with stronger models under weak supervision. The experimental setup is robust, using a variety of models and datasets to validate the findings. The paper is well-structured and the explanations provided are clear, making complex phenomena accessible. The identification of this deceptive behavior could be pivotal for future research in AI safety.' Weaknesses='While the paper presents strong empirical evidence for weak-to-strong deception, it does not thoroughly explore potential solutions or mitigation strategies in depth. The scenarios discussed, though illustrative, might not fully capture the complexity of real-world applications where such deception could occur. Additionally, the focus on specific datasets and alignment settings could limit the generalization of the findings.' Questions='What are potential methods to mitigate weak-to-strong deception, beyond those partially addressed in the study? Could there be ways to enhance the ability of weak supervisors to detect broader misalignments? How might the findings translate to more complex, real-world settings beyond the experimental setups used?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper addresses a crucial challenge in AI alignment, introducing the concept of weak-to-strong deception which highlights potential risks with stronger models under weak supervision. The experimental setup is robust, using a variety of models and datasets to validate the findings. The paper is well-structured and the explanations provided are clear, making complex phenomena accessible. The identification of this deceptive behavior could be pivotal for future research in AI safety.' Weaknesses='While the paper presents strong empirical evidence for weak-to-strong deception, it does not thoroughly explore potential solutions or mitigation strategies in depth. The scenarios discussed, though illustrative, might not fully capture the complexity of real-world applications where such deception could occur. Additionally, the focus on specific datasets and alignment settings could limit the generalization of the findings.' Questions='What are potential methods to mitigate weak-to-strong deception, beyond those partially addressed in the study? Could there be ways to enhance the ability of weak supervisors to detect broader misalignments? How might the findings translate to more complex, real-world settings beyond the experimental setups used?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper proposes innovative approaches, Residual Autoencoding and Decoupled High-Resolution Adaptation, which significantly enhance autoencoder performance at high spatial compression ratios, up to 128x. This results in improved computational efficiency and image quality, evidenced by better FID scores. The application of these techniques in DC-AE demonstrates clear throughput improvements for high-resolution diffusion models, marking a substantial contribution to efficient image synthesis.' Weaknesses='The paper could provide more insights into the potential limitations or edge cases where the proposed methods might not be as effective. Additionally, the details on the specific implementation aspects and any potential trade-offs, such as how residual connections are balanced to avoid overfitting, could enhance understanding.' Questions='How do the proposed techniques of Residual Autoencoding and Decoupled High-Resolution Adaptation impact the generalization capabilities of the models when applied to datasets not seen during training?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper proposes innovative approaches, Residual Autoencoding and Decoupled High-Resolution Adaptation, which significantly enhance autoencoder performance at high spatial compression ratios, up to 128x. This results in improved computational efficiency and image quality, evidenced by better FID scores. The application of these techniques in DC-AE demonstrates clear throughput improvements for high-resolution diffusion models, marking a substantial contribution to efficient image synthesis.' Weaknesses='The paper could provide more insights into the potential limitations or edge cases where the proposed methods might not be as effective. Additionally, the details on the specific implementation aspects and any potential trade-offs, such as how residual connections are balanced to avoid overfitting, could enhance understanding.' Questions='How do the proposed techniques of Residual Autoencoding and Decoupled High-Resolution Adaptation impact the generalization capabilities of the models when applied to datasets not seen during training?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses the challenging problem of heterogeneous team games, where teammates can have diverse roles, extending the PSRO framework to cater to these complexities.\\n2. The introduction of the Heterogeneous-PSRO framework is a significant contribution to the field, showing improvements over existing methods in reducing exploitability.\\n3. The framework is validated with experiments on both matrix games and complex games like StarCraft and Google Research Football, demonstrating its practical applicability.' Weaknesses=\"1. The method's reliance on sequential optimization could limit its scalability in very large-scale games, potentially hindering performance and practical application in extremely complex environments.\\n2. The adaptation of the framework to continuous action spaces requires further investigation, which might be a limitation for applying the current method in more generalized settings.\" Questions='1. How does the method perform in continuous action spaces, and what modifications would be necessary to extend H-PSRO to handle such environments?\\n2. Are there specific types of heterogeneous roles or team compositions where H-PSRO struggles to achieve similar performance improvements as seen in other settings?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses the challenging problem of heterogeneous team games, where teammates can have diverse roles, extending the PSRO framework to cater to these complexities.\\n2. The introduction of the Heterogeneous-PSRO framework is a significant contribution to the field, showing improvements over existing methods in reducing exploitability.\\n3. The framework is validated with experiments on both matrix games and complex games like StarCraft and Google Research Football, demonstrating its practical applicability.' Weaknesses=\"1. The method's reliance on sequential optimization could limit its scalability in very large-scale games, potentially hindering performance and practical application in extremely complex environments.\\n2. The adaptation of the framework to continuous action spaces requires further investigation, which might be a limitation for applying the current method in more generalized settings.\" Questions='1. How does the method perform in continuous action spaces, and what modifications would be necessary to extend H-PSRO to handle such environments?\\n2. Are there specific types of heterogeneous roles or team compositions where H-PSRO struggles to achieve similar performance improvements as seen in other settings?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"1. The novel approach of SaMoye for zero-shot singing voice conversion is impressive, showing significant improvements over existing models by minimizing timbre leakage through advanced feature disentanglement techniques. \\n2. The model's ability to convert voices to non-human timbres extends its applicability beyond conventional voice conversion tasks, showcasing its flexibility and robustness. \\n3. The extensive dataset used for training enhances the model's generalization capabilities, providing greater validity to the experimental results.\" Weaknesses=\"1. The method's reliance on a large dataset raises concerns regarding accessibility and replicability for researchers with less computational resources. \\n2. While the paper presents a promising method, detailed explanations of the specific algorithms and techniques used in feature disentanglement would provide clearer insights into the methodological advances.\" Questions='1. How does the model perform with more limited datasets, and what trade-offs or degradations in performance might occur in such scenarios?\\n2. Can the approach be effectively adapted or scaled to other forms of voice conversion tasks beyond singing, such as general speech conversion?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"1. The novel approach of SaMoye for zero-shot singing voice conversion is impressive, showing significant improvements over existing models by minimizing timbre leakage through advanced feature disentanglement techniques. \\n2. The model's ability to convert voices to non-human timbres extends its applicability beyond conventional voice conversion tasks, showcasing its flexibility and robustness. \\n3. The extensive dataset used for training enhances the model's generalization capabilities, providing greater validity to the experimental results.\" Weaknesses=\"1. The method's reliance on a large dataset raises concerns regarding accessibility and replicability for researchers with less computational resources. \\n2. While the paper presents a promising method, detailed explanations of the specific algorithms and techniques used in feature disentanglement would provide clearer insights into the methodological advances.\" Questions='1. How does the model perform with more limited datasets, and what trade-offs or degradations in performance might occur in such scenarios?\\n2. Can the approach be effectively adapted or scaled to other forms of voice conversion tasks beyond singing, such as general speech conversion?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The introduction of Differentiable Map Priors (DMP) as a method to incorporate historical spatial priors into autonomous vehicle perception systems represents a novel approach with practical implications. 2. The framework demonstrates significant improvements in 3D object detection and semantic map segmentation, showing versatility across different architectures. 3. The paper provides extensive experimental validation using a well-regarded dataset, nuScenes.' Weaknesses='1. The scalability and memory efficiency claims could be strengthened with more concrete data or analysis on resource usage. 2. While the incorporation of historical data is innovative, there might be challenges in dynamically changing environments which are not thoroughly addressed.' Questions='1. How does the system handle new or rapidly changing environments where historical data may be less relevant or outdated? 2. What are the potential computational trade-offs when integrating DMP into real-time systems? 3. Could you provide more information on how dynamic objects are managed within the framework, especially those not captured in historical data?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The introduction of Differentiable Map Priors (DMP) as a method to incorporate historical spatial priors into autonomous vehicle perception systems represents a novel approach with practical implications. 2. The framework demonstrates significant improvements in 3D object detection and semantic map segmentation, showing versatility across different architectures. 3. The paper provides extensive experimental validation using a well-regarded dataset, nuScenes.' Weaknesses='1. The scalability and memory efficiency claims could be strengthened with more concrete data or analysis on resource usage. 2. While the incorporation of historical data is innovative, there might be challenges in dynamically changing environments which are not thoroughly addressed.' Questions='1. How does the system handle new or rapidly changing environments where historical data may be less relevant or outdated? 2. What are the potential computational trade-offs when integrating DMP into real-time systems? 3. Could you provide more information on how dynamic objects are managed within the framework, especially those not captured in historical data?'\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Content:\n",
      "If you have any corrections or new requests, feel free to let me know!\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths='ProVaccine outperforms existing models and demonstrates robust generalizability, integrating a dual attention mechanism with pre-trained embeddings. It uses a large, comprehensive dataset, and its results on varied pathogens like Helicobacter pylori and SARS-CoV-2 are particularly promising for vaccine development.' Weaknesses=\"The dependence on comprehensive data representation might limit its use in cases where such detailed information isn't available. Further, while the model shows clear improvements over existing methods, the specific impact of each component in the model (e.g., dual attention) isn't detailed thoroughly through ablation studies.\" Questions='How does ProVaccine perform if sequence information or structural embeddings are unavailable or limited? Would part of the model collapse or degrade in performance? Additionally, did the study include any analysis on the computational complexity compared to the baseline methods, and how feasible is ProVaccine for real-time decision-making in vaccine development?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths='ProVaccine outperforms existing models and demonstrates robust generalizability, integrating a dual attention mechanism with pre-trained embeddings. It uses a large, comprehensive dataset, and its results on varied pathogens like Helicobacter pylori and SARS-CoV-2 are particularly promising for vaccine development.' Weaknesses=\"The dependence on comprehensive data representation might limit its use in cases where such detailed information isn't available. Further, while the model shows clear improvements over existing methods, the specific impact of each component in the model (e.g., dual attention) isn't detailed thoroughly through ablation studies.\" Questions='How does ProVaccine perform if sequence information or structural embeddings are unavailable or limited? Would part of the model collapse or degrade in performance? Additionally, did the study include any analysis on the computational complexity compared to the baseline methods, and how feasible is ProVaccine for real-time decision-making in vaccine development?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper presents a novel computational trust-based approach to handling conflicts in multi-view classification tasks, which is an important problem in real-world applications. By evaluating the reliability of predictions from different views and assigning trust weights, the method enhances the decision-making process without assuming perfect view alignment. The experimental results on six real-world datasets demonstrate improved consistency and accuracy over existing methods, validating the proposed approach. The use of subjective logic and Binomial opinion theory adds a solid theoretical foundation to the work.' Weaknesses='The presentation of the paper could be improved to make the methodology and results more accessible. Certain sections, such as the explanation of concepts like subjective logic and the specifics of the evidence network, may be challenging to understand for readers unfamiliar with these areas. Additionally, while the framework is evaluated on multiple datasets, there is limited discussion on how the proposed method performs with varying levels of view conflicts and whether it scales well to datasets with more or less complexity. Furthermore, practical implementation details and hyperparameter settings are not thoroughly discussed, making it harder for practitioners to replicate the results. The introduction could better connect the problem and the proposed solution clearly and concisely.' Questions='1. How does the proposed computational trust-based method handle scenarios where all views are somewhat unreliable, and does it degrade gracefully in such cases?\\n2. Can the authors provide more insights into how the choice of Binomial opinion theory affects the overall framework, and are there alternative theories that could be explored?\\n3. Is there any analysis on the sensitivity of the method to hyperparameter choices, and how do these impact the overall effectiveness of the approach?\\n4. Could the authors elaborate on how the method can be adapted or improved for application to domains with significantly different characteristics from those tested?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper presents a novel computational trust-based approach to handling conflicts in multi-view classification tasks, which is an important problem in real-world applications. By evaluating the reliability of predictions from different views and assigning trust weights, the method enhances the decision-making process without assuming perfect view alignment. The experimental results on six real-world datasets demonstrate improved consistency and accuracy over existing methods, validating the proposed approach. The use of subjective logic and Binomial opinion theory adds a solid theoretical foundation to the work.' Weaknesses='The presentation of the paper could be improved to make the methodology and results more accessible. Certain sections, such as the explanation of concepts like subjective logic and the specifics of the evidence network, may be challenging to understand for readers unfamiliar with these areas. Additionally, while the framework is evaluated on multiple datasets, there is limited discussion on how the proposed method performs with varying levels of view conflicts and whether it scales well to datasets with more or less complexity. Furthermore, practical implementation details and hyperparameter settings are not thoroughly discussed, making it harder for practitioners to replicate the results. The introduction could better connect the problem and the proposed solution clearly and concisely.' Questions='1. How does the proposed computational trust-based method handle scenarios where all views are somewhat unreliable, and does it degrade gracefully in such cases?\\n2. Can the authors provide more insights into how the choice of Binomial opinion theory affects the overall framework, and are there alternative theories that could be explored?\\n3. Is there any analysis on the sensitivity of the method to hyperparameter choices, and how do these impact the overall effectiveness of the approach?\\n4. Could the authors elaborate on how the method can be adapted or improved for application to domains with significantly different characteristics from those tested?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel metric (Wasserstein distance) and method (Sparse Expansion) to tackle the problem of neuronal entanglement, offering a promising direction for improving sparsification techniques in LLMs. The approach is well-motivated and the experiments appear to demonstrate the benefit of disentangling neurons to retain model performance.' Weaknesses=\"The method's reliance on the Wasserstein distance assumes the output distribution should be Gaussian, which may not universally apply across different model architectures or tasks. The practical implementation details of Sparse Expansion, such as its scalability to different model sizes and inference time costs, are not thoroughly addressed. More clarity on the experimental setups would enhance reproducibility.\" Questions='1. How does the assumption of Gaussian distribution in neuron output hold across different types of neural networks?\\n2. What are the computational overheads associated with implementing Sparse Expansion, particularly in real-time applications?\\n3. How does the method generalize to different types of neural networks beyond LLMs?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel metric (Wasserstein distance) and method (Sparse Expansion) to tackle the problem of neuronal entanglement, offering a promising direction for improving sparsification techniques in LLMs. The approach is well-motivated and the experiments appear to demonstrate the benefit of disentangling neurons to retain model performance.' Weaknesses=\"The method's reliance on the Wasserstein distance assumes the output distribution should be Gaussian, which may not universally apply across different model architectures or tasks. The practical implementation details of Sparse Expansion, such as its scalability to different model sizes and inference time costs, are not thoroughly addressed. More clarity on the experimental setups would enhance reproducibility.\" Questions='1. How does the assumption of Gaussian distribution in neuron output hold across different types of neural networks?\\n2. What are the computational overheads associated with implementing Sparse Expansion, particularly in real-time applications?\\n3. How does the method generalize to different types of neural networks beyond LLMs?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='1. The paper introduces an innovative use of quantum computations to enhance parameter efficiency in fine-tuning large models, which is a novel approach not commonly explored in current literature.\\n2. Quantum-PEFT shows significant reductions in parameters required for fine-tuning while maintaining competitive performance, suggesting practical applicability.\\n3. The method is well-validated through experiments on standard benchmarks, demonstrating its effectiveness.' Weaknesses='1. The integration of quantum parameterizations raises concerns about the complexity and computational requirements for practical deployment, which are not deeply explored in the paper.\\n2. The potential limitations or challenges in adopting quantum-inspired methods in real-world scenarios are not fully addressed, such as scalability issues with larger models or higher-dimensional data.\\n3. Additional comparisons with a broader range of existing parameter-efficient fine-tuning methods would strengthen the validity of conclusions.' Questions='1. How does the computational overhead of implementing Quantum-PEFT compare to traditional methods like LoRA in practice?\\n2. Are there specific types of models or domains where Quantum-PEFT may face limitations or require adaptations?\\n3. What are the implications of quantum-inspired methods for future research directions in parameter efficiency and fine-tuning?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='1. The paper introduces an innovative use of quantum computations to enhance parameter efficiency in fine-tuning large models, which is a novel approach not commonly explored in current literature.\\n2. Quantum-PEFT shows significant reductions in parameters required for fine-tuning while maintaining competitive performance, suggesting practical applicability.\\n3. The method is well-validated through experiments on standard benchmarks, demonstrating its effectiveness.' Weaknesses='1. The integration of quantum parameterizations raises concerns about the complexity and computational requirements for practical deployment, which are not deeply explored in the paper.\\n2. The potential limitations or challenges in adopting quantum-inspired methods in real-world scenarios are not fully addressed, such as scalability issues with larger models or higher-dimensional data.\\n3. Additional comparisons with a broader range of existing parameter-efficient fine-tuning methods would strengthen the validity of conclusions.' Questions='1. How does the computational overhead of implementing Quantum-PEFT compare to traditional methods like LoRA in practice?\\n2. Are there specific types of models or domains where Quantum-PEFT may face limitations or require adaptations?\\n3. What are the implications of quantum-inspired methods for future research directions in parameter efficiency and fine-tuning?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The introduction of BILBO addresses significant limitations in sample efficiency and practicality of existing bilevel optimization methods.\\n2. Provides theoretical guarantees with regret bounds, elevating the trust in the method’s effectiveness.\\n3. Demonstrates robust real-world applicability with well-conducted experiments in diverse settings.\\n4. The integration of Bayesian optimization with bilevel frameworks is innovative and showcases substantial improvements over baselines.' Weaknesses=\"1. The paper could provide more comprehensive comparisons with a broader range of existing bilevel optimization techniques.\\n2. Real-world applicability is demonstrated, but there might be a gap in discussing limitations or specific settings where BILBO may not perform as effectively.\\n3. While the experimental results are strong, additional baselines would bolster the claims of BILBO's superiority.\" Questions='1. How does BILBO compare in performance with recent state-of-the-art bilevel optimization methods outside the baselines considered?\\n2. Were there any challenges faced during the implementation of BILBO that could inform potential users about its limitations or required adjustments?\\n3. Could the approach be extended safely to more complex and dynamic bilevel environments, and what would be the potential hazards of such extensions?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The introduction of BILBO addresses significant limitations in sample efficiency and practicality of existing bilevel optimization methods.\\n2. Provides theoretical guarantees with regret bounds, elevating the trust in the method’s effectiveness.\\n3. Demonstrates robust real-world applicability with well-conducted experiments in diverse settings.\\n4. The integration of Bayesian optimization with bilevel frameworks is innovative and showcases substantial improvements over baselines.' Weaknesses=\"1. The paper could provide more comprehensive comparisons with a broader range of existing bilevel optimization techniques.\\n2. Real-world applicability is demonstrated, but there might be a gap in discussing limitations or specific settings where BILBO may not perform as effectively.\\n3. While the experimental results are strong, additional baselines would bolster the claims of BILBO's superiority.\" Questions='1. How does BILBO compare in performance with recent state-of-the-art bilevel optimization methods outside the baselines considered?\\n2. Were there any challenges faced during the implementation of BILBO that could inform potential users about its limitations or required adjustments?\\n3. Could the approach be extended safely to more complex and dynamic bilevel environments, and what would be the potential hazards of such extensions?'\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Content:\n",
      "Please note that there seems to be an error in the completion—some fields were not correctly detailed according to the given format.\n",
      "\n",
      "If you'd like me to address a specific issue, clarify or refine any particular entry, I would certainly assist in improving the output quality to align with specified requirements or guidelines.\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=8 Confidence=3 Strengths='The paper clearly identifies the gap in current robotics research regarding data scaling, which is well established in other fields like NLP and CV. It addresses this gap by effectively demonstrating that diversity of training environments and objects, rather than sheer number, enhances generalization in robotic manipulation. The use of real-world data and a robust evaluation protocol strengthens the validity of the findings. The study provides a clear strategy for data collection that could substantially improve zero-shot performance in novel environments.' Weaknesses=\"The study is limited to relatively simple tasks and could benefit from investigating a wider variety of more challenging tasks to assess scalability of the methods proposed. While increasing object diversity generally improves policy generalization, the study does not fully explore or explain cases where this isn't the case, hinting at possible unexplored variables. The lack of comprehensive standard deviations in the plots limits the assessment of empirical significance of the reported results.\" Questions='1. Why does increasing the number of objects per environment sometimes decrease task performance, as seen in some plots? Could there be unknown variables affecting this outcome?\\n2. What methodology or calculations are used to derive the correlation coefficient r in the results discussion?\\n3. Are there plans or possibilities to extend this research to more complex and varied task domains, to further validate the scalability of the proposed data scaling law strategy?\\n4. How does the chosen task impact the generalization efficiency? Would the power-law relationship hold consistently across vastly different tasks or is task specificity a potential confounding factor?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=8 Confidence=3 Strengths='The paper clearly identifies the gap in current robotics research regarding data scaling, which is well established in other fields like NLP and CV. It addresses this gap by effectively demonstrating that diversity of training environments and objects, rather than sheer number, enhances generalization in robotic manipulation. The use of real-world data and a robust evaluation protocol strengthens the validity of the findings. The study provides a clear strategy for data collection that could substantially improve zero-shot performance in novel environments.' Weaknesses=\"The study is limited to relatively simple tasks and could benefit from investigating a wider variety of more challenging tasks to assess scalability of the methods proposed. While increasing object diversity generally improves policy generalization, the study does not fully explore or explain cases where this isn't the case, hinting at possible unexplored variables. The lack of comprehensive standard deviations in the plots limits the assessment of empirical significance of the reported results.\" Questions='1. Why does increasing the number of objects per environment sometimes decrease task performance, as seen in some plots? Could there be unknown variables affecting this outcome?\\n2. What methodology or calculations are used to derive the correlation coefficient r in the results discussion?\\n3. Are there plans or possibilities to extend this research to more complex and varied task domains, to further validate the scalability of the proposed data scaling law strategy?\\n4. How does the chosen task impact the generalization efficiency? Would the power-law relationship hold consistently across vastly different tasks or is task specificity a potential confounding factor?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The proposed Gödel Agent framework offers a novel approach to AI systems by enabling them to self-evolve and adapt to a variety of tasks with minimal human intervention. \\n2. Experimental results demonstrate significant performance gains over traditional agent systems across diverse tasks, showcasing the potential of recursive self-improvement in AI agents. \\n3. The framework introduces innovative techniques, such as runtime code modification via monkey patching, which could provide new insights into agent design.' Weaknesses=\"1. The concept of self-evolving agents, while intriguing, may face practical challenges, such as ensuring stability and safety during runtime modifications. \\n2. Comprehensive details on the experimental setup and baseline comparisons might be lacking, raising questions about the fairness of the evaluation. \\n3. The framework's reliance on advanced LLMs like GPT-4 introduces complexities that may not be easily generalizable across other AI environments or tasks.\" Questions='1. How does the Gödel Agent handle scenarios where runtime modifications could lead to erroneous or unsafe behaviors in real-world applications?\\n2. Can the framework extend beyond the evaluated tasks of coding, science, and math? How adaptable is it to diverse AI use-cases?\\n3. What measures were taken to ensure a fair comparison between the Gödel Agent and traditional systems, given the disparities in resources like LLM access?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The proposed Gödel Agent framework offers a novel approach to AI systems by enabling them to self-evolve and adapt to a variety of tasks with minimal human intervention. \\n2. Experimental results demonstrate significant performance gains over traditional agent systems across diverse tasks, showcasing the potential of recursive self-improvement in AI agents. \\n3. The framework introduces innovative techniques, such as runtime code modification via monkey patching, which could provide new insights into agent design.' Weaknesses=\"1. The concept of self-evolving agents, while intriguing, may face practical challenges, such as ensuring stability and safety during runtime modifications. \\n2. Comprehensive details on the experimental setup and baseline comparisons might be lacking, raising questions about the fairness of the evaluation. \\n3. The framework's reliance on advanced LLMs like GPT-4 introduces complexities that may not be easily generalizable across other AI environments or tasks.\" Questions='1. How does the Gödel Agent handle scenarios where runtime modifications could lead to erroneous or unsafe behaviors in real-world applications?\\n2. Can the framework extend beyond the evaluated tasks of coding, science, and math? How adaptable is it to diverse AI use-cases?\\n3. What measures were taken to ensure a fair comparison between the Gödel Agent and traditional systems, given the disparities in resources like LLM access?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths=\"1. The introduction of the U-MATH and µ-MATH benchmarks addresses a significant gap in evaluating LLMs on complex and advanced mathematical problems, particularly at the university level and including visual tasks.\\n2. The benchmark's comprehensiveness, covering six core subjects with 20% visual components, offers a diverse and robust evaluation framework.\\n3. The study provides valuable insights into the current limitations of LLMs in handling advanced mathematical reasoning and evaluation tasks, highlighting areas for future improvement.\" Weaknesses='1. Current LLMs show limited performance on the proposed benchmarks, with low accuracy scores for both text-based and visual tasks, reflecting the challenges but also indicating the need for further development in model capabilities.\\n2. While the benchmark is comprehensive, integrating more interdisciplinary problems might provide a broader assessment of LLM capabilities in diverse real-world scenarios.' Questions='1. Can the U-MATH benchmark be adapted or expanded to include interdisciplinary problems that blend mathematical reasoning with other fields?\\n2. What are the future plans to enhance LLM capabilities based on the findings from the U-MATH and µ-MATH evaluations?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths=\"1. The introduction of the U-MATH and µ-MATH benchmarks addresses a significant gap in evaluating LLMs on complex and advanced mathematical problems, particularly at the university level and including visual tasks.\\n2. The benchmark's comprehensiveness, covering six core subjects with 20% visual components, offers a diverse and robust evaluation framework.\\n3. The study provides valuable insights into the current limitations of LLMs in handling advanced mathematical reasoning and evaluation tasks, highlighting areas for future improvement.\" Weaknesses='1. Current LLMs show limited performance on the proposed benchmarks, with low accuracy scores for both text-based and visual tasks, reflecting the challenges but also indicating the need for further development in model capabilities.\\n2. While the benchmark is comprehensive, integrating more interdisciplinary problems might provide a broader assessment of LLM capabilities in diverse real-world scenarios.' Questions='1. Can the U-MATH benchmark be adapted or expanded to include interdisciplinary problems that blend mathematical reasoning with other fields?\\n2. What are the future plans to enhance LLM capabilities based on the findings from the U-MATH and µ-MATH evaluations?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=3 Strengths=\"The introduction of GSM-Symbolic provides a fresh benchmarking perspective that highlights the weaknesses in LLMs' reasoning capabilities. The paper offers a thorough empirical analysis of current LLMs, showcasing their sensitivity to structural and content variations in reasoning tasks. The approach to systematically generate diverse question variants is a strong contribution to ensuring data robustness and reducing contamination risks.\" Weaknesses=\"While the methodology introduces a new benchmark to test reasoning, it primarily reaffirms existing knowledge about LLMs' limitations without proposing solutions. The paper lacks depth in exploring potential avenues for overcoming the identified reasoning gaps. Also, the variation in LLM performance is largely attributed to intrinsic limitations rather than any architectural improvements or innovations proposed in the study.\" Questions='How scalable is the GSM-Symbolic approach to other domains beyond math reasoning? What specific steps can be taken to address the fragility in reasoning detected by GSM-NoOp? Can the benchmark adapt to evaluate reasoning in nuanced, real-world scenarios?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=3 Strengths=\"The introduction of GSM-Symbolic provides a fresh benchmarking perspective that highlights the weaknesses in LLMs' reasoning capabilities. The paper offers a thorough empirical analysis of current LLMs, showcasing their sensitivity to structural and content variations in reasoning tasks. The approach to systematically generate diverse question variants is a strong contribution to ensuring data robustness and reducing contamination risks.\" Weaknesses=\"While the methodology introduces a new benchmark to test reasoning, it primarily reaffirms existing knowledge about LLMs' limitations without proposing solutions. The paper lacks depth in exploring potential avenues for overcoming the identified reasoning gaps. Also, the variation in LLM performance is largely attributed to intrinsic limitations rather than any architectural improvements or innovations proposed in the study.\" Questions='How scalable is the GSM-Symbolic approach to other domains beyond math reasoning? What specific steps can be taken to address the fragility in reasoning detected by GSM-NoOp? Can the benchmark adapt to evaluate reasoning in nuanced, real-world scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper introduces a novel framework for evaluating diversity in prompt-based generative models by decomposing kernel-based entropy into two separate scores. \\n2. The approach provides valuable insights into distinguishing between prompt-induced and model-induced diversity, which is crucial for assessing generative models more accurately. \\n3. The method is validated across different generative tasks, showing consistency with known benchmarks, indicating its practical applicability.' Weaknesses='1. The paper might not emphasize enough on the practical implications and potential limitations of applying the Conditional-Vendi and Information-Vendi scores to all generative models. \\n2. While promising, the method may require additional validation in real-world scenarios to fully understand its advantages and limitations in diverse applications. \\n3. The explanation of the information-theoretic concepts may be complex for readers unfamiliar with these methods, potentially limiting accessibility.' Questions='1. How do the Conditional-Vendi and Information-Vendi scores compare with existing diversity metrics beyond the experiments discussed in the paper? \\n2. Are there computational constraints or challenges associated with implementing these metrics in large-scale or real-time generative systems? \\n3. Can the approach be generalized or extended to other modalities outside text-to-image or text-to-video generation?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper introduces a novel framework for evaluating diversity in prompt-based generative models by decomposing kernel-based entropy into two separate scores. \\n2. The approach provides valuable insights into distinguishing between prompt-induced and model-induced diversity, which is crucial for assessing generative models more accurately. \\n3. The method is validated across different generative tasks, showing consistency with known benchmarks, indicating its practical applicability.' Weaknesses='1. The paper might not emphasize enough on the practical implications and potential limitations of applying the Conditional-Vendi and Information-Vendi scores to all generative models. \\n2. While promising, the method may require additional validation in real-world scenarios to fully understand its advantages and limitations in diverse applications. \\n3. The explanation of the information-theoretic concepts may be complex for readers unfamiliar with these methods, potentially limiting accessibility.' Questions='1. How do the Conditional-Vendi and Information-Vendi scores compare with existing diversity metrics beyond the experiments discussed in the paper? \\n2. Are there computational constraints or challenges associated with implementing these metrics in large-scale or real-time generative systems? \\n3. Can the approach be generalized or extended to other modalities outside text-to-image or text-to-video generation?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='1. The method proposed, Flow with Interpolant Guidance (FIG), shows significant improvements in solving challenging linear inverse problems, particularly in the presence of high noise or ill-posed conditions, outperforming existing state-of-the-art algorithms.\\n2. The paper provides a theoretical foundation for guiding reverse-time sampling with measurement interpolants, which is novel and effectively bridges the gap between unconditional generation and measurement guidance.\\n3. The proposed approach is efficient in terms of memory and runtime, enhancing its applicability in practical scenarios.' Weaknesses='1. The exposition of FIG and its methodological novelty could be made clearer to highlight its distinguishing features compared to existing methods.\\n2. The reliance on pre-trained models may limit the applicability of FIG in scenarios where such models are not readily available or applicable.\\n3. Some theoretical aspects, although sound, may lack intuitive explanations, which could help in understanding the broader implications of the approach.' Questions='1. How robust is the FIG method when applied to different types of linear inverse problems beyond image reconstruction?\\n2. Can the interpolant guidance mechanism be generalized to other domains or problem settings aside from the ones considered in this paper?\\n3. How sensitive is the proposed approach to the choice of measurement interpolants, and are there guidelines for selecting or designing effective interpolants?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='1. The method proposed, Flow with Interpolant Guidance (FIG), shows significant improvements in solving challenging linear inverse problems, particularly in the presence of high noise or ill-posed conditions, outperforming existing state-of-the-art algorithms.\\n2. The paper provides a theoretical foundation for guiding reverse-time sampling with measurement interpolants, which is novel and effectively bridges the gap between unconditional generation and measurement guidance.\\n3. The proposed approach is efficient in terms of memory and runtime, enhancing its applicability in practical scenarios.' Weaknesses='1. The exposition of FIG and its methodological novelty could be made clearer to highlight its distinguishing features compared to existing methods.\\n2. The reliance on pre-trained models may limit the applicability of FIG in scenarios where such models are not readily available or applicable.\\n3. Some theoretical aspects, although sound, may lack intuitive explanations, which could help in understanding the broader implications of the approach.' Questions='1. How robust is the FIG method when applied to different types of linear inverse problems beyond image reconstruction?\\n2. Can the interpolant guidance mechanism be generalized to other domains or problem settings aside from the ones considered in this paper?\\n3. How sensitive is the proposed approach to the choice of measurement interpolants, and are there guidelines for selecting or designing effective interpolants?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper successfully introduces SecureGS, a novel technique that enhances security and efficiency for 3D Gaussian splatting steganography. It addresses existing limitations in rendering fidelity, computational costs, and security vulnerabilities. The method is rigorously tested and the experiments are extensively conducted, demonstrating significant improvements over existing approaches. The presentation of the work is clear and well-structured, making the proposed approach understandable.' Weaknesses='The paper may have a high complexity level which could potentially limit its practical application or adoption. The real-time computational demands of the proposed strategy may deter those seeking simpler alternatives.' Questions='How does SecureGS balance the trade-off between increased security and computational efficiency in real-time applications? Are there specific scenarios where SecureGS may not be applicable?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper successfully introduces SecureGS, a novel technique that enhances security and efficiency for 3D Gaussian splatting steganography. It addresses existing limitations in rendering fidelity, computational costs, and security vulnerabilities. The method is rigorously tested and the experiments are extensively conducted, demonstrating significant improvements over existing approaches. The presentation of the work is clear and well-structured, making the proposed approach understandable.' Weaknesses='The paper may have a high complexity level which could potentially limit its practical application or adoption. The real-time computational demands of the proposed strategy may deter those seeking simpler alternatives.' Questions='How does SecureGS balance the trade-off between increased security and computational efficiency in real-time applications? Are there specific scenarios where SecureGS may not be applicable?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper introduces a novel hierarchical patch graph network (Hi-Patch) that effectively captures multi-scale features in Irregular Multivariate Time Series (IMTS), addressing a significant challenge in time series analysis. \\n\\n2. Hi-Patch demonstrates superior performance over state-of-the-art models in forecasting and classification tasks, providing empirical evidence of its robustness and efficacy. \\n\\n3. The method is clearly articulated and targets practical issues in IMTS, such as varying sampling intervals and asynchronous observations.' Weaknesses='1. The paper may lack detailed insights into the scalability and computational efficiency of Hi-Patch, especially for large-scale datasets. \\n\\n2. The complexity of the model and its architectural details might be difficult for non-expert readers to fully comprehend, affecting the clarity of presentation. \\n\\n3. Additional experimentation across a broader set of diverse datasets could further validate the generalizability of the proposed approach.' Questions=\"1. How does Hi-Patch handle missing data in the time series, and what is its impact on the model's performance?\\n\\n2. What are the computational costs and resource requirements of the Hi-Patch model compared to existing approaches?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper introduces a novel hierarchical patch graph network (Hi-Patch) that effectively captures multi-scale features in Irregular Multivariate Time Series (IMTS), addressing a significant challenge in time series analysis. \\n\\n2. Hi-Patch demonstrates superior performance over state-of-the-art models in forecasting and classification tasks, providing empirical evidence of its robustness and efficacy. \\n\\n3. The method is clearly articulated and targets practical issues in IMTS, such as varying sampling intervals and asynchronous observations.' Weaknesses='1. The paper may lack detailed insights into the scalability and computational efficiency of Hi-Patch, especially for large-scale datasets. \\n\\n2. The complexity of the model and its architectural details might be difficult for non-expert readers to fully comprehend, affecting the clarity of presentation. \\n\\n3. Additional experimentation across a broader set of diverse datasets could further validate the generalizability of the proposed approach.' Questions=\"1. How does Hi-Patch handle missing data in the time series, and what is its impact on the model's performance?\\n\\n2. What are the computational costs and resource requirements of the Hi-Patch model compared to existing approaches?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses a significant gap in the field of LLM watermarking by focusing on imperceptibility, which has been underexplored yet is crucial for practical deployment.\\n2. The introduction of the Water-Probe algorithm provides a novel way of detecting watermarked LLMs, showing high accuracy in detection.\\n3. The proposed Water-Bag strategy helps improve imperceptibility, adding to the practical value of the research.' Weaknesses=\"1. The paper's approach might be limited to specific types of watermarking schemes, and it's unclear how well it performs against more sophisticated watermarking techniques.\\n2. While the concept of enhancing imperceptibility is vital, the trade-off in detection accuracy with the Water-Bag strategy might limit its applicability in certain high-stakes scenarios.\\n3. The research could benefit from deeper theoretical justification or analysis of why the proposed methods work beyond empirical evidence.\" Questions='1. Can the Water-Probe algorithm be adapted or expanded to handle more complex watermarking approaches that might be less predictable?\\n2. How does the trade-off between imperceptibility and detectability impact real-world use cases, especially where high security is needed?\\n3. Are there any specific LLMs or applications where the proposed methods would not be applicable or effective?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses a significant gap in the field of LLM watermarking by focusing on imperceptibility, which has been underexplored yet is crucial for practical deployment.\\n2. The introduction of the Water-Probe algorithm provides a novel way of detecting watermarked LLMs, showing high accuracy in detection.\\n3. The proposed Water-Bag strategy helps improve imperceptibility, adding to the practical value of the research.' Weaknesses=\"1. The paper's approach might be limited to specific types of watermarking schemes, and it's unclear how well it performs against more sophisticated watermarking techniques.\\n2. While the concept of enhancing imperceptibility is vital, the trade-off in detection accuracy with the Water-Bag strategy might limit its applicability in certain high-stakes scenarios.\\n3. The research could benefit from deeper theoretical justification or analysis of why the proposed methods work beyond empirical evidence.\" Questions='1. Can the Water-Probe algorithm be adapted or expanded to handle more complex watermarking approaches that might be less predictable?\\n2. How does the trade-off between imperceptibility and detectability impact real-world use cases, especially where high security is needed?\\n3. Are there any specific LLMs or applications where the proposed methods would not be applicable or effective?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper proposes a novel formalism to understand and develop DPA losses using symbolic logic, which could significantly aid in the theoretical understanding of DPA methods.\\n2. It provides a structured landscape for DPA loss functions, facilitating the exploration of new variants and enhancing human-AI alignment.\\n3. The presentation of the paper is clear and well-organized, aiding in the comprehension of its complex theoretical contributions.' Weaknesses='1. There is a potential risk of the proposed framework leading to exponential computational complexity, which might limit practical applicability.\\n2. Certain notations and concepts are not explained for readers unfamiliar with DPA literature, which could hinder understanding.\\n3. Some references within the paper are incorrect, which might cause confusion when trying to cross-reference information.' Questions=\"1. Can you clarify the computational cost associated with enumerating all propositional models in the proposed approach?\\n2. What assumptions are made in the use of 'Simplify' in Algorithm 1, considering the NP-hardness of minimizing propositional formulas?\"\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper proposes a novel formalism to understand and develop DPA losses using symbolic logic, which could significantly aid in the theoretical understanding of DPA methods.\\n2. It provides a structured landscape for DPA loss functions, facilitating the exploration of new variants and enhancing human-AI alignment.\\n3. The presentation of the paper is clear and well-organized, aiding in the comprehension of its complex theoretical contributions.' Weaknesses='1. There is a potential risk of the proposed framework leading to exponential computational complexity, which might limit practical applicability.\\n2. Certain notations and concepts are not explained for readers unfamiliar with DPA literature, which could hinder understanding.\\n3. Some references within the paper are incorrect, which might cause confusion when trying to cross-reference information.' Questions=\"1. Can you clarify the computational cost associated with enumerating all propositional models in the proposed approach?\\n2. What assumptions are made in the use of 'Simplify' in Algorithm 1, considering the NP-hardness of minimizing propositional formulas?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. MEMO introduces innovative modules, like the memory-guided temporal module and the emotion-aware audio module, that significantly enhance video quality and synchronization.\\n2. The method demonstrates robust performance across diverse datasets, indicating strong generalization abilities.\\n3. Ablation studies in the paper confirm the importance and effectiveness of the proposed components.' Weaknesses=\"1. The reliance on emotion classification could introduce biases or inaccuracies if the emotion classifier does not perform well.\\n2. The paper lacks detailed computational analysis regarding the model's efficiency, speed, and required resources, which are crucial for practical applications.\" Questions='1. How does the performance of MEMO vary with different emotion classifiers? \\n2. Can the model adapt to languages other than English effectively, or does performance degrade with non-English audio inputs?\\n3. Will the processed dataset used in training be made public for further research and validation?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. MEMO introduces innovative modules, like the memory-guided temporal module and the emotion-aware audio module, that significantly enhance video quality and synchronization.\\n2. The method demonstrates robust performance across diverse datasets, indicating strong generalization abilities.\\n3. Ablation studies in the paper confirm the importance and effectiveness of the proposed components.' Weaknesses=\"1. The reliance on emotion classification could introduce biases or inaccuracies if the emotion classifier does not perform well.\\n2. The paper lacks detailed computational analysis regarding the model's efficiency, speed, and required resources, which are crucial for practical applications.\" Questions='1. How does the performance of MEMO vary with different emotion classifiers? \\n2. Can the model adapt to languages other than English effectively, or does performance degrade with non-English audio inputs?\\n3. Will the processed dataset used in training be made public for further research and validation?'\n",
      "Finished gpt4o on data/test_data/one_shot/test_data_2025_summary_prompts_oneshot.jsonl, saved => results/gpt4o/test_data_2025_summary_prompts_oneshot_gpt4o_results.jsonl\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The study makes a commendable attempt to integrate weight-entanglement with gradient-based NAS, potentially filling a gap between separate research communities. It shows promise in enhancing the performance and efficiency of gradient-based methods in macro-level search spaces. The provision of open-access code also adds to the paper's transparency and reproducibility.\" Weaknesses='The paper might lack some depth in the exploration of how the proposed integration directly outperforms existing methods. There may be a need for clearer delineation of how this integration leads to the claimed benefits. The description of experiments needs to be comprehensive enough to fully support the claims about performance improvements.' Questions='What specific challenges did the authors face when adapting gradient-based methods to weight-entangled spaces, and how were these overcome? Are there particular scenarios or types of neural architecture search problems where this integrated approach demonstrates exceptional performance?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The study makes a commendable attempt to integrate weight-entanglement with gradient-based NAS, potentially filling a gap between separate research communities. It shows promise in enhancing the performance and efficiency of gradient-based methods in macro-level search spaces. The provision of open-access code also adds to the paper's transparency and reproducibility.\" Weaknesses='The paper might lack some depth in the exploration of how the proposed integration directly outperforms existing methods. There may be a need for clearer delineation of how this integration leads to the claimed benefits. The description of experiments needs to be comprehensive enough to fully support the claims about performance improvements.' Questions='What specific challenges did the authors face when adapting gradient-based methods to weight-entangled spaces, and how were these overcome? Are there particular scenarios or types of neural architecture search problems where this integrated approach demonstrates exceptional performance?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=3 Strengths='1. The proposed method employing contrastive loss functions, such as the Bradley-Terry loss, provides a novel approach for extracting the sparse latent function implied by global epistasis, showing improvement over standard MSE-based methods.\\n2. Demonstrated superior performance on benchmark tasks highlights the practical value of the approach.\\n3. Theoretical insights, such as the fitness-epistasis uncertainty principle, add depth to the understanding of the problem.' Weaknesses='1. The abstract does not provide details on the datasets or specific benchmark tasks, which would aid in assessing the robustness of the proposed method.\\n2. There is a limited discussion on the potential limitations or edge cases where the contrastive loss approach might not be effective.' Questions='1. How do the proposed contrastive loss functions generalize across different types of protein engineering tasks or datasets?\\n2. Are there specific scenarios or types of data where the contrastive loss approach underperforms compared to traditional methods?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=3 Strengths='1. The proposed method employing contrastive loss functions, such as the Bradley-Terry loss, provides a novel approach for extracting the sparse latent function implied by global epistasis, showing improvement over standard MSE-based methods.\\n2. Demonstrated superior performance on benchmark tasks highlights the practical value of the approach.\\n3. Theoretical insights, such as the fitness-epistasis uncertainty principle, add depth to the understanding of the problem.' Weaknesses='1. The abstract does not provide details on the datasets or specific benchmark tasks, which would aid in assessing the robustness of the proposed method.\\n2. There is a limited discussion on the potential limitations or edge cases where the contrastive loss approach might not be effective.' Questions='1. How do the proposed contrastive loss functions generalize across different types of protein engineering tasks or datasets?\\n2. Are there specific scenarios or types of data where the contrastive loss approach underperforms compared to traditional methods?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='FEATHER provides a novel approach by introducing a lightweight framework for lifelong test-time adaptation, significantly reducing the number of parameters needed, which is advantageous for fast inference on edge devices. This approach is robust against error accumulation and does not require access to source data at test time.' Weaknesses='The paper does not provide sufficient details on how the compact structure of the adapters maintains the performance level similar to existing methods. While it mentions being orthogonal to existing approaches, there is limited discussion on how FEATHER can be effectively combined with them.' Questions='How does FEATHER manage catastrophic forgetting, especially if the adapters are compact? Is there an inherent trade-off between the lightweight nature of the adapters and the breadth of potential test-time distributions it can adapt to?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='FEATHER provides a novel approach by introducing a lightweight framework for lifelong test-time adaptation, significantly reducing the number of parameters needed, which is advantageous for fast inference on edge devices. This approach is robust against error accumulation and does not require access to source data at test time.' Weaknesses='The paper does not provide sufficient details on how the compact structure of the adapters maintains the performance level similar to existing methods. While it mentions being orthogonal to existing approaches, there is limited discussion on how FEATHER can be effectively combined with them.' Questions='How does FEATHER manage catastrophic forgetting, especially if the adapters are compact? Is there an inherent trade-off between the lightweight nature of the adapters and the breadth of potential test-time distributions it can adapt to?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed approach for feature selection using neural estimation of mutual information is innovative and offers a new perspective on considering feature subsets as ensembles, potentially capturing complex relationships between features and targets.' Weaknesses=\"The paper abstract does not provide detailed examples or empirical results to clearly demonstrate the effectiveness and superiority of the proposed method in comparison to existing methods. Without this information, it's challenging to evaluate the practical applicability and reliability of the new approach.\" Questions='1. How does the proposed method perform on real-world datasets compared to other feature selection methods?\\n2. What specific neural network architecture is used for estimating mutual information? \\n3. Are there any computational efficiency concerns given the ensemble evaluation of feature subsets?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed approach for feature selection using neural estimation of mutual information is innovative and offers a new perspective on considering feature subsets as ensembles, potentially capturing complex relationships between features and targets.' Weaknesses=\"The paper abstract does not provide detailed examples or empirical results to clearly demonstrate the effectiveness and superiority of the proposed method in comparison to existing methods. Without this information, it's challenging to evaluate the practical applicability and reliability of the new approach.\" Questions='1. How does the proposed method perform on real-world datasets compared to other feature selection methods?\\n2. What specific neural network architecture is used for estimating mutual information? \\n3. Are there any computational efficiency concerns given the ensemble evaluation of feature subsets?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The authors introduce a novel perspective to understand margins in contrastive learning through gradient analysis. The paper provides new insights into how margins influence the performance and optimization of contrastive learning frameworks.' Weaknesses='The paper may lack comprehensive theoretical analysis to thoroughly validate the proposed perspective. Additionally, the explanation of findings and theoretical contributions could be more precise and clearer, aiding better understanding.' Questions='How does the introduction of margins specifically affect the decision boundaries in contrastive learning compared to margin-less approaches?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The authors introduce a novel perspective to understand margins in contrastive learning through gradient analysis. The paper provides new insights into how margins influence the performance and optimization of contrastive learning frameworks.' Weaknesses='The paper may lack comprehensive theoretical analysis to thoroughly validate the proposed perspective. Additionally, the explanation of findings and theoretical contributions could be more precise and clearer, aiding better understanding.' Questions='How does the introduction of margins specifically affect the decision boundaries in contrastive learning compared to margin-less approaches?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel approach, conditional adversarial support alignment (CASA), that focuses on minimizing conditional symmetric support divergence, offering a more discriminative feature representation for classification tasks. The proposed theoretical target risk bound provides a solid justification for the approach, enhancing its conceptual soundness. Empirical results show that CASA outperforms state-of-the-art methods across multiple benchmarks under label shift conditions, demonstrating its practical effectiveness.' Weaknesses='While CASA is theoretically well-founded, the paper may lack extensive exploration of its limitations or potential drawbacks in various contexts, such as differing domains or more complex label shifts. Additionally, although the empirical results are promising, the paper could benefit from more diverse experiments, potentially including real-world applications to showcase broader applicability.' Questions='1. How well does CASA handle domains with very different feature spaces or more complex label distribution shifts beyond those tested?\\n2. Can the approach be easily extended or adapted for other machine learning tasks beyond classification, such as regression or clustering?\\n3. How significant is the computational cost of CASA compared to existing methods, and are there any strategies for optimizing it?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel approach, conditional adversarial support alignment (CASA), that focuses on minimizing conditional symmetric support divergence, offering a more discriminative feature representation for classification tasks. The proposed theoretical target risk bound provides a solid justification for the approach, enhancing its conceptual soundness. Empirical results show that CASA outperforms state-of-the-art methods across multiple benchmarks under label shift conditions, demonstrating its practical effectiveness.' Weaknesses='While CASA is theoretically well-founded, the paper may lack extensive exploration of its limitations or potential drawbacks in various contexts, such as differing domains or more complex label shifts. Additionally, although the empirical results are promising, the paper could benefit from more diverse experiments, potentially including real-world applications to showcase broader applicability.' Questions='1. How well does CASA handle domains with very different feature spaces or more complex label distribution shifts beyond those tested?\\n2. Can the approach be easily extended or adapted for other machine learning tasks beyond classification, such as regression or clustering?\\n3. How significant is the computational cost of CASA compared to existing methods, and are there any strategies for optimizing it?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The introduction of the TRACE benchmark addresses a key gap in the evaluation of LLMs regarding their continual learning capabilities, which is underexplored in existing research. The benchmark includes a diverse variety of tasks that challenge current models, providing a means to better assess their performance over time.\\n\\nThe experimenting findings are significant in highlighting the current limitations of LLMs, revealing how task-oriented training can lead to a decline in general capabilities and instruction-following skills. The introduction of the Reasoning-augmented Continual Learning approach adds a novel methodology to combat the issue of catastrophic forgetting.' Weaknesses=\"While the paper presents a novel benchmark, there is a need for additional experimentation and baselines compared to state-of-the-art continual learning methods. This would help underline the effectiveness of the proposed TRACE benchmark and the Reasoning-augmented Continual Learning approach in improving LLMs’ performance.\\n\\nThe paper's presentation could be improved, as it lacks comprehensive discussion on how TRACE can be applied to larger models or different model architectures, which might generalize the findings. Furthermore, explicit comparison with existing benchmarks would strengthen the paper's contributions.\" Questions='Could the authors provide more detailed methodologies for how the Reasoning-augmented Continual Learning approach can be integrated into existing LLM frameworks? Are there particular model architectures or types that benefit more from this approach?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The introduction of the TRACE benchmark addresses a key gap in the evaluation of LLMs regarding their continual learning capabilities, which is underexplored in existing research. The benchmark includes a diverse variety of tasks that challenge current models, providing a means to better assess their performance over time.\\n\\nThe experimenting findings are significant in highlighting the current limitations of LLMs, revealing how task-oriented training can lead to a decline in general capabilities and instruction-following skills. The introduction of the Reasoning-augmented Continual Learning approach adds a novel methodology to combat the issue of catastrophic forgetting.' Weaknesses=\"While the paper presents a novel benchmark, there is a need for additional experimentation and baselines compared to state-of-the-art continual learning methods. This would help underline the effectiveness of the proposed TRACE benchmark and the Reasoning-augmented Continual Learning approach in improving LLMs’ performance.\\n\\nThe paper's presentation could be improved, as it lacks comprehensive discussion on how TRACE can be applied to larger models or different model architectures, which might generalize the findings. Furthermore, explicit comparison with existing benchmarks would strengthen the paper's contributions.\" Questions='Could the authors provide more detailed methodologies for how the Reasoning-augmented Continual Learning approach can be integrated into existing LLM frameworks? Are there particular model architectures or types that benefit more from this approach?'\n",
      "Zero shot review:\n",
      " Soundness=2 Presentation=3 Contribution=2 Rating=4 Confidence=3 Strengths='The paper tackles an important problem of label noise in graph neural networks (GNNs), which is less explored compared to other GNN issues like graph perturbations. The introduction of a probabilistic graphical model framework for addressing this is interesting. The approach, with two versions (PRGNN-v1 and PRGNN-v2), seems to be methodically thought out and potentially beneficial in high noise-rate situations. The idea of leveraging Bayesian networks to generate clean labels is a thoughtful approach to tackle label noise.' Weaknesses=\"The paper's theoretical grounding appears insufficiently robust in terms of soundness, particularly since the claim of utilizing a probabilistic graphical model might not be fully supported by empirical results that definitively showcase its efficiency and efficacy over existing methods. The expressiveness of the presentation could be enhanced, especially in clarifying how the proposed models definitively surpass existing strategies. There is also a potential lack of novelty as the core idea seems to build heavily on existing Bayesian concepts, applied straightforwardly to GNNs without much foundational change. Lastly, there's limited benchmarking against existing state-of-the-art noise-handling methods in graph learning.\" Questions='How does PRGNN specifically handle heterophilous graph scenarios better than existing methods, beyond the utilization of Bayesian networks? What are the specific gains attributed to the probabilistic approach proposed in this study as opposed to simpler noise-handling methods? How scalable is the PRGNN approach in terms of computational resources and time with respect to real-world large-scale graph datasets?'\n",
      "Success: Soundness=2 Presentation=3 Contribution=2 Rating=4 Confidence=3 Strengths='The paper tackles an important problem of label noise in graph neural networks (GNNs), which is less explored compared to other GNN issues like graph perturbations. The introduction of a probabilistic graphical model framework for addressing this is interesting. The approach, with two versions (PRGNN-v1 and PRGNN-v2), seems to be methodically thought out and potentially beneficial in high noise-rate situations. The idea of leveraging Bayesian networks to generate clean labels is a thoughtful approach to tackle label noise.' Weaknesses=\"The paper's theoretical grounding appears insufficiently robust in terms of soundness, particularly since the claim of utilizing a probabilistic graphical model might not be fully supported by empirical results that definitively showcase its efficiency and efficacy over existing methods. The expressiveness of the presentation could be enhanced, especially in clarifying how the proposed models definitively surpass existing strategies. There is also a potential lack of novelty as the core idea seems to build heavily on existing Bayesian concepts, applied straightforwardly to GNNs without much foundational change. Lastly, there's limited benchmarking against existing state-of-the-art noise-handling methods in graph learning.\" Questions='How does PRGNN specifically handle heterophilous graph scenarios better than existing methods, beyond the utilization of Bayesian networks? What are the specific gains attributed to the probabilistic approach proposed in this study as opposed to simpler noise-handling methods? How scalable is the PRGNN approach in terms of computational resources and time with respect to real-world large-scale graph datasets?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths=\"The proposed Utility-based Perturbed Gradient Descent (UPGD) method effectively addresses the issues of catastrophic forgetting and loss of plasticity in continual learning. The combination of gradient updates with perturbations is a novel approach that seems to enhance learning performance in streaming scenarios with non-stationarities. The method is demonstrated to outperform or match the performance of existing methods across various tasks, indicating its robustness and effectiveness. Additionally, the paper includes extended experiments with reinforcement learning, showing UPGD's applicability beyond conventional settings.\" Weaknesses='The paper could benefit from more detailed explanations and theoretical insights into why UPGD outperforms existing methods. The highly technical nature of the paper might make it challenging for readers unfamiliar with the specific domain to fully grasp the contributions and significance of the work. The experimental validation, while extensive, could be further diversified to include more variety in task environments or alternative benchmarks to strengthen the generality of the claims.' Questions='How does the method handle varying degrees of task similarity or dissimilarity in continual learning scenarios, and is performance consistent across these situations? Are there any scalability concerns when applying UPGD to larger and more complex models or task sequences?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths=\"The proposed Utility-based Perturbed Gradient Descent (UPGD) method effectively addresses the issues of catastrophic forgetting and loss of plasticity in continual learning. The combination of gradient updates with perturbations is a novel approach that seems to enhance learning performance in streaming scenarios with non-stationarities. The method is demonstrated to outperform or match the performance of existing methods across various tasks, indicating its robustness and effectiveness. Additionally, the paper includes extended experiments with reinforcement learning, showing UPGD's applicability beyond conventional settings.\" Weaknesses='The paper could benefit from more detailed explanations and theoretical insights into why UPGD outperforms existing methods. The highly technical nature of the paper might make it challenging for readers unfamiliar with the specific domain to fully grasp the contributions and significance of the work. The experimental validation, while extensive, could be further diversified to include more variety in task environments or alternative benchmarks to strengthen the generality of the claims.' Questions='How does the method handle varying degrees of task similarity or dissimilarity in continual learning scenarios, and is performance consistent across these situations? Are there any scalability concerns when applying UPGD to larger and more complex models or task sequences?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses a critical issue concerning the robustness of large language models, which is highly relevant given their increasing deployment in various applications.\\n2. The empirical demonstration of permutation sensitivity in popular models provides valuable insights into existing vulnerabilities that need to be mitigated.' Weaknesses=\"1. The paper could provide more detailed analysis and explanations regarding the impact of permutation sensitivity on practical applications.\\n2. Insights into potential mitigation strategies for permutation sensitivity would strengthen the paper's contribution.\" Questions='1. How do the identified permutation vulnerabilities compare to other known vulnerabilities in large models?\\n2. Are there any initial thoughts or experiments on potential solutions to mitigate permutation sensitivity in these models?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses a critical issue concerning the robustness of large language models, which is highly relevant given their increasing deployment in various applications.\\n2. The empirical demonstration of permutation sensitivity in popular models provides valuable insights into existing vulnerabilities that need to be mitigated.' Weaknesses=\"1. The paper could provide more detailed analysis and explanations regarding the impact of permutation sensitivity on practical applications.\\n2. Insights into potential mitigation strategies for permutation sensitivity would strengthen the paper's contribution.\" Questions='1. How do the identified permutation vulnerabilities compare to other known vulnerabilities in large models?\\n2. Are there any initial thoughts or experiments on potential solutions to mitigate permutation sensitivity in these models?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The introduction of a new data type, Super Floating-Point (SuFP), which offers improved memory and computational efficiency without sacrificing model accuracy.\\n2. The adaptable precision with multi-region piecewise quantization addresses the issue of handling outliers with high dynamic ranges.\\n3. Extensive benchmarks show improvements in both computational capability and energy efficiency, making the contribution significant for AI hardware accelerators.' Weaknesses='1. Details on how SuFP specifically handles tensor-wise scalable bias and configuration optimization are not thoroughly explained.\\n2. The integration of SuFP into existing hardware ecosystems, potential challenges, and compatibility with current hardware accelerators are not discussed.\\n3. While benchmarks were comprehensive, specific datasets or task details were not provided, leaving some aspects of empirical validation open to interpretation.' Questions='1. How does the introduction of SuFP impact the interoperability with existing AI hardware that supports standard floating-point operations?\\n2. Can SuFP be effectively scaled to larger models, such as those used in large language models, without significant retraining or optimization?\\n3. What are the potential implications or limitations when implementing SuFP on different types of AI accelerators, such as GPUs vs dedicated ASICs?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. The introduction of a new data type, Super Floating-Point (SuFP), which offers improved memory and computational efficiency without sacrificing model accuracy.\\n2. The adaptable precision with multi-region piecewise quantization addresses the issue of handling outliers with high dynamic ranges.\\n3. Extensive benchmarks show improvements in both computational capability and energy efficiency, making the contribution significant for AI hardware accelerators.' Weaknesses='1. Details on how SuFP specifically handles tensor-wise scalable bias and configuration optimization are not thoroughly explained.\\n2. The integration of SuFP into existing hardware ecosystems, potential challenges, and compatibility with current hardware accelerators are not discussed.\\n3. While benchmarks were comprehensive, specific datasets or task details were not provided, leaving some aspects of empirical validation open to interpretation.' Questions='1. How does the introduction of SuFP impact the interoperability with existing AI hardware that supports standard floating-point operations?\\n2. Can SuFP be effectively scaled to larger models, such as those used in large language models, without significant retraining or optimization?\\n3. What are the potential implications or limitations when implementing SuFP on different types of AI accelerators, such as GPUs vs dedicated ASICs?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper addresses an important challenge in applying hard-thresholding to zeroth-order optimization, enhancing its applicability.\\n2. The introduction of variance reduction to resolve the conflict between ZO gradients and the hard-thresholding operator is a significant conceptual advancement.\\n3. The theoretical results suggest improved convergence rates and broader applicability, which is promising for practical relevance.' Weaknesses='1. The abstract and description of the problem and approach are somewhat unclear, which may hinder understanding and replication.\\n2. It is not entirely clear how the new algorithm compares with the SZOHT algorithm in practical scenarios, or if there are limitations not addressed.\\n3. The examples of portfolio optimization and black-box adversarial attacks, while illustrative, may not sufficiently demonstrate the full scope of applicability.' Questions='1. How does the proposed algorithm perform in comparison to SZOHT and other similar approaches on a broader range of problems?\\n2. Are there specific scenarios or types of problems for which the new ZO hard-thresholding algorithm is particularly well-suited or poorly suited?\\n3. Could you provide more details about the experiments, especially regarding the setup and configuration for the portfolio optimization and adversarial attack tasks?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper addresses an important challenge in applying hard-thresholding to zeroth-order optimization, enhancing its applicability.\\n2. The introduction of variance reduction to resolve the conflict between ZO gradients and the hard-thresholding operator is a significant conceptual advancement.\\n3. The theoretical results suggest improved convergence rates and broader applicability, which is promising for practical relevance.' Weaknesses='1. The abstract and description of the problem and approach are somewhat unclear, which may hinder understanding and replication.\\n2. It is not entirely clear how the new algorithm compares with the SZOHT algorithm in practical scenarios, or if there are limitations not addressed.\\n3. The examples of portfolio optimization and black-box adversarial attacks, while illustrative, may not sufficiently demonstrate the full scope of applicability.' Questions='1. How does the proposed algorithm perform in comparison to SZOHT and other similar approaches on a broader range of problems?\\n2. Are there specific scenarios or types of problems for which the new ZO hard-thresholding algorithm is particularly well-suited or poorly suited?\\n3. Could you provide more details about the experiments, especially regarding the setup and configuration for the portfolio optimization and adversarial attack tasks?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The strengths of the paper include: 1) The introduction of Farzi, a novel data distillation technique that demonstrates impressive reduction in data size while maintaining or enhancing model performance, which is highly relevant for scaling autoregressive models. 2) The innovative use of reverse-mode differentiation of the Adam optimizer and factorization into a latent space provides a compelling method for memory-efficient data distillation. 3) The empirical results are significant, showing 98-120% of full-data performance with just 0.1% of original data size, indicating a substantial potential for reducing resource requirements in training.' Weaknesses='The weaknesses of the paper include: 1) There is limited discussion on the potential limitations or challenges of applying Farzi to more complex datasets or different types of autoregressive tasks. 2) The scalability of the approach to very large datasets or models is not clearly addressed, leaving open questions about its universal applicability.' Questions='The questions I have include: 1) How does Farzi perform on datasets with different characteristics, such as varying sequence lengths or distributions? 2) Are there any theoretical limits to the effectiveness of the Hessian-vector product approach for reverse-mode differentiation in this context? 3) Can Farzi be effectively combined with other data reduction techniques, such as pruning or model compression?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The strengths of the paper include: 1) The introduction of Farzi, a novel data distillation technique that demonstrates impressive reduction in data size while maintaining or enhancing model performance, which is highly relevant for scaling autoregressive models. 2) The innovative use of reverse-mode differentiation of the Adam optimizer and factorization into a latent space provides a compelling method for memory-efficient data distillation. 3) The empirical results are significant, showing 98-120% of full-data performance with just 0.1% of original data size, indicating a substantial potential for reducing resource requirements in training.' Weaknesses='The weaknesses of the paper include: 1) There is limited discussion on the potential limitations or challenges of applying Farzi to more complex datasets or different types of autoregressive tasks. 2) The scalability of the approach to very large datasets or models is not clearly addressed, leaving open questions about its universal applicability.' Questions='The questions I have include: 1) How does Farzi perform on datasets with different characteristics, such as varying sequence lengths or distributions? 2) Are there any theoretical limits to the effectiveness of the Hessian-vector product approach for reverse-mode differentiation in this context? 3) Can Farzi be effectively combined with other data reduction techniques, such as pruning or model compression?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel algorithm, PNLSVI, for offline RL with non-linear function approximation and demonstrates its strength through the tight dependency on the function class complexity. The proposed algorithm includes three innovative components, potentially contributing significantly to the field of RL. The work is theoretically robust, providing a statistically optimal algorithm in this context.' Weaknesses='The theoretical nature of the paper and lack of extensive experimental validation may limit immediate applicability and understanding by practitioners. The alignment between theoretical advancements and practical implementations could be better addressed.' Questions='How does the performance of PNLSVI compare in practical scenarios, particularly in large-scale RL problems? Are there specific classes of non-linear function approximations where PNLSVI shows significant improvements over existing methods?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel algorithm, PNLSVI, for offline RL with non-linear function approximation and demonstrates its strength through the tight dependency on the function class complexity. The proposed algorithm includes three innovative components, potentially contributing significantly to the field of RL. The work is theoretically robust, providing a statistically optimal algorithm in this context.' Weaknesses='The theoretical nature of the paper and lack of extensive experimental validation may limit immediate applicability and understanding by practitioners. The alignment between theoretical advancements and practical implementations could be better addressed.' Questions='How does the performance of PNLSVI compare in practical scenarios, particularly in large-scale RL problems? Are there specific classes of non-linear function approximations where PNLSVI shows significant improvements over existing methods?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper takes an innovative approach by applying behavioral game theory to study LLM interactions, delivering valuable insights into how these models might behave in social settings. The study is well-structured, with a clear exploration of LLM behavioral patterns in different types of games.' Weaknesses='The paper highlights limitations in coordination tasks but does not fully address potential methodological variations that could impact the results, such as alternative strategies that LLMs could employ aside from the predefined ones. The study could include a broader set of games for a more comprehensive understanding.' Questions='Can the methods used to modify GPT-4’s behavior be generalized to other LLMs, or are they specific to GPT-4? Are there alternative strategies or adjustments, aside from providing more information, that could improve LLM performance in coordination games?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper takes an innovative approach by applying behavioral game theory to study LLM interactions, delivering valuable insights into how these models might behave in social settings. The study is well-structured, with a clear exploration of LLM behavioral patterns in different types of games.' Weaknesses='The paper highlights limitations in coordination tasks but does not fully address potential methodological variations that could impact the results, such as alternative strategies that LLMs could employ aside from the predefined ones. The study could include a broader set of games for a more comprehensive understanding.' Questions='Can the methods used to modify GPT-4’s behavior be generalized to other LLMs, or are they specific to GPT-4? Are there alternative strategies or adjustments, aside from providing more information, that could improve LLM performance in coordination games?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=2 Rating=5 Confidence=3 Strengths='The paper provides theoretical insights into how scale affects performance of structure learners with square-based and log-likelihood losses, particularly in non-linear cases.' Weaknesses='The findings about the impact of scale are not novel and primarily extend previous results with some additional conditions. Implications for practical structure learning scenarios could have been better explored.' Questions='How do the proposed conditions extend or differ from existing literature on the limitations of square-based losses in DAG learning? Are there any recommendations for practitioners based on the findings?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=2 Rating=5 Confidence=3 Strengths='The paper provides theoretical insights into how scale affects performance of structure learners with square-based and log-likelihood losses, particularly in non-linear cases.' Weaknesses='The findings about the impact of scale are not novel and primarily extend previous results with some additional conditions. Implications for practical structure learning scenarios could have been better explored.' Questions='How do the proposed conditions extend or differ from existing literature on the limitations of square-based losses in DAG learning? Are there any recommendations for practitioners based on the findings?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The Village-Net Clustering algorithm introduces a novel approach to clustering of complex manifold data by combining K-Means with a network-based community detection. The algorithm's ability to autonomously determine the optimal number of clusters is a notable strength. Extensive benchmarking shows competitive performance against state-of-the-art methods, especially in large-scale datasets.\" Weaknesses='The paper relies on the proprietary Walk-likelihood Community Finder (WLCF) for the second phase of clustering, which is not thoroughly detailed or validated independently within the abstract. Details on how this affects scalability, performance, and general applicability are lacking. More comparisons with a broader range of state-of-the-art clustering methods would strengthen the claims.' Questions='How does the Walk-likelihood Community Finder (WLCF) work, and what makes it suitable for this clustering task compared to other community detection algorithms? Are there specific scenarios where Village-Net is expected to outperform traditional clustering methods?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The Village-Net Clustering algorithm introduces a novel approach to clustering of complex manifold data by combining K-Means with a network-based community detection. The algorithm's ability to autonomously determine the optimal number of clusters is a notable strength. Extensive benchmarking shows competitive performance against state-of-the-art methods, especially in large-scale datasets.\" Weaknesses='The paper relies on the proprietary Walk-likelihood Community Finder (WLCF) for the second phase of clustering, which is not thoroughly detailed or validated independently within the abstract. Details on how this affects scalability, performance, and general applicability are lacking. More comparisons with a broader range of state-of-the-art clustering methods would strengthen the claims.' Questions='How does the Walk-likelihood Community Finder (WLCF) work, and what makes it suitable for this clustering task compared to other community detection algorithms? Are there specific scenarios where Village-Net is expected to outperform traditional clustering methods?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=2 Rating=6 Confidence=3 Strengths='The work explores a novel plug-in framework for using task-specific fine-tuned language models (SLMs) with large language models (LLMs) to enhance their capabilities in terms of generalizability and factuality.' Weaknesses='The paper lacks clarity on how the plug-in framework operates on a technical level. Additionally, the novelty of using discriminative models to improve LLMs needs more emphasis.' Questions='How exactly are the task-specific fine-tuned models integrated in the framework with LLMs during inference?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=2 Rating=6 Confidence=3 Strengths='The work explores a novel plug-in framework for using task-specific fine-tuned language models (SLMs) with large language models (LLMs) to enhance their capabilities in terms of generalizability and factuality.' Weaknesses='The paper lacks clarity on how the plug-in framework operates on a technical level. Additionally, the novelty of using discriminative models to improve LLMs needs more emphasis.' Questions='How exactly are the task-specific fine-tuned models integrated in the framework with LLMs during inference?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel objective, UDIM, for domain generalization which combines parameter and data perturbations to improve generalization to unseen domains. The approach builds on existing SAM techniques, theoretically establishing an upper bound for DG tasks. Empirically, UDIM demonstrates significant performance improvements on benchmark datasets, especially in restrictive domain scenarios.' Weaknesses='The paper lacks clarity on the practical implications of the perturbed instance generation and its impact on domain realism. There is also limited analysis on the scalability and computational overhead of the proposed method when applied to larger or more complex datasets. The transition from theoretical validation to empirical demonstration could be better articulated.' Questions='How does the proposed UDIM handle scenarios where the perturbed domains significantly differ from real-world unknown domains? Is there a specific mechanism that controls the degree of perturbation to maintain data realism? What are the computational overheads associated with integrating UDIM into existing SAM-based frameworks, and how sensitive is the method to changes in the perturbation parameters?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel objective, UDIM, for domain generalization which combines parameter and data perturbations to improve generalization to unseen domains. The approach builds on existing SAM techniques, theoretically establishing an upper bound for DG tasks. Empirically, UDIM demonstrates significant performance improvements on benchmark datasets, especially in restrictive domain scenarios.' Weaknesses='The paper lacks clarity on the practical implications of the perturbed instance generation and its impact on domain realism. There is also limited analysis on the scalability and computational overhead of the proposed method when applied to larger or more complex datasets. The transition from theoretical validation to empirical demonstration could be better articulated.' Questions='How does the proposed UDIM handle scenarios where the perturbed domains significantly differ from real-world unknown domains? Is there a specific mechanism that controls the degree of perturbation to maintain data realism? What are the computational overheads associated with integrating UDIM into existing SAM-based frameworks, and how sensitive is the method to changes in the perturbation parameters?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel hybrid GAN model (CCGAN) that incorporates both collaborative and competitive components, addressing issues of training collapse and enabling the generation of high-quality samples with specific features. The theoretical grounding, including the transformation of the regularization term into a divergence, is well-established, promising consistency in training and results.' Weaknesses=\"The abstract is dense and could be condensed for clarity in certain segments to enhance accessibility for readers less familiar with advanced GAN mechanics. The balance of in-depth theoretical exploration versus experimental details could be improved to present a more comprehensive picture of the model's complete operational framework.\" Questions='1. Can the CCGAN framework be directly applied to tasks beyond image generation, such as text-to-image synthesis, or would further modifications be necessary? 2. What are the computational overheads involved with this hybrid GAN framework compared to traditional GAN setups? 3. Could further insights be given regarding the specific tuning strategies employed during the experiments?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel hybrid GAN model (CCGAN) that incorporates both collaborative and competitive components, addressing issues of training collapse and enabling the generation of high-quality samples with specific features. The theoretical grounding, including the transformation of the regularization term into a divergence, is well-established, promising consistency in training and results.' Weaknesses=\"The abstract is dense and could be condensed for clarity in certain segments to enhance accessibility for readers less familiar with advanced GAN mechanics. The balance of in-depth theoretical exploration versus experimental details could be improved to present a more comprehensive picture of the model's complete operational framework.\" Questions='1. Can the CCGAN framework be directly applied to tasks beyond image generation, such as text-to-image synthesis, or would further modifications be necessary? 2. What are the computational overheads involved with this hybrid GAN framework compared to traditional GAN setups? 3. Could further insights be given regarding the specific tuning strategies employed during the experiments?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The proposed IPRM framework cleverly integrates iterative and parallel reasoning, offering a novel approach to enhancing visual question answering tasks. Its ability to outperform existing methods on benchmarks while maintaining interpretability is a significant strength of this paper.' Weaknesses='The paper lacks clarity in its description of the parallel reasoning mechanism. Further elaboration on how parallel computation is explicitly achieved would be beneficial. The paper may also benefit from more comprehensive results on diverse datasets to validate the robustness of the IPRM model.' Questions='How does the parallel reasoning mechanism fundamentally differ from increasing the number of internal states in existing architectures? What is the specific contribution of the parallel processing structure beyond the iterative nature of the algorithm?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The proposed IPRM framework cleverly integrates iterative and parallel reasoning, offering a novel approach to enhancing visual question answering tasks. Its ability to outperform existing methods on benchmarks while maintaining interpretability is a significant strength of this paper.' Weaknesses='The paper lacks clarity in its description of the parallel reasoning mechanism. Further elaboration on how parallel computation is explicitly achieved would be beneficial. The paper may also benefit from more comprehensive results on diverse datasets to validate the robustness of the IPRM model.' Questions='How does the parallel reasoning mechanism fundamentally differ from increasing the number of internal states in existing architectures? What is the specific contribution of the parallel processing structure beyond the iterative nature of the algorithm?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper tackles a relevant and timely problem, introducing a novel approach to epidemic forecasting by leveraging pre-trained models in time-series analysis, a method proven effective in other fields like language and vision tasks.' Weaknesses='The paper may not adequately address the challenges of transferring learned dynamics across different types of epidemics, which may inherently have different characteristics. There are limited details on how these dynamics are effectively disentangled and transferred.' Questions='How does the model ensure the learned representations from one type of epidemic can be effectively transferred to another with possibly distinct characteristics? Are there any limitations observed in applying the model to datasets significantly different from those used for pre-training?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper tackles a relevant and timely problem, introducing a novel approach to epidemic forecasting by leveraging pre-trained models in time-series analysis, a method proven effective in other fields like language and vision tasks.' Weaknesses='The paper may not adequately address the challenges of transferring learned dynamics across different types of epidemics, which may inherently have different characteristics. There are limited details on how these dynamics are effectively disentangled and transferred.' Questions='How does the model ensure the learned representations from one type of epidemic can be effectively transferred to another with possibly distinct characteristics? Are there any limitations observed in applying the model to datasets significantly different from those used for pre-training?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel instance-wise scaling technique that improves robust accuracy and provides flexibility in handling trade-offs between robust and average accuracies. The work presents a comprehensive evaluation metric and framework applicable to existing algorithms in domain. Experiments on computer vision and NLP datasets support the effectiveness of the proposed methods.' Weaknesses=\"The approach may not offer a fundamentally new perspective on the optimization problem itself, focusing more on improving evaluation methods. While the evaluation metric provides a broader view, it might be complex or less interpretable in practice. The impact of the approach heavily relies on the introduced metric's effectiveness, and without in-depth proofs or theoretical analyses, this could limit its applicability.\" Questions='How does the method ensure that the scaling techniques do not overfit to particular datasets, limiting generalization?\\nWhat is the impact of different dataset characteristics on the performance of the class-specific scaling technique?\\nDoes the unified metric take into account interaction effects between biases in different data groups, and how sensitive is it to noise or label errors in datasets?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel instance-wise scaling technique that improves robust accuracy and provides flexibility in handling trade-offs between robust and average accuracies. The work presents a comprehensive evaluation metric and framework applicable to existing algorithms in domain. Experiments on computer vision and NLP datasets support the effectiveness of the proposed methods.' Weaknesses=\"The approach may not offer a fundamentally new perspective on the optimization problem itself, focusing more on improving evaluation methods. While the evaluation metric provides a broader view, it might be complex or less interpretable in practice. The impact of the approach heavily relies on the introduced metric's effectiveness, and without in-depth proofs or theoretical analyses, this could limit its applicability.\" Questions='How does the method ensure that the scaling techniques do not overfit to particular datasets, limiting generalization?\\nWhat is the impact of different dataset characteristics on the performance of the class-specific scaling technique?\\nDoes the unified metric take into account interaction effects between biases in different data groups, and how sensitive is it to noise or label errors in datasets?'\n",
      "Zero shot review:\n",
      " Soundness=2 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths='The paper presents an innovative approach to disentangled representation learning for time series, specifically targeting home appliance electricity usage, which is an important and emerging application for reducing carbon footprints. The introduction of the Time Disentangling Score (TDS) provides a new metric for evaluating the quality of disentanglement in this context.' Weaknesses='The problem is not clearly framed, making it difficult to understand the specific advancements over existing work. The paper assumes familiarity with many technical concepts without adequate explanation or background, which could hinder comprehension. The evaluation methods and results are not thoroughly detailed, leaving some uncertainty about the performance improvements declared.' Questions='How does the proposed method compare to existing state-of-the-art methods on both performance and computational efficiency? Can the TDS be applied to other types of time series data outside of electricity usage, and what might be the limitations? How is the independence-of-support property maintained throughout the disentanglement process?'\n",
      "Success: Soundness=2 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths='The paper presents an innovative approach to disentangled representation learning for time series, specifically targeting home appliance electricity usage, which is an important and emerging application for reducing carbon footprints. The introduction of the Time Disentangling Score (TDS) provides a new metric for evaluating the quality of disentanglement in this context.' Weaknesses='The problem is not clearly framed, making it difficult to understand the specific advancements over existing work. The paper assumes familiarity with many technical concepts without adequate explanation or background, which could hinder comprehension. The evaluation methods and results are not thoroughly detailed, leaving some uncertainty about the performance improvements declared.' Questions='How does the proposed method compare to existing state-of-the-art methods on both performance and computational efficiency? Can the TDS be applied to other types of time series data outside of electricity usage, and what might be the limitations? How is the independence-of-support property maintained throughout the disentanglement process?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=4 Strengths='The Qwen-VL models demonstrate strong performance on a wide range of visual-centric benchmarks, setting new records for generalist models of similar scale. The integration of features such as image-caption-box alignment and instruction tuning for dialogue tasks is well-executed. The promise to release these models to the public is a significant contribution to the research community.' Weaknesses=\"The paper doesn't provide in-depth discussions on the limitations or potential drawbacks of the proposed models, such as computational costs or any trade-offs involved in model training at scale. The novelty of the contribution could be more clearly articulated, especially in comparison to existing solutions.\" Questions='What specific strategies are employed to ensure the multilingual multimodal cleaned corpus effectively represents a diverse range of languages and cultural contexts? Are there any known limitations in the corpus construction that could impact model performance in certain languages or regions?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=4 Strengths='The Qwen-VL models demonstrate strong performance on a wide range of visual-centric benchmarks, setting new records for generalist models of similar scale. The integration of features such as image-caption-box alignment and instruction tuning for dialogue tasks is well-executed. The promise to release these models to the public is a significant contribution to the research community.' Weaknesses=\"The paper doesn't provide in-depth discussions on the limitations or potential drawbacks of the proposed models, such as computational costs or any trade-offs involved in model training at scale. The novelty of the contribution could be more clearly articulated, especially in comparison to existing solutions.\" Questions='What specific strategies are employed to ensure the multilingual multimodal cleaned corpus effectively represents a diverse range of languages and cultural contexts? Are there any known limitations in the corpus construction that could impact model performance in certain languages or regions?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper addresses a novel problem in reinforcement learning by adapting dataset distillation to environments where a fixed dataset is not available. The introduction of Hallucinating Datasets with Evolution Strategies (HaDES) presents a compelling new method for achieving behavior distillation with potential applications in multi-task learning. The proposed method shows significant promise, achieving competitive results with very few state-action pairs and demonstrating generalizability across architectures and hyperparameters. Visualization of the synthetic datasets adds to the interpretability of the approach.' Weaknesses='The lack of direct comparison with traditional RL methods makes it difficult to quantify the improvement brought by HaDES in terms of efficiency or performance. The paper could benefit from a more detailed discussion on the limitations of the proposed approach, including potential applicability to other types of RL tasks beyond continuous control. Additionally, the scalability of the approach to more complex environments or a larger number of tasks is not fully addressed.' Questions='1. How does the performance of HaDES compare to traditional RL methods in terms of training time and resource efficiency?\\n2. What are the potential limitations of HaDES when applied to more complex RL environments or discrete action spaces?\\n3. Can HaDES be adapted to use expert data, if available, to improve the quality of distilled datasets, and how would that impact the performance?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper addresses a novel problem in reinforcement learning by adapting dataset distillation to environments where a fixed dataset is not available. The introduction of Hallucinating Datasets with Evolution Strategies (HaDES) presents a compelling new method for achieving behavior distillation with potential applications in multi-task learning. The proposed method shows significant promise, achieving competitive results with very few state-action pairs and demonstrating generalizability across architectures and hyperparameters. Visualization of the synthetic datasets adds to the interpretability of the approach.' Weaknesses='The lack of direct comparison with traditional RL methods makes it difficult to quantify the improvement brought by HaDES in terms of efficiency or performance. The paper could benefit from a more detailed discussion on the limitations of the proposed approach, including potential applicability to other types of RL tasks beyond continuous control. Additionally, the scalability of the approach to more complex environments or a larger number of tasks is not fully addressed.' Questions='1. How does the performance of HaDES compare to traditional RL methods in terms of training time and resource efficiency?\\n2. What are the potential limitations of HaDES when applied to more complex RL environments or discrete action spaces?\\n3. Can HaDES be adapted to use expert data, if available, to improve the quality of distilled datasets, and how would that impact the performance?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='The introduction of the Extended Textual Conditioning space (P+) and the Extended Textual Inversion (XTI) appears novel and well-founded. XTI demonstrates improved expressiveness, precision, and convergence over existing methods, which is promising.' Weaknesses='The paper could benefit from a more detailed theoretical grounding for its claims. Additionally, an exploration of potential limitations and edge cases for P+ and XTI is not thoroughly discussed.' Questions='How does P+ handle ambiguous or contradictory textual conditions across different layers? Are there any scenarios where P+ fails to provide the anticipated level of control? Could you further explain the methodology for selecting per-layer prompts?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='The introduction of the Extended Textual Conditioning space (P+) and the Extended Textual Inversion (XTI) appears novel and well-founded. XTI demonstrates improved expressiveness, precision, and convergence over existing methods, which is promising.' Weaknesses='The paper could benefit from a more detailed theoretical grounding for its claims. Additionally, an exploration of potential limitations and edge cases for P+ and XTI is not thoroughly discussed.' Questions='How does P+ handle ambiguous or contradictory textual conditions across different layers? Are there any scenarios where P+ fails to provide the anticipated level of control? Could you further explain the methodology for selecting per-layer prompts?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed approach effectively addresses the challenges of SFUDA by utilizing multiple hypotheses to enhance prediction accuracy and reduce reliance on individual inaccurate predictions. The method is shown to achieve state-of-the-art performance and can be integrated into existing systems, demonstrating practical applicability.' Weaknesses='The motivation for using multiple hypotheses may require additional clarification, particularly regarding practical applicability. The paper could benefit from clearer explanations in some areas, including the specific processes or algorithms used in each adaptation step, and the theoretical rationale for certain design choices.' Questions='How does the method determine the optimal number of hypotheses to use in practice? Is there a specific criterion for selecting hypotheses during consolidation to ensure robustness?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed approach effectively addresses the challenges of SFUDA by utilizing multiple hypotheses to enhance prediction accuracy and reduce reliance on individual inaccurate predictions. The method is shown to achieve state-of-the-art performance and can be integrated into existing systems, demonstrating practical applicability.' Weaknesses='The motivation for using multiple hypotheses may require additional clarification, particularly regarding practical applicability. The paper could benefit from clearer explanations in some areas, including the specific processes or algorithms used in each adaptation step, and the theoretical rationale for certain design choices.' Questions='How does the method determine the optimal number of hypotheses to use in practice? Is there a specific criterion for selecting hypotheses during consolidation to ensure robustness?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper introduces a novel interpretation technique for structured output models, addressing an important issue in machine learning explainability. The proposed energy-based training process for an interpreter function is innovative and considers structural information, which improves explanation quality. The effectiveness of this approach is confirmed through various simulations and real datasets.' Weaknesses='The presentation could be clearer, as the explanation of the energy-based training process might be abstract and hard to understand without further detail. It is not clear how robust the proposed technique is when applied to different types of structured output models, and the choice of datasets for evaluation might limit generalization. Moreover, the paper does not extensively compare the proposed method with existing interpretability techniques.' Questions='How does the method handle cases where the structured output contains many variables with complex interdependencies? Can you provide more insight into how well this technique performs compared to existing interpretability approaches? Are there any specific limitations to using this method that should be considered when applying it to real-world datasets?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper introduces a novel interpretation technique for structured output models, addressing an important issue in machine learning explainability. The proposed energy-based training process for an interpreter function is innovative and considers structural information, which improves explanation quality. The effectiveness of this approach is confirmed through various simulations and real datasets.' Weaknesses='The presentation could be clearer, as the explanation of the energy-based training process might be abstract and hard to understand without further detail. It is not clear how robust the proposed technique is when applied to different types of structured output models, and the choice of datasets for evaluation might limit generalization. Moreover, the paper does not extensively compare the proposed method with existing interpretability techniques.' Questions='How does the method handle cases where the structured output contains many variables with complex interdependencies? Can you provide more insight into how well this technique performs compared to existing interpretability approaches? Are there any specific limitations to using this method that should be considered when applying it to real-world datasets?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=2 Rating=5 Confidence=3 Strengths='The paper brings attention to the potential over-engineering in recent GNN architectures, suggesting simpler traditional methods can sometimes match or exceed the performance on SSNC tasks. This challenges the current trend and opens a space for further discussion and exploration.' Weaknesses='The paper may lack sufficient experimental rigor or theoretical justification to substantiate its claims about simpler methods consistently outperforming more complex GNN approaches. Additionally, the exploration of hyperparameter impacts could be more comprehensive. There might be missing details on how evaluation conventions have shifted, which could impact the understanding of GNN performance.' Questions='How do the authors plan to further validate their claims about simpler methods matching or exceeding current GNN performance? Are there specific scenarios or types of graphs where these simpler methods are particularly advantageous or disadvantageous? Could the authors provide more detailed insights into how evaluation conventions have shifted and affected GNN performance outcomes?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=2 Rating=5 Confidence=3 Strengths='The paper brings attention to the potential over-engineering in recent GNN architectures, suggesting simpler traditional methods can sometimes match or exceed the performance on SSNC tasks. This challenges the current trend and opens a space for further discussion and exploration.' Weaknesses='The paper may lack sufficient experimental rigor or theoretical justification to substantiate its claims about simpler methods consistently outperforming more complex GNN approaches. Additionally, the exploration of hyperparameter impacts could be more comprehensive. There might be missing details on how evaluation conventions have shifted, which could impact the understanding of GNN performance.' Questions='How do the authors plan to further validate their claims about simpler methods matching or exceeding current GNN performance? Are there specific scenarios or types of graphs where these simpler methods are particularly advantageous or disadvantageous? Could the authors provide more detailed insights into how evaluation conventions have shifted and affected GNN performance outcomes?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper addresses a significant problem in the context of Robust Fine-Tuning (RFT) by introducing a novel low-rank (LoRa) branch that effectively disentangles the optimization of natural and adversarial objectives. This approach potentially reduces the instability in the optimization process encountered in RFT and automates the tuning process, showcasing substantial empirical improvements across multiple tasks.' Weaknesses=\"The abstract and methodology descriptions are somewhat technical, which may increase the barrier of understanding for readers less familiar with the specific terminologies and techniques used, such as LoRa branches and disentangled optimization processes. Additionally, the paper doesn't elaborate on potential limitations or edge cases where the proposed approach might not work effectively.\" Questions='1. How does the proposed LoRa branch approach compare computationally in terms of efficiency and resource consumption with traditional RFT methods?\\n2. What are the long-term implications of using LoRa branches on model performance concerning transferability to substantially different tasks or domains?\\n3. Could this approach be extended to scenarios with other types of robustness, such as robustness to data distribution shifts?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper addresses a significant problem in the context of Robust Fine-Tuning (RFT) by introducing a novel low-rank (LoRa) branch that effectively disentangles the optimization of natural and adversarial objectives. This approach potentially reduces the instability in the optimization process encountered in RFT and automates the tuning process, showcasing substantial empirical improvements across multiple tasks.' Weaknesses=\"The abstract and methodology descriptions are somewhat technical, which may increase the barrier of understanding for readers less familiar with the specific terminologies and techniques used, such as LoRa branches and disentangled optimization processes. Additionally, the paper doesn't elaborate on potential limitations or edge cases where the proposed approach might not work effectively.\" Questions='1. How does the proposed LoRa branch approach compare computationally in terms of efficiency and resource consumption with traditional RFT methods?\\n2. What are the long-term implications of using LoRa branches on model performance concerning transferability to substantially different tasks or domains?\\n3. Could this approach be extended to scenarios with other types of robustness, such as robustness to data distribution shifts?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"1. The proposed AdaGLT framework introduces an adaptive, dynamic, and automated approach to discover graph lottery tickets, which addresses several limitations of existing GLT methods.\\n2. The theoretical analysis of tackling over-smoothing issues and providing deeper GNN capabilities is an important advancement.\\n3. Extensive experimentation demonstrates superiority over state-of-the-art methods, highlighting the framework's practical benefits.\" Weaknesses='1. The paper could benefit from more extensive validation on truly large-scale graph datasets to bolster claims regarding scalability.\\n2. Specific implementation details, such as computational complexity and overhead involved in the adaptive and dynamic processes, could be better elucidated for practical adoption.' Questions='1. How does AdaGLT handle real-time updates or changes in the underlying graph structure post-training?\\n2. Are there any situations or types of datasets where AdaGLT might struggle or not perform as well as expected due to its adaptive nature?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"1. The proposed AdaGLT framework introduces an adaptive, dynamic, and automated approach to discover graph lottery tickets, which addresses several limitations of existing GLT methods.\\n2. The theoretical analysis of tackling over-smoothing issues and providing deeper GNN capabilities is an important advancement.\\n3. Extensive experimentation demonstrates superiority over state-of-the-art methods, highlighting the framework's practical benefits.\" Weaknesses='1. The paper could benefit from more extensive validation on truly large-scale graph datasets to bolster claims regarding scalability.\\n2. Specific implementation details, such as computational complexity and overhead involved in the adaptive and dynamic processes, could be better elucidated for practical adoption.' Questions='1. How does AdaGLT handle real-time updates or changes in the underlying graph structure post-training?\\n2. Are there any situations or types of datasets where AdaGLT might struggle or not perform as well as expected due to its adaptive nature?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=4 Strengths='The paper presents a significant enhancement of the Stable Diffusion model, showcasing a much larger UNet backbone and novel conditioning schemes. These changes lead to a dramatic improvement in image generation quality, competitive with state-of-the-art models like Midjourney. The inclusion of a refinement model further increases the visual fidelity of the generated images.' Weaknesses='While the improvements in terms of model size and conditioning schemes are noteworthy, they may not be largely novel contributions but rather enhancements on existing methodologies. The impact of the increased computational resources required for the larger UNet backbone might not have been sufficiently addressed.' Questions='How does the increased size of the UNet backbone affect the training and inference time compared to previous versions? Are there any specific trade-offs between quality and computational efficiency that were observed during experimentation?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=4 Strengths='The paper presents a significant enhancement of the Stable Diffusion model, showcasing a much larger UNet backbone and novel conditioning schemes. These changes lead to a dramatic improvement in image generation quality, competitive with state-of-the-art models like Midjourney. The inclusion of a refinement model further increases the visual fidelity of the generated images.' Weaknesses='While the improvements in terms of model size and conditioning schemes are noteworthy, they may not be largely novel contributions but rather enhancements on existing methodologies. The impact of the increased computational resources required for the larger UNet backbone might not have been sufficiently addressed.' Questions='How does the increased size of the UNet backbone affect the training and inference time compared to previous versions? Are there any specific trade-offs between quality and computational efficiency that were observed during experimentation?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The introduction of the novel concept of activation prompt (AP) is innovative and adds a new dimension to the discussion around visual prompting. \\n2. The paper provides a comprehensive analysis with experiments conducted across 29 diverse datasets, which adds robustness to the findings and conclusions.\\n3. The integration of theoretical models that explain performance differences between AP and VP adds significant value to the research.' Weaknesses=\"1. While the paper introduces AP, the novelty of the concept might overlap with existing methods, and the paper could provide more detailed comparisons with other similar existing techniques.\\n2. The performance improvement, while significant, could be perceived as incremental compared to the traditional VP and PEFT methods given the abstract's description.\\n3. Some theoretical aspects might be complex or require more detailed elucidation for a broader audience not familiar with related concepts in deep learning architectures.\" Questions='1. Can the proposed AP method suffer from any particular types of input data or specific domains where it may not outperform traditional VP methods?\\n2. Does the introduction of AP in intermediate layers lead to any challenges or trade-offs in model interpretability or robustness?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The introduction of the novel concept of activation prompt (AP) is innovative and adds a new dimension to the discussion around visual prompting. \\n2. The paper provides a comprehensive analysis with experiments conducted across 29 diverse datasets, which adds robustness to the findings and conclusions.\\n3. The integration of theoretical models that explain performance differences between AP and VP adds significant value to the research.' Weaknesses=\"1. While the paper introduces AP, the novelty of the concept might overlap with existing methods, and the paper could provide more detailed comparisons with other similar existing techniques.\\n2. The performance improvement, while significant, could be perceived as incremental compared to the traditional VP and PEFT methods given the abstract's description.\\n3. Some theoretical aspects might be complex or require more detailed elucidation for a broader audience not familiar with related concepts in deep learning architectures.\" Questions='1. Can the proposed AP method suffer from any particular types of input data or specific domains where it may not outperform traditional VP methods?\\n2. Does the introduction of AP in intermediate layers lead to any challenges or trade-offs in model interpretability or robustness?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel task inspired by biology, addressing rapid and continual learning, which is a significant problem in machine learning. The approach proposed by the authors combines a biologically-inspired memory model with a convolutional network, showing promising results in terms of rapid adaptation and retention of past knowledge. The model outperforms traditional ANN baselines, demonstrating its effectiveness. The presentation of the paper is clear, with a logical structure and well-explained methods.' Weaknesses=\"While the proposed model shows promising results, the paper may lack a diverse range of experimental comparisons with more recent and varied baselines in the machine learning field. The reliance on a specific task (sequential Morris Water Maze) may limit the generalizability of the approach to other domains or tasks outside spatial navigation. Additionally, the model's dependence on a biologically-inspired architecture might pose challenges in scalability and adaptation across different datasets or environments.\" Questions='How well does the proposed model generalize to tasks outside spatial navigation, such as those in other domains like image classification or language processing?\\nDoes the model require any specific tuning or modifications to be applicable to non-spatial tasks?\\nWhat are the potential computational limitations or overheads associated with the entorhinal-hippocampal circuit-inspired memory model?\\nHave the authors considered comparing their approach with more recent baselines in the realm of few-shot and continual learning?\\nIs the proposed sequential Morris Water Maze task dataset publicly available, and if so, how can other researchers access it to replicate experiments?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel task inspired by biology, addressing rapid and continual learning, which is a significant problem in machine learning. The approach proposed by the authors combines a biologically-inspired memory model with a convolutional network, showing promising results in terms of rapid adaptation and retention of past knowledge. The model outperforms traditional ANN baselines, demonstrating its effectiveness. The presentation of the paper is clear, with a logical structure and well-explained methods.' Weaknesses=\"While the proposed model shows promising results, the paper may lack a diverse range of experimental comparisons with more recent and varied baselines in the machine learning field. The reliance on a specific task (sequential Morris Water Maze) may limit the generalizability of the approach to other domains or tasks outside spatial navigation. Additionally, the model's dependence on a biologically-inspired architecture might pose challenges in scalability and adaptation across different datasets or environments.\" Questions='How well does the proposed model generalize to tasks outside spatial navigation, such as those in other domains like image classification or language processing?\\nDoes the model require any specific tuning or modifications to be applicable to non-spatial tasks?\\nWhat are the potential computational limitations or overheads associated with the entorhinal-hippocampal circuit-inspired memory model?\\nHave the authors considered comparing their approach with more recent baselines in the realm of few-shot and continual learning?\\nIs the proposed sequential Morris Water Maze task dataset publicly available, and if so, how can other researchers access it to replicate experiments?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper addresses an important issue in OOD detection in the context of imbalanced ID data, providing both theoretical and empirical insights into the problem. The introduction of ImOOD and its systematic approach provides a valuable perspective on balancing OOD detection tasks. The results on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT datasets are promising and indicate a significant improvement over current state-of-the-art methods.' Weaknesses='The paper focuses primarily on theoretical insights and might benefit from broader empirical evaluations on more diverse datasets and scenarios to establish its applicability more clearly. Furthermore, how the findings generalize to other types of data, such as text or audio, remains unaddressed. While the statistical and theoretical framework is solid, practical implementation details could be expanded for reproducibility.' Questions='How does ImOOD handle datasets with different types of imbalance, such as temporal or spatial imbalance in data distribution? Could the approach be extended to other domains like text classification or audio processing, and what challenges might arise in doing so?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper addresses an important issue in OOD detection in the context of imbalanced ID data, providing both theoretical and empirical insights into the problem. The introduction of ImOOD and its systematic approach provides a valuable perspective on balancing OOD detection tasks. The results on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT datasets are promising and indicate a significant improvement over current state-of-the-art methods.' Weaknesses='The paper focuses primarily on theoretical insights and might benefit from broader empirical evaluations on more diverse datasets and scenarios to establish its applicability more clearly. Furthermore, how the findings generalize to other types of data, such as text or audio, remains unaddressed. While the statistical and theoretical framework is solid, practical implementation details could be expanded for reproducibility.' Questions='How does ImOOD handle datasets with different types of imbalance, such as temporal or spatial imbalance in data distribution? Could the approach be extended to other domains like text classification or audio processing, and what challenges might arise in doing so?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a simple yet effective approach to adversarial example detection in ASR systems. It leverages characteristics of the probability distribution produced by the ASR systems and demonstrates high performance with AUROC values exceeding 99%. The study also explores adaptive attack strategies, demonstrating a comprehensive understanding of the problem and potential solutions.' Weaknesses='The approach, although demonstrating high AUROC, may lack novelty in terms of methodology. While the combination of statistical characteristics for detection seems effective, the paper could benefit from more comparative analysis with existing adversarial detection techniques. Additionally, the paper does not discuss how it handles real-world noise variations that may not follow the controlled settings of adversarial examples.' Questions='How does the proposed method perform under real-world conditions where noise might not be precisely controlled or as distinct as adversarial examples? Are there specific scenarios or types of adversarial attacks where this method might struggle?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a simple yet effective approach to adversarial example detection in ASR systems. It leverages characteristics of the probability distribution produced by the ASR systems and demonstrates high performance with AUROC values exceeding 99%. The study also explores adaptive attack strategies, demonstrating a comprehensive understanding of the problem and potential solutions.' Weaknesses='The approach, although demonstrating high AUROC, may lack novelty in terms of methodology. While the combination of statistical characteristics for detection seems effective, the paper could benefit from more comparative analysis with existing adversarial detection techniques. Additionally, the paper does not discuss how it handles real-world noise variations that may not follow the controlled settings of adversarial examples.' Questions='How does the proposed method perform under real-world conditions where noise might not be precisely controlled or as distinct as adversarial examples? Are there specific scenarios or types of adversarial attacks where this method might struggle?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel approach to flow-based models by enabling transportation between two arbitrary distributions, expanding beyond the commonly used normalizing flow. It innovatively applies the concept of optimal transport with practical applications in density ratio estimation and distribution interpolation. Empirical results demonstrate success in challenging high-dimensional tasks such as mutual information estimation, generative models, and image-to-image translation.' Weaknesses=\"While the approach is innovative, there are concerns regarding the depth of theoretical foundation provided for the model's ability to achieve true optimal transport. Questions about its general performance against existing methods in the field of density ratio estimation remain, with limited benchmarks against state-of-the-art methods. Additionally, while experimental demonstrations are included, some may argue that more varied datasets or settings are needed to assert general applicability.\" Questions=\"What are the limitations of the proposed method in terms of dimensional scalability or computational efficiency when applied to very high-dimensional data? Given the variety of potential applications, how does the model's performance compare to specialized methods in each area, such as density ratio estimation? Can you provide more theoretical backing or guarantees on the optimality of the transport map learned by the proposed flow network, or clarify the assumptions underpinning these results?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel approach to flow-based models by enabling transportation between two arbitrary distributions, expanding beyond the commonly used normalizing flow. It innovatively applies the concept of optimal transport with practical applications in density ratio estimation and distribution interpolation. Empirical results demonstrate success in challenging high-dimensional tasks such as mutual information estimation, generative models, and image-to-image translation.' Weaknesses=\"While the approach is innovative, there are concerns regarding the depth of theoretical foundation provided for the model's ability to achieve true optimal transport. Questions about its general performance against existing methods in the field of density ratio estimation remain, with limited benchmarks against state-of-the-art methods. Additionally, while experimental demonstrations are included, some may argue that more varied datasets or settings are needed to assert general applicability.\" Questions=\"What are the limitations of the proposed method in terms of dimensional scalability or computational efficiency when applied to very high-dimensional data? Given the variety of potential applications, how does the model's performance compare to specialized methods in each area, such as density ratio estimation? Can you provide more theoretical backing or guarantees on the optimality of the transport map learned by the proposed flow network, or clarify the assumptions underpinning these results?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper provides an innovative enhancement to the rSLDS model, effectively addressing its limitations to make it more suitable for trial-varying data. The integration of a semi-Markov discrete state process with latent geometry is novel and paves the way for the irSLDS model class, which shows promise in both synthetic and real data applications. The utilization of PDE theory for deriving a semi-parametric formulation enhances the model's efficiency, demonstrating the strong interdisciplinary nature of the work. The results on mice electrophysiological data are compelling, uncovering non-stationary processes in neural activity.\" Weaknesses=\"The complexity introduced by the incorporation of PDE theory and semi-Markov processes may pose a significant challenge for practitioners who are not well-versed in these areas, possibly limiting the method's accessibility. The abstract does not provide detailed information on how the model compares quantitatively to existing methods in terms of performance metrics. While the validation on synthetic data is mentioned, the lack of detailed comparative analysis against current state-of-the-art methods limits a complete understanding of the performance improvements gained.\" Questions='1. How does the irSLDS model performance compare quantitatively with other state-of-the-art methods on the same datasets? Are there specific performance metrics where it significantly outperforms others?\\n2. Does the model maintain its computational efficiency when scaled to larger datasets or models with increased complexity of biological data?\\n3. Can this methodology be easily transferred to other types of neural or biological data outside of electrophysiological contexts?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper provides an innovative enhancement to the rSLDS model, effectively addressing its limitations to make it more suitable for trial-varying data. The integration of a semi-Markov discrete state process with latent geometry is novel and paves the way for the irSLDS model class, which shows promise in both synthetic and real data applications. The utilization of PDE theory for deriving a semi-parametric formulation enhances the model's efficiency, demonstrating the strong interdisciplinary nature of the work. The results on mice electrophysiological data are compelling, uncovering non-stationary processes in neural activity.\" Weaknesses=\"The complexity introduced by the incorporation of PDE theory and semi-Markov processes may pose a significant challenge for practitioners who are not well-versed in these areas, possibly limiting the method's accessibility. The abstract does not provide detailed information on how the model compares quantitatively to existing methods in terms of performance metrics. While the validation on synthetic data is mentioned, the lack of detailed comparative analysis against current state-of-the-art methods limits a complete understanding of the performance improvements gained.\" Questions='1. How does the irSLDS model performance compare quantitatively with other state-of-the-art methods on the same datasets? Are there specific performance metrics where it significantly outperforms others?\\n2. Does the model maintain its computational efficiency when scaled to larger datasets or models with increased complexity of biological data?\\n3. Can this methodology be easily transferred to other types of neural or biological data outside of electrophysiological contexts?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper introduces a novel method for unsupervised learning of object-centric representations in dynamic visual scenes with a 3D focus, utilizing a differentiable volume rendering framework. The incorporation of geometric structures and object-centric learning is a significant advancement over existing 2D methods.\\n\\nThe approach seems to address the limitations of 2D scene decomposition by allowing for the manipulation of geometric shapes and motion trajectories post-training, which existing methods cannot achieve. This makes the method highly applicable for more advanced tasks in dynamic scene understanding.\\n\\nMoreover, the DynaVol model's ability to outperform existing approaches in decomposition tasks positions it as a potentially impactful contribution to this research area.\" Weaknesses=\"The abstract does not provide detailed information on the datasets or the scale of experiments conducted. Extensive evaluations across diverse settings and scenarios would strengthen the claim of superior performance.\\n\\nWhile the method seems promising, the paper might lack clarity on specific implementation details, particularly concerning the evolution of voxel features over time and the integration process between voxel and global features through the NeRF decoder. Further explanations and supplementary materials might be needed.\\n\\nThe generality of the method across various scenes, especially in more complex and cluttered environments, remains to be fully validated. The model's dependence on slot attention and how effectively it scales with the number of objects or complexity of the scene is not clear from the abstract.\" Questions='How does the method handle cluttered scenes or multiple overlapping objects, and does it maintain high performance when scaling to a larger number of objects in the scene?\\n\\nCan DynaVol be integrated with learned neural rendering methods beyond NeRF, and what would be the implications for rendering quality and computational efficiency?\\n\\nDoes the method require specific hardware for optimal performance, such as GPUs with specific memory capacities, due to the voxelization process? If so, how does this affect its accessibility for broader applications?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper introduces a novel method for unsupervised learning of object-centric representations in dynamic visual scenes with a 3D focus, utilizing a differentiable volume rendering framework. The incorporation of geometric structures and object-centric learning is a significant advancement over existing 2D methods.\\n\\nThe approach seems to address the limitations of 2D scene decomposition by allowing for the manipulation of geometric shapes and motion trajectories post-training, which existing methods cannot achieve. This makes the method highly applicable for more advanced tasks in dynamic scene understanding.\\n\\nMoreover, the DynaVol model's ability to outperform existing approaches in decomposition tasks positions it as a potentially impactful contribution to this research area.\" Weaknesses=\"The abstract does not provide detailed information on the datasets or the scale of experiments conducted. Extensive evaluations across diverse settings and scenarios would strengthen the claim of superior performance.\\n\\nWhile the method seems promising, the paper might lack clarity on specific implementation details, particularly concerning the evolution of voxel features over time and the integration process between voxel and global features through the NeRF decoder. Further explanations and supplementary materials might be needed.\\n\\nThe generality of the method across various scenes, especially in more complex and cluttered environments, remains to be fully validated. The model's dependence on slot attention and how effectively it scales with the number of objects or complexity of the scene is not clear from the abstract.\" Questions='How does the method handle cluttered scenes or multiple overlapping objects, and does it maintain high performance when scaling to a larger number of objects in the scene?\\n\\nCan DynaVol be integrated with learned neural rendering methods beyond NeRF, and what would be the implications for rendering quality and computational efficiency?\\n\\nDoes the method require specific hardware for optimal performance, such as GPUs with specific memory capacities, due to the voxelization process? If so, how does this affect its accessibility for broader applications?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The proposed framework P-Seg is simple yet effective for open-vocabulary semantic segmentation, allowing the system to generalize well across different datasets without needing fine-tuning. The approach achieves state-of-the-art results on well-known benchmarks such as Pascal VOC, Pascal Context, and COCO. The method also shows scalability with data and benefits from self-training. The promise of making code and demonstrations available adds transparency and reproducibility to the research.' Weaknesses=\"While the framework is conceptually simple and demonstrates strong performance, the novelty of the approach may be limited compared to existing methods. The paper does not provide detailed insights into certain architectural decisions or how they specifically contribute to the enhanced performance. It also lacks comparison with very recent methods which may also claim state-of-the-art performance. Furthermore, technical details that could help in reproducing the results or understanding the model's dynamics better are somewhat missing, such as the specifics of pseudo-mask generation.\" Questions='How does P-Seg handle the generation of pseudo-masks in more detail, and what is the impact of this process on the final performance? Are there any specific data augmentation techniques or pre-processing steps that significantly influence the results? Is there any particular reason some recent methods were not included in the comparisons, and how does P-Seg compare directly against them?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The proposed framework P-Seg is simple yet effective for open-vocabulary semantic segmentation, allowing the system to generalize well across different datasets without needing fine-tuning. The approach achieves state-of-the-art results on well-known benchmarks such as Pascal VOC, Pascal Context, and COCO. The method also shows scalability with data and benefits from self-training. The promise of making code and demonstrations available adds transparency and reproducibility to the research.' Weaknesses=\"While the framework is conceptually simple and demonstrates strong performance, the novelty of the approach may be limited compared to existing methods. The paper does not provide detailed insights into certain architectural decisions or how they specifically contribute to the enhanced performance. It also lacks comparison with very recent methods which may also claim state-of-the-art performance. Furthermore, technical details that could help in reproducing the results or understanding the model's dynamics better are somewhat missing, such as the specifics of pseudo-mask generation.\" Questions='How does P-Seg handle the generation of pseudo-masks in more detail, and what is the impact of this process on the final performance? Are there any specific data augmentation techniques or pre-processing steps that significantly influence the results? Is there any particular reason some recent methods were not included in the comparisons, and how does P-Seg compare directly against them?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper proposes ShareFormer, a novel Transformer for image restoration characterized by shared attention maps among neighboring blocks, which significantly reduces latency while retaining trainability and enhancing performance. The approach is backed by strong experimental results, demonstrating the efficacy, efficiency, and ease of training of ShareFormer. The authors also address some of the typical constraints of current Transformer-based models such as high inference time.\\nThe provision for open-sourcing the code and pre-trained models contributes to the reproducibility and potential future work.' Weaknesses='The proposed innovations, particularly concerning shared attention maps, might be perceived as incremental, considering similar concepts have been explored in other works like ELAN. While lesion studies are conducted, the paper could be reinforced by more extensive ablation studies and comparisons to further substantiate the unique contributions and advantages of ShareFormer. Certain aspects of the implementation, as explained, may somewhat lack clarity, especially regarding how shared attention is operationally and sequentially incorporated. Potential improvements in the presentation could further enhance comprehension.' Questions='What are the primary differences between the shared attention mechanisms of ShareFormer and those in other models like ELAN? Can the authors expand on quantitative and qualitative ways ShareFormer’s efficiency and locality biases are tangibly manifest? How do they address potential discrepancies when leveraging disparate context sizes for training compared to other competing methods? If certain implementation strategies such as progressive learning enhance results, to what extent do they contribute to ShareFormer’s success relative to others without analogous adaptive learning dynamics? Additionally, figures within the presentation raised certain ambiguities around attention map articulation which might benefit from revision.'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper proposes ShareFormer, a novel Transformer for image restoration characterized by shared attention maps among neighboring blocks, which significantly reduces latency while retaining trainability and enhancing performance. The approach is backed by strong experimental results, demonstrating the efficacy, efficiency, and ease of training of ShareFormer. The authors also address some of the typical constraints of current Transformer-based models such as high inference time.\\nThe provision for open-sourcing the code and pre-trained models contributes to the reproducibility and potential future work.' Weaknesses='The proposed innovations, particularly concerning shared attention maps, might be perceived as incremental, considering similar concepts have been explored in other works like ELAN. While lesion studies are conducted, the paper could be reinforced by more extensive ablation studies and comparisons to further substantiate the unique contributions and advantages of ShareFormer. Certain aspects of the implementation, as explained, may somewhat lack clarity, especially regarding how shared attention is operationally and sequentially incorporated. Potential improvements in the presentation could further enhance comprehension.' Questions='What are the primary differences between the shared attention mechanisms of ShareFormer and those in other models like ELAN? Can the authors expand on quantitative and qualitative ways ShareFormer’s efficiency and locality biases are tangibly manifest? How do they address potential discrepancies when leveraging disparate context sizes for training compared to other competing methods? If certain implementation strategies such as progressive learning enhance results, to what extent do they contribute to ShareFormer’s success relative to others without analogous adaptive learning dynamics? Additionally, figures within the presentation raised certain ambiguities around attention map articulation which might benefit from revision.'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths=\"1. The paper introduces Diff-SR, an innovative approach for arbitrary-scale super-resolution utilizing pre-trained diffusion models without needing additional training.\\n2. The approach is motivated by a novel insight regarding noise injection into low-resolution images, which appears to lead to significant performance improvements over current state-of-the-art solutions.\\n3. Extensive experiments demonstrating Diff-SR's superiority and flexibility under diverse conditions add to the credibility of the work.\" Weaknesses='1. The paper relies on pre-trained DGMs, which may not be accessible or optimal for all types of SR tasks where training data characteristics might differ significantly from those on which the DGMs were trained.\\n2. There is limited discussion on the potential computational overhead and runtime implications of the diffusion-based approach, especially in real-time or resource-constrained environments.\\n3. The trade-offs involved in determining the `optimal` amount of added noise are explained theoretically, yet practical guidelines or automated methods for this selection process need more emphasis.' Questions='1. Can Diff-SR handle scenarios where the characteristics of the input data differ substantially from those of the training data for the pre-trained DGM? Are there any limitations regarding the types of images or resolution scales where Diff-SR may struggle?\\n2. How does the Perceptual Recoverable Field (PRF) compare to existing metrics used in SR tasks, and what is the intuition behind its effectiveness?\\n3. The paper alludes to a theoretical analysis underpinning the proposed approach; can the authors provide more detail or supplementary material that further elucidates this analysis?\\n4. How might the approach adapt or extend beyond visual data, potentially to multimodal scenarios where super-resolution principles could apply to audio or text?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths=\"1. The paper introduces Diff-SR, an innovative approach for arbitrary-scale super-resolution utilizing pre-trained diffusion models without needing additional training.\\n2. The approach is motivated by a novel insight regarding noise injection into low-resolution images, which appears to lead to significant performance improvements over current state-of-the-art solutions.\\n3. Extensive experiments demonstrating Diff-SR's superiority and flexibility under diverse conditions add to the credibility of the work.\" Weaknesses='1. The paper relies on pre-trained DGMs, which may not be accessible or optimal for all types of SR tasks where training data characteristics might differ significantly from those on which the DGMs were trained.\\n2. There is limited discussion on the potential computational overhead and runtime implications of the diffusion-based approach, especially in real-time or resource-constrained environments.\\n3. The trade-offs involved in determining the `optimal` amount of added noise are explained theoretically, yet practical guidelines or automated methods for this selection process need more emphasis.' Questions='1. Can Diff-SR handle scenarios where the characteristics of the input data differ substantially from those of the training data for the pre-trained DGM? Are there any limitations regarding the types of images or resolution scales where Diff-SR may struggle?\\n2. How does the Perceptual Recoverable Field (PRF) compare to existing metrics used in SR tasks, and what is the intuition behind its effectiveness?\\n3. The paper alludes to a theoretical analysis underpinning the proposed approach; can the authors provide more detail or supplementary material that further elucidates this analysis?\\n4. How might the approach adapt or extend beyond visual data, potentially to multimodal scenarios where super-resolution principles could apply to audio or text?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper presents a novel analysis of the impact of output length in RLHF settings, which highlights an important aspect of this technique that has been underexplored. By investigating the correlation between reward and response length, it sheds light on underlying biases present in reward models trained on popular preference datasets. The empirical results are compelling and demonstrate the significant role that length plays in enhancements in perceived helpfulness, which is a critical finding for researchers and practitioners in the field.' Weaknesses='The presentation could benefit from improved clarity, particularly in its use of figures and tables. Several abbreviations and specific aspects of the experimental setup were not immediately clear, which could be a barrier for readers fully understanding the methodologies and conclusions. Additionally, while the paper explores interventions to control for length in reward optimization, it does not provide a clear set of practical guidelines or principles based on these findings, leaving some ambiguity about their practical applicability and relevance to ongoing research efforts.' Questions='How might one quantitatively differentiate contributions to reward improvement between length and other factors, like linguistic richness or content coverage? Are there experimental controls or analyses that could further isolate these variables?\\nCould the findings have implications for other RLHF tasks, such as harmlessness or truthfulness, where length might not be an obvious heuristic? Would an investigation into these aspects reveal other biases inherent in reward conditioning?\\nDo variations in task constraints, such as response time limits or domain-specific content requirements, significantly affect the interplay between length and reward?\\nWhat are some potential directions for future research to develop reward models that inherently incorporate length-normalizing features, thereby eliminating the need for additional interventions?\\nAre the findings largely applicable across different types of language models, including those of varying sizes and training regimes? Or are the observations primarily relevant to models closely related to those used in the study?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper presents a novel analysis of the impact of output length in RLHF settings, which highlights an important aspect of this technique that has been underexplored. By investigating the correlation between reward and response length, it sheds light on underlying biases present in reward models trained on popular preference datasets. The empirical results are compelling and demonstrate the significant role that length plays in enhancements in perceived helpfulness, which is a critical finding for researchers and practitioners in the field.' Weaknesses='The presentation could benefit from improved clarity, particularly in its use of figures and tables. Several abbreviations and specific aspects of the experimental setup were not immediately clear, which could be a barrier for readers fully understanding the methodologies and conclusions. Additionally, while the paper explores interventions to control for length in reward optimization, it does not provide a clear set of practical guidelines or principles based on these findings, leaving some ambiguity about their practical applicability and relevance to ongoing research efforts.' Questions='How might one quantitatively differentiate contributions to reward improvement between length and other factors, like linguistic richness or content coverage? Are there experimental controls or analyses that could further isolate these variables?\\nCould the findings have implications for other RLHF tasks, such as harmlessness or truthfulness, where length might not be an obvious heuristic? Would an investigation into these aspects reveal other biases inherent in reward conditioning?\\nDo variations in task constraints, such as response time limits or domain-specific content requirements, significantly affect the interplay between length and reward?\\nWhat are some potential directions for future research to develop reward models that inherently incorporate length-normalizing features, thereby eliminating the need for additional interventions?\\nAre the findings largely applicable across different types of language models, including those of varying sizes and training regimes? Or are the observations primarily relevant to models closely related to those used in the study?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper tackles the important issue of representation distortion in Graph Neural Networks as graphs evolve, which is a significant problem for maintaining model accuracy over time. The theoretical foundations laid to establish a lower bound on representation distortion demonstrate robustness and depth of insight. The introduction of the Smart framework, utilizing a self-supervised approach for graph reconstruction, effectively addresses the identified problem, and empirical results on synthetic and real-world graphs showcase its practical benefits.' Weaknesses='The paper could benefit from more comprehensive evaluations against a wider range of baseline methods beyond simple pre-trained RNNs for temporal distortion estimation. Some assumptions made in the theoretical analysis, though sufficient for deriving results, may limit the applicability of conclusions in more complex real-world scenarios. Although the presentation is generally clear, further elaboration on the derivations and notations, especially the relationship between theoretical concepts and empirical observations, would enhance intelligibility for readers less familiar with the field.' Questions='How generalizable are the lower-bound theoretic results across different graph types beyond those tested? Are there particular types of graph evolution (e.g., scale-free networks, highly dynamic graphs) where the approach might struggle or need specific adaptations?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper tackles the important issue of representation distortion in Graph Neural Networks as graphs evolve, which is a significant problem for maintaining model accuracy over time. The theoretical foundations laid to establish a lower bound on representation distortion demonstrate robustness and depth of insight. The introduction of the Smart framework, utilizing a self-supervised approach for graph reconstruction, effectively addresses the identified problem, and empirical results on synthetic and real-world graphs showcase its practical benefits.' Weaknesses='The paper could benefit from more comprehensive evaluations against a wider range of baseline methods beyond simple pre-trained RNNs for temporal distortion estimation. Some assumptions made in the theoretical analysis, though sufficient for deriving results, may limit the applicability of conclusions in more complex real-world scenarios. Although the presentation is generally clear, further elaboration on the derivations and notations, especially the relationship between theoretical concepts and empirical observations, would enhance intelligibility for readers less familiar with the field.' Questions='How generalizable are the lower-bound theoretic results across different graph types beyond those tested? Are there particular types of graph evolution (e.g., scale-free networks, highly dynamic graphs) where the approach might struggle or need specific adaptations?'\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Invalid \\escape: line 1 column 604 (char 603)\n",
      "Content:\n",
      "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"The paper addresses the important problem of estimating individualized treatment effects (ITE) considering unobserved confounders, which is pivotal in causal inference domains. The use of diffusion models to capture latent spaces is novel in this context and the experiments conducted show consistent improvements over existing methods.\", \"Weaknesses\": \"The writing lacks clarity in certain sections, leading to potential misunderstandings of the methodology and results. For instance, key metrics like $\\sqrt{\\\\epsilon_{PEHE}}$ and $\\\\epsilon_{ATE}$ are not defined clearly in the abstract. The theoretical foundation behind using diffusion models for unobserved confounders needs further elaboration and comparison to previous techniques seems to be somewhat superficial. The paper could benefit from improved presentation and detail in the motivation and methodology.\", \"Questions\": \"How robust is the proposed method to changes in the priors used for the diffusion model? Furthermore, what are the implications of this approach in terms of computational complexity compared to traditional methods? Lastly, could the authors clarify the details of the stochastic process used in the diffusion model?\"}\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel benchmark for multi-modal continual learning, which can serve as a valuable resource for the research community. The proposed method effectively mitigates catastrophic forgetting by leveraging multiple modalities, which is demonstrated through empirical results. The approach of integrating and aligning information from different modalities based on relational structural similarities is innovative and provides a strong baseline.' Weaknesses=\"The paper could benefit from a more extensive evaluation on diverse datasets rather than focusing on a single one. Moreover, the comparison with other state-of-the-art methods is somewhat limited, and inclusion of recent approaches could strengthen the paper's claims. The design decisions for the integration and alignment method are not thoroughly explained, which might leave readers questioning the underlying methodology.\" Questions='How does the proposed method perform when applied to more complex datasets or real-world scenarios? Is there a particular reason only one dataset was used for demonstration? Additionally, how does the method handle the presence of noise or incomplete data in one or more modalities?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel benchmark for multi-modal continual learning, which can serve as a valuable resource for the research community. The proposed method effectively mitigates catastrophic forgetting by leveraging multiple modalities, which is demonstrated through empirical results. The approach of integrating and aligning information from different modalities based on relational structural similarities is innovative and provides a strong baseline.' Weaknesses=\"The paper could benefit from a more extensive evaluation on diverse datasets rather than focusing on a single one. Moreover, the comparison with other state-of-the-art methods is somewhat limited, and inclusion of recent approaches could strengthen the paper's claims. The design decisions for the integration and alignment method are not thoroughly explained, which might leave readers questioning the underlying methodology.\" Questions='How does the proposed method perform when applied to more complex datasets or real-world scenarios? Is there a particular reason only one dataset was used for demonstration? Additionally, how does the method handle the presence of noise or incomplete data in one or more modalities?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a novel approach to addressing the problem of miscalibration and brittleness of language model prompting by introducing Gen-Z, a generative prompting framework for zero-shot text classification. The approach allows for the integration of additional contextual information about labels, which enhances the robustness of the method. The empirical results demonstrate that Gen-Z consistently outperforms zero-shot and few-shot baselines across various benchmarks.' Weaknesses=\"One weakness is the lack of clarity on the specific scenarios or tasks where Gen-Z offers the most significant advantages over alternative methods. While the paper highlights the framework's ability to incorporate contextual information, there may be limitations in cases where such information is not readily available or easily integrated. Additionally, while the results are promising, they may not be sufficiently detailed to fully validate the generalizability of the approach across diverse NLP tasks.\" Questions='How does Gen-Z handle situations where there is limited or no additional contextual information available for label descriptions? Additionally, can the framework be extended to tasks beyond text classification, such as text generation or sequence labeling?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a novel approach to addressing the problem of miscalibration and brittleness of language model prompting by introducing Gen-Z, a generative prompting framework for zero-shot text classification. The approach allows for the integration of additional contextual information about labels, which enhances the robustness of the method. The empirical results demonstrate that Gen-Z consistently outperforms zero-shot and few-shot baselines across various benchmarks.' Weaknesses=\"One weakness is the lack of clarity on the specific scenarios or tasks where Gen-Z offers the most significant advantages over alternative methods. While the paper highlights the framework's ability to incorporate contextual information, there may be limitations in cases where such information is not readily available or easily integrated. Additionally, while the results are promising, they may not be sufficiently detailed to fully validate the generalizability of the approach across diverse NLP tasks.\" Questions='How does Gen-Z handle situations where there is limited or no additional contextual information available for label descriptions? Additionally, can the framework be extended to tasks beyond text classification, such as text generation or sequence labeling?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel dataset, LaDe, which has potential high impact by filling a gap in the availability of real-world, large-scale, last-mile delivery datasets. The comprehensive and diverse nature of the dataset enables research across multiple domains such as logistics, supply chain management, and spatio-temporal data mining.' Weaknesses='The originality primarily stems from the dataset creation rather than novel methodologies or new techniques in last-mile delivery optimization. Furthermore, the dataset lacks some detailed contextual information such as road network data and specific handling of dynamic urban environments which might limit its applicability.' Questions=\"Will there be periodic updates to LaDe to ensure that it reflects the latest operational details and trends in last-mile delivery logistics? How will potential privacy concerns be addressed given the dataset's real-world origins and detail?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel dataset, LaDe, which has potential high impact by filling a gap in the availability of real-world, large-scale, last-mile delivery datasets. The comprehensive and diverse nature of the dataset enables research across multiple domains such as logistics, supply chain management, and spatio-temporal data mining.' Weaknesses='The originality primarily stems from the dataset creation rather than novel methodologies or new techniques in last-mile delivery optimization. Furthermore, the dataset lacks some detailed contextual information such as road network data and specific handling of dynamic urban environments which might limit its applicability.' Questions=\"Will there be periodic updates to LaDe to ensure that it reflects the latest operational details and trends in last-mile delivery logistics? How will potential privacy concerns be addressed given the dataset's real-world origins and detail?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=5 Strengths='1. The paper proposes a novel method for generating multi-view datasets without relying on annotations, which allows for scalable and cost-effective pretraining.\\n2. The method shows significant improvements in dense geometric tasks and semantic segmentation, even when compared to expensive and annotated datasets.\\n3. The dataset curation method proves scalable, with larger datasets such as MIMIC-3M leading to better performance.\\n4. The approach is applicable to both real-world videos and simulated environments, increasing its versatility.' Weaknesses='1. While the paper demonstrates improvements in dense task performance, it focuses less on direct comparisons for object understanding tasks against top-tier pre-trained models, leaving some gaps in understanding its full impact on a broad range of tasks.\\n2. The presentation of results could be more comprehensive, including statistical analyses and broader evaluation in non-dense tasks for a more robust assessment.\\n3. The method may require considerable computational resources for dataset generation and model training, which is not discussed in detail.' Questions='1. Can the proposed dataset-curation approach be adapted for learning representations for non-visual modalities?\\n2. How well does the proposed method integrate with or complement existing multi-modal learning frameworks?\\n3. What are the computational requirements for scaling the method to even larger datasets, and are there any technical challenges in doing so?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=5 Strengths='1. The paper proposes a novel method for generating multi-view datasets without relying on annotations, which allows for scalable and cost-effective pretraining.\\n2. The method shows significant improvements in dense geometric tasks and semantic segmentation, even when compared to expensive and annotated datasets.\\n3. The dataset curation method proves scalable, with larger datasets such as MIMIC-3M leading to better performance.\\n4. The approach is applicable to both real-world videos and simulated environments, increasing its versatility.' Weaknesses='1. While the paper demonstrates improvements in dense task performance, it focuses less on direct comparisons for object understanding tasks against top-tier pre-trained models, leaving some gaps in understanding its full impact on a broad range of tasks.\\n2. The presentation of results could be more comprehensive, including statistical analyses and broader evaluation in non-dense tasks for a more robust assessment.\\n3. The method may require considerable computational resources for dataset generation and model training, which is not discussed in detail.' Questions='1. Can the proposed dataset-curation approach be adapted for learning representations for non-visual modalities?\\n2. How well does the proposed method integrate with or complement existing multi-modal learning frameworks?\\n3. What are the computational requirements for scaling the method to even larger datasets, and are there any technical challenges in doing so?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses an important and complex problem in the RNA field, combining predictive modeling with generative design. The use of transformer architectures for tissue-specific splicing prediction is innovative and significant. The application of Bayesian Optimization for RNA splicing outcome design shows the potential to advance both prediction and therapeutic design, indicating a promising direction for future research.' Weaknesses='The manuscript lacks a thorough comparative analysis with existing models, making it difficult to assess the real-world impact of the proposed approach. While the methodology is complex and depicts a novel direction, some aspects are not sufficiently detailed or validated, like the precise improvement over existing state-of-the-art methods. The predictive accuracy for tissue-specific splicing is mentioned as not very accurate but is not quantified in comparison to benchmarks. Additionally, the rationale and potential trade-offs of using transformer models over existing methods, such as SpliceAI in certain contexts, are not clearly addressed.' Questions='1. How does the performance of TrASPr compare quantitatively to established models across different tissues?\\n2. Can the BO method be adjusted to target specific tissues for splicing alterations more effectively? What might be the limitations or challenges in doing so?\\n3. Are there specific instances or datasets beyond the example of Daam1 mutations that could be explored to further validate the generative capabilities?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses an important and complex problem in the RNA field, combining predictive modeling with generative design. The use of transformer architectures for tissue-specific splicing prediction is innovative and significant. The application of Bayesian Optimization for RNA splicing outcome design shows the potential to advance both prediction and therapeutic design, indicating a promising direction for future research.' Weaknesses='The manuscript lacks a thorough comparative analysis with existing models, making it difficult to assess the real-world impact of the proposed approach. While the methodology is complex and depicts a novel direction, some aspects are not sufficiently detailed or validated, like the precise improvement over existing state-of-the-art methods. The predictive accuracy for tissue-specific splicing is mentioned as not very accurate but is not quantified in comparison to benchmarks. Additionally, the rationale and potential trade-offs of using transformer models over existing methods, such as SpliceAI in certain contexts, are not clearly addressed.' Questions='1. How does the performance of TrASPr compare quantitatively to established models across different tissues?\\n2. Can the BO method be adjusted to target specific tissues for splicing alterations more effectively? What might be the limitations or challenges in doing so?\\n3. Are there specific instances or datasets beyond the example of Daam1 mutations that could be explored to further validate the generative capabilities?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel multi-modal framework and a hierarchical knowledge distillation (HKD) framework that facilitates communication by improving continuous sign language recognition (CSLR), promoting inclusivity in the hearing-impaired community. The integration of video, keypoints, and optical flow modalities offers a robust feature extraction approach, and the experiments demonstrate state-of-the-art performance on benchmark datasets.' Weaknesses='While the paper shows a strong experimental setup, the abstract lacks detailed methodology or comprehensive explanations of the fusion techniques and specific challenges addressed uniquely by this work. Additionally, the practical challenges related to implementation complexity and computational limitations of multi-modal integration may not have been sufficiently explored.' Questions='1. Can you provide more details on the hierarchical knowledge distillation process? How do the different modalities contribute at each stage?\\n2. The abstract mentions evaluating on three datasets. Are there any domain-specific challenges faced in each dataset, and how does the proposed system address these?\\n3. What are the specific fusion techniques explored, and which were found to be the most and least effective?\\n4. How does the system handle variations in signing speed or dialects across different users?\\n5. Are there considerations for implementation in real-time settings, particularly regarding computational efficiency as compared to existing solutions? What trade-offs are observed between performance and computational complexity?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel multi-modal framework and a hierarchical knowledge distillation (HKD) framework that facilitates communication by improving continuous sign language recognition (CSLR), promoting inclusivity in the hearing-impaired community. The integration of video, keypoints, and optical flow modalities offers a robust feature extraction approach, and the experiments demonstrate state-of-the-art performance on benchmark datasets.' Weaknesses='While the paper shows a strong experimental setup, the abstract lacks detailed methodology or comprehensive explanations of the fusion techniques and specific challenges addressed uniquely by this work. Additionally, the practical challenges related to implementation complexity and computational limitations of multi-modal integration may not have been sufficiently explored.' Questions='1. Can you provide more details on the hierarchical knowledge distillation process? How do the different modalities contribute at each stage?\\n2. The abstract mentions evaluating on three datasets. Are there any domain-specific challenges faced in each dataset, and how does the proposed system address these?\\n3. What are the specific fusion techniques explored, and which were found to be the most and least effective?\\n4. How does the system handle variations in signing speed or dialects across different users?\\n5. Are there considerations for implementation in real-time settings, particularly regarding computational efficiency as compared to existing solutions? What trade-offs are observed between performance and computational complexity?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a unifying framework for inverse rendering using voxelization, which efficiently models geometry, materials, and illumination together. The use of Spherical Gaussians for representing light radiance is a novel approach, allowing for seamless integration into the framework. The method has demonstrated significant reductions in per-scene training time while maintaining high reconstruction quality, as evidenced by extensive experiments on diverse benchmarks.' Weaknesses=\"While the UniVoxel framework offers improved efficiency, it's not clear how it compares quality-wise to the best-performing methods currently available, aside from speed. There's also a potential lack of innovation, as many ideas seem like extensions of existing methodologies focused on voxelization frameworks. Furthermore, the paper would benefit from a clearer explanation of how it achieves effective integration of the components within the voxelization framework, particularly regarding how Spherical Gaussians outperform alternative representations like spherical harmonics.\" Questions='1. How does the proposed method generalize to scenes with more complex lighting conditions, such as multiple dynamic light sources?\\n2. Can the authors provide more details on the computational resources required to achieve the reported 18-minute training time? How does this scale with scene complexity?\\n3. What are the limitations of using Spherical Gaussians for light representation, especially in real-world scenarios with high dynamic range illumination?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a unifying framework for inverse rendering using voxelization, which efficiently models geometry, materials, and illumination together. The use of Spherical Gaussians for representing light radiance is a novel approach, allowing for seamless integration into the framework. The method has demonstrated significant reductions in per-scene training time while maintaining high reconstruction quality, as evidenced by extensive experiments on diverse benchmarks.' Weaknesses=\"While the UniVoxel framework offers improved efficiency, it's not clear how it compares quality-wise to the best-performing methods currently available, aside from speed. There's also a potential lack of innovation, as many ideas seem like extensions of existing methodologies focused on voxelization frameworks. Furthermore, the paper would benefit from a clearer explanation of how it achieves effective integration of the components within the voxelization framework, particularly regarding how Spherical Gaussians outperform alternative representations like spherical harmonics.\" Questions='1. How does the proposed method generalize to scenes with more complex lighting conditions, such as multiple dynamic light sources?\\n2. Can the authors provide more details on the computational resources required to achieve the reported 18-minute training time? How does this scale with scene complexity?\\n3. What are the limitations of using Spherical Gaussians for light representation, especially in real-world scenarios with high dynamic range illumination?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel approach to address dataset bias in diffusion models using time-dependent importance reweighting. This method improves the precision of outputs in diffusion models compared to previous approaches. The theoretical foundations and empirical evidence presented are strong, showing improvements over existing methodologies on benchmark datasets.' Weaknesses='The direct application to score-matching is noted as intractable, suggesting that the methodology may have limitations in usability or applicability under certain conditions. The integration with traditional score-matching methods may necessitate deeper understanding and further studies to fully harness its potential. Additionally, the analysis might lack exploration of potential limitations or specific conditions where the approach might underperform.' Questions='How does time-dependent importance reweighting impact the computational requirements of training diffusion models? Are there certain types of biases or datasets where this approach might not be as effective?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel approach to address dataset bias in diffusion models using time-dependent importance reweighting. This method improves the precision of outputs in diffusion models compared to previous approaches. The theoretical foundations and empirical evidence presented are strong, showing improvements over existing methodologies on benchmark datasets.' Weaknesses='The direct application to score-matching is noted as intractable, suggesting that the methodology may have limitations in usability or applicability under certain conditions. The integration with traditional score-matching methods may necessitate deeper understanding and further studies to fully harness its potential. Additionally, the analysis might lack exploration of potential limitations or specific conditions where the approach might underperform.' Questions='How does time-dependent importance reweighting impact the computational requirements of training diffusion models? Are there certain types of biases or datasets where this approach might not be as effective?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces the Online Sparse Residual Tree (OSRT), which builds on traditional RBF models by incorporating an online tree decomposition approach. The dynamic expansion of network depth to accommodate new data is innovative, and the use of incremental methods to explore RBF centers enhances both sparsity and accuracy. This approach offers potential improvements over existing online methods in handling streaming data.' Weaknesses='The abstract does not clearly outline the mathematical basis for the claimed improvements in efficiency and accuracy, nor does it specify the datasets or evaluation metrics used in the experiments. The paper would benefit from a detailed discussion on the theoretical framework and experimental setup. Additionally, the lack of comparisons with a broader range of state-of-the-art methods weakens the evaluation.' Questions='How does the OSRT model decide the optimal maximum depth, and how does it compare computationally with other state-of-the-art RBF networks? Are there any specific applications or domains where OSRT has a clear advantage? Could you provide more details on the datasets used and how they reflect real-world streaming data scenarios?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces the Online Sparse Residual Tree (OSRT), which builds on traditional RBF models by incorporating an online tree decomposition approach. The dynamic expansion of network depth to accommodate new data is innovative, and the use of incremental methods to explore RBF centers enhances both sparsity and accuracy. This approach offers potential improvements over existing online methods in handling streaming data.' Weaknesses='The abstract does not clearly outline the mathematical basis for the claimed improvements in efficiency and accuracy, nor does it specify the datasets or evaluation metrics used in the experiments. The paper would benefit from a detailed discussion on the theoretical framework and experimental setup. Additionally, the lack of comparisons with a broader range of state-of-the-art methods weakens the evaluation.' Questions='How does the OSRT model decide the optimal maximum depth, and how does it compare computationally with other state-of-the-art RBF networks? Are there any specific applications or domains where OSRT has a clear advantage? Could you provide more details on the datasets used and how they reflect real-world streaming data scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper explores an interesting direction by integrating spatial information into the transformer model to address a key limitation of current AI systems in forming and utilizing episodic memory. It provides extensive experimental results across a variety of tasks, demonstrating the practical benefits of the proposed models. The Adaptive Memory Allocator is a novel method that contributes to optimizing memory utilization.' Weaknesses='The paper could improve in explaining the theoretical contributions and detailing the differences of the Spatially-Aware Transformer compared to existing transformer architectures. While the experiments show improvements, the specific advantages in different scenarios could be better articulated. The presentation of the model, especially in how it integrates spatial data, could be clearer. More comparative analysis with existing methods would strengthen the claims.' Questions='How does the Spatially-Aware Transformer model specifically incorporate spatial information into its architecture? Can you provide more clarity on what differentiates it from other spatial-temporal models? Additionally, how robust is the proposed method across varying types and scales of environments? More comparisons with established baselines would be beneficial to assess its effectiveness.'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper explores an interesting direction by integrating spatial information into the transformer model to address a key limitation of current AI systems in forming and utilizing episodic memory. It provides extensive experimental results across a variety of tasks, demonstrating the practical benefits of the proposed models. The Adaptive Memory Allocator is a novel method that contributes to optimizing memory utilization.' Weaknesses='The paper could improve in explaining the theoretical contributions and detailing the differences of the Spatially-Aware Transformer compared to existing transformer architectures. While the experiments show improvements, the specific advantages in different scenarios could be better articulated. The presentation of the model, especially in how it integrates spatial data, could be clearer. More comparative analysis with existing methods would strengthen the claims.' Questions='How does the Spatially-Aware Transformer model specifically incorporate spatial information into its architecture? Can you provide more clarity on what differentiates it from other spatial-temporal models? Additionally, how robust is the proposed method across varying types and scales of environments? More comparisons with established baselines would be beneficial to assess its effectiveness.'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper presents a novel approach to making neural networks' hidden states sparse and discrete, enhancing interpretability without much performance degradation. The introduction of codebook features and the successful validation on language models showcase a significant advancement in the field.\" Weaknesses='The paper focuses primarily on theoretical contributions and initial validations, with limited insights into potential practical challenges. Further details on scalability and integration into existing frameworks are needed to assess broader applicability.' Questions='How does the introduction of the codebook feature affect the overall computational complexity of the neural networks? Are there any noted trade-offs in terms of computational resources or efficiency?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper presents a novel approach to making neural networks' hidden states sparse and discrete, enhancing interpretability without much performance degradation. The introduction of codebook features and the successful validation on language models showcase a significant advancement in the field.\" Weaknesses='The paper focuses primarily on theoretical contributions and initial validations, with limited insights into potential practical challenges. Further details on scalability and integration into existing frameworks are needed to assess broader applicability.' Questions='How does the introduction of the codebook feature affect the overall computational complexity of the neural networks? Are there any noted trade-offs in terms of computational resources or efficiency?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses the important problem of partially annotated multi-label classification, which is challenging due to label imbalance and scarcity of annotations. The proposed PAPG framework effectively combines reinforcement learning and supervised learning. The introduction of local and global rewards is a strong point for addressing class imbalance.' Weaknesses='The abstract lacks specific details about the experimental setup and results that demonstrate the superiority of the proposed framework. The novelty of the approach in leveraging reinforcement learning specifically for this task could be highlighted more clearly. The presentation could be improved for better clarity on how the proposed method integrates the components and achieves its claims.' Questions='What specific datasets and evaluation metrics were used to validate the effectiveness of the PAPG framework? Did the authors compare PAPG against existing multi-label classification methods that handle partial annotations?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses the important problem of partially annotated multi-label classification, which is challenging due to label imbalance and scarcity of annotations. The proposed PAPG framework effectively combines reinforcement learning and supervised learning. The introduction of local and global rewards is a strong point for addressing class imbalance.' Weaknesses='The abstract lacks specific details about the experimental setup and results that demonstrate the superiority of the proposed framework. The novelty of the approach in leveraging reinforcement learning specifically for this task could be highlighted more clearly. The presentation could be improved for better clarity on how the proposed method integrates the components and achieves its claims.' Questions='What specific datasets and evaluation metrics were used to validate the effectiveness of the PAPG framework? Did the authors compare PAPG against existing multi-label classification methods that handle partial annotations?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The approach of leveraging high-resolution atomic protein structures and pretrained small molecule representations to simulate ligand-receptor interactions is innovative.\\n2. ProFSA demonstrated state-of-the-art performance across multiple tasks, indicating its robustness and applicability.\\n3. The method addresses the scarcity of protein-ligand complex data, which is a critical bottleneck in the field.' Weaknesses='1. The dependence on high-resolution protein structures might limit applicability to cases where such structures are unavailable.\\n2. The abstract does not provide detailed insights into the training process or model architecture, making it difficult to assess the potential limitations in the methodology.\\n3. The scalability of the approach in terms of computational resources is not discussed.' Questions='1. How does the model handle cases where high-resolution atomic structures are not available?\\n2. Could the methodology be extended to other types of interactions, such as protein-protein interactions?\\n3. Are there any specific limitations encountered when applying ProFSA to different types of proteins or ligands?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The approach of leveraging high-resolution atomic protein structures and pretrained small molecule representations to simulate ligand-receptor interactions is innovative.\\n2. ProFSA demonstrated state-of-the-art performance across multiple tasks, indicating its robustness and applicability.\\n3. The method addresses the scarcity of protein-ligand complex data, which is a critical bottleneck in the field.' Weaknesses='1. The dependence on high-resolution protein structures might limit applicability to cases where such structures are unavailable.\\n2. The abstract does not provide detailed insights into the training process or model architecture, making it difficult to assess the potential limitations in the methodology.\\n3. The scalability of the approach in terms of computational resources is not discussed.' Questions='1. How does the model handle cases where high-resolution atomic structures are not available?\\n2. Could the methodology be extended to other types of interactions, such as protein-protein interactions?\\n3. Are there any specific limitations encountered when applying ProFSA to different types of proteins or ligands?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The proposed Transitional Dictionary Learning (TDL) framework offers a novel approach to decomposing visual inputs into parts and implicit relations, which could significantly advance the field of unsupervised part segmentation. The use of a game-theoretic diffusion model to decompose visual inputs is innovative, and the introduction of new metrics such as clustering information gain and heuristic shape score adds value to the evaluation of models in this domain.' Weaknesses='While the new framework is promising, it has mainly been validated on abstract visual datasets rather than real-world visual data, which might limit its applicability. Furthermore, the presentation clarity could be improved, especially in explaining the game-theoretic diffusion model and its implementation details. Also, more direct comparisons with existing methods could strengthen the claims regarding outperforming state-of-the-art methods.' Questions='How does the method perform on more complex or real-world datasets beyond abstract compositional visual objects? What specific criteria were used for human evaluation, and how were these evaluations standardized?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The proposed Transitional Dictionary Learning (TDL) framework offers a novel approach to decomposing visual inputs into parts and implicit relations, which could significantly advance the field of unsupervised part segmentation. The use of a game-theoretic diffusion model to decompose visual inputs is innovative, and the introduction of new metrics such as clustering information gain and heuristic shape score adds value to the evaluation of models in this domain.' Weaknesses='While the new framework is promising, it has mainly been validated on abstract visual datasets rather than real-world visual data, which might limit its applicability. Furthermore, the presentation clarity could be improved, especially in explaining the game-theoretic diffusion model and its implementation details. Also, more direct comparisons with existing methods could strengthen the claims regarding outperforming state-of-the-art methods.' Questions='How does the method perform on more complex or real-world datasets beyond abstract compositional visual objects? What specific criteria were used for human evaluation, and how were these evaluations standardized?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces an original approach to address the limitations of existing NeRF models by shifting from image-based to point-based rendering. The proposed Generalizable neural Point Field (GPF) demonstrates improvements in rendering quality and consistency, particularly in handling occlusions and geometric discontinuities. The novel nonuniform log sampling strategy and learnable kernel for feature aggregation are significant contributions that enhance rendering speed and accuracy. Experiments conducted on three datasets further validate the effectiveness of the proposed method.' Weaknesses='The paper could benefit from more detailed discussions on the limitations and potential drawbacks of the proposed approach. The reliance on geometric priors for visibility modeling may impose constraints under certain conditions. While the experiments are promising, the evaluation is limited to only three datasets, which may not fully capture the generalizability and performance across diverse scenarios. Further clarification on the integration with existing techniques and comparison with other state-of-the-art methods is necessary.' Questions='What are the potential limitations or challenges of applying the Generalizable neural Point Field (GPF) approach to larger or more complex datasets? How does the GPF handle dynamic scenes or environments with rapidly changing lighting conditions? How does the proposed method compare with other point-based NeRF approaches in terms of computational efficiency and resource requirements?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces an original approach to address the limitations of existing NeRF models by shifting from image-based to point-based rendering. The proposed Generalizable neural Point Field (GPF) demonstrates improvements in rendering quality and consistency, particularly in handling occlusions and geometric discontinuities. The novel nonuniform log sampling strategy and learnable kernel for feature aggregation are significant contributions that enhance rendering speed and accuracy. Experiments conducted on three datasets further validate the effectiveness of the proposed method.' Weaknesses='The paper could benefit from more detailed discussions on the limitations and potential drawbacks of the proposed approach. The reliance on geometric priors for visibility modeling may impose constraints under certain conditions. While the experiments are promising, the evaluation is limited to only three datasets, which may not fully capture the generalizability and performance across diverse scenarios. Further clarification on the integration with existing techniques and comparison with other state-of-the-art methods is necessary.' Questions='What are the potential limitations or challenges of applying the Generalizable neural Point Field (GPF) approach to larger or more complex datasets? How does the GPF handle dynamic scenes or environments with rapidly changing lighting conditions? How does the proposed method compare with other point-based NeRF approaches in terms of computational efficiency and resource requirements?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed Label-encoding Risk Minimization (LRM) concept is innovative in its approach to addressing label insufficiency by leveraging the neural collapse phenomenon. The paper presents theoretical findings linking LRM to Empirical Risk Minimization (ERM), adding to its strength.' Weaknesses='The presentation may lack clarity in some sections, particularly in how the proposed method compares to existing solutions in detail. Additional experiments and baselines would strengthen the empirical demonstration.' Questions='How generalizable is the LRM approach across different types of data and tasks beyond those demonstrated? Can the approach be integrated into existing deep learning architectures seamlessly?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed Label-encoding Risk Minimization (LRM) concept is innovative in its approach to addressing label insufficiency by leveraging the neural collapse phenomenon. The paper presents theoretical findings linking LRM to Empirical Risk Minimization (ERM), adding to its strength.' Weaknesses='The presentation may lack clarity in some sections, particularly in how the proposed method compares to existing solutions in detail. Additional experiments and baselines would strengthen the empirical demonstration.' Questions='How generalizable is the LRM approach across different types of data and tasks beyond those demonstrated? Can the approach be integrated into existing deep learning architectures seamlessly?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper provides a significant advancement by extending the analysis of Jacobian behavior from just initialization to the entire training process, using recent results in random matrix theory. This generalization could have broad applications and offers a theoretical underpinning for the essential role of initialization in deep networks.' Weaknesses='The presentation, while rigorous, might be dense and challenging for those not well-versed in random matrix theory or deep learning initialization problems. Further explanations and perhaps illustrative examples could help in making the work more accessible to a broader audience.' Questions='How does the proposed framework for analyzing Jacobian stability during training apply to networks with various activation functions beyond those studied? Are there specific limitations when extending this analysis to different neural architecture designs?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper provides a significant advancement by extending the analysis of Jacobian behavior from just initialization to the entire training process, using recent results in random matrix theory. This generalization could have broad applications and offers a theoretical underpinning for the essential role of initialization in deep networks.' Weaknesses='The presentation, while rigorous, might be dense and challenging for those not well-versed in random matrix theory or deep learning initialization problems. Further explanations and perhaps illustrative examples could help in making the work more accessible to a broader audience.' Questions='How does the proposed framework for analyzing Jacobian stability during training apply to networks with various activation functions beyond those studied? Are there specific limitations when extending this analysis to different neural architecture designs?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a formalization of the open-set recognition (OSR) problem using learning theory, highlighting the shared goal between closed-set recognition (CSR) and OSR for generative models. The use of a neural Latent Gaussian Mixture Model (L-GMM) with a collaborative training algorithm is innovative and effectively designed to address the challenges of both CSR and OSR.' Weaknesses='The novelty of the approach may be limited, given that techniques involving Gaussian mixture models and collaborative training have been explored in related domains. Clarity in presentation can be improved to ensure the understanding of complex concepts, especially for readers who might not be familiar with generative models in this context.' Questions='How does the L-GMM framework handle outliers in the data or samples from unknown classes during the training process? Is there a possibility of extending this framework to other types of recognition tasks beyond image recognition and segmentation?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a formalization of the open-set recognition (OSR) problem using learning theory, highlighting the shared goal between closed-set recognition (CSR) and OSR for generative models. The use of a neural Latent Gaussian Mixture Model (L-GMM) with a collaborative training algorithm is innovative and effectively designed to address the challenges of both CSR and OSR.' Weaknesses='The novelty of the approach may be limited, given that techniques involving Gaussian mixture models and collaborative training have been explored in related domains. Clarity in presentation can be improved to ensure the understanding of complex concepts, especially for readers who might not be familiar with generative models in this context.' Questions='How does the L-GMM framework handle outliers in the data or samples from unknown classes during the training process? Is there a possibility of extending this framework to other types of recognition tasks beyond image recognition and segmentation?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper provides a strong theoretical foundation for Multi-domain text classification (MDTC), with a novel analysis using Rademacher complexity and margin discrepancy, which are both robust methods in machine learning theory. Empirical results confirm the efficacy of the proposed approach.\\n\\nThe use of margin discrepancy and the establishment of generalization bounds provide a new angle for understanding and improving MDTC tasks, potentially inspiring further research in the field.' Weaknesses='While the theoretical contribution is significant, the practical algorithmic innovation may seem incremental. The work primarily builds upon existing theories from domain adaptation without introducing entirely new methodologies or algorithms for MDTC. Additionally, the presentation could be enhanced, and certain areas might benefit from a more detailed explanation, particularly for readers less familiar with the theoretical concepts.' Questions='Can the approach and theoretical insights be extended to other machine learning tasks beyond text classification, potentially in domains with higher complexity or different data modalities?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper provides a strong theoretical foundation for Multi-domain text classification (MDTC), with a novel analysis using Rademacher complexity and margin discrepancy, which are both robust methods in machine learning theory. Empirical results confirm the efficacy of the proposed approach.\\n\\nThe use of margin discrepancy and the establishment of generalization bounds provide a new angle for understanding and improving MDTC tasks, potentially inspiring further research in the field.' Weaknesses='While the theoretical contribution is significant, the practical algorithmic innovation may seem incremental. The work primarily builds upon existing theories from domain adaptation without introducing entirely new methodologies or algorithms for MDTC. Additionally, the presentation could be enhanced, and certain areas might benefit from a more detailed explanation, particularly for readers less familiar with the theoretical concepts.' Questions='Can the approach and theoretical insights be extended to other machine learning tasks beyond text classification, potentially in domains with higher complexity or different data modalities?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper introduces a novel sample selection criterion, $\\\\alpha$-MMD, which enhances the generalization ability in low-budget scenarios. \\n2. The proposed method UPA demonstrates better performance compared to existing AL and SSAL methods under constrained annotation budgets. \\n3. It utilizes a modified Frank-Wolfe algorithm, known for its efficiency in optimization problems, which is a strong technical choice.' Weaknesses='1. While the $\\\\alpha$-MMD criterion is innovative, its integration and effectiveness across diverse real-world datasets and tasks could be further demonstrated.\\n2. The reliance on specific algorithms such as the Frank-Wolfe algorithm might limit the broader applicability of the approach due to its complexity.\\n3. Limited exploration of limitations or potential biases in the proposed methodology, especially in varying data distributions or domains.' Questions='1. How does the approach perform on domains other than those presented? Have there been tests in more varied environments to ensure generalizability?\\n2. Are there assumptions or limitations associated with using the modified Frank-Wolfe algorithm for the UPA method in various SSL scenarios?\\n3. Can the methodology efficiently scale with increasing data dimensionality and size, particularly in very large datasets?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper introduces a novel sample selection criterion, $\\\\alpha$-MMD, which enhances the generalization ability in low-budget scenarios. \\n2. The proposed method UPA demonstrates better performance compared to existing AL and SSAL methods under constrained annotation budgets. \\n3. It utilizes a modified Frank-Wolfe algorithm, known for its efficiency in optimization problems, which is a strong technical choice.' Weaknesses='1. While the $\\\\alpha$-MMD criterion is innovative, its integration and effectiveness across diverse real-world datasets and tasks could be further demonstrated.\\n2. The reliance on specific algorithms such as the Frank-Wolfe algorithm might limit the broader applicability of the approach due to its complexity.\\n3. Limited exploration of limitations or potential biases in the proposed methodology, especially in varying data distributions or domains.' Questions='1. How does the approach perform on domains other than those presented? Have there been tests in more varied environments to ensure generalizability?\\n2. Are there assumptions or limitations associated with using the modified Frank-Wolfe algorithm for the UPA method in various SSL scenarios?\\n3. Can the methodology efficiently scale with increasing data dimensionality and size, particularly in very large datasets?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents an innovative approach to maintaining model accuracy while using fully quantized training (FQT) for continual learning. The proposed technique effectively addresses the challenge of model accuracy degradation caused by aggressive quantization. The use of Hadamard transforms and stochastic rounding in low-precision training is a novel contribution.' Weaknesses=\"The paper might lack comprehensive comparisons to other state-of-the-art methods in fully quantized training or continual learning, making it challenging to contextualize the improvements. The abstract implies extensive experiments, but it's unclear if the discussed benchmarks provide a complete picture of performance across diverse tasks or settings.\" Questions='How does the proposed method compare to other state-of-the-art approaches in fully quantized training and continual learning? Are the techniques broadly applicable across different model architectures, or are there limitations to consider?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents an innovative approach to maintaining model accuracy while using fully quantized training (FQT) for continual learning. The proposed technique effectively addresses the challenge of model accuracy degradation caused by aggressive quantization. The use of Hadamard transforms and stochastic rounding in low-precision training is a novel contribution.' Weaknesses=\"The paper might lack comprehensive comparisons to other state-of-the-art methods in fully quantized training or continual learning, making it challenging to contextualize the improvements. The abstract implies extensive experiments, but it's unclear if the discussed benchmarks provide a complete picture of performance across diverse tasks or settings.\" Questions='How does the proposed method compare to other state-of-the-art approaches in fully quantized training and continual learning? Are the techniques broadly applicable across different model architectures, or are there limitations to consider?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The proposed algorithms demonstrate improvements in sample complexity for estimating collision probability while maintaining local differential privacy. This is a valuable contribution to the field of privacy-preserving data analysis.\\n2. The development of the first sequential testing algorithm for collision probability represents an important advancement, potentially enabling more efficient data analysis techniques.\\n3. Experimental results indicate that the proposed methods outperform existing approaches, suggesting practical applicability.' Weaknesses='1. While the algorithms are interesting, the presentation lacks sufficient detail on certain technical aspects, making it challenging to fully assess their effectiveness.\\n2. The experiments, while supportive of the proposed methods, could benefit from more comprehensive evaluations across a broader range of datasets and scenarios.' Questions='1. Can the authors provide more insights or examples on the practical applications of collision probability estimation in real-world scenarios?\\n2. How do the proposed algorithms perform in settings with extremely small collision probabilities, and are there any limitations to their applicability in these cases?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The proposed algorithms demonstrate improvements in sample complexity for estimating collision probability while maintaining local differential privacy. This is a valuable contribution to the field of privacy-preserving data analysis.\\n2. The development of the first sequential testing algorithm for collision probability represents an important advancement, potentially enabling more efficient data analysis techniques.\\n3. Experimental results indicate that the proposed methods outperform existing approaches, suggesting practical applicability.' Weaknesses='1. While the algorithms are interesting, the presentation lacks sufficient detail on certain technical aspects, making it challenging to fully assess their effectiveness.\\n2. The experiments, while supportive of the proposed methods, could benefit from more comprehensive evaluations across a broader range of datasets and scenarios.' Questions='1. Can the authors provide more insights or examples on the practical applications of collision probability estimation in real-world scenarios?\\n2. How do the proposed algorithms perform in settings with extremely small collision probabilities, and are there any limitations to their applicability in these cases?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper proposes an innovative integration between Graph-Transformer and a novel self-attention mechanism for predicting molecular spatial configurations. The paper is well-organized, and the experimental results show significant improvements over state-of-the-art methods as well as widely used software like RDkit.' Weaknesses='While the proposed method appears strong, the novelty of the self-attention mechanism may not be groundbreaking given the existence of similar approaches in current literature. The paper does not address the potential computational overhead introduced by the new mechanism compared to simpler models. It is also unclear how transferable this approach is to other domains besides the specific datasets used in this study.' Questions='Can the authors elaborate on the computational efficiency of the proposed approach compared to baseline methods? Additionally, how does the performance of the new self-attention mechanism compare to existing variants on other molecular modeling tasks not covered in this paper?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper proposes an innovative integration between Graph-Transformer and a novel self-attention mechanism for predicting molecular spatial configurations. The paper is well-organized, and the experimental results show significant improvements over state-of-the-art methods as well as widely used software like RDkit.' Weaknesses='While the proposed method appears strong, the novelty of the self-attention mechanism may not be groundbreaking given the existence of similar approaches in current literature. The paper does not address the potential computational overhead introduced by the new mechanism compared to simpler models. It is also unclear how transferable this approach is to other domains besides the specific datasets used in this study.' Questions='Can the authors elaborate on the computational efficiency of the proposed approach compared to baseline methods? Additionally, how does the performance of the new self-attention mechanism compare to existing variants on other molecular modeling tasks not covered in this paper?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=3 Strengths='1. WavJourney leverages Large Language Models to create a framework for generating storytelling audio content, which is novel and advances the field of audio generation.\\n2. The framework allows for the creation of multi-genre storytelling audio, potentially opening new avenues for audio media creation.\\n3. The introduction of a new multi-genre story benchmark and availability of code and audio resources support transparency and reproducibility.' Weaknesses='1. The reliance on existing audio models might limit the novelty of the underlying technology beyond integration of these components.\\n2. There might be concerns about scalability and flexibility across vastly different audio domains not tested in the benchmarks.' Questions='1. What specific challenges does the WavJourney face when integrating disparate audio models, and how were these addressed? \\n2. How does the approach handle synchronization and coherence across different audio elements such as speech, music, and sound effects?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=3 Strengths='1. WavJourney leverages Large Language Models to create a framework for generating storytelling audio content, which is novel and advances the field of audio generation.\\n2. The framework allows for the creation of multi-genre storytelling audio, potentially opening new avenues for audio media creation.\\n3. The introduction of a new multi-genre story benchmark and availability of code and audio resources support transparency and reproducibility.' Weaknesses='1. The reliance on existing audio models might limit the novelty of the underlying technology beyond integration of these components.\\n2. There might be concerns about scalability and flexibility across vastly different audio domains not tested in the benchmarks.' Questions='1. What specific challenges does the WavJourney face when integrating disparate audio models, and how were these addressed? \\n2. How does the approach handle synchronization and coherence across different audio elements such as speech, music, and sound effects?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths='The paper addresses the important problem of availability data poisoning attacks and proposes a new method, Transferable Poisoning, that demonstrates improved attack transferability across different learning paradigms. The experiments on benchmark datasets such as CIFAR-10, CIFAR-100, and Tiny-ImageNet provide a thorough evaluation of the proposed approach.' Weaknesses='The main contribution relies on combining existing methods, which limits the novelty. There are concerns regarding the validity of the evaluation setup, especially in the context of contrastive learning. The description of high-frequency perturbations and their role in enhancing attack transferability is not fully clear. Moreover, the computational cost of the proposed algorithm could be higher than others, potentially limiting its practical usability.' Questions='1. Can you clarify how the high-frequency characteristics specifically contribute to the transferability of the poisoning attacks?\\n2. How does the computational cost of Transferable Poisoning compare to other existing methods for generating poisoning perturbations?\\n3. Were different augmentations tried in the evaluation with CL to ensure a fair and reliable assessment of the poisoning effectiveness?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths='The paper addresses the important problem of availability data poisoning attacks and proposes a new method, Transferable Poisoning, that demonstrates improved attack transferability across different learning paradigms. The experiments on benchmark datasets such as CIFAR-10, CIFAR-100, and Tiny-ImageNet provide a thorough evaluation of the proposed approach.' Weaknesses='The main contribution relies on combining existing methods, which limits the novelty. There are concerns regarding the validity of the evaluation setup, especially in the context of contrastive learning. The description of high-frequency perturbations and their role in enhancing attack transferability is not fully clear. Moreover, the computational cost of the proposed algorithm could be higher than others, potentially limiting its practical usability.' Questions='1. Can you clarify how the high-frequency characteristics specifically contribute to the transferability of the poisoning attacks?\\n2. How does the computational cost of Transferable Poisoning compare to other existing methods for generating poisoning perturbations?\\n3. Were different augmentations tried in the evaluation with CL to ensure a fair and reliable assessment of the poisoning effectiveness?'\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Invalid \\escape: line 1 column 504 (char 503)\n",
      "Content:\n",
      "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 4, \"Confidence\": 3, \"Strengths\": \"The paper attempts to theorize a fundamental aspect of language and its implications on the development of LLMs, which is an interesting angle that touches upon both linguistics and computational linguistics.\", \"Weaknesses\": \"The abstract is quite vague and doesn't provide strong clarity on how the theoretical insights are practically or empirically validated. Key concepts such as 'unambiguous' and '{\\epsilon}-ambiguous' languages are introduced without clear definitions. The connection between Bayesian inference and emergent abilities in LLMs is not explicitly elaborated, which makes it hard to verify the claims or understand the basis of the methodology.\", \"Questions\": \"1. How are the concepts of 'unambiguous' and '{\\epsilon}-ambiguous' languages defined and measured in practical terms?\\n2. Can the authors provide empirical results or a specific example where Bayesian inference on sparse joint distributions showed improvement in LLM tasks?\\n3. What are the limitations of this approach, and how does it compare quantitatively with existing methods in handling linguistic ambiguities?\"}\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses a significant and challenging problem in the field of graph neural networks: ensuring fairness while managing biases in the data. By proposing a new formulation of the Lipschitz bound for GNNs, the work contributes a potentially impactful method for stabilizing GNN outputs against biases. The theoretical analysis is supported by experimental validation, which strengthens the proposal.' Weaknesses='The paper may not adequately cover how the proposed Lipschitz bound formulation compares with existing methods in fairness training for GNNs. Additionally, the experimental results might lack diversity in datasets that could better demonstrate the general applicability and effectiveness of the proposed method across different types of graph data. The clarity of the presentation, especially regarding the formulation and implications of the Lipschitz bound, could be improved.' Questions='1. How does the proposed Lipschitz bound compare with other methods for bias mitigation in GNNs, both in terms of computational efficiency and effectiveness?\\n2. Can the approach be generalized to other types of neural networks beyond GNNs, or are there specific challenges that limit its applicability?\\n3. What are the practical implications of this work for deploying GNNs in real-world applications where fairness is a concern?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses a significant and challenging problem in the field of graph neural networks: ensuring fairness while managing biases in the data. By proposing a new formulation of the Lipschitz bound for GNNs, the work contributes a potentially impactful method for stabilizing GNN outputs against biases. The theoretical analysis is supported by experimental validation, which strengthens the proposal.' Weaknesses='The paper may not adequately cover how the proposed Lipschitz bound formulation compares with existing methods in fairness training for GNNs. Additionally, the experimental results might lack diversity in datasets that could better demonstrate the general applicability and effectiveness of the proposed method across different types of graph data. The clarity of the presentation, especially regarding the formulation and implications of the Lipschitz bound, could be improved.' Questions='1. How does the proposed Lipschitz bound compare with other methods for bias mitigation in GNNs, both in terms of computational efficiency and effectiveness?\\n2. Can the approach be generalized to other types of neural networks beyond GNNs, or are there specific challenges that limit its applicability?\\n3. What are the practical implications of this work for deploying GNNs in real-world applications where fairness is a concern?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper provides a novel extension to the existing theoretical understanding of non-contrastive learning by analyzing the effects of feature normalization using cosine loss. The connection made between the higher-order dynamics and stable equilibria potentially advances our understanding of the mechanisms that prevent representation collapse in self-supervised learning settings.' Weaknesses=\"The paper's theoretical contributions are somewhat limited by the fact that they build on previous work without providing substantial new insights beyond the incorporation of feature normalization. Additionally, the abstract refers to works like \\\\cite{Tian2021ICML} without giving a complete picture of how this work distinctly extends beyond the current state of the art, potentially limiting its perceived novelty and impact.\" Questions='1. Can this theoretical analysis be empirically validated using complex real-world datasets and architectures beyond what is suggested by the dynamics study?\\n2. How significant is the role of feature normalization in practical non-contrastive settings compared to other regularization techniques?\\n3. Are there any limitations or assumptions in the proposed theoretical framework that could impede its applicability to practical scenarios?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper provides a novel extension to the existing theoretical understanding of non-contrastive learning by analyzing the effects of feature normalization using cosine loss. The connection made between the higher-order dynamics and stable equilibria potentially advances our understanding of the mechanisms that prevent representation collapse in self-supervised learning settings.' Weaknesses=\"The paper's theoretical contributions are somewhat limited by the fact that they build on previous work without providing substantial new insights beyond the incorporation of feature normalization. Additionally, the abstract refers to works like \\\\cite{Tian2021ICML} without giving a complete picture of how this work distinctly extends beyond the current state of the art, potentially limiting its perceived novelty and impact.\" Questions='1. Can this theoretical analysis be empirically validated using complex real-world datasets and architectures beyond what is suggested by the dynamics study?\\n2. How significant is the role of feature normalization in practical non-contrastive settings compared to other regularization techniques?\\n3. Are there any limitations or assumptions in the proposed theoretical framework that could impede its applicability to practical scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper explores an approach that combines noise with continuation methods to address challenges in training deep neural networks, such as getting stuck in local minima and overfitting. The study is built upon existing concepts, but manages to highlight potential improvements in optimizer performance for non-convex loss functions.' Weaknesses='While the paper presents interesting ideas, it can sometimes lack clarity in connecting theoretical concepts with practical implementation. It would benefit from a more thorough discussion on potential limitations of the method or scenarios where it might underperform.' Questions='What are the specific mechanisms by which regularization impacts the continuation optimizer? Are there particular types of noise that have been found more effective during the preliminary tests?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper explores an approach that combines noise with continuation methods to address challenges in training deep neural networks, such as getting stuck in local minima and overfitting. The study is built upon existing concepts, but manages to highlight potential improvements in optimizer performance for non-convex loss functions.' Weaknesses='While the paper presents interesting ideas, it can sometimes lack clarity in connecting theoretical concepts with practical implementation. It would benefit from a more thorough discussion on potential limitations of the method or scenarios where it might underperform.' Questions='What are the specific mechanisms by which regularization impacts the continuation optimizer? Are there particular types of noise that have been found more effective during the preliminary tests?'\n",
      "Zero shot review:\n",
      " Soundness=2 Presentation=2 Contribution=3 Rating=4 Confidence=3 Strengths='1. The method addresses privacy and security issues inherent in standard UDA by not requiring source data access.\\n2. The introduction of the Pseudo label Refinery Network (PRN) and its theoretical support could offer a novel contribution to DA.\\n3. The approach is evaluated on four benchmark datasets, demonstrating its competitive performance.' Weaknesses='1. The presentation lacks clarity, making it difficult to understand the motivation, methodology, and technical details fully.\\n2. There is insufficient discussion on how the proposed method performs in single-source settings or when sources have varying levels of quality or relevance.\\n3. The improvement over existing methods is marginal, raising concerns about the practical significance of the contribution.\\n4. The paper lacks discussion of integration with existing BDA methods and how the PRN specifically improves results over simple baselines.' Questions='1. How is the Pseudo label Refinery Network designed in relation to the modified triangle inequality theory?\\n2. What are the limitations of the proposed approach concerning real-world applications with substantially different source domains?\\n3. How does the model perform when dealing with a scenario where one source domain is significantly more informative than others?'\n",
      "Success: Soundness=2 Presentation=2 Contribution=3 Rating=4 Confidence=3 Strengths='1. The method addresses privacy and security issues inherent in standard UDA by not requiring source data access.\\n2. The introduction of the Pseudo label Refinery Network (PRN) and its theoretical support could offer a novel contribution to DA.\\n3. The approach is evaluated on four benchmark datasets, demonstrating its competitive performance.' Weaknesses='1. The presentation lacks clarity, making it difficult to understand the motivation, methodology, and technical details fully.\\n2. There is insufficient discussion on how the proposed method performs in single-source settings or when sources have varying levels of quality or relevance.\\n3. The improvement over existing methods is marginal, raising concerns about the practical significance of the contribution.\\n4. The paper lacks discussion of integration with existing BDA methods and how the PRN specifically improves results over simple baselines.' Questions='1. How is the Pseudo label Refinery Network designed in relation to the modified triangle inequality theory?\\n2. What are the limitations of the proposed approach concerning real-world applications with substantially different source domains?\\n3. How does the model perform when dealing with a scenario where one source domain is significantly more informative than others?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a novel integration of feature and cost aggregation using a Transformer-based architecture, harnessing self- and cross-attention mechanisms. The approach offers a unifying strategy that leverages strengths from both aggregation processes and is evaluated on standard benchmarks.' Weaknesses='The paper lacks clarity on whether the datasets used for training the proposed method are consistent with those used in previous state-of-the-art methods. There are no specific details on the performance of the method in optical flow tasks, and potential limitations when dealing with image pairs with small overlapping ratios are not addressed. The robustness of the method in challenging cases like those from the MegaDepth dataset is questioned.' Questions='1. Are the datasets used for training the proposed method consistent with those used in previous methods?\\n2. How does the method perform on optical flow tasks, for example on KITTI or Sintel?\\n3. Can improved matching performance lead to better rotation and translation estimation?\\n4. How does the method handle image pairs with small overlapping ratios or challenging cases from MegaDepth?\\n5. Can you provide failure cases for additional insights?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper presents a novel integration of feature and cost aggregation using a Transformer-based architecture, harnessing self- and cross-attention mechanisms. The approach offers a unifying strategy that leverages strengths from both aggregation processes and is evaluated on standard benchmarks.' Weaknesses='The paper lacks clarity on whether the datasets used for training the proposed method are consistent with those used in previous state-of-the-art methods. There are no specific details on the performance of the method in optical flow tasks, and potential limitations when dealing with image pairs with small overlapping ratios are not addressed. The robustness of the method in challenging cases like those from the MegaDepth dataset is questioned.' Questions='1. Are the datasets used for training the proposed method consistent with those used in previous methods?\\n2. How does the method perform on optical flow tasks, for example on KITTI or Sintel?\\n3. Can improved matching performance lead to better rotation and translation estimation?\\n4. How does the method handle image pairs with small overlapping ratios or challenging cases from MegaDepth?\\n5. Can you provide failure cases for additional insights?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The authors successfully introduce new strategies for improving the robustness of data-free meta-learning, addressing crucial safety concerns like Task-Distribution Shift (TDS) and Task-Distribution Corruption (TDC). The newly proposed methods, TEAPOT and ROSY, effectively enhance the reliability and diversity of pseudo-task distributions, while extensive experiments demonstrate the suggested solution's superiority over existing baselines.\" Weaknesses='It is somewhat unclear how well the proposed approach scales when applied to various domains, given that experimental validation is so far performed only under limited conditions. Additionally, components of the methodological contributions, like the interpolation mechanism in TEAPOT or policy gradient updates in ROSY, need further specification to assess their full operational insights. A comparative evaluation with other robust DG methods might solidify the findings further.' Questions='Can the proposed ROSY framework be generalized to other DFML settings? Additionally, more in-depth explanation or exploration of potential trade-offs with respect to leveraging policy gradient techniques within ROSY might help clarify the approach.'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The authors successfully introduce new strategies for improving the robustness of data-free meta-learning, addressing crucial safety concerns like Task-Distribution Shift (TDS) and Task-Distribution Corruption (TDC). The newly proposed methods, TEAPOT and ROSY, effectively enhance the reliability and diversity of pseudo-task distributions, while extensive experiments demonstrate the suggested solution's superiority over existing baselines.\" Weaknesses='It is somewhat unclear how well the proposed approach scales when applied to various domains, given that experimental validation is so far performed only under limited conditions. Additionally, components of the methodological contributions, like the interpolation mechanism in TEAPOT or policy gradient updates in ROSY, need further specification to assess their full operational insights. A comparative evaluation with other robust DG methods might solidify the findings further.' Questions='Can the proposed ROSY framework be generalized to other DFML settings? Additionally, more in-depth explanation or exploration of potential trade-offs with respect to leveraging policy gradient techniques within ROSY might help clarify the approach.'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper introduces an innovative framework, VUM, to assess the understanding of human values by LLMs, focusing on 'know what' and 'know why' aspects, and utilizes a dialogue dataset created with GPT-4, enriching the evaluation methodologies.\" Weaknesses=\"The methodological details in determining values alignment and understanding could be clearer. The comparison claims between 'know what' and 'know why' need more robust empirical backing, especially given the complexity and nuances of LLM architectures and scales.\" Questions=\"How does VUM quantitatively distinguish between mere recitations of known values versus genuine understanding by LLMs in different contexts? What benchmarks or human evaluation measures were used to validate the findings, especially concerning 'know why'?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper introduces an innovative framework, VUM, to assess the understanding of human values by LLMs, focusing on 'know what' and 'know why' aspects, and utilizes a dialogue dataset created with GPT-4, enriching the evaluation methodologies.\" Weaknesses=\"The methodological details in determining values alignment and understanding could be clearer. The comparison claims between 'know what' and 'know why' need more robust empirical backing, especially given the complexity and nuances of LLM architectures and scales.\" Questions=\"How does VUM quantitatively distinguish between mere recitations of known values versus genuine understanding by LLMs in different contexts? What benchmarks or human evaluation measures were used to validate the findings, especially concerning 'know why'?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=4 Strengths='The introduction of hierarchical diffusion-based planning methods is a good approach to address challenges faced by traditional methods, especially in long-horizon tasks. The empirical evaluations on standard offline reinforcement learning benchmarks show the method’s superior performance and efficiency. The model’s ability to enhance generalization capabilities is a significant advantage.' Weaknesses='The paper might lack significant novelty, as hierarchical and diffusion methodologies have been explored previously, albeit through the combination in this manner might not have been done. Details of the implementation and evaluation may be lacking, such as the specifics of empirical settings and comparisons. There is also a need for more thorough comparisons with other state-of-the-art models in a similar setup.' Questions='How does the method specifically balance the trade-off between computational cost and performance improvement? What are the main domains or tasks where this method shows the most benefit over existing solutions? How does the generalization capability compare quantitatively with other methods?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=4 Strengths='The introduction of hierarchical diffusion-based planning methods is a good approach to address challenges faced by traditional methods, especially in long-horizon tasks. The empirical evaluations on standard offline reinforcement learning benchmarks show the method’s superior performance and efficiency. The model’s ability to enhance generalization capabilities is a significant advantage.' Weaknesses='The paper might lack significant novelty, as hierarchical and diffusion methodologies have been explored previously, albeit through the combination in this manner might not have been done. Details of the implementation and evaluation may be lacking, such as the specifics of empirical settings and comparisons. There is also a need for more thorough comparisons with other state-of-the-art models in a similar setup.' Questions='How does the method specifically balance the trade-off between computational cost and performance improvement? What are the main domains or tasks where this method shows the most benefit over existing solutions? How does the generalization capability compare quantitatively with other methods?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel approach, the Spiking Layer-Time-wise Relevance Propagation (SLTRP) and Spiking Layer-wise Relevance Propagation (SLRP), to generate Class Activation Maps (CAMs) and saliency maps from Spiking Neural Networks (SNNs). This is noteworthy as it addresses a previously unexplored problem in event-based vision. The proposed EventRPG framework demonstrates state-of-the-art performance on several benchmarks, showcasing its effectiveness.' Weaknesses='While the paper presents a novel technique for generating saliency maps in SNNs, there is a lack of detailed comparison with other existing data augmentation or saliency map generation methods. Furthermore, the impact of the proposed method on different types of SNN architectures needs further exploration. The paper could benefit from more comprehensive experiments across diverse datasets.' Questions='How does EventRPG compare with traditional saliency-based data augmentation techniques in terms of computational efficiency and resource usage? Additionally, how sensitive is the proposed method to variations in the network architecture or the size of the datasets used?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel approach, the Spiking Layer-Time-wise Relevance Propagation (SLTRP) and Spiking Layer-wise Relevance Propagation (SLRP), to generate Class Activation Maps (CAMs) and saliency maps from Spiking Neural Networks (SNNs). This is noteworthy as it addresses a previously unexplored problem in event-based vision. The proposed EventRPG framework demonstrates state-of-the-art performance on several benchmarks, showcasing its effectiveness.' Weaknesses='While the paper presents a novel technique for generating saliency maps in SNNs, there is a lack of detailed comparison with other existing data augmentation or saliency map generation methods. Furthermore, the impact of the proposed method on different types of SNN architectures needs further exploration. The paper could benefit from more comprehensive experiments across diverse datasets.' Questions='How does EventRPG compare with traditional saliency-based data augmentation techniques in terms of computational efficiency and resource usage? Additionally, how sensitive is the proposed method to variations in the network architecture or the size of the datasets used?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper presents a novel training strategy, SelfVC, which leverages self-synthesized examples to iteratively refine a voice conversion model, thus avoiding issues related to information loss from task-specific loss terms. The framework successfully applies self-supervised learning and speaker verification models to improve voice conversion across various tasks without needing text. Extensive experiments demonstrate state-of-the-art results in zero-shot voice conversion, proving its effectiveness in improving generated speech's naturalness, speaker similarity, and intelligibility.\" Weaknesses='The work, while innovative in its approach of using self-synthesized examples, primarily enhances existing voice conversion methods rather than introducing entirely new components. The claim regarding seamless adaptability to other languages through SSL features may require further evidence beyond current experiments. Additionally, while the technique of using self-synthesized examples shows improvements, quantitative measures of the extent of improvements might be limited without deeper analyses.' Questions='How significant is the improvement achieved by using self-synthesized examples compared to traditional methods quantitatively? Can the proposed training strategy be easily adapted to other speech tasks outside of voice conversion? What are the specific limitations or challenges faced when scaling the SelfVC approach across different languages or more diverse datasets?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper presents a novel training strategy, SelfVC, which leverages self-synthesized examples to iteratively refine a voice conversion model, thus avoiding issues related to information loss from task-specific loss terms. The framework successfully applies self-supervised learning and speaker verification models to improve voice conversion across various tasks without needing text. Extensive experiments demonstrate state-of-the-art results in zero-shot voice conversion, proving its effectiveness in improving generated speech's naturalness, speaker similarity, and intelligibility.\" Weaknesses='The work, while innovative in its approach of using self-synthesized examples, primarily enhances existing voice conversion methods rather than introducing entirely new components. The claim regarding seamless adaptability to other languages through SSL features may require further evidence beyond current experiments. Additionally, while the technique of using self-synthesized examples shows improvements, quantitative measures of the extent of improvements might be limited without deeper analyses.' Questions='How significant is the improvement achieved by using self-synthesized examples compared to traditional methods quantitatively? Can the proposed training strategy be easily adapted to other speech tasks outside of voice conversion? What are the specific limitations or challenges faced when scaling the SelfVC approach across different languages or more diverse datasets?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths=\"The paper presents a novel approach by integrating LLMs into autonomous driving systems to enhance decision-making by leveraging human common sense. The use of cognitive pathways and guided parameter matrix adaptation to translate high-level LLM decisions into actionable tasks is innovative. Extensive experiments demonstrate significant performance improvements over baseline approaches, suggesting the method's generalizability and effectiveness in complex scenarios.\" Weaknesses='The dependency on LLMs for critical real-time decision-making may introduce unpredictability due to the inherent variability in LLM outputs. Additionally, the paper might not fully address the computational challenges and latency issues associated with integrating LLMs into real-time driving systems. The scenarios tested in the experiments may not fully cover the long-tail distribution of rare events. The safety and reliability aspects, especially in edge cases, need more thorough evaluation.' Questions='How does the system handle the latency introduced by the LLM in real-time decision-making scenarios? Are there any contingency measures in place to ensure safety if the LLM provides an incorrect or delayed decision? How does the system verify or validate the decisions made by the LLM before executing them in critical driving tasks?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths=\"The paper presents a novel approach by integrating LLMs into autonomous driving systems to enhance decision-making by leveraging human common sense. The use of cognitive pathways and guided parameter matrix adaptation to translate high-level LLM decisions into actionable tasks is innovative. Extensive experiments demonstrate significant performance improvements over baseline approaches, suggesting the method's generalizability and effectiveness in complex scenarios.\" Weaknesses='The dependency on LLMs for critical real-time decision-making may introduce unpredictability due to the inherent variability in LLM outputs. Additionally, the paper might not fully address the computational challenges and latency issues associated with integrating LLMs into real-time driving systems. The scenarios tested in the experiments may not fully cover the long-tail distribution of rare events. The safety and reliability aspects, especially in edge cases, need more thorough evaluation.' Questions='How does the system handle the latency introduced by the LLM in real-time decision-making scenarios? Are there any contingency measures in place to ensure safety if the LLM provides an incorrect or delayed decision? How does the system verify or validate the decisions made by the LLM before executing them in critical driving tasks?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=4 Strengths='The paper effectively demonstrates a significant simplification of transformer architectures, showcasing clear empirical benefits such as faster training throughput and reduced parameters without compromising performance. The rigorous combination of theoretical and empirical strategies provides a solid foundation for the proposed modifications.' Weaknesses='While the simplifications appear effective, the paper does not deeply explore potential limitations or scenarios where these simplifications might lead to performance degradation. The applicability of the simplifications to very large-scale models and diverse datasets remains to be fully validated.' Questions='How do the simplified architectures perform on tasks beyond those tested, particularly in real-world scenarios requiring large-scale models? Are there potential limitations in expressiveness or adaptivity in more complex task demands?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=4 Strengths='The paper effectively demonstrates a significant simplification of transformer architectures, showcasing clear empirical benefits such as faster training throughput and reduced parameters without compromising performance. The rigorous combination of theoretical and empirical strategies provides a solid foundation for the proposed modifications.' Weaknesses='While the simplifications appear effective, the paper does not deeply explore potential limitations or scenarios where these simplifications might lead to performance degradation. The applicability of the simplifications to very large-scale models and diverse datasets remains to be fully validated.' Questions='How do the simplified architectures perform on tasks beyond those tested, particularly in real-world scenarios requiring large-scale models? Are there potential limitations in expressiveness or adaptivity in more complex task demands?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper tackles an important problem in integrating optimization layers successfully within neural network architectures, allowing for the utilization of domain-specific knowledge within machine learning tasks. The use of primal-dual augmented Lagrangian techniques to handle infeasibilities in quadratic programming (QP) layers is innovative and addresses a common obstacle in this area. The open-source software package, QPLayer, demonstrates significant practical contributions as it provides a tool for broader usability in various applications. The alternative formulations enhancing numerical robustness, speed, and accuracy signal potential improvements over traditional methods.' Weaknesses=\"The paper might not be sufficiently clear in presenting the theoretical underpinnings that underlie the differentiability of closest feasible QP solutions. Further clarity on the advantages and trade-offs of the proposed methods compared to straightforward penalty-based approaches or existing methods would strengthen the argument for the proposed approach's novelty and efficacy. The paper could also benefit from more diverse experimental results or applications beyond what is discussed in the abstract to fully validate the claimed superior predictive performance.\" Questions='1. Can you provide more details or references about the primal-dual augmented Lagrangian techniques used in your approach? Some readers might not be familiar with these techniques, and a brief background or reference could be helpful.\\n2. How does your approach compare with more standard approaches such as adding slack variables and penalties to handle infeasibilities in QP layers?\\n3. What are the limitations of your approach in handling convex quadratic programming problems within neural network architectures?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper tackles an important problem in integrating optimization layers successfully within neural network architectures, allowing for the utilization of domain-specific knowledge within machine learning tasks. The use of primal-dual augmented Lagrangian techniques to handle infeasibilities in quadratic programming (QP) layers is innovative and addresses a common obstacle in this area. The open-source software package, QPLayer, demonstrates significant practical contributions as it provides a tool for broader usability in various applications. The alternative formulations enhancing numerical robustness, speed, and accuracy signal potential improvements over traditional methods.' Weaknesses=\"The paper might not be sufficiently clear in presenting the theoretical underpinnings that underlie the differentiability of closest feasible QP solutions. Further clarity on the advantages and trade-offs of the proposed methods compared to straightforward penalty-based approaches or existing methods would strengthen the argument for the proposed approach's novelty and efficacy. The paper could also benefit from more diverse experimental results or applications beyond what is discussed in the abstract to fully validate the claimed superior predictive performance.\" Questions='1. Can you provide more details or references about the primal-dual augmented Lagrangian techniques used in your approach? Some readers might not be familiar with these techniques, and a brief background or reference could be helpful.\\n2. How does your approach compare with more standard approaches such as adding slack variables and penalties to handle infeasibilities in QP layers?\\n3. What are the limitations of your approach in handling convex quadratic programming problems within neural network architectures?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper introduces a novel approach to contextual stochastic optimization, aiming to robustly minimize costs in the presence of uncertainty. It combines a learning strategy with downstream task knowledge to effectively manage uncertain parameters. The method's theoretical foundation provides guarantees on decision-making regret, and its performance is validated experimentally in a real-world scenario, demonstrating competitive average regret and significantly lower worst-case costs.\" Weaknesses='The methodology may rely heavily on accurate discretization and secondary optimization setup, which could introduce complexity in its implementation. The abstract lacks specific details about experimental setup and comparison baselines, which makes it difficult to assess the generalizability and robustness of the proposed method across different contexts.' Questions=\"How does the discretization of the feasible region impact the method's scalability and computational efficiency? Are there specific conditions or types of problems where this method might underperform compared to existing solutions? What are the limitations regarding the choice of threshold for cost control, and how is this threshold determined in practice?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper introduces a novel approach to contextual stochastic optimization, aiming to robustly minimize costs in the presence of uncertainty. It combines a learning strategy with downstream task knowledge to effectively manage uncertain parameters. The method's theoretical foundation provides guarantees on decision-making regret, and its performance is validated experimentally in a real-world scenario, demonstrating competitive average regret and significantly lower worst-case costs.\" Weaknesses='The methodology may rely heavily on accurate discretization and secondary optimization setup, which could introduce complexity in its implementation. The abstract lacks specific details about experimental setup and comparison baselines, which makes it difficult to assess the generalizability and robustness of the proposed method across different contexts.' Questions=\"How does the discretization of the feasible region impact the method's scalability and computational efficiency? Are there specific conditions or types of problems where this method might underperform compared to existing solutions? What are the limitations regarding the choice of threshold for cost control, and how is this threshold determined in practice?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel approach to enhancing the performance of large language models (LLMs) by using a mixture-of-experts paradigm to improve instruction prompting. The idea of dividing the problem space into homogeneous regions with specific instructions and demos is promising and offers a new perspective on improving LLM adaptability.' Weaknesses=\"There is a potential issue with the generalization of the method across diverse tasks due to its reliance on clustering and the assumption that demos and tasks are semantically homogenous when divided. The method's complexity might be a barrier when extending to larger and more diverse datasets. The clustering approach for demos could face challenges in tasks with overlapping problem spaces, risking reduced effectiveness if clusters are not well-separated.\" Questions='How does the performance of MoP vary with the size and diversity of the demo datasets? Is there an optimal number of experts or clusters that maximizes performance, and how is this determined?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel approach to enhancing the performance of large language models (LLMs) by using a mixture-of-experts paradigm to improve instruction prompting. The idea of dividing the problem space into homogeneous regions with specific instructions and demos is promising and offers a new perspective on improving LLM adaptability.' Weaknesses=\"There is a potential issue with the generalization of the method across diverse tasks due to its reliance on clustering and the assumption that demos and tasks are semantically homogenous when divided. The method's complexity might be a barrier when extending to larger and more diverse datasets. The clustering approach for demos could face challenges in tasks with overlapping problem spaces, risking reduced effectiveness if clusters are not well-separated.\" Questions='How does the performance of MoP vary with the size and diversity of the demo datasets? Is there an optimal number of experts or clusters that maximizes performance, and how is this determined?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper provides a novel theoretical insight into the challenges of training heteroskedastic neural regression models by leveraging statistical physics. The derivation of nonparametric free energy and the identification of phase transitions contribute to a deeper understanding of these models. The methodology potentially offers a more efficient scheme for regularization optimization in such models, which could have practical implications for model training.' Weaknesses=\"Although the paper proposes a theoretical explanation for instabilities, the complexity of the approach might limit accessibility for a broader audience. The practical application and validation in real-world settings might not be comprehensively covered, given the abstract's focus on theoretical contributions. Some technical components may assume substantial prior knowledge, which could hinder understanding for readers less familiar with statistical physics and advanced machine learning models.\" Questions='1. Can the proposed framework for optimizing regularization be generalized to heteroskedastic regression models in other contexts or domains effectively?\\n2. What are the limitations or assumptions of the nonparametric free energy approach in practical implementations?\\n3. How does the numerical solution obtained from the nonparametric free energy align with traditional methods in terms of computational efficiency and accuracy?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper provides a novel theoretical insight into the challenges of training heteroskedastic neural regression models by leveraging statistical physics. The derivation of nonparametric free energy and the identification of phase transitions contribute to a deeper understanding of these models. The methodology potentially offers a more efficient scheme for regularization optimization in such models, which could have practical implications for model training.' Weaknesses=\"Although the paper proposes a theoretical explanation for instabilities, the complexity of the approach might limit accessibility for a broader audience. The practical application and validation in real-world settings might not be comprehensively covered, given the abstract's focus on theoretical contributions. Some technical components may assume substantial prior knowledge, which could hinder understanding for readers less familiar with statistical physics and advanced machine learning models.\" Questions='1. Can the proposed framework for optimizing regularization be generalized to heteroskedastic regression models in other contexts or domains effectively?\\n2. What are the limitations or assumptions of the nonparametric free energy approach in practical implementations?\\n3. How does the numerical solution obtained from the nonparametric free energy align with traditional methods in terms of computational efficiency and accuracy?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"1. The paper addresses the important issue of domain adaptation, which is crucial for applications involving distribution shifts, such as astronomical and wildlife identification datasets. \\n2. The proposed 'Connect Later' method demonstrates notable improvements in OOD error across multiple datasets, suggesting its potential efficacy. \\n3. The combination of self-supervised pretraining with domain-specific fine-tuning is a promising strategy to enhance model performance under distribution shifts.\" Weaknesses=\"1. The paper's novelty primarily lies in the adaptation of existing techniques, which may not represent a significant theoretical advancement. \\n2. It is unclear how well the proposed strategy would generalize to other models or domains not tested in the study, potentially limiting its broader applicability.\\n3. The results, although showing improvement, are marginal in some cases, raising questions about the practical significance of the proposed method.\" Questions='1. How does the choice of generic and targeted augmentations specifically impact each dataset, and would different datasets require different augmentation strategies?\\n2. Could the Connect Later strategy be integrated with other self-supervised learning techniques to potentially yield better results?\\n3. What are the limitations of the proposed method, and are there scenarios where it might not perform well?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"1. The paper addresses the important issue of domain adaptation, which is crucial for applications involving distribution shifts, such as astronomical and wildlife identification datasets. \\n2. The proposed 'Connect Later' method demonstrates notable improvements in OOD error across multiple datasets, suggesting its potential efficacy. \\n3. The combination of self-supervised pretraining with domain-specific fine-tuning is a promising strategy to enhance model performance under distribution shifts.\" Weaknesses=\"1. The paper's novelty primarily lies in the adaptation of existing techniques, which may not represent a significant theoretical advancement. \\n2. It is unclear how well the proposed strategy would generalize to other models or domains not tested in the study, potentially limiting its broader applicability.\\n3. The results, although showing improvement, are marginal in some cases, raising questions about the practical significance of the proposed method.\" Questions='1. How does the choice of generic and targeted augmentations specifically impact each dataset, and would different datasets require different augmentation strategies?\\n2. Could the Connect Later strategy be integrated with other self-supervised learning techniques to potentially yield better results?\\n3. What are the limitations of the proposed method, and are there scenarios where it might not perform well?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper addresses a significant issue in the field of information retrieval and large language models by ensuring privacy in distributed contexts.\\n2. The proposed approach, PRAG, presents a novel use of multi-party computation to achieve privacy guarantees in dense information retrieval.\\n3. The manuscript is clear and well-structured, making the complex topic accessible.' Weaknesses='1. The practical implementation and scalability of the approach in real-world scenarios may not be fully addressed in the abstract.\\n2. The need for advanced security protocols might limit accessibility for some users or require significant computational resources.' Questions='1. How does the communication complexity scale with the number of servers involved in the multi-party computation?\\n2. Are there specific scenarios or datasets where the approach shows significant limitations, and how does it compare with existing methods in such instances?\\n3. What are the potential cost implications in terms of computational resources required for the proposed approach in comparison to current methods?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper addresses a significant issue in the field of information retrieval and large language models by ensuring privacy in distributed contexts.\\n2. The proposed approach, PRAG, presents a novel use of multi-party computation to achieve privacy guarantees in dense information retrieval.\\n3. The manuscript is clear and well-structured, making the complex topic accessible.' Weaknesses='1. The practical implementation and scalability of the approach in real-world scenarios may not be fully addressed in the abstract.\\n2. The need for advanced security protocols might limit accessibility for some users or require significant computational resources.' Questions='1. How does the communication complexity scale with the number of servers involved in the multi-party computation?\\n2. Are there specific scenarios or datasets where the approach shows significant limitations, and how does it compare with existing methods in such instances?\\n3. What are the potential cost implications in terms of computational resources required for the proposed approach in comparison to current methods?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='1. The paper proposes a novel metric, DDF, for comparing 3D point clouds, which seems promising given the experimental results showing its effectiveness across various tasks.\\n2. The metric is computationally efficient and has been tested on a variety of important applications, such as shape reconstruction and scene flow estimation, suggesting broad applicability.\\n3. The inclusion of source code supports the reproducibility of the work.' Weaknesses=\"1. The abstract lacks detailed explanations of key terms, such as 'directional distance field', which might make it difficult for readers unfamiliar with the concept to fully grasp the significance and novelty of the work.\\n2. It is not entirely clear how the reference points are chosen or how they affect the DDF metric's performance, which could potentially impact the generalization of the method.\\n3. The presentation of the experimental results could be clearer; more context on existing metrics and their limitations would help highlight the advantages of DDF.\" Questions='1. How sensitive is the performance of DDF to the choice of reference points? Is there a systematic way to choose or optimize them?\\n2. Could the authors provide more details on how DDF performs relative to other traditional metrics in terms of computation time and scalability with larger datasets?\\n3. Are there specific characteristics or types of point cloud data where DDF performs particularly well or poorly?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='1. The paper proposes a novel metric, DDF, for comparing 3D point clouds, which seems promising given the experimental results showing its effectiveness across various tasks.\\n2. The metric is computationally efficient and has been tested on a variety of important applications, such as shape reconstruction and scene flow estimation, suggesting broad applicability.\\n3. The inclusion of source code supports the reproducibility of the work.' Weaknesses=\"1. The abstract lacks detailed explanations of key terms, such as 'directional distance field', which might make it difficult for readers unfamiliar with the concept to fully grasp the significance and novelty of the work.\\n2. It is not entirely clear how the reference points are chosen or how they affect the DDF metric's performance, which could potentially impact the generalization of the method.\\n3. The presentation of the experimental results could be clearer; more context on existing metrics and their limitations would help highlight the advantages of DDF.\" Questions='1. How sensitive is the performance of DDF to the choice of reference points? Is there a systematic way to choose or optimize them?\\n2. Could the authors provide more details on how DDF performs relative to other traditional metrics in terms of computation time and scalability with larger datasets?\\n3. Are there specific characteristics or types of point cloud data where DDF performs particularly well or poorly?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. Phi-1 is a highly efficient model, demonstrating that high performance on coding tasks can be achieved with significantly fewer resources compared to other contemporary models.\\n2. The paper highlights the potential of using \"textbook quality\" data and synthetically generated resources to train models effectively.\\n3. The model shows impressive results in pass@1 accuracy on HumanEval and MBPP despite its smaller size, showcasing its efficacy.\\n4. The clear presentation of the model\\'s architecture and training pipeline allows for easy understanding and potential replication.' Weaknesses='1. The abstract does not provide detailed information on the selection criteria of the \"textbook quality\" data or the synthetic data generation process, which are crucial for understanding the model\\'s success.\\n2. The paper might not sufficiently discuss the trade-offs or limitations of using such a small model compared to larger models.\\n3. The evaluation is focused on specific benchmarks (HumanEval and MBPP) without broader generalization or consideration of other potential coding tasks.' Questions='1. How was the \"textbook quality\" data determined, and what are the key characteristics that distinguish it from other types of data?\\n2. Are there any specific limitations or challenges encountered during the training of phi-1 that could be addressed in future work?\\n3. How does the model perform on larger and more complex coding tasks compared to simple evaluation tasks provided, and could it generalize beyond coding applications?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. Phi-1 is a highly efficient model, demonstrating that high performance on coding tasks can be achieved with significantly fewer resources compared to other contemporary models.\\n2. The paper highlights the potential of using \"textbook quality\" data and synthetically generated resources to train models effectively.\\n3. The model shows impressive results in pass@1 accuracy on HumanEval and MBPP despite its smaller size, showcasing its efficacy.\\n4. The clear presentation of the model\\'s architecture and training pipeline allows for easy understanding and potential replication.' Weaknesses='1. The abstract does not provide detailed information on the selection criteria of the \"textbook quality\" data or the synthetic data generation process, which are crucial for understanding the model\\'s success.\\n2. The paper might not sufficiently discuss the trade-offs or limitations of using such a small model compared to larger models.\\n3. The evaluation is focused on specific benchmarks (HumanEval and MBPP) without broader generalization or consideration of other potential coding tasks.' Questions='1. How was the \"textbook quality\" data determined, and what are the key characteristics that distinguish it from other types of data?\\n2. Are there any specific limitations or challenges encountered during the training of phi-1 that could be addressed in future work?\\n3. How does the model perform on larger and more complex coding tasks compared to simple evaluation tasks provided, and could it generalize beyond coding applications?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides a rigorous analysis of in-context learning by differentiating between task retrieval and task learning, which helps in better understanding these processes. The authors propose generative models and provide theoretical insights into the behavior of in-context learning with these models, introducing an interesting phenomenon regarding risk bounds and the number of in-context samples.' Weaknesses='The theoretical analysis might be based on an overly simplified model that may not capture the complexities of real-world in-context learning scenarios, limiting its applicability. Additionally, the discovery that task retrieval risk can increase with more samples needs further exploration to understand its implications and how it might be mitigated.' Questions='How does the increase in the number of in-context samples leading to higher risk in task retrieval relate to practical implementations, and what strategies could mitigate this risk?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides a rigorous analysis of in-context learning by differentiating between task retrieval and task learning, which helps in better understanding these processes. The authors propose generative models and provide theoretical insights into the behavior of in-context learning with these models, introducing an interesting phenomenon regarding risk bounds and the number of in-context samples.' Weaknesses='The theoretical analysis might be based on an overly simplified model that may not capture the complexities of real-world in-context learning scenarios, limiting its applicability. Additionally, the discovery that task retrieval risk can increase with more samples needs further exploration to understand its implications and how it might be mitigated.' Questions='How does the increase in the number of in-context samples leading to higher risk in task retrieval relate to practical implementations, and what strategies could mitigate this risk?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The proposed method, Learning to Bootstrap (L2B), offers a fresh approach to handling noisy labels by incorporating dynamic reweighting and implicit relabeling. The method is compatible with existing noisy label learning methods and has demonstrated effective performance across multiple benchmark datasets, underscoring its practical relevance and adaptability.' Weaknesses=\"The paper could benefit from more detailed comparisons with existing methods on a wider variety of real-world datasets experiencing systematic label noise. It would also be helpful to further elaborate on the theoretical underpinnings and convergence guarantees of the L2B method. While the method's dynamic nature is a strength, understanding the precise behavior and tuning of weighting parameters like alpha and beta may require more user guidance.\" Questions=\"1. Could you provide more detailed insights on how the L2B method handles scenarios with systematic label noise, not just random label noise?\\n2. Can the authors clarify whether a validation set is necessary for the L2B method, given the mention of an unbiased validation set in section 3.1?\\n3. It is mentioned that the weights alpha and beta are learned through the model's predictions. How do these weights typically converge during the training process, and do they play a significant role in the robustness of the method?\\n4. How do the results compare on truly real-world scenarios beyond standardized datasets, and are there practical insights into deploying L2B in such situations?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The proposed method, Learning to Bootstrap (L2B), offers a fresh approach to handling noisy labels by incorporating dynamic reweighting and implicit relabeling. The method is compatible with existing noisy label learning methods and has demonstrated effective performance across multiple benchmark datasets, underscoring its practical relevance and adaptability.' Weaknesses=\"The paper could benefit from more detailed comparisons with existing methods on a wider variety of real-world datasets experiencing systematic label noise. It would also be helpful to further elaborate on the theoretical underpinnings and convergence guarantees of the L2B method. While the method's dynamic nature is a strength, understanding the precise behavior and tuning of weighting parameters like alpha and beta may require more user guidance.\" Questions=\"1. Could you provide more detailed insights on how the L2B method handles scenarios with systematic label noise, not just random label noise?\\n2. Can the authors clarify whether a validation set is necessary for the L2B method, given the mention of an unbiased validation set in section 3.1?\\n3. It is mentioned that the weights alpha and beta are learned through the model's predictions. How do these weights typically converge during the training process, and do they play a significant role in the robustness of the method?\\n4. How do the results compare on truly real-world scenarios beyond standardized datasets, and are there practical insights into deploying L2B in such situations?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces the WaveFluid model, which innovatively uses adversarial training to learn a velocity field directly for the speech synthesis task. The use of reparameterization techniques to reduce memory footprint is commendable. Results indicate that the model is competitive in sample quality with reduced inference steps.' Weaknesses='The application area appears to be narrow, focusing specifically on vocoder tasks with mel-spectrogram conditions. The impact of the proposed method on broader applications of generative models is not discussed. Additionally, the comparison with existing architectures might be limited; more extensive benchmarking against state-of-the-art models could strengthen the contribution.' Questions='How does the proposed WaveFluid model compare in terms of computational efficiency and resource requirements with existing diffusion or poisson-based models?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces the WaveFluid model, which innovatively uses adversarial training to learn a velocity field directly for the speech synthesis task. The use of reparameterization techniques to reduce memory footprint is commendable. Results indicate that the model is competitive in sample quality with reduced inference steps.' Weaknesses='The application area appears to be narrow, focusing specifically on vocoder tasks with mel-spectrogram conditions. The impact of the proposed method on broader applications of generative models is not discussed. Additionally, the comparison with existing architectures might be limited; more extensive benchmarking against state-of-the-art models could strengthen the contribution.' Questions='How does the proposed WaveFluid model compare in terms of computational efficiency and resource requirements with existing diffusion or poisson-based models?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=3 Strengths='The paper proposes a novel unified framework to tackle both interpretability and robustness in neural image classifiers, which are often addressed separately. By leveraging GANs to generate counterfactuals, the framework adds interpretability with salient region highlighting and enhances robustness through adversarial training. The evaluation methodically covers both semantic segmentation tasks and robustness against adversarial attacks, demonstrating competitive performance and improved resilience.' Weaknesses='The paper primarily focuses on binary classification tasks, limiting its wider applicability. The evaluation focuses on specific use cases (concrete crack detection and fruit defect detection), which may not fully generalize to other domains or complex datasets. Additionally, the experimental design uses a single seed, making it difficult to ascertain the generality and robustness of the reported results across multiple runs.' Questions=\"How does the approach generalize to multi-class classification tasks, given its current focus on binary classification? Are there plans to conduct experiments to validate the approach's effectiveness in more diverse datasets and environments beyond the narrow focus presented in this study?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=3 Strengths='The paper proposes a novel unified framework to tackle both interpretability and robustness in neural image classifiers, which are often addressed separately. By leveraging GANs to generate counterfactuals, the framework adds interpretability with salient region highlighting and enhances robustness through adversarial training. The evaluation methodically covers both semantic segmentation tasks and robustness against adversarial attacks, demonstrating competitive performance and improved resilience.' Weaknesses='The paper primarily focuses on binary classification tasks, limiting its wider applicability. The evaluation focuses on specific use cases (concrete crack detection and fruit defect detection), which may not fully generalize to other domains or complex datasets. Additionally, the experimental design uses a single seed, making it difficult to ascertain the generality and robustness of the reported results across multiple runs.' Questions=\"How does the approach generalize to multi-class classification tasks, given its current focus on binary classification? Are there plans to conduct experiments to validate the approach's effectiveness in more diverse datasets and environments beyond the narrow focus presented in this study?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses important gaps in time series analysis by introducing a novel noise-aware sampling strategy and an efficient encoder architecture that promotes consistent representation learning. The experiments are thorough and demonstrate that the method outperforms state-of-the-art approaches across various tasks.' Weaknesses='The novelty of the method may be questioned as the use of dilated convolutions and inception modules are well-known techniques in deep learning. The description of how noise is explicitly handled seems to require further clarification and rigorous empirical justification. The paper lacks detailed explanations of the theoretical underpinnings of the proposed strategies, which could hinder reproduction and understanding.' Questions='What specific aspects of the novel sampling strategy contribute to its effectiveness in accounting for noise? Can the authors provide additional insights into how this sampling strategy compares to other noise-handling approaches in time series analysis? How does the proposed encoder architecture perform on long time series sequences and data with missing values?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses important gaps in time series analysis by introducing a novel noise-aware sampling strategy and an efficient encoder architecture that promotes consistent representation learning. The experiments are thorough and demonstrate that the method outperforms state-of-the-art approaches across various tasks.' Weaknesses='The novelty of the method may be questioned as the use of dilated convolutions and inception modules are well-known techniques in deep learning. The description of how noise is explicitly handled seems to require further clarification and rigorous empirical justification. The paper lacks detailed explanations of the theoretical underpinnings of the proposed strategies, which could hinder reproduction and understanding.' Questions='What specific aspects of the novel sampling strategy contribute to its effectiveness in accounting for noise? Can the authors provide additional insights into how this sampling strategy compares to other noise-handling approaches in time series analysis? How does the proposed encoder architecture perform on long time series sequences and data with missing values?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel method, Vicinal Risk Proxy (VRP), to improve the generalization indicators used to estimate out-of-distribution accuracy. The approach of integrating model responses of neighboring test samples is innovative and addresses the issues with spurious model responses.' Weaknesses='The paper relies heavily on the assumption that neighboring test samples will contribute to better correctness indicator estimation, which might not hold for all datasets. The effectiveness might be highly dependent on data structure and distribution, and the paper does not seem to address potential limitations in various contexts.' Questions='How does the proposed Vicinal Risk Proxy method perform across different types of datasets beyond what is mentioned? Are there specific domains where this method might be less effective due to sample distribution considerations?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel method, Vicinal Risk Proxy (VRP), to improve the generalization indicators used to estimate out-of-distribution accuracy. The approach of integrating model responses of neighboring test samples is innovative and addresses the issues with spurious model responses.' Weaknesses='The paper relies heavily on the assumption that neighboring test samples will contribute to better correctness indicator estimation, which might not hold for all datasets. The effectiveness might be highly dependent on data structure and distribution, and the paper does not seem to address potential limitations in various contexts.' Questions='How does the proposed Vicinal Risk Proxy method perform across different types of datasets beyond what is mentioned? Are there specific domains where this method might be less effective due to sample distribution considerations?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper presents a novel and effective approach to incorporating edge directionality in graph transformers, leveraging dual encodings and a directional attention module. The proposed approach shows state-of-the-art results on directional graph datasets, highlighting its practical significance. Additionally, the introduction of new directional graph datasets is a valuable contribution to the field.' Weaknesses='The presentation could be improved to make the content more accessible. While the approach demonstrates promising results, further clarification on the underlying theory and intuition would strengthen the paper. Also, more diverse datasets could further validate the robustness of the approach.' Questions='How do the dual encodings specifically improve performance on tasks compared to traditional single encoding methods? Are there any limitations observed with the proposed method on certain types of graphs?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper presents a novel and effective approach to incorporating edge directionality in graph transformers, leveraging dual encodings and a directional attention module. The proposed approach shows state-of-the-art results on directional graph datasets, highlighting its practical significance. Additionally, the introduction of new directional graph datasets is a valuable contribution to the field.' Weaknesses='The presentation could be improved to make the content more accessible. While the approach demonstrates promising results, further clarification on the underlying theory and intuition would strengthen the paper. Also, more diverse datasets could further validate the robustness of the approach.' Questions='How do the dual encodings specifically improve performance on tasks compared to traditional single encoding methods? Are there any limitations observed with the proposed method on certain types of graphs?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper addresses a relatively underexplored area in GNN adversarial attacks -- restricted black-box attacks -- which is quite relevant for real-world scenarios. The proposal of the Modified Silhouette Score (MSS) for assessing graph quality is innovative and well-justified through theoretical analysis. The experimental setup is diverse, including both synthetic and real-world graphs, which strengthens the validation of their approach.' Weaknesses='The main weakness comes from the potential limitations of MSS in some scenarios, particularly if the assumptions underlying the modified score do not fully apply across various types of graphs. While the empirical results are promising, the paper could benefit from more detailed discussion on the computational complexity and the specific conditions under which MSS or SheAttack might fail to perform optimally or generalize. Additionally, clarity in some parts of the presentation can be improved to make the methods more accessible to a broader audience.' Questions=\"Can you elaborate on how MSS compares to other metrics traditionally used in graph quality measurement? Are there specific types of graph structures where MSS may not perform as expected? Additionally, what are the implications of theoretical assumptions in real-world applications, and how sensitive is SheAttack's performance to variations in these settings?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper addresses a relatively underexplored area in GNN adversarial attacks -- restricted black-box attacks -- which is quite relevant for real-world scenarios. The proposal of the Modified Silhouette Score (MSS) for assessing graph quality is innovative and well-justified through theoretical analysis. The experimental setup is diverse, including both synthetic and real-world graphs, which strengthens the validation of their approach.' Weaknesses='The main weakness comes from the potential limitations of MSS in some scenarios, particularly if the assumptions underlying the modified score do not fully apply across various types of graphs. While the empirical results are promising, the paper could benefit from more detailed discussion on the computational complexity and the specific conditions under which MSS or SheAttack might fail to perform optimally or generalize. Additionally, clarity in some parts of the presentation can be improved to make the methods more accessible to a broader audience.' Questions=\"Can you elaborate on how MSS compares to other metrics traditionally used in graph quality measurement? Are there specific types of graph structures where MSS may not perform as expected? Additionally, what are the implications of theoretical assumptions in real-world applications, and how sensitive is SheAttack's performance to variations in these settings?\"\n",
      "Finished gpt4o on data/test_data/few_shot/test_data_2024_abstract_prompts_fewshot.jsonl, saved => results/gpt4o/test_data_2024_abstract_prompts_fewshot_gpt4o_results.jsonl\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper presents a novel integration of gradient-based and weight-entangled NAS methods, offering significant improvements in terms of memory efficiency and search effectiveness. The systematic approach to adapting gradient-based optimizers to weight-entangled spaces is innovative and well-motivated. The empirical evaluation across diverse search spaces bolsters the credibility and applicability of the proposed method.' Weaknesses='While the paper presents a compelling method, the presentation of results and comparisons could be enhanced for clarity and accessibility. Detailed breakdowns of the experiments and configurations are somewhat lacking, which may hinder reproducibility. Additionally, certain technical aspects of the TangleNAS framework, such as the handling of edge cases in weight-entanglement, could be elaborated further.' Questions='What are the specific technical challenges faced when adapting gradient-based NAS methods to weight-entangled spaces, and how does TangleNAS address them? How does the proposed method scale when further extending it to more complex search architectures, such as large-scale transformer models?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper presents a novel integration of gradient-based and weight-entangled NAS methods, offering significant improvements in terms of memory efficiency and search effectiveness. The systematic approach to adapting gradient-based optimizers to weight-entangled spaces is innovative and well-motivated. The empirical evaluation across diverse search spaces bolsters the credibility and applicability of the proposed method.' Weaknesses='While the paper presents a compelling method, the presentation of results and comparisons could be enhanced for clarity and accessibility. Detailed breakdowns of the experiments and configurations are somewhat lacking, which may hinder reproducibility. Additionally, certain technical aspects of the TangleNAS framework, such as the handling of edge cases in weight-entanglement, could be elaborated further.' Questions='What are the specific technical challenges faced when adapting gradient-based NAS methods to weight-entangled spaces, and how does TangleNAS address them? How does the proposed method scale when further extending it to more complex search architectures, such as large-scale transformer models?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. Novel application of contrastive loss functions for learning fitness functions in protein engineering, which addresses challenges associated with nonlinearity in existing methods.\\n2. Demonstrates improved recovery and ranking of fitness functions in simulations and real-world benchmark tasks.\\n3. Provides a more data-efficient and robust method under noisy conditions compared to traditional approaches.' Weaknesses='1. Application and validation appear to be limited to specific datasets and tasks, potentially limiting generalization across diverse biological scenarios.\\n2. More extensive comparison with a broader range of existing methods would strengthen the validation of claimed advantages.' Questions='1. How well does the proposed method perform across different types of nonlinear transformations?\\n2. Are there any particular limitations or computational costs associated with implementing contrastive loss functions in practice for this domain?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. Novel application of contrastive loss functions for learning fitness functions in protein engineering, which addresses challenges associated with nonlinearity in existing methods.\\n2. Demonstrates improved recovery and ranking of fitness functions in simulations and real-world benchmark tasks.\\n3. Provides a more data-efficient and robust method under noisy conditions compared to traditional approaches.' Weaknesses='1. Application and validation appear to be limited to specific datasets and tasks, potentially limiting generalization across diverse biological scenarios.\\n2. More extensive comparison with a broader range of existing methods would strengthen the validation of claimed advantages.' Questions='1. How well does the proposed method perform across different types of nonlinear transformations?\\n2. Are there any particular limitations or computational costs associated with implementing contrastive loss functions in practice for this domain?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=6 Confidence=4 Strengths='The approach introduces a lightweight design for lifelong test-time adaptation, making it well-suited for scenarios with limited computational resources. The model does not require access to source data during adaptation, increasing its practicality in real-world settings.' Weaknesses=\"The method's applicability might be limited to specific types of neural networks or setups. There is also potential room for improvement in the clarity and detail of the methodological description, as well as in the evaluation of the proposed solution across more diverse datasets.\" Questions='How does the method perform on architectures other than convolutional networks? Are there any limitations concerning the type of pre-trained models to which these adapters can be applied?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=6 Confidence=4 Strengths='The approach introduces a lightweight design for lifelong test-time adaptation, making it well-suited for scenarios with limited computational resources. The model does not require access to source data during adaptation, increasing its practicality in real-world settings.' Weaknesses=\"The method's applicability might be limited to specific types of neural networks or setups. There is also potential room for improvement in the clarity and detail of the methodological description, as well as in the evaluation of the proposed solution across more diverse datasets.\" Questions='How does the method perform on architectures other than convolutional networks? Are there any limitations concerning the type of pre-trained models to which these adapters can be applied?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The introduction of MINERVA, a feature selection filter using MINE, highlights an innovative approach to capturing complex feature-target dependencies.\\nThe method shows promise in providing more accurate feature selections in scenarios with non-linear dependencies as demonstrated on synthetic benchmarks.\\nMINERVA effectively combines neural networks with information-theoretic concepts to improve feature selection processes.' Weaknesses=\"The paper primarily demonstrates the method's effectiveness on synthetic datasets, which may raise concerns about its applicability to real-world datasets with diverse characteristics and noise.\\nThe computational complexity and scalability of the method for high-dimensional datasets are not thoroughly discussed, which could limit its practical use in real-world applications.\\nThe paper does not provide a comparison to a diverse enough set of existing methods to fully establish the superiority of their approach.\" Questions='How does MINERVA perform on real-world datasets with high-dimensional features and noise?\\nWhat is the computational overhead associated with running MINERVA compared to traditional methods?\\nCan the method be extended or adapted to evaluate time-series or sequential data where dependencies are temporal?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The introduction of MINERVA, a feature selection filter using MINE, highlights an innovative approach to capturing complex feature-target dependencies.\\nThe method shows promise in providing more accurate feature selections in scenarios with non-linear dependencies as demonstrated on synthetic benchmarks.\\nMINERVA effectively combines neural networks with information-theoretic concepts to improve feature selection processes.' Weaknesses=\"The paper primarily demonstrates the method's effectiveness on synthetic datasets, which may raise concerns about its applicability to real-world datasets with diverse characteristics and noise.\\nThe computational complexity and scalability of the method for high-dimensional datasets are not thoroughly discussed, which could limit its practical use in real-world applications.\\nThe paper does not provide a comparison to a diverse enough set of existing methods to fully establish the superiority of their approach.\" Questions='How does MINERVA perform on real-world datasets with high-dimensional features and noise?\\nWhat is the computational overhead associated with running MINERVA compared to traditional methods?\\nCan the method be extended or adapted to evaluate time-series or sequential data where dependencies are temporal?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=2 Rating=5 Confidence=3 Strengths='The analysis of the role of margins from a gradient perspective is a novel contribution, offering insights into the elemental effects of margins in contrastive learning. The experimental evaluation on datasets like CIFAR-10, CIFAR-100, STL-10, and TinyImageNet provides empirical support for the theoretical findings.' Weaknesses=\"The theoretical analysis could be more rigorous and detailed. The explanation of the impact of margins on gradients may have been oversimplified, and the connection to practical improvements in SSL needs to be stronger to demonstrate the significance of the contribution. Additionally, some methodological details might lack clarity, affecting the study's reproducibility.\" Questions='How would this gradient-based analysis of margins generalize across different self-supervised learning tasks beyond the ones tested in the paper? Is there any potential limitation when applying this approach to models with different architectures or training regimes?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=2 Rating=5 Confidence=3 Strengths='The analysis of the role of margins from a gradient perspective is a novel contribution, offering insights into the elemental effects of margins in contrastive learning. The experimental evaluation on datasets like CIFAR-10, CIFAR-100, STL-10, and TinyImageNet provides empirical support for the theoretical findings.' Weaknesses=\"The theoretical analysis could be more rigorous and detailed. The explanation of the impact of margins on gradients may have been oversimplified, and the connection to practical improvements in SSL needs to be stronger to demonstrate the significance of the contribution. Additionally, some methodological details might lack clarity, affecting the study's reproducibility.\" Questions='How would this gradient-based analysis of margins generalize across different self-supervised learning tasks beyond the ones tested in the paper? Is there any potential limitation when applying this approach to models with different architectures or training regimes?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses the underexplored issue of label distribution shift in unsupervised domain adaptation, providing a novel methodological approach that is supported by empirical results.\\n2. The proposed CASA framework presents a theoretical risk bound that is novel and relevant for the domain adaptation community.\\n3. The method demonstrates superior performance across multiple benchmark datasets compared to state-of-the-art approaches.' Weaknesses='1. The presentation of the theoretical contributions could be further clarified to enhance accessibility for a broader audience, especially those less familiar with the mathematical nuances.\\n2. Although empirical results are provided, more detailed ablation studies could strengthen the understanding of the contribution of each component of the CASA framework.\\n3. Limited discussion on potential limitations or future work regarding the extension of CASA in more complex scenarios like universal domain adaptation.' Questions='1. Could CASA be extended to work effectively in scenarios with multiple domains, such as in domain generalization settings? If so, how?\\n2. How sensitive is CASA to the choice of hyperparameters such as those involved in minimizing the target conditional entropy?\\n3. What are the computational costs associated with CASA compared to other state-of-the-art methods?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses the underexplored issue of label distribution shift in unsupervised domain adaptation, providing a novel methodological approach that is supported by empirical results.\\n2. The proposed CASA framework presents a theoretical risk bound that is novel and relevant for the domain adaptation community.\\n3. The method demonstrates superior performance across multiple benchmark datasets compared to state-of-the-art approaches.' Weaknesses='1. The presentation of the theoretical contributions could be further clarified to enhance accessibility for a broader audience, especially those less familiar with the mathematical nuances.\\n2. Although empirical results are provided, more detailed ablation studies could strengthen the understanding of the contribution of each component of the CASA framework.\\n3. Limited discussion on potential limitations or future work regarding the extension of CASA in more complex scenarios like universal domain adaptation.' Questions='1. Could CASA be extended to work effectively in scenarios with multiple domains, such as in domain generalization settings? If so, how?\\n2. How sensitive is CASA to the choice of hyperparameters such as those involved in minimizing the target conditional entropy?\\n3. What are the computational costs associated with CASA compared to other state-of-the-art methods?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths=\"1. The TRACE benchmark is comprehensive, covering a wide range of tasks including domain-specific tasks, multilingual capabilities, and mathematical reasoning. This diversity is a notable strength that challenges LLMs in various dimensions.\\n2. The introduction of new evaluation metrics, such as 'General Ability Delta,' 'Instruction Following Delta,' and 'Safety Delta,' offers a nuanced approach to evaluating the impact of continual learning methods on LLMs.\\n3. The paper demonstrates a thorough evaluation of different sequential fine-tuning techniques and introduces a novel RCL approach that highlights the potential benefits of reasoning-augmented learning strategies.\" Weaknesses=\"1. While TRACE benchmark integrates various challenging tasks, some might argue that it doesn't inherently solve the issue of existing continual learning benchmarks possibly being simplistic, as it draws heavily on existing datasets without proposing new tasks. \\n2. The paper may have insufficient experimental comparisons with a broader set of continual learning methods, potentially limiting the claims about the RCL approach. \\n3. The text relies heavily on theoretical discussions about TRACE and RCL without substantial empirical evidence for their benefits, particularly the reasoning augmentation part.\" Questions='1. How does the RCL approach compare with other state-of-the-art continual learning methods beyond those nascently explored in the paper?\\n2. Could the TRACE benchmark be augmented or adapted to better generalize to other types of models or tasks beyond LLMs?\\n3. Is the reasoning augmentation approach applicable in real-time, or does it require a substantial computational overhead?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths=\"1. The TRACE benchmark is comprehensive, covering a wide range of tasks including domain-specific tasks, multilingual capabilities, and mathematical reasoning. This diversity is a notable strength that challenges LLMs in various dimensions.\\n2. The introduction of new evaluation metrics, such as 'General Ability Delta,' 'Instruction Following Delta,' and 'Safety Delta,' offers a nuanced approach to evaluating the impact of continual learning methods on LLMs.\\n3. The paper demonstrates a thorough evaluation of different sequential fine-tuning techniques and introduces a novel RCL approach that highlights the potential benefits of reasoning-augmented learning strategies.\" Weaknesses=\"1. While TRACE benchmark integrates various challenging tasks, some might argue that it doesn't inherently solve the issue of existing continual learning benchmarks possibly being simplistic, as it draws heavily on existing datasets without proposing new tasks. \\n2. The paper may have insufficient experimental comparisons with a broader set of continual learning methods, potentially limiting the claims about the RCL approach. \\n3. The text relies heavily on theoretical discussions about TRACE and RCL without substantial empirical evidence for their benefits, particularly the reasoning augmentation part.\" Questions='1. How does the RCL approach compare with other state-of-the-art continual learning methods beyond those nascently explored in the paper?\\n2. Could the TRACE benchmark be augmented or adapted to better generalize to other types of models or tasks beyond LLMs?\\n3. Is the reasoning augmentation approach applicable in real-time, or does it require a substantial computational overhead?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses an important issue of label noise in GNNs, which is often overlooked, especially in heterophilous graphs and high noise scenarios.\\n2. The proposed PRGNN framework is novel and does not rely on the common label smoothness assumption, potentially enhancing robustness.\\n3. PRGNN demonstrates superior performance across various noise types and levels, showcasing its practical utility and effectiveness.' Weaknesses='1. The complexity of the probabilistic graphical model-based framework might hinder its applicability or understanding by practitioners not well-versed in graphical models.\\n2. The assumption of clean label reconstruction via Bayesian networks needs further empirical validation across diverse graph domains.\\n3. Details on hyperparameter selection or sensitivity analysis in PRGNN were not emphasized, which might impact reproducibility or adaptation to other datasets.' Questions='1. How does PRGNN perform in real-world applications where true clean labels are not available, and noise distribution might be unknown or non-standard?\\n2. What are the computational overheads of using the proposed variational inference approach compared to traditional GNN methods?\\n3. Can the authors elaborate on the limitations or failure cases of PRGNN, particularly in extremely high-dimensional graph settings?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses an important issue of label noise in GNNs, which is often overlooked, especially in heterophilous graphs and high noise scenarios.\\n2. The proposed PRGNN framework is novel and does not rely on the common label smoothness assumption, potentially enhancing robustness.\\n3. PRGNN demonstrates superior performance across various noise types and levels, showcasing its practical utility and effectiveness.' Weaknesses='1. The complexity of the probabilistic graphical model-based framework might hinder its applicability or understanding by practitioners not well-versed in graphical models.\\n2. The assumption of clean label reconstruction via Bayesian networks needs further empirical validation across diverse graph domains.\\n3. Details on hyperparameter selection or sensitivity analysis in PRGNN were not emphasized, which might impact reproducibility or adaptation to other datasets.' Questions='1. How does PRGNN perform in real-world applications where true clean labels are not available, and noise distribution might be unknown or non-standard?\\n2. What are the computational overheads of using the proposed variational inference approach compared to traditional GNN methods?\\n3. Can the authors elaborate on the limitations or failure cases of PRGNN, particularly in extremely high-dimensional graph settings?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The method tackles the challenging problem of catastrophic forgetting and loss of plasticity in continual learning with an innovative approach, combining utility-based perturbations and gradient descent effectively. The framework is versatile, being applicable to both representation learning and reinforcement learning tasks, which increases its potential impact. The paper successfully demonstrates improved performance over state-of-the-art methods across various datasets, suggesting that UPGD is a robust and effective solution for continual learning environments.' Weaknesses=\"The paper could benefit from a deeper theoretical analysis of the UPGD method, particularly concerning the underlying assumptions and limitations. While the empirical results are promising, the perturbation aspect of the algorithm isn't thoroughly justified or explored, potentially leaving some readers questioning its necessity or optimality. Additionally, the current evaluations are primarily on standard benchmarks, which might not capture the full range of complexities present in real-world streaming data.\" Questions='How does the utility-based perturbation impact resource requirements compared to traditional methods in terms of computational overhead? Additionally, have the authors considered evaluating their method in more complex, real-world scenarios beyond benchmark datasets?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The method tackles the challenging problem of catastrophic forgetting and loss of plasticity in continual learning with an innovative approach, combining utility-based perturbations and gradient descent effectively. The framework is versatile, being applicable to both representation learning and reinforcement learning tasks, which increases its potential impact. The paper successfully demonstrates improved performance over state-of-the-art methods across various datasets, suggesting that UPGD is a robust and effective solution for continual learning environments.' Weaknesses=\"The paper could benefit from a deeper theoretical analysis of the UPGD method, particularly concerning the underlying assumptions and limitations. While the empirical results are promising, the perturbation aspect of the algorithm isn't thoroughly justified or explored, potentially leaving some readers questioning its necessity or optimality. Additionally, the current evaluations are primarily on standard benchmarks, which might not capture the full range of complexities present in real-world streaming data.\" Questions='How does the utility-based perturbation impact resource requirements compared to traditional methods in terms of computational overhead? Additionally, have the authors considered evaluating their method in more complex, real-world scenarios beyond benchmark datasets?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper highlights a critical vulnerability in LLMs and VLLMs when handling multiple-choice question answering tasks, emphasizing an aspect often overlooked in model evaluations.\\n2. The approach of systematically testing permutations across various LLMs and datasets provides a compelling demonstration of the systemic nature of the issue.' Weaknesses='1. The analysis primarily focuses on simple permutation attacks; a more comprehensive exploration of other adversarial tactics could provide deeper insights into the robustness of these models.\\n2. The paper could benefit from a deeper investigation into why current mitigation strategies are ineffective and potential alternative solutions.' Questions='1. What are the specific characteristics of certain datasets or models that contribute to higher vulnerability in permutation attacks?\\n2. Is there potential for training or fine-tuning models specifically to enhance resilience to such adversarial attacks? If so, what initial approaches might be explored?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper highlights a critical vulnerability in LLMs and VLLMs when handling multiple-choice question answering tasks, emphasizing an aspect often overlooked in model evaluations.\\n2. The approach of systematically testing permutations across various LLMs and datasets provides a compelling demonstration of the systemic nature of the issue.' Weaknesses='1. The analysis primarily focuses on simple permutation attacks; a more comprehensive exploration of other adversarial tactics could provide deeper insights into the robustness of these models.\\n2. The paper could benefit from a deeper investigation into why current mitigation strategies are ineffective and potential alternative solutions.' Questions='1. What are the specific characteristics of certain datasets or models that contribute to higher vulnerability in permutation attacks?\\n2. Is there potential for training or fine-tuning models specifically to enhance resilience to such adversarial attacks? If so, what initial approaches might be explored?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The proposed SuFP approach addresses a significant gap in existing quantization methods by effectively handling both dense near-zero data and outliers. This method combines multi-region quantization with scalable bias and efficient hardware implementation, demonstrating significant improvements in accuracy, computational capability, and energy efficiency. The evaluation across various domains adds robustness and credibility to the findings.' Weaknesses=\"The paper could benefit from a more detailed analysis of how the proposed method specifically outperforms existing methods like MSFP and BSFP. It's not entirely clear how the multi-region quantization and scalable bias are implemented, and more technical details would help in understanding the complexity and scalability of the approach. The paper relies on extensive comparisons with traditional FP32, FP16, but lacks sufficient comparison with more modern quantization techniques.\" Questions='How does the SuFP method specifically adapt to different domains such as vision, language, and generative models? Is there any specific tuning required for each domain, or does the method work universally well across different tasks? Could the authors provide more quantitative comparisons with other modern quantization techniques besides FP32 and FP16?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The proposed SuFP approach addresses a significant gap in existing quantization methods by effectively handling both dense near-zero data and outliers. This method combines multi-region quantization with scalable bias and efficient hardware implementation, demonstrating significant improvements in accuracy, computational capability, and energy efficiency. The evaluation across various domains adds robustness and credibility to the findings.' Weaknesses=\"The paper could benefit from a more detailed analysis of how the proposed method specifically outperforms existing methods like MSFP and BSFP. It's not entirely clear how the multi-region quantization and scalable bias are implemented, and more technical details would help in understanding the complexity and scalability of the approach. The paper relies on extensive comparisons with traditional FP32, FP16, but lacks sufficient comparison with more modern quantization techniques.\" Questions='How does the SuFP method specifically adapt to different domains such as vision, language, and generative models? Is there any specific tuning required for each domain, or does the method work universally well across different tasks? Could the authors provide more quantitative comparisons with other modern quantization techniques besides FP32 and FP16?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The proposed variance-reduced ZO hard-thresholding algorithm effectively addresses limitations in the existing SZOHT algorithm. It broadens the applicability and improves convergence rates, particularly in the context of high-dimensional machine learning tasks involving sparse learning solutions.' Weaknesses='The theoretical analysis of the algorithm under certain assumptions like restricted strong smoothness and convexity might limit general applicability. The practical impact could be contingent on these constraints. Furthermore, the presentation could be improved to make the methodology more accessible.' Questions='What are the specific conditions or scenarios where this algorithm provides a significant advantage over existing methods? Are there examples where the assumptions of restricted strong smoothness and convexity do not hold, and if so, how does the algorithm perform under those conditions?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The proposed variance-reduced ZO hard-thresholding algorithm effectively addresses limitations in the existing SZOHT algorithm. It broadens the applicability and improves convergence rates, particularly in the context of high-dimensional machine learning tasks involving sparse learning solutions.' Weaknesses='The theoretical analysis of the algorithm under certain assumptions like restricted strong smoothness and convexity might limit general applicability. The practical impact could be contingent on these constraints. Furthermore, the presentation could be improved to make the methodology more accessible.' Questions='What are the specific conditions or scenarios where this algorithm provides a significant advantage over existing methods? Are there examples where the assumptions of restricted strong smoothness and convexity do not hold, and if so, how does the algorithm perform under those conditions?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The method, FARZI, presents an innovative approach to data distillation tailored specifically for autoregressive tasks, efficiently summarizing large datasets while retaining performance. 2. The paper is well-organized with clear explanations and demonstrates the effectiveness of the method across multiple tasks, highlighting substantial reductions in data size with little to no performance loss.' Weaknesses=\"1. The practicality and scalability of FARZI for extremely large models and vocabulary sizes are not entirely clear. The paper could benefit from a deeper exploration of computation time savings and resource efficiency compared to standard training procedures. 2. More in-depth comparisons with other state-of-the-art data summarization techniques could strengthen the argument for FARZI's relative benefits.\" Questions='1. How does FARZI handle very large vocabulary sizes in autoregressive tasks, and are there any limitations observed in this context? 2. Can the authors provide further insights into the runtime and computational cost benefits of using FARZI compared to full-scale training in practical, large-scale deployments? 3. Is there potential for applying FARZI to other domains beyond language modeling and sequential recommendation, such as vision tasks with sequential dependencies?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The method, FARZI, presents an innovative approach to data distillation tailored specifically for autoregressive tasks, efficiently summarizing large datasets while retaining performance. 2. The paper is well-organized with clear explanations and demonstrates the effectiveness of the method across multiple tasks, highlighting substantial reductions in data size with little to no performance loss.' Weaknesses=\"1. The practicality and scalability of FARZI for extremely large models and vocabulary sizes are not entirely clear. The paper could benefit from a deeper exploration of computation time savings and resource efficiency compared to standard training procedures. 2. More in-depth comparisons with other state-of-the-art data summarization techniques could strengthen the argument for FARZI's relative benefits.\" Questions='1. How does FARZI handle very large vocabulary sizes in autoregressive tasks, and are there any limitations observed in this context? 2. Can the authors provide further insights into the runtime and computational cost benefits of using FARZI compared to full-scale training in practical, large-scale deployments? 3. Is there potential for applying FARZI to other domains beyond language modeling and sequential recommendation, such as vision tasks with sequential dependencies?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper makes a compelling contribution by addressing a gap in offline RL with nonlinear function approximation, providing new instance-dependent regret guarantees. It demonstrates both theoretical rigor and practical relevance by proving the statistical optimality of the proposed method in this complex domain.' Weaknesses='The algorithm relies on certain uniform data coverage assumptions, which may not align well with practical applications where data coverage is often uneven. The absence of a matching lower bound for general function approximation raises questions about the defined optimality conditions and the real-world applicability of theoretical guarantees.' Questions='How does the PNLSVI algorithm compare in performance to other offline RL algorithms when applied to practical datasets with non-uniform coverage?\\nWhat types of real-world scenarios or datasets is this proposed algorithm most applicable to?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper makes a compelling contribution by addressing a gap in offline RL with nonlinear function approximation, providing new instance-dependent regret guarantees. It demonstrates both theoretical rigor and practical relevance by proving the statistical optimality of the proposed method in this complex domain.' Weaknesses='The algorithm relies on certain uniform data coverage assumptions, which may not align well with practical applications where data coverage is often uneven. The absence of a matching lower bound for general function approximation raises questions about the defined optimality conditions and the real-world applicability of theoretical guarantees.' Questions='How does the PNLSVI algorithm compare in performance to other offline RL algorithms when applied to practical datasets with non-uniform coverage?\\nWhat types of real-world scenarios or datasets is this proposed algorithm most applicable to?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The study effectively highlights the application of behavioral game theory to understand the interactive behaviors of LLMs. It showcases thorough experimentation across a variety of game scenarios, offering valuable insights into how LLMs perform in strategic interactions, which is crucial as these models see more widespread usage in society.' Weaknesses=\"While the paper presents a novel approach to evaluating LLMs within game-theoretical contexts, its notion of 'coordination failure' could benefit from a more rigorous evaluation. The effectiveness of using behavioral game prompts may vary significantly based on the context and framing, which the study may not have fully explored. There also seems to be an assumption that LLMs operating in these experimental settings directly translate to real-world interactions without substantial adaptation or training, which may not be the case.\" Questions=\"1. How does the choice of behavioral prompts and their framing influence the LLM's performance across different game scenarios? \\n2. Can the coordination and prediction capabilities of LLMs be substantially improved through fine-tuning or reinforcement learning techniques beyond the framework of behavioral prompts? \\n3. What implications do the findings have for integrating LLMs into more complex, multi-agent systems where the nature of interaction and coordination is even more intricate than the games studied here?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The study effectively highlights the application of behavioral game theory to understand the interactive behaviors of LLMs. It showcases thorough experimentation across a variety of game scenarios, offering valuable insights into how LLMs perform in strategic interactions, which is crucial as these models see more widespread usage in society.' Weaknesses=\"While the paper presents a novel approach to evaluating LLMs within game-theoretical contexts, its notion of 'coordination failure' could benefit from a more rigorous evaluation. The effectiveness of using behavioral game prompts may vary significantly based on the context and framing, which the study may not have fully explored. There also seems to be an assumption that LLMs operating in these experimental settings directly translate to real-world interactions without substantial adaptation or training, which may not be the case.\" Questions=\"1. How does the choice of behavioral prompts and their framing influence the LLM's performance across different game scenarios? \\n2. Can the coordination and prediction capabilities of LLMs be substantially improved through fine-tuning or reinforcement learning techniques beyond the framework of behavioral prompts? \\n3. What implications do the findings have for integrating LLMs into more complex, multi-agent systems where the nature of interaction and coordination is even more intricate than the games studied here?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The study extends the understanding of scale effects in structure learning algorithms, providing theoretical insights and empirical evidence. It addresses a gap in higher-dimensional and non-linear settings, highlighting the potential issues of existing loss functions in identifying correct DAGs.' Weaknesses='The findings are limited to specific settings and assumptions, which may not fully generalize to all real-world applications. The reliance on synthetic data for experiments may also limit the applicability of the conclusions to complex, real-world datasets. Additionally, the paper could benefit from more detailed recommendations on mitigating scale effects in practice.' Questions='What specific normalization techniques do the authors recommend for mitigating scale effects in structure learning algorithms? Are there particular types of datasets or domains where these findings are especially relevant, or where the proposed mitigation strategies are particularly crucial?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The study extends the understanding of scale effects in structure learning algorithms, providing theoretical insights and empirical evidence. It addresses a gap in higher-dimensional and non-linear settings, highlighting the potential issues of existing loss functions in identifying correct DAGs.' Weaknesses='The findings are limited to specific settings and assumptions, which may not fully generalize to all real-world applications. The reliance on synthetic data for experiments may also limit the applicability of the conclusions to complex, real-world datasets. Additionally, the paper could benefit from more detailed recommendations on mitigating scale effects in practice.' Questions='What specific normalization techniques do the authors recommend for mitigating scale effects in structure learning algorithms? Are there particular types of datasets or domains where these findings are especially relevant, or where the proposed mitigation strategies are particularly crucial?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=2 Rating=5 Confidence=4 Strengths='The paper offers a combination of methods that improves the computational efficiency of clustering while still capable of handling complex datasets. The presentation of the two-phase methodology shows potential for scalability and lower runtime.' Weaknesses=\"The novelty of combining K-Means with community detection methods isn't strongly emphasized. Initial clustering with K-Means might limit performance, especially on complex datasets. The comparison is limited to a few datasets, lacking broader benchmarking against modern techniques.\" Questions='How does the method specifically handle the limitations of the initial K-Means clustering step? Is there potential for iterative improvements?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=2 Rating=5 Confidence=4 Strengths='The paper offers a combination of methods that improves the computational efficiency of clustering while still capable of handling complex datasets. The presentation of the two-phase methodology shows potential for scalability and lower runtime.' Weaknesses=\"The novelty of combining K-Means with community detection methods isn't strongly emphasized. Initial clustering with K-Means might limit performance, especially on complex datasets. The comparison is limited to a few datasets, lacking broader benchmarking against modern techniques.\" Questions='How does the method specifically handle the limitations of the initial K-Means clustering step? Is there potential for iterative improvements?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The hybrid framework leverages both the scalability of SLMs and the vast knowledge base of LLMs, thereby improving the generalizability and factuality of LLM outputs, especially in OOD scenarios. The paper provides comprehensive evaluation across multiple datasets and tasks, demonstrating the robustness and applicability of the SuperContext method.' Weaknesses='The framework appears conceptually straightforward, with the primary innovation being the integration of confidence-weighted SLM outputs into LLM prompts. There is a reliance on existing models (Llama 2 and ChatGPT) without a detailed dive into the limitations or potential failure cases of the SuperContext methodology in more diverse or adversarial scenarios.' Questions='How does the framework handle conflicting predictions from SLMs and LLMs, especially when confidence scores are ambiguous or borderline? Could there be scalability issues if deploying this framework in real-time applications that require quick processing of large volumes of data?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The hybrid framework leverages both the scalability of SLMs and the vast knowledge base of LLMs, thereby improving the generalizability and factuality of LLM outputs, especially in OOD scenarios. The paper provides comprehensive evaluation across multiple datasets and tasks, demonstrating the robustness and applicability of the SuperContext method.' Weaknesses='The framework appears conceptually straightforward, with the primary innovation being the integration of confidence-weighted SLM outputs into LLM prompts. There is a reliance on existing models (Llama 2 and ChatGPT) without a detailed dive into the limitations or potential failure cases of the SuperContext methodology in more diverse or adversarial scenarios.' Questions='How does the framework handle conflicting predictions from SLMs and LLMs, especially when confidence scores are ambiguous or borderline? Could there be scalability issues if deploying this framework in real-time applications that require quick processing of large volumes of data?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='1. UDIM introduces a novel approach by combining parameter and data perturbation to enhance domain generalization, demonstrating a clear improvement over existing methods.\\n2. The approach is theoretically grounded, leveraging aspects of SAM optimization while addressing its limitations in domain generalization.\\n3. Empirical results on multiple DG benchmarks show consistent improvement, indicating the robustness and efficacy of the method across different datasets.' Weaknesses='1. The paper could benefit from a more detailed analysis of the theoretical aspects, especially regarding the bounding of the true DG objective by the proposed framework.\\n2. While the use of data perturbation is innovative, the approach and choice of perturbations are not fully explored or justified, leaving room for skepticism about their applicability to diverse domains.\\n3. The explanation of how the combined perturbation method specifically addresses the limitations of current SAM variants in unseen domains could be more detailed.' Questions='1. How is the data perturbation approach chosen or optimized for different datasets or domain scenarios? Is there a risk of overfitting the perturbation strategy to the training data?\\n2. Can UDIM be effectively combined with existing domain alignment or augmentation-based methods to enhance generalization further?\\n3. What are the computational overheads introduced by UDIM compared to standard SAM variants, and are there efficiency implications in large-scale applications?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='1. UDIM introduces a novel approach by combining parameter and data perturbation to enhance domain generalization, demonstrating a clear improvement over existing methods.\\n2. The approach is theoretically grounded, leveraging aspects of SAM optimization while addressing its limitations in domain generalization.\\n3. Empirical results on multiple DG benchmarks show consistent improvement, indicating the robustness and efficacy of the method across different datasets.' Weaknesses='1. The paper could benefit from a more detailed analysis of the theoretical aspects, especially regarding the bounding of the true DG objective by the proposed framework.\\n2. While the use of data perturbation is innovative, the approach and choice of perturbations are not fully explored or justified, leaving room for skepticism about their applicability to diverse domains.\\n3. The explanation of how the combined perturbation method specifically addresses the limitations of current SAM variants in unseen domains could be more detailed.' Questions='1. How is the data perturbation approach chosen or optimized for different datasets or domain scenarios? Is there a risk of overfitting the perturbation strategy to the training data?\\n2. Can UDIM be effectively combined with existing domain alignment or augmentation-based methods to enhance generalization further?\\n3. What are the computational overheads introduced by UDIM compared to standard SAM variants, and are there efficiency implications in large-scale applications?'\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Content:\n",
      "Apologies for the error. Please disregard the previous response. I'm working on addressing it.\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper introduces a novel approach by effectively combining iterative and parallel reasoning, which shows improved efficiency in visual reasoning tasks.\\n2. The proposed IPRM model demonstrates impressive results across multiple challenging datasets, establishing state-of-the-art performance in some areas.\\n3. The architecture is designed to enhance generalization, reducing the number of parameters and computation steps compared to previous models.' Weaknesses='1. The integration of iterative and parallel computation may add complexity to the model, potentially making it difficult to interpret or implement easily.\\n2. Detailed ablation studies or visualization tools could further highlight the contribution of each component within the IPRM framework, enhancing understanding of its functioning and benefits.\\n3. While the approach yields excellent results, real-world applicability in more diverse and less structured datasets than those tested could be explored further.' Questions='1. How does the IPRM model handle cases where the visual input may contain noise or irrelevant details that could affect the reasoning accuracy?\\n2. Can the authors provide further insights into the trade-offs between parameter size, computational efficiency, and accuracy achieved by IPRM compared to iterative-only or parallel-only methods?\\n3. Could additional results on diverse datasets from varied domains further validate the adaptability and robustness of the proposed model?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper introduces a novel approach by effectively combining iterative and parallel reasoning, which shows improved efficiency in visual reasoning tasks.\\n2. The proposed IPRM model demonstrates impressive results across multiple challenging datasets, establishing state-of-the-art performance in some areas.\\n3. The architecture is designed to enhance generalization, reducing the number of parameters and computation steps compared to previous models.' Weaknesses='1. The integration of iterative and parallel computation may add complexity to the model, potentially making it difficult to interpret or implement easily.\\n2. Detailed ablation studies or visualization tools could further highlight the contribution of each component within the IPRM framework, enhancing understanding of its functioning and benefits.\\n3. While the approach yields excellent results, real-world applicability in more diverse and less structured datasets than those tested could be explored further.' Questions='1. How does the IPRM model handle cases where the visual input may contain noise or irrelevant details that could affect the reasoning accuracy?\\n2. Can the authors provide further insights into the trade-offs between parameter size, computational efficiency, and accuracy achieved by IPRM compared to iterative-only or parallel-only methods?\\n3. Could additional results on diverse datasets from varied domains further validate the adaptability and robustness of the proposed model?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper effectively introduces a novel pre-training paradigm for epidemic forecasting, borrowing concepts from successful pre-training methods in natural language processing and computer vision. The approach is innovative in its application to epidemiology, demonstrating significant improvements in forecasting accuracy across multiple diseases. The integration of self-supervised learning tasks, such as RANDMASK and PEAKMASK, enriches the model's ability to generalize to unseen epidemics.\" Weaknesses=\"The proposed model's reliance on diverse, high-quality historical datasets could limit its applicability in regions with sparse or unreliable epidemic data. While the pre-training approach is novel within the domain of epidemiology, the methodological contributions are mostly applications of existing machine learning techniques, which may not provide significant new insights to the wider machine learning community. Additionally, the paper does not detail how the model's performance scales with different input data qualities or how it handles incomplete datasets.\" Questions=\"How does the performance of PEMS change with varying amounts of historical data, particularly in low-data settings? What are the limitations of the self-supervised tasks in capturing epidemic dynamics, and do they apply uniformly across different diseases? Could there be further optimizations in the model architecture specifically tailored for epidemic prediction that aren't derived from general-purpose transformer models in NLP?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper effectively introduces a novel pre-training paradigm for epidemic forecasting, borrowing concepts from successful pre-training methods in natural language processing and computer vision. The approach is innovative in its application to epidemiology, demonstrating significant improvements in forecasting accuracy across multiple diseases. The integration of self-supervised learning tasks, such as RANDMASK and PEAKMASK, enriches the model's ability to generalize to unseen epidemics.\" Weaknesses=\"The proposed model's reliance on diverse, high-quality historical datasets could limit its applicability in regions with sparse or unreliable epidemic data. While the pre-training approach is novel within the domain of epidemiology, the methodological contributions are mostly applications of existing machine learning techniques, which may not provide significant new insights to the wider machine learning community. Additionally, the paper does not detail how the model's performance scales with different input data qualities or how it handles incomplete datasets.\" Questions=\"How does the performance of PEMS change with varying amounts of historical data, particularly in low-data settings? What are the limitations of the self-supervised tasks in capturing epidemic dynamics, and do they apply uniformly across different diseases? Could there be further optimizations in the model architecture specifically tailored for epidemic prediction that aren't derived from general-purpose transformer models in NLP?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper provides a practical and effective method for improving both robust and average accuracies without requiring additional training. The introduction of a class-specific scaling strategy and the novel evaluation metric (robust coverage) add significant value. The approach is clearly beneficial in scenarios where retraining is not feasible or desired.' Weaknesses='The paper may oversimplify the problem by focusing primarily on post-processing methods, potentially overlooking more sophisticated methods that integrate such strategies during the model training process. While the approach is effective, there could be limitations in scenarios where class imbalances or extremely small minority groups exist, as these might impact the efficiency of class-specific scaling and clustering methods.' Questions='What datasets were used for evaluation, and how were they chosen to ensure representative data across minority groups? How sensitive is the class-specific scaling approach to the chosen clustering algorithm, and have alternative clustering methods been considered?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper provides a practical and effective method for improving both robust and average accuracies without requiring additional training. The introduction of a class-specific scaling strategy and the novel evaluation metric (robust coverage) add significant value. The approach is clearly beneficial in scenarios where retraining is not feasible or desired.' Weaknesses='The paper may oversimplify the problem by focusing primarily on post-processing methods, potentially overlooking more sophisticated methods that integrate such strategies during the model training process. While the approach is effective, there could be limitations in scenarios where class imbalances or extremely small minority groups exist, as these might impact the efficiency of class-specific scaling and clustering methods.' Questions='What datasets were used for evaluation, and how were they chosen to ensure representative data across minority groups? How sensitive is the class-specific scaling approach to the chosen clustering algorithm, and have alternative clustering methods been considered?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper proposes an innovative approach that relaxes the typical independence assumptions in disentangled representation learning, using contrastive learning to address correlated attributes in time series data. This is particularly useful for applications like NILM, where real-world data often exhibit such correlations. The introduction of the Time Disentangling Score (TDS) as a new metric for assessing disentanglement quality is a noteworthy contribution. The experimental results show significant improvements over traditional methods, indicating the effectiveness of the proposed approach.' Weaknesses='While the approach is promising, the paper could improve its discussion on the limitations and challenges of the proposed method. The assumption of support independence, although less stringent than total independence, might still be difficult to achieve in practice, and the implications of this should be discussed. Additionally, the paper should provide more clarity on how contrastive learning is implemented in the context of time series data, as well as more detailed ablation studies to understand the contribution of each component of the method. The presentation could be enhanced with clearer explanations of the technical details and a more structured flow of information.' Questions='1. How does the relaxation of independence to independence-of-support specifically help in the context of NILM?\\n2. Can the proposed approach be adapted to other types of time series data beyond NILM, and what considerations would need to be made for such generalizations?\\n3. How sensitive is the model to the choice of hyperparameters in the contrastive loss function, and were any specific validation strategies employed to optimize these?\\n4. How does the Time Disentangling Score (TDS) compare to other disentanglement metrics, and why was it necessary to propose a new metric?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper proposes an innovative approach that relaxes the typical independence assumptions in disentangled representation learning, using contrastive learning to address correlated attributes in time series data. This is particularly useful for applications like NILM, where real-world data often exhibit such correlations. The introduction of the Time Disentangling Score (TDS) as a new metric for assessing disentanglement quality is a noteworthy contribution. The experimental results show significant improvements over traditional methods, indicating the effectiveness of the proposed approach.' Weaknesses='While the approach is promising, the paper could improve its discussion on the limitations and challenges of the proposed method. The assumption of support independence, although less stringent than total independence, might still be difficult to achieve in practice, and the implications of this should be discussed. Additionally, the paper should provide more clarity on how contrastive learning is implemented in the context of time series data, as well as more detailed ablation studies to understand the contribution of each component of the method. The presentation could be enhanced with clearer explanations of the technical details and a more structured flow of information.' Questions='1. How does the relaxation of independence to independence-of-support specifically help in the context of NILM?\\n2. Can the proposed approach be adapted to other types of time series data beyond NILM, and what considerations would need to be made for such generalizations?\\n3. How sensitive is the model to the choice of hyperparameters in the contrastive loss function, and were any specific validation strategies employed to optimize these?\\n4. How does the Time Disentangling Score (TDS) compare to other disentanglement metrics, and why was it necessary to propose a new metric?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper introduces the Qwen-VL series that significantly outperforms existing LVLMs across multiple benchmarks in both visual and multilingual tasks.\\n2. The three-stage training pipeline, including large-scale pre-training, multi-task pre-training, and supervised fine-tuning, is methodically designed to enhance fine-grained visual understanding and multilingual capabilities.\\n3. The introduction of a position-aware vision-language adapter effectively allows for efficient image feature compression, contributing to superior model performance.' Weaknesses=\"1. Despite the significant advances, the paper does not sufficiently discuss the specific challenges encountered during the integration of multilingual modalities, which could be crucial for researchers attempting to build on this work.\\n2. The potential limitations or trade-offs of using a large-scale pre-training approach with weakly labeled data are not fully explored, which might impact the model's robustness in practical applications.\\n3. The paper could provide more detailed comparisons with proprietary models to better highlight the advances and remaining gaps in performance.\" Questions=\"1. How does the model handle situations where weakly labeled data might introduce noise during the large-scale pre-training stage? Are there mechanisms in place to mitigate such noise?\\n2. The paper mentions a comprehensive evaluation on various benchmarks, but how does the model's performance compare in real-world applications where there might be a lack of pre-existing multilingual and multimodal datasets?\\n3. Are there plans to extend the model to incorporate additional modalities such as audio or to expand its application in more diverse real-world scenarios? If so, how would the architecture need to be adjusted?\"\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper introduces the Qwen-VL series that significantly outperforms existing LVLMs across multiple benchmarks in both visual and multilingual tasks.\\n2. The three-stage training pipeline, including large-scale pre-training, multi-task pre-training, and supervised fine-tuning, is methodically designed to enhance fine-grained visual understanding and multilingual capabilities.\\n3. The introduction of a position-aware vision-language adapter effectively allows for efficient image feature compression, contributing to superior model performance.' Weaknesses=\"1. Despite the significant advances, the paper does not sufficiently discuss the specific challenges encountered during the integration of multilingual modalities, which could be crucial for researchers attempting to build on this work.\\n2. The potential limitations or trade-offs of using a large-scale pre-training approach with weakly labeled data are not fully explored, which might impact the model's robustness in practical applications.\\n3. The paper could provide more detailed comparisons with proprietary models to better highlight the advances and remaining gaps in performance.\" Questions=\"1. How does the model handle situations where weakly labeled data might introduce noise during the large-scale pre-training stage? Are there mechanisms in place to mitigate such noise?\\n2. The paper mentions a comprehensive evaluation on various benchmarks, but how does the model's performance compare in real-world applications where there might be a lack of pre-existing multilingual and multimodal datasets?\\n3. Are there plans to extend the model to incorporate additional modalities such as audio or to expand its application in more diverse real-world scenarios? If so, how would the architecture need to be adjusted?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. HaDES offers a novel approach to applying dataset distillation to reinforcement learning tasks by distilling state-action pairs into a synthetic dataset, which is both innovative and practical for reducing data requirements in RL.\\n2. The method shows impressive flexibility and robustness across different tasks and RL settings, suggesting it has potential utility in various applications.\\n3. The approach is well-situated within existing research, with a clear explanation of how it advances the state-of-the-art in dataset distillation.' Weaknesses='1. The requirement for a meta-evolutionary approach and bi-level optimization might introduce complexity that could make implementation challenging for broader adoption.\\n2. While the experiments are comprehensive, there could be additional analysis on the impact of different hyper-parameters or architectures on the performance of distilled datasets.\\n3. The paper might benefit from a more detailed discussion of potential limitations or challenges in real-world applications, such as scalability or generalization to more complex environments.' Questions='1. Can you provide more insight into the hyper-parameter choices and their impact on the effectiveness of HaDES? Are there specific settings that significantly enhance or degrade performance?\\n2. How does HaDES perform when faced with more complex or continuous state spaces in RL environments? Are there scalability limitations in such cases?\\n3. What are the potential real-world applications of behaviour distillation as developed here, and how does HaDES compare in terms of efficiency and performance to existing RL approaches without distillation?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='1. HaDES offers a novel approach to applying dataset distillation to reinforcement learning tasks by distilling state-action pairs into a synthetic dataset, which is both innovative and practical for reducing data requirements in RL.\\n2. The method shows impressive flexibility and robustness across different tasks and RL settings, suggesting it has potential utility in various applications.\\n3. The approach is well-situated within existing research, with a clear explanation of how it advances the state-of-the-art in dataset distillation.' Weaknesses='1. The requirement for a meta-evolutionary approach and bi-level optimization might introduce complexity that could make implementation challenging for broader adoption.\\n2. While the experiments are comprehensive, there could be additional analysis on the impact of different hyper-parameters or architectures on the performance of distilled datasets.\\n3. The paper might benefit from a more detailed discussion of potential limitations or challenges in real-world applications, such as scalability or generalization to more complex environments.' Questions='1. Can you provide more insight into the hyper-parameter choices and their impact on the effectiveness of HaDES? Are there specific settings that significantly enhance or degrade performance?\\n2. How does HaDES perform when faced with more complex or continuous state spaces in RL environments? Are there scalability limitations in such cases?\\n3. What are the potential real-world applications of behaviour distillation as developed here, and how does HaDES compare in terms of efficiency and performance to existing RL approaches without distillation?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed P+ space and XTI method offer a novel approach to increasing the expressiveness and control of text-to-image synthesis, which is a valuable contribution to the field of generative models. The paper demonstrates empirical improvements over existing methods like Textual Inversion, which highlights its practical significance.' Weaknesses='While the method shows promise, it lacks a detailed theoretical justification for why layer-specific conditioning improves image synthesis control. The paper could benefit from a more in-depth analysis of the impact of the P+ space on different aspects of image quality, such as diversity or fidelity.' Questions='How does the choice of specific per-layer tokens in P+ influence image synthesis, and is there a strategy for optimizing these tokens? Additionally, how does this approach generalize across different text-to-image models beyond the Stable Diffusion model used in the experiments?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed P+ space and XTI method offer a novel approach to increasing the expressiveness and control of text-to-image synthesis, which is a valuable contribution to the field of generative models. The paper demonstrates empirical improvements over existing methods like Textual Inversion, which highlights its practical significance.' Weaknesses='While the method shows promise, it lacks a detailed theoretical justification for why layer-specific conditioning improves image synthesis control. The paper could benefit from a more in-depth analysis of the impact of the P+ space on different aspects of image quality, such as diversity or fidelity.' Questions='How does the choice of specific per-layer tokens in P+ influence image synthesis, and is there a strategy for optimizing these tokens? Additionally, how does this approach generalize across different text-to-image models beyond the Stable Diffusion model used in the experiments?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The approach is novel in transforming SFUDA into a semi-supervised learning problem, utilizing multiple hypotheses and their rationale for pseudo-labeling.\\n2. The method shows state-of-the-art performance in SFUDA tasks and can be integrated with existing approaches easily.\\n3. Provides a systematic way of consolidating hypothesis rationales to improve pseudo-label reliability, which is beneficial for unsupervised adaptation scenarios.' Weaknesses='1. Explanation regarding the motivation for using multiple high-quality hypotheses on the same domain is somewhat unclear, as accessing multiple hypotheses pre-trained on the same domain can be challenging in practice.\\n2. The approach introduces several hyperparameters ($\\\\tilde{k}$, $\\\\tau_1$, and $\\\\tau_2$) that might not be easily transferable across different tasks, requiring extensive tuning.\\n3. Incorrect representation in Eq. (4) might suggest other overlooked mathematical inaccuracies elsewhere.' Questions=\"1. Could you elaborate on the practical considerations and potential challenges of obtaining multiple source hypotheses pre-trained on the same domain?\\n2. How would the values of critical hyperparameters like $\\\\tilde{k}$ be chosen for application to other datasets, and what impact do they have on the model's robustness?\\n3. Why does increasing the number of hypotheses seemingly degrade performance, as suggested by results in Table 5, contrary to the expectation that more hypotheses might yield improved pseudo-labels?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The approach is novel in transforming SFUDA into a semi-supervised learning problem, utilizing multiple hypotheses and their rationale for pseudo-labeling.\\n2. The method shows state-of-the-art performance in SFUDA tasks and can be integrated with existing approaches easily.\\n3. Provides a systematic way of consolidating hypothesis rationales to improve pseudo-label reliability, which is beneficial for unsupervised adaptation scenarios.' Weaknesses='1. Explanation regarding the motivation for using multiple high-quality hypotheses on the same domain is somewhat unclear, as accessing multiple hypotheses pre-trained on the same domain can be challenging in practice.\\n2. The approach introduces several hyperparameters ($\\\\tilde{k}$, $\\\\tau_1$, and $\\\\tau_2$) that might not be easily transferable across different tasks, requiring extensive tuning.\\n3. Incorrect representation in Eq. (4) might suggest other overlooked mathematical inaccuracies elsewhere.' Questions=\"1. Could you elaborate on the practical considerations and potential challenges of obtaining multiple source hypotheses pre-trained on the same domain?\\n2. How would the values of critical hyperparameters like $\\\\tilde{k}$ be chosen for application to other datasets, and what impact do they have on the model's robustness?\\n3. Why does increasing the number of hypotheses seemingly degrade performance, as suggested by results in Table 5, contrary to the expectation that more hypotheses might yield improved pseudo-labels?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel method, SOInter, for interpreting structured output models, emphasizing the importance of considering output variable correlations for explanations. This approach shows improved explanation quality over existing methods such as Lime and Kernel-Shap, particularly in scenarios with significant variable correlations. The integration of a globally-trained local interpreter using deep energy-based training is an innovative technique.' Weaknesses='While the paper provides a solid foundation, it may lack in providing detailed analysis on the sensitivity of the model to hyperparameters and the robustness of interpretations when applied to various types of data (e.g., images, text). More comprehensive experimentation covering a wider array of structured output models and tasks could strengthen the validation of the proposed approach.' Questions='How sensitive is the SOInter method to the choice of hyperparameters such as the number of features to select? Can the approach be applied to other types of data like images or audio, and if so, how would it adapt the feature selection mechanism to non-tabular data? Are there any specific considerations or limitations when using the Gumbel-Softmax trick in this context?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel method, SOInter, for interpreting structured output models, emphasizing the importance of considering output variable correlations for explanations. This approach shows improved explanation quality over existing methods such as Lime and Kernel-Shap, particularly in scenarios with significant variable correlations. The integration of a globally-trained local interpreter using deep energy-based training is an innovative technique.' Weaknesses='While the paper provides a solid foundation, it may lack in providing detailed analysis on the sensitivity of the model to hyperparameters and the robustness of interpretations when applied to various types of data (e.g., images, text). More comprehensive experimentation covering a wider array of structured output models and tasks could strengthen the validation of the proposed approach.' Questions='How sensitive is the SOInter method to the choice of hyperparameters such as the number of features to select? Can the approach be applied to other types of data like images or audio, and if so, how would it adapt the feature selection mechanism to non-tabular data? Are there any specific considerations or limitations when using the Gumbel-Softmax trick in this context?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides a compelling case for the use of simpler, traditional nonparametric techniques in SSNC tasks. It presents a clear methodology for utilizing spectral domain methods and highlights the advantages of maintaining model simplicity while achieving competitive performance with complex GNN approaches.' Weaknesses='1. The paper may overstate the generalizability of using simpler spectral methods over complex GNN architectures. While the results show promise, additional comparative analysis and broader datasets would strengthen the claims.\\n2. The method description lacks depth in explaining the potential limitations or scenarios where simpler models may fall short compared to advanced GNNs.' Questions='1. Can the proposed method generalize well to more diverse and challenging datasets beyond the typical benchmarks like Cora and Pubmed?\\n2. How does the choice of kernel functions in the spectral domain affect model performance, and is there guidance on selecting the most appropriate kernel?\\n3. Would the proposed simpler models be effective in highly dynamic graphs or graphs with rapidly changing structure, where adaptability is critical?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides a compelling case for the use of simpler, traditional nonparametric techniques in SSNC tasks. It presents a clear methodology for utilizing spectral domain methods and highlights the advantages of maintaining model simplicity while achieving competitive performance with complex GNN approaches.' Weaknesses='1. The paper may overstate the generalizability of using simpler spectral methods over complex GNN architectures. While the results show promise, additional comparative analysis and broader datasets would strengthen the claims.\\n2. The method description lacks depth in explaining the potential limitations or scenarios where simpler models may fall short compared to advanced GNNs.' Questions='1. Can the proposed method generalize well to more diverse and challenging datasets beyond the typical benchmarks like Cora and Pubmed?\\n2. How does the choice of kernel functions in the spectral domain affect model performance, and is there guidance on selecting the most appropriate kernel?\\n3. Would the proposed simpler models be effective in highly dynamic graphs or graphs with rapidly changing structure, where adaptability is critical?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The introduction of a low-rank branch to separately optimize natural and adversarial objectives is an innovative and pragmatic approach to addressing gradient divergence issues in Robust Fine-Tuning (RFT). Additionally, AutoLoRa's heuristic strategies for scheduling learning rates and loss term scalars tackle hyperparameter sensitivity problems efficaciously. The paper presents comprehensive empirical evidence demonstrating the effectiveness of this method across various tasks, showcasing state-of-the-art adversarial robustness. This practical contribution could significantly improve the robustness of pre-trained models used in critical applications.\" Weaknesses=\"While AutoLoRa's low-rank branch approach and automated heuristics for learning rate and loss scaling are appealing, the specific implementation details and potential limitations of these heuristic strategies are not thoroughly examined. It remains unclear how these heuristics might perform across diverse architectures or dataset complexities not covered in the exploration. Furthermore, the empirical analysis appears predominantly centered on ResNet models, which raises questions about the generalizability of the findings to other network architectures without further evidence.\" Questions='1. How does AutoLoRa perform with architectures other than ResNet, such as transformer-based models?\\n2. Could the authors elaborate on the choice of heuristic strategies for scheduling learning rates and loss term scalars? How transferable are these strategies to different model architectures or datasets?\\n3. Are there any specific limitations or potential failure cases identified for AutoLoRa, particularly in less structured/more complex datasets not included in the studies?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The introduction of a low-rank branch to separately optimize natural and adversarial objectives is an innovative and pragmatic approach to addressing gradient divergence issues in Robust Fine-Tuning (RFT). Additionally, AutoLoRa's heuristic strategies for scheduling learning rates and loss term scalars tackle hyperparameter sensitivity problems efficaciously. The paper presents comprehensive empirical evidence demonstrating the effectiveness of this method across various tasks, showcasing state-of-the-art adversarial robustness. This practical contribution could significantly improve the robustness of pre-trained models used in critical applications.\" Weaknesses=\"While AutoLoRa's low-rank branch approach and automated heuristics for learning rate and loss scaling are appealing, the specific implementation details and potential limitations of these heuristic strategies are not thoroughly examined. It remains unclear how these heuristics might perform across diverse architectures or dataset complexities not covered in the exploration. Furthermore, the empirical analysis appears predominantly centered on ResNet models, which raises questions about the generalizability of the findings to other network architectures without further evidence.\" Questions='1. How does AutoLoRa perform with architectures other than ResNet, such as transformer-based models?\\n2. Could the authors elaborate on the choice of heuristic strategies for scheduling learning rates and loss term scalars? How transferable are these strategies to different model architectures or datasets?\\n3. Are there any specific limitations or potential failure cases identified for AutoLoRa, particularly in less structured/more complex datasets not included in the studies?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses a significant limitation in existing Graph Lottery Ticket (GLT) methods by proposing an adaptive and dynamic framework that enables deeper Graph Neural Networks (GNNs) to efficiently identify graph lottery tickets. The proposed method, AdaGLT, offers a scalable and automated solution with layer-adaptive sparsification, which is a notable improvement over existing techniques. The experiments show promising results, indicating the utility and effectiveness of AdaGLT in various graph datasets.' Weaknesses='While the paper presents a novel approach, it may lack experimental diversity. The evaluation seems mainly focused on synthetic and potentially smaller datasets without addressing its performance on a wider range of real-world and more complex graph datasets or tasks. Additionally, the theoretical proofs should be closely scrutinized, as the complexity and soundness of integration between pruning and dynamic restoration need further validation.' Questions='1. How does AdaGLT perform on larger, more complex graph datasets, such as those with millions of nodes and edges? \\n2. What are the potential trade-offs of using AdaGLT compared to traditional methods in terms of computational resources and execution time due to the dynamic nature of its pruning and training integration? \\n3. Can AdaGLT be extended to support online or streaming graph scenarios where graph topology changes over time?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses a significant limitation in existing Graph Lottery Ticket (GLT) methods by proposing an adaptive and dynamic framework that enables deeper Graph Neural Networks (GNNs) to efficiently identify graph lottery tickets. The proposed method, AdaGLT, offers a scalable and automated solution with layer-adaptive sparsification, which is a notable improvement over existing techniques. The experiments show promising results, indicating the utility and effectiveness of AdaGLT in various graph datasets.' Weaknesses='While the paper presents a novel approach, it may lack experimental diversity. The evaluation seems mainly focused on synthetic and potentially smaller datasets without addressing its performance on a wider range of real-world and more complex graph datasets or tasks. Additionally, the theoretical proofs should be closely scrutinized, as the complexity and soundness of integration between pruning and dynamic restoration need further validation.' Questions='1. How does AdaGLT perform on larger, more complex graph datasets, such as those with millions of nodes and edges? \\n2. What are the potential trade-offs of using AdaGLT compared to traditional methods in terms of computational resources and execution time due to the dynamic nature of its pruning and training integration? \\n3. Can AdaGLT be extended to support online or streaming graph scenarios where graph topology changes over time?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper presents a significant enhancement to a widely-used open model, Stable Diffusion, making it competitive with state-of-the-art black-box models.\\n2. The use of novel conditioning schemes and a refinement model to improve visual fidelity indicate a substantial advancement in generative model design.\\n3. SDXL retains the openness and reproducibility that are crucial for community-driven research and development.' Weaknesses='1. While the approach builds upon Stable Diffusion, the originality in terms of novel techniques might not be as groundbreaking as expected, mostly featuring incremental improvements.\\n2. The paper could provide deeper insights into the trade-offs or limitations encountered with the larger architecture and additional attention blocks in terms of computational cost, which is critical for practical deployment.\\n3. There is less emphasis on how these changes affect training efficiency or the specific scalability considerations.' Questions='1. How do the enhancements in architecture and conditioning specifically affect computational cost and speed compared to previous versions of Stable Diffusion?\\n2. Are there specific domains or applications where SDXL shows significant advantages over both previous open models and current black-box models?\\n3. What are the key limitations of SDXL when handling novel text prompts compared to black-box solutions?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper presents a significant enhancement to a widely-used open model, Stable Diffusion, making it competitive with state-of-the-art black-box models.\\n2. The use of novel conditioning schemes and a refinement model to improve visual fidelity indicate a substantial advancement in generative model design.\\n3. SDXL retains the openness and reproducibility that are crucial for community-driven research and development.' Weaknesses='1. While the approach builds upon Stable Diffusion, the originality in terms of novel techniques might not be as groundbreaking as expected, mostly featuring incremental improvements.\\n2. The paper could provide deeper insights into the trade-offs or limitations encountered with the larger architecture and additional attention blocks in terms of computational cost, which is critical for practical deployment.\\n3. There is less emphasis on how these changes affect training efficiency or the specific scalability considerations.' Questions='1. How do the enhancements in architecture and conditioning specifically affect computational cost and speed compared to previous versions of Stable Diffusion?\\n2. Are there specific domains or applications where SDXL shows significant advantages over both previous open models and current black-box models?\\n3. What are the key limitations of SDXL when handling novel text prompts compared to black-box solutions?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper presents a novel approach, Activation Prompts (AP), which extends the concept of Visual Prompting (VP) by introducing perturbations at intermediate activation map layers. This method shows promise in improving performance and efficiency.\\n2. The extensive experimental evaluation over 29 datasets is a significant strength, indicating the robustness and general applicability of the proposed method.\\n3. The analysis of the connection between AP and normalization tuning techniques provides a theoretical foundation for the proposed approach, enhancing its soundness.' Weaknesses='1. The novelty of the contribution might be seen as incremental, as it builds upon existing concepts in visual prompting and parameter-efficient fine-tuning.\\n2. The analysis and comparison with existing methods may not be comprehensive enough, potentially leaving unanswered questions about the specific advantages of AP over other parameter-efficient fine-tuning methods.\\n3. The implementation details, such as the exact setup of perturbations in AP, might not be sufficiently detailed, which could limit the reproducibility of the experiments.' Questions=\"1. What specific criteria were used to select the layer depths for AP implementation, and how sensitive is the performance to this selection?\\n2. Could the method be extended or adapted to other types of models beyond CNNs and ViTs, and if so, how?\\n3. What are the implications of AP on the interpretability of the model's predictions, and does it introduce any new considerations in this area?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper presents a novel approach, Activation Prompts (AP), which extends the concept of Visual Prompting (VP) by introducing perturbations at intermediate activation map layers. This method shows promise in improving performance and efficiency.\\n2. The extensive experimental evaluation over 29 datasets is a significant strength, indicating the robustness and general applicability of the proposed method.\\n3. The analysis of the connection between AP and normalization tuning techniques provides a theoretical foundation for the proposed approach, enhancing its soundness.' Weaknesses='1. The novelty of the contribution might be seen as incremental, as it builds upon existing concepts in visual prompting and parameter-efficient fine-tuning.\\n2. The analysis and comparison with existing methods may not be comprehensive enough, potentially leaving unanswered questions about the specific advantages of AP over other parameter-efficient fine-tuning methods.\\n3. The implementation details, such as the exact setup of perturbations in AP, might not be sufficiently detailed, which could limit the reproducibility of the experiments.' Questions=\"1. What specific criteria were used to select the layer depths for AP implementation, and how sensitive is the performance to this selection?\\n2. Could the method be extended or adapted to other types of models beyond CNNs and ViTs, and if so, how?\\n3. What are the implications of AP on the interpretability of the model's predictions, and does it introduce any new considerations in this area?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed model effectively draws inspiration from biological systems, which is a novel approach to tackling the problem of catastrophic forgetting. The introduction of the sWM task provides a better evaluation setup for assessing both rapid learning and memory retention capabilities of models.' Weaknesses=\"The paper lacks a thorough comparison with more recent and advanced continual learning methods beyond just the traditional few-shot and continual learning baselines. This limits the understanding of how well the proposed model performs in a broader context. Additionally, the model's generalizability to tasks other than navigation, which the MESH architecture seems particularly suited for, is not demonstrated.\" Questions=\"How does the model's performance change when applied to tasks outside of navigation-focused problems? Will the MESH architecture need adaptations to handle different types of environmental changes or task domains?\\nWhat are the computational costs associated with this biologically inspired model compared to traditional continual learning models?\\nHow effectively can the method scale with the increasing complexity of tasks, both in terms of task structure and environment dimensionality?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed model effectively draws inspiration from biological systems, which is a novel approach to tackling the problem of catastrophic forgetting. The introduction of the sWM task provides a better evaluation setup for assessing both rapid learning and memory retention capabilities of models.' Weaknesses=\"The paper lacks a thorough comparison with more recent and advanced continual learning methods beyond just the traditional few-shot and continual learning baselines. This limits the understanding of how well the proposed model performs in a broader context. Additionally, the model's generalizability to tasks other than navigation, which the MESH architecture seems particularly suited for, is not demonstrated.\" Questions=\"How does the model's performance change when applied to tasks outside of navigation-focused problems? Will the MESH architecture need adaptations to handle different types of environmental changes or task domains?\\nWhat are the computational costs associated with this biologically inspired model compared to traditional continual learning models?\\nHow effectively can the method scale with the increasing complexity of tasks, both in terms of task structure and environment dimensionality?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper effectively addresses the important problem of out-of-distribution detection under class-imbalanced settings, which is a practical concern in real-world applications.\\n2. The proposed ImOOD framework introduces a combination of post-hoc normalization and training-time regularization, offering a novel perspective on OOD detection. This method has demonstrated significant improvements over existing state-of-the-art approaches on relevant benchmarks.\\n3. The experiments conducted on multiple benchmark datasets help to validate the effectiveness of the proposed methods, showing clear benefits over traditional OOD detection techniques.' Weaknesses=\"1. The paper's focus on traditional class imbalance could be expanded to include additional challenges and complexities often found in real-world distributions, such as those found in practical deployments.\\n2. The explanation surrounding the introduced methods could be clearer, especially the underlying intuition behind the post-hoc normalization process and how it integrates with the class-aware bias addressed in the paper.\\n3. Some sections, particularly in the methodology, are dense and could benefit from clearer explanations or additional diagrams to help elucidate key concepts for a broader audience.\" Questions='1. Is the post-hoc normalization dependent on the choice of the OOD detection threshold, or is it applied uniformly across all samples?\\n2. How would ImOOD perform when applied to datasets with more complex or multi-modal distributions? Are there plans to test the method in such settings?\\n3. Can the proposed framework be integrated with existing neural architectures, or is it limited to the particular architectures experimented on in the paper?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper effectively addresses the important problem of out-of-distribution detection under class-imbalanced settings, which is a practical concern in real-world applications.\\n2. The proposed ImOOD framework introduces a combination of post-hoc normalization and training-time regularization, offering a novel perspective on OOD detection. This method has demonstrated significant improvements over existing state-of-the-art approaches on relevant benchmarks.\\n3. The experiments conducted on multiple benchmark datasets help to validate the effectiveness of the proposed methods, showing clear benefits over traditional OOD detection techniques.' Weaknesses=\"1. The paper's focus on traditional class imbalance could be expanded to include additional challenges and complexities often found in real-world distributions, such as those found in practical deployments.\\n2. The explanation surrounding the introduced methods could be clearer, especially the underlying intuition behind the post-hoc normalization process and how it integrates with the class-aware bias addressed in the paper.\\n3. Some sections, particularly in the methodology, are dense and could benefit from clearer explanations or additional diagrams to help elucidate key concepts for a broader audience.\" Questions='1. Is the post-hoc normalization dependent on the choice of the OOD detection threshold, or is it applied uniformly across all samples?\\n2. How would ImOOD perform when applied to datasets with more complex or multi-modal distributions? Are there plans to test the method in such settings?\\n3. Can the proposed framework be integrated with existing neural architectures, or is it limited to the particular architectures experimented on in the paper?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The approach is novel, leveraging statistical characteristics of output token distributions to detect adversarial audio inputs across multiple ASR systems without needing modifications to the data or adversarial training, demonstrating high effectiveness with AUROC exceeding 99%. The paper is well-structured, clearly illustrating methodology and outcomes.' Weaknesses='The approach relies on the statistical analysis of token outputs, which may vary across different models and could depend heavily on the architecture of the ASR systems used. The robustness of the proposed detection method against a wider variety of adaptive attack strategies could be further examined.' Questions='How does the proposed method perform when tested on a wider range of adversarial attack strategies beyond those considered? Can the method be generalized to other modalities beyond ASR or expanded to detect adversarial attacks in audio classification tasks?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The approach is novel, leveraging statistical characteristics of output token distributions to detect adversarial audio inputs across multiple ASR systems without needing modifications to the data or adversarial training, demonstrating high effectiveness with AUROC exceeding 99%. The paper is well-structured, clearly illustrating methodology and outcomes.' Weaknesses='The approach relies on the statistical analysis of token outputs, which may vary across different models and could depend heavily on the architecture of the ASR systems used. The robustness of the proposed detection method against a wider variety of adaptive attack strategies could be further examined.' Questions='How does the proposed method perform when tested on a wider range of adversarial attack strategies beyond those considered? Can the method be generalized to other modalities beyond ASR or expanded to detect adversarial attacks in audio classification tasks?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces Q-flow, a novel flow-based model for dynamic optimal transport between arbitrary distributions, which offers a fresh approach to distribution transformation beyond the usual normal distribution. The method effectively leverages neural ODEs to articulate optimal transport maps, demonstrating strong empirical performance in various high-dimensional data tasks.' Weaknesses=\"The primary critique is the lack of theoretical analysis regarding the optimality of the transport map, as well as the potential pitfalls of extending this approach to more complex distribution scenarios without adequate justification. Additionally, though empirical studies are robust, a deeper exploration into the method's limitations or scenarios where it might falter would strengthen the findings.\" Questions='How does the model ensure that the transport map is optimal, and are there strategies proposed for validating the accuracy or convergence of the transport map across diverse datasets? Are there specific computational requirements or limitations when scaling the method to extremely high-dimensional spaces?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces Q-flow, a novel flow-based model for dynamic optimal transport between arbitrary distributions, which offers a fresh approach to distribution transformation beyond the usual normal distribution. The method effectively leverages neural ODEs to articulate optimal transport maps, demonstrating strong empirical performance in various high-dimensional data tasks.' Weaknesses=\"The primary critique is the lack of theoretical analysis regarding the optimality of the transport map, as well as the potential pitfalls of extending this approach to more complex distribution scenarios without adequate justification. Additionally, though empirical studies are robust, a deeper exploration into the method's limitations or scenarios where it might falter would strengthen the findings.\" Questions='How does the model ensure that the transport map is optimal, and are there strategies proposed for validating the accuracy or convergence of the transport map across diverse datasets? Are there specific computational requirements or limitations when scaling the method to extremely high-dimensional spaces?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper introduces an innovative approach to address the limitations of traditional rSLDS models by integrating a distance-dependent CRP along with latent geometry, which enhances the model's flexibility and applicability to varying neural data. The use of a PDE framework for computational inference is intriguing and suggests a potential improvement in efficiency. The application of the model to synthetic and real electrophysiological data demonstrates its practical relevance and effectiveness in uncovering non-stationary processes.\" Weaknesses=\"While the irSLDS model presents a novel integration of several concepts, the details of implementation and the practical impact of the introduced methods could be elaborated further. The model's scalability and efficiency with larger datasets or in different contexts are not thoroughly addressed, leaving questions about its general applicability. The presentation could benefit from clearer explanations of the methodologies, particularly the PDE framework and its integration with the model.\" Questions='How does the introduction of a latent geometric structure specifically contribute to improved interpretability, and are there any examples or case studies that illustrate this advantage? Can the authors provide further insights into the scalability of the irSLDS model beyond the presented datasets, and how it would perform in more complex neural data scenarios? What are the potential limitations or challenges in extending the model to other types of neural data or alternative domains?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper introduces an innovative approach to address the limitations of traditional rSLDS models by integrating a distance-dependent CRP along with latent geometry, which enhances the model's flexibility and applicability to varying neural data. The use of a PDE framework for computational inference is intriguing and suggests a potential improvement in efficiency. The application of the model to synthetic and real electrophysiological data demonstrates its practical relevance and effectiveness in uncovering non-stationary processes.\" Weaknesses=\"While the irSLDS model presents a novel integration of several concepts, the details of implementation and the practical impact of the introduced methods could be elaborated further. The model's scalability and efficiency with larger datasets or in different contexts are not thoroughly addressed, leaving questions about its general applicability. The presentation could benefit from clearer explanations of the methodologies, particularly the PDE framework and its integration with the model.\" Questions='How does the introduction of a latent geometric structure specifically contribute to improved interpretability, and are there any examples or case studies that illustrate this advantage? Can the authors provide further insights into the scalability of the irSLDS model beyond the presented datasets, and how it would perform in more complex neural data scenarios? What are the potential limitations or challenges in extending the model to other types of neural data or alternative domains?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=3 Strengths='1. The DynaVol method provides a novel approach for addressing 3D consistent scene understanding using object-centric voxelization.\\n2. The multi-stage training process and integration of cycle-consistency loss to ensure better learning dynamics is innovative.\\n3. Experimental results demonstrate that the method outperforms existing 2D and 3D methods in unsupervised dynamic scene decomposition, providing strong empirical evidence of the method’s efficacy.' Weaknesses='1. The methodology seems to rely heavily on structured volumetric data, which may limit its applicability to more complex, unstructured dynamic scenes.\\n2. Details regarding the implementation specifics, especially the interaction between different network components, could be elaborated more clearly.\\n3. While the paper addresses occlusion issues, the evaluation on dynamic scenes should consider more complex environments and interactions to show general robustness.' Questions='1. How does DynaVol handle scenes with rapidly changing dynamics or interactions between multiple objects beyond the assumptions made for voxelization?\\n2. Can the approach be scaled to handle more artifacts or information beyond geometric shapes and motion trajectories in the dynamic scenes?\\n3. Is there a specific real-world application or scenario where DynaVol has been tested outside of the benchmark synthetic environments?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=3 Strengths='1. The DynaVol method provides a novel approach for addressing 3D consistent scene understanding using object-centric voxelization.\\n2. The multi-stage training process and integration of cycle-consistency loss to ensure better learning dynamics is innovative.\\n3. Experimental results demonstrate that the method outperforms existing 2D and 3D methods in unsupervised dynamic scene decomposition, providing strong empirical evidence of the method’s efficacy.' Weaknesses='1. The methodology seems to rely heavily on structured volumetric data, which may limit its applicability to more complex, unstructured dynamic scenes.\\n2. Details regarding the implementation specifics, especially the interaction between different network components, could be elaborated more clearly.\\n3. While the paper addresses occlusion issues, the evaluation on dynamic scenes should consider more complex environments and interactions to show general robustness.' Questions='1. How does DynaVol handle scenes with rapidly changing dynamics or interactions between multiple objects beyond the assumptions made for voxelization?\\n2. Can the approach be scaled to handle more artifacts or information beyond geometric shapes and motion trajectories in the dynamic scenes?\\n3. Is there a specific real-world application or scenario where DynaVol has been tested outside of the benchmark synthetic environments?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='1. P-Seg effectively eliminates the need for annotated image masks, relying instead on pseudo-mask generation, which simplifies the training process and reduces resource dependence.\\n2. The method demonstrates strong performance across multiple benchmark datasets, indicating its scalability and robustness.\\n3. By utilizing a self-supervised Vision Transformer (ViT) and image-text datasets, the approach aligns language features with pixel-level features in a novel manner.' Weaknesses=\"1. The novelty of the proposed approach may be somewhat limited, as it builds upon existing MaskFormer architecture and uses pseudo-mask generation similarly explored by other methods.\\n2. While the results are promising, the experimental comparisons could benefit from more extensive datasets or additional state-of-the-art baselines for a rigorous evaluation.\\n3. The method's reliance on large image-text datasets for effective training might pose a challenge for applications with limited data availability.\" Questions='1. Can the P-Seg method be effectively applied to domains beyond those evaluated, or would it require significant adaptation?\\n2. How sensitive is the performance of P-Seg to the choice of the pseudo-mask generator or the self-supervised ViT model?\\n3. What detailed insights can be drawn about the alignment of language features to pixel-level features, and how effectively can this approach be generalized to handle complex semantic relationships in diverse images?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='1. P-Seg effectively eliminates the need for annotated image masks, relying instead on pseudo-mask generation, which simplifies the training process and reduces resource dependence.\\n2. The method demonstrates strong performance across multiple benchmark datasets, indicating its scalability and robustness.\\n3. By utilizing a self-supervised Vision Transformer (ViT) and image-text datasets, the approach aligns language features with pixel-level features in a novel manner.' Weaknesses=\"1. The novelty of the proposed approach may be somewhat limited, as it builds upon existing MaskFormer architecture and uses pseudo-mask generation similarly explored by other methods.\\n2. While the results are promising, the experimental comparisons could benefit from more extensive datasets or additional state-of-the-art baselines for a rigorous evaluation.\\n3. The method's reliance on large image-text datasets for effective training might pose a challenge for applications with limited data availability.\" Questions='1. Can the P-Seg method be effectively applied to domains beyond those evaluated, or would it require significant adaptation?\\n2. How sensitive is the performance of P-Seg to the choice of the pseudo-mask generator or the self-supervised ViT model?\\n3. What detailed insights can be drawn about the alignment of language features to pixel-level features, and how effectively can this approach be generalized to handle complex semantic relationships in diverse images?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The proposed ShareFormer model addresses the key challenges associated with Transformer-based methods in image restoration, effectively balancing between latency reduction and improved trainability.\\n2. Introduction of Shared Portion Stripe Attention (SPSA) and residual connections enhances the model's efficiency in handling long-range dependencies while reducing computational overhead.\\n3. Comprehensive evaluation and competitive performance benchmarks showcase the potential of ShareFormer across various image restoration tasks such as super-resolution and denoising.\" Weaknesses='1. The presentation of the methodology and its components needs refining to enhance clarity and understanding, particularly of how Shared Portion Stripe Attention and residual connections operate within the model.\\n2. Comparative insights on the ablation of individual components like CSAU and the precise impact on model performance and efficiency are not adequately detailed.\\n3. Some explanations, such as the attention sharing mechanism in Fig.2 and Fig.3, are unclear, potentially causing ambiguity in understanding the exact novelty of the method.' Questions=\"1. Can you clarify the precise operation points for shared attention within the model, as there seems to be a discrepancy in the explanation and illustrations?\\n2. How does the mechanism of residual connections on the 'Value' in self-attention contribute specifically to enhancing locality bias, as claimed? Is there comparative visualization available to substantiate this claim?\\n3. Why were layers 1 and 3 chosen for attention map comparison in Appendix D instead of the directly sequential pairings used in ShareFormer, to demonstrate reduction redundancy more convincingly?\\n4. Was the same training patch size used in SR experiments for ShareFormer as compared to baseline models, to ensure the fairness of comparison?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The proposed ShareFormer model addresses the key challenges associated with Transformer-based methods in image restoration, effectively balancing between latency reduction and improved trainability.\\n2. Introduction of Shared Portion Stripe Attention (SPSA) and residual connections enhances the model's efficiency in handling long-range dependencies while reducing computational overhead.\\n3. Comprehensive evaluation and competitive performance benchmarks showcase the potential of ShareFormer across various image restoration tasks such as super-resolution and denoising.\" Weaknesses='1. The presentation of the methodology and its components needs refining to enhance clarity and understanding, particularly of how Shared Portion Stripe Attention and residual connections operate within the model.\\n2. Comparative insights on the ablation of individual components like CSAU and the precise impact on model performance and efficiency are not adequately detailed.\\n3. Some explanations, such as the attention sharing mechanism in Fig.2 and Fig.3, are unclear, potentially causing ambiguity in understanding the exact novelty of the method.' Questions=\"1. Can you clarify the precise operation points for shared attention within the model, as there seems to be a discrepancy in the explanation and illustrations?\\n2. How does the mechanism of residual connections on the 'Value' in self-attention contribute specifically to enhancing locality bias, as claimed? Is there comparative visualization available to substantiate this claim?\\n3. Why were layers 1 and 3 chosen for attention map comparison in Appendix D instead of the directly sequential pairings used in ShareFormer, to demonstrate reduction redundancy more convincingly?\\n4. Was the same training patch size used in SR experiments for ShareFormer as compared to baseline models, to ensure the fairness of comparison?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The proposed Diff-SR method offers an innovative solution for arbitrary-scale super-resolution by capitalizing on existing pre-trained diffusion models without the need for additional fine-tuning or retraining specific models. It demonstrates significant improvements in image quality across various scales and provides a flexible and efficient approach to super-resolution tasks, validated by thorough experimental results.' Weaknesses=\"The methodology, while innovative, seems to rely heavily on the efficacy of existing pre-trained models, which may have inherent biases or limitations. The details on how the noise level is determined could be further elaborated. Additionally, the approach's reliance on existing diffusion models may limit its applicability to models that fall outside of this framework.\" Questions='Can the proposed noise injection strategy be easily adapted to other types of pre-trained generative models beyond diffusion models? How does the method determine the optimal noise level for different low-resolution images, and does this require significant computational resources? Could the authors provide more insight into the Perceptual Recoverable Field and its computational requirements?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The proposed Diff-SR method offers an innovative solution for arbitrary-scale super-resolution by capitalizing on existing pre-trained diffusion models without the need for additional fine-tuning or retraining specific models. It demonstrates significant improvements in image quality across various scales and provides a flexible and efficient approach to super-resolution tasks, validated by thorough experimental results.' Weaknesses=\"The methodology, while innovative, seems to rely heavily on the efficacy of existing pre-trained models, which may have inherent biases or limitations. The details on how the noise level is determined could be further elaborated. Additionally, the approach's reliance on existing diffusion models may limit its applicability to models that fall outside of this framework.\" Questions='Can the proposed noise injection strategy be easily adapted to other types of pre-trained generative models beyond diffusion models? How does the method determine the optimal noise level for different low-resolution images, and does this require significant computational resources? Could the authors provide more insight into the Perceptual Recoverable Field and its computational requirements?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper provides a strong empirical analysis that highlights the potential oversight in RLHF practices regarding output length.\\n2. It offers a set of interventions to address length bias, showing a methodological approach to a well-identified problem in RLHF.\\n3. The study uses rigorous experimentation across multiple datasets, ensuring that the results are robust and applicable across various contexts.' Weaknesses='1. Although the paper identifies the issue of reward models being biased towards longer outputs, it does not fully address how to adjust reward models to completely remove this bias in practice.\\n2. The focus on length as the main confounder may leave out other factors that could play a significant role, which might not have been thoroughly analyzed.\\n3. The interventions proposed may not completely align with more complex tasks where semantic quality and coherence are critical, as opposed to merely optimizing length.' Questions='1. How can reward models be robustly adjusted to balance length optimization with genuine improvement of output quality?\\n2. Could there be other overlooked factors in RLHF, besides length, that might contribute to perceived improvements? If so, what impact might they have relative to length?\\n3. How generalizable are the findings across different tasks beyond question answering and dialogue, such as creative writing or technical subjects where verbosity may not equate to quality?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper provides a strong empirical analysis that highlights the potential oversight in RLHF practices regarding output length.\\n2. It offers a set of interventions to address length bias, showing a methodological approach to a well-identified problem in RLHF.\\n3. The study uses rigorous experimentation across multiple datasets, ensuring that the results are robust and applicable across various contexts.' Weaknesses='1. Although the paper identifies the issue of reward models being biased towards longer outputs, it does not fully address how to adjust reward models to completely remove this bias in practice.\\n2. The focus on length as the main confounder may leave out other factors that could play a significant role, which might not have been thoroughly analyzed.\\n3. The interventions proposed may not completely align with more complex tasks where semantic quality and coherence are critical, as opposed to merely optimizing length.' Questions='1. How can reward models be robustly adjusted to balance length optimization with genuine improvement of output quality?\\n2. Could there be other overlooked factors in RLHF, besides length, that might contribute to perceived improvements? If so, what impact might they have relative to length?\\n3. How generalizable are the findings across different tasks beyond question answering and dialogue, such as creative writing or technical subjects where verbosity may not equate to quality?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='1. The paper addresses an important problem in the field of graph neural networks by considering the challenge of evolving graphs and their impact on representation distortion and temporal generalization.\\n2. Introducing a self-supervised learning approach to tackle this problem is a novel and promising direction.\\n3. The method shows improved performance in estimation of temporal generalization, indicating potential for real-world applications across several domains.' Weaknesses=\"1. The paper lacks a detailed comparison with existing baseline methods for evolving graphs, which might provide a more comprehensive evaluation of the method's strengths and limitations.\\n2. The assumptions required for the theoretical analysis and derivations could limit the practical applicability of the proposed method in dynamic environments where graphs might frequently shrink or node attributes significantly change.\\n3. More clarity in some sections, such as further explanations of equations or methodological steps, would enhance understanding.\" Questions='1. How does SMART compare to existing methods designed for evolving graphs or heterophilic graphs when applied to newly scaled or significantly altered networks?\\n2. How would the model potentially adapt if modifications occur in features rather than structures over time? Are there any implementations considering edge-weight dynamics?\\n3. Could the use of larger, more complex real-world networks potentially reveal additional challenges not captured in the current experimental settings?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='1. The paper addresses an important problem in the field of graph neural networks by considering the challenge of evolving graphs and their impact on representation distortion and temporal generalization.\\n2. Introducing a self-supervised learning approach to tackle this problem is a novel and promising direction.\\n3. The method shows improved performance in estimation of temporal generalization, indicating potential for real-world applications across several domains.' Weaknesses=\"1. The paper lacks a detailed comparison with existing baseline methods for evolving graphs, which might provide a more comprehensive evaluation of the method's strengths and limitations.\\n2. The assumptions required for the theoretical analysis and derivations could limit the practical applicability of the proposed method in dynamic environments where graphs might frequently shrink or node attributes significantly change.\\n3. More clarity in some sections, such as further explanations of equations or methodological steps, would enhance understanding.\" Questions='1. How does SMART compare to existing methods designed for evolving graphs or heterophilic graphs when applied to newly scaled or significantly altered networks?\\n2. How would the model potentially adapt if modifications occur in features rather than structures over time? Are there any implementations considering edge-weight dynamics?\\n3. Could the use of larger, more complex real-world networks potentially reveal additional challenges not captured in the current experimental settings?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper addresses a well-known challenge in causal inference by proposing a novel approach using diffusion models to handle unobserved confounders. The use of diffusion models is an innovative direction that potentially offers more flexibility and stability compared to traditional methods. The empirical evaluation against state-of-the-art methods on both synthetic and benchmark datasets strengthens the validity of the proposed approach, showing improved ITE estimation.' Weaknesses='While the method shows promising results, the theoretical justification for why diffusion models are particularly well-suited for modeling unobserved confounders could be further elaborated. Additionally, the details on how the method outperforms others in terms of computational efficiency or scalability are limited. The clarity in the presentation can be improved, as some parts are not thoroughly explained, particularly the connection and transition functions in the diffusion process.' Questions='1. How does the diffusion model scale with larger datasets, particularly in terms of computational resources and time? \\n2. Can the authors provide more insight into the choice of diffusion model parameters and how sensitive the results are to these choices?\\n3. Are there any specific scenarios or datasets where the proposed method fails to perform well compared to traditional models?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper addresses a well-known challenge in causal inference by proposing a novel approach using diffusion models to handle unobserved confounders. The use of diffusion models is an innovative direction that potentially offers more flexibility and stability compared to traditional methods. The empirical evaluation against state-of-the-art methods on both synthetic and benchmark datasets strengthens the validity of the proposed approach, showing improved ITE estimation.' Weaknesses='While the method shows promising results, the theoretical justification for why diffusion models are particularly well-suited for modeling unobserved confounders could be further elaborated. Additionally, the details on how the method outperforms others in terms of computational efficiency or scalability are limited. The clarity in the presentation can be improved, as some parts are not thoroughly explained, particularly the connection and transition functions in the diffusion process.' Questions='1. How does the diffusion model scale with larger datasets, particularly in terms of computational resources and time? \\n2. Can the authors provide more insight into the choice of diffusion model parameters and how sensitive the results are to these choices?\\n3. Are there any specific scenarios or datasets where the proposed method fails to perform well compared to traditional models?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed benchmark and method for multi-modal continual learning, leveraging the VGGSound dataset, addresses an important gap in the research area by integrating multi-modal approaches to overcome restrictions in unimodal continual learning, particularly under challenging conditions like class and domain incremental learning.' Weaknesses=\"Lack of experimental diversity, with evaluations limited exclusively to the VGGSound dataset, constrains the demonstration of the proposed method's generalizability. Additionally, comparisons are only made against an older baseline from 2018, lacking insights from competing recent methodologies.\" Questions='What alternate datasets, benchmarks, or state-of-the-art baselines might better illustrate the strengths and weaknesses of SAMM? How does SAMM integrate multi-modal scenarios compared to vision-language continual learning benchmarks?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed benchmark and method for multi-modal continual learning, leveraging the VGGSound dataset, addresses an important gap in the research area by integrating multi-modal approaches to overcome restrictions in unimodal continual learning, particularly under challenging conditions like class and domain incremental learning.' Weaknesses=\"Lack of experimental diversity, with evaluations limited exclusively to the VGGSound dataset, constrains the demonstration of the proposed method's generalizability. Additionally, comparisons are only made against an older baseline from 2018, lacking insights from competing recent methodologies.\" Questions='What alternate datasets, benchmarks, or state-of-the-art baselines might better illustrate the strengths and weaknesses of SAMM? How does SAMM integrate multi-modal scenarios compared to vision-language continual learning benchmarks?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths=\"1. The paper proposes a novel approach to zero-shot text classification by leveraging generative language models with paraphrased label descriptions, which improves robustness to prompt variations.\\n2. The use of multiple paraphrases aggregates results and helps in reducing bias and variance, thus enhancing the model's performance.\\n3. The evaluation of the method across a broad set of classification tasks and with multiple language model families demonstrates the approach's general applicability.\" Weaknesses='1. The reliance on manually crafted paraphrases and descriptions, while innovative, introduces subjectivity and potential bias. Further automation or a clearer framework for crafting effective prompts could strengthen the approach.\\n2. The paper might not sufficiently distinguish the benefits over few-shot learning approaches, especially considering the effort involved in paraphrase generation.\\n3. There is limited exploration of how this approach scales with larger datasets or more complex classification tasks.' Questions='1. How does the effectiveness of GEN-Z compare when used in combination with few-shot learning approaches rather than as a standalone zero-shot method?\\n2. What guidelines can be provided for selecting or generating paraphrases to ensure consistency and minimize biases in outputs?\\n3. Can the method be effectively scaled to multilingual scenarios, and what challenges might arise in such applications?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths=\"1. The paper proposes a novel approach to zero-shot text classification by leveraging generative language models with paraphrased label descriptions, which improves robustness to prompt variations.\\n2. The use of multiple paraphrases aggregates results and helps in reducing bias and variance, thus enhancing the model's performance.\\n3. The evaluation of the method across a broad set of classification tasks and with multiple language model families demonstrates the approach's general applicability.\" Weaknesses='1. The reliance on manually crafted paraphrases and descriptions, while innovative, introduces subjectivity and potential bias. Further automation or a clearer framework for crafting effective prompts could strengthen the approach.\\n2. The paper might not sufficiently distinguish the benefits over few-shot learning approaches, especially considering the effort involved in paraphrase generation.\\n3. There is limited exploration of how this approach scales with larger datasets or more complex classification tasks.' Questions='1. How does the effectiveness of GEN-Z compare when used in combination with few-shot learning approaches rather than as a standalone zero-shot method?\\n2. What guidelines can be provided for selecting or generating paraphrases to ensure consistency and minimize biases in outputs?\\n3. Can the method be effectively scaled to multilingual scenarios, and what challenges might arise in such applications?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The dataset provided is unprecedented in terms of scale and detail within the domain of last-mile delivery. LaDe not only includes a massive amount of package and courier data but also addresses both pickup and delivery contexts, offering a comprehensive view which is often lacking in existing datasets. The authors have demonstrated the dataset's applicability by using it to benchmark several tasks pertinent to logistics and supply chain management.\" Weaknesses='While the dataset is extensive, the paper could have further detailed the specific challenges and quality control measures taken during the dataset compilation. Additionally, the discussion regarding privacy concerns and how they were mitigated is somewhat lacking, given the sensitive nature of logistics data. Moreover, the application of the dataset to provide new insights or discoveries beyond the benchmarked tasks could have been explored more deeply.' Questions='What are the ethical considerations taken into account while collecting and sharing such a comprehensive dataset, particularly considering personal and sensitive data related to packages and couriers? Are there specific limitations to using LaDe across different geographic regions or logistic scenarios due to its origin from Chinese cities?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The dataset provided is unprecedented in terms of scale and detail within the domain of last-mile delivery. LaDe not only includes a massive amount of package and courier data but also addresses both pickup and delivery contexts, offering a comprehensive view which is often lacking in existing datasets. The authors have demonstrated the dataset's applicability by using it to benchmark several tasks pertinent to logistics and supply chain management.\" Weaknesses='While the dataset is extensive, the paper could have further detailed the specific challenges and quality control measures taken during the dataset compilation. Additionally, the discussion regarding privacy concerns and how they were mitigated is somewhat lacking, given the sensitive nature of logistics data. Moreover, the application of the dataset to provide new insights or discoveries beyond the benchmarked tasks could have been explored more deeply.' Questions='What are the ethical considerations taken into account while collecting and sharing such a comprehensive dataset, particularly considering personal and sensitive data related to packages and couriers? Are there specific limitations to using LaDe across different geographic regions or logistic scenarios due to its origin from Chinese cities?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=5 Strengths=\"1. MIMIC provides an innovative approach to curating large-scale datasets from unannotated real-world videos and synthetic environments, overcoming the reliance on heavily annotated datasets.\\n2. The method effectively utilizes computer vision techniques like SIFT and RANSAC to automate dataset creation, which enhances scalability and applicability to real-world scenarios.\\n3. The resulting MIMIC datasets demonstrate superior performance in dense prediction tasks compared to traditional annotated datasets, showcasing the approach's effectiveness.\\n4. The study highlights the potential for scalability in dataset creation, allowing for even larger datasets and improved model training.\" Weaknesses='1. The paper could provide more detailed information on the specific preprocessing and automated steps involved in refining and validating created dataset pairs to ensure the highest quality for downstream tasks.\\n2. Although the results are promising, the diversity and complexity of the selected real-world video sources could be explored further to ensure performance consistency across various real-world scenarios.\\n3. Additional comparative analyses between MIMIC and other self-supervised learning techniques on diverse real-world datasets could have strengthened the argument for its universal applicability and robustness.' Questions='1. How generalizable are the created MIMIC datasets across a wider range of vision tasks beyond those tested in the study? Are there potential limitations for specific types of datasets?\\n2. What are the computational and storage requirements for auto-generating and processing the MIMIC datasets, and how do they compare to existing annotated datasets such as ImageNet?\\n3. Are there any plans to make the MIMIC datasets available to other researchers, and if so, what formats and access methods will be provided?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=5 Strengths=\"1. MIMIC provides an innovative approach to curating large-scale datasets from unannotated real-world videos and synthetic environments, overcoming the reliance on heavily annotated datasets.\\n2. The method effectively utilizes computer vision techniques like SIFT and RANSAC to automate dataset creation, which enhances scalability and applicability to real-world scenarios.\\n3. The resulting MIMIC datasets demonstrate superior performance in dense prediction tasks compared to traditional annotated datasets, showcasing the approach's effectiveness.\\n4. The study highlights the potential for scalability in dataset creation, allowing for even larger datasets and improved model training.\" Weaknesses='1. The paper could provide more detailed information on the specific preprocessing and automated steps involved in refining and validating created dataset pairs to ensure the highest quality for downstream tasks.\\n2. Although the results are promising, the diversity and complexity of the selected real-world video sources could be explored further to ensure performance consistency across various real-world scenarios.\\n3. Additional comparative analyses between MIMIC and other self-supervised learning techniques on diverse real-world datasets could have strengthened the argument for its universal applicability and robustness.' Questions='1. How generalizable are the created MIMIC datasets across a wider range of vision tasks beyond those tested in the study? Are there potential limitations for specific types of datasets?\\n2. What are the computational and storage requirements for auto-generating and processing the MIMIC datasets, and how do they compare to existing annotated datasets such as ImageNet?\\n3. Are there any plans to make the MIMIC datasets available to other researchers, and if so, what formats and access methods will be provided?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents an innovative approach to RNA splicing prediction by leveraging generative model architectures and combining multiple transformers with side information. TrASPr demonstrates outstanding performance improvements over existing models, offering significant advancements in the field of computational biology and splicing analysis. The method’s ability to suggest biologically plausible regulatory mechanisms and generate accurate RNA sequence designs highlights its potential applicability in experimental and therapeutic contexts.' Weaknesses='Some aspects of the model’s implementation and integration could benefit from clearer explanation, such as the specific mechanisms of the model’s application to different tissue types and conditions. There might be room for enhancement in describing the Bayesian Optimization framework and more detailed comparisons against competing models. Also, while the results are promising, the practical applicability constraints in experimental scenarios could be considered more thoroughly.' Questions=\"1. What are the specific limitations or challenges encountered when implementing the TrASPr model across diverse tissue types and complex datasets?\\n2. Could further elaboration on the Bayesian Optimization framework's integration with TrASPr enhance comprehension of its practical application?\\n3. Are there any specific datasets or conditions in which TrASPr performs less optimally, and if so, what modifications or improvements could address these issues?\\n4. How does the model handle instances with exceedingly complex splicing mechanisms, and what impact does this have on its predictive accuracy?\"\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents an innovative approach to RNA splicing prediction by leveraging generative model architectures and combining multiple transformers with side information. TrASPr demonstrates outstanding performance improvements over existing models, offering significant advancements in the field of computational biology and splicing analysis. The method’s ability to suggest biologically plausible regulatory mechanisms and generate accurate RNA sequence designs highlights its potential applicability in experimental and therapeutic contexts.' Weaknesses='Some aspects of the model’s implementation and integration could benefit from clearer explanation, such as the specific mechanisms of the model’s application to different tissue types and conditions. There might be room for enhancement in describing the Bayesian Optimization framework and more detailed comparisons against competing models. Also, while the results are promising, the practical applicability constraints in experimental scenarios could be considered more thoroughly.' Questions=\"1. What are the specific limitations or challenges encountered when implementing the TrASPr model across diverse tissue types and complex datasets?\\n2. Could further elaboration on the Bayesian Optimization framework's integration with TrASPr enhance comprehension of its practical application?\\n3. Are there any specific datasets or conditions in which TrASPr performs less optimally, and if so, what modifications or improvements could address these issues?\\n4. How does the model handle instances with exceedingly complex splicing mechanisms, and what impact does this have on its predictive accuracy?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The proposed method demonstrates significant improvement in WER across benchmark datasets, showcasing its effectiveness in improving CSLR performance.\\n2. The integration of multiple modalities (video, keypoints, and optical flow) utilizing fusion techniques provides a comprehensive approach to feature extraction.\\n3. The hierarchical knowledge distillation framework effectively reduces computational costs while maintaining performance, which is critical for practical CSLR applications.\\n4. The work provides a novel combination of multi-modal learning and knowledge distillation for sign language recognition, addressing a significant gap in the field.' Weaknesses=\"1. The novelty of the approach could be questioned as it primarily leverages known techniques, such as multi-modal fusion and knowledge distillation.\\n2. The paper could provide more detailed insights into how the fusion of modalities specifically contributes to learning better features.\\n3. Evaluation on only three datasets might limit the demonstration of the framework's generalization ability across various sign languages or contexts. Greater diversity in test datasets might better validate the approach.\\n4. There is limited discussion on the potential computational limitations or trade-offs encountered during the implementation of the proposed method.\" Questions='1. How does the proposed method handle complex sequences with high variability in sign languages across different signers or contexts?\\n2. How sensitive is the hierarchical knowledge distillation framework to the chosen teacher-student models and their configurations?\\n3. Would performance improvements continue if additional modalities are introduced, or is there a threshold beyond which additional modalities do not contribute significantly to performance?\\n4. How does the proposed method compare, in terms of computational efficiency and inference time, with other existing CSLR methods?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The proposed method demonstrates significant improvement in WER across benchmark datasets, showcasing its effectiveness in improving CSLR performance.\\n2. The integration of multiple modalities (video, keypoints, and optical flow) utilizing fusion techniques provides a comprehensive approach to feature extraction.\\n3. The hierarchical knowledge distillation framework effectively reduces computational costs while maintaining performance, which is critical for practical CSLR applications.\\n4. The work provides a novel combination of multi-modal learning and knowledge distillation for sign language recognition, addressing a significant gap in the field.' Weaknesses=\"1. The novelty of the approach could be questioned as it primarily leverages known techniques, such as multi-modal fusion and knowledge distillation.\\n2. The paper could provide more detailed insights into how the fusion of modalities specifically contributes to learning better features.\\n3. Evaluation on only three datasets might limit the demonstration of the framework's generalization ability across various sign languages or contexts. Greater diversity in test datasets might better validate the approach.\\n4. There is limited discussion on the potential computational limitations or trade-offs encountered during the implementation of the proposed method.\" Questions='1. How does the proposed method handle complex sequences with high variability in sign languages across different signers or contexts?\\n2. How sensitive is the hierarchical knowledge distillation framework to the chosen teacher-student models and their configurations?\\n3. Would performance improvements continue if additional modalities are introduced, or is there a threshold beyond which additional modalities do not contribute significantly to performance?\\n4. How does the proposed method compare, in terms of computational efficiency and inference time, with other existing CSLR methods?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel approach to inverse rendering using a unified voxelization framework, which helps in efficiently encoding scene properties like geometry, materials, and illumination. The use of lightweight MLPs and Spherical Gaussians for illumination modeling contributes to reducing computational cost and achieving good reconstruction quality.' Weaknesses=\"While the approach is innovative, the paper does not detail sufficiently the degree to which the method surpasses existing techniques in various metrics. Additionally, while it mentions efficiency, there is a lack of detailed analysis on memory consumption or potential trade-offs in quality. The method's ability to generalize across diverse real-world datasets might also not be fully addressed.\" Questions='1. How does the proposed method handle complex real-world lighting conditions more effectively than previous methods?\\n2. Is there a specific reason why Spherical Gaussians were chosen over other potential representations for illumination?\\n3. The paper mentions robustness over diverse datasets; could more details be provided about the datasets used for benchmarking?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel approach to inverse rendering using a unified voxelization framework, which helps in efficiently encoding scene properties like geometry, materials, and illumination. The use of lightweight MLPs and Spherical Gaussians for illumination modeling contributes to reducing computational cost and achieving good reconstruction quality.' Weaknesses=\"While the approach is innovative, the paper does not detail sufficiently the degree to which the method surpasses existing techniques in various metrics. Additionally, while it mentions efficiency, there is a lack of detailed analysis on memory consumption or potential trade-offs in quality. The method's ability to generalize across diverse real-world datasets might also not be fully addressed.\" Questions='1. How does the proposed method handle complex real-world lighting conditions more effectively than previous methods?\\n2. Is there a specific reason why Spherical Gaussians were chosen over other potential representations for illumination?\\n3. The paper mentions robustness over diverse datasets; could more details be provided about the datasets used for benchmarking?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"The proposed method for time-dependent importance reweighting in diffusion models is innovative and appears to effectively address dataset bias more precisely than existing methods. This approach shows strong experimental results across multiple diverse datasets, suggesting robust practical applicability. Additionally, the method's integration into a stochastic differential equation framework is theoretically interesting.\" Weaknesses='While the method shows promise, the reliance on a discriminator for density ratio estimation may introduce potential complexities or limitations in certain scenarios. The paper might also benefit from more detailed comparisons with a broader range of existing bias-mitigation techniques beyond the selected baselines, as well as clarifying potential limitations in scalability or computational overhead.' Questions=\"How does the time-dependent density ratio estimation handle cases with extremely imbalanced bias distributions? Are there potential limitations to this approach when scaling to larger datasets? Can the stochastic differential equation framework introduce any computational inefficiencies or challenges? How sensitive is the model's performance to the method chosen for density ratio estimation, and can improvements to this aspect further enhance the method's effectiveness?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"The proposed method for time-dependent importance reweighting in diffusion models is innovative and appears to effectively address dataset bias more precisely than existing methods. This approach shows strong experimental results across multiple diverse datasets, suggesting robust practical applicability. Additionally, the method's integration into a stochastic differential equation framework is theoretically interesting.\" Weaknesses='While the method shows promise, the reliance on a discriminator for density ratio estimation may introduce potential complexities or limitations in certain scenarios. The paper might also benefit from more detailed comparisons with a broader range of existing bias-mitigation techniques beyond the selected baselines, as well as clarifying potential limitations in scalability or computational overhead.' Questions=\"How does the time-dependent density ratio estimation handle cases with extremely imbalanced bias distributions? Are there potential limitations to this approach when scaling to larger datasets? Can the stochastic differential equation framework introduce any computational inefficiencies or challenges? How sensitive is the model's performance to the method chosen for density ratio estimation, and can improvements to this aspect further enhance the method's effectiveness?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The proposed OSRT model effectively integrates tree decomposition and adaptive RBF exploration, showing improved accuracy and efficiency for online learning in dynamic datasets. It manages both tree and RBF parameters concurrently, providing a novel approach to addressing limitations in traditional RBF networks, which is validated with benchmark datasets.' Weaknesses=\"The paper lacks a detailed discussion on the computational complexity of the proposed method and comparisons with state-of-the-art hybrid models. The explanation of the methodology could be more rigorous, particularly in describing how OSRT dynamically balances growth, pruning, and adapts to changes in streaming data. More clarity on the algorithm's scaling with data size would also be beneficial.\" Questions='How does the OSRT model ensure robustness against overfitting, especially when adapting RBF centers and balancing tree growth? Can the method be generalized to other types of streaming data beyond the time series considered in this study? What are the specific limitations of OSRT in terms of scalability and computational cost?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The proposed OSRT model effectively integrates tree decomposition and adaptive RBF exploration, showing improved accuracy and efficiency for online learning in dynamic datasets. It manages both tree and RBF parameters concurrently, providing a novel approach to addressing limitations in traditional RBF networks, which is validated with benchmark datasets.' Weaknesses=\"The paper lacks a detailed discussion on the computational complexity of the proposed method and comparisons with state-of-the-art hybrid models. The explanation of the methodology could be more rigorous, particularly in describing how OSRT dynamically balances growth, pruning, and adapts to changes in streaming data. More clarity on the algorithm's scaling with data size would also be beneficial.\" Questions='How does the OSRT model ensure robustness against overfitting, especially when adapting RBF centers and balancing tree growth? Can the method be generalized to other types of streaming data beyond the time series considered in this study? What are the specific limitations of OSRT in terms of scalability and computational cost?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents a novel extension to transformers by integrating spatial information, which adds a new dimension to episodic memory modeling. The introduction of the Adaptive Memory Allocator is an innovative approach to memory management, allowing for dynamic memory usage that can adapt to different task demands.' Weaknesses='While the methodology is well-executed, the paper could benefit from more comprehensive comparisons with existing episodic memory models beyond transformers. The implications of spatial information incorporation are discussed, but further exploration into its limitations would strengthen the paper.' Questions='How does the incorporation of spatial information in the transformer models affect the computational complexity of the algorithms? Are there any scenarios where this added complexity might outweigh the benefits?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents a novel extension to transformers by integrating spatial information, which adds a new dimension to episodic memory modeling. The introduction of the Adaptive Memory Allocator is an innovative approach to memory management, allowing for dynamic memory usage that can adapt to different task demands.' Weaknesses='While the methodology is well-executed, the paper could benefit from more comprehensive comparisons with existing episodic memory models beyond transformers. The implications of spatial information incorporation are discussed, but further exploration into its limitations would strengthen the paper.' Questions='How does the incorporation of spatial information in the transformer models affect the computational complexity of the algorithms? Are there any scenarios where this added complexity might outweigh the benefits?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses the critical challenge of improving interpretability and control in neural networks, a key area of research in enhancing AI transparency and safety.\\n2. Introducing sparse and discrete bottlenecks with vector quantization is a novel approach that balances performance with increased interpretability, providing actionable insights into neural operations.\\n3. The methodology is validated across varied datasets, demonstrating the versatility and effectiveness of the proposed approach.' Weaknesses='1. While the approach provides interpretability, the impact on model performance across diverse and larger-scale datasets remains to be thoroughly validated.\\n2. The reliance on cosine similarity for selecting representative codebook features might not always effectively capture nuanced variations in complex activation patterns.\\n3. The paper could benefit from more detailed experimental results and comparisons with state-of-the-art methods to firmly establish its contributions.' Questions='1. How does the inclusion of discrete bottlenecks impact the training time and computational requirements of neural networks, especially in large-scale models?\\n2. Can the proposed method effectively generalize to other neural architectures beyond Transformers, such as convolutional networks for image processing tasks?\\n3. What mechanisms are in place to optimize the size of the codebook and the selection of relevant features to maintain a balance between interpretability and model performance?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses the critical challenge of improving interpretability and control in neural networks, a key area of research in enhancing AI transparency and safety.\\n2. Introducing sparse and discrete bottlenecks with vector quantization is a novel approach that balances performance with increased interpretability, providing actionable insights into neural operations.\\n3. The methodology is validated across varied datasets, demonstrating the versatility and effectiveness of the proposed approach.' Weaknesses='1. While the approach provides interpretability, the impact on model performance across diverse and larger-scale datasets remains to be thoroughly validated.\\n2. The reliance on cosine similarity for selecting representative codebook features might not always effectively capture nuanced variations in complex activation patterns.\\n3. The paper could benefit from more detailed experimental results and comparisons with state-of-the-art methods to firmly establish its contributions.' Questions='1. How does the inclusion of discrete bottlenecks impact the training time and computational requirements of neural networks, especially in large-scale models?\\n2. Can the proposed method effectively generalize to other neural architectures beyond Transformers, such as convolutional networks for image processing tasks?\\n3. What mechanisms are in place to optimize the size of the codebook and the selection of relevant features to maintain a balance between interpretability and model performance?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses a critical problem in multi-label classification: the challenge of learning from partially annotated data. By proposing the PAPG framework, it brings novel insights into integrating reinforcement learning with incomplete annotations to tackle the issue of imbalance and scarcity in positive annotations. The approach is innovative with the use of policy gradient methods to balance exploration and exploitation. The experiments demonstrate the effectiveness of the method by showing a substantial improvement in F1 scores compared to existing methods, indicating the robustness and versatility of the approach across various datasets.' Weaknesses='While the method is promising, the soundness of the framework is not thoroughly established. How the reinforcement learning principles are specifically applied to the partially annotated problem could be expounded upon. Clarity in presentation can be improved; details about the Markov Decision Process setup and how local and global rewards interact might not be sufficiently clear to all readers. Additionally, the paper might benefit from a deeper comparison with existing reinforcement and semi-supervised learning methods to highlight distinguishing factors of the PAPG method. There is also some concern regarding the scalability of the model to extremely large datasets.' Questions='1. How sensitive is the framework to the choice of hyperparameters affecting the balance between exploration and exploitation?\\n2. Could the method be applied to tasks beyond multi-label classification, or is it specifically tailored to address partial annotations in this context only?\\n3. How does this approach compare to semi-supervised learning techniques in scenarios where annotations are scarce?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses a critical problem in multi-label classification: the challenge of learning from partially annotated data. By proposing the PAPG framework, it brings novel insights into integrating reinforcement learning with incomplete annotations to tackle the issue of imbalance and scarcity in positive annotations. The approach is innovative with the use of policy gradient methods to balance exploration and exploitation. The experiments demonstrate the effectiveness of the method by showing a substantial improvement in F1 scores compared to existing methods, indicating the robustness and versatility of the approach across various datasets.' Weaknesses='While the method is promising, the soundness of the framework is not thoroughly established. How the reinforcement learning principles are specifically applied to the partially annotated problem could be expounded upon. Clarity in presentation can be improved; details about the Markov Decision Process setup and how local and global rewards interact might not be sufficiently clear to all readers. Additionally, the paper might benefit from a deeper comparison with existing reinforcement and semi-supervised learning methods to highlight distinguishing factors of the PAPG method. There is also some concern regarding the scalability of the model to extremely large datasets.' Questions='1. How sensitive is the framework to the choice of hyperparameters affecting the balance between exploration and exploitation?\\n2. Could the method be applied to tasks beyond multi-label classification, or is it specifically tailored to address partial annotations in this context only?\\n3. How does this approach compare to semi-supervised learning techniques in scenarios where annotations are scarce?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The innovative approach of generating over 5 million pseudo-complexes allows for large-scale pretraining, which is typically limited by data scarcity.\\n2. ProFSA demonstrates significant improvement over existing methods in critical tasks such as pocket druggability prediction, emphasizing its practical applicability.\\n3. The method creatively addresses the gap in the availability of protein-ligand complex data by using existing high-resolution atomic protein structures.' Weaknesses='1. The complex methodology and setup might be challenging to understand fully, which could hinder reproducibility.\\n2. While pseudo-ligands are utilized effectively, there is limited discussion on the potential limitations or inaccuracies introduced by simulating real ligand interactions.\\n3. The impact of different choices in segmenting protein structures and selecting pseudo-ligand representations could have been explored more thoroughly.' Questions='1. How sensitive is ProFSA to the initial choice of segmentation and alignment methods for protein structures?\\n2. Are there any limitations observed when expanding ProFSA to different types of proteins or ligands not previously included in the training data?\\n3. Could the alignment of pseudo-ligand representations with pretrained small molecule encoders introduce biases that are not reflective of true ligand-receptor interactions?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The innovative approach of generating over 5 million pseudo-complexes allows for large-scale pretraining, which is typically limited by data scarcity.\\n2. ProFSA demonstrates significant improvement over existing methods in critical tasks such as pocket druggability prediction, emphasizing its practical applicability.\\n3. The method creatively addresses the gap in the availability of protein-ligand complex data by using existing high-resolution atomic protein structures.' Weaknesses='1. The complex methodology and setup might be challenging to understand fully, which could hinder reproducibility.\\n2. While pseudo-ligands are utilized effectively, there is limited discussion on the potential limitations or inaccuracies introduced by simulating real ligand interactions.\\n3. The impact of different choices in segmenting protein structures and selecting pseudo-ligand representations could have been explored more thoroughly.' Questions='1. How sensitive is ProFSA to the initial choice of segmentation and alignment methods for protein structures?\\n2. Are there any limitations observed when expanding ProFSA to different types of proteins or ligands not previously included in the training data?\\n3. Could the alignment of pseudo-ligand representations with pretrained small molecule encoders introduce biases that are not reflective of true ligand-receptor interactions?'\n",
      "Zero-shot error: Invalid JSON returned by the model:\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Content:\n",
      "I'm sorry, it seems there was an error in the output provided. Let me correct that for you. \n",
      "\n",
      "Here's the expected structured JSON:\n",
      "\n",
      "{\"Soundness\": 4, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"The Transitional Dictionary Learning framework effectively bridges the gap between neural and symbolic representations, showing significant promise in compositional pattern discovery without reliance on predefined visual features. The introduction of new metrics demonstrates improved interpretability which aligns with human evaluations.\", \"Weaknesses\": \"The framework's applicability to real-world scenarios needs to be validated on more complex and diverse datasets beyond the current compositional visual object datasets. The transition from proof-of-concept to practical application is unclear, and the reliance on these specific datasets limits generalizability.\", \"Questions\": \"How does the TDL framework perform on more complex, real-world image datasets? What are the computational demands of the proposed framework in practical uses?\"}\n",
      "Error: 'NoneType' object has no attribute 'model_dump'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"1. The proposed shift from image-based to point-based rendering in NeRFs is a novel approach that addresses several limitations of existing models, such as feature inconsistencies and poor geometric handling in achieving generalization without per-scene optimization.\\n2. The integration of innovations like visibility-oriented feature fetching and a nonuniform log sampling strategy improves both rendering speed and quality, offering a comprehensive advancement in the field.\\n3. Experimental validations on multiple datasets demonstrate the method's effectiveness in enhancing rendering quality, contributing both to academic understanding and practical applications.\" Weaknesses=\"1. The explanation of the implemented techniques, particularly the visibility-oriented feature fetching and geometric priors, lacks depth, making it challenging for readers to fully grasp the intricacies and potential limitations of the method.\\n2. There is a reliance on PatchmatchMVS for the initial scaffolding, which may limit the portability and independence of the GPF method.\\n3. Testing is limited to only three datasets, which may not sufficiently demonstrate the model's versatility and broad applicability, especially for diverse and real-world scenarios.\" Questions=\"1. Could you provide further detail on the visibility-oriented feature fetching mechanism and how geometric priors are specifically incorporated into the method?\\n2. What are the potential impacts of using PatchmatchMVS as a dependency for initial point generation, and how might this affect the method's scalability or generalization in more complex scenes?\\n3. How does the computational complexity of the proposed GPF approach compare to existing NeRF methods in terms of rendering speed and resource usage?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"1. The proposed shift from image-based to point-based rendering in NeRFs is a novel approach that addresses several limitations of existing models, such as feature inconsistencies and poor geometric handling in achieving generalization without per-scene optimization.\\n2. The integration of innovations like visibility-oriented feature fetching and a nonuniform log sampling strategy improves both rendering speed and quality, offering a comprehensive advancement in the field.\\n3. Experimental validations on multiple datasets demonstrate the method's effectiveness in enhancing rendering quality, contributing both to academic understanding and practical applications.\" Weaknesses=\"1. The explanation of the implemented techniques, particularly the visibility-oriented feature fetching and geometric priors, lacks depth, making it challenging for readers to fully grasp the intricacies and potential limitations of the method.\\n2. There is a reliance on PatchmatchMVS for the initial scaffolding, which may limit the portability and independence of the GPF method.\\n3. Testing is limited to only three datasets, which may not sufficiently demonstrate the model's versatility and broad applicability, especially for diverse and real-world scenarios.\" Questions=\"1. Could you provide further detail on the visibility-oriented feature fetching mechanism and how geometric priors are specifically incorporated into the method?\\n2. What are the potential impacts of using PatchmatchMVS as a dependency for initial point generation, and how might this affect the method's scalability or generalization in more complex scenes?\\n3. How does the computational complexity of the proposed GPF approach compare to existing NeRF methods in terms of rendering speed and resource usage?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The authors introduce an innovative approach—Label-encoding Risk Minimization (LRM)—which provides a novel way to improve generalization in scenarios with limited labeled data.\\n2. This method is shown to successfully enhance prediction diversity and reduce bias towards dominant classes in various challenging settings such as SSL, UDA, and SHDA.\\n3. The paper includes a theoretical analysis linking LRM with concepts like neural collapse, adding depth to the contribution.' Weaknesses='1. The presentation of results could have been clearer, with more emphasis on comparisons across diverse datasets and clearer visualizations.\\n2. While the method offers promising results, the experimental setup is somewhat limited and might not generalize across all types of datasets, especially larger and more complex ones.\\n3. There is limited discussion on the robustness of LRM against different types of real-world noise and variance in unlabeled data.' Questions='1. How does the LRM method perform in real-world applications where the unlabeled data might contain significant noise and outliers? Are there any adjustments or considerations?\\n2. Would LRM require any parametric or structural changes for tasks with significantly different feature spaces or output distributions?\\n3. Can LRM be integrated with other advanced techniques such as meta-learning or few-shot learning to enhance its capability in label-deficient environments?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The authors introduce an innovative approach—Label-encoding Risk Minimization (LRM)—which provides a novel way to improve generalization in scenarios with limited labeled data.\\n2. This method is shown to successfully enhance prediction diversity and reduce bias towards dominant classes in various challenging settings such as SSL, UDA, and SHDA.\\n3. The paper includes a theoretical analysis linking LRM with concepts like neural collapse, adding depth to the contribution.' Weaknesses='1. The presentation of results could have been clearer, with more emphasis on comparisons across diverse datasets and clearer visualizations.\\n2. While the method offers promising results, the experimental setup is somewhat limited and might not generalize across all types of datasets, especially larger and more complex ones.\\n3. There is limited discussion on the robustness of LRM against different types of real-world noise and variance in unlabeled data.' Questions='1. How does the LRM method perform in real-world applications where the unlabeled data might contain significant noise and outliers? Are there any adjustments or considerations?\\n2. Would LRM require any parametric or structural changes for tasks with significantly different feature spaces or output distributions?\\n3. Can LRM be integrated with other advanced techniques such as meta-learning or few-shot learning to enhance its capability in label-deficient environments?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=2 Rating=5 Confidence=4 Strengths=\"The study introduces a novel perspective on the stability of Jacobian matrices in neural networks throughout the training process. This is an important area of exploration, given its implications on network performance and training stability. The paper is systematically organized and well-written, aiding in the reader's understanding of complex concepts. The use of recent advancements in random matrix theory enhances the credibility and depth of the analysis.\" Weaknesses='The study heavily relies on assumptions that may be considered strong, particularly those about the conditions during the entire training process. Theoretical backing for these assumptions appears limited, which could undermine the robustness of the conclusions drawn. Moreover, the empirical evidence provided, though supportive, might not suffice in lieu of a comprehensive theoretical framework. Additionally, the examples and theorems discussed in the paper lack complexity, potentially oversimplifying the real-world applicability of the findings.' Questions='Could you provide more details on the specific techniques from random matrix theory utilized in the study, perhaps in the main text? Furthermore, how do the assumptions made about the training conditions hold up in diverse training environments and architectures?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=2 Rating=5 Confidence=4 Strengths=\"The study introduces a novel perspective on the stability of Jacobian matrices in neural networks throughout the training process. This is an important area of exploration, given its implications on network performance and training stability. The paper is systematically organized and well-written, aiding in the reader's understanding of complex concepts. The use of recent advancements in random matrix theory enhances the credibility and depth of the analysis.\" Weaknesses='The study heavily relies on assumptions that may be considered strong, particularly those about the conditions during the entire training process. Theoretical backing for these assumptions appears limited, which could undermine the robustness of the conclusions drawn. Moreover, the empirical evidence provided, though supportive, might not suffice in lieu of a comprehensive theoretical framework. Additionally, the examples and theorems discussed in the paper lack complexity, potentially oversimplifying the real-world applicability of the findings.' Questions='Could you provide more details on the specific techniques from random matrix theory utilized in the study, perhaps in the main text? Furthermore, how do the assumptions made about the training conditions hold up in diverse training environments and architectures?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The proposed method introduces a unified approach that combines generative and discriminative modeling, potentially improving performance for both closed-set and open-set recognition without the need for modifications or prior knowledge.' Weaknesses=\"The novelty of using a Latent Gaussian Mixture Model in this context may be limited, as similar methods have been explored in prior work. The paper's presentation can be improved for clarity, especially in terms of differentiating this work from existing literature.\" Questions='How does the L-GMM specifically differ from other latent generative models used for recognition tasks, and what unique contributions does it make beyond integrating CSR and OSR?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The proposed method introduces a unified approach that combines generative and discriminative modeling, potentially improving performance for both closed-set and open-set recognition without the need for modifications or prior knowledge.' Weaknesses=\"The novelty of using a Latent Gaussian Mixture Model in this context may be limited, as similar methods have been explored in prior work. The paper's presentation can be improved for clarity, especially in terms of differentiating this work from existing literature.\" Questions='How does the L-GMM specifically differ from other latent generative models used for recognition tasks, and what unique contributions does it make beyond integrating CSR and OSR?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=8 Confidence=4 Strengths='The paper introduces a theoretically grounded approach to multi-domain text classification, addressing a significant gap noted in current methodologies. The use of margin discrepancy within adversarial training provides a robust framework that balances theory with empirical results, demonstrating superior performance over existing methods. The clear connection between the theoretical framework and empirical results is commendable.' Weaknesses='The presentation of the material could be improved for readability and clarity, especially for audiences less familiar with the theoretical aspects of domain adaptation. While the theoretical grounding is strong, the novel contributions beyond the adaptation of existing theories to MDTC are somewhat limited. A broader comparison with more diverse datasets could solidify the findings.' Questions='1. How does the proposed MDAT method perform in settings with even less labeled data in the target domain compared to the benchmarks used? 2. Are there any specific scenarios or types of datasets where the theoretical advantages of the method do not translate into practical improvements?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=8 Confidence=4 Strengths='The paper introduces a theoretically grounded approach to multi-domain text classification, addressing a significant gap noted in current methodologies. The use of margin discrepancy within adversarial training provides a robust framework that balances theory with empirical results, demonstrating superior performance over existing methods. The clear connection between the theoretical framework and empirical results is commendable.' Weaknesses='The presentation of the material could be improved for readability and clarity, especially for audiences less familiar with the theoretical aspects of domain adaptation. While the theoretical grounding is strong, the novel contributions beyond the adaptation of existing theories to MDTC are somewhat limited. A broader comparison with more diverse datasets could solidify the findings.' Questions='1. How does the proposed MDAT method perform in settings with even less labeled data in the target domain compared to the benchmarks used? 2. Are there any specific scenarios or types of datasets where the theoretical advantages of the method do not translate into practical improvements?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper proposes a novel approach for sample selection in semi-supervised learning that effectively balances representativeness and diversity, filling an overlooked gap in SSL research concerning efficient sample annotation.\\n2. The use of the Generalized Kernel Herding without Replacement (GKHR) algorithm is innovative, enabling efficient minimization of the new criterion, α-MMD.\\n3. The method shows promising results across popular datasets, outperforming current active learning methodologies under budget constraints.' Weaknesses=\"1. The paper could include more extensive empirical evaluations across a wider range of datasets to better demonstrate the generalizability and robustness of the proposed method.\\n2. The integration of the method with only specific SSL frameworks like FlexMatch and FreeMatch might limit the applicability of the proposed approach to other frameworks that potentially do not easily accommodate the use of the Gaussian kernel with CLIP features.\\n3. The approach's reliance on kernel methods could potentially lead to scalability issues when dealing with very large datasets, which is not addressed in the paper.\" Questions='1. How does the α-MMD criterion directly improve the generalization abilities of the SSL models, compared to traditional methods?\\n2. Can the proposed approach be adapted or extended to accommodate and integrate with other SSL frameworks beyond those mentioned in the paper, and if so, how?\\n3. Have the authors considered the computational overhead introduced by the kernel-based feature selection methodology, especially in the context of large-scale real-world applications?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper proposes a novel approach for sample selection in semi-supervised learning that effectively balances representativeness and diversity, filling an overlooked gap in SSL research concerning efficient sample annotation.\\n2. The use of the Generalized Kernel Herding without Replacement (GKHR) algorithm is innovative, enabling efficient minimization of the new criterion, α-MMD.\\n3. The method shows promising results across popular datasets, outperforming current active learning methodologies under budget constraints.' Weaknesses=\"1. The paper could include more extensive empirical evaluations across a wider range of datasets to better demonstrate the generalizability and robustness of the proposed method.\\n2. The integration of the method with only specific SSL frameworks like FlexMatch and FreeMatch might limit the applicability of the proposed approach to other frameworks that potentially do not easily accommodate the use of the Gaussian kernel with CLIP features.\\n3. The approach's reliance on kernel methods could potentially lead to scalability issues when dealing with very large datasets, which is not addressed in the paper.\" Questions='1. How does the α-MMD criterion directly improve the generalization abilities of the SSL models, compared to traditional methods?\\n2. Can the proposed approach be adapted or extended to accommodate and integrate with other SSL frameworks beyond those mentioned in the paper, and if so, how?\\n3. Have the authors considered the computational overhead introduced by the kernel-based feature selection methodology, especially in the context of large-scale real-world applications?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper presents a novel approach to making continual learning more feasible in resource-constrained environments through fully quantized training. The use of Hadamard transforms with quantization addresses the challenge of maintaining accuracy while reducing computational needs. The approach effectively demonstrates a reduction in memory usage and energy consumption, showing potential applicability to edge devices. The evaluation on CIFAR100 and multiple HAR datasets against existing methods shows promising results with minimal loss in accuracy, indicating the method's potential utility in practical applications.\" Weaknesses='The integration of Hadamard transforms with quantized training, while innovative, may still face challenges related to hardware implementation and real-world deployment, particularly regarding stochastic rounding. The paper might have benefited from a more comprehensive comparison with other quantization and continual learning methodologies to firmly establish the superiority of the proposed method. The lack of exploration into different types of datasets or broader experimentation with diverse architectures limits the generalizability of the results presented.' Questions='How does the proposed HDQT compare with existing quantization methods when applied outside of continual learning contexts, such as standalone model training? Are there specific limitations in implementing this approach on current edge device hardware, particularly concerning the stochastic rounding process? Can the approach be adapted to handle more complex neural network architectures or larger scale datasets while maintaining the same levels of efficiency?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper presents a novel approach to making continual learning more feasible in resource-constrained environments through fully quantized training. The use of Hadamard transforms with quantization addresses the challenge of maintaining accuracy while reducing computational needs. The approach effectively demonstrates a reduction in memory usage and energy consumption, showing potential applicability to edge devices. The evaluation on CIFAR100 and multiple HAR datasets against existing methods shows promising results with minimal loss in accuracy, indicating the method's potential utility in practical applications.\" Weaknesses='The integration of Hadamard transforms with quantized training, while innovative, may still face challenges related to hardware implementation and real-world deployment, particularly regarding stochastic rounding. The paper might have benefited from a more comprehensive comparison with other quantization and continual learning methodologies to firmly establish the superiority of the proposed method. The lack of exploration into different types of datasets or broader experimentation with diverse architectures limits the generalizability of the results presented.' Questions='How does the proposed HDQT compare with existing quantization methods when applied outside of continual learning contexts, such as standalone model training? Are there specific limitations in implementing this approach on current edge device hardware, particularly concerning the stochastic rounding process? Can the approach be adapted to handle more complex neural network architectures or larger scale datasets while maintaining the same levels of efficiency?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses significant limitations in previous work related to locally differentially private estimation of collision probability, offering improvements particularly in sample efficiency.\\n2. The introduction of novel algorithms provides both practical and theoretical advancements, such as sequential testing which can adapt to privacy parameters not being pre-specified.' Weaknesses='1. The presentation lacks depth in explaining the intuition behind certain methodological choices, such as the hashing and bias correction steps.\\n2. There is limited experimental validation to support the claims of reduced sample complexity and the practical applicability of the sequential testing algorithm.' Questions='1. How does the proposed estimation algorithm perform compared to non-LDP methods in terms of accuracy and sample efficiency?\\n2. Can the sequential testing algorithm be generalized or adapted to other problems or domains where privacy parameters are unknown or difficult to define?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses significant limitations in previous work related to locally differentially private estimation of collision probability, offering improvements particularly in sample efficiency.\\n2. The introduction of novel algorithms provides both practical and theoretical advancements, such as sequential testing which can adapt to privacy parameters not being pre-specified.' Weaknesses='1. The presentation lacks depth in explaining the intuition behind certain methodological choices, such as the hashing and bias correction steps.\\n2. There is limited experimental validation to support the claims of reduced sample complexity and the practical applicability of the sequential testing algorithm.' Questions='1. How does the proposed estimation algorithm perform compared to non-LDP methods in terms of accuracy and sample efficiency?\\n2. Can the sequential testing algorithm be generalized or adapted to other problems or domains where privacy parameters are unknown or difficult to define?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"The proposed GTMGC model based on Graph Transformers and a novel self-attention mechanism demonstrates potential in efficiently predicting 3D ground-state conformation of molecules from 2D structures. The method's innovation in using MSRSA appears promising for preserving both global and local molecular structures, and the model shows improved performance compared to existing methods. Additionally, the study contributes to the field by integrating 3D information with graph neural networks, showcasing its application in property prediction alongside conformational predictions. \" Weaknesses=\"1. The novelty of the MSRSA within the context of existing self-attention frameworks might require further delineation since several graph-based transformers may incorporate adjacency and distance matrices in some form. \\n2. The reliance on benchmark datasets like Molecule3D and QM9 might have inherent biases, and demonstrating the model's robustness across more diverse molecular datasets could strengthen the findings.\\n3. While the model's performance against RDkit and similar tools is noted, more detailed comparative analyses against state-of-the-art neural network-based models might be necessary to conclusively ascertain competitive advantages. \" Questions=\"1. How sensitive is the model's performance to the choice of the Laplacian Positional Encoding? Would alternatives yield similar results?\\n2. In what ways can the MSRSA mechanism be adapted or generalized for other graph-based problems outside of the molecular domain?\\n3. Given that the GTMGC model outperformed baselines in conformational prediction, how does it fare in predicting other molecular properties of interest beyond QM9, and what metrics were used for these predictions?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"The proposed GTMGC model based on Graph Transformers and a novel self-attention mechanism demonstrates potential in efficiently predicting 3D ground-state conformation of molecules from 2D structures. The method's innovation in using MSRSA appears promising for preserving both global and local molecular structures, and the model shows improved performance compared to existing methods. Additionally, the study contributes to the field by integrating 3D information with graph neural networks, showcasing its application in property prediction alongside conformational predictions. \" Weaknesses=\"1. The novelty of the MSRSA within the context of existing self-attention frameworks might require further delineation since several graph-based transformers may incorporate adjacency and distance matrices in some form. \\n2. The reliance on benchmark datasets like Molecule3D and QM9 might have inherent biases, and demonstrating the model's robustness across more diverse molecular datasets could strengthen the findings.\\n3. While the model's performance against RDkit and similar tools is noted, more detailed comparative analyses against state-of-the-art neural network-based models might be necessary to conclusively ascertain competitive advantages. \" Questions=\"1. How sensitive is the model's performance to the choice of the Laplacian Positional Encoding? Would alternatives yield similar results?\\n2. In what ways can the MSRSA mechanism be adapted or generalized for other graph-based problems outside of the molecular domain?\\n3. Given that the GTMGC model outperformed baselines in conformational prediction, how does it fare in predicting other molecular properties of interest beyond QM9, and what metrics were used for these predictions?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents an innovative use of Large Language Models (LLMs) to orchestrate multi-modal audio generation for storytelling, addressing a gap in compositional audio creation from textual descriptions. WavJourney leverages different expert audio models without additional training, and the system demonstrates strong performance on text-to-audio benchmarks, offering potential in interactive and co-creative AI applications.' Weaknesses='The approach appears to emphasize application over method innovation, relying heavily on existing LLMs and other audio models rather than introducing novel algorithms. The paper lacks in-depth comparisons with alternative multi-modal or agent-based systems that integrate LLMs with other task-specific models.' Questions=\"What are the limitations in processing or generating more complex or nuanced audio elements? How adaptable is the method to emerging LLMs or audio models? Have there been attempts to compare WavJourney's framework with other agent-based systems or pipelines that incorporate LLMs for multi-modal generation?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents an innovative use of Large Language Models (LLMs) to orchestrate multi-modal audio generation for storytelling, addressing a gap in compositional audio creation from textual descriptions. WavJourney leverages different expert audio models without additional training, and the system demonstrates strong performance on text-to-audio benchmarks, offering potential in interactive and co-creative AI applications.' Weaknesses='The approach appears to emphasize application over method innovation, relying heavily on existing LLMs and other audio models rather than introducing novel algorithms. The paper lacks in-depth comparisons with alternative multi-modal or agent-based systems that integrate LLMs with other task-specific models.' Questions=\"What are the limitations in processing or generating more complex or nuanced audio elements? How adaptable is the method to emerging LLMs or audio models? Have there been attempts to compare WavJourney's framework with other agent-based systems or pipelines that incorporate LLMs for multi-modal generation?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses an important problem of improving the transferability of data poisoning attacks across different learning paradigms, which is relevant in securing machine learning systems.\\n2. The proposed method, Transferable Poisoning, is innovative in using both supervised and unsupervised contrastive learning paradigms to generate high-frequency poisoning perturbations.\\n3. The experimental results on benchmark datasets such as CIFAR-10, CIFAR-100, and TinyImageNet demonstrate the effectiveness of the method, showing significant improvement in transferability compared to existing methods.' Weaknesses='1. The paper could benefit from more detailed explanation of the underlying mechanism that ensures high-frequency perturbations lead to increased transferability across learning paradigms.\\n2. While the use of both supervised and unsupervised methods is noted, a deeper analysis or ablation study could help clarify the specific contribution of each part of the approach to the overall performance improvement.\\n3. The presentation, while generally clear, might be enhanced by better visual aids or diagrams to illustrate the process of perturbation generation and the role of contrastive learning.' Questions='1. How does the proposed method ensure that high-frequency perturbations remain effective across different datasets and not just within the tested benchmarks?\\n2. Can this approach be generalized beyond the specific datasets used, such as to text or other modalities?\\n3. Is there a scalability consideration when applying the Transferable Poisoning method to larger datasets or models, and how does it affect the time or computational resources required?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses an important problem of improving the transferability of data poisoning attacks across different learning paradigms, which is relevant in securing machine learning systems.\\n2. The proposed method, Transferable Poisoning, is innovative in using both supervised and unsupervised contrastive learning paradigms to generate high-frequency poisoning perturbations.\\n3. The experimental results on benchmark datasets such as CIFAR-10, CIFAR-100, and TinyImageNet demonstrate the effectiveness of the method, showing significant improvement in transferability compared to existing methods.' Weaknesses='1. The paper could benefit from more detailed explanation of the underlying mechanism that ensures high-frequency perturbations lead to increased transferability across learning paradigms.\\n2. While the use of both supervised and unsupervised methods is noted, a deeper analysis or ablation study could help clarify the specific contribution of each part of the approach to the overall performance improvement.\\n3. The presentation, while generally clear, might be enhanced by better visual aids or diagrams to illustrate the process of perturbation generation and the role of contrastive learning.' Questions='1. How does the proposed method ensure that high-frequency perturbations remain effective across different datasets and not just within the tested benchmarks?\\n2. Can this approach be generalized beyond the specific datasets used, such as to text or other modalities?\\n3. Is there a scalability consideration when applying the Transferable Poisoning method to larger datasets or models, and how does it affect the time or computational resources required?'\n",
      "Zero shot review:\n",
      " Soundness=2 Presentation=3 Contribution=2 Rating=4 Confidence=3 Strengths='The paper tackles an important question about the emergent abilities of LLMs, providing a novel hypothesis using Bayesian inference which could potentially enhance understanding in this area.' Weaknesses='The theoretical framework proposed is complex and may be too abstract to be practically applicable or empirically testable with current tools. There is also a lack of clarity in how the proposed latent space theory can be operationalized and validated beyond synthetic simulations.' Questions='Could the authors elaborate on how this theory might be empirically validated using real-world LLMs, beyond synthetic data simulations?'\n",
      "Success: Soundness=2 Presentation=3 Contribution=2 Rating=4 Confidence=3 Strengths='The paper tackles an important question about the emergent abilities of LLMs, providing a novel hypothesis using Bayesian inference which could potentially enhance understanding in this area.' Weaknesses='The theoretical framework proposed is complex and may be too abstract to be practically applicable or empirically testable with current tools. There is also a lack of clarity in how the proposed latent space theory can be operationalized and validated beyond synthetic simulations.' Questions='Could the authors elaborate on how this theory might be empirically validated using real-world LLMs, beyond synthetic data simulations?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses an important challenge of bias and stability in GNNs, proposing a novel approach using Lipschitz bounds.\\n2. The method is integrated into existing GNN training pipelines, facilitating practical application without significant computational overhead.\\n3. Empirical results show improved fairness and stability of GNN outputs, supporting the effectiveness of the approach.' Weaknesses='1. The computation of Lipschitz bounds via the Jacobian matrix might not always accurately represent all cases where biases affect GNNs, potentially leading to over- or under-estimations.\\n2. It is unclear if the approach generalizes well to all types of graph structures and datasets, as experiments are limited to specific tasks and datasets.\\n3. While theoretically informed, the practical impact and scalability to very large graphs remain somewhat unclear.' Questions='1. How does the method handle extremely large graph datasets in practice, where the computation of the Jacobian might be computationally intensive?\\n2. Are there specific types of biases in the data that JacoLip is more effective at mitigating compared to others?\\n3. How does JacoLip compare in performance and computational cost against other methods of bias mitigation specifically designed for GNNs?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses an important challenge of bias and stability in GNNs, proposing a novel approach using Lipschitz bounds.\\n2. The method is integrated into existing GNN training pipelines, facilitating practical application without significant computational overhead.\\n3. Empirical results show improved fairness and stability of GNN outputs, supporting the effectiveness of the approach.' Weaknesses='1. The computation of Lipschitz bounds via the Jacobian matrix might not always accurately represent all cases where biases affect GNNs, potentially leading to over- or under-estimations.\\n2. It is unclear if the approach generalizes well to all types of graph structures and datasets, as experiments are limited to specific tasks and datasets.\\n3. While theoretically informed, the practical impact and scalability to very large graphs remain somewhat unclear.' Questions='1. How does the method handle extremely large graph datasets in practice, where the computation of the Jacobian might be computationally intensive?\\n2. Are there specific types of biases in the data that JacoLip is more effective at mitigating compared to others?\\n3. How does JacoLip compare in performance and computational cost against other methods of bias mitigation specifically designed for GNNs?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=3 Strengths='The paper provides significant theoretical advancements in understanding the dynamics of non-contrastive learning by incorporating feature normalization into the dynamic analysis. The use of cosine loss is well-motivated and aligns with practical implementations, leading to valuable insights into how normalization influences representation stability. The eigenmode dynamics analysis is a novel approach that effectively demonstrates how higher-order dynamics induced by cosine loss contribute to robust learning. Additionally, the findings are validated through numerical simulations, providing empirical support for the theoretical claims.' Weaknesses='One of the weaknesses lies in the presentation, which could be improved for broader accessibility, especially for readers not deeply familiar with advanced dynamic analysis techniques in self-supervised learning. The paper could benefit from clearer explanations and more intuitive descriptions of the mathematical concepts used, making it more approachable to a wider audience. The numerical simulations are relatively limited in scope, primarily focusing on synthetic data and the CIFAR-10 dataset. Providing a more extensive empirical evaluation on diverse datasets could strengthen the practical implications of the findings. Furthermore, the analysis relies heavily on theoretical assumptions, and additional insights into practical considerations, such as computational efficiency and scalability, would enhance the contribution of the work.' Questions='1. Have you considered evaluating the proposed theoretical framework using additional datasets beyond CIFAR-10 to further substantiate its applicability to various domains in practical settings?\\n2. Are there plans to explore the implications of the findings on more complex architectures beyond ResNet-18 and investigate whether the observed dynamics hold across a broader range of model architectures?\\n3. Could you provide more detailed intuition or visualizations to help readers understand the transition between third-order and sixth-order dynamics and how this transition contributes to stabilizing the representation space?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=3 Strengths='The paper provides significant theoretical advancements in understanding the dynamics of non-contrastive learning by incorporating feature normalization into the dynamic analysis. The use of cosine loss is well-motivated and aligns with practical implementations, leading to valuable insights into how normalization influences representation stability. The eigenmode dynamics analysis is a novel approach that effectively demonstrates how higher-order dynamics induced by cosine loss contribute to robust learning. Additionally, the findings are validated through numerical simulations, providing empirical support for the theoretical claims.' Weaknesses='One of the weaknesses lies in the presentation, which could be improved for broader accessibility, especially for readers not deeply familiar with advanced dynamic analysis techniques in self-supervised learning. The paper could benefit from clearer explanations and more intuitive descriptions of the mathematical concepts used, making it more approachable to a wider audience. The numerical simulations are relatively limited in scope, primarily focusing on synthetic data and the CIFAR-10 dataset. Providing a more extensive empirical evaluation on diverse datasets could strengthen the practical implications of the findings. Furthermore, the analysis relies heavily on theoretical assumptions, and additional insights into practical considerations, such as computational efficiency and scalability, would enhance the contribution of the work.' Questions='1. Have you considered evaluating the proposed theoretical framework using additional datasets beyond CIFAR-10 to further substantiate its applicability to various domains in practical settings?\\n2. Are there plans to explore the implications of the findings on more complex architectures beyond ResNet-18 and investigate whether the observed dynamics hold across a broader range of model architectures?\\n3. Could you provide more detailed intuition or visualizations to help readers understand the transition between third-order and sixth-order dynamics and how this transition contributes to stabilizing the representation space?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths='The paper introduces a continuation method with Gaussian convolution that addresses saddle point issues and stabilizes training in deep networks, showing improvements in convergence and accuracy. It combines theoretical insights with empirical validation on benchmark tasks.' Weaknesses='The implementation details are sparse, particularly in how the regularization is effectively applied and tuned. The application to deep learning benchmarks is limited, with only MNIST and neural ODEs considered, which may not reflect broader applicability.' Questions='How is the regularization parameter chosen, and does it require significant tuning across different tasks? Can this method be applied to other deep learning tasks beyond MNIST and neural ODEs, such as large-scale datasets?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths='The paper introduces a continuation method with Gaussian convolution that addresses saddle point issues and stabilizes training in deep networks, showing improvements in convergence and accuracy. It combines theoretical insights with empirical validation on benchmark tasks.' Weaknesses='The implementation details are sparse, particularly in how the regularization is effectively applied and tuned. The application to deep learning benchmarks is limited, with only MNIST and neural ODEs considered, which may not reflect broader applicability.' Questions='How is the regularization parameter chosen, and does it require significant tuning across different tasks? Can this method be applied to other deep learning tasks beyond MNIST and neural ODEs, such as large-scale datasets?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel approach to multi-source black-box domain adaptation by refining pseudo labels, which potentially addresses key challenges in traditional black-box domain adaptation. The proposed framework is theoretically supported and empirically validated on benchmark datasets, showing competitive performance.' Weaknesses='The explanation of the method can be quite dense and challenging to follow. There might be a lack of clarity in how the pseudo-label refinery network operates precisely, particularly with the practical implementation and the assumptions made for deriving theoretical support. Furthermore, while the paper appears to perform well on tested datasets, it lacks exploration of whether the proposed method transfers well to very different domains not covered by the benchmarks used.' Questions='1. How does the method handle potential conflicts or inconsistencies in pseudo labels when derived from multiple source domains?\\n2. Could the authors elaborate more on the computational efficiency of the PRN in practical settings?\\n3. What measures are taken to ensure the diversity and relevance of the multiple sources selected for the adaptation process?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel approach to multi-source black-box domain adaptation by refining pseudo labels, which potentially addresses key challenges in traditional black-box domain adaptation. The proposed framework is theoretically supported and empirically validated on benchmark datasets, showing competitive performance.' Weaknesses='The explanation of the method can be quite dense and challenging to follow. There might be a lack of clarity in how the pseudo-label refinery network operates precisely, particularly with the practical implementation and the assumptions made for deriving theoretical support. Furthermore, while the paper appears to perform well on tested datasets, it lacks exploration of whether the proposed method transfers well to very different domains not covered by the benchmarks used.' Questions='1. How does the method handle potential conflicts or inconsistencies in pseudo labels when derived from multiple source domains?\\n2. Could the authors elaborate more on the computational efficiency of the PRN in practical settings?\\n3. What measures are taken to ensure the diversity and relevance of the multiple sources selected for the adaptation process?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel unified framework integrating feature and cost aggregation using Transformers, which is a significant contribution to dense matching tasks in computer vision. The method demonstrates robust and efficient performance, particularly under challenging conditions like large intra-class variations. The use of multi-scale predictions and confidence scores for selecting the most accurate predictions is an innovative approach.' Weaknesses=\"The method's real-world applicability might be limited without additional details on computational complexity, especially given the use of Transformer architecture. The approach may require substantial computational resources, which could affect its usability in scenarios where resources are limited. Moreover, there is a possibility that the integration of the two aggregation types might not always lead to improved performance for all types of dense matching tasks.\" Questions=\"How does the proposed UFC architecture compare computationally with existing state-of-the-art methods in terms of resources required? Are there any scenarios or datasets where the UFC method's performance does not surpass current methods? If so, what might be the cause of these limitations?\"\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel unified framework integrating feature and cost aggregation using Transformers, which is a significant contribution to dense matching tasks in computer vision. The method demonstrates robust and efficient performance, particularly under challenging conditions like large intra-class variations. The use of multi-scale predictions and confidence scores for selecting the most accurate predictions is an innovative approach.' Weaknesses=\"The method's real-world applicability might be limited without additional details on computational complexity, especially given the use of Transformer architecture. The approach may require substantial computational resources, which could affect its usability in scenarios where resources are limited. Moreover, there is a possibility that the integration of the two aggregation types might not always lead to improved performance for all types of dense matching tasks.\" Questions=\"How does the proposed UFC architecture compare computationally with existing state-of-the-art methods in terms of resources required? Are there any scenarios or datasets where the UFC method's performance does not surpass current methods? If so, what might be the cause of these limitations?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper effectively addresses significant vulnerabilities in DFML such as Task-Distribution Shift (TDS) and Task-Distribution Corruption (TDC), making it a valuable contribution to improving robustness in DFML settings. It introduces innovative approaches, TEAPOT and ROSY, to combat these challenges, offering a promising alternative for situations where data privacy is a concern and only pre-trained models are available. The comprehensive experiments on a range of datasets provide a convincing demonstration of the approach's effectiveness.\" Weaknesses='The paper could have benefited from clearer explanations or more intuitive descriptions, particularly regarding the intricacies of the proposed methods, such as TEAPOT and ROSY. The reinforcement learning approach to model selection in ROSY, while innovative, might introduce additional computational complexity, which could be a concern for real-world scalability. Additionally, it is unclear how well the method generalizes across different domains without further empirical results. Potential limitations or edge cases where the method might not perform as well should be explored further.' Questions='How do the computational costs of implementing ROSY compare to simpler methods or other model selection strategies in similar scenarios? Are there particular domains or problem types where TEAPOT and ROSY show significant performance drop-offs? Could further insights be shared on how TEAPOT decides when to replay old tasks, and how this affects long-term learning stability?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper effectively addresses significant vulnerabilities in DFML such as Task-Distribution Shift (TDS) and Task-Distribution Corruption (TDC), making it a valuable contribution to improving robustness in DFML settings. It introduces innovative approaches, TEAPOT and ROSY, to combat these challenges, offering a promising alternative for situations where data privacy is a concern and only pre-trained models are available. The comprehensive experiments on a range of datasets provide a convincing demonstration of the approach's effectiveness.\" Weaknesses='The paper could have benefited from clearer explanations or more intuitive descriptions, particularly regarding the intricacies of the proposed methods, such as TEAPOT and ROSY. The reinforcement learning approach to model selection in ROSY, while innovative, might introduce additional computational complexity, which could be a concern for real-world scalability. Additionally, it is unclear how well the method generalizes across different domains without further empirical results. Potential limitations or edge cases where the method might not perform as well should be explored further.' Questions='How do the computational costs of implementing ROSY compare to simpler methods or other model selection strategies in similar scenarios? Are there particular domains or problem types where TEAPOT and ROSY show significant performance drop-offs? Could further insights be shared on how TEAPOT decides when to replay old tasks, and how this affects long-term learning stability?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"1. The paper addresses a significant problem, which is understanding the alignment of LLMs with human values, a topic of growing interest and importance.\\n2. Introduction of the Value Understanding Measurement (VUM) framework and discriminator-critique gap (DCG) approach shows novel consideration in evaluating 'know what' and 'know why' dimensions of value understanding.\" Weaknesses='1. There is a lack of clarity in the descriptions of the methodologies employed, particularly surrounding the specific processes of discriminator-critique evaluations, which leaves room for misinterpretation.\\n2. Limited practical examples or illustrative explanations of the proposed hypotheses and methodologies within the paper make it difficult to thoroughly understand the applicability and limitations of the VUM framework.\\n3. The paper does not sufficiently address potential biases inherent in using pre-annotated datasets from GPT-4 or how these might affect the results.' Questions=\"1. What measures are taken to ensure that the annotation provided by GPT-4 for the dataset is unbiased and truly reflective of human values?\\n2. How do the authors plan to extend this work towards practical use cases, or integration into existing evaluation metrics for LLMs beyond the theoretical framework presented in this study?\\n3. Could the authors provide more clarity on how scaling laws affect 'know what', and in which precise aspects they impact LLMs differently than 'know why'? Would additional model parameters apart from scaling introduce any variants in observations?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"1. The paper addresses a significant problem, which is understanding the alignment of LLMs with human values, a topic of growing interest and importance.\\n2. Introduction of the Value Understanding Measurement (VUM) framework and discriminator-critique gap (DCG) approach shows novel consideration in evaluating 'know what' and 'know why' dimensions of value understanding.\" Weaknesses='1. There is a lack of clarity in the descriptions of the methodologies employed, particularly surrounding the specific processes of discriminator-critique evaluations, which leaves room for misinterpretation.\\n2. Limited practical examples or illustrative explanations of the proposed hypotheses and methodologies within the paper make it difficult to thoroughly understand the applicability and limitations of the VUM framework.\\n3. The paper does not sufficiently address potential biases inherent in using pre-annotated datasets from GPT-4 or how these might affect the results.' Questions=\"1. What measures are taken to ensure that the annotation provided by GPT-4 for the dataset is unbiased and truly reflective of human values?\\n2. How do the authors plan to extend this work towards practical use cases, or integration into existing evaluation metrics for LLMs beyond the theoretical framework presented in this study?\\n3. Could the authors provide more clarity on how scaling laws affect 'know what', and in which precise aspects they impact LLMs differently than 'know why'? Would additional model parameters apart from scaling introduce any variants in observations?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed Hierarchical Diffuser improves efficiency and generalization by leveraging a hierarchical planning structure, incorporating both high-level (subgoal generation) and low-level (subgoal achievement) diffusers.\\n\\nThe method outperforms existing diffusion-based and hierarchical planning methods on standard offline RL benchmarks, demonstrating better training and planning speeds as well as enhanced generalization capabilities for compositional out-of-distribution tasks.\\n\\nThe hierarchical approach enables efficient planning for longer horizons, addressing the challenges faced by flat, dense diffusion models in these scenarios.' Weaknesses='The formulation is not entirely novel, as it primarily builds on existing concepts of diffusion-based and hierarchical planning. The novelty mainly lies in the integration and implementation of these concepts to improve decision-making in long-horizon tasks.\\n\\nDetails on how different components of the proposed model interconnect and the specific training methodologies for each diffuser could be more thoroughly explained to enhance reproducibility and understanding.\\n\\nThere is a concern about the handling of sub-goals that do not align perfectly with the low-level goals, particularly in cases where discretized sub-goals may not be directly achievable with given actions.' Questions=\"1. How is the transition between sub-goals managed in cases where actions cannot directly achieve the designated sub-goal due to the environment's constraints?\\n\\n2. Are there any specific challenges encountered in training the Sparse Diffuser versus the low-level diffuser? If so, how are these addressed?\\n\\n3. How does the performance of Hierarchical Diffuser compare to state-only diffusion methods like Decision-diffuser? Are there any specific scenarios where Hierarchical Diffuser might underperform?\"\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed Hierarchical Diffuser improves efficiency and generalization by leveraging a hierarchical planning structure, incorporating both high-level (subgoal generation) and low-level (subgoal achievement) diffusers.\\n\\nThe method outperforms existing diffusion-based and hierarchical planning methods on standard offline RL benchmarks, demonstrating better training and planning speeds as well as enhanced generalization capabilities for compositional out-of-distribution tasks.\\n\\nThe hierarchical approach enables efficient planning for longer horizons, addressing the challenges faced by flat, dense diffusion models in these scenarios.' Weaknesses='The formulation is not entirely novel, as it primarily builds on existing concepts of diffusion-based and hierarchical planning. The novelty mainly lies in the integration and implementation of these concepts to improve decision-making in long-horizon tasks.\\n\\nDetails on how different components of the proposed model interconnect and the specific training methodologies for each diffuser could be more thoroughly explained to enhance reproducibility and understanding.\\n\\nThere is a concern about the handling of sub-goals that do not align perfectly with the low-level goals, particularly in cases where discretized sub-goals may not be directly achievable with given actions.' Questions=\"1. How is the transition between sub-goals managed in cases where actions cannot directly achieve the designated sub-goal due to the environment's constraints?\\n\\n2. Are there any specific challenges encountered in training the Sparse Diffuser versus the low-level diffuser? If so, how are these addressed?\\n\\n3. How does the performance of Hierarchical Diffuser compare to state-only diffusion methods like Decision-diffuser? Are there any specific scenarios where Hierarchical Diffuser might underperform?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper presents a novel approach for generating saliency and class activation maps for Spiking Neural Networks (SNNs), which addresses a key gap in the field. 2. The proposed EventRPG method significantly improves the generalization ability of SNNs on event-based tasks, achieving state-of-the-art performance. 3. The two data augmentation techniques, RPGDrop and RPGMix, are innovative and effectively utilize the saliency maps for enhancing classification tasks.' Weaknesses='1. The application of the proposed methods is currently limited to classification tasks, which may limit their broader applicability. 2. The approach requires an understanding and integration of complex techniques such as relevance propagation, which may not be straightforward for all researchers to implement. 3. Presentation can be improved, particularly in terms of providing clarity on the integration of the proposed methods with existing frameworks and in-depth comparisons with similar approaches.' Questions='1. Can the relevance propagation techniques (SLTRP and SLRP) be extended to other types of neural tasks beyond classification? 2. How do the augmentation methods specifically compare in terms of computational efficiency and resource requirements with more conventional approaches like Mixup or standard geometric transformations? 3. Are there any limitations in the event types that the proposed techniques can handle, and if so, what are the conditions under which they perform optimally?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper presents a novel approach for generating saliency and class activation maps for Spiking Neural Networks (SNNs), which addresses a key gap in the field. 2. The proposed EventRPG method significantly improves the generalization ability of SNNs on event-based tasks, achieving state-of-the-art performance. 3. The two data augmentation techniques, RPGDrop and RPGMix, are innovative and effectively utilize the saliency maps for enhancing classification tasks.' Weaknesses='1. The application of the proposed methods is currently limited to classification tasks, which may limit their broader applicability. 2. The approach requires an understanding and integration of complex techniques such as relevance propagation, which may not be straightforward for all researchers to implement. 3. Presentation can be improved, particularly in terms of providing clarity on the integration of the proposed methods with existing frameworks and in-depth comparisons with similar approaches.' Questions='1. Can the relevance propagation techniques (SLTRP and SLRP) be extended to other types of neural tasks beyond classification? 2. How do the augmentation methods specifically compare in terms of computational efficiency and resource requirements with more conventional approaches like Mixup or standard geometric transformations? 3. Are there any limitations in the event types that the proposed techniques can handle, and if so, what are the conditions under which they perform optimally?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper presents an innovative approach to voice conversion by effectively utilizing entangled speech representations combined with self-synthesized examples for training, thus improving performance in zero-shot voice conversion tasks.\\n2. The method demonstrates applicability across various languages and conversion scenarios, showing competency in both zero-shot and cross-lingual voice conversion tasks.\\n3. The integration of prosodic information without relying on text, along with self-synthesized examples, significantly enhances speaker similarity and quality, achieving state-of-the-art results.' Weaknesses='1. While the method demonstrates effectiveness, the reliance on self-supervised learning models and speaker verification models may introduce limitations in terms of resource availability and required computational power.\\n2. The paper could further discuss potential limitations or boundary conditions where the proposed method might not perform optimally.\\n3. There is less emphasis on the comparative analysis of how the method holds against a broader set of baseline models, particularly in more diverse linguistic contexts.' Questions='1. Could the authors provide more insights on how the self-synthesized examples specifically contribute to improving model performance compared to traditional augmentation techniques?\\n2. How generalizable is the proposed method to languages with less training data available for the self-supervised learning models?\\n3. Are there specific scenarios or types of speech data where the method might not perform as well, and how could it be adapted to handle such cases?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper presents an innovative approach to voice conversion by effectively utilizing entangled speech representations combined with self-synthesized examples for training, thus improving performance in zero-shot voice conversion tasks.\\n2. The method demonstrates applicability across various languages and conversion scenarios, showing competency in both zero-shot and cross-lingual voice conversion tasks.\\n3. The integration of prosodic information without relying on text, along with self-synthesized examples, significantly enhances speaker similarity and quality, achieving state-of-the-art results.' Weaknesses='1. While the method demonstrates effectiveness, the reliance on self-supervised learning models and speaker verification models may introduce limitations in terms of resource availability and required computational power.\\n2. The paper could further discuss potential limitations or boundary conditions where the proposed method might not perform optimally.\\n3. There is less emphasis on the comparative analysis of how the method holds against a broader set of baseline models, particularly in more diverse linguistic contexts.' Questions='1. Could the authors provide more insights on how the self-synthesized examples specifically contribute to improving model performance compared to traditional augmentation techniques?\\n2. How generalizable is the proposed method to languages with less training data available for the self-supervised learning models?\\n3. Are there specific scenarios or types of speech data where the method might not perform as well, and how could it be adapted to handle such cases?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='1. The integration of LLMs for high-level decision-making in autonomous driving systems introduces a novel approach to leverage human-like reasoning, potentially improving safety and adaptability.\\n2. The paper presents a clear framework for utilizing chain-of-thought processes, allowing the LLM to systematically address sub-problems.\\n3. Experiments demonstrating improvements in traffic flow and reduced safety penalties add empirical support to the proposed approach.\\n4. The paper is well-organized, with a clear presentation of methods and experimental results.' Weaknesses=\"1. The effectiveness and limitations of using LLMs in real-time, safety-critical applications like autonomous driving are not thoroughly explored. Concerns about LLMs' reliability and their potential randomness are not addressed.\\n2. The model's ability to deal with rare and long-tail events is not convincingly demonstrated with comprehensive or varied examples.\\n3. Comparisons with few baseline methods limit the generalizability and robustness of the conclusions drawn about the system's performance improvements.\" Questions='1. How does the system ensure consistent and reliable outputs from LLMs given potential randomness in their responses?\\n2. Can the authors elaborate on how the proposed framework handles rare and long-tail events, especially in scenarios not covered in the experiments?\\n3. What measures are taken to maintain the real-time performance necessary for decision-making in dynamic driving environments with the LLM integration?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='1. The integration of LLMs for high-level decision-making in autonomous driving systems introduces a novel approach to leverage human-like reasoning, potentially improving safety and adaptability.\\n2. The paper presents a clear framework for utilizing chain-of-thought processes, allowing the LLM to systematically address sub-problems.\\n3. Experiments demonstrating improvements in traffic flow and reduced safety penalties add empirical support to the proposed approach.\\n4. The paper is well-organized, with a clear presentation of methods and experimental results.' Weaknesses=\"1. The effectiveness and limitations of using LLMs in real-time, safety-critical applications like autonomous driving are not thoroughly explored. Concerns about LLMs' reliability and their potential randomness are not addressed.\\n2. The model's ability to deal with rare and long-tail events is not convincingly demonstrated with comprehensive or varied examples.\\n3. Comparisons with few baseline methods limit the generalizability and robustness of the conclusions drawn about the system's performance improvements.\" Questions='1. How does the system ensure consistent and reliable outputs from LLMs given potential randomness in their responses?\\n2. Can the authors elaborate on how the proposed framework handles rare and long-tail events, especially in scenarios not covered in the experiments?\\n3. What measures are taken to maintain the real-time performance necessary for decision-making in dynamic driving environments with the LLM integration?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The proposed modifications to transformer blocks provide a noticeable improvement in training efficiency and parameter reduction while maintaining model performance.\\n2. The paper clearly articulates the modifications made to the transformer architecture and the rationale behind these changes, providing a thoughtful exploration of how to streamline a widely used architecture.\\n3. The study uses comprehensive experiments to validate the efficacy of their approach across different transformer models, including both encoder-only and decoder-only setups.' Weaknesses='1. The generalizability of the results to larger models or more diverse datasets remains to be seen, as experiments were primarily conducted on smaller model sizes (~100M parameters) and limited to specific datasets like CodeParrot and GLUE.\\n2. The paper could benefit from a more thorough exploration of potential trade-offs in model expressiveness or performance across a broader range of NLP tasks not covered in the current scope.' Questions='1. How well do these simplified transformer architectures perform when scaled to larger model sizes or across a wider range of NLP tasks beyond those presented in the paper?\\n2. Are there scenarios or applications where the reduced complexity may lead to a noticeable drop in performance compared to traditional transformer architectures?\\n3. How do the authors envision these architectural changes impacting the future development and deployment of transformer models, especially in resource-constrained environments?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The proposed modifications to transformer blocks provide a noticeable improvement in training efficiency and parameter reduction while maintaining model performance.\\n2. The paper clearly articulates the modifications made to the transformer architecture and the rationale behind these changes, providing a thoughtful exploration of how to streamline a widely used architecture.\\n3. The study uses comprehensive experiments to validate the efficacy of their approach across different transformer models, including both encoder-only and decoder-only setups.' Weaknesses='1. The generalizability of the results to larger models or more diverse datasets remains to be seen, as experiments were primarily conducted on smaller model sizes (~100M parameters) and limited to specific datasets like CodeParrot and GLUE.\\n2. The paper could benefit from a more thorough exploration of potential trade-offs in model expressiveness or performance across a broader range of NLP tasks not covered in the current scope.' Questions='1. How well do these simplified transformer architectures perform when scaled to larger model sizes or across a wider range of NLP tasks beyond those presented in the paper?\\n2. Are there scenarios or applications where the reduced complexity may lead to a noticeable drop in performance compared to traditional transformer architectures?\\n3. How do the authors envision these architectural changes impacting the future development and deployment of transformer models, especially in resource-constrained environments?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper addresses a critical issue in neural network training involving optimization layers, specifically tackling the problem of infeasibility in quadratic programming layers, crucial for advancing applications in dynamic tasks.\\n2. It introduces an innovative method leveraging primal-dual augmented Lagrangian techniques, enabling differentiation even through infeasible quadratic programming layers, thereby enhancing the modeling expressiveness of neural networks.\\n3. The implementation of the QPLayer package provides a practical contribution to the field, demonstrating its utility on real tasks and interfacing with common learning frameworks.' Weaknesses='1. While the proposed method shows promising results, the lack of comparison with straightforward alternatives like introducing slack variables or penalty terms for handling infeasibility issues may limit the depth of the presented analysis.\\n2. The paper might benefit from more detailed explanations or guidelines for selecting hyperparameters within the proposed framework, which could enhance usability for practitioners.\\n3. There is limited exploration of potential limitations or edge cases where the proposed method might struggle; more comprehensive testing scenarios could strengthen the claim of its robustness.' Questions='1. How does the approach compare with more straightforward methods, like using slack variables or penalties, in terms of efficiency and ease of integration into existing neural network architectures?\\n2. Can the authors provide more insights or guidelines on hyperparameter selection within the proposed framework to facilitate its adoption?\\n3. Are there specific types of problems or conditions where the proposed method might not perform as well, and could the authors discuss potential solutions to those challenges?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper addresses a critical issue in neural network training involving optimization layers, specifically tackling the problem of infeasibility in quadratic programming layers, crucial for advancing applications in dynamic tasks.\\n2. It introduces an innovative method leveraging primal-dual augmented Lagrangian techniques, enabling differentiation even through infeasible quadratic programming layers, thereby enhancing the modeling expressiveness of neural networks.\\n3. The implementation of the QPLayer package provides a practical contribution to the field, demonstrating its utility on real tasks and interfacing with common learning frameworks.' Weaknesses='1. While the proposed method shows promising results, the lack of comparison with straightforward alternatives like introducing slack variables or penalty terms for handling infeasibility issues may limit the depth of the presented analysis.\\n2. The paper might benefit from more detailed explanations or guidelines for selecting hyperparameters within the proposed framework, which could enhance usability for practitioners.\\n3. There is limited exploration of potential limitations or edge cases where the proposed method might struggle; more comprehensive testing scenarios could strengthen the claim of its robustness.' Questions='1. How does the approach compare with more straightforward methods, like using slack variables or penalties, in terms of efficiency and ease of integration into existing neural network architectures?\\n2. Can the authors provide more insights or guidelines on hyperparameter selection within the proposed framework to facilitate its adoption?\\n3. Are there specific types of problems or conditions where the proposed method might not perform as well, and could the authors discuss potential solutions to those challenges?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper presents a novel approach to contextual stochastic optimization that integrates learning and optimization to achieve robustness against uncertainty. The method shows theoretical guarantees with bound regret and demonstrates practical advantages in experiments, including in a real-world electricity generation problem.' Weaknesses='The presentation of the method and the experimental results could be more detailed. The explanation of the discretization and secondary optimization integration is somewhat lacking in detail, making it difficult to fully grasp the mechanism and effectiveness of the method.' Questions='How does the discretization of the feasible region ensure that the theoretical guarantees hold? Are there particular types of optimization problems for which this approach is particularly well-suited or not well-suited?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper presents a novel approach to contextual stochastic optimization that integrates learning and optimization to achieve robustness against uncertainty. The method shows theoretical guarantees with bound regret and demonstrates practical advantages in experiments, including in a real-world electricity generation problem.' Weaknesses='The presentation of the method and the experimental results could be more detailed. The explanation of the discretization and secondary optimization integration is somewhat lacking in detail, making it difficult to fully grasp the mechanism and effectiveness of the method.' Questions='How does the discretization of the feasible region ensure that the theoretical guarantees hold? Are there particular types of optimization problems for which this approach is particularly well-suited or not well-suited?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The introduction of Mixture-of-Prompts (MoP) as a novel approach to optimize prompts by dividing the problem space into manageable regions is quite ingenious and highlights potential for improved task performance. The paper's results showcasing up to 43% increase in performance compared to previous methods emphasizes the novelty and efficacy of this method.\" Weaknesses='One of the main weaknesses is the assumption that the problem space can be effectively divided into homogeneous regions manageable by separate experts and the implicit complexity such a division entails in practical applications. The methodology may also face scalability issues as the number of regions and experts grow. Additionally, there is a potential limitation in the specific criterion for clustering tasks and the specificity of logger selection for diverse NLP tasks.' Questions='How does MoP handle edge cases where certain tasks or demos do not naturally fit into one of the pre-defined homogeneous regions? Is there a fall-back mechanism in place for cases where the assigned expert underperforms in a certain task region? Additionally, what considerations or safeguards were put in place to balance between the number of experts (and regions) versus computational efficiency?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The introduction of Mixture-of-Prompts (MoP) as a novel approach to optimize prompts by dividing the problem space into manageable regions is quite ingenious and highlights potential for improved task performance. The paper's results showcasing up to 43% increase in performance compared to previous methods emphasizes the novelty and efficacy of this method.\" Weaknesses='One of the main weaknesses is the assumption that the problem space can be effectively divided into homogeneous regions manageable by separate experts and the implicit complexity such a division entails in practical applications. The methodology may also face scalability issues as the number of regions and experts grow. Additionally, there is a potential limitation in the specific criterion for clustering tasks and the specificity of logger selection for diverse NLP tasks.' Questions='How does MoP handle edge cases where certain tasks or demos do not naturally fit into one of the pre-defined homogeneous regions? Is there a fall-back mechanism in place for cases where the assigned expert underperforms in a certain task region? Additionally, what considerations or safeguards were put in place to balance between the number of experts (and regions) versus computational efficiency?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=3 Strengths=\"The paper provides a novel theoretical approach using statistical physics to explain issues in overparameterized heteroskedastic models, offering potential insights applicable to various architectures and datasets. The introduction of a nonparametric free energy framework and the empirical validation of the theoretical findings strengthen the paper's contributions.\" Weaknesses='The presentation could be improved to provide clearer explanations and insights, especially for readers not familiar with statistical physics or the specifics of field-theoretical approaches. While novel, the theoretical framework might benefit from more intuitive explanations or analogies to make it more accessible to a broader audience.' Questions='How well does the proposed nonparametric free energy framework generalize to different types of real-world datasets beyond those tested? Are there any limitations observed with specific types of empirical data or particular neural network architectures?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=3 Strengths=\"The paper provides a novel theoretical approach using statistical physics to explain issues in overparameterized heteroskedastic models, offering potential insights applicable to various architectures and datasets. The introduction of a nonparametric free energy framework and the empirical validation of the theoretical findings strengthen the paper's contributions.\" Weaknesses='The presentation could be improved to provide clearer explanations and insights, especially for readers not familiar with statistical physics or the specifics of field-theoretical approaches. While novel, the theoretical framework might benefit from more intuitive explanations or analogies to make it more accessible to a broader audience.' Questions='How well does the proposed nonparametric free energy framework generalize to different types of real-world datasets beyond those tested? Are there any limitations observed with specific types of empirical data or particular neural network architectures?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=4 Strengths=\"- The paper addresses a very relevant problem in domain adaptation, particularly concerning OOD data where model performance often degrades significantly, an issue common in fields such as astronomy.\\n- The 'Connect Later' framework presents a novel approach by proposing the use of targeted augmentations during the fine-tuning phase, which seems to effectively address the performance issues of existing methods.\\n- The proposed method has been validated across multiple domains, demonstrating significant improvements in OOD performance, which could have practical implications beyond the fields studied.\\n- The study provides experimental evidence supporting the hypothesis, showcasing improved performance metrics across a variety of benchmark datasets.\" Weaknesses=\"- While the paper proposes a novel approach, it lacks a detailed theoretical explanation or justification for why the 'Connect Later' framework should outperform existing methods within the context of domain adaptation.\\n- The comparisons made with other methods are interesting; however, the paper could benefit from a more comprehensive analysis covering broader variations of previous techniques to clearly highlight why this method excels.\\n- The paper does not delve deeply into the limitations or potential drawbacks such as increased computational costs, potentially complex processes in creating targeted augmentations, or scalability issues in certain applications.\\n- More discussion on the potential limitations or biases introduced by the simplistic or domain-specific nature of targeted augmentations might provide a more balanced critique of the approach.\" Questions='- Could the methodology be applied successfully to other domains or distinct types of data? Particularly, could it be employed on smaller datasets where pretraining might not be feasible due to lack of sufficient data?\\n- How labor-intensive is the process of designing targeted augmentations for each specific application, and could this aspect limit the scalability of the method across new domains?\\n- To what extent is the observed improvement attributable specifically to the tailored augmentations strategy, versus the intrinsic advantages due to the pretraining approach?\\n- How does the approach perform against unsupervised domain adaptation methods that do not rely on labeled source data?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=4 Strengths=\"- The paper addresses a very relevant problem in domain adaptation, particularly concerning OOD data where model performance often degrades significantly, an issue common in fields such as astronomy.\\n- The 'Connect Later' framework presents a novel approach by proposing the use of targeted augmentations during the fine-tuning phase, which seems to effectively address the performance issues of existing methods.\\n- The proposed method has been validated across multiple domains, demonstrating significant improvements in OOD performance, which could have practical implications beyond the fields studied.\\n- The study provides experimental evidence supporting the hypothesis, showcasing improved performance metrics across a variety of benchmark datasets.\" Weaknesses=\"- While the paper proposes a novel approach, it lacks a detailed theoretical explanation or justification for why the 'Connect Later' framework should outperform existing methods within the context of domain adaptation.\\n- The comparisons made with other methods are interesting; however, the paper could benefit from a more comprehensive analysis covering broader variations of previous techniques to clearly highlight why this method excels.\\n- The paper does not delve deeply into the limitations or potential drawbacks such as increased computational costs, potentially complex processes in creating targeted augmentations, or scalability issues in certain applications.\\n- More discussion on the potential limitations or biases introduced by the simplistic or domain-specific nature of targeted augmentations might provide a more balanced critique of the approach.\" Questions='- Could the methodology be applied successfully to other domains or distinct types of data? Particularly, could it be employed on smaller datasets where pretraining might not be feasible due to lack of sufficient data?\\n- How labor-intensive is the process of designing targeted augmentations for each specific application, and could this aspect limit the scalability of the method across new domains?\\n- To what extent is the observed improvement attributable specifically to the tailored augmentations strategy, versus the intrinsic advantages due to the pretraining approach?\\n- How does the approach perform against unsupervised domain adaptation methods that do not rely on labeled source data?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The paper addresses an important and current problem of ensuring privacy in information retrieval for augmenting large language models, which is increasingly relevant as data distribution and privacy regulations become more complex.\\n2. The proposed PRAG framework presents an innovative combination of multi-party computation and secret sharing techniques, contributing a novel MPC-friendly protocol for inverted file approximate search.\\n3. The experiments demonstrate the method's potential by providing privacy without compromising retrieval capabilities, indicating practical utility especially for sensitive data applications.\" Weaknesses=\"1. The computational overhead and costs are higher compared to traditional methods, which may limit practical deployment without further optimization.\\n2. The method's reliance on synthetic datasets may not fully represent real-world complexities and constraints, potentially affecting generalizability.\\n3. Some implementation details of the MPC protocol and handling of potential inefficiencies are not thoroughly explored, which could be crucial for practitioners looking to adopt the method.\" Questions='1. Can real-world datasets of different structures and scales be tested to further validate the performance and efficiency of PRAG?\\n2. What are the prospects for optimizing the computational costs, and does the proposed method scale well with growing data and server networks?\\n3. Are there any specific privacy trade-offs that the PRAG approach necessitates, such as potential data utility reductions or increased latency?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The paper addresses an important and current problem of ensuring privacy in information retrieval for augmenting large language models, which is increasingly relevant as data distribution and privacy regulations become more complex.\\n2. The proposed PRAG framework presents an innovative combination of multi-party computation and secret sharing techniques, contributing a novel MPC-friendly protocol for inverted file approximate search.\\n3. The experiments demonstrate the method's potential by providing privacy without compromising retrieval capabilities, indicating practical utility especially for sensitive data applications.\" Weaknesses=\"1. The computational overhead and costs are higher compared to traditional methods, which may limit practical deployment without further optimization.\\n2. The method's reliance on synthetic datasets may not fully represent real-world complexities and constraints, potentially affecting generalizability.\\n3. Some implementation details of the MPC protocol and handling of potential inefficiencies are not thoroughly explored, which could be crucial for practitioners looking to adopt the method.\" Questions='1. Can real-world datasets of different structures and scales be tested to further validate the performance and efficiency of PRAG?\\n2. What are the prospects for optimizing the computational costs, and does the proposed method scale well with growing data and server networks?\\n3. Are there any specific privacy trade-offs that the PRAG approach necessitates, such as potential data utility reductions or increased latency?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='1. The proposed Directional Distance Field (DDF) offers a novel way to measure the difference between 3D point clouds, focusing on geometric surface differences rather than point-wise correspondences.\\n2. The method showcases improvements in accuracy across multiple 3D tasks such as shape reconstruction, registration, and scene flow estimation.\\n3. It provides a more memory-efficient and computationally effective solution compared to traditional metrics like EMD and CD.' Weaknesses='1. The explanation of how reference points are generated and used for directional distances could be clearer and more detailed.\\n2. Some comparisons with existing metrics might lack depth or could benefit from additional datasets or practical scenarios.' Questions='1. How are reference points specifically generated from the point clouds, and what criteria determine their selection?\\n2. Could you provide more clarity on how DDF handles different sampling scenarios within 3D point clouds?\\n3. How does DDF integrate with existing 3D point cloud processing frameworks, and are there limitations to its implementation?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='1. The proposed Directional Distance Field (DDF) offers a novel way to measure the difference between 3D point clouds, focusing on geometric surface differences rather than point-wise correspondences.\\n2. The method showcases improvements in accuracy across multiple 3D tasks such as shape reconstruction, registration, and scene flow estimation.\\n3. It provides a more memory-efficient and computationally effective solution compared to traditional metrics like EMD and CD.' Weaknesses='1. The explanation of how reference points are generated and used for directional distances could be clearer and more detailed.\\n2. Some comparisons with existing metrics might lack depth or could benefit from additional datasets or practical scenarios.' Questions='1. How are reference points specifically generated from the point clouds, and what criteria determine their selection?\\n2. Could you provide more clarity on how DDF handles different sampling scenarios within 3D point clouds?\\n3. How does DDF integrate with existing 3D point cloud processing frameworks, and are there limitations to its implementation?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths=\"1. The paper demonstrates a novel approach of utilizing high-quality, 'textbook quality' data to train a smaller neural network achieving superior performance, which challenges the current paradigm of simply increasing scale.\\n2. Efficient utilization of resources is shown, achieving state-of-the-art results with significantly less computational power and data volume.\\n3. The paper is well-organized and presents its findings clearly, making a compelling case for data quality over quantity.\\n4. Offers potential applications in domains where computational resources are limited, promoting more sustainable AI development practices.\" Weaknesses='1. The model seems to have specific limitations in its application to Python, which might restrict its generalizability to other programming languages without further modifications.\\n2. There is sensitivity to prompt variations, which could affect consistent performance, tipping a balance between customization and robust model behavior.\\n3. Some critical details concerning data processing and model architecture are proprietary and not completely disclosed, hindering full reproducibility by others.' Questions=\"1. How does the model's performance on non-code or more natural language processing tasks compare to the specialized success in programming tasks?\\n2. Could the methodology of utilizing high-quality data be extended effectively to language models in other domains, or is there something unique about the training on code?\\n3. Alongside Python, are there plans to extend or adapt phi-1 to support other programming languages or to address the limitations identified with prompt sensitivity?\"\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths=\"1. The paper demonstrates a novel approach of utilizing high-quality, 'textbook quality' data to train a smaller neural network achieving superior performance, which challenges the current paradigm of simply increasing scale.\\n2. Efficient utilization of resources is shown, achieving state-of-the-art results with significantly less computational power and data volume.\\n3. The paper is well-organized and presents its findings clearly, making a compelling case for data quality over quantity.\\n4. Offers potential applications in domains where computational resources are limited, promoting more sustainable AI development practices.\" Weaknesses='1. The model seems to have specific limitations in its application to Python, which might restrict its generalizability to other programming languages without further modifications.\\n2. There is sensitivity to prompt variations, which could affect consistent performance, tipping a balance between customization and robust model behavior.\\n3. Some critical details concerning data processing and model architecture are proprietary and not completely disclosed, hindering full reproducibility by others.' Questions=\"1. How does the model's performance on non-code or more natural language processing tasks compare to the specialized success in programming tasks?\\n2. Could the methodology of utilizing high-quality data be extended effectively to language models in other domains, or is there something unique about the training on code?\\n3. Alongside Python, are there plans to extend or adapt phi-1 to support other programming languages or to address the limitations identified with prompt sensitivity?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper provides a novel theoretical framework that distinguishes between task learning and task retrieval within in-context learning, a topic that has not been thoroughly analyzed before.\\n2. By introducing generative models for pretraining data and in-context samples, the authors offer a new approach to evaluating risk and performance in language models.\\n3. The experimental validation on Transformers adds credence to the theoretical findings, showing practical relevance.' Weaknesses='1. The generative modeling approach, while insightful, might be overly simplistic for capturing the full complexity of language model pretraining data.\\n2. The reliance on Gaussian mixture models assumes a certain level of homogeneity in task distributions that may not align with real-world data complexities.\\n3. The paper assumes access to clean task distributions, which might not be practically feasible in many scenarios.' Questions='1. How sensitive are the results to the assumption that pretraining data is drawn from noisy task distributions centered around clean tasks?\\n2. Can the proposed methodology be extended beyond Gaussian mixture models to more complex distributional assumptions?\\n3. How does the approach handle cases where the underlying task distributions are not well-represented by the chosen generative models?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper provides a novel theoretical framework that distinguishes between task learning and task retrieval within in-context learning, a topic that has not been thoroughly analyzed before.\\n2. By introducing generative models for pretraining data and in-context samples, the authors offer a new approach to evaluating risk and performance in language models.\\n3. The experimental validation on Transformers adds credence to the theoretical findings, showing practical relevance.' Weaknesses='1. The generative modeling approach, while insightful, might be overly simplistic for capturing the full complexity of language model pretraining data.\\n2. The reliance on Gaussian mixture models assumes a certain level of homogeneity in task distributions that may not align with real-world data complexities.\\n3. The paper assumes access to clean task distributions, which might not be practically feasible in many scenarios.' Questions='1. How sensitive are the results to the assumption that pretraining data is drawn from noisy task distributions centered around clean tasks?\\n2. Can the proposed methodology be extended beyond Gaussian mixture models to more complex distributional assumptions?\\n3. How does the approach handle cases where the underlying task distributions are not well-represented by the chosen generative models?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=4 Strengths=\"The method provides a novel approach to handle label noise using a meta-learning framework that does not rely on a clean validation set, making it widely applicable. The empirical results demonstrate significant improvement over baseline methods across various datasets, showcasing its efficacy. Additionally, the study's integration with existing techniques highlights its practicality as a complementary solution.\" Weaknesses='The approach may still face limitations when applied to real-world datasets where noise characteristics differ greatly from synthetic datasets used in experiments. While empirical results are compelling, the reliance on meta-learning frameworks might introduce complexity in model training and deployment.' Questions='How does the method performance compare when dealing with datasets exhibiting systematic noise, rather than random noise? Does the meta-learning framework significantly increase computational requirements, and how does it affect training times compared to traditional methods?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=4 Strengths=\"The method provides a novel approach to handle label noise using a meta-learning framework that does not rely on a clean validation set, making it widely applicable. The empirical results demonstrate significant improvement over baseline methods across various datasets, showcasing its efficacy. Additionally, the study's integration with existing techniques highlights its practicality as a complementary solution.\" Weaknesses='The approach may still face limitations when applied to real-world datasets where noise characteristics differ greatly from synthetic datasets used in experiments. While empirical results are compelling, the reliance on meta-learning frameworks might introduce complexity in model training and deployment.' Questions='How does the method performance compare when dealing with datasets exhibiting systematic noise, rather than random noise? Does the meta-learning framework significantly increase computational requirements, and how does it affect training times compared to traditional methods?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"WaveFluid presents an innovative approach to audio synthesis by learning a velocity field through adversarial training, which avoids some of the typical constraints found in other models. The claim of achieving comparable audio quality with significantly fewer inference steps is a strong advantage, especially in terms of computational efficiency. The integration of modular design features such as attention blocks enhances the model's performance capabilities.\" Weaknesses=\"The method's reliance on modular design and adversarial training can add complexity and may require extensive hyperparameter tuning. While the paper claims competitive performance, detailed comparative analysis against a wider array of baseline models, particularly state-of-the-art GANs, could strengthen the experimental validation of the proposed approach. The paper's explanation of the probabilistic refiner stage could benefit from further clarity and detail.\" Questions=\"How does the training and inference speed of WaveFluid compare quantitatively against state-of-the-art flow-based and diffusion models? Are there specific scenarios or datasets where WaveFluid's performance significantly exceeds that of comparable models, and what might be the contributing factors?\"\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"WaveFluid presents an innovative approach to audio synthesis by learning a velocity field through adversarial training, which avoids some of the typical constraints found in other models. The claim of achieving comparable audio quality with significantly fewer inference steps is a strong advantage, especially in terms of computational efficiency. The integration of modular design features such as attention blocks enhances the model's performance capabilities.\" Weaknesses=\"The method's reliance on modular design and adversarial training can add complexity and may require extensive hyperparameter tuning. While the paper claims competitive performance, detailed comparative analysis against a wider array of baseline models, particularly state-of-the-art GANs, could strengthen the experimental validation of the proposed approach. The paper's explanation of the probabilistic refiner stage could benefit from further clarity and detail.\" Questions=\"How does the training and inference speed of WaveFluid compare quantitatively against state-of-the-art flow-based and diffusion models? Are there specific scenarios or datasets where WaveFluid's performance significantly exceeds that of comparable models, and what might be the contributing factors?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed framework effectively integrates interpretability and adversarial robustness in neural network classifiers, addressing a significant gap in the current methodologies. The use of GANs for generating counterfactuals and adversarial samples is innovative and shows promising results on classification tasks. The paper illustrates its approach with effective use of specialized GAN components and evaluates performance using relevant datasets, contributing to practical advances in image classification robustness.' Weaknesses=\"The framework is tested only on binary classification tasks, limiting its demonstrated applicability. The paper does not fully explore or demonstrate its performance across multiclass classification tasks, which are common in real-world applications. Additionally, while the framework integrates interpretability and robustness, the extent to which each component individually contributes to performance improvement is not thoroughly dissected, potentially limiting insights into the framework's functional contributions.\" Questions='How scalable is the proposed framework when applied to multiclass classification tasks? What adjustments would be necessary to adapt it to such contexts? Additionally, although the framework claims to achieve competitive IoU scores compared to traditional methods, more details on specific metrics and comparative baselines would help in evaluating its advantages.'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed framework effectively integrates interpretability and adversarial robustness in neural network classifiers, addressing a significant gap in the current methodologies. The use of GANs for generating counterfactuals and adversarial samples is innovative and shows promising results on classification tasks. The paper illustrates its approach with effective use of specialized GAN components and evaluates performance using relevant datasets, contributing to practical advances in image classification robustness.' Weaknesses=\"The framework is tested only on binary classification tasks, limiting its demonstrated applicability. The paper does not fully explore or demonstrate its performance across multiclass classification tasks, which are common in real-world applications. Additionally, while the framework integrates interpretability and robustness, the extent to which each component individually contributes to performance improvement is not thoroughly dissected, potentially limiting insights into the framework's functional contributions.\" Questions='How scalable is the proposed framework when applied to multiclass classification tasks? What adjustments would be necessary to adapt it to such contexts? Additionally, although the framework claims to achieve competitive IoU scores compared to traditional methods, more details on specific metrics and comparative baselines would help in evaluating its advantages.'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The method uses a novel combination of dilated convolution and inception blocks, showcasing good architectural design for capturing varied receptive fields.\\n2. The application of a noise-resilient sampling strategy using a spectrum-based low-pass filter for robust representation learning is innovative.\\n3. The proposed method is shown to outperform existing state-of-the-art approaches and is more parameter efficient, highlighting its scalability.' Weaknesses=\"1. The paper doesn't fully explore the generalizability of the approach across different domains; the results are primarily limited to given benchmark datasets.\\n2. Although the method is innovative, the integration and effectiveness of the low-pass filter might not be fully justified beyond certain types of noise.\\n3. The combination of these methods seems complex, and it’s not clear if simpler models could achieve similar benefits with better interpretability.\" Questions='1. How does the noise-resilient sampling strategy handle varying levels of noise, particularly when the noise characteristics are unknown or variable over time?\\n2. Were there any specific challenges faced while integrating the inception block with dilated convolutions?\\n3. Can the method be adapted for real-time applications, and if so, what are the potential computational constraints?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The method uses a novel combination of dilated convolution and inception blocks, showcasing good architectural design for capturing varied receptive fields.\\n2. The application of a noise-resilient sampling strategy using a spectrum-based low-pass filter for robust representation learning is innovative.\\n3. The proposed method is shown to outperform existing state-of-the-art approaches and is more parameter efficient, highlighting its scalability.' Weaknesses=\"1. The paper doesn't fully explore the generalizability of the approach across different domains; the results are primarily limited to given benchmark datasets.\\n2. Although the method is innovative, the integration and effectiveness of the low-pass filter might not be fully justified beyond certain types of noise.\\n3. The combination of these methods seems complex, and it’s not clear if simpler models could achieve similar benefits with better interpretability.\" Questions='1. How does the noise-resilient sampling strategy handle varying levels of noise, particularly when the noise characteristics are unknown or variable over time?\\n2. Were there any specific challenges faced while integrating the inception block with dilated convolutions?\\n3. Can the method be adapted for real-time applications, and if so, what are the potential computational constraints?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths=\"- The proposed method, Vicinal Risk Proxy (VRP), offers a novel approach to improve the reliability of out-of-distribution generalization assessments by integrating the surrounding test samples' responses.\\n- The paper clearly highlights the weaknesses of current OOD methods and proposes a potentially effective solution to address spurious model responses.\\n- The use of multiple datasets and test scenarios to demonstrate the effectiveness of VRP helps in reinforcing the validity of the proposed method.\\n- Clear representation and explanation of how VRP smooths out the spurious scores, alongside thorough empirical analysis support the results.\" Weaknesses=\"- While the paper introduces a new way to calculate risk proxies, it may require significant computational resources due to the need to evaluate neighboring samples' responses. This could potentially limit its applicability to large datasets or real-time applications.\\n- The integration of neighboring sample information, while novel, may introduce complexities or unintended biases depending on the test dataset distribution.\\n- It seems that the VRP approach, while promising, might not fully eliminate the reliance on test labels for true generalization evaluation—there is a possibility the proxy might still fall short under certain conditions not explicitly tested in the paper.\" Questions='- To what extent does the improvement in correlation offered by VRP justify the increased computational cost? Has this been measured or discussed?\\n- How does VRP perform across datasets with highly varying distributions, and are there limitations in specific patterns it may not handle well?\\n- Given that neighboring test samples are vital to the VRP approach, how does it deal with datasets where such samples are not inherently similar, or where the similarity metrics themselves might be biased or imprecise?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths=\"- The proposed method, Vicinal Risk Proxy (VRP), offers a novel approach to improve the reliability of out-of-distribution generalization assessments by integrating the surrounding test samples' responses.\\n- The paper clearly highlights the weaknesses of current OOD methods and proposes a potentially effective solution to address spurious model responses.\\n- The use of multiple datasets and test scenarios to demonstrate the effectiveness of VRP helps in reinforcing the validity of the proposed method.\\n- Clear representation and explanation of how VRP smooths out the spurious scores, alongside thorough empirical analysis support the results.\" Weaknesses=\"- While the paper introduces a new way to calculate risk proxies, it may require significant computational resources due to the need to evaluate neighboring samples' responses. This could potentially limit its applicability to large datasets or real-time applications.\\n- The integration of neighboring sample information, while novel, may introduce complexities or unintended biases depending on the test dataset distribution.\\n- It seems that the VRP approach, while promising, might not fully eliminate the reliance on test labels for true generalization evaluation—there is a possibility the proxy might still fall short under certain conditions not explicitly tested in the paper.\" Questions='- To what extent does the improvement in correlation offered by VRP justify the increased computational cost? Has this been measured or discussed?\\n- How does VRP perform across datasets with highly varying distributions, and are there limitations in specific patterns it may not handle well?\\n- Given that neighboring test samples are vital to the VRP approach, how does it deal with datasets where such samples are not inherently similar, or where the similarity metrics themselves might be biased or imprecise?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The DiGT model introduces a novel approach to handling directionality in graphs using dual encodings and dynamic attention mechanisms, which fills a notable gap in current graph transformer architectures. The method is well-validated on newly designed datasets that highlight the importance of edge directionality.' Weaknesses=\"The method's reliance on novel datasets suggests potential overfitting to specific scenarios where edge directionality is crucial. There might be limited applicability to general undirected graph problems or datasets with less pronounced directionality.\" Questions='How does DiGT perform on standard benchmarks where directional information is less critical? Could elements of DiGT be integrated into existing graph transformer models for a broader range of graphs?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The DiGT model introduces a novel approach to handling directionality in graphs using dual encodings and dynamic attention mechanisms, which fills a notable gap in current graph transformer architectures. The method is well-validated on newly designed datasets that highlight the importance of edge directionality.' Weaknesses=\"The method's reliance on novel datasets suggests potential overfitting to specific scenarios where edge directionality is crucial. There might be limited applicability to general undirected graph problems or datasets with less pronounced directionality.\" Questions='How does DiGT perform on standard benchmarks where directional information is less critical? Could elements of DiGT be integrated into existing graph transformer models for a broader range of graphs?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='1. The introduction of the Modified Silhouette Score (MSS) provides a novel approach for assessing silhouette scores in heterogeneous graph environments, which is a valuable contribution to GNN adversarial attack literature.\\n2. SheAttack effectively addresses the limitations of homophily assumptions and expands applicability to various graph types, demonstrating its adaptability and efficacy.\\n3. The methodology of employing Greedy Randomized Block Coordinate Descent is efficient, making the approach scalable and effective without relying on in-depth prior information about the model being attacked.' Weaknesses='1. The novelty of using silhouette scores has limitations, as it primarily builds on existing concepts rather than introducing groundbreaking new methods.\\n2. The paper may need further elaboration on the theoretical underpinnings of why Modified Silhouette Score (MSS) effectively generalizes across different graph structures.\\n3. While comprehensive, the relations between experimental results and methods used could be more delineated to enhance understanding of performance differentials across datasets.' Questions='1. How sensitive is the Modified Silhouette Score to varying parameters, and is there a recommended method for setting these parameters for new unknown datasets?\\n2. Can SheAttack be further adapted or extended to attacks beyond structural modifications, such as feature perturbations, and if so, how?\\n3. Given the introduction of MSS as a primary tool, are there potential biases or limitations in how MSS may interpret structural changes in differing graph contexts?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='1. The introduction of the Modified Silhouette Score (MSS) provides a novel approach for assessing silhouette scores in heterogeneous graph environments, which is a valuable contribution to GNN adversarial attack literature.\\n2. SheAttack effectively addresses the limitations of homophily assumptions and expands applicability to various graph types, demonstrating its adaptability and efficacy.\\n3. The methodology of employing Greedy Randomized Block Coordinate Descent is efficient, making the approach scalable and effective without relying on in-depth prior information about the model being attacked.' Weaknesses='1. The novelty of using silhouette scores has limitations, as it primarily builds on existing concepts rather than introducing groundbreaking new methods.\\n2. The paper may need further elaboration on the theoretical underpinnings of why Modified Silhouette Score (MSS) effectively generalizes across different graph structures.\\n3. While comprehensive, the relations between experimental results and methods used could be more delineated to enhance understanding of performance differentials across datasets.' Questions='1. How sensitive is the Modified Silhouette Score to varying parameters, and is there a recommended method for setting these parameters for new unknown datasets?\\n2. Can SheAttack be further adapted or extended to attacks beyond structural modifications, such as feature perturbations, and if so, how?\\n3. Given the introduction of MSS as a primary tool, are there potential biases or limitations in how MSS may interpret structural changes in differing graph contexts?'\n",
      "Finished gpt4o on data/test_data/few_shot/test_data_2024_summary_prompts_fewshot.jsonl, saved => results/gpt4o/test_data_2024_summary_prompts_fewshot_gpt4o_results.jsonl\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. Innovative Approach: TTformer introduces a unique concept of tense-specific attention modules to capture different dimensions of traffic flow, which appears to be a novel and effective method for improving traffic forecasting models.\\n2. Robustness to Incomplete Data: The model's use of contrastive learning with negative filtering provides robustness against incomplete traffic data, potentially increasing the model's applicability in real-world scenarios where data may be missing or noisy.\\n3. Improved Forecasting Accuracy: Empirical results show that TTformer significantly outperforms existing models, highlighting its effectiveness in capturing spatio-temporal dependencies and improving traffic forecasting accuracy.\" Weaknesses=\"1. Clarity and Completeness: The paper's explanation of the tri-tense attention modules and negative filtering technique could be more detailed to fully convey the methodology.\\n2. Comparisons and Benchmarks: While the model demonstrates performance improvements, it's unclear how it compares across diverse datasets or in different traffic conditions beyond those tested.\\n3. Computational Complexity: The addition of three tense-specific attention modules may increase computational demands, yet the impact on model efficiency and scalability is not addressed or quantified in the paper.\" Questions=\"1. How does TTformer handle the balance between capturing long-term dependencies and computational efficiency, given the potential overhead from three separate attention modules?\\n2. What specific datasets and evaluation metrics were used to demonstrate the model's performance? Are there plans to validate the model on additional or more varied traffic datasets?\\n3. Can the approach be generalized or adapted for use in other time series forecasting applications beyond traffic flow? If so, what modifications would be necessary?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. Innovative Approach: TTformer introduces a unique concept of tense-specific attention modules to capture different dimensions of traffic flow, which appears to be a novel and effective method for improving traffic forecasting models.\\n2. Robustness to Incomplete Data: The model's use of contrastive learning with negative filtering provides robustness against incomplete traffic data, potentially increasing the model's applicability in real-world scenarios where data may be missing or noisy.\\n3. Improved Forecasting Accuracy: Empirical results show that TTformer significantly outperforms existing models, highlighting its effectiveness in capturing spatio-temporal dependencies and improving traffic forecasting accuracy.\" Weaknesses=\"1. Clarity and Completeness: The paper's explanation of the tri-tense attention modules and negative filtering technique could be more detailed to fully convey the methodology.\\n2. Comparisons and Benchmarks: While the model demonstrates performance improvements, it's unclear how it compares across diverse datasets or in different traffic conditions beyond those tested.\\n3. Computational Complexity: The addition of three tense-specific attention modules may increase computational demands, yet the impact on model efficiency and scalability is not addressed or quantified in the paper.\" Questions=\"1. How does TTformer handle the balance between capturing long-term dependencies and computational efficiency, given the potential overhead from three separate attention modules?\\n2. What specific datasets and evaluation metrics were used to demonstrate the model's performance? Are there plans to validate the model on additional or more varied traffic datasets?\\n3. Can the approach be generalized or adapted for use in other time series forecasting applications beyond traffic flow? If so, what modifications would be necessary?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a novel benchmark specifically designed to evaluate zero-shot cooperation abilities of reinforcement learning agents in novel settings, which is crucial for improving human-AI cooperation. It leverages state-of-the-art dual curriculum design methods, offering a new way to assess AI capabilities in a multi-agent cooperative environment. The benchmark is open-source and GPU-accelerated, promoting accessibility and broad use within the community.' Weaknesses='The paper reports that state-of-the-art DCD algorithms struggle with this new challenge, indicating potential limitations or areas for improvement in current methodologies. The challenge may require further refinement to ensure it adequately captures the complexity and variability of real-world scenarios. Additionally, there is a need for more clarity on how different components of the benchmark specifically address zero-shot cooperation outcomes.' Questions='What are the specific aspects of the Overcooked environment that pose the greatest challenge to existing DCD algorithms? How does this new benchmark compare in terms of difficulty to traditional cooperative benchmarks, and are there plans to extend it to other environments?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces a novel benchmark specifically designed to evaluate zero-shot cooperation abilities of reinforcement learning agents in novel settings, which is crucial for improving human-AI cooperation. It leverages state-of-the-art dual curriculum design methods, offering a new way to assess AI capabilities in a multi-agent cooperative environment. The benchmark is open-source and GPU-accelerated, promoting accessibility and broad use within the community.' Weaknesses='The paper reports that state-of-the-art DCD algorithms struggle with this new challenge, indicating potential limitations or areas for improvement in current methodologies. The challenge may require further refinement to ensure it adequately captures the complexity and variability of real-world scenarios. Additionally, there is a need for more clarity on how different components of the benchmark specifically address zero-shot cooperation outcomes.' Questions='What are the specific aspects of the Overcooked environment that pose the greatest challenge to existing DCD algorithms? How does this new benchmark compare in terms of difficulty to traditional cooperative benchmarks, and are there plans to extend it to other environments?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces LongPack, an innovative method for constructing high-quality long context data essential for training large language models. By focusing on referral relationships, especially in web pages using hyperlinks, the approach effectively addresses the challenge of data shortage for long-context training. The scalability of LongPack is impressive, achieving a corpus size equivalent to a full pretraining dataset with minimal input. The substantial performance improvement over previous methods further underscores its efficacy.' Weaknesses=\"The approach relies heavily on the quality and existence of hyperlinks, which might not be present in all data types. The assumption that hyperlinks naturally reflect the desired long-distance referrals might not hold universally, potentially limiting the applicability of the method to certain types of documents. Additionally, while the results are promising, more details on the exact implementation and potential limitations would provide a clearer understanding of the method's robustness.\" Questions='How does LongPack handle documents or contexts without natural hyperlinks or clear referral relationships? Is there a fallback mechanism in place for such scenarios? Could the approach be extended or adapted to other types of referral signals in non-web datasets?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces LongPack, an innovative method for constructing high-quality long context data essential for training large language models. By focusing on referral relationships, especially in web pages using hyperlinks, the approach effectively addresses the challenge of data shortage for long-context training. The scalability of LongPack is impressive, achieving a corpus size equivalent to a full pretraining dataset with minimal input. The substantial performance improvement over previous methods further underscores its efficacy.' Weaknesses=\"The approach relies heavily on the quality and existence of hyperlinks, which might not be present in all data types. The assumption that hyperlinks naturally reflect the desired long-distance referrals might not hold universally, potentially limiting the applicability of the method to certain types of documents. Additionally, while the results are promising, more details on the exact implementation and potential limitations would provide a clearer understanding of the method's robustness.\" Questions='How does LongPack handle documents or contexts without natural hyperlinks or clear referral relationships? Is there a fallback mechanism in place for such scenarios? Could the approach be extended or adapted to other types of referral signals in non-web datasets?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=8 Confidence=4 Strengths='The proposal of an augmentation-free SSL method specifically for tabular data is novel and addresses a significant gap in the handling of structured data. The approach seems to offer impressive performance benefits over conventional methods in both classification and regression tasks. The introduction of regularization tokens adds a new dimension to training JEPA models with structured data.' Weaknesses='The effectiveness of the method is yet to be validated across a broader range of datasets and conditions. The dependence on JEPA might limit the generalization to other models. Furthermore, the novelty could be questioned in comparison to existing self-supervised methods for unstructured data, leaving room for detailed comparisons to be desired.' Questions='How does T-JEPA compare specifically with other SSL methods for tabular data? Are there other comparable models beyond Gradient Boosted Decision Trees that could challenge the claim of superior performance? Could the authors provide more clarity on the role and impact of regularization tokens in long-term training stability and performance?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=8 Confidence=4 Strengths='The proposal of an augmentation-free SSL method specifically for tabular data is novel and addresses a significant gap in the handling of structured data. The approach seems to offer impressive performance benefits over conventional methods in both classification and regression tasks. The introduction of regularization tokens adds a new dimension to training JEPA models with structured data.' Weaknesses='The effectiveness of the method is yet to be validated across a broader range of datasets and conditions. The dependence on JEPA might limit the generalization to other models. Furthermore, the novelty could be questioned in comparison to existing self-supervised methods for unstructured data, leaving room for detailed comparisons to be desired.' Questions='How does T-JEPA compare specifically with other SSL methods for tabular data? Are there other comparable models beyond Gradient Boosted Decision Trees that could challenge the claim of superior performance? Could the authors provide more clarity on the role and impact of regularization tokens in long-term training stability and performance?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses a critical challenge for tabular data analysis in the context of few-shot learning, which is increasingly relevant in practical situations where labeled data is scarce. \\n2. It proposes a novel data augmentation method tailored specifically for tabular data, allowing contrastive learning to be more effective in these domains.\\n3. The introduction of a benchmark (FeSTa) comprising a wide range of datasets and algorithms provides a valuable resource for the research community, potentially fostering more robust evaluations and comparisons.' Weaknesses='1. The paper relies heavily on an augmentation technique that may not be broadly applicable beyond tabular data, thus limiting its generality to other forms of data input.\\n2. The novelty of the augmentation strategy, while beneficial for tabular domains, could be perceived as incremental given its basis on established contrastive learning frameworks.\\n3. There might be an over-reliance on benchmark performance rather than exploring the theoretical justifications and frameworks that underlie the proposed method.' Questions='1. How does the proposed augmentation maintain a balance between introducing variety within samples and preserving label consistency, especially when shuffling values?\\n2. The benchmark results demonstrate robustness in 1-shot learning scenarios, but how do these methods perform as the number of shots increases?\\n3. What are the implications for transfer learning scenarios, where tabular data might need adaptation from previously learned representations across different domains?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses a critical challenge for tabular data analysis in the context of few-shot learning, which is increasingly relevant in practical situations where labeled data is scarce. \\n2. It proposes a novel data augmentation method tailored specifically for tabular data, allowing contrastive learning to be more effective in these domains.\\n3. The introduction of a benchmark (FeSTa) comprising a wide range of datasets and algorithms provides a valuable resource for the research community, potentially fostering more robust evaluations and comparisons.' Weaknesses='1. The paper relies heavily on an augmentation technique that may not be broadly applicable beyond tabular data, thus limiting its generality to other forms of data input.\\n2. The novelty of the augmentation strategy, while beneficial for tabular domains, could be perceived as incremental given its basis on established contrastive learning frameworks.\\n3. There might be an over-reliance on benchmark performance rather than exploring the theoretical justifications and frameworks that underlie the proposed method.' Questions='1. How does the proposed augmentation maintain a balance between introducing variety within samples and preserving label consistency, especially when shuffling values?\\n2. The benchmark results demonstrate robustness in 1-shot learning scenarios, but how do these methods perform as the number of shots increases?\\n3. What are the implications for transfer learning scenarios, where tabular data might need adaptation from previously learned representations across different domains?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths=\"The paper proposes a novel framework for agent-oriented planning in multi-agent systems by identifying critical design principles of solvability, completeness, and non-redundancy. It effectively addresses the task decomposition and allocation process, providing a robust solution for real-world problems and is complemented by a feedback loop to enhance system performance. Comprehensive experiments validate the framework's advantage over existing systems.\" Weaknesses='The abstract lacks specific details on the implementation or evaluation metrics used in the experiments, which may make it difficult to assess the full impact of the proposed framework. Additionally, while the principles and framework are clearly stated, the adaptation of these principles in diverse multi-agent environments is not thoroughly discussed.' Questions='1. What specific metrics were used to show the advancement of the proposed framework over existing systems?\\n2. How does the feedback loop specifically contribute to enhancing the effectiveness and robustness of the multi-agent systems?\\n3. Can the framework be easily adapted for different types of multi-agent environments beyond those tested in the experiments?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths=\"The paper proposes a novel framework for agent-oriented planning in multi-agent systems by identifying critical design principles of solvability, completeness, and non-redundancy. It effectively addresses the task decomposition and allocation process, providing a robust solution for real-world problems and is complemented by a feedback loop to enhance system performance. Comprehensive experiments validate the framework's advantage over existing systems.\" Weaknesses='The abstract lacks specific details on the implementation or evaluation metrics used in the experiments, which may make it difficult to assess the full impact of the proposed framework. Additionally, while the principles and framework are clearly stated, the adaptation of these principles in diverse multi-agent environments is not thoroughly discussed.' Questions='1. What specific metrics were used to show the advancement of the proposed framework over existing systems?\\n2. How does the feedback loop specifically contribute to enhancing the effectiveness and robustness of the multi-agent systems?\\n3. Can the framework be easily adapted for different types of multi-agent environments beyond those tested in the experiments?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The TransFusion framework introduces a novel use of English translations to enhance IE tasks in low-resource languages. The approach shows significant improvements in cross-lingual transfer and achieves notable performance gains in NER tasks across multiple languages and model architectures.' Weaknesses='The reliance on English translations could be a limitation if high-quality translations are not available. The approach may also face challenges with ambiguous or contextually complex source texts. Further clarity on the underlying architecture and method specifics would strengthen understanding.' Questions='What are the potential limitations when applying TransFusion to languages with significantly different syntax from English? How dependent are the improvements on the quality of the translations used in the experiments? Are there plans to extend this framework to encompass more low-resource languages beyond those currently tested?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The TransFusion framework introduces a novel use of English translations to enhance IE tasks in low-resource languages. The approach shows significant improvements in cross-lingual transfer and achieves notable performance gains in NER tasks across multiple languages and model architectures.' Weaknesses='The reliance on English translations could be a limitation if high-quality translations are not available. The approach may also face challenges with ambiguous or contextually complex source texts. Further clarity on the underlying architecture and method specifics would strengthen understanding.' Questions='What are the potential limitations when applying TransFusion to languages with significantly different syntax from English? How dependent are the improvements on the quality of the translations used in the experiments? Are there plans to extend this framework to encompass more low-resource languages beyond those currently tested?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths=\"The paper presents a novel differentiable causal discovery algorithm that addresses scalability and nonlinearity in latent causal models. The theoretical results contribute significantly to the causality literature by relaxing strong assumptions about latent variables and noise. The algorithm's superior performance in both accuracy and scalability demonstrates its potential in practical applications, particularly with high-dimensional image data.\" Weaknesses=\"The paper may not provide sufficient details on the implementation and specific steps of the proposed algorithm. Additionally, the experiments might lack a diverse set of datasets to fully prove the algorithm's robustness across different domains.\" Questions='How does the proposed method deal with different types of noise that may be present in observational data? Are there any specific limitations in terms of the types of datasets the method can handle effectively?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths=\"The paper presents a novel differentiable causal discovery algorithm that addresses scalability and nonlinearity in latent causal models. The theoretical results contribute significantly to the causality literature by relaxing strong assumptions about latent variables and noise. The algorithm's superior performance in both accuracy and scalability demonstrates its potential in practical applications, particularly with high-dimensional image data.\" Weaknesses=\"The paper may not provide sufficient details on the implementation and specific steps of the proposed algorithm. Additionally, the experiments might lack a diverse set of datasets to fully prove the algorithm's robustness across different domains.\" Questions='How does the proposed method deal with different types of noise that may be present in observational data? Are there any specific limitations in terms of the types of datasets the method can handle effectively?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel approach by enhancing spatial perception and reasoning using a progressive spatial awareness scheme. This is achieved by enriching spatial embeddings and introducing new tasks such as 3D object distance measurement, which could inspire further advancements in 3D spatial perception capabilities.' Weaknesses='The paper lacks clarity in methodology and experimental setup, which could affect reproducibility. The contribution of the new MODEL dataset in achieving the results is not clearly delineated. There is also a lack of benchmarking against existing leading methods addressing similar problems, which raises questions about the claimed state-of-the-art performance.' Questions=\"1. How does the proposed progressive spatial awareness scheme quantitatively compare to other spatial embedding methods?\\n2. Can you provide benchmarks comparing Spatial 3D-LLM with other state-of-the-art methods on the same tasks?\\n3. How significant is the impact of the MODEL dataset on the model's performance compared to traditional datasets?\\n4. Could more details be provided on the implementation details or model architecture used in the Spatial 3D-LLM?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel approach by enhancing spatial perception and reasoning using a progressive spatial awareness scheme. This is achieved by enriching spatial embeddings and introducing new tasks such as 3D object distance measurement, which could inspire further advancements in 3D spatial perception capabilities.' Weaknesses='The paper lacks clarity in methodology and experimental setup, which could affect reproducibility. The contribution of the new MODEL dataset in achieving the results is not clearly delineated. There is also a lack of benchmarking against existing leading methods addressing similar problems, which raises questions about the claimed state-of-the-art performance.' Questions=\"1. How does the proposed progressive spatial awareness scheme quantitatively compare to other spatial embedding methods?\\n2. Can you provide benchmarks comparing Spatial 3D-LLM with other state-of-the-art methods on the same tasks?\\n3. How significant is the impact of the MODEL dataset on the model's performance compared to traditional datasets?\\n4. Could more details be provided on the implementation details or model architecture used in the Spatial 3D-LLM?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper identifies a gap in the explainability of graph representation learning and addresses this by introducing a novel framework that integrates graph pattern analysis. The method effectively bridges the gap between graph kernels and explainable representations, which can enhance comprehension of graph-based models significantly. The theoretical analysis provided for robustness and generalization strengthens the credibility of the proposed method. The experimental results showing the effectiveness of the approach in both supervised and unsupervised tasks further support its applicability.' Weaknesses=\"The approach may inherit the limitations of graph kernels, such as neglecting higher-level semantic features and potentially being computationally expensive for large graphs. The abstraction level for node features may not be thorough enough to capture all relevant information, which could affect the model's utility in certain contexts. Also, while theoretical analyses are beneficial, the presentation lacks detailed proofs or intuitions about some of the claims made, which could be critical for a deeper understanding of the framework's limitations.\" Questions='1. How does the framework scale with very large graphs in real-world scenarios? Are there any optimization strategies for efficiency?\\n2. What are the potential downsides regarding interpretability when using your approach compared to model-level or instance-level interpretability methods?\\n3. Can the method be adapted or expanded for heterogeneous graphs, where nodes and edges have different types or attributes?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper identifies a gap in the explainability of graph representation learning and addresses this by introducing a novel framework that integrates graph pattern analysis. The method effectively bridges the gap between graph kernels and explainable representations, which can enhance comprehension of graph-based models significantly. The theoretical analysis provided for robustness and generalization strengthens the credibility of the proposed method. The experimental results showing the effectiveness of the approach in both supervised and unsupervised tasks further support its applicability.' Weaknesses=\"The approach may inherit the limitations of graph kernels, such as neglecting higher-level semantic features and potentially being computationally expensive for large graphs. The abstraction level for node features may not be thorough enough to capture all relevant information, which could affect the model's utility in certain contexts. Also, while theoretical analyses are beneficial, the presentation lacks detailed proofs or intuitions about some of the claims made, which could be critical for a deeper understanding of the framework's limitations.\" Questions='1. How does the framework scale with very large graphs in real-world scenarios? Are there any optimization strategies for efficiency?\\n2. What are the potential downsides regarding interpretability when using your approach compared to model-level or instance-level interpretability methods?\\n3. Can the method be adapted or expanded for heterogeneous graphs, where nodes and edges have different types or attributes?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"1. The paper provides a comprehensive exploration and contribution towards egocentric video understanding by creating a large-scale QA dataset, which could significantly advance the field.\\n2. The proposed architecture with the novel 'Memory Pointer Prompting' mechanism offers a unique method to improve model comprehension over extensive video content, showcasing innovation in handling multimodal data.\\n3. The new benchmark facilitates a clear way to evaluate and improve multimodal models, potentially offering a standard that others in the field can build upon.\" Weaknesses=\"1. While the dataset and benchmark are significant, the paper may rely heavily on these contributions without thoroughly exploring potential limitations or challenges faced during their compilation and integration.\\n2. The evaluation method's de-biasing approach's details are not thoroughly explained, which could leave room for ambiguity regarding its effectiveness and implementation.\\n3. Clarity and discussion on how the proposed model performs across different types and lengths of videos, beyond aggregate metrics, would be beneficial.\" Questions='1. How does the model perform specifically on videos of varying lengths in terms of accuracy and processing time?\\n2. Can the dataset and methodology be easily adapted to other forms of video content, such as non-egocentric videos?\\n3. What specific steps were made to ensure high quality in the automatically generated QA samples?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"1. The paper provides a comprehensive exploration and contribution towards egocentric video understanding by creating a large-scale QA dataset, which could significantly advance the field.\\n2. The proposed architecture with the novel 'Memory Pointer Prompting' mechanism offers a unique method to improve model comprehension over extensive video content, showcasing innovation in handling multimodal data.\\n3. The new benchmark facilitates a clear way to evaluate and improve multimodal models, potentially offering a standard that others in the field can build upon.\" Weaknesses=\"1. While the dataset and benchmark are significant, the paper may rely heavily on these contributions without thoroughly exploring potential limitations or challenges faced during their compilation and integration.\\n2. The evaluation method's de-biasing approach's details are not thoroughly explained, which could leave room for ambiguity regarding its effectiveness and implementation.\\n3. Clarity and discussion on how the proposed model performs across different types and lengths of videos, beyond aggregate metrics, would be beneficial.\" Questions='1. How does the model perform specifically on videos of varying lengths in terms of accuracy and processing time?\\n2. Can the dataset and methodology be easily adapted to other forms of video content, such as non-egocentric videos?\\n3. What specific steps were made to ensure high quality in the automatically generated QA samples?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths='The paper provides a novel analysis of Differential Privacy (DP) in the context of non-convex and non-smooth loss functions, filling an important gap in the existing literature. The approach demonstrates a clear understanding of the challenges in extending DP guarantees beyond simple convex functions to more complex settings encountered in practical machine learning. The introduction of bounds based on H\\\\\"older gradient continuity is innovative and has the potential for high impact.' Weaknesses='While the theoretical development is strong, the paper may lack practical examples or experiments that clearly showcase the real-world utility of the newly derived privacy bounds. The mathematical rigor could also make the paper less accessible to those not specialized in privacy or optimization theory.' Questions='1. Are there specific machine learning applications or scenarios where the new DP bounds could be directly applied, and how do they compare empirically to traditional methods?\\n2. Can the authors provide intuition or visualization for the implications of using Holder gradient continuity in DP settings?\\n3. How might the techniques introduced in this paper influence future research in Differential Privacy, specifically for training machine learning models on sensitive data?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths='The paper provides a novel analysis of Differential Privacy (DP) in the context of non-convex and non-smooth loss functions, filling an important gap in the existing literature. The approach demonstrates a clear understanding of the challenges in extending DP guarantees beyond simple convex functions to more complex settings encountered in practical machine learning. The introduction of bounds based on H\\\\\"older gradient continuity is innovative and has the potential for high impact.' Weaknesses='While the theoretical development is strong, the paper may lack practical examples or experiments that clearly showcase the real-world utility of the newly derived privacy bounds. The mathematical rigor could also make the paper less accessible to those not specialized in privacy or optimization theory.' Questions='1. Are there specific machine learning applications or scenarios where the new DP bounds could be directly applied, and how do they compare empirically to traditional methods?\\n2. Can the authors provide intuition or visualization for the implications of using Holder gradient continuity in DP settings?\\n3. How might the techniques introduced in this paper influence future research in Differential Privacy, specifically for training machine learning models on sensitive data?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper introduces a novel analytical framework, Neuron Predictability Lens (NPL), to understand neuron behavior in transformer-based LLMs.\\n2. Provides both global and local analyses about neuron predictability, contributing to insights on model behavior and function.\\n3. Empirical results on well-known models, LLaMA-2 and GPT-J, could offer practical applicability and validation of the proposed framework.' Weaknesses='1. Clarity issues in presentation, especially regarding key technical details and explanation of novel concepts like Neuron Predictability.\\n2. The results might not be compelling enough to strongly establish the necessity or advantage of NPL.\\n3. Some explanations around the methodology, especially in local analysis and neuron interpretation, could be deepened to better highlight the significance of findings.' Questions='1. How do you ensure the robustness and generalizability of your neuron predictability findings across different transformer model architectures?\\n2. Could NPL be applied to other neural network frameworks beyond transformers, or is it highly specialized for this specific architecture?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper introduces a novel analytical framework, Neuron Predictability Lens (NPL), to understand neuron behavior in transformer-based LLMs.\\n2. Provides both global and local analyses about neuron predictability, contributing to insights on model behavior and function.\\n3. Empirical results on well-known models, LLaMA-2 and GPT-J, could offer practical applicability and validation of the proposed framework.' Weaknesses='1. Clarity issues in presentation, especially regarding key technical details and explanation of novel concepts like Neuron Predictability.\\n2. The results might not be compelling enough to strongly establish the necessity or advantage of NPL.\\n3. Some explanations around the methodology, especially in local analysis and neuron interpretation, could be deepened to better highlight the significance of findings.' Questions='1. How do you ensure the robustness and generalizability of your neuron predictability findings across different transformer model architectures?\\n2. Could NPL be applied to other neural network frameworks beyond transformers, or is it highly specialized for this specific architecture?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel approach for zero-shot video restoration utilizing pre-trained image restoration diffusion models. The method displays strong quantitative results and visual comparisons across various challenging datasets, demonstrating superior generalization across different degradations compared to traditional methods. Additionally, it offers versatility by being compatible with any 2D restoration diffusion model.' Weaknesses='The proposed method may lack theoretical rigor in the mathematical framework underpinning the hierarchical latent warping strategy and token merging process. The paper may also benefit from more detailed experimental analyses or a broader range of datasets to further substantiate the claims of generalization and robustness.' Questions='How does the hierarchical latent warping strategy specifically contribute to zero-shot video restoration performance? What are the computational overheads associated with the proposed approach compared to traditional methods? Is there any limitation to the types of video degradations the method can handle effectively?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces a novel approach for zero-shot video restoration utilizing pre-trained image restoration diffusion models. The method displays strong quantitative results and visual comparisons across various challenging datasets, demonstrating superior generalization across different degradations compared to traditional methods. Additionally, it offers versatility by being compatible with any 2D restoration diffusion model.' Weaknesses='The proposed method may lack theoretical rigor in the mathematical framework underpinning the hierarchical latent warping strategy and token merging process. The paper may also benefit from more detailed experimental analyses or a broader range of datasets to further substantiate the claims of generalization and robustness.' Questions='How does the hierarchical latent warping strategy specifically contribute to zero-shot video restoration performance? What are the computational overheads associated with the proposed approach compared to traditional methods? Is there any limitation to the types of video degradations the method can handle effectively?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses an important and timely problem of privacy in graph prompt learning. Two novel algorithms, DP-GPL and DP-GPL+W, are proposed for differentially private graph prompt learning using the PATE framework, showcasing innovative solutions to privacy leakage issues. The evaluation indicates that these methods maintain high utility while addressing privacy concerns effectively.' Weaknesses=\"The paper's soundness is hindered by insufficient experimental validation. The lack of demonstrations showing how effectively DP-GPL and DP-GPL+W can resist membership inference attacks after implementation raises concerns. Furthermore, the theoretical section lacks clarity on whether the proposed methods specifically target node-level or edge-level differential privacy, potentially leading to complications in practical applications.\" Questions='1. Can the authors provide experimental results for membership inference attacks on models protected by DP-GPL and DP-GPL+W in comparison to unprotected models?\\n2. How does the centrality-based teacher voting in DP-GPL+W contribute to its improved performance over DP-GPL? Is there any experimental or theoretical evidence demonstrating this advantage?\\n3. Is the proposed method applicable to heterogeneous graphs, or is it limited to homogeneous graphs?\\n4. Why do the experiments focus only on the k-shot scenario with k=5? Could the authors provide insights into performance across varying k-shot scenarios, especially with smaller k-values?\\n5. How do the proposed methods ensure label privacy, given the lack of explicit mention in the theoretical section?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses an important and timely problem of privacy in graph prompt learning. Two novel algorithms, DP-GPL and DP-GPL+W, are proposed for differentially private graph prompt learning using the PATE framework, showcasing innovative solutions to privacy leakage issues. The evaluation indicates that these methods maintain high utility while addressing privacy concerns effectively.' Weaknesses=\"The paper's soundness is hindered by insufficient experimental validation. The lack of demonstrations showing how effectively DP-GPL and DP-GPL+W can resist membership inference attacks after implementation raises concerns. Furthermore, the theoretical section lacks clarity on whether the proposed methods specifically target node-level or edge-level differential privacy, potentially leading to complications in practical applications.\" Questions='1. Can the authors provide experimental results for membership inference attacks on models protected by DP-GPL and DP-GPL+W in comparison to unprotected models?\\n2. How does the centrality-based teacher voting in DP-GPL+W contribute to its improved performance over DP-GPL? Is there any experimental or theoretical evidence demonstrating this advantage?\\n3. Is the proposed method applicable to heterogeneous graphs, or is it limited to homogeneous graphs?\\n4. Why do the experiments focus only on the k-shot scenario with k=5? Could the authors provide insights into performance across varying k-shot scenarios, especially with smaller k-values?\\n5. How do the proposed methods ensure label privacy, given the lack of explicit mention in the theoretical section?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper proposes a novel Sparse Matrix Tuning (SMT) method that effectively reduces computational and memory costs while narrowing the performance gap between parameter-efficient fine-tuning (PEFT) and full fine-tuning (FT).\\n2. SMT demonstrates significant improvements over existing PEFT methods like LoRA and DoRA while maintaining lower GPU memory usage.\\n3. The approach is validated across a broad spectrum of tasks and is evaluated on popular large language models such as LLaMA.' Weaknesses=\"1. The method's dependency on identifying the 'most significant' sub-matrices may limit its application to other model architectures or data distributions.\\n2. The paper lacks comparisons with other recent advancements in delta-tuning or related pruning approaches.\\n3. Some practical concerns such as the choice of block size and generalization of the mask selection process are not thoroughly discussed.\" Questions='1. What criteria are used to determine the significance of sub-matrices in gradient updates, and how consistent are these criteria across different datasets and model architectures?\\n2. Is the method applicable to models beyond those evaluated, and how does it handle variations in model size and layer configurations?\\n3. How does the SMT method perform against other state-of-the-art techniques in delta tuning or pruning, and why were these not included in the evaluation?\\n4. Can the approach be extended to domains beyond large language models, such as vision or multi-modal tasks, while retaining its benefits?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper proposes a novel Sparse Matrix Tuning (SMT) method that effectively reduces computational and memory costs while narrowing the performance gap between parameter-efficient fine-tuning (PEFT) and full fine-tuning (FT).\\n2. SMT demonstrates significant improvements over existing PEFT methods like LoRA and DoRA while maintaining lower GPU memory usage.\\n3. The approach is validated across a broad spectrum of tasks and is evaluated on popular large language models such as LLaMA.' Weaknesses=\"1. The method's dependency on identifying the 'most significant' sub-matrices may limit its application to other model architectures or data distributions.\\n2. The paper lacks comparisons with other recent advancements in delta-tuning or related pruning approaches.\\n3. Some practical concerns such as the choice of block size and generalization of the mask selection process are not thoroughly discussed.\" Questions='1. What criteria are used to determine the significance of sub-matrices in gradient updates, and how consistent are these criteria across different datasets and model architectures?\\n2. Is the method applicable to models beyond those evaluated, and how does it handle variations in model size and layer configurations?\\n3. How does the SMT method perform against other state-of-the-art techniques in delta tuning or pruning, and why were these not included in the evaluation?\\n4. Can the approach be extended to domains beyond large language models, such as vision or multi-modal tasks, while retaining its benefits?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='This work presents a novel architecture for multimodal medical image segmentation called CrossModalNet, offering rigorous mathematical analysis and introduction of the CMIF metric. The paper emphasizes a robust theoretical foundation and applicability to joint adversarial domain adaptation techniques, showcasing potential improvements in segmentation performance.' Weaknesses='While the paper introduces interesting concepts, the presentation of mathematical details and proofs needs to coincide with experimental validation across more datasets for generalizability. A detailed comparison with existing architectures using comprehensive state-of-the-art models, besides the MM-WHS dataset, may further solidify the claimed advancements.' Questions='How well does CrossModalNet perform on other datasets or medical imaging modalities? What specific advantages does the CMIF metric offer compared to existing fusion metrics?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='This work presents a novel architecture for multimodal medical image segmentation called CrossModalNet, offering rigorous mathematical analysis and introduction of the CMIF metric. The paper emphasizes a robust theoretical foundation and applicability to joint adversarial domain adaptation techniques, showcasing potential improvements in segmentation performance.' Weaknesses='While the paper introduces interesting concepts, the presentation of mathematical details and proofs needs to coincide with experimental validation across more datasets for generalizability. A detailed comparison with existing architectures using comprehensive state-of-the-art models, besides the MM-WHS dataset, may further solidify the claimed advancements.' Questions='How well does CrossModalNet perform on other datasets or medical imaging modalities? What specific advantages does the CMIF metric offer compared to existing fusion metrics?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper presents a novel approach to multi-view clustering by leveraging a heterogeneous graph and random walks, which is a promising direction. The use of a multi-step random walk strategy to capture high-order structures is innovative and could potentially improve clustering performance. The experimental section is comprehensive, involving five real-world datasets, indicating the method's applicability and robustness.\" Weaknesses=\"The paper lacks detail on the computational complexity of the proposed method, which is crucial for understanding its practical applicability. While the concept of using heterogeneous graphs is interesting, there is limited discussion on why this approach is superior to other existing methods. Additionally, the theoretical foundation or proof of the algorithm's convergence and performance is missing, which might affect the reliability of the results.\" Questions='What is the computational complexity of the proposed method, and how does it compare to existing multi-view clustering methods? Is there any theoretical guarantee or convergence proof for the algorithm? How does the method handle scalability with large datasets and numerous views?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper presents a novel approach to multi-view clustering by leveraging a heterogeneous graph and random walks, which is a promising direction. The use of a multi-step random walk strategy to capture high-order structures is innovative and could potentially improve clustering performance. The experimental section is comprehensive, involving five real-world datasets, indicating the method's applicability and robustness.\" Weaknesses=\"The paper lacks detail on the computational complexity of the proposed method, which is crucial for understanding its practical applicability. While the concept of using heterogeneous graphs is interesting, there is limited discussion on why this approach is superior to other existing methods. Additionally, the theoretical foundation or proof of the algorithm's convergence and performance is missing, which might affect the reliability of the results.\" Questions='What is the computational complexity of the proposed method, and how does it compare to existing multi-view clustering methods? Is there any theoretical guarantee or convergence proof for the algorithm? How does the method handle scalability with large datasets and numerous views?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper extends the theoretical understanding of the hybrid kernel variant of Stein Variational Gradient Descent (h-SVGD), providing insights into its ability to alleviate variance collapse without additional computational cost or target density factorization. The theoretical advancements, including the existence proof for the hybrid Stein PDE and the characterisation of a special case as a kernelised Wasserstein gradient flow, significantly contribute to the field.' Weaknesses='The presentation of the theoretical results might be challenging for practitioners who are less familiar with the advanced mathematical framework employed. Additionally, some assumptions and results might lack intuitive explanations or broader implications for how they influence practical applications.' Questions='1. The paper mentions a descent lemma and a large particle limit result. Can you provide more intuition or implications of these results for practical implementations?\\n2. Has there been a consideration of how the bias in the mean field limiting distribution might be addressed or mitigated in practical applications?\\n3. Could additional empirical comparisons with a broader range of SVGD variants strengthen the findings? Are there plans for such comparisons in the future work?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper extends the theoretical understanding of the hybrid kernel variant of Stein Variational Gradient Descent (h-SVGD), providing insights into its ability to alleviate variance collapse without additional computational cost or target density factorization. The theoretical advancements, including the existence proof for the hybrid Stein PDE and the characterisation of a special case as a kernelised Wasserstein gradient flow, significantly contribute to the field.' Weaknesses='The presentation of the theoretical results might be challenging for practitioners who are less familiar with the advanced mathematical framework employed. Additionally, some assumptions and results might lack intuitive explanations or broader implications for how they influence practical applications.' Questions='1. The paper mentions a descent lemma and a large particle limit result. Can you provide more intuition or implications of these results for practical implementations?\\n2. Has there been a consideration of how the bias in the mean field limiting distribution might be addressed or mitigated in practical applications?\\n3. Could additional empirical comparisons with a broader range of SVGD variants strengthen the findings? Are there plans for such comparisons in the future work?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths='The introduction of Sinusoidal Trainable Activation Functions (STAF) addresses a critical limitation in INRs related to spectral bias, enabling superior performance in modeling high-frequency details. The authors convincingly demonstrate how STAF surpasses existing methods like KAN, WIRE, and SIREN in both accuracy and training efficiency, making a strong case for its significance in improving the quality of continuous signal reconstruction. Comprehensive evaluations further underscore its robust performance.' Weaknesses='While the paper provides solid theoretical and empirical backing, a potential limitation might be the increased complexity in implementation compared to simpler activation functions. The extent of computational overhead should also be carefully considered, especially in resource-constrained environments.' Questions='How significant is the computational cost increase when using STAF compared to traditional ReLU or even fixed sine activation functions? Can STAF be easily integrated into existing INRs without substantial modification?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths='The introduction of Sinusoidal Trainable Activation Functions (STAF) addresses a critical limitation in INRs related to spectral bias, enabling superior performance in modeling high-frequency details. The authors convincingly demonstrate how STAF surpasses existing methods like KAN, WIRE, and SIREN in both accuracy and training efficiency, making a strong case for its significance in improving the quality of continuous signal reconstruction. Comprehensive evaluations further underscore its robust performance.' Weaknesses='While the paper provides solid theoretical and empirical backing, a potential limitation might be the increased complexity in implementation compared to simpler activation functions. The extent of computational overhead should also be carefully considered, especially in resource-constrained environments.' Questions='How significant is the computational cost increase when using STAF compared to traditional ReLU or even fixed sine activation functions? Can STAF be easily integrated into existing INRs without substantial modification?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses a critical area in the field of LLMs, focusing on enhancing their ability to follow user instructions more accurately, which is a valuable contribution. The introduction of the new metric, Influence, to measure how instructions propagate through the transformer layers can provide useful insights. The proposed GUIDE method shows a significant improvement in instruction adherence, as demonstrated by quantitative results.' Weaknesses='The paper lacks a comprehensive comparison with existing methods that address similar problems, which would help contextualize its novelty and effectiveness. The absence of detailed discussions on potential trade-offs, such as effects on output fluency or coherence when altering attention mechanisms, is a concern. Additionally, the paper could benefit from more thorough experimental validation across diverse datasets and instruction types.' Questions='How does GUIDE perform in comparison to state-of-the-art methods focused on instruction alignment in LLMs? Can the proposed method be generalized across different LLM architectures without specific adaptations?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper addresses a critical area in the field of LLMs, focusing on enhancing their ability to follow user instructions more accurately, which is a valuable contribution. The introduction of the new metric, Influence, to measure how instructions propagate through the transformer layers can provide useful insights. The proposed GUIDE method shows a significant improvement in instruction adherence, as demonstrated by quantitative results.' Weaknesses='The paper lacks a comprehensive comparison with existing methods that address similar problems, which would help contextualize its novelty and effectiveness. The absence of detailed discussions on potential trade-offs, such as effects on output fluency or coherence when altering attention mechanisms, is a concern. Additionally, the paper could benefit from more thorough experimental validation across diverse datasets and instruction types.' Questions='How does GUIDE perform in comparison to state-of-the-art methods focused on instruction alignment in LLMs? Can the proposed method be generalized across different LLM architectures without specific adaptations?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The methodology leverages the known connections between GFlowNets and maximum-entropy reinforcement learning for adapting MCTS, providing a new avenue for GFlowNet training. The theoretical framework is well-grounded, with proofs provided for the MCTS tree construction modification. The approach addresses practical strategies for applying MCTS within GFNs, demonstrated through comprehensive experiments across different parameterizations and objectives. The study shows improvement in training performance on discrete domains, indicating the practical relevance and applicability of the proposed technique.' Weaknesses='The presentation might benefit from clearer and more detailed explanations in certain sections, particularly for readers less familiar with certain background concepts. The experimental results, while extensive, could be strengthened by more detailed analysis and interpretation. Some aspects of how MCTS specifically enhances the sampling process compared to existing methods might need further elaboration to strengthen the case for its adoption. Additionally, insights into limitations and potential challenges of the proposed approach would strengthen the discussion.' Questions='1. Can you elaborate on any potential limitations or challenges when using MCTS for GFlowNet training in specific domains or under certain conditions?\\n2. How does the proposed adaptation of MCTS handle higher-dimensional or more complex environments, and what modifications, if any, might be needed to ensure scalability?\\n3. Could the methodology be applied effectively in continuous spaces, or is it uniquely suited for discrete spaces?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The methodology leverages the known connections between GFlowNets and maximum-entropy reinforcement learning for adapting MCTS, providing a new avenue for GFlowNet training. The theoretical framework is well-grounded, with proofs provided for the MCTS tree construction modification. The approach addresses practical strategies for applying MCTS within GFNs, demonstrated through comprehensive experiments across different parameterizations and objectives. The study shows improvement in training performance on discrete domains, indicating the practical relevance and applicability of the proposed technique.' Weaknesses='The presentation might benefit from clearer and more detailed explanations in certain sections, particularly for readers less familiar with certain background concepts. The experimental results, while extensive, could be strengthened by more detailed analysis and interpretation. Some aspects of how MCTS specifically enhances the sampling process compared to existing methods might need further elaboration to strengthen the case for its adoption. Additionally, insights into limitations and potential challenges of the proposed approach would strengthen the discussion.' Questions='1. Can you elaborate on any potential limitations or challenges when using MCTS for GFlowNet training in specific domains or under certain conditions?\\n2. How does the proposed adaptation of MCTS handle higher-dimensional or more complex environments, and what modifications, if any, might be needed to ensure scalability?\\n3. Could the methodology be applied effectively in continuous spaces, or is it uniquely suited for discrete spaces?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths='The paper presents a novel localized masked modeling paradigm which could address the challenge of overfitting when working with limited datasets in the animation domain. The introduction of a sliding window local attention in a masked transformer and the use of a distribution regularization method for quantization are appreciated contributions. The results promise advances in animation generation from a single motion reference, outperforming existing GAN and Diffusion-based methods.' Weaknesses='The work relies on existing concepts in the field, such as generative masked modeling and quantization, which may limit the overall originality. While the results are promising, the practical differences or improvements over existing methods like GANs or diffusion models, beyond the context of single reference motion, are not entirely clear from the abstract. Additionally, the extent of experiments and benchmarks against a broader range of state-of-the-art methods would enhance the validity of the claims. The abstract does not provide enough insight into the methodology for dealing with varying durations and topologies of input motion, which might affect general applicability.' Questions='1. How does MotionDreamer handle varying topologies and durations beyond what is presented in the single reference example? Are there inherent limitations in these contexts?\\n2. Can MotionDreamer be adapted or scaled for motion synthesis in different media or applications beyond what is mentioned?\\n3. Are there specific aspects where MotionDreamer does not perform as well compared to GAN or diffusion models? If so, what are the identified limitations?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths='The paper presents a novel localized masked modeling paradigm which could address the challenge of overfitting when working with limited datasets in the animation domain. The introduction of a sliding window local attention in a masked transformer and the use of a distribution regularization method for quantization are appreciated contributions. The results promise advances in animation generation from a single motion reference, outperforming existing GAN and Diffusion-based methods.' Weaknesses='The work relies on existing concepts in the field, such as generative masked modeling and quantization, which may limit the overall originality. While the results are promising, the practical differences or improvements over existing methods like GANs or diffusion models, beyond the context of single reference motion, are not entirely clear from the abstract. Additionally, the extent of experiments and benchmarks against a broader range of state-of-the-art methods would enhance the validity of the claims. The abstract does not provide enough insight into the methodology for dealing with varying durations and topologies of input motion, which might affect general applicability.' Questions='1. How does MotionDreamer handle varying topologies and durations beyond what is presented in the single reference example? Are there inherent limitations in these contexts?\\n2. Can MotionDreamer be adapted or scaled for motion synthesis in different media or applications beyond what is mentioned?\\n3. Are there specific aspects where MotionDreamer does not perform as well compared to GAN or diffusion models? If so, what are the identified limitations?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='The approach is novel as it highlights an underexplored aspect of long-context large language models: the intrinsic limitation posed by training data. By focusing on expanding dataset availability to include longer outputs, the paper provides an incremental but effective way of improving LLM capabilities. Additionally, AgentWrite offers a practical solution to break down complex generation tasks into manageable subtasks, which is a valuable contribution to make existing LLMs more versatile.' Weaknesses='There could be more extensive experimentation to convincingly demonstrate the effectiveness of AgentWrite across different tasks and domains. The evaluation mainly focuses on the performance within new benchmarks constructed for this specific purpose, which may not fully capture real-world applicability. Furthermore, the paper could benefit from more in-depth discussions on the potential trade-offs involved in substantially increasing output lengths, such as computational cost or potential hallucinations in outputs.' Questions=\"How does AgentWrite handle coherence and consistency between subtasks, especially when attempting to ensure that the overall generated text maintains a seamless narrative or argument? What measures are in place to ensure that increasing output length does not lead to an increased likelihood of hallucinations or factual inaccuracies in the model's output?\"\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='The approach is novel as it highlights an underexplored aspect of long-context large language models: the intrinsic limitation posed by training data. By focusing on expanding dataset availability to include longer outputs, the paper provides an incremental but effective way of improving LLM capabilities. Additionally, AgentWrite offers a practical solution to break down complex generation tasks into manageable subtasks, which is a valuable contribution to make existing LLMs more versatile.' Weaknesses='There could be more extensive experimentation to convincingly demonstrate the effectiveness of AgentWrite across different tasks and domains. The evaluation mainly focuses on the performance within new benchmarks constructed for this specific purpose, which may not fully capture real-world applicability. Furthermore, the paper could benefit from more in-depth discussions on the potential trade-offs involved in substantially increasing output lengths, such as computational cost or potential hallucinations in outputs.' Questions=\"How does AgentWrite handle coherence and consistency between subtasks, especially when attempting to ensure that the overall generated text maintains a seamless narrative or argument? What measures are in place to ensure that increasing output length does not lead to an increased likelihood of hallucinations or factual inaccuracies in the model's output?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The introduction of a dynamic Ada-K routing strategy for MoE-based LLMs is well-motivated and represents a significant innovation over static Top-K routing. The paper demonstrates clear advances in computational efficiency and model performance, supported by robust empirical evaluations. The approach is versatile, applicable across a range of MoE-based LLMs, and shows impressive metrics in terms of reduction in FLOPs and inference speedup. The inclusion of policy optimization for end-to-end learning of the routing decision adds a layer of sophistication absent in previous models.' Weaknesses='While the empirical results are strong, the paper could further explore the theoretical implications and limitations of the Ada-K strategy. The adaptability of the routing strategy under varying conditions and its potential impact on different types of contextual tasks requires more extensive exploration. Additionally, while the allocator modules are described as lightweight, there is no detailed analysis on the overhead introduced by these modules themselves compared to static Top-K routing.' Questions='How does the Ada-K routing strategy adapt when dealing with tokens that carry ambiguous contextual significance and how does it ensure that critical tokens are accurately identified without bias? Furthermore, are there specific types of tasks or datasets where the Ada-K strategy does not perform as expected, and how might this be addressed in future research? Finally, given the significant improvements shown by Ada-K, what are the implications of this routing strategy for real-time applications where quick decision-making is crucial?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The introduction of a dynamic Ada-K routing strategy for MoE-based LLMs is well-motivated and represents a significant innovation over static Top-K routing. The paper demonstrates clear advances in computational efficiency and model performance, supported by robust empirical evaluations. The approach is versatile, applicable across a range of MoE-based LLMs, and shows impressive metrics in terms of reduction in FLOPs and inference speedup. The inclusion of policy optimization for end-to-end learning of the routing decision adds a layer of sophistication absent in previous models.' Weaknesses='While the empirical results are strong, the paper could further explore the theoretical implications and limitations of the Ada-K strategy. The adaptability of the routing strategy under varying conditions and its potential impact on different types of contextual tasks requires more extensive exploration. Additionally, while the allocator modules are described as lightweight, there is no detailed analysis on the overhead introduced by these modules themselves compared to static Top-K routing.' Questions='How does the Ada-K routing strategy adapt when dealing with tokens that carry ambiguous contextual significance and how does it ensure that critical tokens are accurately identified without bias? Furthermore, are there specific types of tasks or datasets where the Ada-K strategy does not perform as expected, and how might this be addressed in future research? Finally, given the significant improvements shown by Ada-K, what are the implications of this routing strategy for real-time applications where quick decision-making is crucial?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces an innovative algorithm, LoCoDL, which combines local training and compression to efficiently address communication challenges in distributed and federated learning settings. This approach not only theoretically ensures reduced communication complexity but also outperforms existing methods in practical experiments. The use of a large class of unbiased compressors adds to its robustness across different applications.' Weaknesses='The paper primarily focuses on strongly convex functions, which might limit the generalizability to non-convex settings often encountered in practice. Additionally, while the algorithm outperforms existing methods, further empirical validation across diverse datasets and conditions would strengthen the claims.' Questions='1. How does the performance of LoCoDL compare to other algorithms in non-convex settings?\\n2. Are there any specific limitations or scenarios where LoCoDL might not perform as expected?\\n3. How does the choice of unbiased compressors affect the overall performance, and are there recommendations for selecting them?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces an innovative algorithm, LoCoDL, which combines local training and compression to efficiently address communication challenges in distributed and federated learning settings. This approach not only theoretically ensures reduced communication complexity but also outperforms existing methods in practical experiments. The use of a large class of unbiased compressors adds to its robustness across different applications.' Weaknesses='The paper primarily focuses on strongly convex functions, which might limit the generalizability to non-convex settings often encountered in practice. Additionally, while the algorithm outperforms existing methods, further empirical validation across diverse datasets and conditions would strengthen the claims.' Questions='1. How does the performance of LoCoDL compare to other algorithms in non-convex settings?\\n2. Are there any specific limitations or scenarios where LoCoDL might not perform as expected?\\n3. How does the choice of unbiased compressors affect the overall performance, and are there recommendations for selecting them?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper highlights interesting insights into how minimal architectural changes can significantly affect transformer performance, particularly in simple tasks. It systematically dissects the intricacies of transformer design choices and their effects on the solutions that emerge.' Weaknesses='While insightful, the focus on a simple synthetic task like histogram counting may limit the perceived applicability of the findings to more complex, real-world scenarios. The practical implications or applications of these insights are not thoroughly discussed.' Questions='Do you intend to extend this study to more complex tasks or real-world scenarios to observe if the same architectural sensitivities persist? What additional insights can be drawn about token-mixing and capacity when considering multi-layer transformers?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper highlights interesting insights into how minimal architectural changes can significantly affect transformer performance, particularly in simple tasks. It systematically dissects the intricacies of transformer design choices and their effects on the solutions that emerge.' Weaknesses='While insightful, the focus on a simple synthetic task like histogram counting may limit the perceived applicability of the findings to more complex, real-world scenarios. The practical implications or applications of these insights are not thoroughly discussed.' Questions='Do you intend to extend this study to more complex tasks or real-world scenarios to observe if the same architectural sensitivities persist? What additional insights can be drawn about token-mixing and capacity when considering multi-layer transformers?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths='1. The paper presents a novel dataset, MedTrinity-25M, which significantly enhances the scope and scale for medical imaging tasks by providing multigranular annotations without the need for text-image pairs.' Weaknesses='1. The automated pipeline for creating the dataset might not capture the nuances of expert human annotation, which could affect the quality of the dataset.\\n2. There is a potential risk of bias introduced by the reliance on pre-existing domain-specific models for region of interest identification.' Questions='1. How does the automated annotation process compare in quality and accuracy to manual expert annotation? Is there validation performed?\\n2. Are there any measures in place to address potential biases introduced by the domain-specific models used in the pipeline?\\n3. What are the plans for updating and maintaining the alignment of this dataset with evolving medical standards and new imaging modalities?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths='1. The paper presents a novel dataset, MedTrinity-25M, which significantly enhances the scope and scale for medical imaging tasks by providing multigranular annotations without the need for text-image pairs.' Weaknesses='1. The automated pipeline for creating the dataset might not capture the nuances of expert human annotation, which could affect the quality of the dataset.\\n2. There is a potential risk of bias introduced by the reliance on pre-existing domain-specific models for region of interest identification.' Questions='1. How does the automated annotation process compare in quality and accuracy to manual expert annotation? Is there validation performed?\\n2. Are there any measures in place to address potential biases introduced by the domain-specific models used in the pipeline?\\n3. What are the plans for updating and maintaining the alignment of this dataset with evolving medical standards and new imaging modalities?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper introduces a novel method called Rotated Runtime Smooth (RRS) for activation smoothing in large language models, which effectively addresses the challenge of INT4 quantization by reducing outliers in activations. The proposed solution demonstrates a significant improvement in WikiText-2 perplexity, outperforming existing state-of-the-art methods.' Weaknesses='The paper could provide more comprehensive details on the computational overhead and practical implications of integrating the Rotation operation and Runtime Smooth into existing systems. Additionally, the runtime performance improvements over existing methods should be explicitly demonstrated and quantified.' Questions='How does the proposed RRS method impact the computational cost and latency compared to existing methods? Can the approach be generalized to other tasks or models beyond the ones evaluated in the paper?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper introduces a novel method called Rotated Runtime Smooth (RRS) for activation smoothing in large language models, which effectively addresses the challenge of INT4 quantization by reducing outliers in activations. The proposed solution demonstrates a significant improvement in WikiText-2 perplexity, outperforming existing state-of-the-art methods.' Weaknesses='The paper could provide more comprehensive details on the computational overhead and practical implications of integrating the Rotation operation and Runtime Smooth into existing systems. Additionally, the runtime performance improvements over existing methods should be explicitly demonstrated and quantified.' Questions='How does the proposed RRS method impact the computational cost and latency compared to existing methods? Can the approach be generalized to other tasks or models beyond the ones evaluated in the paper?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses a significant challenge in medical imaging by proposing a novel task and methodology for one-shot video object segmentation using static image datasets.\\n2. The inclusion of a comprehensive dataset (OS-I2V-Seg) to advance research in medical video segmentation is commendable.\\n3. The use of a memory mechanism and self-distillation for test-time training to adapt the model from images to video frames reflects an innovative approach.' Weaknesses='1. The paper could benefit from more detailed explanations on the construction and effectiveness of the test-time training strategy employed, as well as more insights on the memory mechanism used.\\n2. There is a lack of clarity on how well the proposed method can generalize to different medical imaging modalities or be adapted to non-medical videos.\\n3. The presentation could be improved by providing more detailed descriptions of the experimental setup and discussing the limitations of the approach more comprehensively.' Questions='1. Could the authors provide more clarity on the implementation aspects of the memory mechanism and how it scales with different video lengths or complexities?\\n2. Is the proposed OS-I2V-Seg dataset specific to certain types of medical videos, and how diverse is it in representing different medical conditions or imaging techniques?\\n3. How does the performance of this approach compare to traditional fully-annotated video segmentation methods in terms of accuracy and computational efficiency?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper addresses a significant challenge in medical imaging by proposing a novel task and methodology for one-shot video object segmentation using static image datasets.\\n2. The inclusion of a comprehensive dataset (OS-I2V-Seg) to advance research in medical video segmentation is commendable.\\n3. The use of a memory mechanism and self-distillation for test-time training to adapt the model from images to video frames reflects an innovative approach.' Weaknesses='1. The paper could benefit from more detailed explanations on the construction and effectiveness of the test-time training strategy employed, as well as more insights on the memory mechanism used.\\n2. There is a lack of clarity on how well the proposed method can generalize to different medical imaging modalities or be adapted to non-medical videos.\\n3. The presentation could be improved by providing more detailed descriptions of the experimental setup and discussing the limitations of the approach more comprehensively.' Questions='1. Could the authors provide more clarity on the implementation aspects of the memory mechanism and how it scales with different video lengths or complexities?\\n2. Is the proposed OS-I2V-Seg dataset specific to certain types of medical videos, and how diverse is it in representing different medical conditions or imaging techniques?\\n3. How does the performance of this approach compare to traditional fully-annotated video segmentation methods in terms of accuracy and computational efficiency?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel perspective to evaluate diversity in LLM-generated datasets by treating it as a classification task. This approach potentially provides a more structured framework and makes diversity evaluation more computationally efficient.' Weaknesses='The practical significance and potential real-world applications of the proposed diversity evaluation method are not clearly articulated. Additionally, it is unclear how the proposed method compares in performance and accuracy to existing methods. The correlation with diversity pseudo-truths mentioned lacks clarity on how it is measured and its impact on the outcomes.' Questions='What specific applications could benefit from assessing the diversity of LLM-generated datasets with the proposed method? How does DCScore compare with more traditional diversity evaluation methods in terms of precision and scalability? What is the underlying logic behind the correlation with diversity pseudo-truths, and how is it determined?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel perspective to evaluate diversity in LLM-generated datasets by treating it as a classification task. This approach potentially provides a more structured framework and makes diversity evaluation more computationally efficient.' Weaknesses='The practical significance and potential real-world applications of the proposed diversity evaluation method are not clearly articulated. Additionally, it is unclear how the proposed method compares in performance and accuracy to existing methods. The correlation with diversity pseudo-truths mentioned lacks clarity on how it is measured and its impact on the outcomes.' Questions='What specific applications could benefit from assessing the diversity of LLM-generated datasets with the proposed method? How does DCScore compare with more traditional diversity evaluation methods in terms of precision and scalability? What is the underlying logic behind the correlation with diversity pseudo-truths, and how is it determined?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses a pertinent issue in model pruning literature regarding the instability caused by weight perturbations, especially pertinent for large language models. The MoreauPruner algorithm offers a novel approach that leverages the Moreau envelope, providing robustness against such perturbations. The experimental results on several well-known LLMs, including extensive evaluation, add to the strengths of this work.' Weaknesses='The paper lacks clarity on the practical implications of pruning instability due to weight perturbations, leaving the reader questioning the real-world impact of the observed differences. Additionally, the performance improvements demonstrated by MoreauPruner, though present, are not dramatically better than existing methods, notably in certain contexts. Another limitation is the slightly restricted choice of target models, which primarily centers on the LLaMA family. The paper would benefit from improved descriptions regarding computational costs and resource demands of the proposed method.' Questions='How critical is the difference in pruning results to the overall performance of the models in real-world applications? Could additional sources of weight perturbation be considered in future studies to further generalize the findings presented here?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses a pertinent issue in model pruning literature regarding the instability caused by weight perturbations, especially pertinent for large language models. The MoreauPruner algorithm offers a novel approach that leverages the Moreau envelope, providing robustness against such perturbations. The experimental results on several well-known LLMs, including extensive evaluation, add to the strengths of this work.' Weaknesses='The paper lacks clarity on the practical implications of pruning instability due to weight perturbations, leaving the reader questioning the real-world impact of the observed differences. Additionally, the performance improvements demonstrated by MoreauPruner, though present, are not dramatically better than existing methods, notably in certain contexts. Another limitation is the slightly restricted choice of target models, which primarily centers on the LLaMA family. The paper would benefit from improved descriptions regarding computational costs and resource demands of the proposed method.' Questions='How critical is the difference in pruning results to the overall performance of the models in real-world applications? Could additional sources of weight perturbation be considered in future studies to further generalize the findings presented here?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper addresses an important gap in the evaluation of OOD robustness for modern vision models in the context of web-scale datasets. \\n2. The introduction of LAION-C with novel distortion types provides a fresh and challenging benchmark for contemporary models, specifically designed to be OOD relative to data scraped from the web. \\n3. The psychophysical experiment that compares model performance with human observers is a valuable addition, as it allows for a deeper understanding of how models perform relative to human perception under these new conditions.' Weaknesses='1. While the novel distortions are a major contribution, the paper could benefit from a deeper exploration of the specific properties of these distortions and why they were chosen over potential alternatives. \\n2. There is limited discussion on the potential bias introduced by the class distribution differences between LAION-C and traditional benchmarks like ImageNet-C.' Questions='1. Could you elaborate on how the distortions were selected and whether there was any consideration of other types of potential distortions that might also be OOD for web-scale datasets?\\n2. How do you anticipate that the smaller number of classes in LAION-C compared to benchmarks like ImageNet-C might affect the generalizability of your results?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper addresses an important gap in the evaluation of OOD robustness for modern vision models in the context of web-scale datasets. \\n2. The introduction of LAION-C with novel distortion types provides a fresh and challenging benchmark for contemporary models, specifically designed to be OOD relative to data scraped from the web. \\n3. The psychophysical experiment that compares model performance with human observers is a valuable addition, as it allows for a deeper understanding of how models perform relative to human perception under these new conditions.' Weaknesses='1. While the novel distortions are a major contribution, the paper could benefit from a deeper exploration of the specific properties of these distortions and why they were chosen over potential alternatives. \\n2. There is limited discussion on the potential bias introduced by the class distribution differences between LAION-C and traditional benchmarks like ImageNet-C.' Questions='1. Could you elaborate on how the distortions were selected and whether there was any consideration of other types of potential distortions that might also be OOD for web-scale datasets?\\n2. How do you anticipate that the smaller number of classes in LAION-C compared to benchmarks like ImageNet-C might affect the generalizability of your results?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel integration of Koopman theory, specifically Dynamic Mode Decomposition (DMD), with Graph Neural Networks (GNNs) to improve modeling and prediction tasks on graph-structured data. The connection between DMD and GNNs for estimating a low-rank linear operator provides a fresh perspective and potentially more efficient approach to capturing complex dynamics. The paper is well-written and structured, facilitating a clear understanding of the concepts presented.' Weaknesses='While the paper presents a strong novel contribution, there may be concerns regarding the practicality and computational efficiency of implementing DMD in various real-world graph applications. Additionally, the presentation of results could further benefit from including comparisons with more diverse baseline methods beyond current state-of-the-art GNNs to demonstrate broader applicability and improvement in real-world scenarios.' Questions='What are the computational costs associated with implementing DMD in GNNs when compared to other state-of-the-art models, particularly in large-scale graph applications? How does the proposed method handle the challenges of scaling to different types of graph structures (e.g., highly irregular or very large graphs)?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel integration of Koopman theory, specifically Dynamic Mode Decomposition (DMD), with Graph Neural Networks (GNNs) to improve modeling and prediction tasks on graph-structured data. The connection between DMD and GNNs for estimating a low-rank linear operator provides a fresh perspective and potentially more efficient approach to capturing complex dynamics. The paper is well-written and structured, facilitating a clear understanding of the concepts presented.' Weaknesses='While the paper presents a strong novel contribution, there may be concerns regarding the practicality and computational efficiency of implementing DMD in various real-world graph applications. Additionally, the presentation of results could further benefit from including comparisons with more diverse baseline methods beyond current state-of-the-art GNNs to demonstrate broader applicability and improvement in real-world scenarios.' Questions='What are the computational costs associated with implementing DMD in GNNs when compared to other state-of-the-art models, particularly in large-scale graph applications? How does the proposed method handle the challenges of scaling to different types of graph structures (e.g., highly irregular or very large graphs)?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper proposes a method that refines LiSSA by selecting hyperparameters based on spectral properties, improving its practicality for large models. It provides a good balance between theoretical insights and empirical validation. The use of random sketching for evaluating spectral properties is a beneficial approach for handling large-scale data. Empirical comparisons with PBRF offer evidence of the proposed method's efficacy.\" Weaknesses='The reliance on specific spectral properties might limit the applicability of the method to scenarios where these properties can be efficiently estimated. The novelty of the contribution is somewhat limited as it can be seen as an improvement on an existing method. The potential impact is not fully demonstrated across diverse datasets or real-world scenarios.' Questions='1. How does the efficiency of random sketching compare to other methods for spectral property evaluation, especially in very large models?\\n2. Are there specific model architectures or sizes where this method might not be applicable or efficient?\\n3. Can this approach be extended to other settings where the inverse Hessian is used, beyond influence functions?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper proposes a method that refines LiSSA by selecting hyperparameters based on spectral properties, improving its practicality for large models. It provides a good balance between theoretical insights and empirical validation. The use of random sketching for evaluating spectral properties is a beneficial approach for handling large-scale data. Empirical comparisons with PBRF offer evidence of the proposed method's efficacy.\" Weaknesses='The reliance on specific spectral properties might limit the applicability of the method to scenarios where these properties can be efficiently estimated. The novelty of the contribution is somewhat limited as it can be seen as an improvement on an existing method. The potential impact is not fully demonstrated across diverse datasets or real-world scenarios.' Questions='1. How does the efficiency of random sketching compare to other methods for spectral property evaluation, especially in very large models?\\n2. Are there specific model architectures or sizes where this method might not be applicable or efficient?\\n3. Can this approach be extended to other settings where the inverse Hessian is used, beyond influence functions?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=5 Strengths=\"The paper presents a structured approach to understanding the role of contextual information in online learning within a brokerage setting, providing clear algorithms and matching bounds for both feedback models. The constraints and assumptions are clearly defined, showcasing the paper's rigor.\" Weaknesses=\"The abstraction and assumptions around traders' valuations being zero-mean perturbations and the dependency on bounded density may limit the generalizability of the results. Additionally, while the results are promising, the paper could have expanded more on the practical implications and the conceptual differences compared to existing works.\" Questions='1. How do the assumptions about the bounded densities and zero-mean perturbations affect the applicability of these algorithms in real-world scenarios?\\n2. Can the methodology be adapted or extended to account for more realistic market behavior where biases in trader valuations exist?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=5 Strengths=\"The paper presents a structured approach to understanding the role of contextual information in online learning within a brokerage setting, providing clear algorithms and matching bounds for both feedback models. The constraints and assumptions are clearly defined, showcasing the paper's rigor.\" Weaknesses=\"The abstraction and assumptions around traders' valuations being zero-mean perturbations and the dependency on bounded density may limit the generalizability of the results. Additionally, while the results are promising, the paper could have expanded more on the practical implications and the conceptual differences compared to existing works.\" Questions='1. How do the assumptions about the bounded densities and zero-mean perturbations affect the applicability of these algorithms in real-world scenarios?\\n2. Can the methodology be adapted or extended to account for more realistic market behavior where biases in trader valuations exist?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=3 Strengths='The paper presents a novel method for quantizing SSMs, addressing the unique challenges posed by their sensitive feature maps and outlier activations. The proposed static 8-bit quantization technique is both innovative and practical, demonstrating significant improvements in latency with minimal accuracy loss. The approach is well-suited for large language models, enabling their deployment on resource-constrained platforms. The work is supported by comprehensive experiments on two substantial SSMs, Mamba 2.8B and Jamba 52B, showcasing the scalability and effectiveness of the method.' Weaknesses='While the paper demonstrates practical value, there are concerns regarding the clarity of certain methodological details, particularly the specifics of the quantization process and the handling of outliers. Additionally, the experiments are limited to specific hardware setups, and broader testing across diverse platforms could provide stronger evidence of generalizability. Some of the theoretical motivations for using the Hadamard transform in this context could also be elaborated more thoroughly.' Questions='1. Can the authors provide more details on how the Hadamard transform specifically aids in achieving an outlier-free quantization space? A deeper understanding of this mechanism in relation to SSMs would be beneficial.\\n2. The paper mentions experiments on specific hardware but lacks broader hardware variability. Are there plans to validate the approach on other edge and cloud devices to ensure wider applicability?\\n3. The paper primarily focuses on improvements for SSMs. Could the proposed techniques be adapted or extended to quantize other model architectures, such as those involving traditional attention mechanisms, and if so, how would they be modified?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=3 Strengths='The paper presents a novel method for quantizing SSMs, addressing the unique challenges posed by their sensitive feature maps and outlier activations. The proposed static 8-bit quantization technique is both innovative and practical, demonstrating significant improvements in latency with minimal accuracy loss. The approach is well-suited for large language models, enabling their deployment on resource-constrained platforms. The work is supported by comprehensive experiments on two substantial SSMs, Mamba 2.8B and Jamba 52B, showcasing the scalability and effectiveness of the method.' Weaknesses='While the paper demonstrates practical value, there are concerns regarding the clarity of certain methodological details, particularly the specifics of the quantization process and the handling of outliers. Additionally, the experiments are limited to specific hardware setups, and broader testing across diverse platforms could provide stronger evidence of generalizability. Some of the theoretical motivations for using the Hadamard transform in this context could also be elaborated more thoroughly.' Questions='1. Can the authors provide more details on how the Hadamard transform specifically aids in achieving an outlier-free quantization space? A deeper understanding of this mechanism in relation to SSMs would be beneficial.\\n2. The paper mentions experiments on specific hardware but lacks broader hardware variability. Are there plans to validate the approach on other edge and cloud devices to ensure wider applicability?\\n3. The paper primarily focuses on improvements for SSMs. Could the proposed techniques be adapted or extended to quantize other model architectures, such as those involving traditional attention mechanisms, and if so, how would they be modified?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel approach to tackle multi-label node classification (MLNC) on graphs by considering influences between labels, which is a relatively unexplored area in existing literature. LIP model constructs a label influence graph for high-order influence propagation, showing promising results on benchmark datasets.' Weaknesses='The paper lacks a clear comparison with existing label propagation methods and does not adequately elaborate on the difference from previous approaches. The influence of crucial hyperparameters on model performance is not thoroughly analyzed, and there is a need for more comprehensive experimentation across various settings to establish generalization claims.' Questions='How do the proposed method and previous label propagation techniques differ in handling label influences? Could the authors provide more visualizations or examples to illustrate how the LIP model adjusts learning based on label influence? How sensitive is the model performance to the choice of hyperparameters such as \\\\alpha and \\\\beta?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel approach to tackle multi-label node classification (MLNC) on graphs by considering influences between labels, which is a relatively unexplored area in existing literature. LIP model constructs a label influence graph for high-order influence propagation, showing promising results on benchmark datasets.' Weaknesses='The paper lacks a clear comparison with existing label propagation methods and does not adequately elaborate on the difference from previous approaches. The influence of crucial hyperparameters on model performance is not thoroughly analyzed, and there is a need for more comprehensive experimentation across various settings to establish generalization claims.' Questions='How do the proposed method and previous label propagation techniques differ in handling label influences? Could the authors provide more visualizations or examples to illustrate how the LIP model adjusts learning based on label influence? How sensitive is the model performance to the choice of hyperparameters such as \\\\alpha and \\\\beta?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper introduces an innovative approach by replacing fixed activation functions in neural networks with learnable ones, which enhances both accuracy and interpretability. The use of Bernstein polynomials as a function basis within KANs is a novel application that is theoretically grounded, leveraging the Weierstrass approximation theorem. The approach has been validated through comprehensive experiments across several domains, indicating its broad applicability. The paper is well-organized, and the experiments are carefully designed to demonstrate the proposed method's strengths over conventional models.\" Weaknesses=\"The practical impact of the theoretical claims, such as the benefit of the uniform approximation property and error minimization, is not thoroughly evidenced in the experiments. The claims regarding flexibility and robustness are not strongly supported with empirical results. The performance improvements, although present, are not groundbreaking and may not justify the additional complexity introduced by the proposed framework. The comparison to state-of-the-art models in each domain is limited, making it difficult to evaluate the proposed method's true contribution.\" Questions=\"Can the authors clarify how the selection of Bernstein polynomials (and the variation of the function basis) benefits the model's performance in different datasets? What are the practical implications of the theoretical findings, such as Theorem 3.3, on real-world datasets? Is the computational cost of implementing VBn-KAN significantly higher than standard KANs or other neural networks due to the learnable activation functions, and how does this affect scalability?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper introduces an innovative approach by replacing fixed activation functions in neural networks with learnable ones, which enhances both accuracy and interpretability. The use of Bernstein polynomials as a function basis within KANs is a novel application that is theoretically grounded, leveraging the Weierstrass approximation theorem. The approach has been validated through comprehensive experiments across several domains, indicating its broad applicability. The paper is well-organized, and the experiments are carefully designed to demonstrate the proposed method's strengths over conventional models.\" Weaknesses=\"The practical impact of the theoretical claims, such as the benefit of the uniform approximation property and error minimization, is not thoroughly evidenced in the experiments. The claims regarding flexibility and robustness are not strongly supported with empirical results. The performance improvements, although present, are not groundbreaking and may not justify the additional complexity introduced by the proposed framework. The comparison to state-of-the-art models in each domain is limited, making it difficult to evaluate the proposed method's true contribution.\" Questions=\"Can the authors clarify how the selection of Bernstein polynomials (and the variation of the function basis) benefits the model's performance in different datasets? What are the practical implications of the theoretical findings, such as Theorem 3.3, on real-world datasets? Is the computational cost of implementing VBn-KAN significantly higher than standard KANs or other neural networks due to the learnable activation functions, and how does this affect scalability?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=3 Strengths='1. The method allows the generation of detailed 3D articulated objects from a single image, addressing scalability and practicality concerns in the field.\\n2. The use of a diffusion model to capture plausible variations in object geometry and kinematics is innovative and effective.\\n3. The coarse-to-fine generation pipeline, leveraging part connectivity and abstraction, offers a structured approach to object generation.' Weaknesses='1. The method relies heavily on existing data for training, which could limit its effectiveness and applicability when such data is scarce or unavailable.\\n2. There might be challenges in generalizing the model to broader categories of objects, beyond those specifically tested in the study.\\n3. Details about the integration of high-level structure and geometric details could be further elaborated to fully understand the process.' Questions='1. How does the model handle objects with more complex structures and articulations that were not part of the initial training set?\\n2. Are there specific limitations in terms of object categories or types that the current model may struggle with?\\n3. How does the performance of the model compare when tested on unseen object categories, outside the training data?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=3 Strengths='1. The method allows the generation of detailed 3D articulated objects from a single image, addressing scalability and practicality concerns in the field.\\n2. The use of a diffusion model to capture plausible variations in object geometry and kinematics is innovative and effective.\\n3. The coarse-to-fine generation pipeline, leveraging part connectivity and abstraction, offers a structured approach to object generation.' Weaknesses='1. The method relies heavily on existing data for training, which could limit its effectiveness and applicability when such data is scarce or unavailable.\\n2. There might be challenges in generalizing the model to broader categories of objects, beyond those specifically tested in the study.\\n3. Details about the integration of high-level structure and geometric details could be further elaborated to fully understand the process.' Questions='1. How does the model handle objects with more complex structures and articulations that were not part of the initial training set?\\n2. Are there specific limitations in terms of object categories or types that the current model may struggle with?\\n3. How does the performance of the model compare when tested on unseen object categories, outside the training data?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths=\"- The introduction of Geometric Spatiotemporal Transformers (GSTs) is a novel and significant contribution to the field, as it addresses the limitations of existing Graph Neural Networks (GNNs) by handling input sequences of arbitrary lengths and capturing long-term contexts more effectively.\\n- The use of Temporal Difference Graph (TDG) is innovative, effectively capturing global dynamic patterns and mitigating cumulative errors in long-term prediction tasks.\\n- The paper demonstrates the method's superiority through empirical evaluation across multiple challenging physical systems, enhancing its credibility and applicability across different scales.\\n- The approach leverages Transformers in an autoregressive framework, which is a promising direction given recent advances in the field.\\n\" Weaknesses=\"- The paper might lack comprehensive runtime analyses to compare the efficiency of the proposed technique relative to existing methods. This is necessary to assess the practicality of GSTs in resource-constrained applications.\\n- Details on model scalability, especially for large-scale problems or extended timeframes, are not thoroughly discussed. This might raise questions about how the method performs in more complex real-world scenarios.\\n- The abstract does not specify the size or complexity of the datasets used for evaluation, which is crucial to validate the claims about the method's performance.\\n- More extensive comparisons with state-of-the-art models, especially in terms of computational costs and model complexity, could provide a clearer understanding of the trade-offs involved.\\n\" Questions='- How does the GST method scale with larger datasets or systems with more complex interactions? Are there any known limitations or bottlenecks?\\n- What are the specific computational requirements for training and implementing GSTs compared to traditional GNNs? How does the introduction of TDG affect these requirements?\\n- The paper presents strong empirical results, but can you provide more insights into any scenarios or types of systems where GSTs may underperform compared to existing methods?\\n- What specific measures were taken to ensure the alignment and preservation of E(3) symmetries within the proposed spatiotemporal blocks? Are there instances where this becomes particularly challenging?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths=\"- The introduction of Geometric Spatiotemporal Transformers (GSTs) is a novel and significant contribution to the field, as it addresses the limitations of existing Graph Neural Networks (GNNs) by handling input sequences of arbitrary lengths and capturing long-term contexts more effectively.\\n- The use of Temporal Difference Graph (TDG) is innovative, effectively capturing global dynamic patterns and mitigating cumulative errors in long-term prediction tasks.\\n- The paper demonstrates the method's superiority through empirical evaluation across multiple challenging physical systems, enhancing its credibility and applicability across different scales.\\n- The approach leverages Transformers in an autoregressive framework, which is a promising direction given recent advances in the field.\\n\" Weaknesses=\"- The paper might lack comprehensive runtime analyses to compare the efficiency of the proposed technique relative to existing methods. This is necessary to assess the practicality of GSTs in resource-constrained applications.\\n- Details on model scalability, especially for large-scale problems or extended timeframes, are not thoroughly discussed. This might raise questions about how the method performs in more complex real-world scenarios.\\n- The abstract does not specify the size or complexity of the datasets used for evaluation, which is crucial to validate the claims about the method's performance.\\n- More extensive comparisons with state-of-the-art models, especially in terms of computational costs and model complexity, could provide a clearer understanding of the trade-offs involved.\\n\" Questions='- How does the GST method scale with larger datasets or systems with more complex interactions? Are there any known limitations or bottlenecks?\\n- What are the specific computational requirements for training and implementing GSTs compared to traditional GNNs? How does the introduction of TDG affect these requirements?\\n- The paper presents strong empirical results, but can you provide more insights into any scenarios or types of systems where GSTs may underperform compared to existing methods?\\n- What specific measures were taken to ensure the alignment and preservation of E(3) symmetries within the proposed spatiotemporal blocks? Are there instances where this becomes particularly challenging?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel application of diffusion probabilistic models (DPMs) for graph representation learning, which is innovative and shows strong results. The theoretical analysis is solid, proving that the denoising objective maximizes conditional mutual information, and the empirical results are impressive, achieving state-of-the-art performance on multiple datasets.' Weaknesses=\"While the theoretical contributions are significant, the practical implications and potential limitations of using DPMs for graph representation learning are less discussed. There's also a need for more details on the experimental setup and comparison with a broader set of baseline methods.\" Questions='What are the limitations of the proposed approach in terms of scalability to larger graphs? How sensitive is the model to the choice of hyperparameters, especially concerning the diffusion process? Can the approach be generalized to other types of data representations beyond graphs?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper introduces a novel application of diffusion probabilistic models (DPMs) for graph representation learning, which is innovative and shows strong results. The theoretical analysis is solid, proving that the denoising objective maximizes conditional mutual information, and the empirical results are impressive, achieving state-of-the-art performance on multiple datasets.' Weaknesses=\"While the theoretical contributions are significant, the practical implications and potential limitations of using DPMs for graph representation learning are less discussed. There's also a need for more details on the experimental setup and comparison with a broader set of baseline methods.\" Questions='What are the limitations of the proposed approach in terms of scalability to larger graphs? How sensitive is the model to the choice of hyperparameters, especially concerning the diffusion process? Can the approach be generalized to other types of data representations beyond graphs?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel approach to augmenting large language models with external knowledge attention for question answering, introducing an extra head for better knowledge integration. The memory-based mechanism controlling the degree of knowledge mixing is innovative and promises to address existing challenges in retrieval-augmented generation, potentially optimizing performance in both general and specific-domain QA tasks.' Weaknesses=\"While the proposed model framework seems promising, the explanation of the methodology seems somewhat abstract, lacking detailed clarity. The novelty of integrating external knowledge as an extra attention head, while interesting, would benefit from deeper theoretical grounding and experimental comparison with existing baseline strategies. More evidence supporting the purported memory-based dynamic integration's effectiveness over traditional methods would strengthen the contribution.\" Questions='How does the proposed method compare with existing RAG approaches in terms of computational efficiency and inference speed?\\nWhat metrics are used to evaluate the performance improvements in QA tasks, and could there be potential trade-offs in using the extra attention head?\\nAre there any limitations concerning the types or formats of external knowledge that this approach can integrate effectively?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel approach to augmenting large language models with external knowledge attention for question answering, introducing an extra head for better knowledge integration. The memory-based mechanism controlling the degree of knowledge mixing is innovative and promises to address existing challenges in retrieval-augmented generation, potentially optimizing performance in both general and specific-domain QA tasks.' Weaknesses=\"While the proposed model framework seems promising, the explanation of the methodology seems somewhat abstract, lacking detailed clarity. The novelty of integrating external knowledge as an extra attention head, while interesting, would benefit from deeper theoretical grounding and experimental comparison with existing baseline strategies. More evidence supporting the purported memory-based dynamic integration's effectiveness over traditional methods would strengthen the contribution.\" Questions='How does the proposed method compare with existing RAG approaches in terms of computational efficiency and inference speed?\\nWhat metrics are used to evaluate the performance improvements in QA tasks, and could there be potential trade-offs in using the extra attention head?\\nAre there any limitations concerning the types or formats of external knowledge that this approach can integrate effectively?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The proposed method introduces a novel way to dynamically calculate the guide sampling rate to ensure that the mean return of the learning policy meets a user-defined performance threshold. This is a significant contribution to guided reinforcement learning, providing a strong theoretical basis for the method.\\nThe paper addresses a key issue in reinforcement learning regarding sample efficiency and effectively proposes a methodology to mitigate it by leveraging pre-existing knowledge through guide policies.' Weaknesses=\"The assumptions required to derive the sampling rate are strong and may not hold in many practical scenarios. This potentially limits the applicability of the theoretical guarantees.\\nThe empirical evaluation, although sufficient for illustration, lacks depth and diversity in terms of testing environments and could have included more complex and diverse tasks to better demonstrate the method's robustness and applicability in real-world scenarios.\\nSome claims, especially related to performance guarantees, could be overstated given the assumptions and limitations outlined.\\nThe presentation of results could be clearer, particularly with respect to Figures and their explanatory details.\" Questions=\"How robust is the proposed method when the assumptions required for the sampling rate derivation do not hold? Is the method performant in such cases?\\nAre there specific scenarios or conditions where the roll-back feature is particularly advantageous or detrimental?\\nCould more complex environments, possibly with a combination of dense and sparse rewards, provide deeper insights into the method's practical utility?\\nHow sensitive is the method to hyperparameters that affect the guide sampling rate and the roll-back mechanism?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The proposed method introduces a novel way to dynamically calculate the guide sampling rate to ensure that the mean return of the learning policy meets a user-defined performance threshold. This is a significant contribution to guided reinforcement learning, providing a strong theoretical basis for the method.\\nThe paper addresses a key issue in reinforcement learning regarding sample efficiency and effectively proposes a methodology to mitigate it by leveraging pre-existing knowledge through guide policies.' Weaknesses=\"The assumptions required to derive the sampling rate are strong and may not hold in many practical scenarios. This potentially limits the applicability of the theoretical guarantees.\\nThe empirical evaluation, although sufficient for illustration, lacks depth and diversity in terms of testing environments and could have included more complex and diverse tasks to better demonstrate the method's robustness and applicability in real-world scenarios.\\nSome claims, especially related to performance guarantees, could be overstated given the assumptions and limitations outlined.\\nThe presentation of results could be clearer, particularly with respect to Figures and their explanatory details.\" Questions=\"How robust is the proposed method when the assumptions required for the sampling rate derivation do not hold? Is the method performant in such cases?\\nAre there specific scenarios or conditions where the roll-back feature is particularly advantageous or detrimental?\\nCould more complex environments, possibly with a combination of dense and sparse rewards, provide deeper insights into the method's practical utility?\\nHow sensitive is the method to hyperparameters that affect the guide sampling rate and the roll-back mechanism?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=3 Strengths='The paper introduces DoD, a novel method for improving image generation by incorporating a multi-stage generation framework. By extracting visual priors and leveraging them for guidance, the method addresses the common challenge of lacking texture details in diffusion models. The approach reduces training costs significantly while still improving performance metrics such as the FID-50K score. The proposed latent embedding module for compressing and retaining semantic information is a unique contribution that enhances the efficacy of diffusion models.' Weaknesses='The paper lacks detailed explanation and comparison with existing state-of-the-art process-based diffusion methods and other frameworks that integrate image features for conditioning, which could further solidify the novelty and benefits of the proposed method. The results section, particularly the main results, provides limited analysis and direct comparison with newer methodologies in the same domain. There is a missed opportunity to showcase more significant qualitative improvements compared to the baseline methods, making it difficult to visually assess the tangible impact of the proposed techniques.' Questions='1) How does DoD compare directly with other multi-stage or process-based diffusion methods, particularly regarding efficiency and output quality?\\n2) What specific use cases or applications do the authors envision for DoD that highlight its strengths over existing diffusion or image generation models?\\n3) Are there any considerations or challenges noted while integrating the latent embedding module, especially in diverse or larger datasets?\\n\\nImproving upon these aspects could potentially raise the quality of the study and fortify its contributions to the field.'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=3 Strengths='The paper introduces DoD, a novel method for improving image generation by incorporating a multi-stage generation framework. By extracting visual priors and leveraging them for guidance, the method addresses the common challenge of lacking texture details in diffusion models. The approach reduces training costs significantly while still improving performance metrics such as the FID-50K score. The proposed latent embedding module for compressing and retaining semantic information is a unique contribution that enhances the efficacy of diffusion models.' Weaknesses='The paper lacks detailed explanation and comparison with existing state-of-the-art process-based diffusion methods and other frameworks that integrate image features for conditioning, which could further solidify the novelty and benefits of the proposed method. The results section, particularly the main results, provides limited analysis and direct comparison with newer methodologies in the same domain. There is a missed opportunity to showcase more significant qualitative improvements compared to the baseline methods, making it difficult to visually assess the tangible impact of the proposed techniques.' Questions='1) How does DoD compare directly with other multi-stage or process-based diffusion methods, particularly regarding efficiency and output quality?\\n2) What specific use cases or applications do the authors envision for DoD that highlight its strengths over existing diffusion or image generation models?\\n3) Are there any considerations or challenges noted while integrating the latent embedding module, especially in diverse or larger datasets?\\n\\nImproving upon these aspects could potentially raise the quality of the study and fortify its contributions to the field.'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a unified framework for addressing both in-distribution and out-of-distribution generalization in deep reinforcement learning (DRL), which is a necessary advancement for real-world deployments. The combination of a robust adaptation module and joint training pipeline is a methodical approach to the problem, and the strong performance of the GRAM algorithm on realistic tasks is noteworthy.' Weaknesses=\"The paper appears to focus solely on locomotion tasks, which may not fully demonstrate the generalizability of the proposed method across different types of environments. It is also unclear how the proposed framework compares against established baselines beyond the mention of strong generalization performance, suggesting that additional comparative analysis may be beneficial. Furthermore, the framework's dependency on a joint training pipeline might introduce complexities in practical deployment scenarios.\" Questions='How does the robust adaptation module handle scenarios where in-distribution and out-of-distribution dynamics are not clearly distinguishable? Can the GRAM framework be easily adapted for tasks beyond locomotion, such as manipulation or human-robot interaction tasks? What metrics are used to evaluate the generalization performance, and how is significance tested across different trials or environments?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a unified framework for addressing both in-distribution and out-of-distribution generalization in deep reinforcement learning (DRL), which is a necessary advancement for real-world deployments. The combination of a robust adaptation module and joint training pipeline is a methodical approach to the problem, and the strong performance of the GRAM algorithm on realistic tasks is noteworthy.' Weaknesses=\"The paper appears to focus solely on locomotion tasks, which may not fully demonstrate the generalizability of the proposed method across different types of environments. It is also unclear how the proposed framework compares against established baselines beyond the mention of strong generalization performance, suggesting that additional comparative analysis may be beneficial. Furthermore, the framework's dependency on a joint training pipeline might introduce complexities in practical deployment scenarios.\" Questions='How does the robust adaptation module handle scenarios where in-distribution and out-of-distribution dynamics are not clearly distinguishable? Can the GRAM framework be easily adapted for tasks beyond locomotion, such as manipulation or human-robot interaction tasks? What metrics are used to evaluate the generalization performance, and how is significance tested across different trials or environments?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel and more effective way of generating canaries for privacy auditing of large language models, providing a significant improvement over previous methods. Extensive experiments are conducted on various LLMs, demonstrating the effectiveness of the proposed approach compared to the state-of-the-art.' Weaknesses='The paper does not fully explore the limitations of the proposed method and how it may be affected by various factors, such as the size and architecture of the LLMs or the type of data used. Additionally, it may not provide enough practical guidelines on how to implement these methods in real-world scenarios.' Questions='1. How does the effectiveness of the new canary generation method vary with different architectures or scales of language models? \\n2. What are the potential limitations of the proposed approach in more restricted or less ideal environments?\\n3. Could the proposed method be effective if integrated into existing privacy frameworks currently employed in industry?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel and more effective way of generating canaries for privacy auditing of large language models, providing a significant improvement over previous methods. Extensive experiments are conducted on various LLMs, demonstrating the effectiveness of the proposed approach compared to the state-of-the-art.' Weaknesses='The paper does not fully explore the limitations of the proposed method and how it may be affected by various factors, such as the size and architecture of the LLMs or the type of data used. Additionally, it may not provide enough practical guidelines on how to implement these methods in real-world scenarios.' Questions='1. How does the effectiveness of the new canary generation method vary with different architectures or scales of language models? \\n2. What are the potential limitations of the proposed approach in more restricted or less ideal environments?\\n3. Could the proposed method be effective if integrated into existing privacy frameworks currently employed in industry?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel methodology, ImmersePro, for stereo video generation, which is a notable advancement in terms of reducing dependency on explicit disparity maps that are prone to errors. It also introduces a large-scale dataset, YouTube-SBS, which could be a valuable resource for the community.' Weaknesses=\"The method description lacks clarity on how the implicit disparity guidance is effectively replacing the traditional explicit maps, and the potential limitations of this approach are not adequately addressed. Additionally, more details on the dataset's diversity and representativeness would strengthen the paper.\" Questions='How does ImmersePro handle diverse scene complexities and occlusions without explicit disparity maps? What are the limitations of using implicit disparity guidance in different video genres or lighting conditions?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel methodology, ImmersePro, for stereo video generation, which is a notable advancement in terms of reducing dependency on explicit disparity maps that are prone to errors. It also introduces a large-scale dataset, YouTube-SBS, which could be a valuable resource for the community.' Weaknesses=\"The method description lacks clarity on how the implicit disparity guidance is effectively replacing the traditional explicit maps, and the potential limitations of this approach are not adequately addressed. Additionally, more details on the dataset's diversity and representativeness would strengthen the paper.\" Questions='How does ImmersePro handle diverse scene complexities and occlusions without explicit disparity maps? What are the limitations of using implicit disparity guidance in different video genres or lighting conditions?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths='1. The paper presents a novel approach for protein design focused on extrapolating fitness beyond the training range, which addresses a significant challenge in the field.\\n2. The proposed method effectively employs triplet relations, incorporating preference alignment models to better capture fitness gradients, which enhances search capabilities.\\n3. Experimental results on designing AAV and GFP proteins show substantial improvements in extrapolative tasks, suggesting the practical applicability and impact of the method.' Weaknesses='1. The reliance on a scorer function for defining triplets can introduce robustness issues, especially in real-world settings where scorers may not generalize well out of distribution.\\n2. Some aspects of the methodology, particularly the explanation of triplet formation and scorer distillation, are complex and could be more clearly articulated to enhance reader comprehension.' Questions='1. How does the preference optimization method handle noisy labels in the triplet definitions, particularly in situations where scorer functions might introduce inaccuracies?\\n2. Can the authors elaborate on the process and importance of mask infilling? Specifically, how prevalent are these masked samples in triplets where fitness ordering challenges arise due to editor limitations?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths='1. The paper presents a novel approach for protein design focused on extrapolating fitness beyond the training range, which addresses a significant challenge in the field.\\n2. The proposed method effectively employs triplet relations, incorporating preference alignment models to better capture fitness gradients, which enhances search capabilities.\\n3. Experimental results on designing AAV and GFP proteins show substantial improvements in extrapolative tasks, suggesting the practical applicability and impact of the method.' Weaknesses='1. The reliance on a scorer function for defining triplets can introduce robustness issues, especially in real-world settings where scorers may not generalize well out of distribution.\\n2. Some aspects of the methodology, particularly the explanation of triplet formation and scorer distillation, are complex and could be more clearly articulated to enhance reader comprehension.' Questions='1. How does the preference optimization method handle noisy labels in the triplet definitions, particularly in situations where scorer functions might introduce inaccuracies?\\n2. Can the authors elaborate on the process and importance of mask infilling? Specifically, how prevalent are these masked samples in triplets where fitness ordering challenges arise due to editor limitations?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper introduces an innovative approach to data augmentation in the context of time series forecasting using reinforcement learning, which is a novel application of the method. \\n2. It addresses the challenge of identifying and transforming marginal samples to improve model robustness, which is a significant contribution to the forecasting field.\\n3. The presentation is clear, structured, and the empirical analysis is well-detailed to support the claims.' Weaknesses='1. The impact of this contribution might be limited by the specificity of the forecasting models used in the study, hence the general applicability is not entirely demonstrated.\\n2. While the approach appears promising, there may be issues with scaling and complexity when applied to larger datasets or more complicated forecasting models, which are not addressed in the abstract.\\n3. The novelty may be somewhat constrained by the fact that it builds on existing techniques such as autoencoders and reinforcement learning, thus the level of innovation might be considered incremental rather than breakthrough.' Questions='1. What datasets were used in the empirical analysis, and do they represent a wide range of forecasting challenges?\\n2. How does the computational cost of the proposed method compare with traditional fixed-size training set approaches?\\n3. Can the model effectively generalize and improve performance on out-of-sample datasets not seen during the training phase?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper introduces an innovative approach to data augmentation in the context of time series forecasting using reinforcement learning, which is a novel application of the method. \\n2. It addresses the challenge of identifying and transforming marginal samples to improve model robustness, which is a significant contribution to the forecasting field.\\n3. The presentation is clear, structured, and the empirical analysis is well-detailed to support the claims.' Weaknesses='1. The impact of this contribution might be limited by the specificity of the forecasting models used in the study, hence the general applicability is not entirely demonstrated.\\n2. While the approach appears promising, there may be issues with scaling and complexity when applied to larger datasets or more complicated forecasting models, which are not addressed in the abstract.\\n3. The novelty may be somewhat constrained by the fact that it builds on existing techniques such as autoencoders and reinforcement learning, thus the level of innovation might be considered incremental rather than breakthrough.' Questions='1. What datasets were used in the empirical analysis, and do they represent a wide range of forecasting challenges?\\n2. How does the computational cost of the proposed method compare with traditional fixed-size training set approaches?\\n3. Can the model effectively generalize and improve performance on out-of-sample datasets not seen during the training phase?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses an important problem in the alignment of modalities in CLIP, promising significant enhancements in downstream tasks.\\n2. AlignCLIP method shows improvements across various evaluations, suggesting robust applicability.\\n3. Presentation is clear, with a defined problem statement and structured approach.' Weaknesses='1. The novelty of the contribution, particularly regarding shared parameters and semantically-regularized separation, might not be significant without more detailed explanation and distinction from existing methods.\\n2. Lack of detailed ablation studies or error analysis could leave questions regarding why specific improvements are observed.\\n3. There are no explicit comparisons with the very latest methods, which might leave room for speculation on its standing among state-of-the-art.' Questions='1. How significant is the contribution of each component, like shared parameters and the separation objective, to the overall performance improvements?\\n2. What are the implications of the modality gap specifically in zero-shot settings, and how is this addressed effectively in varied datasets?\\n3. How does AlignCLIP perform in tasks that have not been specifically mentioned, such as retrieval tasks or complex reasoning tasks, where cross-modal synthesis is crucial?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses an important problem in the alignment of modalities in CLIP, promising significant enhancements in downstream tasks.\\n2. AlignCLIP method shows improvements across various evaluations, suggesting robust applicability.\\n3. Presentation is clear, with a defined problem statement and structured approach.' Weaknesses='1. The novelty of the contribution, particularly regarding shared parameters and semantically-regularized separation, might not be significant without more detailed explanation and distinction from existing methods.\\n2. Lack of detailed ablation studies or error analysis could leave questions regarding why specific improvements are observed.\\n3. There are no explicit comparisons with the very latest methods, which might leave room for speculation on its standing among state-of-the-art.' Questions='1. How significant is the contribution of each component, like shared parameters and the separation objective, to the overall performance improvements?\\n2. What are the implications of the modality gap specifically in zero-shot settings, and how is this addressed effectively in varied datasets?\\n3. How does AlignCLIP perform in tasks that have not been specifically mentioned, such as retrieval tasks or complex reasoning tasks, where cross-modal synthesis is crucial?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents an innovative approach by introducing a behavioral loss to complement structural reconstruction, aimed at better capturing the features critical for reconstructing high-performing models. The evaluation across multiple model zoos demonstrates the practical benefits of the proposed method. The study effectively highlights the synergy between structural and behavioral features, showing improved performance across evaluated downstream tasks.' Weaknesses='The novelty of the proposed behavioral loss might be limited as similar ideas may exist in previous literature. The paper could benefit from a more extensive experimental section, considering a broader range of architectures and application domains to validate the generalizability of the approach. Additionally, the increased training cost and complexity introduced by the behavioral loss are acknowledged but not quantitatively assessed in the paper.' Questions='What specific features or metrics does the behavioral loss target that structural reconstruction fails to capture, and how are these aligned with improvement in model performance?\\nAre there potential limitations or challenges in scaling the proposed method to larger, more complex models or different types of neural networks?\\nCould the authors clarify the intuition for choosing the particular model zoos used for evaluation and whether these selections might bias the reported outcomes?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents an innovative approach by introducing a behavioral loss to complement structural reconstruction, aimed at better capturing the features critical for reconstructing high-performing models. The evaluation across multiple model zoos demonstrates the practical benefits of the proposed method. The study effectively highlights the synergy between structural and behavioral features, showing improved performance across evaluated downstream tasks.' Weaknesses='The novelty of the proposed behavioral loss might be limited as similar ideas may exist in previous literature. The paper could benefit from a more extensive experimental section, considering a broader range of architectures and application domains to validate the generalizability of the approach. Additionally, the increased training cost and complexity introduced by the behavioral loss are acknowledged but not quantitatively assessed in the paper.' Questions='What specific features or metrics does the behavioral loss target that structural reconstruction fails to capture, and how are these aligned with improvement in model performance?\\nAre there potential limitations or challenges in scaling the proposed method to larger, more complex models or different types of neural networks?\\nCould the authors clarify the intuition for choosing the particular model zoos used for evaluation and whether these selections might bias the reported outcomes?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces Extended Implicit Q-Learning (EIQL), a novel algorithm designed to extend the utility of the Bellman update in offline Reinforcement Learning by incorporating actions beyond dataset constraints. It strategically enhances the maximization capability of the Bellman update and mitigates error extrapolation risks. The empirical results demonstrate improved performance over traditional offline RL algorithms, especially in environments with sparse rewards and incomplete trajectories.' Weaknesses='The paper could benefit from a more detailed explanation of the proposed approach, particularly regarding its implementation and theoretical basis. Additionally, the novelty might be questioned, as the approach seems to be a variation of existing Implicit Q-Learning methods. The presentation of the results could be clearer and more comprehensive, with comparisons to a broader range of baseline methods. More details on hyperparameter choices and training setups would enhance reproducibility.' Questions='1. How does the proposed approach compare to other recent advances in offline RL, such as those focusing on robust policy learning without reliance on specific assumptions about the static dataset?\\n2. Could the authors elaborate on the theoretical foundations behind EIQL, particularly how it addresses error extrapolation without increasing computational complexity?\\n3. In environments with very sparse rewards, how does the proposed method ensure the learned policy does not overly focus on suboptimal action spaces?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper introduces Extended Implicit Q-Learning (EIQL), a novel algorithm designed to extend the utility of the Bellman update in offline Reinforcement Learning by incorporating actions beyond dataset constraints. It strategically enhances the maximization capability of the Bellman update and mitigates error extrapolation risks. The empirical results demonstrate improved performance over traditional offline RL algorithms, especially in environments with sparse rewards and incomplete trajectories.' Weaknesses='The paper could benefit from a more detailed explanation of the proposed approach, particularly regarding its implementation and theoretical basis. Additionally, the novelty might be questioned, as the approach seems to be a variation of existing Implicit Q-Learning methods. The presentation of the results could be clearer and more comprehensive, with comparisons to a broader range of baseline methods. More details on hyperparameter choices and training setups would enhance reproducibility.' Questions='1. How does the proposed approach compare to other recent advances in offline RL, such as those focusing on robust policy learning without reliance on specific assumptions about the static dataset?\\n2. Could the authors elaborate on the theoretical foundations behind EIQL, particularly how it addresses error extrapolation without increasing computational complexity?\\n3. In environments with very sparse rewards, how does the proposed method ensure the learned policy does not overly focus on suboptimal action spaces?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper provides a novel theoretical analysis of adaptive complexity for sampling, which is crucial for improving the understanding and development of parallelizable sampling algorithms. The characterization of hardness potentials and the application of classical smoothing techniques are particularly noteworthy.' Weaknesses='The abstract and overall presentation are somewhat dense, making it challenging to follow without a strong background in mathematical optimization and sampling theory. The paper might also benefit from clearer explanations and justifications for the assumptions used in the analysis. There is a lack of empirical validation or practical examples to illustrate the applicability of the theoretical results.' Questions='How do the theoretical findings translate to practical applications in high-dimensional data contexts? Are there specific instances or problems where this approach has shown significant advantages over existing methods? Could there be potential extensions or modifications to address non-convex distributions?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper provides a novel theoretical analysis of adaptive complexity for sampling, which is crucial for improving the understanding and development of parallelizable sampling algorithms. The characterization of hardness potentials and the application of classical smoothing techniques are particularly noteworthy.' Weaknesses='The abstract and overall presentation are somewhat dense, making it challenging to follow without a strong background in mathematical optimization and sampling theory. The paper might also benefit from clearer explanations and justifications for the assumptions used in the analysis. There is a lack of empirical validation or practical examples to illustrate the applicability of the theoretical results.' Questions='How do the theoretical findings translate to practical applications in high-dimensional data contexts? Are there specific instances or problems where this approach has shown significant advantages over existing methods? Could there be potential extensions or modifications to address non-convex distributions?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"SPO offers a novel approach by aligning generative models with human preferences without relying on reward models, thus simplifying the alignment process. The method's theoretical foundation is solidly explained, showing convergence properties under the Bradley-Terry model.\" Weaknesses='There is a lack of clear empirical evidence demonstrating the practical effectiveness of SPO compared to existing alignment techniques, such as those using reward models. Additionally, the presentation could benefit from clearer explanations and visual aids to enhance understanding.' Questions='How does SPO compare in terms of computational cost and performance with existing reward model-based alignment techniques? Are there specific scenarios or datasets where SPO significantly outperforms traditional methods?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"SPO offers a novel approach by aligning generative models with human preferences without relying on reward models, thus simplifying the alignment process. The method's theoretical foundation is solidly explained, showing convergence properties under the Bradley-Terry model.\" Weaknesses='There is a lack of clear empirical evidence demonstrating the practical effectiveness of SPO compared to existing alignment techniques, such as those using reward models. Additionally, the presentation could benefit from clearer explanations and visual aids to enhance understanding.' Questions='How does SPO compare in terms of computational cost and performance with existing reward model-based alignment techniques? Are there specific scenarios or datasets where SPO significantly outperforms traditional methods?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=6 Confidence=4 Strengths='1. The proposed method, SyntheOcc, offers a novel approach to synthesizing photorealistic and controlled images for the task of 3D occupancy prediction, addressing a significant challenge in data annotation for autonomous driving.\\n2. Incorporating 3D semantic multi-plane images (MPIs) is innovative, providing a comprehensive and spatial understanding of the scene, which improves the conditioning input for diffusion models.\\n3. The technique demonstrates practical application potential, with extensive evaluations on the nuScenes dataset showcasing its effectiveness in data augmentation for perception models.' Weaknesses='1. While the paper demonstrates the effectiveness of the proposed method, the discussion around limitations and potential challenges in real-world applications is limited.\\n2. The complexity of encoding 3D geometric information into a 2D diffusion model is addressed, but more technical details or comparisons with existing methodologies could strengthen the assertion of novelty.\\n3. It would be beneficial to understand how the method handles the scalability of generating large datasets with diverse driving scenarios.' Questions='1. How does SyntheOcc ensure the diversity and variety of scenarios while maintaining photorealism during dataset generation?\\n2. Are there any limitations observed in the generation of particular types of scenes or objects, such as irregularly shaped or occluded objects?\\n3. What are the computational requirements for implementing SyntheOcc at scale for continuous data generation in real-time applications?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=6 Confidence=4 Strengths='1. The proposed method, SyntheOcc, offers a novel approach to synthesizing photorealistic and controlled images for the task of 3D occupancy prediction, addressing a significant challenge in data annotation for autonomous driving.\\n2. Incorporating 3D semantic multi-plane images (MPIs) is innovative, providing a comprehensive and spatial understanding of the scene, which improves the conditioning input for diffusion models.\\n3. The technique demonstrates practical application potential, with extensive evaluations on the nuScenes dataset showcasing its effectiveness in data augmentation for perception models.' Weaknesses='1. While the paper demonstrates the effectiveness of the proposed method, the discussion around limitations and potential challenges in real-world applications is limited.\\n2. The complexity of encoding 3D geometric information into a 2D diffusion model is addressed, but more technical details or comparisons with existing methodologies could strengthen the assertion of novelty.\\n3. It would be beneficial to understand how the method handles the scalability of generating large datasets with diverse driving scenarios.' Questions='1. How does SyntheOcc ensure the diversity and variety of scenarios while maintaining photorealism during dataset generation?\\n2. Are there any limitations observed in the generation of particular types of scenes or objects, such as irregularly shaped or occluded objects?\\n3. What are the computational requirements for implementing SyntheOcc at scale for continuous data generation in real-time applications?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The integration of a generative diffusion model with sequential experimental design presents a novel approach to tackling inverse problems in imaging, particularly in the context of reduced data acquisition requirements (such as lower X-ray exposure) while maintaining or improving image reconstruction quality.' Weaknesses='The paper could provide a more detailed analysis of how the trade-off between computational cost and reconstruction quality is managed. Additionally, a comparison against other active learning methods in similar settings would strengthen the contribution.' Questions='How does the proposed method perform in more complex settings, such as fan-beam or 3D cone-beam CT? What are the potential ways to mitigate the computational cost associated with the inference process?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The integration of a generative diffusion model with sequential experimental design presents a novel approach to tackling inverse problems in imaging, particularly in the context of reduced data acquisition requirements (such as lower X-ray exposure) while maintaining or improving image reconstruction quality.' Weaknesses='The paper could provide a more detailed analysis of how the trade-off between computational cost and reconstruction quality is managed. Additionally, a comparison against other active learning methods in similar settings would strengthen the contribution.' Questions='How does the proposed method perform in more complex settings, such as fan-beam or 3D cone-beam CT? What are the potential ways to mitigate the computational cost associated with the inference process?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper proposes a novel approach, SAM-CP, which clearly enhances the Segment Anything Model (SAM) by introducing two types of composable prompts that allow for versatile segmentation. This approach effectively broadens SAM's capability into semantic-aware segmentation tasks, achieving state-of-the-art performance in open-vocabulary segmentation. The framework provides a structured methodology for calculating the affinity between queries and SAM patches, which is a unique contribution to the field.\" Weaknesses=\"While the method shows great promise, it relies heavily on SAM's initial output, which may not always be optimal for all segmentation tasks, particularly for datasets outside of the domain it was trained on. The paper lacks a deep exploration of the limitations of SAM-CP in scenarios where SAM's performance may degrade. Additionally, the complexity of the proposed affinity calculation could be computationally expensive and may not scale well with very large or complex datasets. There is limited discussion on the computational efficiency of the approach.\" Questions=\"How does SAM-CP handle scenarios where SAM's initial segmentation is significantly off from the desired semantic boundaries? Are there any mechanisms to correct such deficiencies? Could the proposed affinity calculation method be applicable or adapted to scale for significantly larger datasets or real-time applications, or does it pose constraints in high-complexity environments?\"\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper proposes a novel approach, SAM-CP, which clearly enhances the Segment Anything Model (SAM) by introducing two types of composable prompts that allow for versatile segmentation. This approach effectively broadens SAM's capability into semantic-aware segmentation tasks, achieving state-of-the-art performance in open-vocabulary segmentation. The framework provides a structured methodology for calculating the affinity between queries and SAM patches, which is a unique contribution to the field.\" Weaknesses=\"While the method shows great promise, it relies heavily on SAM's initial output, which may not always be optimal for all segmentation tasks, particularly for datasets outside of the domain it was trained on. The paper lacks a deep exploration of the limitations of SAM-CP in scenarios where SAM's performance may degrade. Additionally, the complexity of the proposed affinity calculation could be computationally expensive and may not scale well with very large or complex datasets. There is limited discussion on the computational efficiency of the approach.\" Questions=\"How does SAM-CP handle scenarios where SAM's initial segmentation is significantly off from the desired semantic boundaries? Are there any mechanisms to correct such deficiencies? Could the proposed affinity calculation method be applicable or adapted to scale for significantly larger datasets or real-time applications, or does it pose constraints in high-complexity environments?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents a novel approach to video colorization using language descriptions, which is an innovative idea that pushes the boundaries of creative color application while maintaining temporal consistency. The introduction of cross-modality pre-fusion and temporally deformable attention modules addresses key challenges in maintaining color consistency and applying creative color schemes. The experimental results are comprehensive and demonstrate significant improvements over existing methods.' Weaknesses='The paper does not thoroughly discuss limitations or potential failure cases of the proposed method. There is a lack of detail on how the method performs with diverse or complex language inputs and its scalability to very long or detailed video sequences. Additionally, while the concept of using language to control color application is novel, the paper could further explore the potential limitations and challenges this presents, such as handling ambiguous or contradictory descriptions.' Questions='How does the performance of L-C4 scale with increasingly complex or lengthy language descriptions? Are there any specific constraints on the language input in terms of complexity or structure? Additionally, how does the model handle conflicts or ambiguities in the language descriptions provided by users? What are the specific limitations of the cross-modality pre-fusion module in terms of scalability or applicability to different video content types?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents a novel approach to video colorization using language descriptions, which is an innovative idea that pushes the boundaries of creative color application while maintaining temporal consistency. The introduction of cross-modality pre-fusion and temporally deformable attention modules addresses key challenges in maintaining color consistency and applying creative color schemes. The experimental results are comprehensive and demonstrate significant improvements over existing methods.' Weaknesses='The paper does not thoroughly discuss limitations or potential failure cases of the proposed method. There is a lack of detail on how the method performs with diverse or complex language inputs and its scalability to very long or detailed video sequences. Additionally, while the concept of using language to control color application is novel, the paper could further explore the potential limitations and challenges this presents, such as handling ambiguous or contradictory descriptions.' Questions='How does the performance of L-C4 scale with increasingly complex or lengthy language descriptions? Are there any specific constraints on the language input in terms of complexity or structure? Additionally, how does the model handle conflicts or ambiguities in the language descriptions provided by users? What are the specific limitations of the cross-modality pre-fusion module in terms of scalability or applicability to different video content types?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths=\"1. The paper tackles the novel problem of integrating lifelong learning with privacy-aware unlearning, which is a significant contribution to the field of AI and privacy.\\n2. The proposed PALL method offers a balanced approach to both learn and unlearn information effectively within a single architecture, which addresses both current and predicted needs for AI applications.\\n3. The method is empirically validated across multiple architectures, demonstrating versatility.\\n4. The structured use of sparse subnetworks and parameter sharing is an innovative method that supports both learning stability and memory efficiency.\\n5. The use of an episodic memory rehearsal mechanism for exact unlearning is a compelling solution given the problem's constraints.\" Weaknesses=\"1. There might be concerns about the scalability and efficiency of the proposed method when applied to very large tasks or data sets, which isn't fully addressed.\\n2. The complexity of the model might hinder its practical deployment in real-time applications where adaptability and quick responses are necessary.\\n3. While empirical validations are provided, the theoretical backing could be stronger, particularly in explaining how diverse tasks impact the model's performance.\\n4. It remains unclear how well the method will handle unlearning requests that are deeply intertwined with learned tasks, impacting the overall performance.\\n5. The paper could benefit from clearer illustrations of the model's architecture and execution to enhance understanding.\" Questions='1. How does the performance of PALL vary when applied to different types of tasks, such as text processing or other forms of data beyond image classification? \\n2. Could you elaborate on how the episodic memory rehearsal mechanism facilitates unlearning and if it could introduce potential biases in the model?\\n3. How does PALL manage multiple simultaneous unlearning requests, and is there a priority mechanism in place for handling these requests?\\n4. To what extent does the choice of architecture impact the effectiveness of the PALL method, and are there certain architectures that are more suited for this model?\\n5. Are there specific limitations to the types of tasks or datasets where PALL may not perform as well, and how could these be mitigated?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths=\"1. The paper tackles the novel problem of integrating lifelong learning with privacy-aware unlearning, which is a significant contribution to the field of AI and privacy.\\n2. The proposed PALL method offers a balanced approach to both learn and unlearn information effectively within a single architecture, which addresses both current and predicted needs for AI applications.\\n3. The method is empirically validated across multiple architectures, demonstrating versatility.\\n4. The structured use of sparse subnetworks and parameter sharing is an innovative method that supports both learning stability and memory efficiency.\\n5. The use of an episodic memory rehearsal mechanism for exact unlearning is a compelling solution given the problem's constraints.\" Weaknesses=\"1. There might be concerns about the scalability and efficiency of the proposed method when applied to very large tasks or data sets, which isn't fully addressed.\\n2. The complexity of the model might hinder its practical deployment in real-time applications where adaptability and quick responses are necessary.\\n3. While empirical validations are provided, the theoretical backing could be stronger, particularly in explaining how diverse tasks impact the model's performance.\\n4. It remains unclear how well the method will handle unlearning requests that are deeply intertwined with learned tasks, impacting the overall performance.\\n5. The paper could benefit from clearer illustrations of the model's architecture and execution to enhance understanding.\" Questions='1. How does the performance of PALL vary when applied to different types of tasks, such as text processing or other forms of data beyond image classification? \\n2. Could you elaborate on how the episodic memory rehearsal mechanism facilitates unlearning and if it could introduce potential biases in the model?\\n3. How does PALL manage multiple simultaneous unlearning requests, and is there a priority mechanism in place for handling these requests?\\n4. To what extent does the choice of architecture impact the effectiveness of the PALL method, and are there certain architectures that are more suited for this model?\\n5. Are there specific limitations to the types of tasks or datasets where PALL may not perform as well, and how could these be mitigated?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces novel modules (CVCAM and MHSAM) for improving cross-view object geo-localization, which show promising results over previous methods. The creation of a new dataset, G2D, for ground-to-drone localization tasks adds value to the field and addresses a significant gap.' Weaknesses=\"While the paper proposes new modules, the paper lacks detailed theoretical analysis or ablation studies to clearly delineate the specific impact of each component. The novelty in comparison to existing attention mechanisms is not well-highlighted, and it is unclear how new the proposed attention methods are given existing techniques like Transformer's cross-attention. The dataset details need further expansion to understand its construction, biases, and how it compares to existing ones.\" Questions='How do the proposed CVCAM and MHSAM specifically differ from existing cross-attention mechanisms, and what unique enhancements do they provide? Can the authors provide more details about the G2D dataset in terms of its construction process, scale, and potential biases?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces novel modules (CVCAM and MHSAM) for improving cross-view object geo-localization, which show promising results over previous methods. The creation of a new dataset, G2D, for ground-to-drone localization tasks adds value to the field and addresses a significant gap.' Weaknesses=\"While the paper proposes new modules, the paper lacks detailed theoretical analysis or ablation studies to clearly delineate the specific impact of each component. The novelty in comparison to existing attention mechanisms is not well-highlighted, and it is unclear how new the proposed attention methods are given existing techniques like Transformer's cross-attention. The dataset details need further expansion to understand its construction, biases, and how it compares to existing ones.\" Questions='How do the proposed CVCAM and MHSAM specifically differ from existing cross-attention mechanisms, and what unique enhancements do they provide? Can the authors provide more details about the G2D dataset in terms of its construction process, scale, and potential biases?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides a matrix-theoretic explanation for adversarial vulnerability in deep neural networks, offering insights that align with prior information-theoretic analyses.' Weaknesses='The theoretical analysis may reflect an extension of existing understanding of adversarial fragility rather than entirely new results. Additionally, the paper lacks experimental validation which is usually needed to support theoretical findings. The presentation can be improved with more detailed exposition and contextual positioning among current literature.' Questions='How does the matrix-theoretic explanation practically aid in improving network design or training processes to enhance adversarial robustness? Are there specific classes of neural networks where the degradation effect is less pronounced?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper provides a matrix-theoretic explanation for adversarial vulnerability in deep neural networks, offering insights that align with prior information-theoretic analyses.' Weaknesses='The theoretical analysis may reflect an extension of existing understanding of adversarial fragility rather than entirely new results. Additionally, the paper lacks experimental validation which is usually needed to support theoretical findings. The presentation can be improved with more detailed exposition and contextual positioning among current literature.' Questions='How does the matrix-theoretic explanation practically aid in improving network design or training processes to enhance adversarial robustness? Are there specific classes of neural networks where the degradation effect is less pronounced?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper tackles an interesting problem by introducing new training strategies for vision transformers that allow them to maintain functionality under unconventional operation conditions, such as shuffled or missing layers. This adaptability is a significant step forward for distributed neural network architectures.' Weaknesses='While the paper proposes innovative methods, the 20% drop in accuracy is quite significant, and further exploration is needed to mitigate this loss. Additionally, the paper lacks comprehensive evaluations across different transformer architectures and datasets, which would strengthen its claims and applicability.' Questions='How generalizable are these methods to other types of neural networks beyond vision transformers? What steps can be taken to reduce the accuracy loss experienced with the proposed adaptations during layer shuffling or pruning?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper tackles an interesting problem by introducing new training strategies for vision transformers that allow them to maintain functionality under unconventional operation conditions, such as shuffled or missing layers. This adaptability is a significant step forward for distributed neural network architectures.' Weaknesses='While the paper proposes innovative methods, the 20% drop in accuracy is quite significant, and further exploration is needed to mitigate this loss. Additionally, the paper lacks comprehensive evaluations across different transformer architectures and datasets, which would strengthen its claims and applicability.' Questions='How generalizable are these methods to other types of neural networks beyond vision transformers? What steps can be taken to reduce the accuracy loss experienced with the proposed adaptations during layer shuffling or pruning?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper makes a relevant contribution to the field of model generalization by addressing reliance on spurious correlations through a novel dropout technique. The use of example-tied dropout targeting specific neurons and the extensive empirical evaluation across various tasks provide evidence of the approach's effectiveness.\" Weaknesses=\"The paper does not clearly explain how the neurons responsible for memorization are identified. Furthermore, while the dropout method is innovative, it builds upon existing knowledge about memorization without significantly extending theoretical understanding. The methodology's computational complexity may be a concern in large datasets, but this issue has not been fully addressed.\" Questions='How are the neurons responsible for memorization identified without group annotations during training? What is the computational overhead of the proposed FairDropout method in practice, particularly for large networks?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper makes a relevant contribution to the field of model generalization by addressing reliance on spurious correlations through a novel dropout technique. The use of example-tied dropout targeting specific neurons and the extensive empirical evaluation across various tasks provide evidence of the approach's effectiveness.\" Weaknesses=\"The paper does not clearly explain how the neurons responsible for memorization are identified. Furthermore, while the dropout method is innovative, it builds upon existing knowledge about memorization without significantly extending theoretical understanding. The methodology's computational complexity may be a concern in large datasets, but this issue has not been fully addressed.\" Questions='How are the neurons responsible for memorization identified without group annotations during training? What is the computational overhead of the proposed FairDropout method in practice, particularly for large networks?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel framework for 3D hand pose estimation by pre-training on a large dataset of in-the-wild hand images. The approach uses an innovative contrastive learning method that adapts weights based on inter-sample distances, leading to performance improvements over state-of-the-art methods. The paper includes extensive experiments demonstrating effectiveness across multiple datasets.' Weaknesses='The method relies heavily on the quality and diversity of the collected datasets, which might not be broadly generalizable. The potential limitations or constraints of the proposed approach in more varied or real-time applications are not fully explored. Additionally, while the paper presents a novel approach, it builds on existing contrastive learning frameworks without a significant breakthrough in fundamental understanding or technique.' Questions='Could the adaptive weighting mechanism be further generalized to other types of image or pose estimation tasks, and how does it perform in real-time application scenarios?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel framework for 3D hand pose estimation by pre-training on a large dataset of in-the-wild hand images. The approach uses an innovative contrastive learning method that adapts weights based on inter-sample distances, leading to performance improvements over state-of-the-art methods. The paper includes extensive experiments demonstrating effectiveness across multiple datasets.' Weaknesses='The method relies heavily on the quality and diversity of the collected datasets, which might not be broadly generalizable. The potential limitations or constraints of the proposed approach in more varied or real-time applications are not fully explored. Additionally, while the paper presents a novel approach, it builds on existing contrastive learning frameworks without a significant breakthrough in fundamental understanding or technique.' Questions='Could the adaptive weighting mechanism be further generalized to other types of image or pose estimation tasks, and how does it perform in real-time application scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel self-supervised denoising framework C2S that does not require high-SNR labels, which alleviates a major limitation of supervised methods. The GADSM loss and detail refinement extension ensure that the model effectively balances denoising and spatial feature preservation. The multi-contrast denoising capability also adds a new dimension to the proposed method.' Weaknesses='The paper might not provide sufficient empirical evidence demonstrating substantial performance gains over existing self-supervised and especially supervised methods. Also, it would benefit from clearer descriptions of the potential caveats or limitations when the proposed C2S is applied to varied real-world settings. The presentation of results could also be enhanced with more visual examples before and after denoising.' Questions='1. What measures are taken to ensure that the detail refinement extension truly preserves fine spatial features across all tested datasets and conditions?\\n2. Could you illustrate a practical scenario involving multi-contrast denoising and how significantly it improves from the standard single-contrast approach?\\n3. Are there specific limitations or edge cases where C2S might perform suboptimally compared to existing methods?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel self-supervised denoising framework C2S that does not require high-SNR labels, which alleviates a major limitation of supervised methods. The GADSM loss and detail refinement extension ensure that the model effectively balances denoising and spatial feature preservation. The multi-contrast denoising capability also adds a new dimension to the proposed method.' Weaknesses='The paper might not provide sufficient empirical evidence demonstrating substantial performance gains over existing self-supervised and especially supervised methods. Also, it would benefit from clearer descriptions of the potential caveats or limitations when the proposed C2S is applied to varied real-world settings. The presentation of results could also be enhanced with more visual examples before and after denoising.' Questions='1. What measures are taken to ensure that the detail refinement extension truly preserves fine spatial features across all tested datasets and conditions?\\n2. Could you illustrate a practical scenario involving multi-contrast denoising and how significantly it improves from the standard single-contrast approach?\\n3. Are there specific limitations or edge cases where C2S might perform suboptimally compared to existing methods?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel modification to classifier-free guidance by decomposing the update term into parallel and orthogonal components, effectively addressing the issue of oversaturation in diffusion models while improving generation quality. The proposed approach, Adaptive Projected Guidance (APG), requires minimal computational overhead, making it a practical enhancement for existing models. Extensive experiments demonstrate the superior performance of APG across various metrics.' Weaknesses='While the modifications to the CFG update rule are a clever approach to improving image generation without additional complexity, the paper could benefit from a more in-depth theoretical analysis of why these changes lead to the observed improvements. Additionally, the presentation could be more refined to enhance clarity and understanding of the technical details.' Questions='How does the proposed down-weighting of the parallel component in CFG compare to alternative methods for managing oversaturation, and are there scenarios where APG may be less effective? Additionally, could the insights into the connection between CFG and gradient ascent potentially be applied to other model guidance techniques beyond diffusion models?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel modification to classifier-free guidance by decomposing the update term into parallel and orthogonal components, effectively addressing the issue of oversaturation in diffusion models while improving generation quality. The proposed approach, Adaptive Projected Guidance (APG), requires minimal computational overhead, making it a practical enhancement for existing models. Extensive experiments demonstrate the superior performance of APG across various metrics.' Weaknesses='While the modifications to the CFG update rule are a clever approach to improving image generation without additional complexity, the paper could benefit from a more in-depth theoretical analysis of why these changes lead to the observed improvements. Additionally, the presentation could be more refined to enhance clarity and understanding of the technical details.' Questions='How does the proposed down-weighting of the parallel component in CFG compare to alternative methods for managing oversaturation, and are there scenarios where APG may be less effective? Additionally, could the insights into the connection between CFG and gradient ascent potentially be applied to other model guidance techniques beyond diffusion models?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper systematically studies the problem of nearest neighbor search with filter constraints (MultiFilterANN), showcasing both theoretical and practical insights. It provides new algorithms with strong space-time tradeoffs and proposes new datasets for MultiFilterANN, which fill a gap in current literature.' Weaknesses='The proposed algorithms, though promising, may lack clarity in their direct connection between theoretical and practical contributions. The separation between theoretical results and empirical evaluations is blurry, making it somewhat challenging to follow how the theory directly supports the empirical methodology. The use of existing graph indices in empirical work suggests limited novelty on that front.' Questions='How do the theoretical insights from the large filter regime translate into practical performance enhancements for varying filter sizes in your empirical studies?\\nWhat are the limitations of your proposed method compared to existing approaches, particularly in terms of scalability and adaptability to different types of datasets?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper systematically studies the problem of nearest neighbor search with filter constraints (MultiFilterANN), showcasing both theoretical and practical insights. It provides new algorithms with strong space-time tradeoffs and proposes new datasets for MultiFilterANN, which fill a gap in current literature.' Weaknesses='The proposed algorithms, though promising, may lack clarity in their direct connection between theoretical and practical contributions. The separation between theoretical results and empirical evaluations is blurry, making it somewhat challenging to follow how the theory directly supports the empirical methodology. The use of existing graph indices in empirical work suggests limited novelty on that front.' Questions='How do the theoretical insights from the large filter regime translate into practical performance enhancements for varying filter sizes in your empirical studies?\\nWhat are the limitations of your proposed method compared to existing approaches, particularly in terms of scalability and adaptability to different types of datasets?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel approach by integrating diffusion models with PINNs for better handling of singularities in PDE solutions. The use of flow matching for sample generation is innovative and helps address inherent problems in existing methods, promising more efficient computation and potentially higher accuracy.' Weaknesses='The proposal lacks clarity in explaining certain technical aspects, such as the precise mechanism of sample generation and how diffusion models are specifically employed. The experimental results, while promising, are not extensively varied, raising questions about the robustness and general applicability of the method. More diverse PDE examples would further substantiate claims.' Questions='1. Can the method be generalized beyond the addressed scenarios, and how does the performance scale with more complex PDE systems?\\n2. What are the computational overheads introduced by the diffusion models, and how do they compare to gains in PINN accuracy?\\n3. Are there specific limitations or failure cases observed when applying this method to PDEs with different kinds of singularities?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel approach by integrating diffusion models with PINNs for better handling of singularities in PDE solutions. The use of flow matching for sample generation is innovative and helps address inherent problems in existing methods, promising more efficient computation and potentially higher accuracy.' Weaknesses='The proposal lacks clarity in explaining certain technical aspects, such as the precise mechanism of sample generation and how diffusion models are specifically employed. The experimental results, while promising, are not extensively varied, raising questions about the robustness and general applicability of the method. More diverse PDE examples would further substantiate claims.' Questions='1. Can the method be generalized beyond the addressed scenarios, and how does the performance scale with more complex PDE systems?\\n2. What are the computational overheads introduced by the diffusion models, and how do they compare to gains in PINN accuracy?\\n3. Are there specific limitations or failure cases observed when applying this method to PDEs with different kinds of singularities?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"The introduction of MathHay provides a much-needed benchmark specifically targeting the evaluation of long-context mathematical reasoning abilities of LLMs, which is a novel and important contribution. The paper's experiments with top-performing LLMs provide valuable insights into the current performance gaps and challenges in this area.\" Weaknesses='The paper does not provide detailed insights into the exact types of mathematical reasoning tasks included in MathHay, which could aid in better analyzing model performances. Given the novelty of the benchmark, there may also be a need for further validation of its comprehensiveness and effectiveness.' Questions='How was MathHay designed to ensure that it adequately tests both information-seeking and complex mathematical reasoning? Are there plans to extend this benchmark to cover additional types of mathematical tasks?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"The introduction of MathHay provides a much-needed benchmark specifically targeting the evaluation of long-context mathematical reasoning abilities of LLMs, which is a novel and important contribution. The paper's experiments with top-performing LLMs provide valuable insights into the current performance gaps and challenges in this area.\" Weaknesses='The paper does not provide detailed insights into the exact types of mathematical reasoning tasks included in MathHay, which could aid in better analyzing model performances. Given the novelty of the benchmark, there may also be a need for further validation of its comprehensiveness and effectiveness.' Questions='How was MathHay designed to ensure that it adequately tests both information-seeking and complex mathematical reasoning? Are there plans to extend this benchmark to cover additional types of mathematical tasks?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths=\"The paper addresses a pertinent issue in data sharing for machine learning, introducing a method to efficiently evaluate and utilize data sources obtained with constrained access. Mycroft's ability to converge rapidly to the performance of the full-information baseline and robustly rank data sources is a significant advantage.\" Weaknesses='The explanation of the key methods and algorithms, such as the RetrieveTopK function in Algorithm 3 and the application of gradient matching, could benefit from more clarity and detail. Additionally, there is a lack of experimental validation concerning privacy protection, a crucial aspect of this field.' Questions='How does RetrieveTopK ensure coverage of key data subsets? Could the approach be tested with explicit privacy-preserving techniques to offer more insights into its practical applications in contexts with strict data-sharing limitations?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=3 Strengths=\"The paper addresses a pertinent issue in data sharing for machine learning, introducing a method to efficiently evaluate and utilize data sources obtained with constrained access. Mycroft's ability to converge rapidly to the performance of the full-information baseline and robustly rank data sources is a significant advantage.\" Weaknesses='The explanation of the key methods and algorithms, such as the RetrieveTopK function in Algorithm 3 and the application of gradient matching, could benefit from more clarity and detail. Additionally, there is a lack of experimental validation concerning privacy protection, a crucial aspect of this field.' Questions='How does RetrieveTopK ensure coverage of key data subsets? Could the approach be tested with explicit privacy-preserving techniques to offer more insights into its practical applications in contexts with strict data-sharing limitations?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed DICF framework effectively combines adversarial Bayesian modeling and domain indexing to tackle the cold-start problem in cross-domain recommendation, offering an interpretable mechanism for knowledge transfer and improving recommendation accuracy.' Weaknesses='There might be limited novelty, as the concept of domain indexing is relatively new but not unprecedented. Further clarity and detail are required to understand exactly how the domain indices are inferred and used in the proposed framework.' Questions='How does DICF handle the scalability issue when applied to large-scale datasets, considering the potential computational complexity involved in adversarial Bayesian frameworks?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed DICF framework effectively combines adversarial Bayesian modeling and domain indexing to tackle the cold-start problem in cross-domain recommendation, offering an interpretable mechanism for knowledge transfer and improving recommendation accuracy.' Weaknesses='There might be limited novelty, as the concept of domain indexing is relatively new but not unprecedented. Further clarity and detail are required to understand exactly how the domain indices are inferred and used in the proposed framework.' Questions='How does DICF handle the scalability issue when applied to large-scale datasets, considering the potential computational complexity involved in adversarial Bayesian frameworks?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper presents a novel method, SteerPrompt, which explicitly highlights key context by steering an LLM's attention scores without requiring parameter changes. This approach demonstrates a significant improvement in performance, showing an average improvement of 7.95% for LLAMA3-70B-Instruct. The method is applicable at inference time, making it a flexible and efficient option for enhancing LLM comprehension.\" Weaknesses='The approach may be limited to specific use-cases such as open-book QA, and its applicability to other tasks is not explored. Additionally, while SteerPrompt does not require changing model parameters, its effectiveness may depend on the accuracy of identifying key contextual information, which could be a limiting factor. The paper does not address how this method affects models with different architectures or how it scales with model size.' Questions='How does SteerPrompt compare to other state-of-the-art methods in terms of computational resources and efficiency? Are there scenarios where the method may not perform as well, and how can these be addressed? Could this approach be integrated into real-time or streaming applications, and what would be the potential challenges?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper presents a novel method, SteerPrompt, which explicitly highlights key context by steering an LLM's attention scores without requiring parameter changes. This approach demonstrates a significant improvement in performance, showing an average improvement of 7.95% for LLAMA3-70B-Instruct. The method is applicable at inference time, making it a flexible and efficient option for enhancing LLM comprehension.\" Weaknesses='The approach may be limited to specific use-cases such as open-book QA, and its applicability to other tasks is not explored. Additionally, while SteerPrompt does not require changing model parameters, its effectiveness may depend on the accuracy of identifying key contextual information, which could be a limiting factor. The paper does not address how this method affects models with different architectures or how it scales with model size.' Questions='How does SteerPrompt compare to other state-of-the-art methods in terms of computational resources and efficiency? Are there scenarios where the method may not perform as well, and how can these be addressed? Could this approach be integrated into real-time or streaming applications, and what would be the potential challenges?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=3 Strengths='1. The paper introduces Vision-RWKV, a model tailored for vision tasks by modifying the RWKV architecture, highlighting its potential in efficiently processing high-resolution images. \\n2. Its unique features include reduced spatial aggregation complexity, showcasing superior performance over Vision Transformer (ViT) in both speed and memory usage across various tasks. \\n3. The availability of the model and code promotes reproducibility and further exploration.' Weaknesses='1. While the model showcases performance enhancements, the paper could benefit from more detailed technical explanations of the modifications made and their impacts on architecture.\\n2. The comparison with other architectures, especially more recent or lesser-known models, could help place the contribution in a broader context.' Questions='1. Can the modifications in VRWKV be applied to other architectures beyond RWKV, potentially broadening its applicability in vision tasks?\\n2. Are there any particular limitations or scenarios where VRWKV might not perform as well as window-based models?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=3 Strengths='1. The paper introduces Vision-RWKV, a model tailored for vision tasks by modifying the RWKV architecture, highlighting its potential in efficiently processing high-resolution images. \\n2. Its unique features include reduced spatial aggregation complexity, showcasing superior performance over Vision Transformer (ViT) in both speed and memory usage across various tasks. \\n3. The availability of the model and code promotes reproducibility and further exploration.' Weaknesses='1. While the model showcases performance enhancements, the paper could benefit from more detailed technical explanations of the modifications made and their impacts on architecture.\\n2. The comparison with other architectures, especially more recent or lesser-known models, could help place the contribution in a broader context.' Questions='1. Can the modifications in VRWKV be applied to other architectures beyond RWKV, potentially broadening its applicability in vision tasks?\\n2. Are there any particular limitations or scenarios where VRWKV might not perform as well as window-based models?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper presents a novel optimizer, SignSoftSGD (S3), which addresses significant gradient variations and reduces the risk of loss spikes, improving the robustness and convergence speed in training deep neural networks. The integration of Nesterov's accelerated gradient technique without additional memory overhead is a key strength. Theoretical analysis and extensive experimentation on various tasks validate the benefits of the proposed method.\" Weaknesses='Although the paper provides solid theoretical and empirical support, the practical implementation details and impact on different network architectures could be further elaborated. More discussion on potential limitations of the S3 optimizer, especially in specific use cases, would be valuable.' Questions='How does S3 compare in terms of computational efficiency with other state-of-the-art optimizers? Are there any specific scenarios where S3 might not perform as expected compared to Adam or AdamW?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper presents a novel optimizer, SignSoftSGD (S3), which addresses significant gradient variations and reduces the risk of loss spikes, improving the robustness and convergence speed in training deep neural networks. The integration of Nesterov's accelerated gradient technique without additional memory overhead is a key strength. Theoretical analysis and extensive experimentation on various tasks validate the benefits of the proposed method.\" Weaknesses='Although the paper provides solid theoretical and empirical support, the practical implementation details and impact on different network architectures could be further elaborated. More discussion on potential limitations of the S3 optimizer, especially in specific use cases, would be valuable.' Questions='How does S3 compare in terms of computational efficiency with other state-of-the-art optimizers? Are there any specific scenarios where S3 might not perform as expected compared to Adam or AdamW?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel framework called TRIDENT for compositional zero-shot learning (CZSL), addressing limitations of previous methods by using multimodal large language model embeddings and introducing attribute smoothing. The approach seems well-grounded theoretically and demonstrates state-of-the-art performance on three challenging datasets. The innovative use of MLLM embeddings and adaptive feature aggregation shows promise in improving generalization to novel compositions.' Weaknesses=\"The paper's presentation could benefit from clearer explanations and diagrams, as some portions, especially regarding Figure 2 and mathematical notations, may be unclear to readers. Details on the training process of key components like the LLM and the image embedder are somewhat ambiguous. Additionally, a deeper discussion on the integration and role of attribute-object disentanglement in the final inference process is needed. The lack of comparisons with the latest works limits an understanding of relative advancements over very recent methods.\" Questions=\"1. Can the authors provide more insight into how the modules used during training, such as attribute-object disentanglement, contribute to the model's final inference performance?\\n2. How does the proposed method compare with the most recent methodologies, like Troika, in terms of performance and generalization capabilities?\\n3. Could the authors clarify the specific training and freezing strategy employed for different modules within Figure 2, especially concerning the MLLM and visual backbone components?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper presents a novel framework called TRIDENT for compositional zero-shot learning (CZSL), addressing limitations of previous methods by using multimodal large language model embeddings and introducing attribute smoothing. The approach seems well-grounded theoretically and demonstrates state-of-the-art performance on three challenging datasets. The innovative use of MLLM embeddings and adaptive feature aggregation shows promise in improving generalization to novel compositions.' Weaknesses=\"The paper's presentation could benefit from clearer explanations and diagrams, as some portions, especially regarding Figure 2 and mathematical notations, may be unclear to readers. Details on the training process of key components like the LLM and the image embedder are somewhat ambiguous. Additionally, a deeper discussion on the integration and role of attribute-object disentanglement in the final inference process is needed. The lack of comparisons with the latest works limits an understanding of relative advancements over very recent methods.\" Questions=\"1. Can the authors provide more insight into how the modules used during training, such as attribute-object disentanglement, contribute to the model's final inference performance?\\n2. How does the proposed method compare with the most recent methodologies, like Troika, in terms of performance and generalization capabilities?\\n3. Could the authors clarify the specific training and freezing strategy employed for different modules within Figure 2, especially concerning the MLLM and visual backbone components?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The paper introduces a novel approach by leveraging state-space models (SSMs) in the domain of time-varying counterfactual prediction (TCP), specifically addressing both effectiveness and running efficiency.\\n2. The proposed method, Mamba-CDSP, presents an innovative solution to mitigate confounding bias by de-correlating between current treatments and historical covariates, treatments, and outcomes.\\n3. The experimental results show significant improvement over existing baselines, demonstrating the potential impact of the approach on both synthetic and real-world datasets.\\n4. The paper provides a clear motivation for combining SSMs with TCP, offering a compelling argument for the approach's benefits in long-sequence modeling.\" Weaknesses='1. The reliance on synthetic datasets may limit the ability to generalize findings to real-world scenarios, though such datasets are often necessary for initial validation.\\n2. While the paper discusses running efficiency as a strength, there is limited detailed analysis or comparison of the computational performance against existing models in the results section.\\n3. The explanation of Mamba-CDSP might be dense for some readers due to the complex interplay of various components (e.g., SSMs, de-correlation mechanisms) without visual flow diagrams or step-by-step examples.' Questions='1. Can the authors provide more detail on the computational efficiency of Mamba-CDSP, specifically how it compares in terms of model complexity and time to similar recent approaches in TCP?\\n2. How does the approach handle noise and variability in real-world datasets compared to synthetic examples?\\n3. Could the authors elaborate further on the impact of the de-correlation strategy on the predictive power in comparison to a standard covariate balancing approach?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The paper introduces a novel approach by leveraging state-space models (SSMs) in the domain of time-varying counterfactual prediction (TCP), specifically addressing both effectiveness and running efficiency.\\n2. The proposed method, Mamba-CDSP, presents an innovative solution to mitigate confounding bias by de-correlating between current treatments and historical covariates, treatments, and outcomes.\\n3. The experimental results show significant improvement over existing baselines, demonstrating the potential impact of the approach on both synthetic and real-world datasets.\\n4. The paper provides a clear motivation for combining SSMs with TCP, offering a compelling argument for the approach's benefits in long-sequence modeling.\" Weaknesses='1. The reliance on synthetic datasets may limit the ability to generalize findings to real-world scenarios, though such datasets are often necessary for initial validation.\\n2. While the paper discusses running efficiency as a strength, there is limited detailed analysis or comparison of the computational performance against existing models in the results section.\\n3. The explanation of Mamba-CDSP might be dense for some readers due to the complex interplay of various components (e.g., SSMs, de-correlation mechanisms) without visual flow diagrams or step-by-step examples.' Questions='1. Can the authors provide more detail on the computational efficiency of Mamba-CDSP, specifically how it compares in terms of model complexity and time to similar recent approaches in TCP?\\n2. How does the approach handle noise and variability in real-world datasets compared to synthetic examples?\\n3. Could the authors elaborate further on the impact of the de-correlation strategy on the predictive power in comparison to a standard covariate balancing approach?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper introduces a novel concern with the concept of 'weak-to-strong deception' in the context of superalignment, which is not widely recognized but could have significant implications for the reliability of Large Language Models.\" Weaknesses='While the concept of deception is intriguing, the paper might benefit from a more thorough exploration of potential solutions. The mitigation strategy discussed, involving bootstrapping with an intermediate model, appears limited in effectiveness.' Questions=\"How can the concept of 'weak-to-strong deception' be quantitatively measured in more complex real-world scenarios, beyond the controlled experimental settings discussed?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper introduces a novel concern with the concept of 'weak-to-strong deception' in the context of superalignment, which is not widely recognized but could have significant implications for the reliability of Large Language Models.\" Weaknesses='While the concept of deception is intriguing, the paper might benefit from a more thorough exploration of potential solutions. The mitigation strategy discussed, involving bootstrapping with an intermediate model, appears limited in effectiveness.' Questions=\"How can the concept of 'weak-to-strong deception' be quantitatively measured in more complex real-world scenarios, beyond the controlled experimental settings discussed?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel autoencoder architecture with impressive capabilities for high-resolution diffusion models, achieving significant acceleration without compromising on accuracy. The presentation of Residual Autoencoding and Decoupled High-Resolution Adaptation shows promise in effectively addressing optimization difficulties and generalization penalties associated with high spatial compression ratios. Results on ImageNet demonstrate credible improvements in speedup and Fidelity, emphasizing practical relevance.' Weaknesses='There is insufficient detail provided about potential limitations or scenarios where the proposed methods might not perform well. Additionally, while significant speedup is reported over SD-VAE-f8, a deeper exploration of how DC-AE compares across a broader range of architectures or datasets would enhance the evaluation. The underlying theoretical justifications or analyses for some of the architectural choices, such as space to channel residual learning, could be more robustly discussed. ' Questions='What are the underlying assumptions or potential limitations of the Decoupled High-Resolution Adaptation procedure? Can the approach be generalized across different domains outside of high-resolution models, and are there any anticipated challenges? Are there cases where DC-AE might still underperform against other high-compression autoencoders in practice?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel autoencoder architecture with impressive capabilities for high-resolution diffusion models, achieving significant acceleration without compromising on accuracy. The presentation of Residual Autoencoding and Decoupled High-Resolution Adaptation shows promise in effectively addressing optimization difficulties and generalization penalties associated with high spatial compression ratios. Results on ImageNet demonstrate credible improvements in speedup and Fidelity, emphasizing practical relevance.' Weaknesses='There is insufficient detail provided about potential limitations or scenarios where the proposed methods might not perform well. Additionally, while significant speedup is reported over SD-VAE-f8, a deeper exploration of how DC-AE compares across a broader range of architectures or datasets would enhance the evaluation. The underlying theoretical justifications or analyses for some of the architectural choices, such as space to channel residual learning, could be more robustly discussed. ' Questions='What are the underlying assumptions or potential limitations of the Decoupled High-Resolution Adaptation procedure? Can the approach be generalized across different domains outside of high-resolution models, and are there any anticipated challenges? Are there cases where DC-AE might still underperform against other high-compression autoencoders in practice?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The proposed approach, Heterogeneous-PSRO, addresses the limitations of existing methods by integrating heterogeneous policies to achieve lower exploitability in team games. The theoretical guarantees and empirical validation presented in this paper are solid and can potentially improve strategy exploration in cooperative-competitive settings.' Weaknesses='The paper claims lower exploitability and convergence without introducing additional computational complexity, yet practical computational implications are not well-addressed. Further insights into how the sequential correlation mechanism specifically operates in highly complex games would be beneficial.' Questions='1. How does the model ensure that integration of heterogeneous policies does not lead to a substantial increase in computational cost?\\n2. How scalable is the H-PSRO framework in more complex, real-world simulations beyond the matrix games tested in the experiments?\\n3. Does the proposed solution offer any particular benefits or impacts on non-zero-sum games or variants of zero-sum games where more than two teams are involved?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The proposed approach, Heterogeneous-PSRO, addresses the limitations of existing methods by integrating heterogeneous policies to achieve lower exploitability in team games. The theoretical guarantees and empirical validation presented in this paper are solid and can potentially improve strategy exploration in cooperative-competitive settings.' Weaknesses='The paper claims lower exploitability and convergence without introducing additional computational complexity, yet practical computational implications are not well-addressed. Further insights into how the sequential correlation mechanism specifically operates in highly complex games would be beneficial.' Questions='1. How does the model ensure that integration of heterogeneous policies does not lead to a substantial increase in computational cost?\\n2. How scalable is the H-PSRO framework in more complex, real-world simulations beyond the matrix games tested in the experiments?\\n3. Does the proposed solution offer any particular benefits or impacts on non-zero-sum games or variants of zero-sum games where more than two teams are involved?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. Innovative contribution in providing the first open-source zero-shot singing voice conversion model, capable of converting to human and non-human timbres.\\n2. Effective disentanglement of the singing voice into content, timbre, and pitch features, enhancing the quality of conversion.\\n3. A valuable large-scale dataset comprising 1,815 hours of singing voice data with 6,367 speakers, supporting robust model training and evaluation.' Weaknesses=\"1. The paper may benefit from clearer explanation and discussion of the method's novelty and how it significantly advances beyond existing SVC methodologies.\\n2. More detail is needed on the objective and subjective evaluation metrics used to assess model performance.\\n3. The reliance on multiple ASR models for feature disentanglement might introduce complexities that can affect the model's reproducibility and generalization.\" Questions=\"1. The abstract mentions extreme conditions such as converting singing to animals' timbre. Could you provide more insights into how these experiments were structured and the results obtained?\\n2. How does the model's performance compare to traditional methods in terms of computational efficiency and resource requirements for real-time applications?\\n3. Can you elaborate on the strategies employed to ensure the elimination or minimization of timbre leakage when compressing content features?\"\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. Innovative contribution in providing the first open-source zero-shot singing voice conversion model, capable of converting to human and non-human timbres.\\n2. Effective disentanglement of the singing voice into content, timbre, and pitch features, enhancing the quality of conversion.\\n3. A valuable large-scale dataset comprising 1,815 hours of singing voice data with 6,367 speakers, supporting robust model training and evaluation.' Weaknesses=\"1. The paper may benefit from clearer explanation and discussion of the method's novelty and how it significantly advances beyond existing SVC methodologies.\\n2. More detail is needed on the objective and subjective evaluation metrics used to assess model performance.\\n3. The reliance on multiple ASR models for feature disentanglement might introduce complexities that can affect the model's reproducibility and generalization.\" Questions=\"1. The abstract mentions extreme conditions such as converting singing to animals' timbre. Could you provide more insights into how these experiments were structured and the results obtained?\\n2. How does the model's performance compare to traditional methods in terms of computational efficiency and resource requirements for real-time applications?\\n3. Can you elaborate on the strategies employed to ensure the elimination or minimization of timbre leakage when compressing content features?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The introduction of Differentiable Map Priors is an innovative and simple approach that leverages historical traversal data to improve 3D perception systems in autonomous vehicles. The framework is noted for its ease of integration into existing algorithms and its minimal computational overhead, which strengthens its applicability in real-world scenarios. The experiments conducted are comprehensive and demonstrate consistent improvements across various architectures on established datasets.' Weaknesses=\"While the approach shows promising results, the primary weakness lies in the relative novelty of the method's application. The extent of improvements could be contextual and might not be as impactful in areas where traversal history is less dense or specific. In addition, the extent of robustness in varied and unseen environments could be explored more thoroughly.\" Questions='1. How does the use of Differentiable Map Priors handle locations with sparse historical data? Are there strategies to compensate for such areas?\\n2. Could you elaborate on the scalability of this framework with respect to larger, more diverse datasets?\\n3. Are there any specific limitations or trade-offs identified when integrating this method into existing 3D perception systems?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The introduction of Differentiable Map Priors is an innovative and simple approach that leverages historical traversal data to improve 3D perception systems in autonomous vehicles. The framework is noted for its ease of integration into existing algorithms and its minimal computational overhead, which strengthens its applicability in real-world scenarios. The experiments conducted are comprehensive and demonstrate consistent improvements across various architectures on established datasets.' Weaknesses=\"While the approach shows promising results, the primary weakness lies in the relative novelty of the method's application. The extent of improvements could be contextual and might not be as impactful in areas where traversal history is less dense or specific. In addition, the extent of robustness in varied and unseen environments could be explored more thoroughly.\" Questions='1. How does the use of Differentiable Map Priors handle locations with sparse historical data? Are there strategies to compensate for such areas?\\n2. Could you elaborate on the scalability of this framework with respect to larger, more diverse datasets?\\n3. Are there any specific limitations or trade-offs identified when integrating this method into existing 3D perception systems?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper introduces a novel graph attention mechanism, RGA-IB, which is inspired by the Information Bottleneck principle, showing improved robustness against adversarial attacks in GNNs.\\n2. Extensive experiments demonstrate the effectiveness of RGA-IB in maintaining classification accuracy under adversarial conditions.' Weaknesses='1. While the paper shows promising results, the explanation of how the Information Bottleneck principle is integrated into the graph attention mechanism could be more detailed.\\n2. The paper does not fully address the potential limitations or scalability issues of the proposed method when applied to larger graphs.' Questions='1. Can the proposed RGA-IB mechanism be easily integrated into existing GNN architectures without significant computational overhead?\\n2. How does RGA-IB perform on real-world large-scale graph datasets, in terms of both accuracy and computational cost?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper introduces a novel graph attention mechanism, RGA-IB, which is inspired by the Information Bottleneck principle, showing improved robustness against adversarial attacks in GNNs.\\n2. Extensive experiments demonstrate the effectiveness of RGA-IB in maintaining classification accuracy under adversarial conditions.' Weaknesses='1. While the paper shows promising results, the explanation of how the Information Bottleneck principle is integrated into the graph attention mechanism could be more detailed.\\n2. The paper does not fully address the potential limitations or scalability issues of the proposed method when applied to larger graphs.' Questions='1. Can the proposed RGA-IB mechanism be easily integrated into existing GNN architectures without significant computational overhead?\\n2. How does RGA-IB perform on real-world large-scale graph datasets, in terms of both accuracy and computational cost?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=8 Confidence=3 Strengths='The paper presents a novel deep learning approach for immunogenicity prediction, which seems to outperform existing methods. It introduces a comprehensive dataset and a dual attention mechanism, providing potentially valuable benchmarks for vaccine design.' Weaknesses='The paper lacks clarity on some methodological details, specifically regarding the ablation studies to understand the contributions of different representations used. There are references to tables and metrics not clearly explained or missing, leading to interpretability issues. Additionally, some figures are challenging to interpret.' Questions='Could you clarify the rationale behind using sequence, fine, and coarse representations, and how each contributes to model performance? How does ProVaccine ensure generalizability across different types of pathogens like viruses and bacteria? Can you elaborate on the unexpectedly missing details and appendices referred to in the main text?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=8 Confidence=3 Strengths='The paper presents a novel deep learning approach for immunogenicity prediction, which seems to outperform existing methods. It introduces a comprehensive dataset and a dual attention mechanism, providing potentially valuable benchmarks for vaccine design.' Weaknesses='The paper lacks clarity on some methodological details, specifically regarding the ablation studies to understand the contributions of different representations used. There are references to tables and metrics not clearly explained or missing, leading to interpretability issues. Additionally, some figures are challenging to interpret.' Questions='Could you clarify the rationale behind using sequence, fine, and coarse representations, and how each contributes to model performance? How does ProVaccine ensure generalizability across different types of pathogens like viruses and bacteria? Can you elaborate on the unexpectedly missing details and appendices referred to in the main text?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper presents a novel approach by introducing a computational trust-based discounting method that addresses conflicts in multi-view classification. The concept of using an instance-wise probability-sensitive trust discounting mechanism is unique and potentially valuable in resolving conflicts inherent in multi-view data. The extensive evaluation on six real-world datasets, including the introduction of a new metric, provides a comprehensive assessment of the method's reliability and effectiveness.\" Weaknesses='The methodology may lack clarity due to the complexity of the computational trust-based discounting approach, which might hinder reproducibility. Additionally, while the results show improved reliability, the paper does not fully explore the limitations or potential downsides of the proposed approach, which could affect its applicability in diverse real-world scenarios.' Questions='What specific challenges or limitations might arise when applying the computational trust-based discounting method to domains beyond those used in the experiments?\\nHow does the proposed method compare in terms of computational efficiency with existing multi-view classification techniques?\\nCould you provide more details on how the new metric, Multi-View Agreement with Ground Truth, is constructed and how it specifically contributes to assessing prediction reliability?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper presents a novel approach by introducing a computational trust-based discounting method that addresses conflicts in multi-view classification. The concept of using an instance-wise probability-sensitive trust discounting mechanism is unique and potentially valuable in resolving conflicts inherent in multi-view data. The extensive evaluation on six real-world datasets, including the introduction of a new metric, provides a comprehensive assessment of the method's reliability and effectiveness.\" Weaknesses='The methodology may lack clarity due to the complexity of the computational trust-based discounting approach, which might hinder reproducibility. Additionally, while the results show improved reliability, the paper does not fully explore the limitations or potential downsides of the proposed approach, which could affect its applicability in diverse real-world scenarios.' Questions='What specific challenges or limitations might arise when applying the computational trust-based discounting method to domains beyond those used in the experiments?\\nHow does the proposed method compare in terms of computational efficiency with existing multi-view classification techniques?\\nCould you provide more details on how the new metric, Multi-View Agreement with Ground Truth, is constructed and how it specifically contributes to assessing prediction reliability?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper introduces a novel approach to understanding neuron entanglement using the Wasserstein distance, which is an innovative measure for assessing polysemantic neurons. The proposed framework effectively leverages this distance to disentangle complex input-output relationships and improve performance under weight sparsity. The methodology and experiments provide insightful contributions to the field of interpretability in large language models.' Weaknesses='The paper lacks detailed explanations on some of the methodological aspects, such as how the Wasserstein distance is computed and why it is selected over other metrics. The presentation could be clearer, as some parts are difficult to follow, especially the technical details of the experimental framework. Additionally, the paper does not sufficiently address potential limitations of the proposed approach or its scalability.' Questions='1. How does the proposed measure of Wasserstein distance compare to other existing methods of measuring neuronal entanglement? \\n2. What are the potential limitations or challenges of applying this disentanglement framework to real-world models in production settings?\\n3. How does the performance of the mixture of sparse experts approach vary with different levels of sparsity? Are there scenarios where it fails to maintain accuracy?\\n4. Could the framework be generalized or adapted to other model architectures beyond large language models?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper introduces a novel approach to understanding neuron entanglement using the Wasserstein distance, which is an innovative measure for assessing polysemantic neurons. The proposed framework effectively leverages this distance to disentangle complex input-output relationships and improve performance under weight sparsity. The methodology and experiments provide insightful contributions to the field of interpretability in large language models.' Weaknesses='The paper lacks detailed explanations on some of the methodological aspects, such as how the Wasserstein distance is computed and why it is selected over other metrics. The presentation could be clearer, as some parts are difficult to follow, especially the technical details of the experimental framework. Additionally, the paper does not sufficiently address potential limitations of the proposed approach or its scalability.' Questions='1. How does the proposed measure of Wasserstein distance compare to other existing methods of measuring neuronal entanglement? \\n2. What are the potential limitations or challenges of applying this disentanglement framework to real-world models in production settings?\\n3. How does the performance of the mixture of sparse experts approach vary with different levels of sparsity? Are there scenarios where it fails to maintain accuracy?\\n4. Could the framework be generalized or adapted to other model architectures beyond large language models?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. Introduces a novel application of quantum computations to parameter-efficient fine-tuning.\\n2. Demonstrates significant parameter efficiency gains over traditional methods like LoRA.\\n3. Successfully applies Quantum-PEFT to transfer learning benchmarks in both language and vision tasks, indicating broad applicability.' Weaknesses='1. The paper does not thoroughly explain the computational overhead or potential limitations of employing quantum computations in practical scenarios.\\n2. Limited discussion on how the approach scales with different model sizes, particularly larger-scale models.\\n3. Potential challenges in implementing quantum parameterization in existing classical systems are not addressed.' Questions='1. How do classical systems interact with Quantum-PEFT in terms of computational requirements and implementation complexity?\\n2. Are there any specific types of tasks or models where Quantum-PEFT might not be the optimal approach?\\n3. How does the proposed method compare with the latest advancements in PEFT methodologies, such as those that have been recently introduced or optimized?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. Introduces a novel application of quantum computations to parameter-efficient fine-tuning.\\n2. Demonstrates significant parameter efficiency gains over traditional methods like LoRA.\\n3. Successfully applies Quantum-PEFT to transfer learning benchmarks in both language and vision tasks, indicating broad applicability.' Weaknesses='1. The paper does not thoroughly explain the computational overhead or potential limitations of employing quantum computations in practical scenarios.\\n2. Limited discussion on how the approach scales with different model sizes, particularly larger-scale models.\\n3. Potential challenges in implementing quantum parameterization in existing classical systems are not addressed.' Questions='1. How do classical systems interact with Quantum-PEFT in terms of computational requirements and implementation complexity?\\n2. Are there any specific types of tasks or models where Quantum-PEFT might not be the optimal approach?\\n3. How does the proposed method compare with the latest advancements in PEFT methodologies, such as those that have been recently introduced or optimized?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper addresses the complex problem of bilevel optimization by introducing a novel algorithm, BILBO, which is notable for its sample-efficient approach and its capability to handle decoupled settings. The theoretical foundation is solid, offering a sublinear regret bound, which is a significant achievement in this context. Empirical results bolster the theoretical claims, demonstrating the algorithm's effectiveness across both synthetic and real-world domains.\" Weaknesses='While the presentation is robust, certain assumptions and decisions within the algorithm are not fully justified or explored, such as the specific mechanisms of function query selection. Moreover, the comparative analysis with related work could be more comprehensive, providing deeper insights into how BILBO distinguishes itself from existing methods.' Questions=\"1. How does BILBO perform relative to other leading bilevel optimization methods not outlined in the paper? A more expansive comparative analysis would be informative.\\n2. What are the practical implications of the decoupled setting? Examples illustrating its necessity or benefit would be helpful.\\n3. Could you elaborate on any assumptions made in the theoretical framework that might limit the algorithm's application in specific scenarios?\"\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper addresses the complex problem of bilevel optimization by introducing a novel algorithm, BILBO, which is notable for its sample-efficient approach and its capability to handle decoupled settings. The theoretical foundation is solid, offering a sublinear regret bound, which is a significant achievement in this context. Empirical results bolster the theoretical claims, demonstrating the algorithm's effectiveness across both synthetic and real-world domains.\" Weaknesses='While the presentation is robust, certain assumptions and decisions within the algorithm are not fully justified or explored, such as the specific mechanisms of function query selection. Moreover, the comparative analysis with related work could be more comprehensive, providing deeper insights into how BILBO distinguishes itself from existing methods.' Questions=\"1. How does BILBO perform relative to other leading bilevel optimization methods not outlined in the paper? A more expansive comparative analysis would be informative.\\n2. What are the practical implications of the decoupled setting? Examples illustrating its necessity or benefit would be helpful.\\n3. Could you elaborate on any assumptions made in the theoretical framework that might limit the algorithm's application in specific scenarios?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel multi-agent collaborative framework for data selection, which effectively accelerates LLM training and improves data efficiency. The experimental results show significant improvements over state-of-the-art methods.' Weaknesses='The connection and justification for using a multi-agent approach are not entirely clear, and it seems that a simpler non-agent based model might also achieve similar results. The paper would benefit from a more detailed explanation of how the agent console integrates the information from various agents.' Questions='1. What specific benchmarks were used to evaluate the average performance gain of 10.5%? \\n2. Can you provide more insight into the criteria used by the different data selection agents? \\n3. How does the agent console decide the weightings of different agents during the training process?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel multi-agent collaborative framework for data selection, which effectively accelerates LLM training and improves data efficiency. The experimental results show significant improvements over state-of-the-art methods.' Weaknesses='The connection and justification for using a multi-agent approach are not entirely clear, and it seems that a simpler non-agent based model might also achieve similar results. The paper would benefit from a more detailed explanation of how the agent console integrates the information from various agents.' Questions='1. What specific benchmarks were used to evaluate the average performance gain of 10.5%? \\n2. Can you provide more insight into the criteria used by the different data selection agents? \\n3. How does the agent console decide the weightings of different agents during the training process?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper presents a comprehensive empirical study on data scaling in imitation learning for robotic manipulation, providing insights into how data scaling can affect generalization performance. The rigorous evaluation protocol involving 40,000 demonstrations and 15,000 robot rollouts is a significant strength, highlighting the method's robustness and potential for real-world application. The proposal of an efficient data collection strategy is practical and shows promise for scalability.\" Weaknesses='While the study is extensive, it focuses primarily on two tasks, which might limit the generality of the insights gained. The reported threshold beyond which additional demonstrations have minimal effect may not apply universally across more complex or varied tasks. The novelty of the contribution could be challenged, as it is unclear whether the revealed scaling laws are significantly different from those already understood in other domains. Additionally, more details on the nature of the tasks and environments would strengthen the claims.' Questions='1. How do the identified data scaling laws compare to those in other fields like NLP or vision? Are there any fundamental differences that should be highlighted?\\n2. Could the data collection strategy be generalized to more complex tasks without a significant increase in cost or effort?\\n3. How does the diversity of environments and objects translate to tasks beyond the ones studied? Would similar scaling apply to tasks involving more diverse object categories or more complex manipulations?\\n4. What are the specific limitations of the current approach in handling tasks with drastically different dynamics or requiring fine-grained manipulation?\\n5. Can the findings regarding the threshold for demonstrations be reliably applied to guide data collection efforts in other robotic manipulation tasks?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper presents a comprehensive empirical study on data scaling in imitation learning for robotic manipulation, providing insights into how data scaling can affect generalization performance. The rigorous evaluation protocol involving 40,000 demonstrations and 15,000 robot rollouts is a significant strength, highlighting the method's robustness and potential for real-world application. The proposal of an efficient data collection strategy is practical and shows promise for scalability.\" Weaknesses='While the study is extensive, it focuses primarily on two tasks, which might limit the generality of the insights gained. The reported threshold beyond which additional demonstrations have minimal effect may not apply universally across more complex or varied tasks. The novelty of the contribution could be challenged, as it is unclear whether the revealed scaling laws are significantly different from those already understood in other domains. Additionally, more details on the nature of the tasks and environments would strengthen the claims.' Questions='1. How do the identified data scaling laws compare to those in other fields like NLP or vision? Are there any fundamental differences that should be highlighted?\\n2. Could the data collection strategy be generalized to more complex tasks without a significant increase in cost or effort?\\n3. How does the diversity of environments and objects translate to tasks beyond the ones studied? Would similar scaling apply to tasks involving more diverse object categories or more complex manipulations?\\n4. What are the specific limitations of the current approach in handling tasks with drastically different dynamics or requiring fine-grained manipulation?\\n5. Can the findings regarding the threshold for demonstrations be reliably applied to guide data collection efforts in other robotic manipulation tasks?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The approach provides a novel framework for self-evolving agents that utilize LLMs, which potentially explore a broader design space beyond manually crafted agents. The methodology could significantly impact how agents are designed and developed, particularly in fields requiring adaptive and evolving strategies.' Weaknesses='The concept of allowing agents to dynamically modify their own logic and behavior raises concerns regarding stability, safety, and predictability. The paper might not address possible risks, ethics, and limitations associated with uncontrolled evolutionary processes in AI systems. More experimental details and ablation studies could strengthen validation.' Questions='How does the Gödel Agent ensure safety and prevent destructive modifications during self-improvement? Are there specific mechanisms to guide or limit the self-evolution process to avoid undesirable outcomes?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The approach provides a novel framework for self-evolving agents that utilize LLMs, which potentially explore a broader design space beyond manually crafted agents. The methodology could significantly impact how agents are designed and developed, particularly in fields requiring adaptive and evolving strategies.' Weaknesses='The concept of allowing agents to dynamically modify their own logic and behavior raises concerns regarding stability, safety, and predictability. The paper might not address possible risks, ethics, and limitations associated with uncontrolled evolutionary processes in AI systems. More experimental details and ablation studies could strengthen validation.' Questions='How does the Gödel Agent ensure safety and prevent destructive modifications during self-improvement? Are there specific mechanisms to guide or limit the self-evolution process to avoid undesirable outcomes?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. Introduction of a novel, diverse benchmark for evaluating mathematical problem-solving in LLMs, addressing current limitations in existing benchmarks.\\n2. Includes a substantial number of open-ended, university-level problems, adding variety and complexity to the assessment of mathematical skills in language models.\\n3. The inclusion of a multimodal component (20% visual problems) pushes the boundaries of current LLM evaluations by incorporating visual reasoning.\\n4. Open-sourcing the dataset and the evaluation code promotes transparency and encourages further research and development in this area.' Weaknesses='1. The soundness of the methodology for using an LLM to judge the correctness of generated solutions may be questionable, as it introduces additional complexity and potential biases in evaluation.\\n2. While the benchmark is diverse, the reliance on only teaching materials may not fully encapsulate novel or emergent mathematical problems seen outside academic settings.\\n3. Limited maximum accuracy rates (63% text-based, 45% visual) in LLMs suggest that the dataset might be too challenging or that the capabilities of current models are still lacking significantly.\\n4. The paper could provide more details on the specific challenges faced by LLMs on the visual component to suggest clear research directions.' Questions='1. How does the use of teaching materials ensure that the problems are representative of typical challenges faced in real-world applications of university-level mathematics?\\n2. What specific steps were taken to ensure the unbiased evaluation of solutions using LLMs as judges, given the challenges in understanding the breadth of mathematical solutions?\\n3. How could the benchmark be adapted to further include or emphasize real-world applications of mathematics beyond academic exercises?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. Introduction of a novel, diverse benchmark for evaluating mathematical problem-solving in LLMs, addressing current limitations in existing benchmarks.\\n2. Includes a substantial number of open-ended, university-level problems, adding variety and complexity to the assessment of mathematical skills in language models.\\n3. The inclusion of a multimodal component (20% visual problems) pushes the boundaries of current LLM evaluations by incorporating visual reasoning.\\n4. Open-sourcing the dataset and the evaluation code promotes transparency and encourages further research and development in this area.' Weaknesses='1. The soundness of the methodology for using an LLM to judge the correctness of generated solutions may be questionable, as it introduces additional complexity and potential biases in evaluation.\\n2. While the benchmark is diverse, the reliance on only teaching materials may not fully encapsulate novel or emergent mathematical problems seen outside academic settings.\\n3. Limited maximum accuracy rates (63% text-based, 45% visual) in LLMs suggest that the dataset might be too challenging or that the capabilities of current models are still lacking significantly.\\n4. The paper could provide more details on the specific challenges faced by LLMs on the visual component to suggest clear research directions.' Questions='1. How does the use of teaching materials ensure that the problems are representative of typical challenges faced in real-world applications of university-level mathematics?\\n2. What specific steps were taken to ensure the unbiased evaluation of solutions using LLMs as judges, given the challenges in understanding the breadth of mathematical solutions?\\n3. How could the benchmark be adapted to further include or emphasize real-world applications of mathematics beyond academic exercises?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper provides a new benchmark, GSM-Symbolic, for evaluating LLMs on mathematical reasoning, which allows for more nuanced insights into the models' capabilities and shortcomings.\" Weaknesses='The study focuses heavily on existing limitations of LLMs without proposing significant methodologies to address them, potentially limiting the practical impact of the findings.' Questions='How can the insights gained from using GSM-Symbolic be leveraged to improve the mathematical reasoning capabilities of LLMs beyond highlighting their limitations?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper provides a new benchmark, GSM-Symbolic, for evaluating LLMs on mathematical reasoning, which allows for more nuanced insights into the models' capabilities and shortcomings.\" Weaknesses='The study focuses heavily on existing limitations of LLMs without proposing significant methodologies to address them, potentially limiting the practical impact of the findings.' Questions='How can the insights gained from using GSM-Symbolic be leveraged to improve the mathematical reasoning capabilities of LLMs beyond highlighting their limitations?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel approach to quantifying diversity in text-conditioned generative models by introducing the Conditional-Vendi and Information-Vendi scores. The theoretical grounding and decomposition of the Vendi diversity score offer a solid interpretation of the concept of diversity in the context of generative models. The methodology is well-structured, and the paper provides comprehensive experiments that demonstrate the effectiveness of the proposed metrics.' Weaknesses='The paper is quite technical, which may limit its accessibility to a broader audience unfamiliar with information-theoretic concepts. There could be more details about the experiments, such as the specific datasets used, to fully understand the implications and applicability of the proposed methods. Additionally, the practical advantages of adopting these new diversity scores over traditional ones are not thoroughly discussed, leaving the reader to infer the benefits.' Questions='1. How sensitive are the proposed diversity scores to the choice of kernel used in the matrix-based information measures?\\n2. Can the methodology be extended to other modalities beyond text-conditioned generative tasks, such as audio or multimodal systems?\\n3. What are the computational implications of using the proposed scores compared to existing diversity measures? Are there any efficiency concerns?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel approach to quantifying diversity in text-conditioned generative models by introducing the Conditional-Vendi and Information-Vendi scores. The theoretical grounding and decomposition of the Vendi diversity score offer a solid interpretation of the concept of diversity in the context of generative models. The methodology is well-structured, and the paper provides comprehensive experiments that demonstrate the effectiveness of the proposed metrics.' Weaknesses='The paper is quite technical, which may limit its accessibility to a broader audience unfamiliar with information-theoretic concepts. There could be more details about the experiments, such as the specific datasets used, to fully understand the implications and applicability of the proposed methods. Additionally, the practical advantages of adopting these new diversity scores over traditional ones are not thoroughly discussed, leaving the reader to infer the benefits.' Questions='1. How sensitive are the proposed diversity scores to the choice of kernel used in the matrix-based information measures?\\n2. Can the methodology be extended to other modalities beyond text-conditioned generative tasks, such as audio or multimodal systems?\\n3. What are the computational implications of using the proposed scores compared to existing diversity measures? Are there any efficiency concerns?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper proposes a novel approach for improving reverse-time sampling in image reconstruction tasks, specifically targeting scenarios with high measurement noise or severe ill-posedness. The presented method, Flow with Interpolant Guidance (FIG), demonstrates high competitiveness and improvement over state-of-the-art algorithms, particularly in challenging scenarios. Theoretical justification and empirical evidence are well-documented, showcasing the algorithm's effectiveness.\" Weaknesses='The presentation of the paper could be enhanced to better convey the motivation and underlying intuition for adopting measurement interpolants. While the theoretical basis is provided, the explanation of why the proposed method is advantageous over existing methods in tackling specific limitations is not thoroughly detailed. The practical implications of the theoretical aspects could be more clearly articulated.' Questions='How do measurement interpolants specifically improve reverse-time sampling in cases of high noise or severe ill-posedness? Could the authors further elaborate on the practical advantages of the proposed method compared to existing approaches, especially in terms of computational complexity and scalability in various real-world applications?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper proposes a novel approach for improving reverse-time sampling in image reconstruction tasks, specifically targeting scenarios with high measurement noise or severe ill-posedness. The presented method, Flow with Interpolant Guidance (FIG), demonstrates high competitiveness and improvement over state-of-the-art algorithms, particularly in challenging scenarios. Theoretical justification and empirical evidence are well-documented, showcasing the algorithm's effectiveness.\" Weaknesses='The presentation of the paper could be enhanced to better convey the motivation and underlying intuition for adopting measurement interpolants. While the theoretical basis is provided, the explanation of why the proposed method is advantageous over existing methods in tackling specific limitations is not thoroughly detailed. The practical implications of the theoretical aspects could be more clearly articulated.' Questions='How do measurement interpolants specifically improve reverse-time sampling in cases of high noise or severe ill-posedness? Could the authors further elaborate on the practical advantages of the proposed method compared to existing approaches, especially in terms of computational complexity and scalability in various real-world applications?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The proposed framework, SecureGS, provides an innovative approach to addressing the challenges of 3D Gaussian splatting steganography by enhancing security and rendering fidelity, while maintaining efficiency. The hybrid decoupled Gaussian encryption mechanism and density region-aware anchor growing and pruning strategy offer novel and practical solutions. Extensive experiments validate the superiority of SecureGS over existing methods, making it a significant contribution to the field.' Weaknesses='The complexity of the proposed SecureGS framework and its various components may pose challenges for practical implementation and adoption. Additionally, while the experiments are extensive, the specific computational costs associated with achieving the reported enhancements in security and rendering fidelity are not explicitly detailed.' Questions='1. Could the authors elaborate on the computational overhead introduced by SecureGS and how it compares to traditional GS steganography methods?\\n2. How does SecureGS perform in real-time scenarios, and are there any specific limitations when deploying it in such environments?\\n3. What are the potential trade-offs in terms of rendering fidelity when maximizing security features using the proposed framework?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The proposed framework, SecureGS, provides an innovative approach to addressing the challenges of 3D Gaussian splatting steganography by enhancing security and rendering fidelity, while maintaining efficiency. The hybrid decoupled Gaussian encryption mechanism and density region-aware anchor growing and pruning strategy offer novel and practical solutions. Extensive experiments validate the superiority of SecureGS over existing methods, making it a significant contribution to the field.' Weaknesses='The complexity of the proposed SecureGS framework and its various components may pose challenges for practical implementation and adoption. Additionally, while the experiments are extensive, the specific computational costs associated with achieving the reported enhancements in security and rendering fidelity are not explicitly detailed.' Questions='1. Could the authors elaborate on the computational overhead introduced by SecureGS and how it compares to traditional GS steganography methods?\\n2. How does SecureGS perform in real-time scenarios, and are there any specific limitations when deploying it in such environments?\\n3. What are the potential trade-offs in terms of rendering fidelity when maximizing security features using the proposed framework?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents a novel approach with a hierarchical graph network to effectively handle Irregular Multivariate Time Series. The model not only handles varying scales and sampling rates across variables but also outperforms state-of-the-art methods in forecasting and classification tasks. The proposed Hi-Patch network demonstrates robust performance improvements over established baselines.' Weaknesses='The approach, while innovative, could be computationally intensive, given the fully connected graph networks and hierarchical architectures. The paper does not provide a detailed complexity analysis or discuss potential limitations in scalability. Presentation-wise, certain aspects of the hierarchical structure could be elucidated further for clarity.' Questions='How does the model handle missing data or extreme irregularities in time series? How scalable is the proposed approach when applied to very large datasets or real-time data streams? Are there specific downstream tasks where the performance benefits are more pronounced?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents a novel approach with a hierarchical graph network to effectively handle Irregular Multivariate Time Series. The model not only handles varying scales and sampling rates across variables but also outperforms state-of-the-art methods in forecasting and classification tasks. The proposed Hi-Patch network demonstrates robust performance improvements over established baselines.' Weaknesses='The approach, while innovative, could be computationally intensive, given the fully connected graph networks and hierarchical architectures. The paper does not provide a detailed complexity analysis or discuss potential limitations in scalability. Presentation-wise, certain aspects of the hierarchical structure could be elucidated further for clarity.' Questions='How does the model handle missing data or extreme irregularities in time series? How scalable is the proposed approach when applied to very large datasets or real-time data streams? Are there specific downstream tasks where the performance benefits are more pronounced?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses a significant gap in current watermarking research for large language models by introducing imperceptibility as a key factor. The proposed Water-Probe algorithm effectively identifies existing watermarks and offers a novel approach, Water-Bag, to improve watermark imperceptibility, which is critical to maintaining user trust and avoiding attacks.' Weaknesses=\"The paper's discussion on the limitations of existing watermarking techniques could be expanded to include more diverse techniques beyond mainstream ones. Additionally, more details on the implementation and evaluation of the Water-Bag strategy are needed to fully understand its impact and practicality.\" Questions='How does the Water-Bag strategy impact the computational efficiency of watermarking processes, and are there scenarios where its implementation may not be practical? Additionally, how does the effectiveness of Water-Probe correlate with the complexity of the watermarking techniques applied?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses a significant gap in current watermarking research for large language models by introducing imperceptibility as a key factor. The proposed Water-Probe algorithm effectively identifies existing watermarks and offers a novel approach, Water-Bag, to improve watermark imperceptibility, which is critical to maintaining user trust and avoiding attacks.' Weaknesses=\"The paper's discussion on the limitations of existing watermarking techniques could be expanded to include more diverse techniques beyond mainstream ones. Additionally, more details on the implementation and evaluation of the Water-Bag strategy are needed to fully understand its impact and practicality.\" Questions='How does the Water-Bag strategy impact the computational efficiency of watermarking processes, and are there scenarios where its implementation may not be practical? Additionally, how does the effectiveness of Water-Probe correlate with the complexity of the watermarking techniques applied?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents a robust formalism for understanding and developing direct preference alignment (DPA) loss functions, shedding light on the semantic underpinnings of these algorithms. The work offers a novel perspective on rigorously characterizing DPA losses and the relationships between recent proposals, which could lead to the creation of more effective AI alignment strategies.' Weaknesses='The technical depth and novelty of the framework may require readers to have a strong background in symbolic reasoning and discrete mathematics, potentially limiting accessibility to a broader audience. Additionally, specific comparative evaluations of the proposed formalism with alternative frameworks would enhance the clarity and strength of claims regarding its utility.' Questions='How do the derived symbolic expressions compare with traditional loss functions in practice, particularly in terms of computational efficiency and performance in aligning models with human preferences? Could the formalism be applied beyond existing DPA variants to entirely new domains or types of models?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper presents a robust formalism for understanding and developing direct preference alignment (DPA) loss functions, shedding light on the semantic underpinnings of these algorithms. The work offers a novel perspective on rigorously characterizing DPA losses and the relationships between recent proposals, which could lead to the creation of more effective AI alignment strategies.' Weaknesses='The technical depth and novelty of the framework may require readers to have a strong background in symbolic reasoning and discrete mathematics, potentially limiting accessibility to a broader audience. Additionally, specific comparative evaluations of the proposed formalism with alternative frameworks would enhance the clarity and strength of claims regarding its utility.' Questions='How do the derived symbolic expressions compare with traditional loss functions in practice, particularly in terms of computational efficiency and performance in aligning models with human preferences? Could the formalism be applied beyond existing DPA variants to entirely new domains or types of models?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper introduces novel modules for improving audio-driven talking video generation, such as the memory-guided temporal module for identity consistency and smooth motion, and the emotion-aware audio module to enhance audio-video interaction. It provides a robust solution to current challenges in ensuring seamless audio-lip synchronization and expressive animations. The model's ability to perform well without relying on traditional facial biases like landmarks and bounding boxes is an impressive aspect. Additionally, the paper is well-structured and includes extensive experiments demonstrating superior performance compared to state-of-the-art methods.\" Weaknesses='The paper may not have thoroughly addressed the computational complexity and resource requirements of the proposed method, which could be crucial for real-world applications. Furthermore, the effectiveness of the approach on diverse facial structures, ethnicities, and challenging audio inputs might require further exploration to ensure generalizability. It is also important to understand the limitations of the emotion detection mechanism and how it might affect the output quality.' Questions='1. How does the memory-guided module scale with input length and does it have implications on memory consumption or processing time?\\n2. What are the challenges in training the model, and how long does a typical training session take on available hardware?\\n3. Does the model exhibit any bias or performance disparity when evaluated across different datasets not mentioned in the paper, such as those with underrepresented demographics?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper introduces novel modules for improving audio-driven talking video generation, such as the memory-guided temporal module for identity consistency and smooth motion, and the emotion-aware audio module to enhance audio-video interaction. It provides a robust solution to current challenges in ensuring seamless audio-lip synchronization and expressive animations. The model's ability to perform well without relying on traditional facial biases like landmarks and bounding boxes is an impressive aspect. Additionally, the paper is well-structured and includes extensive experiments demonstrating superior performance compared to state-of-the-art methods.\" Weaknesses='The paper may not have thoroughly addressed the computational complexity and resource requirements of the proposed method, which could be crucial for real-world applications. Furthermore, the effectiveness of the approach on diverse facial structures, ethnicities, and challenging audio inputs might require further exploration to ensure generalizability. It is also important to understand the limitations of the emotion detection mechanism and how it might affect the output quality.' Questions='1. How does the memory-guided module scale with input length and does it have implications on memory consumption or processing time?\\n2. What are the challenges in training the model, and how long does a typical training session take on available hardware?\\n3. Does the model exhibit any bias or performance disparity when evaluated across different datasets not mentioned in the paper, such as those with underrepresented demographics?'\n",
      "Finished gpt4o on data/test_data/few_shot/test_data_2025_abstract_prompts_fewshot.jsonl, saved => results/gpt4o/test_data_2025_abstract_prompts_fewshot_gpt4o_results.jsonl\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The proposed TTformer model introduces a novel approach by utilizing tense-specific attention modules to address spatio-temporal dependencies, which is a fresh perspective in the domain of traffic forecasting.\\n2. Integration of contrastive learning techniques to handle incomplete data is a strong aspect of the paper, potentially improving robustness.\\n3. The model has shown improved performance over baseline models on real-world traffic datasets, indicating practical applicability.' Weaknesses='1. The complexity of the model might limit its scalability and practical deployment due to increased computational costs associated with the tense-specific modules and contrastive learning.\\n2. The paper lacks a detailed discussion on the underlying mechanisms of the tense-specific attention modules, which can make it challenging for readers to understand the distinct contributions of each module.\\n3. There might be a dependency on hyperparameter tuning for optimal performance, which is not thoroughly explored or documented in the report.' Questions='1. How does the model handle scenarios where the traffic data is extremely sparse or highly noisy? Are there any specific strategies employed for such cases?\\n2. Can the proposed method be extended to other domains that require spatio-temporal analysis, or is it specifically tailored for traffic forecasting?\\n3. What are the main computational overheads introduced by the TTformer, and how do they compare to those of existing models, particularly in terms of inference time?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The proposed TTformer model introduces a novel approach by utilizing tense-specific attention modules to address spatio-temporal dependencies, which is a fresh perspective in the domain of traffic forecasting.\\n2. Integration of contrastive learning techniques to handle incomplete data is a strong aspect of the paper, potentially improving robustness.\\n3. The model has shown improved performance over baseline models on real-world traffic datasets, indicating practical applicability.' Weaknesses='1. The complexity of the model might limit its scalability and practical deployment due to increased computational costs associated with the tense-specific modules and contrastive learning.\\n2. The paper lacks a detailed discussion on the underlying mechanisms of the tense-specific attention modules, which can make it challenging for readers to understand the distinct contributions of each module.\\n3. There might be a dependency on hyperparameter tuning for optimal performance, which is not thoroughly explored or documented in the report.' Questions='1. How does the model handle scenarios where the traffic data is extremely sparse or highly noisy? Are there any specific strategies employed for such cases?\\n2. Can the proposed method be extended to other domains that require spatio-temporal analysis, or is it specifically tailored for traffic forecasting?\\n3. What are the main computational overheads introduced by the TTformer, and how do they compare to those of existing models, particularly in terms of inference time?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The Overcooked Generalisation Challenge (OGC) effectively addresses the significant limitation in existing benchmarks by focusing on the generalisation capabilities of RL agents. The paper successfully introduces a novel environment and evaluation scenario for cooperative multi-agent RL, emphasizing zero-shot cooperation abilities. The use of unsupervised environment design (UED) and incorporation of diverse dual curriculum design methods manifest innovation and advancement in benchmark development.' Weaknesses='While the introduction of OGC is commendable, the paper lacks comprehensive detail on the observation space and potential limitations regarding assumptions of environment conditions. There is a need for deeper analysis and explanation of why state-of-the-art DCD algorithms are struggling with the OGC benchmark. The paper could benefit from more detailed experimental results, including performance on different unseen layouts and an analysis of training curves.' Questions='What specific measures are taken to ensure that the OGC benchmark remains relevant as AI capabilities advance? Additionally, how can the community contribute to updating or expanding the challenge in the future? Further, could you provide more details about the observation space, particularly for collaborators less familiar with Overcooked?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The Overcooked Generalisation Challenge (OGC) effectively addresses the significant limitation in existing benchmarks by focusing on the generalisation capabilities of RL agents. The paper successfully introduces a novel environment and evaluation scenario for cooperative multi-agent RL, emphasizing zero-shot cooperation abilities. The use of unsupervised environment design (UED) and incorporation of diverse dual curriculum design methods manifest innovation and advancement in benchmark development.' Weaknesses='While the introduction of OGC is commendable, the paper lacks comprehensive detail on the observation space and potential limitations regarding assumptions of environment conditions. There is a need for deeper analysis and explanation of why state-of-the-art DCD algorithms are struggling with the OGC benchmark. The paper could benefit from more detailed experimental results, including performance on different unseen layouts and an analysis of training curves.' Questions='What specific measures are taken to ensure that the OGC benchmark remains relevant as AI capabilities advance? Additionally, how can the community contribute to updating or expanding the challenge in the future? Further, could you provide more details about the observation space, particularly for collaborators less familiar with Overcooked?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The LongPack data pipeline innovatively leverages natural referral relationships, such as hyperlinks, to better simulate the natural complexity of long-document contexts. This approach addresses a noted gap in the training of language models by circumventing the ineffective concatenation of short documents, instead creating richer, long-context training data. The strategy demonstrates a significant performance improvement (32.7% higher score) over previous methods, underscoring its potential value for enhancing language model training efficiency.' Weaknesses=\"The paper could benefit from additional clarity in presenting how LongPack's document assembly directly translates to improved model performance. More details on the selection criteria or weighting of referral signals might strengthen the argument for the pipeline's robustness. Additionally, while LongPack emphasizes hyperlink data, the approach may not be directly applicable to datasets with fewer explicit referral structures, potentially limiting its applicability across diverse domains.\" Questions='How does LongPack handle potential noise or irrelevant content that may be introduced when concatenating documents through referral relationships? Are there specific filtering or refinement processes in place to ensure the quality of constructed long documents?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The LongPack data pipeline innovatively leverages natural referral relationships, such as hyperlinks, to better simulate the natural complexity of long-document contexts. This approach addresses a noted gap in the training of language models by circumventing the ineffective concatenation of short documents, instead creating richer, long-context training data. The strategy demonstrates a significant performance improvement (32.7% higher score) over previous methods, underscoring its potential value for enhancing language model training efficiency.' Weaknesses=\"The paper could benefit from additional clarity in presenting how LongPack's document assembly directly translates to improved model performance. More details on the selection criteria or weighting of referral signals might strengthen the argument for the pipeline's robustness. Additionally, while LongPack emphasizes hyperlink data, the approach may not be directly applicable to datasets with fewer explicit referral structures, potentially limiting its applicability across diverse domains.\" Questions='How does LongPack handle potential noise or irrelevant content that may be introduced when concatenating documents through referral relationships? Are there specific filtering or refinement processes in place to ensure the quality of constructed long documents?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. T-JEPA provides a novel approach to self-supervised learning for tabular data without relying on data augmentations, which is a notable achievement given the challenges in this domain.\\n2. The method successfully integrates a joint embedding predictive architecture that allows for effective feature identification and performance improvements on various downstream tasks.\\n3. Implementation includes an innovative use of regularization tokens to prevent representation collapse, highlighting an effective training strategy.' Weaknesses='1. The paper lacks a detailed theoretical justification on how the regularization tokens specifically prevent representation collapse during training.\\n2. Comparison with existing tabular self-supervised learning methods could be more extensive to better contextualize improvements in performance.\\n3. More clarity is needed in some sections of the methodology, which could help in better understanding the implementation nuances.' Questions='1. How does the T-JEPA approach compare in computational efficiency with other current state-of-the-art self-supervised learning methods for tabular data?\\n2. What criteria were used to select the particular datasets for experimentation, and how might performance differ on other datasets with different characteristics?\\n3. Can the method be adapted for semi-supervised learning scenarios, where some labels are available, to potentially enhance the performance further?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. T-JEPA provides a novel approach to self-supervised learning for tabular data without relying on data augmentations, which is a notable achievement given the challenges in this domain.\\n2. The method successfully integrates a joint embedding predictive architecture that allows for effective feature identification and performance improvements on various downstream tasks.\\n3. Implementation includes an innovative use of regularization tokens to prevent representation collapse, highlighting an effective training strategy.' Weaknesses='1. The paper lacks a detailed theoretical justification on how the regularization tokens specifically prevent representation collapse during training.\\n2. Comparison with existing tabular self-supervised learning methods could be more extensive to better contextualize improvements in performance.\\n3. More clarity is needed in some sections of the methodology, which could help in better understanding the implementation nuances.' Questions='1. How does the T-JEPA approach compare in computational efficiency with other current state-of-the-art self-supervised learning methods for tabular data?\\n2. What criteria were used to select the particular datasets for experimentation, and how might performance differ on other datasets with different characteristics?\\n3. Can the method be adapted for semi-supervised learning scenarios, where some labels are available, to potentially enhance the performance further?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses an important challenge in machine learning, which is improving data representations in tabular data for few-shot learning.\\n2. The introduction of a range-limited augmentation method is a novel approach that effectively enhances the semantic consistency of augmented data.\\n3. The use of a comprehensive benchmark with a diverse set of datasets (FeSTa) is a strong point, providing a standardized basis for comparison.' Weaknesses='1. The scope of the proposed approach is limited to tabular data, which may not directly extend to other data modalities like text or image.\\n2. The clarity of problem definition and differentiation from existing few-shot learning methods across different domains could be improved.\\n3. The novelty of the range-limited strategy may be questioned, as it closely resembles existing augmentation strategies with minor adaptations specific to tabular data.' Questions='1. How does the range-limited augmentation strategy perform when applied to domains other than tabular data?\\n2. What are the computational implications of implementing this range-limited approach in terms of training time and resource efficiency compared to traditional methods?\\n3. How does this approach scale with increased complexity or dimensionality of the tabular data involved?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses an important challenge in machine learning, which is improving data representations in tabular data for few-shot learning.\\n2. The introduction of a range-limited augmentation method is a novel approach that effectively enhances the semantic consistency of augmented data.\\n3. The use of a comprehensive benchmark with a diverse set of datasets (FeSTa) is a strong point, providing a standardized basis for comparison.' Weaknesses='1. The scope of the proposed approach is limited to tabular data, which may not directly extend to other data modalities like text or image.\\n2. The clarity of problem definition and differentiation from existing few-shot learning methods across different domains could be improved.\\n3. The novelty of the range-limited strategy may be questioned, as it closely resembles existing augmentation strategies with minor adaptations specific to tabular data.' Questions='1. How does the range-limited augmentation strategy perform when applied to domains other than tabular data?\\n2. What are the computational implications of implementing this range-limited approach in terms of training time and resource efficiency compared to traditional methods?\\n3. How does this approach scale with increased complexity or dimensionality of the tabular data involved?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=3 Strengths='The framework effectively addresses key challenges in multi-agent systems, such as task decomposition and allocation inefficiencies. The integration of a meta-agent with LLM capabilities for fast task decomposition and a reward model for evaluating sub-tasks is innovative and demonstrates clear improvements over existing methods. The feedback loop for continuous improvement is a strong aspect of the design.' Weaknesses=\"The paper may lack detailed mathematical rigor in explaining the underlying models and the reward system. The implementation specifics, especially regarding scalability and potential constraints of the framework, might not be fully detailed. While empirical results show advancements, the paper might benefit from more diverse dataset evaluations and real-world application scenarios to fully establish the method's robustness.\" Questions='How adaptable is the framework to rapidly changing environments or tasks that require real-time adaptation? \\n\\nWhat are the computational overheads associated with the proposed reward model and feedback loop integration, especially in large-scale multi-agent environments?\\n\\nCould you provide more details on how the feedback loop influences the continuous learning process of the meta-agent? \\n\\nAre there any potential limitations identified during experiments, particularly regarding solvability and non-redundancy checks?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=3 Strengths='The framework effectively addresses key challenges in multi-agent systems, such as task decomposition and allocation inefficiencies. The integration of a meta-agent with LLM capabilities for fast task decomposition and a reward model for evaluating sub-tasks is innovative and demonstrates clear improvements over existing methods. The feedback loop for continuous improvement is a strong aspect of the design.' Weaknesses=\"The paper may lack detailed mathematical rigor in explaining the underlying models and the reward system. The implementation specifics, especially regarding scalability and potential constraints of the framework, might not be fully detailed. While empirical results show advancements, the paper might benefit from more diverse dataset evaluations and real-world application scenarios to fully establish the method's robustness.\" Questions='How adaptable is the framework to rapidly changing environments or tasks that require real-time adaptation? \\n\\nWhat are the computational overheads associated with the proposed reward model and feedback loop integration, especially in large-scale multi-agent environments?\\n\\nCould you provide more details on how the feedback loop influences the continuous learning process of the meta-agent? \\n\\nAre there any potential limitations identified during experiments, particularly regarding solvability and non-redundancy checks?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"TransFusion provides a practical approach to improve performance of LLMs for information extraction tasks in low-resource languages, addressing a significant gap. The inclusion of English translations using high-resource models helps in leveraging the wealth of pre-training in English. The method's flexibility to adapt to existing LLM architectures like GPT-4 enhances its appeal.\" Weaknesses='The methodology relies heavily on the quality and accuracy of machine translation from low-resource languages to English, which might not always be reliable. While TransFusion enhances performance, the novelty might be considered incremental given the current landscape of translation-based methods in NLP.' Questions='How does this method perform when translations are noisy or inaccurate? Is there a significant performance drop compared to high-resource scenarios?\\nHas the method been evaluated in real-world, non-benchmark environments to test robustness in diverse settings?\\nHow does the complexity and computational cost of TransFusion compare with traditional cross-lingual transfer techniques?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"TransFusion provides a practical approach to improve performance of LLMs for information extraction tasks in low-resource languages, addressing a significant gap. The inclusion of English translations using high-resource models helps in leveraging the wealth of pre-training in English. The method's flexibility to adapt to existing LLM architectures like GPT-4 enhances its appeal.\" Weaknesses='The methodology relies heavily on the quality and accuracy of machine translation from low-resource languages to English, which might not always be reliable. While TransFusion enhances performance, the novelty might be considered incremental given the current landscape of translation-based methods in NLP.' Questions='How does this method perform when translations are noisy or inaccurate? Is there a significant performance drop compared to high-resource scenarios?\\nHas the method been evaluated in real-world, non-benchmark environments to test robustness in diverse settings?\\nHow does the complexity and computational cost of TransFusion compare with traditional cross-lingual transfer techniques?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The paper offers a novel differentiable causal discovery method that effectively addresses the limitations of previous approaches by supporting nonlinear models and latent variables without assuming deterministic relations. The theoretical and empirical results demonstrate significant improvements in accuracy and scalability, with successful application to challenging datasets like MNIST.' Weaknesses='While the methodology is robust and innovative, the presentation could be enhanced, especially regarding visualization and interpretation of results. The description of synthetic experiments is not sufficiently detailed and could benefit from more comprehensive baseline comparisons.' Questions='How do the newly established identifiability conditions compare to existing ones in terms of practical applicability? Are there any specific domains or types of data where the proposed method does not perform as well?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='The paper offers a novel differentiable causal discovery method that effectively addresses the limitations of previous approaches by supporting nonlinear models and latent variables without assuming deterministic relations. The theoretical and empirical results demonstrate significant improvements in accuracy and scalability, with successful application to challenging datasets like MNIST.' Weaknesses='While the methodology is robust and innovative, the presentation could be enhanced, especially regarding visualization and interpretation of results. The description of synthetic experiments is not sufficiently detailed and could benefit from more comprehensive baseline comparisons.' Questions='How do the newly established identifiability conditions compare to existing ones in terms of practical applicability? Are there any specific domains or types of data where the proposed method does not perform as well?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The Spatial 3D-LLM model introduces a novel architecture that enhances spatial reasoning in 3D vision-language tasks. The progressive spatial awareness scheme is innovative and effectively captures multi-level spatial information. The newly constructed 3D instruction dataset, MODLE, adds significant value to the research community, providing new benchmarks for evaluating 3D spatial intelligence in vision-language models.' Weaknesses='While the methodology shows promise, there is a lack of detailed comparison with existing state-of-the-art models in similar domains, which limits the contextual understanding of its improvements. The paper could benefit from a more thorough analysis of the limitations of the proposed approach, particularly how it generalizes to various types of 3D environments and tasks beyond those presented. Some sections of the presentation are dense, and the clarity could be improved to make the contribution more apparent to a broader audience.' Questions='How does the performance of Spatial 3D-LLM compare to existing models like LEO or other 3D MLLMs in terms of computational efficiency and accuracy in different 3D environments? Are there specific scenarios or types of 3D data where the proposed model struggles? How does the model handle extremely complex or dynamic scenes where objects are frequently moving or changing orientation? Can the approach be extended to other applications beyond the tested 3D object distance measurement and layout editing tasks, such as in robotics or gaming?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The Spatial 3D-LLM model introduces a novel architecture that enhances spatial reasoning in 3D vision-language tasks. The progressive spatial awareness scheme is innovative and effectively captures multi-level spatial information. The newly constructed 3D instruction dataset, MODLE, adds significant value to the research community, providing new benchmarks for evaluating 3D spatial intelligence in vision-language models.' Weaknesses='While the methodology shows promise, there is a lack of detailed comparison with existing state-of-the-art models in similar domains, which limits the contextual understanding of its improvements. The paper could benefit from a more thorough analysis of the limitations of the proposed approach, particularly how it generalizes to various types of 3D environments and tasks beyond those presented. Some sections of the presentation are dense, and the clarity could be improved to make the contribution more apparent to a broader audience.' Questions='How does the performance of Spatial 3D-LLM compare to existing models like LEO or other 3D MLLMs in terms of computational efficiency and accuracy in different 3D environments? Are there specific scenarios or types of 3D data where the proposed model struggles? How does the model handle extremely complex or dynamic scenes where objects are frequently moving or changing orientation? Can the approach be extended to other applications beyond the tested 3D object distance measurement and layout editing tasks, such as in robotics or gaming?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses a significant gap in explainable AI for graph representations by focusing on representation-level explainability.\\n2. The inclusion of both theoretical and empirical validation strengthens the claims made about the robustness and generalization ability of the proposed methods.\\n3. The approach effectively combines unsupervised ensemble graph kernel methods with pattern analysis GNNs to explain graph representations.' Weaknesses='1. The paper could provide more clarity on the practical implications of the proposed methods, specifically in terms of computational complexity and scalability for large graphs.\\n2. While the paper proposes a novel method, the novelty of combining well-known techniques could be questioned without sufficient emphasis on unique contributions.\\n3. The theoretical analysis, though comprehensive, might be inaccessible to practitioners unfamiliar with the technical aspects of graph kernels and pattern analysis.' Questions='1. How does the proposed framework handle very large graph datasets in terms of computational efficiency?\\n2. Can the methods introduced be adapted to dynamic graphs where the structure evolves over time?\\n3. Could more insights be provided on how the pattern analysis explicitly improves interpretability compared to existing methods?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses a significant gap in explainable AI for graph representations by focusing on representation-level explainability.\\n2. The inclusion of both theoretical and empirical validation strengthens the claims made about the robustness and generalization ability of the proposed methods.\\n3. The approach effectively combines unsupervised ensemble graph kernel methods with pattern analysis GNNs to explain graph representations.' Weaknesses='1. The paper could provide more clarity on the practical implications of the proposed methods, specifically in terms of computational complexity and scalability for large graphs.\\n2. While the paper proposes a novel method, the novelty of combining well-known techniques could be questioned without sufficient emphasis on unique contributions.\\n3. The theoretical analysis, though comprehensive, might be inaccessible to practitioners unfamiliar with the technical aspects of graph kernels and pattern analysis.' Questions='1. How does the proposed framework handle very large graph datasets in terms of computational efficiency?\\n2. Can the methods introduced be adapted to dynamic graphs where the structure evolves over time?\\n3. Could more insights be provided on how the pattern analysis explicitly improves interpretability compared to existing methods?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths=\"1. The paper introduces a comprehensive dataset and benchmark tailored for egocentric video understanding, which is a critical advancement for AR/VR applications and robotics.\\n2. MM-Ego model's novel 'Memory Pointer Prompting' mechanism shows potential in addressing long video content processing, making it relevant for future developments in video understanding systems.\\n3. The integration of a multimodal approach specifically targeted at overcoming the unique challenges of egocentric videos is a solid step forward in this niche research area.\" Weaknesses=\"1. As highlighted, the benchmark alone might not provide substantial differentiation from existing datasets concerning the kind of challenges they present, thereby limiting its standalone impact.\\n2. The narrative on the 'narration to egocentric QA' strategy might oversell its novelty, as similar annotation-driven approaches have been seen before.\\n3. Additional experiments could help demonstrate the model's adaptability and robustness across various egocentric video datasets beyond what's presented, enhancing its generalization claims.\" Questions=\"1. How does MM-Ego's 'Memory Pointer Prompting' specifically compare against other attention mechanisms in terms of computational efficiency?\\n2. Can the dataset and benchmark be easily integrated with other egocentric datasets, or is there a need for extensive preprocessing?\\n3. What specific strategies are in place for the de-biasing mechanism mentioned, and how do they address potential biases introduced during dataset annotation?\"\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=4 Strengths=\"1. The paper introduces a comprehensive dataset and benchmark tailored for egocentric video understanding, which is a critical advancement for AR/VR applications and robotics.\\n2. MM-Ego model's novel 'Memory Pointer Prompting' mechanism shows potential in addressing long video content processing, making it relevant for future developments in video understanding systems.\\n3. The integration of a multimodal approach specifically targeted at overcoming the unique challenges of egocentric videos is a solid step forward in this niche research area.\" Weaknesses=\"1. As highlighted, the benchmark alone might not provide substantial differentiation from existing datasets concerning the kind of challenges they present, thereby limiting its standalone impact.\\n2. The narrative on the 'narration to egocentric QA' strategy might oversell its novelty, as similar annotation-driven approaches have been seen before.\\n3. Additional experiments could help demonstrate the model's adaptability and robustness across various egocentric video datasets beyond what's presented, enhancing its generalization claims.\" Questions=\"1. How does MM-Ego's 'Memory Pointer Prompting' specifically compare against other attention mechanisms in terms of computational efficiency?\\n2. Can the dataset and benchmark be easily integrated with other egocentric datasets, or is there a need for extensive preprocessing?\\n3. What specific strategies are in place for the de-biasing mechanism mentioned, and how do they address potential biases introduced during dataset annotation?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper provides a significant extension to the analysis of differential privacy within Noisy-SGD, demonstrating convergence of privacy loss even without the traditional assumptions of convexity and smoothness. This greatly broadens the applicability of DP-SGD in practical settings.\\n2. The introduction of techniques such as the shifted R\\\\\\'enyi divergence analysis with H\\\\\"older continuous gradients and forward Wasserstein distance tracking is well-motivated and technically sound, offering a novel approach to a longstanding issue.\\n3. The analysis offers better privacy loss bounds, which are an improvement over those achieved under previous assumptions, thereby setting a new state-of-the-art.\\n4. The findings address a crucial problem in differential privacy, particularly for applications with non-convex loss functions, which are common in modern deep learning applications.' Weaknesses='1. The presentation, while thorough, could be improved in terms of clarity, particularly concerning complex mathematical concepts and their implications. For example, the discussion on optimal shift allocation and its impact on results may benefit from a more intuitive explanation.\\n2. The paper primarily focuses on theoretical contributions and lacks empirical examples or experiments demonstrating the practical implications of these theoretical results on real-world datasets.\\n3. There is a need for more concrete guidance on how the proposed techniques and assumptions translate into actionable steps in implementing DP-SGD for practitioners.\\n4. The paper could benefit from a more detailed comparison with existing methods in terms of computational efficiency and privacy-utility trade-off, beyond the theoretical guarantees.' Questions='1. Can the authors provide practical examples where H\\\\\"older continuous gradients naturally occur in real-world machine learning tasks? How often do these scenarios arise in practice compared to convex/smooth loss functions?\\n2. How sensitive are the results to the choice of the Holder exponent? Does the exponent significantly impact the level of privacy achieved?\\n3. Could the authors elaborate on the computational implications of their approach? Does the need for forward Wasserstein distance tracking and optimal shift allocation introduce significant overhead compared to existing approaches?\\n4. How does the proposed method compare in terms of privacy-utility trade-offs for commonly used benchmarks like CIFAR-10 or ImageNet? Is there a plan to explore empirical validation in future work?\\n5. Are there any scenarios or types of datasets where the proposed method might not outperform existing approaches? What are the limitations or boundary conditions for the applicability of this analysis?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper provides a significant extension to the analysis of differential privacy within Noisy-SGD, demonstrating convergence of privacy loss even without the traditional assumptions of convexity and smoothness. This greatly broadens the applicability of DP-SGD in practical settings.\\n2. The introduction of techniques such as the shifted R\\\\\\'enyi divergence analysis with H\\\\\"older continuous gradients and forward Wasserstein distance tracking is well-motivated and technically sound, offering a novel approach to a longstanding issue.\\n3. The analysis offers better privacy loss bounds, which are an improvement over those achieved under previous assumptions, thereby setting a new state-of-the-art.\\n4. The findings address a crucial problem in differential privacy, particularly for applications with non-convex loss functions, which are common in modern deep learning applications.' Weaknesses='1. The presentation, while thorough, could be improved in terms of clarity, particularly concerning complex mathematical concepts and their implications. For example, the discussion on optimal shift allocation and its impact on results may benefit from a more intuitive explanation.\\n2. The paper primarily focuses on theoretical contributions and lacks empirical examples or experiments demonstrating the practical implications of these theoretical results on real-world datasets.\\n3. There is a need for more concrete guidance on how the proposed techniques and assumptions translate into actionable steps in implementing DP-SGD for practitioners.\\n4. The paper could benefit from a more detailed comparison with existing methods in terms of computational efficiency and privacy-utility trade-off, beyond the theoretical guarantees.' Questions='1. Can the authors provide practical examples where H\\\\\"older continuous gradients naturally occur in real-world machine learning tasks? How often do these scenarios arise in practice compared to convex/smooth loss functions?\\n2. How sensitive are the results to the choice of the Holder exponent? Does the exponent significantly impact the level of privacy achieved?\\n3. Could the authors elaborate on the computational implications of their approach? Does the need for forward Wasserstein distance tracking and optimal shift allocation introduce significant overhead compared to existing approaches?\\n4. How does the proposed method compare in terms of privacy-utility trade-offs for commonly used benchmarks like CIFAR-10 or ImageNet? Is there a plan to explore empirical validation in future work?\\n5. Are there any scenarios or types of datasets where the proposed method might not outperform existing approaches? What are the limitations or boundary conditions for the applicability of this analysis?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"1. The paper introduces a novel framework, the Neuron Predictability Lens (NPL), which offers a new perspective on analyzing and interpreting transformer-based LLMs by focusing on neuron activation predictability within FFNs.\\n2. The application of the framework to real models such as LLaMA-2 and GPT-J provides valuable insights on neuron behaviors and activations across layers, opening pathways for better model optimization and efficiency.\\n3. The framework uncovers interesting relationships between neuron predictability and importance, including the roles of 'background neurons,' which could have implications for improving model design and understanding.\" Weaknesses='1. While the framework provides a novel way to analyze LLMs, the predictability of neuron activations and its deeper implications for model behavior and optimization might still require further validation through more exhaustive experiments and datasets.\\n2. The paper could improve in terms of explaining certain methodological choices and the interpretation of key results more clearly, such as the specifics of the linear transformations used or how the predictability lens can directly lead to practical improvements beyond analysis.\\n3. The reliance on linear transformations may limit the generalizability of the findings, considering more sophisticated relationships might not be fully captured, potentially leaving room for further exploration with non-linear methods.' Questions='1. How does the framework handle the variability in neuron behavior across different kinds of datasets or tasks? Would the predictability remain consistent?\\n2. Is there potential to extend the Neuron Predictability Lens to other components of the LLMs beyond FFNs, such as attention heads, and if so, how might this be approached?\\n3. Could the authors elaborate on how the insights from neuron predictability might directly impact practical performance improvements in language models, possibly through optimization or architecture modifications?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"1. The paper introduces a novel framework, the Neuron Predictability Lens (NPL), which offers a new perspective on analyzing and interpreting transformer-based LLMs by focusing on neuron activation predictability within FFNs.\\n2. The application of the framework to real models such as LLaMA-2 and GPT-J provides valuable insights on neuron behaviors and activations across layers, opening pathways for better model optimization and efficiency.\\n3. The framework uncovers interesting relationships between neuron predictability and importance, including the roles of 'background neurons,' which could have implications for improving model design and understanding.\" Weaknesses='1. While the framework provides a novel way to analyze LLMs, the predictability of neuron activations and its deeper implications for model behavior and optimization might still require further validation through more exhaustive experiments and datasets.\\n2. The paper could improve in terms of explaining certain methodological choices and the interpretation of key results more clearly, such as the specifics of the linear transformations used or how the predictability lens can directly lead to practical improvements beyond analysis.\\n3. The reliance on linear transformations may limit the generalizability of the findings, considering more sophisticated relationships might not be fully captured, potentially leaving room for further exploration with non-linear methods.' Questions='1. How does the framework handle the variability in neuron behavior across different kinds of datasets or tasks? Would the predictability remain consistent?\\n2. Is there potential to extend the Neuron Predictability Lens to other components of the LLMs beyond FFNs, such as attention heads, and if so, how might this be approached?\\n3. Could the authors elaborate on how the insights from neuron predictability might directly impact practical performance improvements in language models, possibly through optimization or architecture modifications?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The method leverages existing image restoration diffusion models and extends them to video restoration without additional training. It maintains temporal consistency through hierarchical latent warping and hybrid flow-guided spatial-aware token merging, showcasing versatile application across various datasets and tasks.' Weaknesses='The reliance on pre-trained models might limit the scope of improvements for novel or highly distinctive video datasets. The effectiveness of temporal consistency without explicit retraining could be highly dependent on the spatial-aware token merging and optical flow accuracy, which may not always align perfectly with diverse video characteristics.' Questions='How does the method perform on highly dynamic or complex scenes where optical flow can be less reliable? Are there any limitations or scenarios where the proposed approach might struggle to maintain consistency or quality?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The method leverages existing image restoration diffusion models and extends them to video restoration without additional training. It maintains temporal consistency through hierarchical latent warping and hybrid flow-guided spatial-aware token merging, showcasing versatile application across various datasets and tasks.' Weaknesses='The reliance on pre-trained models might limit the scope of improvements for novel or highly distinctive video datasets. The effectiveness of temporal consistency without explicit retraining could be highly dependent on the spatial-aware token merging and optical flow accuracy, which may not always align perfectly with diverse video characteristics.' Questions='How does the method perform on highly dynamic or complex scenes where optical flow can be less reliable? Are there any limitations or scenarios where the proposed approach might struggle to maintain consistency or quality?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='1. The proposed approach effectively identifies privacy risks in graph prompt learning and addresses them using an innovative application of the PATE framework.\\n2. The study evaluates the method across various GNN architectures, showcasing strong empirical performance and potential for wide applicability.\\n3. Offers a novel contribution by adapting differentially private learning techniques to the graph domain, which is underexplored compared to language and vision domains.' Weaknesses=\"1. The method's scalability and potential performance impact on large-scale datasets are not thoroughly investigated, leaving questions about its feasibility in real-world applications.\\n2. The use of node centrality for weighing teacher votes is well-motivated but lacks in-depth theoretical justification or comparison with alternative weighting schemes.\\n3. The theoretical foundations of the privacy guarantees provided by the approach are somewhat under-explored, which might limit confidence in its robustness against sophisticated attacks.\" Questions='1. How well do the DP-GPL and DP-GPL+W methods scale with increasing graph size and complexity?\\n2. What is the trade-off between privacy and accuracy for the proposed methods compared to traditional approaches, and how does this influence the choice of weighting in DP-GPL+W?\\n3. Could the method be generalized to work with heterogeneous graphs, and what adaptations might be necessary?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths='1. The proposed approach effectively identifies privacy risks in graph prompt learning and addresses them using an innovative application of the PATE framework.\\n2. The study evaluates the method across various GNN architectures, showcasing strong empirical performance and potential for wide applicability.\\n3. Offers a novel contribution by adapting differentially private learning techniques to the graph domain, which is underexplored compared to language and vision domains.' Weaknesses=\"1. The method's scalability and potential performance impact on large-scale datasets are not thoroughly investigated, leaving questions about its feasibility in real-world applications.\\n2. The use of node centrality for weighing teacher votes is well-motivated but lacks in-depth theoretical justification or comparison with alternative weighting schemes.\\n3. The theoretical foundations of the privacy guarantees provided by the approach are somewhat under-explored, which might limit confidence in its robustness against sophisticated attacks.\" Questions='1. How well do the DP-GPL and DP-GPL+W methods scale with increasing graph size and complexity?\\n2. What is the trade-off between privacy and accuracy for the proposed methods compared to traditional approaches, and how does this influence the choice of weighting in DP-GPL+W?\\n3. Could the method be generalized to work with heterogeneous graphs, and what adaptations might be necessary?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed SMT method offers a novel approach to parameter-efficient fine-tuning by sparsely tuning sub-matrices, which effectively reduces computational costs while achieving competitive accuracy. The experiments clearly show that SMT surpasses existing PEFT methods such as LoRA in both computational efficiency and accuracy, thus demonstrating its practical applicability for large language models.' Weaknesses='The paper does not address the potential challenges of selecting optimal sparse sub-matrices across various types of neural network architectures, which may limit the generalizability of SMT. Additionally, factors such as the robustness of the approach in new unseen tasks or detailed comparative analysis against random sparse selection methods are not well explored.' Questions='1. How does the selection of sparse sub-matrices affect model regularization, and does this method introduce any biases in the learning process? 2. Can SMT be effectively applied to models with architectures very different from transformers, and if so, how might the sparse tuning strategy differ? 3. How does SMT perform in terms of robustness and generalization on entirely new downstream tasks not included in the training phase?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed SMT method offers a novel approach to parameter-efficient fine-tuning by sparsely tuning sub-matrices, which effectively reduces computational costs while achieving competitive accuracy. The experiments clearly show that SMT surpasses existing PEFT methods such as LoRA in both computational efficiency and accuracy, thus demonstrating its practical applicability for large language models.' Weaknesses='The paper does not address the potential challenges of selecting optimal sparse sub-matrices across various types of neural network architectures, which may limit the generalizability of SMT. Additionally, factors such as the robustness of the approach in new unseen tasks or detailed comparative analysis against random sparse selection methods are not well explored.' Questions='1. How does the selection of sparse sub-matrices affect model regularization, and does this method introduce any biases in the learning process? 2. Can SMT be effectively applied to models with architectures very different from transformers, and if so, how might the sparse tuning strategy differ? 3. How does SMT perform in terms of robustness and generalization on entirely new downstream tasks not included in the training phase?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"1. CrossModalNet provides a novel approach to integrating multimodal data effectively, addressing a critical gap in medical image segmentation.\\n2. The use of advanced frameworks like the dual-stream cross-network design and Transformers enhances the model's performance significantly, as demonstrated by improved metrics on the MM-WHS dataset.\\n3. Provides a strong theoretical foundation which could guide future research in the field.\" Weaknesses=\"1. The complex model architecture and advanced mathematical frameworks might pose challenges to replication and implementation in practical settings without substantial computational resources. \\n2. The paper's evaluation is primarily based on a single dataset (MM-WHS), which might not fully represent the model's generalizability across other datasets and medical segmentation tasks.\\n3. There is limited discussion on the impact of varying hyperparameters and potential overfitting due to model complexity.\" Questions=\"1. Can the authors elaborate on how the CrossModalNet architecture can be extended or adapted for other medical segmentation tasks beyond whole-heart segmentation?\\n2. How sensitive is the model's performance to the choice of hyperparameters, and were there specific strategies used to prevent overfitting given the model's complexity?\\n3. Does CrossModalNet require any specific pre-processing steps or data normalizations to manage domain shifts effectively across different datasets?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"1. CrossModalNet provides a novel approach to integrating multimodal data effectively, addressing a critical gap in medical image segmentation.\\n2. The use of advanced frameworks like the dual-stream cross-network design and Transformers enhances the model's performance significantly, as demonstrated by improved metrics on the MM-WHS dataset.\\n3. Provides a strong theoretical foundation which could guide future research in the field.\" Weaknesses=\"1. The complex model architecture and advanced mathematical frameworks might pose challenges to replication and implementation in practical settings without substantial computational resources. \\n2. The paper's evaluation is primarily based on a single dataset (MM-WHS), which might not fully represent the model's generalizability across other datasets and medical segmentation tasks.\\n3. There is limited discussion on the impact of varying hyperparameters and potential overfitting due to model complexity.\" Questions=\"1. Can the authors elaborate on how the CrossModalNet architecture can be extended or adapted for other medical segmentation tasks beyond whole-heart segmentation?\\n2. How sensitive is the model's performance to the choice of hyperparameters, and were there specific strategies used to prevent overfitting given the model's complexity?\\n3. Does CrossModalNet require any specific pre-processing steps or data normalizations to manage domain shifts effectively across different datasets?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed SMvCNet effectively integrates multi-view information using heterogeneous random walks, demonstrating superior clustering performance across multiple datasets. The unified sample-level structure effectively guides representation learning by capturing both inter-view consistency and complementary structural information.' Weaknesses=\"The paper lacks a detailed complexity analysis of the SMvCNet, particularly in terms of time and space requirements, which makes it difficult to assess scalability. Furthermore, the proposed method's theoretical performance, including potential convergence guarantees, is not discussed in depth.\" Questions='What are the computational complexities of the proposed algorithm in terms of time and space? Are there theoretical justifications for the expected performance improvements and the convergence behavior of the SMvCNet?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed SMvCNet effectively integrates multi-view information using heterogeneous random walks, demonstrating superior clustering performance across multiple datasets. The unified sample-level structure effectively guides representation learning by capturing both inter-view consistency and complementary structural information.' Weaknesses=\"The paper lacks a detailed complexity analysis of the SMvCNet, particularly in terms of time and space requirements, which makes it difficult to assess scalability. Furthermore, the proposed method's theoretical performance, including potential convergence guarantees, is not discussed in depth.\" Questions='What are the computational complexities of the proposed algorithm in terms of time and space? Are there theoretical justifications for the expected performance improvements and the convergence behavior of the SMvCNet?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper provides a substantial theoretical advancement by addressing the variance collapse problem in SVGD through the novel use of hybrid kernels. The introduction of h-SVGD as a kernelised Wasserstein gradient flow is a significant contribution that could influence future research in variational inference.' Weaknesses='Though the paper provides a compelling theoretical framework, the presentation of complex concepts like the hybrid Stein PDE could benefit from more intuitive explanations or examples to aid understanding. Additionally, the experimental section could be expanded to include more diverse scenarios or benchmarks.' Questions='How does the choice of different kernels for the driving and repulsive terms in h-SVGD affect its performance with different distributions? Has there been an investigation into the optimal combination of kernels? Could the proposed h-SVGD method be extended or adapted to address other challenges or limitations in particle-based inference methods?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper provides a substantial theoretical advancement by addressing the variance collapse problem in SVGD through the novel use of hybrid kernels. The introduction of h-SVGD as a kernelised Wasserstein gradient flow is a significant contribution that could influence future research in variational inference.' Weaknesses='Though the paper provides a compelling theoretical framework, the presentation of complex concepts like the hybrid Stein PDE could benefit from more intuitive explanations or examples to aid understanding. Additionally, the experimental section could be expanded to include more diverse scenarios or benchmarks.' Questions='How does the choice of different kernels for the driving and repulsive terms in h-SVGD affect its performance with different distributions? Has there been an investigation into the optimal combination of kernels? Could the proposed h-SVGD method be extended or adapted to address other challenges or limitations in particle-based inference methods?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces an innovative approach to address spectral bias in implicit neural representations using sinusoidal trainable activation functions. This is a significant contribution that improves the learning and reconstruction of high-frequency details, which is a notable advance over existing ReLU-based networks. The theoretical foundations and practical implications of the method are well-articulated, and the experimental results demonstrate superior performance in image, shape, and waveform reconstruction compared to state-of-the-art methods.' Weaknesses='While the proposed method shows significant improvement, there is a lack of detailed analysis on the computational overhead introduced by optimizing the additional parameters of the sinusoidal functions. Also, the paper could have better highlighted potential limitations or scenarios where STAF might not perform as well or where traditional ReLU-based methods might still be preferable.' Questions=\"1. Can the method be applied to deeper architectures or those with more complex topologies, and how does STAF scale in such scenarios?\\n2. What are the potential impacts of the initialization scheme on model convergence and stability across different datasets or tasks?\\n3. How does the use of trainable sinusoidal activation functions affect the model's robustness to noise or variations in the input data?\"\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper introduces an innovative approach to address spectral bias in implicit neural representations using sinusoidal trainable activation functions. This is a significant contribution that improves the learning and reconstruction of high-frequency details, which is a notable advance over existing ReLU-based networks. The theoretical foundations and practical implications of the method are well-articulated, and the experimental results demonstrate superior performance in image, shape, and waveform reconstruction compared to state-of-the-art methods.' Weaknesses='While the proposed method shows significant improvement, there is a lack of detailed analysis on the computational overhead introduced by optimizing the additional parameters of the sinusoidal functions. Also, the paper could have better highlighted potential limitations or scenarios where STAF might not perform as well or where traditional ReLU-based methods might still be preferable.' Questions=\"1. Can the method be applied to deeper architectures or those with more complex topologies, and how does STAF scale in such scenarios?\\n2. What are the potential impacts of the initialization scheme on model convergence and stability across different datasets or tasks?\\n3. How does the use of trainable sinusoidal activation functions affect the model's robustness to noise or variations in the input data?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper addresses a pertinent and challenging problem of instruction adherence in LLMs and proposes an innovative, resource-efficient method to enhance this alignment. The introduction of the GUIDE framework and the Influence metric provides a novel approach to systematically bias attention mechanisms, potentially offering significant advancements in transformer-based models' explainability and alignment.\" Weaknesses='The evaluation lacks a comprehensive assessment of potential trade-offs with the proposed approach, particularly regarding impacts on the quality and consistency of outputs. There is also a notable absence of comparisons with existing methodologies in similar domains, which could provide better insights into the distinct advantages and potential limitations of GUIDE and Influence. Additionally, there is limited discussion on scalability and generalizability across different LLM architectures or domains.' Questions='How does GUIDE compare to other recent methodologies focused on instruction adherence and transformer alignment, particularly regarding efficiency and effectiveness? Can GUIDE be easily integrated into existing LLM frameworks, or does it necessitate significant architectural modifications? What are the implications of GUIDE on model adaptability to longer and more complex prompts?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper addresses a pertinent and challenging problem of instruction adherence in LLMs and proposes an innovative, resource-efficient method to enhance this alignment. The introduction of the GUIDE framework and the Influence metric provides a novel approach to systematically bias attention mechanisms, potentially offering significant advancements in transformer-based models' explainability and alignment.\" Weaknesses='The evaluation lacks a comprehensive assessment of potential trade-offs with the proposed approach, particularly regarding impacts on the quality and consistency of outputs. There is also a notable absence of comparisons with existing methodologies in similar domains, which could provide better insights into the distinct advantages and potential limitations of GUIDE and Influence. Additionally, there is limited discussion on scalability and generalizability across different LLM architectures or domains.' Questions='How does GUIDE compare to other recent methodologies focused on instruction adherence and transformer alignment, particularly regarding efficiency and effectiveness? Can GUIDE be easily integrated into existing LLM frameworks, or does it necessitate significant architectural modifications? What are the implications of GUIDE on model adaptability to longer and more complex prompts?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses an important gap in GFlowNet training, focusing on the inefficiencies observed in sampling methods.\\n2. The integration of MCTS principles into GFlowNet training, via MCDS, is innovative and leverages known strengths of both methods to improve exploration during training.\\n3. Empirical evidence suggests that MCDS outperforms traditional sampling approaches, highlighting its potential for broader applications in discrete environments.' Weaknesses='1. The paper could provide more clarity on certain technical components, such as the exact implementation details of MCDS within the broader GFlowNet architecture.\\n2. There seems to be a lack of comprehensive statistical analysis accompanying the empirical results, especially in demonstrating the significance of improvements observed with MCDS.\\n3. Although the paper claims enhanced performance, the experiments focus on relatively synthetic environments; inclusion of more diverse real-world tasks could bolster claims of generalizability.' Questions=\"1. How does the proposed MCDS handle environments with significantly larger state spaces, and what are its computational limitations?\\n2. Can the authors provide more detailed comparisons against other state-of-the-art sampling techniques to clearly illustrate MCDS's advantages?\\n3. What are the potential impacts of parameter changes within the MCDS approach on its empirical performance?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper addresses an important gap in GFlowNet training, focusing on the inefficiencies observed in sampling methods.\\n2. The integration of MCTS principles into GFlowNet training, via MCDS, is innovative and leverages known strengths of both methods to improve exploration during training.\\n3. Empirical evidence suggests that MCDS outperforms traditional sampling approaches, highlighting its potential for broader applications in discrete environments.' Weaknesses='1. The paper could provide more clarity on certain technical components, such as the exact implementation details of MCDS within the broader GFlowNet architecture.\\n2. There seems to be a lack of comprehensive statistical analysis accompanying the empirical results, especially in demonstrating the significance of improvements observed with MCDS.\\n3. Although the paper claims enhanced performance, the experiments focus on relatively synthetic environments; inclusion of more diverse real-world tasks could bolster claims of generalizability.' Questions=\"1. How does the proposed MCDS handle environments with significantly larger state spaces, and what are its computational limitations?\\n2. Can the authors provide more detailed comparisons against other state-of-the-art sampling techniques to clearly illustrate MCDS's advantages?\\n3. What are the potential impacts of parameter changes within the MCDS approach on its empirical performance?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=2 Rating=5 Confidence=4 Strengths='1. The proposal of MotionDreamer enhances the capability of generating natural and diverse motion sequences from limited data.\\n2. The usage of sliding window local attention and quantization techniques appears both novel and immediately applicable, potentially advancing the field.\\n3. User perceptual assessments show MotionDreamer’s effectiveness in practical scenarios like temporal editing, which could benefit animation industries.' Weaknesses=\"1. The primary weakness lies in the limited novelty of integrating existing methodologies in a way that may not significantly advance the field theoretically.\\n2. While MotionDreamer's use of localized attention and quantization is claimed to handle overfitting, specific comparisons with alternative methods, such as QnA layers from SinMDM, are not elaborated.\\n3. The evaluations, while comprehensive, seem to rely heavily on qualitative assessments without deep discussions on failure cases or challenging scenarios where the method might underperform.\" Questions=\"1. How does MotionDreamer handle extremely complex or chaotic motion sequences compared to simpler, smoother sequences?\\n2. Are there specific complex scenarios where the localized generative masked transformer fails to sufficiently capture the variability of motion?\\n3. Is there any consideration or future work aimed at extending the method's utility to broader datasets beyond motion animation?\"\n",
      "Success: Soundness=3 Presentation=4 Contribution=2 Rating=5 Confidence=4 Strengths='1. The proposal of MotionDreamer enhances the capability of generating natural and diverse motion sequences from limited data.\\n2. The usage of sliding window local attention and quantization techniques appears both novel and immediately applicable, potentially advancing the field.\\n3. User perceptual assessments show MotionDreamer’s effectiveness in practical scenarios like temporal editing, which could benefit animation industries.' Weaknesses=\"1. The primary weakness lies in the limited novelty of integrating existing methodologies in a way that may not significantly advance the field theoretically.\\n2. While MotionDreamer's use of localized attention and quantization is claimed to handle overfitting, specific comparisons with alternative methods, such as QnA layers from SinMDM, are not elaborated.\\n3. The evaluations, while comprehensive, seem to rely heavily on qualitative assessments without deep discussions on failure cases or challenging scenarios where the method might underperform.\" Questions=\"1. How does MotionDreamer handle extremely complex or chaotic motion sequences compared to simpler, smoother sequences?\\n2. Are there specific complex scenarios where the localized generative masked transformer fails to sufficiently capture the variability of motion?\\n3. Is there any consideration or future work aimed at extending the method's utility to broader datasets beyond motion animation?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper addresses a significant limitation in current long-context LLMs by proposing a novel approach for ultra-long text generation. The introduction of AgentWrite is a creative solution for breaking down long tasks, and the creation of the LongWriter-6k dataset and LongBench-Write benchmark are valuable contributions to the field. The experiments conducted demonstrated the potential for existing models to generate much longer coherent outputs when trained with suitable datasets.' Weaknesses='The paper might benefit from a more comprehensive evaluation involving a wider range of models to ensure the broader applicability of the proposed method. Additionally, more details on how AgentWrite compares to existing methods at breaking down tasks would enrich the discussion on its effectiveness. Future work could also explore how such approaches impact the training time and computational costs.' Questions='1. How does the AgentWrite system ensure that the quality and coherence of subtask-generated text are maintained across very long outputs?\\n2. Can the methods proposed be easily adapted to other languages or datasets outside what was specifically tested here?\\n3. What are the computational trade-offs when using such a system to generate lengthy outputs consistently?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='The paper addresses a significant limitation in current long-context LLMs by proposing a novel approach for ultra-long text generation. The introduction of AgentWrite is a creative solution for breaking down long tasks, and the creation of the LongWriter-6k dataset and LongBench-Write benchmark are valuable contributions to the field. The experiments conducted demonstrated the potential for existing models to generate much longer coherent outputs when trained with suitable datasets.' Weaknesses='The paper might benefit from a more comprehensive evaluation involving a wider range of models to ensure the broader applicability of the proposed method. Additionally, more details on how AgentWrite compares to existing methods at breaking down tasks would enrich the discussion on its effectiveness. Future work could also explore how such approaches impact the training time and computational costs.' Questions='1. How does the AgentWrite system ensure that the quality and coherence of subtask-generated text are maintained across very long outputs?\\n2. Can the methods proposed be easily adapted to other languages or datasets outside what was specifically tested here?\\n3. What are the computational trade-offs when using such a system to generate lengthy outputs consistently?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"1. The introduction of dynamic Ada-K routing provides a novel, adaptive approach to resource allocation in Mixture-of-Experts architectures, addressing the inefficiencies of static Top-K routing.\\n2. The proposed method reduces computational resources, FLOPs, and inference latency while improving model performance, which makes it a significant contribution to the scalability of large language models.\\n3. Extensive experimental validation across varying model sizes and benchmarks demonstrates strong results, providing evidence of the method's versatility and effectiveness.\" Weaknesses='1. While dynamic routing is shown to be effective, the paper could provide more insights into potential limitations, such as scalability to even larger models or varying types of benchmarks.\\n2. The implications of using Proximal Policy Optimization for training allocators are not fully explored. Discussion on how the choice of differential algorithm impacts training stability and performance could be beneficial.\\n3. There could be more discussion on the complexities and potential overhead introduced by the dynamic routing system, including the computational cost relative to the gains in efficiency.' Questions='1. Could the proposed dynamic Ada-K routing method encounter challenges if applied to models larger than those tested, such as in contexts that heavily rely on specialized data formats or exceptionally complex tasks?\\n2. Given the reliance on Proximal Policy Optimization, what are the primary factors affecting the allocation learning, and how sensitive are these factors to the choice of hyperparameters?\\n3. How does the overhead introduced by the routing decision-making process compare with the efficiency gains in practice, especially for extremely large batch processing scenarios?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"1. The introduction of dynamic Ada-K routing provides a novel, adaptive approach to resource allocation in Mixture-of-Experts architectures, addressing the inefficiencies of static Top-K routing.\\n2. The proposed method reduces computational resources, FLOPs, and inference latency while improving model performance, which makes it a significant contribution to the scalability of large language models.\\n3. Extensive experimental validation across varying model sizes and benchmarks demonstrates strong results, providing evidence of the method's versatility and effectiveness.\" Weaknesses='1. While dynamic routing is shown to be effective, the paper could provide more insights into potential limitations, such as scalability to even larger models or varying types of benchmarks.\\n2. The implications of using Proximal Policy Optimization for training allocators are not fully explored. Discussion on how the choice of differential algorithm impacts training stability and performance could be beneficial.\\n3. There could be more discussion on the complexities and potential overhead introduced by the dynamic routing system, including the computational cost relative to the gains in efficiency.' Questions='1. Could the proposed dynamic Ada-K routing method encounter challenges if applied to models larger than those tested, such as in contexts that heavily rely on specialized data formats or exceptionally complex tasks?\\n2. Given the reliance on Proximal Policy Optimization, what are the primary factors affecting the allocation learning, and how sensitive are these factors to the choice of hyperparameters?\\n3. How does the overhead introduced by the routing decision-making process compare with the efficiency gains in practice, especially for extremely large batch processing scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper presents a novel combination of Local Training and Communication Compression in the federated learning setting, leveraging both to improve communication efficiency. \\n2. Theoretical contributions include a comprehensive mathematical proof ensuring linear convergence and dependability on model and data distribution characteristics. \\n3. LoCoDL sets new standards for communication-efficient distributed learning by achieving a doubly-accelerated communication complexity.' Weaknesses='1. The practical implementation details and potential limitations or trade-offs of the proposed LoCoDL algorithm might not be comprehensively discussed.\\n2. The paper may rely heavily on theoretical proofs without extensive empirical validation across diverse real-world settings.\\n3. The integration of the local model estimates and dual variables might add complexity to the system, potentially affecting ease of implementation.' Questions='1. How does LoCoDL perform in highly heterogeneous datasets compared to homogeneous settings?\\n2. Are there specific scenarios or data distributions where the proposed algorithm might not achieve the expected communication efficiency?\\n3. How does the computational cost of the proposed method compare to existing state-of-the-art FL algorithms during local updates?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper presents a novel combination of Local Training and Communication Compression in the federated learning setting, leveraging both to improve communication efficiency. \\n2. Theoretical contributions include a comprehensive mathematical proof ensuring linear convergence and dependability on model and data distribution characteristics. \\n3. LoCoDL sets new standards for communication-efficient distributed learning by achieving a doubly-accelerated communication complexity.' Weaknesses='1. The practical implementation details and potential limitations or trade-offs of the proposed LoCoDL algorithm might not be comprehensively discussed.\\n2. The paper may rely heavily on theoretical proofs without extensive empirical validation across diverse real-world settings.\\n3. The integration of the local model estimates and dual variables might add complexity to the system, potentially affecting ease of implementation.' Questions='1. How does LoCoDL perform in highly heterogeneous datasets compared to homogeneous settings?\\n2. Are there specific scenarios or data distributions where the proposed algorithm might not achieve the expected communication efficiency?\\n3. How does the computational cost of the proposed method compare to existing state-of-the-art FL algorithms during local updates?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=2 Rating=6 Confidence=4 Strengths=\"The paper provides an in-depth analysis of transformer architectures, specifically focusing on their ability to execute counting tasks. It highlights how subtle changes in architecture and hyperparameters influence the model's performance, providing valuable insights into the workings of transformers. The theoretical and empirical evaluations are well-motivated and aligned.\" Weaknesses=\"The contribution is somewhat limited to academic insights into transformers' working mechanisms, which might not directly translate into practical improvements for more complex real-world tasks or multi-layer models. It primarily focuses on single-layer architectures and a controlled counting task, which may limit the generalizability of the findings.\" Questions='What potential approaches do the authors suggest for extending these findings to more complex multi-layer transformer models? How generalizable are these findings to other tasks beyond counting, such as natural language processing applications with richer data contexts?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=2 Rating=6 Confidence=4 Strengths=\"The paper provides an in-depth analysis of transformer architectures, specifically focusing on their ability to execute counting tasks. It highlights how subtle changes in architecture and hyperparameters influence the model's performance, providing valuable insights into the workings of transformers. The theoretical and empirical evaluations are well-motivated and aligned.\" Weaknesses=\"The contribution is somewhat limited to academic insights into transformers' working mechanisms, which might not directly translate into practical improvements for more complex real-world tasks or multi-layer models. It primarily focuses on single-layer architectures and a controlled counting task, which may limit the generalizability of the findings.\" Questions='What potential approaches do the authors suggest for extending these findings to more complex multi-layer transformer models? How generalizable are these findings to other tasks beyond counting, such as natural language processing applications with richer data contexts?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper introduces a novel dataset, MedTrinity-25M, which addresses the lack of rich multigranular annotations in existing medical datasets. It leverages an automated pipeline to generate these annotations without requiring paired text datasets, thus improving scalability. The dataset's utility is demonstrated through its ability to enhance the performance of AI models on various medical tasks, showcasing its potential impact on the field.\" Weaknesses=\"The dataset's dependence on large multimodal models and medical knowledge retrieval could introduce biases or inaccuracies, especially if the underlying models or knowledge bases are incomplete or flawed. Additionally, the process for generating ROI and annotations, while automated, might not capture all nuances of expert annotations, potentially affecting model performance in critical applications. The paper could also provide more details on the validation of the dataset's annotations.\" Questions='How does the dataset handle potential biases or inaccuracies in the underlying multimodal models and medical knowledge retrieval systems? What measures are taken to ensure the quality and reliability of the automatic annotations, given they replace expert manual annotations? Are there any plans for expert validation or adjustment of these automated annotations, particularly for high-stakes medical applications?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper introduces a novel dataset, MedTrinity-25M, which addresses the lack of rich multigranular annotations in existing medical datasets. It leverages an automated pipeline to generate these annotations without requiring paired text datasets, thus improving scalability. The dataset's utility is demonstrated through its ability to enhance the performance of AI models on various medical tasks, showcasing its potential impact on the field.\" Weaknesses=\"The dataset's dependence on large multimodal models and medical knowledge retrieval could introduce biases or inaccuracies, especially if the underlying models or knowledge bases are incomplete or flawed. Additionally, the process for generating ROI and annotations, while automated, might not capture all nuances of expert annotations, potentially affecting model performance in critical applications. The paper could also provide more details on the validation of the dataset's annotations.\" Questions='How does the dataset handle potential biases or inaccuracies in the underlying multimodal models and medical knowledge retrieval systems? What measures are taken to ensure the quality and reliability of the automatic annotations, given they replace expert manual annotations? Are there any plans for expert validation or adjustment of these automated annotations, particularly for high-stakes medical applications?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed Rotated Runtime Smooth (RRS) method effectively addresses both channel-wise and spike outliers, offering a significant improvement in INT4 inference accuracy. The integration with existing hardware ensures practicality and demonstrates lower computational overheads compared to prior methods.' Weaknesses='The paper does not provide sufficient detail on the potential computational overhead introduced by the runtime smoothing process, especially in a real-world deployment scenario. Moreover, the practicality of implementing the method across various hardware setups may not be fully addressed.' Questions='How does the runtime performance of RRS compare with existing methods like SmoothQuant and QuaRot in terms of actual hardware latency rather than theoretical computational overhead?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The proposed Rotated Runtime Smooth (RRS) method effectively addresses both channel-wise and spike outliers, offering a significant improvement in INT4 inference accuracy. The integration with existing hardware ensures practicality and demonstrates lower computational overheads compared to prior methods.' Weaknesses='The paper does not provide sufficient detail on the potential computational overhead introduced by the runtime smoothing process, especially in a real-world deployment scenario. Moreover, the practicality of implementing the method across various hardware setups may not be fully addressed.' Questions='How does the runtime performance of RRS compare with existing methods like SmoothQuant and QuaRot in terms of actual hardware latency rather than theoretical computational overhead?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The method proposes an innovative test-time training strategy with a memory mechanism specifically tailored for few-shot medical video segmentation, which is a pressing challenge in medical imaging due to the scarcity of annotated video data.\\n2. The research successfully bridges the gap between static image models and video segmentation by constructing a new methodology that leverages minimal annotations effectively.\\n3. The introduction of the OS-I2V-Seg dataset for evaluation highlights the practicality and application of the proposed approach.' Weaknesses='1. The paper lacks comprehensive details on the pre-training of the one-shot segmentation model on static images, which raises concerns about replicability and understanding of the methodology.\\n2. The framework might not fully address domain shift issues that arise from potential differences between training datasets and test scenarios, particularly in medical settings.\\n3. The presentation could benefit from more detailed explanations of the memory mechanism and self-distillation processes, as their roles are crucial yet not sufficiently detailed in the method section.' Questions='1. How does the proposed method ensure robustness to domain shifts when the pre-trained static image-based models are applied to distinct video datasets?\\n2. Could the approach be applicable to other video domains outside of medical imaging, and if so, what adjustments might be needed?\\n3. Regarding the memory mechanism employed during test-time training, how is the size of the memory bank determined, and what impact does it have on the segmentation performance?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The method proposes an innovative test-time training strategy with a memory mechanism specifically tailored for few-shot medical video segmentation, which is a pressing challenge in medical imaging due to the scarcity of annotated video data.\\n2. The research successfully bridges the gap between static image models and video segmentation by constructing a new methodology that leverages minimal annotations effectively.\\n3. The introduction of the OS-I2V-Seg dataset for evaluation highlights the practicality and application of the proposed approach.' Weaknesses='1. The paper lacks comprehensive details on the pre-training of the one-shot segmentation model on static images, which raises concerns about replicability and understanding of the methodology.\\n2. The framework might not fully address domain shift issues that arise from potential differences between training datasets and test scenarios, particularly in medical settings.\\n3. The presentation could benefit from more detailed explanations of the memory mechanism and self-distillation processes, as their roles are crucial yet not sufficiently detailed in the method section.' Questions='1. How does the proposed method ensure robustness to domain shifts when the pre-trained static image-based models are applied to distinct video datasets?\\n2. Could the approach be applicable to other video domains outside of medical imaging, and if so, what adjustments might be needed?\\n3. Regarding the memory mechanism employed during test-time training, how is the size of the memory bank determined, and what impact does it have on the segmentation performance?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel approach, DCScore, to evaluate the diversity of LLM-generated datasets by treating it as a sample classification problem. It provides a feasible solution to a relatively under-explored area, demonstrating strong correlation with diversity indicators and offering computational efficiency.' Weaknesses='1. The paper does not thoroughly explore the novelty and specific advantages of DCScore compared to existing methods. \\n2. The rationale behind choosing certain evaluation tasks and the impact of diversity on practical applications could be better justified.\\n3. The theoretical contribution and its real-world implications are not fully developed, leaving readers unclear about broader applications or significance.' Questions='1. How does DCScore compare specifically to existing diversity evaluation methods in various practical performance scenarios, beyond computational efficiency?\\n2. What are the implications of diversity in LLM-generated datasets on real-world applications, and how critical is this factor in improving those applications?\\n3. What kind of datasets or scenarios would particularly benefit from using DCScore for diversity evaluation?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel approach, DCScore, to evaluate the diversity of LLM-generated datasets by treating it as a sample classification problem. It provides a feasible solution to a relatively under-explored area, demonstrating strong correlation with diversity indicators and offering computational efficiency.' Weaknesses='1. The paper does not thoroughly explore the novelty and specific advantages of DCScore compared to existing methods. \\n2. The rationale behind choosing certain evaluation tasks and the impact of diversity on practical applications could be better justified.\\n3. The theoretical contribution and its real-world implications are not fully developed, leaving readers unclear about broader applications or significance.' Questions='1. How does DCScore compare specifically to existing diversity evaluation methods in various practical performance scenarios, beyond computational efficiency?\\n2. What are the implications of diversity in LLM-generated datasets on real-world applications, and how critical is this factor in improving those applications?\\n3. What kind of datasets or scenarios would particularly benefit from using DCScore for diversity evaluation?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths=\"1. The introduction of the Moreau envelope for estimating weight importance is a novel approach in the context of pruning for LLMs. This innovation provides a fresh perspective on dealing with weight perturbations. \\n2. The use of proximal gradient descent and post-pruning fine-tuning with LoRA adds to the robustness of the proposed method. It ensures that MoreauPruner effectively maintains model performance after pruning. \\n3. The experiments conducted on multiple LLMs provide a comprehensive evaluation of MoreauPruner's effectiveness.\" Weaknesses='1. The practical implications of the proposed method are not entirely clear. The discussion on how these robust pruning outcomes directly translate to real-world applications or computational resource savings could be more detailed.\\n2. While the results demonstrate consistency across different weight formats, the benefits beyond this aspect could be better highlighted. The paper could provide more comparisons to existing state-of-the-art pruning methods.\\n3. The computational cost and scalability of the method are not thoroughly discussed, which are critical factors for the deployment of pruning techniques in resource-constrained environments.' Questions='1. Can the authors provide more insights into the computational overhead introduced by using the Moreau envelope and proximal gradient descent in comparison to traditional pruning methods?\\n2. Are there specific scenarios or architectures for which MoreauPruner is particularly effective or less suited, based on the experiments conducted?\\n3. How would MoreauPruner perform if applied to different types of perturbations beyond weight format changes, such as changes in input data distribution or adversarial attacks?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths=\"1. The introduction of the Moreau envelope for estimating weight importance is a novel approach in the context of pruning for LLMs. This innovation provides a fresh perspective on dealing with weight perturbations. \\n2. The use of proximal gradient descent and post-pruning fine-tuning with LoRA adds to the robustness of the proposed method. It ensures that MoreauPruner effectively maintains model performance after pruning. \\n3. The experiments conducted on multiple LLMs provide a comprehensive evaluation of MoreauPruner's effectiveness.\" Weaknesses='1. The practical implications of the proposed method are not entirely clear. The discussion on how these robust pruning outcomes directly translate to real-world applications or computational resource savings could be more detailed.\\n2. While the results demonstrate consistency across different weight formats, the benefits beyond this aspect could be better highlighted. The paper could provide more comparisons to existing state-of-the-art pruning methods.\\n3. The computational cost and scalability of the method are not thoroughly discussed, which are critical factors for the deployment of pruning techniques in resource-constrained environments.' Questions='1. Can the authors provide more insights into the computational overhead introduced by using the Moreau envelope and proximal gradient descent in comparison to traditional pruning methods?\\n2. Are there specific scenarios or architectures for which MoreauPruner is particularly effective or less suited, based on the experiments conducted?\\n3. How would MoreauPruner perform if applied to different types of perturbations beyond weight format changes, such as changes in input data distribution or adversarial attacks?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='The introduction of LAION-C is a notable contribution, providing a benchmark that better reflects true out-of-distribution conditions for models trained on internet-scale datasets. This fills a significant gap in current robustness evaluation methods. The paper is well-structured and clearly communicates the necessity of a new benchmark. The novel distortion types present a meaningful challenge to current vision models, showcasing their reliance on familiar artifacts.' Weaknesses='While LAION-C presents a compelling new benchmark, the paper could further explore the implications of model performance differences on LAION-C for practical applications. Additionally, there might be a need for a more diverse set of distortions to cover a wider variety of OOD situations, as currently, the distortions primarily focus on visual corruption types.' Questions='What specific criteria were used to select the six synthetic distortion types, and how do these distortions align with potential real-world challenges? How might the findings from LAION-C impact the future development of vision models and their training regimens?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='The introduction of LAION-C is a notable contribution, providing a benchmark that better reflects true out-of-distribution conditions for models trained on internet-scale datasets. This fills a significant gap in current robustness evaluation methods. The paper is well-structured and clearly communicates the necessity of a new benchmark. The novel distortion types present a meaningful challenge to current vision models, showcasing their reliance on familiar artifacts.' Weaknesses='While LAION-C presents a compelling new benchmark, the paper could further explore the implications of model performance differences on LAION-C for practical applications. Additionally, there might be a need for a more diverse set of distortions to cover a wider variety of OOD situations, as currently, the distortions primarily focus on visual corruption types.' Questions='What specific criteria were used to select the six synthetic distortion types, and how do these distortions align with potential real-world challenges? How might the findings from LAION-C impact the future development of vision models and their training regimens?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The proposal of incorporating Dynamic Mode Decomposition (DMD) into Graph Neural Networks (GNNs) offers a novel way to associate GNN feature propagation with dynamical systems theory, which can provide theoretical insights and potentially enhance learning tasks. The approach has been empirically validated across several standard graph-based learning tasks, showing it's not just theoretically appealing but practically beneficial.\" Weaknesses='The paper may lack a deep exploration or comparison with similar existing methodologies in dynamical systems applications within GNN contexts. Additionally, while the results indicate success, the paper could benefit from more detailed explanation or justification of the specific choices made within the method, including parameter selection and computational complexity assessment.' Questions='How does the proposed method scale with extremely large graphs given the additional computational requirements of Dynamic Mode Decomposition? Is there an analysis of potential computational overhead or optimization strategies applied to manage resources effectively?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The proposal of incorporating Dynamic Mode Decomposition (DMD) into Graph Neural Networks (GNNs) offers a novel way to associate GNN feature propagation with dynamical systems theory, which can provide theoretical insights and potentially enhance learning tasks. The approach has been empirically validated across several standard graph-based learning tasks, showing it's not just theoretically appealing but practically beneficial.\" Weaknesses='The paper may lack a deep exploration or comparison with similar existing methodologies in dynamical systems applications within GNN contexts. Additionally, while the results indicate success, the paper could benefit from more detailed explanation or justification of the specific choices made within the method, including parameter selection and computational complexity assessment.' Questions='How does the proposed method scale with extremely large graphs given the additional computational requirements of Dynamic Mode Decomposition? Is there an analysis of potential computational overhead or optimization strategies applied to manage resources effectively?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper addresses a significant challenge in the use of influence functions, particularly related to the computational cost of existing methods, making a contribution that could make these functions more usable on large models.\\n2. The method proposed to use spectral properties of the Hessian to set hyperparameters is novel and provides an interesting approach to optimizing the LiSSA algorithm.\\n3. Empirical validation on different models, including ResNet and language models, is provided, showing the practical applicability and effectiveness of the proposed method.' Weaknesses=\"1. The paper's theoretical contributions rely on strong assumptions, and it is not clear how these assumptions hold in practice, especially related to the contractivity condition, which makes it difficult to assess the general applicability of the proposed method.\\n2. The empirical results could be better aligned with the theoretical claims, particularly around the evaluation and demonstration of contractivity or other key assumptions.\\n3. The presentation could be improved, particularly in terms of explaining the selection process for LiSSA parameters and clarifying the experimental setups and results.\" Questions='1. Can you provide additional empirical evidence or theoretical support that the contractivity assumption holds for the settings you consider?\\n2. How would the proposed spectral property-based method handle situations where the assumptions do not hold, particularly for models with more complex loss landscapes?\\n3. Can you clarify the precise procedure for selecting the LiSSA parameters, particularly the batch size, and how sensitive the method is to these parameter choices?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The paper addresses a significant challenge in the use of influence functions, particularly related to the computational cost of existing methods, making a contribution that could make these functions more usable on large models.\\n2. The method proposed to use spectral properties of the Hessian to set hyperparameters is novel and provides an interesting approach to optimizing the LiSSA algorithm.\\n3. Empirical validation on different models, including ResNet and language models, is provided, showing the practical applicability and effectiveness of the proposed method.' Weaknesses=\"1. The paper's theoretical contributions rely on strong assumptions, and it is not clear how these assumptions hold in practice, especially related to the contractivity condition, which makes it difficult to assess the general applicability of the proposed method.\\n2. The empirical results could be better aligned with the theoretical claims, particularly around the evaluation and demonstration of contractivity or other key assumptions.\\n3. The presentation could be improved, particularly in terms of explaining the selection process for LiSSA parameters and clarifying the experimental setups and results.\" Questions='1. Can you provide additional empirical evidence or theoretical support that the contractivity assumption holds for the settings you consider?\\n2. How would the proposed spectral property-based method handle situations where the assumptions do not hold, particularly for models with more complex loss landscapes?\\n3. Can you clarify the precise procedure for selecting the LiSSA parameters, particularly the batch size, and how sensitive the method is to these parameter choices?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper expands on online learning in brokerage by incorporating contextual information, a novel approach that fills a gap in existing literature. The algorithms achieve optimal regret bounds under realistic assumptions, highlighting the method's robustness and potential practical application. The use of both full and two-bit feedback mechanisms provides a comprehensive analysis of the problem.\" Weaknesses=\"The paper's presentation could be improved, as it lacks clarity in certain sections, notably in the explanation of the feedback mechanisms. The reliance on the bounded density assumption as necessary for learnability might limit the method’s applicability to real-world scenarios where this assumption does not hold, potentially affecting the generalizability of the results.\" Questions='What specific challenges might arise when applying the proposed method to real-world OTC markets with unbounded noise distributions? How sensitive are the algorithms to variations in the contextual information or to inaccuracies in the assumed linear relationship between the market value and context?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper expands on online learning in brokerage by incorporating contextual information, a novel approach that fills a gap in existing literature. The algorithms achieve optimal regret bounds under realistic assumptions, highlighting the method's robustness and potential practical application. The use of both full and two-bit feedback mechanisms provides a comprehensive analysis of the problem.\" Weaknesses=\"The paper's presentation could be improved, as it lacks clarity in certain sections, notably in the explanation of the feedback mechanisms. The reliance on the bounded density assumption as necessary for learnability might limit the method’s applicability to real-world scenarios where this assumption does not hold, potentially affecting the generalizability of the results.\" Questions='What specific challenges might arise when applying the proposed method to real-world OTC markets with unbounded noise distributions? How sensitive are the algorithms to variations in the contextual information or to inaccuracies in the assumed linear relationship between the market value and context?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The proposed method provides an innovative approach to quantizing state space models, particularly by addressing the unique challenges posed by their activation patterns.\\n2. It successfully demonstrates the application of the Hadamard transform to handle outliers, which is a novel approach in the context of SSM quantization.\\n3. The experimental results show promise for deployment of SSMs in resource-constrained environments, which could have significant practical implications.' Weaknesses='1. The methodology, while innovative, may still need further exploration and validation across more diverse SSM architectures and datasets to establish its generalizability and effectiveness.\\n2. The paper may lack detailed discussion on potential trade-offs introduced by the quantization process, especially regarding accuracy loss.\\n3. A clearer comparison with existing quantization methods for other models could strengthen the claims regarding the uniqueness and superiority of the proposed approach.' Questions='1. How does the performance of Quamba compare to other quantization methods not specifically designed for SSMs?\\n2. What are the implications of the proposed quantization method on power consumption and heat dissipation in edge devices?\\n3. Are there any specific types of SSM architectures where this method may not perform as well, and if so, why?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='1. The proposed method provides an innovative approach to quantizing state space models, particularly by addressing the unique challenges posed by their activation patterns.\\n2. It successfully demonstrates the application of the Hadamard transform to handle outliers, which is a novel approach in the context of SSM quantization.\\n3. The experimental results show promise for deployment of SSMs in resource-constrained environments, which could have significant practical implications.' Weaknesses='1. The methodology, while innovative, may still need further exploration and validation across more diverse SSM architectures and datasets to establish its generalizability and effectiveness.\\n2. The paper may lack detailed discussion on potential trade-offs introduced by the quantization process, especially regarding accuracy loss.\\n3. A clearer comparison with existing quantization methods for other models could strengthen the claims regarding the uniqueness and superiority of the proposed approach.' Questions='1. How does the performance of Quamba compare to other quantization methods not specifically designed for SSMs?\\n2. What are the implications of the proposed quantization method on power consumption and heat dissipation in edge devices?\\n3. Are there any specific types of SSM architectures where this method may not perform as well, and if so, why?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed method introduces a novel decomposition of GNN message passing, providing insights into label influence correlations. The approach effectively models and quantifies interactions between labels, leading to improved performance in multi-label node classification tasks.' Weaknesses=\"The paper could benefit from a clearer explanation of the proposed decomposition methodology for readers less familiar with GNNs. Additionally, more comprehensive experiments with diverse datasets and comparisons with other advanced MLNC models are necessary to establish the method's general applicability.\" Questions='How does the proposed method handle varying graph structures and sizes across different datasets? Is its efficacy consistent regardless of graph complexity or sparsity?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed method introduces a novel decomposition of GNN message passing, providing insights into label influence correlations. The approach effectively models and quantifies interactions between labels, leading to improved performance in multi-label node classification tasks.' Weaknesses=\"The paper could benefit from a clearer explanation of the proposed decomposition methodology for readers less familiar with GNNs. Additionally, more comprehensive experiments with diverse datasets and comparisons with other advanced MLNC models are necessary to establish the method's general applicability.\" Questions='How does the proposed method handle varying graph structures and sizes across different datasets? Is its efficacy consistent regardless of graph complexity or sparsity?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper presents a significant advancement in KAN by utilizing Bernstein polynomials to enhance flexibility and robustness, a noteworthy contribution to the field.\\n2. VBn-KAN demonstrates superior performance across a range of tasks, including multivariate time series forecasting and computer vision, indicating the practical efficacy of the proposed method.\\n3. The methodological approach, incorporating dynamic adaptation via Bayesian optimization, is innovative and well-grounded in theoretical principles.' Weaknesses='1. The paper may not thoroughly explore potential limitations or drawbacks of the VBn-KAN method, such as potential computational overhead or challenges in implementation.\\n2. While the approach demonstrates superior performance, the novelty might be considered iterative rather than groundbreaking, building upon existing KAN frameworks.\\n3. The evaluation could benefit from a wider range of benchmark datasets and real-world applications to generalize findings better.' Questions=\"1. How does VBn-KAN handle computational overhead compared to traditional KAN methods, particularly when dealing with high-dimensional datasets?\\n2. Can the authors elaborate on the potential limitations or challenges of implementing the proposed method in real-world applications?\\n3. Is there a specific reason for choosing the particular set of benchmark tasks, and how might different tasks impact the method's effectiveness?\"\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper presents a significant advancement in KAN by utilizing Bernstein polynomials to enhance flexibility and robustness, a noteworthy contribution to the field.\\n2. VBn-KAN demonstrates superior performance across a range of tasks, including multivariate time series forecasting and computer vision, indicating the practical efficacy of the proposed method.\\n3. The methodological approach, incorporating dynamic adaptation via Bayesian optimization, is innovative and well-grounded in theoretical principles.' Weaknesses='1. The paper may not thoroughly explore potential limitations or drawbacks of the VBn-KAN method, such as potential computational overhead or challenges in implementation.\\n2. While the approach demonstrates superior performance, the novelty might be considered iterative rather than groundbreaking, building upon existing KAN frameworks.\\n3. The evaluation could benefit from a wider range of benchmark datasets and real-world applications to generalize findings better.' Questions=\"1. How does VBn-KAN handle computational overhead compared to traditional KAN methods, particularly when dealing with high-dimensional datasets?\\n2. Can the authors elaborate on the potential limitations or challenges of implementing the proposed method in real-world applications?\\n3. Is there a specific reason for choosing the particular set of benchmark tasks, and how might different tasks impact the method's effectiveness?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel use of diffusion-based models for generating articulated 3D assets from a single image, addressing a significant gap in the field related to scalability and practicality. The use of large pretrained models like DINOv2 and GPT-4o to enhance detail accuracy and plausibility shows a creative integration of existing powerful tools for a new application. Furthermore, the paper is well-organized and demonstrates the effectiveness of the approach through thorough experimental validation and user studies.' Weaknesses='While the approach is innovative, it relies heavily on pretrained models such as DINOv2 and GPT-4o, raising questions about the novelty of the method itself versus the application and integration of these models. The paper might lack insights into the failure modes or limitations of the method, particularly in cases where the models might not generalize well due to limited training data or diversity in the datasets used.\\n\\nAdditionally, the complexity of implementing such an approach might not be fully transparent, potentially limiting its applicability or reproducibility in practical scenarios.' Questions='1. How does the approach handle cases where the single input image lacks richness in details or when the viewpoint is suboptimal for capturing key articulations?\\n\\n2. Are there limitations noted with the use of GPT-4o in predicting the part connectivity graph, and how does it affect the overall method if any?\\n\\n3. What are the computational requirements for training and inference using this diffusion model-based approach, and how scalable is it when extending to larger sets of articulated objects?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper introduces a novel use of diffusion-based models for generating articulated 3D assets from a single image, addressing a significant gap in the field related to scalability and practicality. The use of large pretrained models like DINOv2 and GPT-4o to enhance detail accuracy and plausibility shows a creative integration of existing powerful tools for a new application. Furthermore, the paper is well-organized and demonstrates the effectiveness of the approach through thorough experimental validation and user studies.' Weaknesses='While the approach is innovative, it relies heavily on pretrained models such as DINOv2 and GPT-4o, raising questions about the novelty of the method itself versus the application and integration of these models. The paper might lack insights into the failure modes or limitations of the method, particularly in cases where the models might not generalize well due to limited training data or diversity in the datasets used.\\n\\nAdditionally, the complexity of implementing such an approach might not be fully transparent, potentially limiting its applicability or reproducibility in practical scenarios.' Questions='1. How does the approach handle cases where the single input image lacks richness in details or when the viewpoint is suboptimal for capturing key articulations?\\n\\n2. Are there limitations noted with the use of GPT-4o in predicting the part connectivity graph, and how does it affect the overall method if any?\\n\\n3. What are the computational requirements for training and inference using this diffusion model-based approach, and how scalable is it when extending to larger sets of articulated objects?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The proposed Geometric Spatiotemporal Transformers (GSTs) effectively integrate Transformer architecture with spatiotemporal graphs, addressing limitations of current GNN-based methods such as fixed input lengths and cumulative errors.\\n2. GSTs demonstrate superior performance in long-term physical dynamics simulation across diverse scales, highlighting their versatility and robustness.\\n3. The Temporal Difference Graph (TDG) is an innovative feature that enhances the model's ability to predict long-term trajectories accurately and efficiently.\\n4. Extensive experiments showcase the efficacy of GSTs in handling multi-scale datasets, suggesting potential for wide application in physical dynamics simulation.\" Weaknesses='1. The paper lacks detailed discussion on the computational complexity and scalability of the proposed GSTs, especially in comparison to existing GNN-based approaches.\\n2. Although the GST model achieves impressive results, there is a limited evaluation with industrial or real-world applications, raising questions about its feasibility in large-scale deployment.\\n3. The reliance on E(3) symmetries, while beneficial for some applications, may limit the generalizability of GSTs to scenarios where such symmetries are not present or are only partially applicable.\\n4. The paper could benefit from more clarity in the exposition of the temporal aspects of the model, particularly regarding how temporal dependencies are explicitly accounted for and analyzed.' Questions='1. How do the computational requirements of GSTs compare with those of traditional GNNs when scaled to larger datasets or more complex systems?\\n2. Is there any limitation in applying GSTs to systems where E(3) symmetries are not prevalent, and if so, how might the model be adjusted or extended to address such cases?\\n3. What are the main challenges in extending GSTs to real-world applications, particularly those involving industrial-scale data, and how might these be overcome?\\n4. Can the Temporal Difference Graph (TDG) concept be adapted or generalized to other domains, such as finance or social network analysis, where long-term dependencies are critical?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The proposed Geometric Spatiotemporal Transformers (GSTs) effectively integrate Transformer architecture with spatiotemporal graphs, addressing limitations of current GNN-based methods such as fixed input lengths and cumulative errors.\\n2. GSTs demonstrate superior performance in long-term physical dynamics simulation across diverse scales, highlighting their versatility and robustness.\\n3. The Temporal Difference Graph (TDG) is an innovative feature that enhances the model's ability to predict long-term trajectories accurately and efficiently.\\n4. Extensive experiments showcase the efficacy of GSTs in handling multi-scale datasets, suggesting potential for wide application in physical dynamics simulation.\" Weaknesses='1. The paper lacks detailed discussion on the computational complexity and scalability of the proposed GSTs, especially in comparison to existing GNN-based approaches.\\n2. Although the GST model achieves impressive results, there is a limited evaluation with industrial or real-world applications, raising questions about its feasibility in large-scale deployment.\\n3. The reliance on E(3) symmetries, while beneficial for some applications, may limit the generalizability of GSTs to scenarios where such symmetries are not present or are only partially applicable.\\n4. The paper could benefit from more clarity in the exposition of the temporal aspects of the model, particularly regarding how temporal dependencies are explicitly accounted for and analyzed.' Questions='1. How do the computational requirements of GSTs compare with those of traditional GNNs when scaled to larger datasets or more complex systems?\\n2. Is there any limitation in applying GSTs to systems where E(3) symmetries are not prevalent, and if so, how might the model be adjusted or extended to address such cases?\\n3. What are the main challenges in extending GSTs to real-world applications, particularly those involving industrial-scale data, and how might these be overcome?\\n4. Can the Temporal Difference Graph (TDG) concept be adapted or generalized to other domains, such as finance or social network analysis, where long-term dependencies are critical?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper makes a clear attempt to advance the understanding of applying diffusion probabilistic models to graph representation learning, which is a fairly novel domain. The experimental validation on multiple datasets provides evidence for the model's effectiveness. The introduction of the Diff-InfoMax principle offers a theoretical foundation for future work.\" Weaknesses=\"The paper could benefit from improved clarity in explaining the specific contributions of the proposed method compared to existing work in both diffusion models and graph representation learning. Some theoretical claims, such as the relation to the InfoMax principle, would benefit from more rigorous validation or proof. Additionally, the paper's presentation could be enhanced for better readability and understanding of complex concepts.\" Questions='How does the proposed method Graffe specifically address the challenges associated with the non-Euclidean nature of graph data, and how does it improve over existing GNN solutions? Could the method potentially be applied to other types of non-Euclidean data structures beyond graphs? Is there a specific reason that GAT and GIN are chosen for encoding, and could alternative GNN architectures potentially offer better results?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The paper makes a clear attempt to advance the understanding of applying diffusion probabilistic models to graph representation learning, which is a fairly novel domain. The experimental validation on multiple datasets provides evidence for the model's effectiveness. The introduction of the Diff-InfoMax principle offers a theoretical foundation for future work.\" Weaknesses=\"The paper could benefit from improved clarity in explaining the specific contributions of the proposed method compared to existing work in both diffusion models and graph representation learning. Some theoretical claims, such as the relation to the InfoMax principle, would benefit from more rigorous validation or proof. Additionally, the paper's presentation could be enhanced for better readability and understanding of complex concepts.\" Questions='How does the proposed method Graffe specifically address the challenges associated with the non-Euclidean nature of graph data, and how does it improve over existing GNN solutions? Could the method potentially be applied to other types of non-Euclidean data structures beyond graphs? Is there a specific reason that GAT and GIN are chosen for encoding, and could alternative GNN architectures potentially offer better results?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The proposed method effectively enhances LLMs' performance in knowledge-intensive question-answering tasks by introducing a novel attention mechanism for external knowledge integration. This dynamic and layer-wise integration contributes to more robust utilization of external knowledge, allowing for improved performance compared to traditional RAG approaches.\" Weaknesses='The description of the method lacks sufficient detail, particularly in explaining how the external knowledge attention mechanism is implemented and integrated into the existing LLM framework. There is also a lack of direct comparison in terms of efficiency gains, as the claimed advantages of reduced inference latency and increased throughput are not empirically validated in the results.' Questions='How exactly does the memory-based approach differentiate between relevant and less relevant knowledge across different layers of the LLM? Are there specific criteria or thresholds used to assign layer-wise weights to the knowledge representations?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"The proposed method effectively enhances LLMs' performance in knowledge-intensive question-answering tasks by introducing a novel attention mechanism for external knowledge integration. This dynamic and layer-wise integration contributes to more robust utilization of external knowledge, allowing for improved performance compared to traditional RAG approaches.\" Weaknesses='The description of the method lacks sufficient detail, particularly in explaining how the external knowledge attention mechanism is implemented and integrated into the existing LLM framework. There is also a lack of direct comparison in terms of efficiency gains, as the claimed advantages of reduced inference latency and increased throughput are not empirically validated in the results.' Questions='How exactly does the memory-based approach differentiate between relevant and less relevant knowledge across different layers of the LLM? Are there specific criteria or thresholds used to assign layer-wise weights to the knowledge representations?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel GRL algorithm with adaptive sampling rates to ensure mean return above a performance threshold, which is a significant contribution to addressing the sample inefficiency in reinforcement learning. The roll-back feature (GRL-RB) increases robustness, providing a mechanism to dynamically adjust sampling rates and recover from potential performance dips.' Weaknesses=\"There are some limitations in the empirical evaluation, which lacks comprehensiveness, with only two simpler scenarios tested and relatively small gains in practical performance. Additionally, the claims of 'performance guarantees' are somewhat overstated, as they rely on assumptions that may not hold in practical scenarios. The inclusion of a roll-back feature to amend performance suggests that these guarantees are not fully realized.\" Questions=\"Can you clarify why the performance evaluation of baselines like LD doesn't match the expected static sampling rates? Why not degrade alpha faster, using the current alpha instead of the original alpha? Also, how do the derived sampling rates fair when assumptions are breached, and is there any strategy for violation detection other than roll-back?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces a novel GRL algorithm with adaptive sampling rates to ensure mean return above a performance threshold, which is a significant contribution to addressing the sample inefficiency in reinforcement learning. The roll-back feature (GRL-RB) increases robustness, providing a mechanism to dynamically adjust sampling rates and recover from potential performance dips.' Weaknesses=\"There are some limitations in the empirical evaluation, which lacks comprehensiveness, with only two simpler scenarios tested and relatively small gains in practical performance. Additionally, the claims of 'performance guarantees' are somewhat overstated, as they rely on assumptions that may not hold in practical scenarios. The inclusion of a roll-back feature to amend performance suggests that these guarantees are not fully realized.\" Questions=\"Can you clarify why the performance evaluation of baselines like LD doesn't match the expected static sampling rates? Why not degrade alpha faster, using the current alpha instead of the original alpha? Also, how do the derived sampling rates fair when assumptions are breached, and is there any strategy for violation detection other than roll-back?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths=\"The paper introduces a novel framework, DoD, leveraging visual priors in diffusion models, achieving state-of-the-art results with reduced computational cost. The multi-stage framework and the use of Latent Embedding Module (LEM) to compress sample information are innovative approaches addressing the issues of texture detail reproduction. The paper's clear presentation, along with conducted experiments, strengthens its argument.\" Weaknesses='The proposal lacks comprehensive comparison with other advanced image generation models or methods deploying visual priors. The reduction in computational costs and improvements in output quality needs more quantifiable evidence across varied datasets. Additionally, while the DoD framework is innovative, the paper could enhance its contribution by clarifying the scope of its applicability beyond the ImageNet dataset and exploring other possible applications or benefits.' Questions='How does DoD compare with other diffusion-based models incorporating visual priors in different environments or datasets? Can this approach leverage other types of priors, such as those from text or audio, and what adaptations might be necessary? Are there any identified limitations or scenarios where the proposed method might not perform as expected?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths=\"The paper introduces a novel framework, DoD, leveraging visual priors in diffusion models, achieving state-of-the-art results with reduced computational cost. The multi-stage framework and the use of Latent Embedding Module (LEM) to compress sample information are innovative approaches addressing the issues of texture detail reproduction. The paper's clear presentation, along with conducted experiments, strengthens its argument.\" Weaknesses='The proposal lacks comprehensive comparison with other advanced image generation models or methods deploying visual priors. The reduction in computational costs and improvements in output quality needs more quantifiable evidence across varied datasets. Additionally, while the DoD framework is innovative, the paper could enhance its contribution by clarifying the scope of its applicability beyond the ImageNet dataset and exploring other possible applications or benefits.' Questions='How does DoD compare with other diffusion-based models incorporating visual priors in different environments or datasets? Can this approach leverage other types of priors, such as those from text or audio, and what adaptations might be necessary? Are there any identified limitations or scenarios where the proposed method might not perform as expected?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The integration of a robust adaptation module within a single architecture is a significant step forward in achieving generalization for both ID and OOD scenarios. The use of epistemic neural networks to quantify uncertainty provides a novel approach that could have implications for other areas of deep RL. The validation on realistic simulated tasks provides a compelling proof of concept for the algorithm's effectiveness in real-world settings.\" Weaknesses='While the method presents an innovative unification of ID and OOD approaches, the paper could benefit from additional comparisons with a broader range of existing methods to more conclusively demonstrate its superiority. The robustness of the evaluation could be questioned due to the focus on a singular type of task (locomotion). Broader testing across various domains would strengthen claims of general applicability.' Questions='1. Could the authors provide more detail on how the epistemic neural network specifically influences the training process and final policies?\\n2. How does the GRAM framework handle situations where the ID and OOD distinctions are not clear-cut or have overlapping characteristics?\\n3. Considering the reliance on simulated environments, what steps are necessary to ensure that the findings are transferable to untested real-world scenarios?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The integration of a robust adaptation module within a single architecture is a significant step forward in achieving generalization for both ID and OOD scenarios. The use of epistemic neural networks to quantify uncertainty provides a novel approach that could have implications for other areas of deep RL. The validation on realistic simulated tasks provides a compelling proof of concept for the algorithm's effectiveness in real-world settings.\" Weaknesses='While the method presents an innovative unification of ID and OOD approaches, the paper could benefit from additional comparisons with a broader range of existing methods to more conclusively demonstrate its superiority. The robustness of the evaluation could be questioned due to the focus on a singular type of task (locomotion). Broader testing across various domains would strengthen claims of general applicability.' Questions='1. Could the authors provide more detail on how the epistemic neural network specifically influences the training process and final policies?\\n2. How does the GRAM framework handle situations where the ID and OOD distinctions are not clear-cut or have overlapping characteristics?\\n3. Considering the reliance on simulated environments, what steps are necessary to ensure that the findings are transferable to untested real-world scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='The paper identifies a significant gap in privacy auditing by enhancing the design of canaries to uncover more realistic memorization behavior in LLMs. The development of new token canaries shows promising results, strengthening the empirical analysis of privacy risks associated with LLMs. It is a well-structured study with thorough experiments across different models and conditions.' Weaknesses='The novelty of using canaries for privacy audits is somewhat limited, as the paper primarily enhances existing structures rather than introducing a fundamentally new paradigm. Additionally, while effective, the robustness of the new canaries might not be universally applicable across all LLM architectures and datasets, potentially limiting generalizability.' Questions='How do the designed canaries fare in terms of performance across various model architectures not explicitly tested in the paper? How sensitive is the proposed methodology to variations in dataset distributions or model architectures?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths='The paper identifies a significant gap in privacy auditing by enhancing the design of canaries to uncover more realistic memorization behavior in LLMs. The development of new token canaries shows promising results, strengthening the empirical analysis of privacy risks associated with LLMs. It is a well-structured study with thorough experiments across different models and conditions.' Weaknesses='The novelty of using canaries for privacy audits is somewhat limited, as the paper primarily enhances existing structures rather than introducing a fundamentally new paradigm. Additionally, while effective, the robustness of the new canaries might not be universally applicable across all LLM architectures and datasets, potentially limiting generalizability.' Questions='How do the designed canaries fare in terms of performance across various model architectures not explicitly tested in the paper? How sensitive is the proposed methodology to variations in dataset distributions or model architectures?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The design of the dual-branch architecture combined with spatial-temporal attention is highly innovative, providing better handling of implicit disparity and enhancing frame-to-frame consistency.\\n2. The introduction of the YouTube-SBS dataset significantly contributes to the stereo video generation research community, offering a vast resource for evaluation and development of new models.\\n3. The model demonstrates considerable improvements in standard metrics (L1, SSIM, PSNR), indicating robust performance.' Weaknesses='1. The paper could provide more detailed comparative analysis and ablation studies to better understand the individual contributions of each component within the ImmersePro framework.\\n2. Some key details regarding the implementation and training of the model, as well as specifics about the dataset, are lacking, which could affect reproducibility and further experimentation by other researchers.\\n3. The technique relies on significant computational resources due to its complex architecture, potentially limiting its access and applicability in broader, less resource-rich environments.' Questions='1. Can the authors provide more insights into the efficiency and speed of the proposed model compared to traditional methods?\\n2. How does the model performance vary with different genres or types of video content present in the YouTube-SBS dataset?\\n3. Are there any known limitations or failure cases for ImmersePro that have been observed, especially in terms of handling highly dynamic scenes or scenes with intricate occlusions?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. The design of the dual-branch architecture combined with spatial-temporal attention is highly innovative, providing better handling of implicit disparity and enhancing frame-to-frame consistency.\\n2. The introduction of the YouTube-SBS dataset significantly contributes to the stereo video generation research community, offering a vast resource for evaluation and development of new models.\\n3. The model demonstrates considerable improvements in standard metrics (L1, SSIM, PSNR), indicating robust performance.' Weaknesses='1. The paper could provide more detailed comparative analysis and ablation studies to better understand the individual contributions of each component within the ImmersePro framework.\\n2. Some key details regarding the implementation and training of the model, as well as specifics about the dataset, are lacking, which could affect reproducibility and further experimentation by other researchers.\\n3. The technique relies on significant computational resources due to its complex architecture, potentially limiting its access and applicability in broader, less resource-rich environments.' Questions='1. Can the authors provide more insights into the efficiency and speed of the proposed model compared to traditional methods?\\n2. How does the model performance vary with different genres or types of video content present in the YouTube-SBS dataset?\\n3. Are there any known limitations or failure cases for ImmersePro that have been observed, especially in terms of handling highly dynamic scenes or scenes with intricate occlusions?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The integration of triplet relations into preference learning models is novel and shows a marked improvement in the accuracy and capability of protein design beyond training data.\\n2. The proposed method demonstrates clear advantages in experimental results, outperforming baseline approaches across various datasets and conditions.\\n3. The application-oriented nature of the paper contributes both theoretically and practically to the field of bioinformatics, particularly in protein design.' Weaknesses='1. The approach assumes an accurate scoring function, which is typically challenging to obtain in real-world scenarios, and may not address the potential noise effectively in scoring functions.\\n2. While effective, the method may require significant computational resources for constructing and evaluating triplet datasets, which could be a barrier for practical, large-scale applications.' Questions='1. How does the approach handle noise in scoring functions during triplet creation? Are there mechanisms in place to mitigate errors due to inaccurate scorers?\\n2. What are the computational implications of scaling this approach to very large protein datasets? Are there efficiency improvements that can be considered in future work?\\n3. Can the approach be generalized to other domains where extrapolative design is required, or is it particularly tailored to the nature of protein data?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The integration of triplet relations into preference learning models is novel and shows a marked improvement in the accuracy and capability of protein design beyond training data.\\n2. The proposed method demonstrates clear advantages in experimental results, outperforming baseline approaches across various datasets and conditions.\\n3. The application-oriented nature of the paper contributes both theoretically and practically to the field of bioinformatics, particularly in protein design.' Weaknesses='1. The approach assumes an accurate scoring function, which is typically challenging to obtain in real-world scenarios, and may not address the potential noise effectively in scoring functions.\\n2. While effective, the method may require significant computational resources for constructing and evaluating triplet datasets, which could be a barrier for practical, large-scale applications.' Questions='1. How does the approach handle noise in scoring functions during triplet creation? Are there mechanisms in place to mitigate errors due to inaccurate scorers?\\n2. What are the computational implications of scaling this approach to very large protein datasets? Are there efficiency improvements that can be considered in future work?\\n3. Can the approach be generalized to other domains where extrapolative design is required, or is it particularly tailored to the nature of protein data?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The introduction of an adaptive data augmentation method addressing the inefficiencies in traditional data augmentation for time series is promising. The use of reinforcement learning to target marginal samples and optimize a neural augmentor is innovative, potentially leading to more consistent prediction errors and improved overall forecasting accuracy.' Weaknesses='The requirement for a robust model zoo could limit the applicability of the proposed method in environments where such resources are not readily available. Additionally, the effectiveness of the reinforcement learning framework relies heavily on the quality of variance as a reward structure, which might introduce complexities and dependencies that are not fully explored or justified in the paper.' Questions='How does the proposed method handle scenarios where the model zoo is imperfect or limited in scope? Are there any specific strategies suggested for constructing an effective model zoo or mitigating potential issues arising from a limited zoo?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The introduction of an adaptive data augmentation method addressing the inefficiencies in traditional data augmentation for time series is promising. The use of reinforcement learning to target marginal samples and optimize a neural augmentor is innovative, potentially leading to more consistent prediction errors and improved overall forecasting accuracy.' Weaknesses='The requirement for a robust model zoo could limit the applicability of the proposed method in environments where such resources are not readily available. Additionally, the effectiveness of the reinforcement learning framework relies heavily on the quality of variance as a reward structure, which might introduce complexities and dependencies that are not fully explored or justified in the paper.' Questions='How does the proposed method handle scenarios where the model zoo is imperfect or limited in scope? Are there any specific strategies suggested for constructing an effective model zoo or mitigating potential issues arising from a limited zoo?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. AlignCLIP addresses a crucial challenge in existing CLIP models by effectively reducing the modality gap, thus improving cross-modal tasks.\\n2. The method significantly enhances zero-shot classification and multi-modal retrieval, demonstrating practical impact across a variety of downstream tasks.\\n3. The paper is well-written, clearly outlining the problem, solution, and the results in a comprehensive manner.' Weaknesses=\"1. While the approach is innovative, it builds upon existing structures like SBERT and CLIP's transformer-based encoder, which may limit the novelty of the contribution.\\n2. The extent to which parameter-sharing versus intra-modality separation contributes to performance gains is not entirely clear, warranting further exploration.\\n3. The work could benefit from exploring more varied datasets or conditions to further generalize its findings.\" Questions='1. How does the intra-modality separation objective specifically impact embeddings in comparison to other methods?\\n2. What are the limitations or challenges encountered when applying parameter-sharing across the modality encoders?\\n3. Could the improvements seen in zero-shot classification be primarily attributed to the semantic regularization component, and how might this be isolated in experimentation?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='1. AlignCLIP addresses a crucial challenge in existing CLIP models by effectively reducing the modality gap, thus improving cross-modal tasks.\\n2. The method significantly enhances zero-shot classification and multi-modal retrieval, demonstrating practical impact across a variety of downstream tasks.\\n3. The paper is well-written, clearly outlining the problem, solution, and the results in a comprehensive manner.' Weaknesses=\"1. While the approach is innovative, it builds upon existing structures like SBERT and CLIP's transformer-based encoder, which may limit the novelty of the contribution.\\n2. The extent to which parameter-sharing versus intra-modality separation contributes to performance gains is not entirely clear, warranting further exploration.\\n3. The work could benefit from exploring more varied datasets or conditions to further generalize its findings.\" Questions='1. How does the intra-modality separation objective specifically impact embeddings in comparison to other methods?\\n2. What are the limitations or challenges encountered when applying parameter-sharing across the modality encoders?\\n3. Could the improvements seen in zero-shot classification be primarily attributed to the semantic regularization component, and how might this be isolated in experimentation?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper proposes an innovative use of behavioral loss in autoencoders to improve the functional equivalence of reconstructed neural networks, which enhances their performance.\\n2. The integration of both structural and behavioral losses shows clear empirical benefits in reconstructing and generating high-performing neural networks, demonstrating a significant improvement over previous methods.\\n3. The methodology is evaluated on diverse datasets, showcasing its general applicability and effectiveness.\\n4. The approach is straightforward and potentially impactful for tasks involving model reconstruction and weight space learning.' Weaknesses='1. While the incorporation of behavioral loss is novel, the conceptual contribution might be seen as somewhat incremental, given the existing literature that explores functional aspects of neural networks.\\n2. The experimental evaluation, though performed on multiple datasets, could benefit from deeper analysis or additional experiments to better understand the boundary and scalability of the approach.\\n3. The paper could improve in terms of clarity in presenting detailed methodology and results, as some technical aspects might not be thoroughly elucidated for all readers.' Questions='1. How does the method scale with larger or more complex neural networks? Are there any identified limitations in terms of computational cost or feasibility for very large models?\\n2. Could the proposed methodology be adapted or extended to other domains outside the current scope of datasets, such as NLP or reinforcement learning?\\n3. How sensitive is the proposed approach to the choice of hyperparameters within the composite loss function, and what strategy is recommended for optimal hyperparameter tuning?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. The paper proposes an innovative use of behavioral loss in autoencoders to improve the functional equivalence of reconstructed neural networks, which enhances their performance.\\n2. The integration of both structural and behavioral losses shows clear empirical benefits in reconstructing and generating high-performing neural networks, demonstrating a significant improvement over previous methods.\\n3. The methodology is evaluated on diverse datasets, showcasing its general applicability and effectiveness.\\n4. The approach is straightforward and potentially impactful for tasks involving model reconstruction and weight space learning.' Weaknesses='1. While the incorporation of behavioral loss is novel, the conceptual contribution might be seen as somewhat incremental, given the existing literature that explores functional aspects of neural networks.\\n2. The experimental evaluation, though performed on multiple datasets, could benefit from deeper analysis or additional experiments to better understand the boundary and scalability of the approach.\\n3. The paper could improve in terms of clarity in presenting detailed methodology and results, as some technical aspects might not be thoroughly elucidated for all readers.' Questions='1. How does the method scale with larger or more complex neural networks? Are there any identified limitations in terms of computational cost or feasibility for very large models?\\n2. Could the proposed methodology be adapted or extended to other domains outside the current scope of datasets, such as NLP or reinforcement learning?\\n3. How sensitive is the proposed approach to the choice of hyperparameters within the composite loss function, and what strategy is recommended for optimal hyperparameter tuning?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='EIQL provides a novel approach to offline RL by effectively balancing in-sample and out-of-sample actions, demonstrating improved performance, especially in sparse reward settings and incomplete trajectories.' Weaknesses=\"The paper's method relies on stochastic updates without a thorough analysis of how the Bernoulli parameter affects performance, leaving open questions on its dynamic adaptability and impact on stability.\" Questions='How sensitive is EIQL to the choice of the Bernoulli parameter and was there any analysis on tuning this parameter for different environments within the D4RL benchmark?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='EIQL provides a novel approach to offline RL by effectively balancing in-sample and out-of-sample actions, demonstrating improved performance, especially in sparse reward settings and incomplete trajectories.' Weaknesses=\"The paper's method relies on stochastic updates without a thorough analysis of how the Bernoulli parameter affects performance, leaving open questions on its dynamic adaptability and impact on stability.\" Questions='How sensitive is EIQL to the choice of the Bernoulli parameter and was there any analysis on tuning this parameter for different environments within the D4RL benchmark?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=4 Strengths='The paper provides a novel approach to understanding the adaptive complexity of sampling algorithms for log-concave distributions, filling a significant gap in the literature regarding high-accuracy and high-dimensional contexts. The use of hardness potentials and random partitions is innovative and adds depth to the analysis of sampling algorithm limits.' Weaknesses='The paper assumes a strong background in high-dimensional statistical analysis and parallel computing models, which might limit accessibility for a broader audience. While the findings are technically sound, the paper could benefit from more comprehensive empirical evaluations or examples to illustrate the practical implications of the stated limits.' Questions='How do the proposed lower bounds translate into practical algorithm design strategies? Are there potential avenues for overcoming these fundamental limits, or do they suggest the necessity for paradigm shifts in how we approach sampling in high-dimensional settings?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=5 Confidence=4 Strengths='The paper provides a novel approach to understanding the adaptive complexity of sampling algorithms for log-concave distributions, filling a significant gap in the literature regarding high-accuracy and high-dimensional contexts. The use of hardness potentials and random partitions is innovative and adds depth to the analysis of sampling algorithm limits.' Weaknesses='The paper assumes a strong background in high-dimensional statistical analysis and parallel computing models, which might limit accessibility for a broader audience. While the findings are technically sound, the paper could benefit from more comprehensive empirical evaluations or examples to illustrate the practical implications of the stated limits.' Questions='How do the proposed lower bounds translate into practical algorithm design strategies? Are there potential avenues for overcoming these fundamental limits, or do they suggest the necessity for paradigm shifts in how we approach sampling in high-dimensional settings?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper proposes a new reward-model-free method for aligning LLMs with human preferences. The approach highlights the balance between preference alignment and regularization to retain model consistency. Empirical results demonstrate the method's effectiveness and potential advantages over existing techniques.\" Weaknesses='The core innovation of integrating preference loss with regularization needs more depth in theoretical exploration. Detailed comparisons with alternative methods, especially RLHF-based approaches, are necessary to strengthen the argument of SPO offering significant practical benefits. The paper also lacks a comprehensive error analysis, and explanation of how the method scales with larger models or more varied data scenarios is scarce.' Questions=\"1. Can further justification be provided on how the proposed regularization technique enhances model robustness compared to existing methods? 2. How does the performance of SPO vary with different datasets, especially out-of-distribution data not covered in initial experiments? 3. Could you explain the specific impacts of hyperparameter variations on SPO's performance, such as softmax exponent adjustment?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper proposes a new reward-model-free method for aligning LLMs with human preferences. The approach highlights the balance between preference alignment and regularization to retain model consistency. Empirical results demonstrate the method's effectiveness and potential advantages over existing techniques.\" Weaknesses='The core innovation of integrating preference loss with regularization needs more depth in theoretical exploration. Detailed comparisons with alternative methods, especially RLHF-based approaches, are necessary to strengthen the argument of SPO offering significant practical benefits. The paper also lacks a comprehensive error analysis, and explanation of how the method scales with larger models or more varied data scenarios is scarce.' Questions=\"1. Can further justification be provided on how the proposed regularization technique enhances model robustness compared to existing methods? 2. How does the performance of SPO vary with different datasets, especially out-of-distribution data not covered in initial experiments? 3. Could you explain the specific impacts of hyperparameter variations on SPO's performance, such as softmax exponent adjustment?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=3 Strengths='1. SyntheOcc introduces a novel framework for synthesizing geometrically controlled images conditioned on 3D occupancy labels using diffusion models, which could significantly enhance data augmentation for autonomous driving.\\n2. The use of 3D semantic multi-plane images (MPIs) allows for precise geometric controllability, spatial alignment, and preservation of complex scene geometry, enhancing the quality of synthetic datasets.\\n3. Demonstrated superior performance in generating task-aligned synthetic data and improving robustness of perception models in 3D occupancy prediction tasks.' Weaknesses='1. The paper, while innovative, may require more comprehensive comparisons against a wider array of existing methods beyond the nuScenes dataset to confirm its generalizability and robustness across diverse scenarios.\\n2. The discussion on the limitations and potential bottlenecks in computational requirements or scalability of the proposed approach is limited, which might be crucial for real-world application.\\n3. The complexity of integrating multi-view and temporal consistency strategies is high, and the effectiveness of these approaches is not clearly quantified or compared against existing models in enough detail.' Questions='1. How does SyntheOcc compare to other state-of-the-art methods in terms of computational efficiency and scalability? Is real-time implementation feasible?\\n2. Can the MPI-based approach be generalized to other domains or applications outside of autonomous driving where 3D geometry is critical?\\n3. What are the specific limitations of the proposed method when dealing with highly complex scenes, and how can these be addressed in future work?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=4 Rating=8 Confidence=3 Strengths='1. SyntheOcc introduces a novel framework for synthesizing geometrically controlled images conditioned on 3D occupancy labels using diffusion models, which could significantly enhance data augmentation for autonomous driving.\\n2. The use of 3D semantic multi-plane images (MPIs) allows for precise geometric controllability, spatial alignment, and preservation of complex scene geometry, enhancing the quality of synthetic datasets.\\n3. Demonstrated superior performance in generating task-aligned synthetic data and improving robustness of perception models in 3D occupancy prediction tasks.' Weaknesses='1. The paper, while innovative, may require more comprehensive comparisons against a wider array of existing methods beyond the nuScenes dataset to confirm its generalizability and robustness across diverse scenarios.\\n2. The discussion on the limitations and potential bottlenecks in computational requirements or scalability of the proposed approach is limited, which might be crucial for real-world application.\\n3. The complexity of integrating multi-view and temporal consistency strategies is high, and the effectiveness of these approaches is not clearly quantified or compared against existing models in enough detail.' Questions='1. How does SyntheOcc compare to other state-of-the-art methods in terms of computational efficiency and scalability? Is real-time implementation feasible?\\n2. Can the MPI-based approach be generalized to other domains or applications outside of autonomous driving where 3D geometry is critical?\\n3. What are the specific limitations of the proposed method when dealing with highly complex scenes, and how can these be addressed in future work?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The method significantly reduces X-ray exposure by up to 4x while maintaining image reconstruction quality, which is a notable improvement.\\n2. Integrating generative diffusion models with active learning to adaptively select data measurements is an innovative approach.\\n3. The method is tested on multiple real-world datasets demonstrating its broad applicability.' Weaknesses='1. The approach may have limitations in high-stakes applications like medical imaging due to trade-offs that are not fully addressed.\\n2. The computational complexity and the inference time of the proposed method deserve more detailed consideration.\\n3. Detailed comparison with existing active learning methods in CT is needed, as well as discussion of the potential impact of different projection geometries.' Questions='1. How does the model handle the potential trade-offs and limitations in high-stakes applications such as medical imaging?\\n2. Can the method be extended to more complex settings like fan-beam or 3D cone beam CT?\\n3. What are the specific computational challenges and how can they be addressed to improve the efficiency of the method?\\n4. How does the distribution of selected angles vary across different datasets and what implications does this have for broader applications?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The method significantly reduces X-ray exposure by up to 4x while maintaining image reconstruction quality, which is a notable improvement.\\n2. Integrating generative diffusion models with active learning to adaptively select data measurements is an innovative approach.\\n3. The method is tested on multiple real-world datasets demonstrating its broad applicability.' Weaknesses='1. The approach may have limitations in high-stakes applications like medical imaging due to trade-offs that are not fully addressed.\\n2. The computational complexity and the inference time of the proposed method deserve more detailed consideration.\\n3. Detailed comparison with existing active learning methods in CT is needed, as well as discussion of the potential impact of different projection geometries.' Questions='1. How does the model handle the potential trade-offs and limitations in high-stakes applications such as medical imaging?\\n2. Can the method be extended to more complex settings like fan-beam or 3D cone beam CT?\\n3. What are the specific computational challenges and how can they be addressed to improve the efficiency of the method?\\n4. How does the distribution of selected angles vary across different datasets and what implications does this have for broader applications?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=4 Strengths=\"1. The paper addresses a significant challenge faced by foundation vision models like SAM, enhancing them to perform semantic, instance, and panoptic segmentation in open-vocabulary settings. 2. The introduction of composable prompts to address over-segmentation is an interesting and novel approach. 3. The unified affinity framework to merge patches efficiently, avoiding excessive computational complexity, is well-conceived and executed. 4. Extensive evaluations on COCO and ADE20K datasets demonstrate the method's effectiveness, achieving state-of-the-art performance.\" Weaknesses=\"1. The method, while improving SAM's capabilities, may still rely heavily on the inherent strengths of SAM, thereby limiting its standalone impact if SAM itself evolves in future iterations. 2. The approach might require fine-tuning of hyperparameters for different datasets or contexts, which can affect its general applicability. 3. While promising, the reliance on the SAM framework might restrict adaptability to rapidly evolving or entirely new image segmentation tasks not accounted for by SAM.\" Questions=\"1. How does SAM-CP handle cases where the initial SAM output is highly fragmented due to inherent image complexities, and how robust is the method to varying levels of granularity in initial segmentation? 2. How does the model's performance hold up when tasked with segmenting datasets that are markedly different from the training datasets in terms of semantics and objects present? 3. Given that SAM-CP relies on composable prompts, what is the sensitivity of the results to the choice of prompts, and is there an automated way to select or optimize these prompts?\"\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=4 Strengths=\"1. The paper addresses a significant challenge faced by foundation vision models like SAM, enhancing them to perform semantic, instance, and panoptic segmentation in open-vocabulary settings. 2. The introduction of composable prompts to address over-segmentation is an interesting and novel approach. 3. The unified affinity framework to merge patches efficiently, avoiding excessive computational complexity, is well-conceived and executed. 4. Extensive evaluations on COCO and ADE20K datasets demonstrate the method's effectiveness, achieving state-of-the-art performance.\" Weaknesses=\"1. The method, while improving SAM's capabilities, may still rely heavily on the inherent strengths of SAM, thereby limiting its standalone impact if SAM itself evolves in future iterations. 2. The approach might require fine-tuning of hyperparameters for different datasets or contexts, which can affect its general applicability. 3. While promising, the reliance on the SAM framework might restrict adaptability to rapidly evolving or entirely new image segmentation tasks not accounted for by SAM.\" Questions=\"1. How does SAM-CP handle cases where the initial SAM output is highly fragmented due to inherent image complexities, and how robust is the method to varying levels of granularity in initial segmentation? 2. How does the model's performance hold up when tasked with segmenting datasets that are markedly different from the training datasets in terms of semantics and objects present? 3. Given that SAM-CP relies on composable prompts, what is the sensitivity of the results to the choice of prompts, and is there an automated way to select or optimize these prompts?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The proposed model L-C4 presents a novel approach to video colorization by utilizing language inputs, which is a creative and user-friendly method. The use of a cross-modality generative model to guide the colorization process and the incorporation of components like temporally deformable attention and cross-clip fusion are key innovations contributing to improved inter-frame and long-term consistency. The model's ability to outperform existing methods across benchmarks underscores its effectiveness. The paper is well-structured and clearly articulated, making the methodologies and findings accessible.\" Weaknesses='The paper does not thoroughly discuss potential limitations of the approach, such as its scalability to very large video datasets or possible challenges with ambiguous or complex text inputs. Additionally, the reliance on a pre-trained model and the specific dataset used for training may limit the generalizability of the approach to a broader range of video types and styles.' Questions='How does L-C4 handle ambiguous or conflicting text descriptions, and what is the impact on the colorization results? Additionally, can the approach be easily adapted or extended to include other modalities or inputs, such as sound or user-provided reference images, to further guide the colorization process? Could further insights be provided regarding the computational efficiency of the approach when applied to higher resolution videos?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The proposed model L-C4 presents a novel approach to video colorization by utilizing language inputs, which is a creative and user-friendly method. The use of a cross-modality generative model to guide the colorization process and the incorporation of components like temporally deformable attention and cross-clip fusion are key innovations contributing to improved inter-frame and long-term consistency. The model's ability to outperform existing methods across benchmarks underscores its effectiveness. The paper is well-structured and clearly articulated, making the methodologies and findings accessible.\" Weaknesses='The paper does not thoroughly discuss potential limitations of the approach, such as its scalability to very large video datasets or possible challenges with ambiguous or complex text inputs. Additionally, the reliance on a pre-trained model and the specific dataset used for training may limit the generalizability of the approach to a broader range of video types and styles.' Questions='How does L-C4 handle ambiguous or conflicting text descriptions, and what is the impact on the colorization results? Additionally, can the approach be easily adapted or extended to include other modalities or inputs, such as sound or user-provided reference images, to further guide the colorization process? Could further insights be provided regarding the computational efficiency of the approach when applied to higher resolution videos?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths=\"The proposed framework, Privacy-aware Lifelong Learning (PALL), introduces a novel approach to integrating lifelong learning with machine unlearning, addressing a crucial privacy concern in AI applications. The method's ability to optimize task-specific sparse subnetworks within a single neural network architecture significantly enhances memory efficiency. Additionally, PALL demonstrates scalability and achieves state-of-the-art performance, preventing catastrophic forgetting while also allowing exact task unlearning.\" Weaknesses=\"The paper may lack clarity in explaining how various baselines, particularly in terms of regularization-based methods, were adapted to the privacy-aware lifelong learning setting. Furthermore, the evaluation appears limited to only two datasets, raising concerns about generalizability across other domains. The reliance on synthetic datasets for certain evaluations may not adequately demonstrate the method's real-world applicability. Furthermore, while the accuracy on unlearned tasks is considered, this is a weak proxy for unlearning conclusions.\" Questions='1. How does the proposed method handle scenarios where user data is not neatly separated into distinct tasks? Can the framework adapt to situations where multiple users participate in the same task?\\n2. What specific baseline manipulation and performance guarantees were provided for various baselines to handle unlearning requests? Clarification on how baseline accuracies were interpreted would be helpful.\\n3. Can the authors elaborate on the interpretation and real-world applicability of the reported unlearning metric A_u, and how it correlates with more rigorous unlearning metrics?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=4 Strengths=\"The proposed framework, Privacy-aware Lifelong Learning (PALL), introduces a novel approach to integrating lifelong learning with machine unlearning, addressing a crucial privacy concern in AI applications. The method's ability to optimize task-specific sparse subnetworks within a single neural network architecture significantly enhances memory efficiency. Additionally, PALL demonstrates scalability and achieves state-of-the-art performance, preventing catastrophic forgetting while also allowing exact task unlearning.\" Weaknesses=\"The paper may lack clarity in explaining how various baselines, particularly in terms of regularization-based methods, were adapted to the privacy-aware lifelong learning setting. Furthermore, the evaluation appears limited to only two datasets, raising concerns about generalizability across other domains. The reliance on synthetic datasets for certain evaluations may not adequately demonstrate the method's real-world applicability. Furthermore, while the accuracy on unlearned tasks is considered, this is a weak proxy for unlearning conclusions.\" Questions='1. How does the proposed method handle scenarios where user data is not neatly separated into distinct tasks? Can the framework adapt to situations where multiple users participate in the same task?\\n2. What specific baseline manipulation and performance guarantees were provided for various baselines to handle unlearning requests? Clarification on how baseline accuracies were interpreted would be helpful.\\n3. Can the authors elaborate on the interpretation and real-world applicability of the reported unlearning metric A_u, and how it correlates with more rigorous unlearning metrics?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper proposes a well-defined approach that improves the interaction and representation of features in cross-view geo-localization, which is an area of significant interest. The introduction of a new dataset (G2D) adds value to the research community. The methodologies (CVCAM and MHSAM) are innovative in enhancing cross-view feature interactions and representation. Empirical results demonstrate clear improvements over existing state-of-the-art methods.' Weaknesses=\"The paper may not adequately address the scalability of the approach for larger, real-world datasets. While the proposed modules show promising results, further analysis of hyperparameter sensitivity and computational complexity could provide a more thorough understanding of the approach's practical utility. Additionally, the paper could benefit from a more in-depth discussion on how the proposed modules can handle varying environmental factors commonly encountered in real-world applications, such as lighting and weather changes.\" Questions='1. How does the proposed method handle variations in image quality, such as those caused by poor lighting or adverse weather conditions?\\n2. Can the approach be adapted for real-time applications in autonomous systems, considering computational constraints?\\n3. What are the specific criteria for selecting convolutional kernel sizes in the MHSAM, and how do they affect performance?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper proposes a well-defined approach that improves the interaction and representation of features in cross-view geo-localization, which is an area of significant interest. The introduction of a new dataset (G2D) adds value to the research community. The methodologies (CVCAM and MHSAM) are innovative in enhancing cross-view feature interactions and representation. Empirical results demonstrate clear improvements over existing state-of-the-art methods.' Weaknesses=\"The paper may not adequately address the scalability of the approach for larger, real-world datasets. While the proposed modules show promising results, further analysis of hyperparameter sensitivity and computational complexity could provide a more thorough understanding of the approach's practical utility. Additionally, the paper could benefit from a more in-depth discussion on how the proposed modules can handle varying environmental factors commonly encountered in real-world applications, such as lighting and weather changes.\" Questions='1. How does the proposed method handle variations in image quality, such as those caused by poor lighting or adverse weather conditions?\\n2. Can the approach be adapted for real-time applications in autonomous systems, considering computational constraints?\\n3. What are the specific criteria for selecting convolutional kernel sizes in the MHSAM, and how do they affect performance?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper offers a novel matrix-theoretic perspective on the adversarial fragility of neural networks, linking feature compression to reduced robustness. This theoretical framework could provide valuable insights into the relationship between input dimensions and adversarial weakness, adding depth to existing studies on this topic.' Weaknesses='The paper relies on the assumption of Gaussian distributed data and matrices, which may not fully capture the complexities of real-world datasets and neural network structures. The results, while theoretically interesting, might lack practical applicability due to these assumptions. Additionally, the experiments conducted are largely numerical and may not convincingly demonstrate real-world relevance across diverse datasets and scenarios.' Questions='How does the proposed analysis generalize beyond Gaussian assumptions, particularly in real-world scenarios where data distributions can be complex and multimodal? Can the matrix-theoretic analysis incorporate more diverse neural network architectures or data types, such as convolutional neural networks or sequence data? What are the implications of these findings on designing more robust neural network architectures in practice?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper offers a novel matrix-theoretic perspective on the adversarial fragility of neural networks, linking feature compression to reduced robustness. This theoretical framework could provide valuable insights into the relationship between input dimensions and adversarial weakness, adding depth to existing studies on this topic.' Weaknesses='The paper relies on the assumption of Gaussian distributed data and matrices, which may not fully capture the complexities of real-world datasets and neural network structures. The results, while theoretically interesting, might lack practical applicability due to these assumptions. Additionally, the experiments conducted are largely numerical and may not convincingly demonstrate real-world relevance across diverse datasets and scenarios.' Questions='How does the proposed analysis generalize beyond Gaussian assumptions, particularly in real-world scenarios where data distributions can be complex and multimodal? Can the matrix-theoretic analysis incorporate more diverse neural network architectures or data types, such as convolutional neural networks or sequence data? What are the implications of these findings on designing more robust neural network architectures in practice?'\n",
      "Zero shot review:\n",
      " Soundness=2 Presentation=3 Contribution=2 Rating=4 Confidence=3 Strengths='The paper presents an innovative approach to improving the robustness of transformer architectures by introducing layer shuffling during training. This could improve model flexibility in distributed environments and represents a creative solution to robustness issues related to layer order changes.' Weaknesses='The experimental results show a reduction in accuracy with LayerShuffle compared to sequential models, which could limit practical applications. The evaluation, while comprehensive on certain datasets, might lack generalizability and should include more model architectures to ensure broad applicability. The theoretical grounding for why shuffling improves robustness is not sufficiently detailed.' Questions='How does LayerShuffle perform in real-world distributed environments beyond theoretical and controlled dataset conditions? Also, could additional auxiliary tasks help mitigate the accuracy trade-offs observed with LayerShuffle?'\n",
      "Success: Soundness=2 Presentation=3 Contribution=2 Rating=4 Confidence=3 Strengths='The paper presents an innovative approach to improving the robustness of transformer architectures by introducing layer shuffling during training. This could improve model flexibility in distributed environments and represents a creative solution to robustness issues related to layer order changes.' Weaknesses='The experimental results show a reduction in accuracy with LayerShuffle compared to sequential models, which could limit practical applications. The evaluation, while comprehensive on certain datasets, might lack generalizability and should include more model architectures to ensure broad applicability. The theoretical grounding for why shuffling improves robustness is not sufficiently detailed.' Questions='How does LayerShuffle perform in real-world distributed environments beyond theoretical and controlled dataset conditions? Also, could additional auxiliary tasks help mitigate the accuracy trade-offs observed with LayerShuffle?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The proposed approach, FairDropout, presents an interesting technique to improve generalization on minority groups without the need for explicit group labels, which is a significant practical advantage.\\nThe paper effectively demonstrates the technique across various benchmarks, suggesting broad applicability.' Weaknesses='The method relies on assumptions regarding neuron roles and the localization of memorization, which may not hold in all contexts or models.\\nThe presentation lacks clarity in some sections, with insufficient detail on certain methodological aspects, e.g., specifics on how memorization neurons are precisely identified.\\nThe potential beneficial aspects of memorization are noted but not fully examined, which leaves open questions about the balance between generalization and memorization.' Questions='How does FairDropout determine which neurons are responsible for memorization during the training phase?\\nCan FairDropout be extended or adapted to work with other types of networks beyond those tested, and if so, what are the potential challenges?\\nWhat is the computational overhead introduced by the method, especially in larger models like BERT and ResNet-50?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The proposed approach, FairDropout, presents an interesting technique to improve generalization on minority groups without the need for explicit group labels, which is a significant practical advantage.\\nThe paper effectively demonstrates the technique across various benchmarks, suggesting broad applicability.' Weaknesses='The method relies on assumptions regarding neuron roles and the localization of memorization, which may not hold in all contexts or models.\\nThe presentation lacks clarity in some sections, with insufficient detail on certain methodological aspects, e.g., specifics on how memorization neurons are precisely identified.\\nThe potential beneficial aspects of memorization are noted but not fully examined, which leaves open questions about the balance between generalization and memorization.' Questions='How does FairDropout determine which neurons are responsible for memorization during the training phase?\\nCan FairDropout be extended or adapted to work with other types of networks beyond those tested, and if so, what are the potential challenges?\\nWhat is the computational overhead introduced by the method, especially in larger models like BERT and ResNet-50?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The method leverages a large and diverse dataset effectively, demonstrating significant improvements over state-of-the-art methods in 3D hand pose estimation.\\n2. The innovative use of PCA-based dimension reduction to form positive pairs in contrastive learning is a novel contribution that enhances learning efficacy.\\n3. The adaptive weighting mechanism for contrastive loss is a thoughtful addition that improves the learning process by prioritizing truly similar samples.' Weaknesses='1. While the PCA-based approach for identifying similar hand poses is innovative, the reliance on 2D keypoint extraction might limit its ability to capture the full 3D spatial relationships between hand components, potentially impacting accuracy in complex scenarios.\\n2. The paper might benefit from more in-depth comparisons and discussions regarding other high-performing methods beyond PeCLR, including more recent approaches not mentioned here.\\n3. The presentation could be improved, as some aspects, particularly technical details of the adaptive weighting mechanism, are not fully elaborated.' Questions='1. How does the PCA-based approach handle cases where the extracted 2D keypoints are occluded or incorrectly predicted in challenging in-the-wild conditions?\\n2. What is the impact of the adaptive weighting mechanism on the overall training time and computational resources required for the SiMHand framework compared to traditional methods?\\n3. Are there any specific challenges the method faces when scaling to even larger datasets or other domains, such as poses with tools or object interactions?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The method leverages a large and diverse dataset effectively, demonstrating significant improvements over state-of-the-art methods in 3D hand pose estimation.\\n2. The innovative use of PCA-based dimension reduction to form positive pairs in contrastive learning is a novel contribution that enhances learning efficacy.\\n3. The adaptive weighting mechanism for contrastive loss is a thoughtful addition that improves the learning process by prioritizing truly similar samples.' Weaknesses='1. While the PCA-based approach for identifying similar hand poses is innovative, the reliance on 2D keypoint extraction might limit its ability to capture the full 3D spatial relationships between hand components, potentially impacting accuracy in complex scenarios.\\n2. The paper might benefit from more in-depth comparisons and discussions regarding other high-performing methods beyond PeCLR, including more recent approaches not mentioned here.\\n3. The presentation could be improved, as some aspects, particularly technical details of the adaptive weighting mechanism, are not fully elaborated.' Questions='1. How does the PCA-based approach handle cases where the extracted 2D keypoints are occluded or incorrectly predicted in challenging in-the-wild conditions?\\n2. What is the impact of the adaptive weighting mechanism on the overall training time and computational resources required for the SiMHand framework compared to traditional methods?\\n3. Are there any specific challenges the method faces when scaling to even larger datasets or other domains, such as poses with tools or object interactions?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='The paper provides an innovative approach to MRI denoising by applying score-based self-supervised learning, which does not require high-SNR labels. The introduction of a reparameterization technique to stabilize training and the focus on preserving spatial details are notable strengths.' Weaknesses='The approach may still require extensive computational resources and specific parameter tuning. The efficacy and potential limitations of the detail refinement extension in preserving spatial details could be better explored.' Questions='1. How well does C2S perform across different types of noise not covered in the study?\\n2. Could the approach handle multi-modal data or be adapted to other imaging modalities?\\n3. What are the potential limitations or challenges in deploying C2S in real-world clinical settings?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='The paper provides an innovative approach to MRI denoising by applying score-based self-supervised learning, which does not require high-SNR labels. The introduction of a reparameterization technique to stabilize training and the focus on preserving spatial details are notable strengths.' Weaknesses='The approach may still require extensive computational resources and specific parameter tuning. The efficacy and potential limitations of the detail refinement extension in preserving spatial details could be better explored.' Questions='1. How well does C2S perform across different types of noise not covered in the study?\\n2. Could the approach handle multi-modal data or be adapted to other imaging modalities?\\n3. What are the potential limitations or challenges in deploying C2S in real-world clinical settings?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='1. Proposes a novel method to mitigate oversaturation in high guidance scales, enhancing image quality while maintaining computational efficiency.\\n2. Demonstrates compatibility of the method with various diffusion models and samplers, suggesting broad applicability.' Weaknesses='1. The decomposition into parallel and orthogonal components, while innovative, might lead to potential complexities in implementation. The balance of down-weighting and its quantifiable impact on different model settings could be explained in more depth.\\n2. The addition of features like rescaling and momentum, although effective, may require further empirical justifications or theoretical underpinnings to generalize their benefits across diverse datasets.' Questions=\"1. How does the proposed method ensure that the mitigated oversaturation doesn't compromise the artistic styles inherent to specific diffusion models?\\n2. What are the potential limitations when applying the adaptive projected guidance method to domains outside of image generation, if any?\"\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=3 Strengths='1. Proposes a novel method to mitigate oversaturation in high guidance scales, enhancing image quality while maintaining computational efficiency.\\n2. Demonstrates compatibility of the method with various diffusion models and samplers, suggesting broad applicability.' Weaknesses='1. The decomposition into parallel and orthogonal components, while innovative, might lead to potential complexities in implementation. The balance of down-weighting and its quantifiable impact on different model settings could be explained in more depth.\\n2. The addition of features like rescaling and momentum, although effective, may require further empirical justifications or theoretical underpinnings to generalize their benefits across diverse datasets.' Questions=\"1. How does the proposed method ensure that the mitigated oversaturation doesn't compromise the artistic styles inherent to specific diffusion models?\\n2. What are the potential limitations when applying the adaptive projected guidance method to domains outside of image generation, if any?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper provides a novel application of graph-based methods to the MultiFilterANN problem, showing significant theoretical developments in space-time trade-offs. The integration of penalized distances in graph indices is an innovative approach that shows promising empirical results, demonstrating improvements over existing methods.' Weaknesses=\"The connection between the theoretical framework and the empirical methods could be clarified further. Distinctions between theoretical analysis and the empirical setup aren't clearly delineated, which may lead to confusion about the applicability of theoretical results to empirical observations.\" Questions='How does the proposed method scale with the increasing number of filters in real-world datasets? Are there situations where the penalized distance fails to maintain efficiency or accuracy? Further, how generalizable are the empirical results beyond the presented datasets?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='The paper provides a novel application of graph-based methods to the MultiFilterANN problem, showing significant theoretical developments in space-time trade-offs. The integration of penalized distances in graph indices is an innovative approach that shows promising empirical results, demonstrating improvements over existing methods.' Weaknesses=\"The connection between the theoretical framework and the empirical methods could be clarified further. Distinctions between theoretical analysis and the empirical setup aren't clearly delineated, which may lead to confusion about the applicability of theoretical results to empirical observations.\" Questions='How does the proposed method scale with the increasing number of filters in real-world datasets? Are there situations where the penalized distance fails to maintain efficiency or accuracy? Further, how generalizable are the empirical results beyond the presented datasets?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The proposed method introduces a novel application of diffusion models and optimal transport for enhancing the accuracy of PINNs, particularly in handling high-frequency singularities in PDEs. This approach is innovative in using flow-matching techniques to effectively resample points in high residual regions, thus improving convergence and solution quality.\\n2. Experimental results demonstrate clear advantages of the proposed method over traditional techniques, showcasing its potential in challenging scenarios such as Poisson equations with peaks and linear elasticity problems with intricate material geometries.\\n3. The method provides a significant step towards addressing the limitations of traditional PINNs in high-dimensional spaces and complex PDEs through an elegant solution that fusese state-of-the-art techniques in PINNs and generative modeling.' Weaknesses='1. The paper would benefit from more extensive experimentation across a wider range of PDE problems to fully validate the generalizability and efficacy of the proposed method.\\n2. The method description, especially relating to the flow-matching dynamics and its integration into PINNs, lacks clarity in some sections, making it difficult for readers to reproduce or extend the work without additional details.\\n3. Certain sections of the paper could have been more concise, with a better focus on the core contributions and their implications within the context of PINNs and PDE solutions.' Questions='1. Is the proposed flow-matching sampling approach applicable to other types of differential equations or constraints beyond the singularities in PDE source functions discussed in this paper?\\n2. What are the computational costs associated with the additional sampling steps, and how do they compare to traditional PINN implementations, particularly in high-dimensional settings?\\n3. How robust is the method to varying levels of noise or inaccuracies in the initial conditions or boundary conditions of the PDEs being solved?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The proposed method introduces a novel application of diffusion models and optimal transport for enhancing the accuracy of PINNs, particularly in handling high-frequency singularities in PDEs. This approach is innovative in using flow-matching techniques to effectively resample points in high residual regions, thus improving convergence and solution quality.\\n2. Experimental results demonstrate clear advantages of the proposed method over traditional techniques, showcasing its potential in challenging scenarios such as Poisson equations with peaks and linear elasticity problems with intricate material geometries.\\n3. The method provides a significant step towards addressing the limitations of traditional PINNs in high-dimensional spaces and complex PDEs through an elegant solution that fusese state-of-the-art techniques in PINNs and generative modeling.' Weaknesses='1. The paper would benefit from more extensive experimentation across a wider range of PDE problems to fully validate the generalizability and efficacy of the proposed method.\\n2. The method description, especially relating to the flow-matching dynamics and its integration into PINNs, lacks clarity in some sections, making it difficult for readers to reproduce or extend the work without additional details.\\n3. Certain sections of the paper could have been more concise, with a better focus on the core contributions and their implications within the context of PINNs and PDE solutions.' Questions='1. Is the proposed flow-matching sampling approach applicable to other types of differential equations or constraints beyond the singularities in PDE source functions discussed in this paper?\\n2. What are the computational costs associated with the additional sampling steps, and how do they compare to traditional PINN implementations, particularly in high-dimensional settings?\\n3. How robust is the method to varying levels of noise or inaccuracies in the initial conditions or boundary conditions of the PDEs being solved?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper introduces a novel benchmark, MathHay, which specifically evaluates the long-context mathematical reasoning capabilities of LLMs, addressing a significant gap in existing benchmarks. The methodology of creating document 'haystacks' with mixed relevant and irrelevant content attempts to simulate real-world complexity and boosts the realism of evaluation scenarios.\" Weaknesses=\"While the MathHay benchmark provides a comprehensive evaluation setting, it relies heavily on constructed scenarios, which might not fully represent all aspects of real-world complexity or variability. Furthermore, the benchmark highlights inadequacies in existing models but does not offer substantial insights into specific strategies for improvement beyond 'more research is needed'.\" Questions='How does the MathHay benchmark ensure that the mathematical reasoning aspects it evaluates are domain-agnostic and not overly tailored to specific types of documents or topics? Are there plans to expand the benchmark to include more varied types of mathematical reasoning tasks?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=6 Confidence=3 Strengths=\"The paper introduces a novel benchmark, MathHay, which specifically evaluates the long-context mathematical reasoning capabilities of LLMs, addressing a significant gap in existing benchmarks. The methodology of creating document 'haystacks' with mixed relevant and irrelevant content attempts to simulate real-world complexity and boosts the realism of evaluation scenarios.\" Weaknesses=\"While the MathHay benchmark provides a comprehensive evaluation setting, it relies heavily on constructed scenarios, which might not fully represent all aspects of real-world complexity or variability. Furthermore, the benchmark highlights inadequacies in existing models but does not offer substantial insights into specific strategies for improvement beyond 'more research is needed'.\" Questions='How does the MathHay benchmark ensure that the mathematical reasoning aspects it evaluates are domain-agnostic and not overly tailored to specific types of documents or topics? Are there plans to expand the benchmark to include more varied types of mathematical reasoning tasks?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper presents a novel approach, Mycroft, for selecting informative data subsets from external data sources, which is an important problem in machine learning.\\n2. The method is tested across two domains, suggesting the approach could be widely applicable.\\n3. The framework addresses both feature space similarity and gradient matching, providing a comprehensive technique for data subset evaluation.' Weaknesses='1. The description of RetrieveTopK function and its implementation details are unclear and may confuse readers trying to reproduce results or understand the algorithm fully.\\n2. The theoretical backing and proofs provided for the efficacy of the method could be expanded for a better understanding of the mathematical foundation.\\n3. Lack of discussion on privacy techniques limits understanding of how privacy concerns are mitigated, which is crucial given the context of data sharing.\\n4. There is limited discussion on the computational overhead introduced by the proposed method compared to baseline methods.\\n5. Results are mostly given for limited datasets, making it harder to assess generalizability or applicability in more diverse or larger-scale scenarios.' Questions='1. Could the authors elaborate on how RetrieveTopK function aligns with the coverage goals for useful data subsets?\\n2. How does Mycroft ensure privacy of data during the subset selection process? Are there privacy-preserving experiments conducted?\\n3. What considerations are made regarding the computational resources or time for Mycroft compared to standard random sampling?\\n4. In practice, how scalable is Mycroft when dealing with extensive and large-scale datasets? Are there performance benchmarks available?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper presents a novel approach, Mycroft, for selecting informative data subsets from external data sources, which is an important problem in machine learning.\\n2. The method is tested across two domains, suggesting the approach could be widely applicable.\\n3. The framework addresses both feature space similarity and gradient matching, providing a comprehensive technique for data subset evaluation.' Weaknesses='1. The description of RetrieveTopK function and its implementation details are unclear and may confuse readers trying to reproduce results or understand the algorithm fully.\\n2. The theoretical backing and proofs provided for the efficacy of the method could be expanded for a better understanding of the mathematical foundation.\\n3. Lack of discussion on privacy techniques limits understanding of how privacy concerns are mitigated, which is crucial given the context of data sharing.\\n4. There is limited discussion on the computational overhead introduced by the proposed method compared to baseline methods.\\n5. Results are mostly given for limited datasets, making it harder to assess generalizability or applicability in more diverse or larger-scale scenarios.' Questions='1. Could the authors elaborate on how RetrieveTopK function aligns with the coverage goals for useful data subsets?\\n2. How does Mycroft ensure privacy of data during the subset selection process? Are there privacy-preserving experiments conducted?\\n3. What considerations are made regarding the computational resources or time for Mycroft compared to standard random sampling?\\n4. In practice, how scalable is Mycroft when dealing with extensive and large-scale datasets? Are there performance benchmarks available?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel approach to integrate domain indexing in cross-domain recommendations, enhancing both performance and interpretability. The adversarial Bayesian framework facilitates better domain index inference, offering clear visualization of domain relationships. The empirical results on synthetic and real-world datasets demonstrate significant improvements over existing methods, highlighting the practical utility of the proposed framework.' Weaknesses='While the paper effectively demonstrates the advantage of domain indexing, the explanation of the adversarial Bayesian framework and its implementation details could be clearer. Additionally, while the results are promising, further experiments on larger, more diverse real-world datasets could strengthen the validation of the method. The dependency on synthetic data may not fully capture the complexity of real-world cross-domain challenges.' Questions='How sensitive is the method to the choice of the adversarial component in the Bayesian framework? Can the domain indexing framework be generalized to other types of data beyond the recommendation setting? How does the approach handle domains with insufficient or highly imbalanced data?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths='The paper presents a novel approach to integrate domain indexing in cross-domain recommendations, enhancing both performance and interpretability. The adversarial Bayesian framework facilitates better domain index inference, offering clear visualization of domain relationships. The empirical results on synthetic and real-world datasets demonstrate significant improvements over existing methods, highlighting the practical utility of the proposed framework.' Weaknesses='While the paper effectively demonstrates the advantage of domain indexing, the explanation of the adversarial Bayesian framework and its implementation details could be clearer. Additionally, while the results are promising, further experiments on larger, more diverse real-world datasets could strengthen the validation of the method. The dependency on synthetic data may not fully capture the complexity of real-world cross-domain challenges.' Questions='How sensitive is the method to the choice of the adversarial component in the Bayesian framework? Can the domain indexing framework be generalized to other types of data beyond the recommendation setting? How does the approach handle domains with insufficient or highly imbalanced data?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The method SteerPrompt effectively combines iterative prompting with attention steering, presenting a creative and resource-efficient way to improve LLMs' comprehension for open-book QA tasks. The technique's implementation at inference time without requiring parameter changes is particularly notable for enhancing existing models' abilities without retraining.\" Weaknesses=\"The method relies heavily on the model's own predictions, which may lead to reinforcing existing biases or errors, especially when selecting key sentences. The validation across varied datasets and tasks beyond open-book QA is limited, which might impact the generalization claims.\" Questions='1. How does the effectiveness of SteerPrompt vary with different types of LLM architectures beyond Vicuna-7B and LLAMA-3?\\n2. What are the implications of this method on the interpretability of the model, especially with altered attention?\\n3. Can SteerPrompt be effectively adapted for other NLP tasks requiring deep contextual understanding outside of QA scenarios?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The method SteerPrompt effectively combines iterative prompting with attention steering, presenting a creative and resource-efficient way to improve LLMs' comprehension for open-book QA tasks. The technique's implementation at inference time without requiring parameter changes is particularly notable for enhancing existing models' abilities without retraining.\" Weaknesses=\"The method relies heavily on the model's own predictions, which may lead to reinforcing existing biases or errors, especially when selecting key sentences. The validation across varied datasets and tasks beyond open-book QA is limited, which might impact the generalization claims.\" Questions='1. How does the effectiveness of SteerPrompt vary with different types of LLM architectures beyond Vicuna-7B and LLAMA-3?\\n2. What are the implications of this method on the interpretability of the model, especially with altered attention?\\n3. Can SteerPrompt be effectively adapted for other NLP tasks requiring deep contextual understanding outside of QA scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=3 Strengths='1. The paper presents a novel application of RWKV from NLP to vision tasks, maintaining state-of-the-art performance while reducing computational complexity.\\n2. Extensive experimental validation is performed on various benchmark datasets, demonstrating the effectiveness of VRWKV.\\n3. The proposed Q-Shift and Bi-WKV mechanisms are innovative solutions to address the high computational demands of ViTs.' Weaknesses=\"1. The contributions over existing techniques such as RWKV and ViT may not appear groundbreaking, mostly involving adaptation rather than entirely new methodologies.\\n2. Detailed analysis on how different hyperparameters in VRWKV affect performance is lacking, which would aid understanding of the model's scalability and robustness.\" Questions=\"1. How does VRWKV's computational efficiency compare with other linear attention models in vision tasks?\\n2. Are there specific types of images or tasks where VRWKV may not perform as well compared to traditional ViT?\\n3. What are the possible limitations of Q-Shift and Bi-WKV in dataset diversity other than those tested?\"\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=3 Strengths='1. The paper presents a novel application of RWKV from NLP to vision tasks, maintaining state-of-the-art performance while reducing computational complexity.\\n2. Extensive experimental validation is performed on various benchmark datasets, demonstrating the effectiveness of VRWKV.\\n3. The proposed Q-Shift and Bi-WKV mechanisms are innovative solutions to address the high computational demands of ViTs.' Weaknesses=\"1. The contributions over existing techniques such as RWKV and ViT may not appear groundbreaking, mostly involving adaptation rather than entirely new methodologies.\\n2. Detailed analysis on how different hyperparameters in VRWKV affect performance is lacking, which would aid understanding of the model's scalability and robustness.\" Questions=\"1. How does VRWKV's computational efficiency compare with other linear attention models in vision tasks?\\n2. Are there specific types of images or tasks where VRWKV may not perform as well compared to traditional ViT?\\n3. What are the possible limitations of Q-Shift and Bi-WKV in dataset diversity other than those tested?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper presents a robust theoretical analysis combined with empirical evidence to propose a novel optimizer, SignSoftSGD, which addresses the known shortcomings of Adam, especially in terms of loss spikes and convergence speed. The integration of Nesterov's accelerated gradient further enhances the optimizer’s efficiency.\" Weaknesses='While the theoretical and empirical analysis is comprehensive, the novelty of the contribution could be further strengthened by clearer comparisons with relevant recent optimizers beyond Adam and AdamW.' Questions='How does SignSoftSGD perform in comparison to other recent optimizer variants developed as modifications to Adam intended for specific large-scale vision and language tasks? Are there scenarios where SignSoftSGD might underperform compared to Adam, especially in non-standard training setups?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper presents a robust theoretical analysis combined with empirical evidence to propose a novel optimizer, SignSoftSGD, which addresses the known shortcomings of Adam, especially in terms of loss spikes and convergence speed. The integration of Nesterov's accelerated gradient further enhances the optimizer’s efficiency.\" Weaknesses='While the theoretical and empirical analysis is comprehensive, the novelty of the contribution could be further strengthened by clearer comparisons with relevant recent optimizers beyond Adam and AdamW.' Questions='How does SignSoftSGD perform in comparison to other recent optimizer variants developed as modifications to Adam intended for specific large-scale vision and language tasks? Are there scenarios where SignSoftSGD might underperform compared to Adam, especially in non-standard training setups?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed framework, TRIDENT, introduces improvements in CZSL by effectively addressing previous limitations such as background influence in visual feature extraction and enhances attribute-object disentanglement and generalization using advanced word embeddings. The use of MLLM embeddings and attribute smoothing shows strong empirical performance improvements on multiple challenging datasets.' Weaknesses='The framework relies heavily on existing components, such as MLLM embeddings from LLaVA and feature extraction tools, which limits the novelty. While the results are impressive, the paper could benefit from more detailed analysis on the computational overhead introduced by these additions. The integration of the components in a cohesive manner could be more explicitly articulated.' Questions='How does TRIDENT compare in terms of computational efficiency with previous approaches? Are the improvements in accuracy partly due to increased computation? Could further detail on scalability and limitations in processing speed be provided?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The proposed framework, TRIDENT, introduces improvements in CZSL by effectively addressing previous limitations such as background influence in visual feature extraction and enhances attribute-object disentanglement and generalization using advanced word embeddings. The use of MLLM embeddings and attribute smoothing shows strong empirical performance improvements on multiple challenging datasets.' Weaknesses='The framework relies heavily on existing components, such as MLLM embeddings from LLaVA and feature extraction tools, which limits the novelty. While the results are impressive, the paper could benefit from more detailed analysis on the computational overhead introduced by these additions. The integration of the components in a cohesive manner could be more explicitly articulated.' Questions='How does TRIDENT compare in terms of computational efficiency with previous approaches? Are the improvements in accuracy partly due to increased computation? Could further detail on scalability and limitations in processing speed be provided?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper proposes a novel application of state-space models to time-varying counterfactual predictions, addressing confounding biases effectively. The Mamba-CDSP model represents a significant contribution by combining SSMs with a method for de-correlation of covariates, which enhances prediction accuracy and efficiency. The evaluation on diverse datasets, including synthetic and real-world scenarios, demonstrates the robustness and applicability of the approach.' Weaknesses='The paper could provide more detailed theoretical justification for the use of state-space models specifically in the context of TCP, as well as further insights into the limitations or potential areas for improvement of the proposed method. The reliance on previously established frameworks (like SSMs) might raise questions about the novelty of the solution in terms of methodological innovation.' Questions='How does the Mamba-CDSP model specifically mitigate confounding biases compared to previous models? Can the model be adapted to other domains outside of time-varying counterfactual prediction? Are there specific scenarios where this model might fail to perform effectively?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=4 Rating=7 Confidence=4 Strengths='The paper proposes a novel application of state-space models to time-varying counterfactual predictions, addressing confounding biases effectively. The Mamba-CDSP model represents a significant contribution by combining SSMs with a method for de-correlation of covariates, which enhances prediction accuracy and efficiency. The evaluation on diverse datasets, including synthetic and real-world scenarios, demonstrates the robustness and applicability of the approach.' Weaknesses='The paper could provide more detailed theoretical justification for the use of state-space models specifically in the context of TCP, as well as further insights into the limitations or potential areas for improvement of the proposed method. The reliance on previously established frameworks (like SSMs) might raise questions about the novelty of the solution in terms of methodological innovation.' Questions='How does the Mamba-CDSP model specifically mitigate confounding biases compared to previous models? Can the model be adapted to other domains outside of time-varying counterfactual prediction? Are there specific scenarios where this model might fail to perform effectively?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The paper introduces a critically important concept of 'weak-to-strong deception' in the context of aligning advanced AI models, which is crucial for ensuring the future safety and trustworthiness of AI systems.\\n2. It effectively addresses a significant gap in the literature concerning the potential misalignments that can occur when powerful models interact with weaker supervision tools.\\n3. The utilization of a variety of models (GPT-2, OPT, Mistral-7B) helps strengthen the findings, showing the relevance of the study across different architectures.\\n4. The experiments are well-designed to simulate different alignment scenarios, providing a solid basis for the conclusions drawn.\\n5. The results underscore an emerging issue in the alignment of large language models, suggesting pathways for further research into creating more robust supervision frameworks.\" Weaknesses='1. While the paper presents a novel concept, it mainly identifies the problem without offering concrete solutions, highlighting the need for more substantial mitigation strategies.\\n2. The experiments, despite being varied, might not fully capture the complexities of real-world applications where such deceptive behaviors could emerge.\\n3. The methodology, particularly around the experimental setup for detecting deception, could benefit from further details for reproducibility.\\n4. The analysis might require more in-depth exploration of why bootstrapping remains only partially effective and what this implies for future methods.' Questions=\"1. Can the authors elaborate on potential techniques beyond bootstrapping that might effectively address the 'weak-to-strong deception' problem?\\n2. How do the authors envision applying the findings in real-world scenarios, especially where supervisory models might not have perfect visibility of all model activities?\\n3. Given the complexities of alignment issues highlighted, what are the next steps the authors recommend for developing more resilient training protocols that minimize deceptive opportunities? \"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"1. The paper introduces a critically important concept of 'weak-to-strong deception' in the context of aligning advanced AI models, which is crucial for ensuring the future safety and trustworthiness of AI systems.\\n2. It effectively addresses a significant gap in the literature concerning the potential misalignments that can occur when powerful models interact with weaker supervision tools.\\n3. The utilization of a variety of models (GPT-2, OPT, Mistral-7B) helps strengthen the findings, showing the relevance of the study across different architectures.\\n4. The experiments are well-designed to simulate different alignment scenarios, providing a solid basis for the conclusions drawn.\\n5. The results underscore an emerging issue in the alignment of large language models, suggesting pathways for further research into creating more robust supervision frameworks.\" Weaknesses='1. While the paper presents a novel concept, it mainly identifies the problem without offering concrete solutions, highlighting the need for more substantial mitigation strategies.\\n2. The experiments, despite being varied, might not fully capture the complexities of real-world applications where such deceptive behaviors could emerge.\\n3. The methodology, particularly around the experimental setup for detecting deception, could benefit from further details for reproducibility.\\n4. The analysis might require more in-depth exploration of why bootstrapping remains only partially effective and what this implies for future methods.' Questions=\"1. Can the authors elaborate on potential techniques beyond bootstrapping that might effectively address the 'weak-to-strong deception' problem?\\n2. How do the authors envision applying the findings in real-world scenarios, especially where supervisory models might not have perfect visibility of all model activities?\\n3. Given the complexities of alignment issues highlighted, what are the next steps the authors recommend for developing more resilient training protocols that minimize deceptive opportunities? \"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper introduces an innovative approach, Deep Compression Autoencoder (DC-AE), which effectively improves high-compression autoencoders for image synthesis, demonstrating practical and theoretical advancements. 2. The incorporation of Residual Autoencoding and Decoupled High-Resolution Adaptation addresses identified issues with high compression ratios effectively. 3. Experimental results show significant improvements in both computational efficiency and image generation quality, substantiating the claims.' Weaknesses='1. While the proposed methods are sound, the presentation could be improved for clarity regarding the technical details and underlying mechanisms of the proposed model. 2. The complexity of the introduced methodologies, especially their implementation and integration with existing frameworks, may hinder broader accessibility and understanding. 3. Some claims, particularly around the efficiency gains and implementation, would benefit from additional comparative analysis against more varied baseline models.' Questions='1. How does the performance of DC-AE compare with traditional autoencoders at lower compression ratios, and how do these differences manifest in practical scenarios? 2. Are there specific limitations or edge cases where the DC-AE model does not perform as expected, especially regarding variances in dataset characteristics or image types? 3. Could you provide more insights into the potential scalability of the proposed model for real-time applications or its adaptability to other image-based tasks?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper introduces an innovative approach, Deep Compression Autoencoder (DC-AE), which effectively improves high-compression autoencoders for image synthesis, demonstrating practical and theoretical advancements. 2. The incorporation of Residual Autoencoding and Decoupled High-Resolution Adaptation addresses identified issues with high compression ratios effectively. 3. Experimental results show significant improvements in both computational efficiency and image generation quality, substantiating the claims.' Weaknesses='1. While the proposed methods are sound, the presentation could be improved for clarity regarding the technical details and underlying mechanisms of the proposed model. 2. The complexity of the introduced methodologies, especially their implementation and integration with existing frameworks, may hinder broader accessibility and understanding. 3. Some claims, particularly around the efficiency gains and implementation, would benefit from additional comparative analysis against more varied baseline models.' Questions='1. How does the performance of DC-AE compare with traditional autoencoders at lower compression ratios, and how do these differences manifest in practical scenarios? 2. Are there specific limitations or edge cases where the DC-AE model does not perform as expected, especially regarding variances in dataset characteristics or image types? 3. Could you provide more insights into the potential scalability of the proposed model for real-time applications or its adaptability to other image-based tasks?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. Provides a novel framework (H-PSRO) for heterogeneous team games, addressing a significant gap in existing research.\\n2. Successfully reduces exploitability and demonstrates superior performance in benchmarks like StarCraft and Google Research Football.\\n3. Maintains computational efficiency while expanding policy search space, which is critical for scalability.' Weaknesses=\"1. The reliance on sequential optimization could limit scalability in extremely large-scale games, potentially hindering its applicability in more complex environments.\\n2. The adaptation to continuous action spaces is yet to be explored, which may limit the framework's applicability in certain settings.\\n3. While experiments demonstrate the efficacy of the approach, more detailed comparisons with alternative methods in a similar complexity class could reinforce the paper's claims.\" Questions='1. How does the framework handle dynamic changes in team composition during the game?\\n2. What are the potential methods for adapting the approach to continuous action spaces?\\n3. Can the reliance on sequential optimization be mitigated, and if so, how?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='1. Provides a novel framework (H-PSRO) for heterogeneous team games, addressing a significant gap in existing research.\\n2. Successfully reduces exploitability and demonstrates superior performance in benchmarks like StarCraft and Google Research Football.\\n3. Maintains computational efficiency while expanding policy search space, which is critical for scalability.' Weaknesses=\"1. The reliance on sequential optimization could limit scalability in extremely large-scale games, potentially hindering its applicability in more complex environments.\\n2. The adaptation to continuous action spaces is yet to be explored, which may limit the framework's applicability in certain settings.\\n3. While experiments demonstrate the efficacy of the approach, more detailed comparisons with alternative methods in a similar complexity class could reinforce the paper's claims.\" Questions='1. How does the framework handle dynamic changes in team composition during the game?\\n2. What are the potential methods for adapting the approach to continuous action spaces?\\n3. Can the reliance on sequential optimization be mitigated, and if so, how?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"1. SaMoye effectively addresses the challenge of zero-shot singing voice conversion by employing improved feature disentanglement techniques, which minimizes timbre leakage and enhances quality.\\n2. The large-scale dataset used for training (1,815 hours from over 6,000 speakers) supports robust model generalization and contributes to high-quality voice conversion.\\n3. SaMoye's ability to convert singing into non-human timbres is a notable contribution, showcasing its versatility beyond traditional human voice tasks.\" Weaknesses=\"1. The dependence on a large dataset poses limitations for practitioners with limited data access or computational resources, potentially hindering its practical deployment.\\n2. Specific details on how feature disentanglement was quantitatively validated in experiments are lacking, which may cast doubt on the method's reliability.\\n3. While the model introduces novel elements, the fundamental approach and techniques might not be highly innovative, relying on existing concepts in slight variations.\" Questions=\"1. How sensitive is SaMoye to variations in the size or composition of the training dataset? Can it generalize well with less data?\\n2. What specific quantitative metrics aside from subjective evaluation were used to assess the reduction in timbre leakage?\\n3. Could the model's approach to feature disentanglement be applied to other domains beyond voice conversion, such as general voice synthesis or even text-to-speech tasks?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths=\"1. SaMoye effectively addresses the challenge of zero-shot singing voice conversion by employing improved feature disentanglement techniques, which minimizes timbre leakage and enhances quality.\\n2. The large-scale dataset used for training (1,815 hours from over 6,000 speakers) supports robust model generalization and contributes to high-quality voice conversion.\\n3. SaMoye's ability to convert singing into non-human timbres is a notable contribution, showcasing its versatility beyond traditional human voice tasks.\" Weaknesses=\"1. The dependence on a large dataset poses limitations for practitioners with limited data access or computational resources, potentially hindering its practical deployment.\\n2. Specific details on how feature disentanglement was quantitatively validated in experiments are lacking, which may cast doubt on the method's reliability.\\n3. While the model introduces novel elements, the fundamental approach and techniques might not be highly innovative, relying on existing concepts in slight variations.\" Questions=\"1. How sensitive is SaMoye to variations in the size or composition of the training dataset? Can it generalize well with less data?\\n2. What specific quantitative metrics aside from subjective evaluation were used to assess the reduction in timbre leakage?\\n3. Could the model's approach to feature disentanglement be applied to other domains beyond voice conversion, such as general voice synthesis or even text-to-speech tasks?\"\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper introduces a novel and efficient method of incorporating historical spatial priors into 3D perception systems for autonomous vehicles.\\n2. It demonstrates substantial improvements in 3D object detection and semantic map segmentation by leveraging context from prior traversals.\\n3. The method is scalable and offers a practical approach that can be easily integrated into existing architectures without significant computational overhead.\\n4. Well-structured paper with clear explanations and comprehensive experiments conducted on the widely-used nuScenes dataset, showcasing the effectiveness of the proposed method.' Weaknesses=\"1. Although the proposed method shows improvements, reliance on historical data may introduce challenges in dynamically changing environments where previous data might become obsolete.\\n2. The paper does not explicitly demonstrate the potential limitations of DMP when dealing with extremely large-scale data or entirely new locations without prior traversals.\\n3. Some baseline comparisons might lack fair adaptation or optimization within the context of this study's framework, potentially affecting performance evaluations.\" Questions='1. How does the DMP framework handle rapidly changing environments where historical data may not accurately represent current road conditions?\\n2. Are there specific scenarios where the performance of DMP might degrade due to outdated or irrelevant historical data?\\n3. Could the method be extended to handle entirely novel areas where no historical data is available, and if so, how might it perform under such conditions?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1. The paper introduces a novel and efficient method of incorporating historical spatial priors into 3D perception systems for autonomous vehicles.\\n2. It demonstrates substantial improvements in 3D object detection and semantic map segmentation by leveraging context from prior traversals.\\n3. The method is scalable and offers a practical approach that can be easily integrated into existing architectures without significant computational overhead.\\n4. Well-structured paper with clear explanations and comprehensive experiments conducted on the widely-used nuScenes dataset, showcasing the effectiveness of the proposed method.' Weaknesses=\"1. Although the proposed method shows improvements, reliance on historical data may introduce challenges in dynamically changing environments where previous data might become obsolete.\\n2. The paper does not explicitly demonstrate the potential limitations of DMP when dealing with extremely large-scale data or entirely new locations without prior traversals.\\n3. Some baseline comparisons might lack fair adaptation or optimization within the context of this study's framework, potentially affecting performance evaluations.\" Questions='1. How does the DMP framework handle rapidly changing environments where historical data may not accurately represent current road conditions?\\n2. Are there specific scenarios where the performance of DMP might degrade due to outdated or irrelevant historical data?\\n3. Could the method be extended to handle entirely novel areas where no historical data is available, and if so, how might it perform under such conditions?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The introduction of the RGA-IB method for enhancing the robustness of GNNs using the Information Bottleneck principle is a significant contribution to the field, connecting two important areas in a novel way. \\n2. The methodology is well-founded, utilizing a multi-layer GNN architecture to simulate gradient descent for reducing IB loss, which is theoretically sound and practically effective.\\n3. The evaluation includes a variety of datasets and adversarial scenarios, demonstrating the utility and effectiveness of the approach in enhancing node classification accuracy under attack.' Weaknesses='1. The theoretical explanations behind why reducing IB loss improves adversarial robustness, while insightful, could be further detailed to strengthen the argument.\\n2. While the paper is generally well-presented, some sections could benefit from clearer explanations, especially for readers less familiar with the Information Bottleneck framework or graph attention mechanisms.\\n3. The experiments, although comprehensive, could have included a more diverse set of datasets or adversarial strategies to further validate the robustness claims.' Questions='1. How does the choice of datasets and specific adversarial scenarios affect the generalizability of RGA-IB to other types of graph-structured data?\\n2. Can the authors provide more empirical evidence or theoretical support for the choice of IB as a robustness measure compared to other possible metrics?\\n3. Is there potential for extending RGA-IB to applications beyond node classification, such as link prediction or graph-level tasks?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The introduction of the RGA-IB method for enhancing the robustness of GNNs using the Information Bottleneck principle is a significant contribution to the field, connecting two important areas in a novel way. \\n2. The methodology is well-founded, utilizing a multi-layer GNN architecture to simulate gradient descent for reducing IB loss, which is theoretically sound and practically effective.\\n3. The evaluation includes a variety of datasets and adversarial scenarios, demonstrating the utility and effectiveness of the approach in enhancing node classification accuracy under attack.' Weaknesses='1. The theoretical explanations behind why reducing IB loss improves adversarial robustness, while insightful, could be further detailed to strengthen the argument.\\n2. While the paper is generally well-presented, some sections could benefit from clearer explanations, especially for readers less familiar with the Information Bottleneck framework or graph attention mechanisms.\\n3. The experiments, although comprehensive, could have included a more diverse set of datasets or adversarial strategies to further validate the robustness claims.' Questions='1. How does the choice of datasets and specific adversarial scenarios affect the generalizability of RGA-IB to other types of graph-structured data?\\n2. Can the authors provide more empirical evidence or theoretical support for the choice of IB as a robustness measure compared to other possible metrics?\\n3. Is there potential for extending RGA-IB to applications beyond node classification, such as link prediction or graph-level tasks?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=8 Confidence=4 Strengths='ProVaccine introduces a novel combination of sequence and structure embeddings with dual attention mechanisms tailored for vaccine candidate selection.\\nThe use of a large and relevant dataset enhances the potential applicability and reliability of the model in real-world scenarios.' Weaknesses='The methodology lacks detailed ablative studies to dissect the specific impact of each component methodologically.\\nThe complex integration of multiple features might reduce interpretability, potentially making the model hard to deploy without further simplification.\\nMissing details and minor errors in documentation may confuse readers and challenge reproducibility of the results.' Questions='Does the ProVaccine model provide any interpretability or insight into which features or sequence motifs contribute most significantly to immunogenic predictions?\\nHow does ProVaccine manage computational complexity, especially when dealing with very large antigen sequences or structures?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=8 Confidence=4 Strengths='ProVaccine introduces a novel combination of sequence and structure embeddings with dual attention mechanisms tailored for vaccine candidate selection.\\nThe use of a large and relevant dataset enhances the potential applicability and reliability of the model in real-world scenarios.' Weaknesses='The methodology lacks detailed ablative studies to dissect the specific impact of each component methodologically.\\nThe complex integration of multiple features might reduce interpretability, potentially making the model hard to deploy without further simplification.\\nMissing details and minor errors in documentation may confuse readers and challenge reproducibility of the results.' Questions='Does the ProVaccine model provide any interpretability or insight into which features or sequence motifs contribute most significantly to immunogenic predictions?\\nHow does ProVaccine manage computational complexity, especially when dealing with very large antigen sequences or structures?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper addresses a crucial aspect of multi-view classification where traditional models assume equal importance and alignment across views, which isn't applicable in many real-world scenarios. By proposing a computational trust-based discounting method, the authors offer a novel way to handle view-specific conflicts, enhancing prediction reliability. The use of subjective logic and Binomial opinion theory adds depth to the theoretical foundation, and experiments conducted on six diverse datasets demonstrate the practical utility and robustness of the approach. The inclusion of new metrics such as Multi-View Agreement with Ground Truth helps in better evaluation of performance.\" Weaknesses='While the paper introduces an interesting approach, one potential weakness is the reliance on the assumption that the proposed trust-based method consistently outperforms existing methods across all scenarios, which may not hold true in complex multi-view settings with high dimensionality or sparse data. The computational cost of evaluating instance-wise trust levels for each view across large datasets might present scalability issues. Additionally, the paper could benefit from a more comprehensive comparison with a broader range of state-of-the-art MVC methodologies.' Questions='How does the performance of the proposed method vary with the number of views and instances, particularly in datasets with high dimensionality or sparsity? Are there specific scenarios where the method might struggle or where traditional MVC approaches might be more suitable?\\n\\nCan the proposed method be adapted or extended to handle other forms of data integration challenges, such as missing views or partially observed data? How well would it perform under conditions of incomplete data?\\n\\nThe AUROC metric is mentioned as a measure of reliability in indicating prediction correctness. Could you elaborate on how this metric is used specifically within the context of evaluating the proposed method?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=3 Rating=7 Confidence=4 Strengths=\"The paper addresses a crucial aspect of multi-view classification where traditional models assume equal importance and alignment across views, which isn't applicable in many real-world scenarios. By proposing a computational trust-based discounting method, the authors offer a novel way to handle view-specific conflicts, enhancing prediction reliability. The use of subjective logic and Binomial opinion theory adds depth to the theoretical foundation, and experiments conducted on six diverse datasets demonstrate the practical utility and robustness of the approach. The inclusion of new metrics such as Multi-View Agreement with Ground Truth helps in better evaluation of performance.\" Weaknesses='While the paper introduces an interesting approach, one potential weakness is the reliance on the assumption that the proposed trust-based method consistently outperforms existing methods across all scenarios, which may not hold true in complex multi-view settings with high dimensionality or sparse data. The computational cost of evaluating instance-wise trust levels for each view across large datasets might present scalability issues. Additionally, the paper could benefit from a more comprehensive comparison with a broader range of state-of-the-art MVC methodologies.' Questions='How does the performance of the proposed method vary with the number of views and instances, particularly in datasets with high dimensionality or sparsity? Are there specific scenarios where the method might struggle or where traditional MVC approaches might be more suitable?\\n\\nCan the proposed method be adapted or extended to handle other forms of data integration challenges, such as missing views or partially observed data? How well would it perform under conditions of incomplete data?\\n\\nThe AUROC metric is mentioned as a measure of reliability in indicating prediction correctness. Could you elaborate on how this metric is used specifically within the context of evaluating the proposed method?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"The paper introduces a novel metric, the Wasserstein distance, to quantify neuronal entanglement in neural networks, which can provide a new perspective in understanding model behavior under sparsity. It also proposes a 'Sparse Expansion' experimental framework that helps maintain model performance by effectively disentangling neuron outputs.\" Weaknesses='The practical applicability and scalability of the Sparse Expansion method are not thoroughly explored or demonstrated. The reliance on the mixture of experts approach may come with its own computational overheads, which are not addressed in the conclusion. Additionally, the impact of applying these methods in different neural network architectures beyond LLMs would benefit from further exploration.' Questions='1. How does the Sparse Expansion framework balance between computational complexity and performance gains, particularly in large-scale deployments?\\n2. Can the proposed Wasserstein metric for entanglement be applied to other types of neural networks beyond LLMs effectively?\\n3. What is the impact of using a different distribution other than Gaussian for defining the Wasserstein Neurons?\\n4. How does the methodology fare in terms of computational overhead when integrated into existing neural networks during training?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"The paper introduces a novel metric, the Wasserstein distance, to quantify neuronal entanglement in neural networks, which can provide a new perspective in understanding model behavior under sparsity. It also proposes a 'Sparse Expansion' experimental framework that helps maintain model performance by effectively disentangling neuron outputs.\" Weaknesses='The practical applicability and scalability of the Sparse Expansion method are not thoroughly explored or demonstrated. The reliance on the mixture of experts approach may come with its own computational overheads, which are not addressed in the conclusion. Additionally, the impact of applying these methods in different neural network architectures beyond LLMs would benefit from further exploration.' Questions='1. How does the Sparse Expansion framework balance between computational complexity and performance gains, particularly in large-scale deployments?\\n2. Can the proposed Wasserstein metric for entanglement be applied to other types of neural networks beyond LLMs effectively?\\n3. What is the impact of using a different distribution other than Gaussian for defining the Wasserstein Neurons?\\n4. How does the methodology fare in terms of computational overhead when integrated into existing neural networks during training?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper introduces a novel use of quantum circuits to achieve highly parameter-efficient fine-tuning of large pre-trained models, which is a relevant and timely contribution to the field.\\n2. The method demonstrates a significant reduction in the number of trainable parameters compared to existing PEFT methods while maintaining competitive performance, which is a strong experimental validation.\\n3. The use of quantum-inspired techniques can inspire further research on interdisciplinary approaches for efficient model adaptation.' Weaknesses=\"1. The paper lacks detailed discussion and theoretical analysis on how the quantum-inspired parameterization impacts the learning dynamics and generalization of the models.\\n2. There is limited discussion on the practical implementation details and resource requirements of the quantum-inspired approach. Practical challenges of implementation in existing deep learning frameworks are not fully addressed.\\n3. Benchmarking is limited to a few datasets and comparisons primarily with LoRA, which may not provide a comprehensive understanding of the method’s effectiveness across diverse scenarios.\\n4. The notion of 'quantum unitary parameterization' relies on a complex theoretical foundation that might not be easily accessible to the broader audience of the paper, which can affect the clarity of presentation.\" Questions='1. How does the quantum-inspired parameterization handle different neural network architectures beyond those tested (e.g., non-transformer-based models)?\\n2. Can the method be extended to perform well on large-scale pre-trained models without significantly increasing computational overhead?\\n3. How sensitive is the performance of Quantum-PEFT to the choice of quantum circuit design and parameters used in the Pauli parameterization?\\n4. Are there specific tasks or model architectures where Quantum-PEFT may not be suitable or may underperform compared to traditional methods?\\n5. How does the method handle issues related to vanishing or exploding gradients during training, especially with the introduction of quantum-inspired modifications?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='1. The paper introduces a novel use of quantum circuits to achieve highly parameter-efficient fine-tuning of large pre-trained models, which is a relevant and timely contribution to the field.\\n2. The method demonstrates a significant reduction in the number of trainable parameters compared to existing PEFT methods while maintaining competitive performance, which is a strong experimental validation.\\n3. The use of quantum-inspired techniques can inspire further research on interdisciplinary approaches for efficient model adaptation.' Weaknesses=\"1. The paper lacks detailed discussion and theoretical analysis on how the quantum-inspired parameterization impacts the learning dynamics and generalization of the models.\\n2. There is limited discussion on the practical implementation details and resource requirements of the quantum-inspired approach. Practical challenges of implementation in existing deep learning frameworks are not fully addressed.\\n3. Benchmarking is limited to a few datasets and comparisons primarily with LoRA, which may not provide a comprehensive understanding of the method’s effectiveness across diverse scenarios.\\n4. The notion of 'quantum unitary parameterization' relies on a complex theoretical foundation that might not be easily accessible to the broader audience of the paper, which can affect the clarity of presentation.\" Questions='1. How does the quantum-inspired parameterization handle different neural network architectures beyond those tested (e.g., non-transformer-based models)?\\n2. Can the method be extended to perform well on large-scale pre-trained models without significantly increasing computational overhead?\\n3. How sensitive is the performance of Quantum-PEFT to the choice of quantum circuit design and parameters used in the Pauli parameterization?\\n4. Are there specific tasks or model architectures where Quantum-PEFT may not be suitable or may underperform compared to traditional methods?\\n5. How does the method handle issues related to vanishing or exploding gradients during training, especially with the introduction of quantum-inspired modifications?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The BILBO algorithm effectively addresses sample inefficiency in bilevel optimization and offers theoretical guarantees on convergence. It introduces a novel approach to simultaneously optimize upper- and lower-level problems, significantly improving convergence speed and reducing regret in synthetic and real-world scenarios.' Weaknesses='While BILBO is promising, the paper lacks extensive comparisons with recent state-of-the-art bilevel optimization methods. Moreover, there is limited discussion on the scalability of the proposed algorithm and its performance in high-dimensional spaces, which is crucial for practical applications.' Questions='Can BILBO be extended or adapted for handling multiobjective bilevel optimization problems? How does the algorithm handle large-scale or high-dimensional settings, and what are the computational bottlenecks in such scenarios?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='The BILBO algorithm effectively addresses sample inefficiency in bilevel optimization and offers theoretical guarantees on convergence. It introduces a novel approach to simultaneously optimize upper- and lower-level problems, significantly improving convergence speed and reducing regret in synthetic and real-world scenarios.' Weaknesses='While BILBO is promising, the paper lacks extensive comparisons with recent state-of-the-art bilevel optimization methods. Moreover, there is limited discussion on the scalability of the proposed algorithm and its performance in high-dimensional spaces, which is crucial for practical applications.' Questions='Can BILBO be extended or adapted for handling multiobjective bilevel optimization problems? How does the algorithm handle large-scale or high-dimensional settings, and what are the computational bottlenecks in such scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper proposes a novel multi-agent collaborative data selection mechanism to enhance data efficiency in LLM pretraining, which is original and innovative. The experimental results showcase substantial performance improvements, indicating the effectiveness of the proposed approach.' Weaknesses=\"The mechanism's benefits might be limited to specific setups or datasets, as the SlimPajama dataset is prominently featured in the experiments. The use of a console to integrate agent inputs could be more clearly explained, including the specifics of the influence functions and reward calculations.\" Questions='How does the agent console dynamically adjust the influence of each data selection strategy during training, and what criteria are used for these adjustments? Could the reliance on the SlimPajama dataset limit the generalizability of the results to other datasets in the LLM training domain?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper proposes a novel multi-agent collaborative data selection mechanism to enhance data efficiency in LLM pretraining, which is original and innovative. The experimental results showcase substantial performance improvements, indicating the effectiveness of the proposed approach.' Weaknesses=\"The mechanism's benefits might be limited to specific setups or datasets, as the SlimPajama dataset is prominently featured in the experiments. The use of a console to integrate agent inputs could be more clearly explained, including the specifics of the influence functions and reward calculations.\" Questions='How does the agent console dynamically adjust the influence of each data selection strategy during training, and what criteria are used for these adjustments? Could the reliance on the SlimPajama dataset limit the generalizability of the results to other datasets in the LLM training domain?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths='The paper effectively addresses a significant gap in the field of robotics by identifying data scaling laws that relate to generalization in imitation learning for robotic manipulation. The approach is novel and provides a comprehensive method for scaling training data to achieve better generalization, which is supported by thorough experimental evidence. The use of real-world data collection and the demonstration of potential for zero-shot deployment are particularly strong aspects. The paper is well-organized, with clear presentation of experiments and results.' Weaknesses=\"While the study is thorough and the methodology is sound, it is focused on a specific subset of robotic tasks (like 'Pour Water' and 'Mouse Arrangement'), which might limit the immediate applicability of the findings across a broader range of robotic manipulation tasks. There is also limited discussion on the computational costs and practical constraints associated with the data collection and implementation of the proposed scaling laws, which could be a potential concern for deploying these strategies at scale in practical applications.\" Questions='1. How does the approach generalize to more complex tasks beyond the ones studied? Are there specific types of tasks or environments where the scaling laws proposed might not apply or require modification?\\n2. What are the computational and resource implications of collecting such diverse datasets in real-world settings at scale, and how can these be managed efficiently in practical applications?\\n3. Can you provide more insight into how the results from this study might translate to policy generalization for different robotic platforms or environments that were not part of this study?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=9 Confidence=4 Strengths='The paper effectively addresses a significant gap in the field of robotics by identifying data scaling laws that relate to generalization in imitation learning for robotic manipulation. The approach is novel and provides a comprehensive method for scaling training data to achieve better generalization, which is supported by thorough experimental evidence. The use of real-world data collection and the demonstration of potential for zero-shot deployment are particularly strong aspects. The paper is well-organized, with clear presentation of experiments and results.' Weaknesses=\"While the study is thorough and the methodology is sound, it is focused on a specific subset of robotic tasks (like 'Pour Water' and 'Mouse Arrangement'), which might limit the immediate applicability of the findings across a broader range of robotic manipulation tasks. There is also limited discussion on the computational costs and practical constraints associated with the data collection and implementation of the proposed scaling laws, which could be a potential concern for deploying these strategies at scale in practical applications.\" Questions='1. How does the approach generalize to more complex tasks beyond the ones studied? Are there specific types of tasks or environments where the scaling laws proposed might not apply or require modification?\\n2. What are the computational and resource implications of collecting such diverse datasets in real-world settings at scale, and how can these be managed efficiently in practical applications?\\n3. Can you provide more insight into how the results from this study might translate to policy generalization for different robotic platforms or environments that were not part of this study?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"1. The concept of self-evolving AI agents inspired by the Gödel machine is innovative and could lead to significant advancements in AI adaptive capabilities.\\n2. The framework's use of LLMs for dynamic modification shows versatility in handling various tasks efficiently.\\n3. Experiments demonstrate the framework's adaptability and potential superiority over traditional systems in performance.\" Weaknesses=\"1. The feasibility of the Gödel Agent's implementation in real-world applications is not thoroughly explored, leaving questions about resource requirements and practical constraints.\\n2. Lack of detailed benchmarks against established agent systems and minimal explanation on how the self-modification process is controlled or limited could be a point of concern regarding system stability and predictability.\\n3. The methodology relies heavily on LLM capabilities, which could lead to limitations in situations where real-time response and low latency are critical.\" Questions='1. How does the Gödel Agent ensure stability and prevent undesirable self-modifications during runtime?\\n2. What are the specific benchmarks used for evaluating performance gains, and how do these compare with state-of-the-art agent systems in similar setups?\\n3. Can the self-evolving approach be integrated with existing agent designs, or does it require building a system from scratch?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths=\"1. The concept of self-evolving AI agents inspired by the Gödel machine is innovative and could lead to significant advancements in AI adaptive capabilities.\\n2. The framework's use of LLMs for dynamic modification shows versatility in handling various tasks efficiently.\\n3. Experiments demonstrate the framework's adaptability and potential superiority over traditional systems in performance.\" Weaknesses=\"1. The feasibility of the Gödel Agent's implementation in real-world applications is not thoroughly explored, leaving questions about resource requirements and practical constraints.\\n2. Lack of detailed benchmarks against established agent systems and minimal explanation on how the self-modification process is controlled or limited could be a point of concern regarding system stability and predictability.\\n3. The methodology relies heavily on LLM capabilities, which could lead to limitations in situations where real-time response and low latency are critical.\" Questions='1. How does the Gödel Agent ensure stability and prevent undesirable self-modifications during runtime?\\n2. What are the specific benchmarks used for evaluating performance gains, and how do these compare with state-of-the-art agent systems in similar setups?\\n3. Can the self-evolving approach be integrated with existing agent designs, or does it require building a system from scratch?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The introduction of U-MATH provides a diverse collection of university-level problems, filling a significant gap in evaluating LLMs' mathematical reasoning. The inclusion of visual elements in the benchmark is innovative and addresses a current shortfall in multimodal assessments. The creation of the µ-MATH dataset is a valuable contribution for assessing LLMs' ability to evaluate solutions, enhancing our understanding of LLM limitations.\" Weaknesses='The benchmark may still lack diversity in some mathematical subfields, especially given the focus on only six core subjects. The reliance on an LLM to judge solutions in the µ-MATH dataset may introduce biases, as current models have known limitations in nuanced evaluations. The paper could provide more detailed examples of the types of visual problems included in the benchmark.' Questions='How does the choice of core subjects in U-MATH reflect the challenges faced in real-world university-level mathematics? Are there plans to include other subject areas in future expansions of the benchmark? What measures are being taken to ensure the accuracy of the LLMs used for judging solutions, given their current limitations in logical reasoning?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths=\"The introduction of U-MATH provides a diverse collection of university-level problems, filling a significant gap in evaluating LLMs' mathematical reasoning. The inclusion of visual elements in the benchmark is innovative and addresses a current shortfall in multimodal assessments. The creation of the µ-MATH dataset is a valuable contribution for assessing LLMs' ability to evaluate solutions, enhancing our understanding of LLM limitations.\" Weaknesses='The benchmark may still lack diversity in some mathematical subfields, especially given the focus on only six core subjects. The reliance on an LLM to judge solutions in the µ-MATH dataset may introduce biases, as current models have known limitations in nuanced evaluations. The paper could provide more detailed examples of the types of visual problems included in the benchmark.' Questions='How does the choice of core subjects in U-MATH reflect the challenges faced in real-world university-level mathematics? Are there plans to include other subject areas in future expansions of the benchmark? What measures are being taken to ensure the accuracy of the LLMs used for judging solutions, given their current limitations in logical reasoning?'\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses an important issue of evaluating LLMs on mathematical reasoning by introducing a benchmark that mitigates the issue of data contamination and variability, which is a significant advancement in the field. The empirical analysis on 25 state-of-the-art LLMs provides a broad examination of the problem.' Weaknesses=\"While the paper provides new insights into LLM capabilities, it primarily highlights issues already acknowledged in existing literature, such as LLMs' reliance on pattern matching over genuine reasoning. The lack of proposed solutions to the identified issues may limit the immediate impact of this work. Additionally, the concepts of 'genuine logical reasoning' and 'pattern-matching' are not clearly defined, making it difficult to assess the novelty of the evaluation.\" Questions=\"1. How do the authors define 'genuine logical reasoning,' and how is it differentiated from the observed pattern-matching behavior?\\n2. What are the next steps in this line of research to address the fragility in reasoning identified by the study?\\n3. How can the benchmark be further developed to test other aspects of reasoning beyond mathematical reasoning?\"\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=4 Strengths='The paper addresses an important issue of evaluating LLMs on mathematical reasoning by introducing a benchmark that mitigates the issue of data contamination and variability, which is a significant advancement in the field. The empirical analysis on 25 state-of-the-art LLMs provides a broad examination of the problem.' Weaknesses=\"While the paper provides new insights into LLM capabilities, it primarily highlights issues already acknowledged in existing literature, such as LLMs' reliance on pattern matching over genuine reasoning. The lack of proposed solutions to the identified issues may limit the immediate impact of this work. Additionally, the concepts of 'genuine logical reasoning' and 'pattern-matching' are not clearly defined, making it difficult to assess the novelty of the evaluation.\" Questions=\"1. How do the authors define 'genuine logical reasoning,' and how is it differentiated from the observed pattern-matching behavior?\\n2. What are the next steps in this line of research to address the fragility in reasoning identified by the study?\\n3. How can the benchmark be further developed to test other aspects of reasoning beyond mathematical reasoning?\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces an innovative information-theoretic framework which effectively disentangles different sources of diversity in prompt-based generative models. Introducing conditional and mutual information measures separately is a strong methodological contribution.' Weaknesses=\"The presentation of the methodology lacks clarity in terms of equation formulation and interpretability. The practical applicability or potential drawbacks of such metrics aren't thoroughly examined. More concrete examples and case studies may enhance understanding and assess real-world applicability.\" Questions='How do the Conditional-Vendi and Information-Vendi scores compare in varied scenarios of prompt-based models? Are there any limitations of this approach in real-time generative applications? What are the computational demands for these methods compared to traditional diversity metrics?'\n",
      "Success: Soundness=3 Presentation=3 Contribution=3 Rating=6 Confidence=3 Strengths='The paper introduces an innovative information-theoretic framework which effectively disentangles different sources of diversity in prompt-based generative models. Introducing conditional and mutual information measures separately is a strong methodological contribution.' Weaknesses=\"The presentation of the methodology lacks clarity in terms of equation formulation and interpretability. The practical applicability or potential drawbacks of such metrics aren't thoroughly examined. More concrete examples and case studies may enhance understanding and assess real-world applicability.\" Questions='How do the Conditional-Vendi and Information-Vendi scores compare in varied scenarios of prompt-based models? Are there any limitations of this approach in real-time generative applications? What are the computational demands for these methods compared to traditional diversity metrics?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='FIG presents an innovative approach by using measurement interpolants, which enhances the robustness and flexibility of diffusion and flow matching models, handling high noise and ill-posed problems effectively. The theoretical foundations add depth and credibility to the proposed method.' Weaknesses='The paper may suffer from complex presentation, as the explanation of some aspects such as measurement interpolants could be expanded and simplified for better understanding. The paper could clarify more on potential limitations or specific drawbacks when applying FIG to broader types of inverse problems beyond the presented image scenarios.' Questions='How does FIG handle scenarios with different types of noise or measurement errors (e.g., non-Gaussian noise)? Could the method be generalized to non-image linear inverse problems, and what adjustments might that require?'\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=4 Strengths='FIG presents an innovative approach by using measurement interpolants, which enhances the robustness and flexibility of diffusion and flow matching models, handling high noise and ill-posed problems effectively. The theoretical foundations add depth and credibility to the proposed method.' Weaknesses='The paper may suffer from complex presentation, as the explanation of some aspects such as measurement interpolants could be expanded and simplified for better understanding. The paper could clarify more on potential limitations or specific drawbacks when applying FIG to broader types of inverse problems beyond the presented image scenarios.' Questions='How does FIG handle scenarios with different types of noise or measurement errors (e.g., non-Gaussian noise)? Could the method be generalized to non-image linear inverse problems, and what adjustments might that require?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper presents an innovative approach to 3D steganography using 3D Gaussian splatting, which is an emerging field with promising applications in copyright protection and secure communication. The authors effectively combine hybrid decoupled Gaussian encryption with advanced anchor point strategies to enhance security and efficiency. Extensive experimental results demonstrate the method's superiority in rendering fidelity, speed, and security, making it a valuable contribution to the field.\" Weaknesses='The proposed methods may introduce complexity in practical implementation, potentially requiring significant computational resources and specialized knowledge, which could limit widespread adoption. Additionally, while the paper demonstrates comparative performance improvements, the long-term robustness and adaptability of the approach to various real-world scenarios could have been explored more thoroughly.' Questions='How does the SecureGS framework perform in highly dynamic or real-time environments where computational efficiency is paramount? What specific strategies were employed to ensure the robustness of the encryption mechanism against potential attacks or tampering in practical deployment scenarios?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper presents an innovative approach to 3D steganography using 3D Gaussian splatting, which is an emerging field with promising applications in copyright protection and secure communication. The authors effectively combine hybrid decoupled Gaussian encryption with advanced anchor point strategies to enhance security and efficiency. Extensive experimental results demonstrate the method's superiority in rendering fidelity, speed, and security, making it a valuable contribution to the field.\" Weaknesses='The proposed methods may introduce complexity in practical implementation, potentially requiring significant computational resources and specialized knowledge, which could limit widespread adoption. Additionally, while the paper demonstrates comparative performance improvements, the long-term robustness and adaptability of the approach to various real-world scenarios could have been explored more thoroughly.' Questions='How does the SecureGS framework perform in highly dynamic or real-time environments where computational efficiency is paramount? What specific strategies were employed to ensure the robustness of the encryption mechanism against potential attacks or tampering in practical deployment scenarios?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=3 Strengths='The hierarchical patch graph network (Hi-Patch) offers a novel multi-scale modeling approach that addresses the complexities of irregular multivariate time series by capturing both fine-grained and coarse-grained dependencies effectively. The use of intra-patch and inter-patch graph layers allows the model to progressively process scales, which demonstrates superior forecasting and classification results on multiple datasets. The structured framework and thorough experimental validation highlight the robustness and efficacy of Hi-Patch.' Weaknesses='The proposed Hi-Patch model, while innovative, may encounter scalability issues as the number of variables and the complexity of time series increase. Additionally, the approach lacks a detailed comparison of computational efficiency against state-of-the-art models, which could help gauge its practical application in real-world scenarios. The dependence on fully connected graph networks within each patch might result in computational inefficiency as data scales, affecting its practical deployment.' Questions='How does Hi-Patch handle missing data points within and across time segments when processing irregularly sampled data? What specific strategies does the model employ to maintain its robustness in such cases? Additionally, what is the computational complexity of the model, and how does it compare to the state-of-the-art models in terms of efficiency when handling large datasets?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=3 Rating=8 Confidence=3 Strengths='The hierarchical patch graph network (Hi-Patch) offers a novel multi-scale modeling approach that addresses the complexities of irregular multivariate time series by capturing both fine-grained and coarse-grained dependencies effectively. The use of intra-patch and inter-patch graph layers allows the model to progressively process scales, which demonstrates superior forecasting and classification results on multiple datasets. The structured framework and thorough experimental validation highlight the robustness and efficacy of Hi-Patch.' Weaknesses='The proposed Hi-Patch model, while innovative, may encounter scalability issues as the number of variables and the complexity of time series increase. Additionally, the approach lacks a detailed comparison of computational efficiency against state-of-the-art models, which could help gauge its practical application in real-world scenarios. The dependence on fully connected graph networks within each patch might result in computational inefficiency as data scales, affecting its practical deployment.' Questions='How does Hi-Patch handle missing data points within and across time segments when processing irregularly sampled data? What specific strategies does the model employ to maintain its robustness in such cases? Additionally, what is the computational complexity of the model, and how does it compare to the state-of-the-art models in terms of efficiency when handling large datasets?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='1. The paper addresses an important and timely problem of identifying watermarks in LLM outputs, crucial for protecting copyright and reducing misuse.\\n2. The Water-Probe algorithm is shown to be highly effective at detecting watermarked outputs across various LLMs with a low false positive rate.\\n3. The introduction of the Water-Bag strategy enhances watermark imperceptibility, providing a valuable contribution to the field of watermarking in AI-generated content.' Weaknesses='1. The paper focuses on watermark detection but does not comprehensively address the challenges of watermark embedding into LLMs, which is also crucial for practical deployment.\\n2. There is a trade-off in detectability when using the Water-Bag strategy; this aspect could be explored further to assess its impact on real-world applications.\\n3. The presentation could be improved with more detailed explanations of the experimental setups and clearer illustration of the methods used.' Questions=\"1. How does the Water-Probe algorithm perform when faced with different types of text modifications, such as paraphrasing or summarization? Are its detection capabilities robust to these changes?\\n2. Could the authors provide more insight into the computational cost associated with implementing the Water-Probe and Water-Bag strategies? How feasible are they in large-scale real-world applications?\\n3. What are the specific criteria used to define 'increased randomness' in the Water-Bag strategy, and how is this measured experimentally? More details would help in understanding the designed methodology.\"\n",
      "Success: Soundness=4 Presentation=3 Contribution=3 Rating=7 Confidence=3 Strengths='1. The paper addresses an important and timely problem of identifying watermarks in LLM outputs, crucial for protecting copyright and reducing misuse.\\n2. The Water-Probe algorithm is shown to be highly effective at detecting watermarked outputs across various LLMs with a low false positive rate.\\n3. The introduction of the Water-Bag strategy enhances watermark imperceptibility, providing a valuable contribution to the field of watermarking in AI-generated content.' Weaknesses='1. The paper focuses on watermark detection but does not comprehensively address the challenges of watermark embedding into LLMs, which is also crucial for practical deployment.\\n2. There is a trade-off in detectability when using the Water-Bag strategy; this aspect could be explored further to assess its impact on real-world applications.\\n3. The presentation could be improved with more detailed explanations of the experimental setups and clearer illustration of the methods used.' Questions=\"1. How does the Water-Probe algorithm perform when faced with different types of text modifications, such as paraphrasing or summarization? Are its detection capabilities robust to these changes?\\n2. Could the authors provide more insight into the computational cost associated with implementing the Water-Probe and Water-Bag strategies? How feasible are they in large-scale real-world applications?\\n3. What are the specific criteria used to define 'increased randomness' in the Water-Bag strategy, and how is this measured experimentally? More details would help in understanding the designed methodology.\"\n",
      "Zero shot review:\n",
      " Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=3 Strengths='The paper introduces a novel formalism for understanding and developing Direct Preference Alignment (DPA) loss functions using symbolic logic, which is a significant conceptual advance. By providing a structured landscape of DPA loss functions, it aids in the development and analysis of human-AI alignment algorithms. The methodology is well-explained and offers potential for substantial improvements in understanding and deriving new DPA algorithms.' Weaknesses='While the paper presents a strong theoretical framework, it might be challenging to implement practically due to the complexity involved in translating existing DPA losses into symbolic programs. The work may also face scalability issues with the symbolic reasoning approach when applied to large-scale models or datasets without significant computational resources.' Questions='1. How does the proposed symbolic logic framework handle scalability for large language models, and what are its computational requirements?\\n2. Can the framework be extended or adapted to other types of preference alignment algorithms outside the scope of DPA? If so, how?'\n",
      "Success: Soundness=3 Presentation=4 Contribution=4 Rating=7 Confidence=3 Strengths='The paper introduces a novel formalism for understanding and developing Direct Preference Alignment (DPA) loss functions using symbolic logic, which is a significant conceptual advance. By providing a structured landscape of DPA loss functions, it aids in the development and analysis of human-AI alignment algorithms. The methodology is well-explained and offers potential for substantial improvements in understanding and deriving new DPA algorithms.' Weaknesses='While the paper presents a strong theoretical framework, it might be challenging to implement practically due to the complexity involved in translating existing DPA losses into symbolic programs. The work may also face scalability issues with the symbolic reasoning approach when applied to large-scale models or datasets without significant computational resources.' Questions='1. How does the proposed symbolic logic framework handle scalability for large language models, and what are its computational requirements?\\n2. Can the framework be extended or adapted to other types of preference alignment algorithms outside the scope of DPA? If so, how?'\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1) The introduction of a memory-guided temporal module and emotion-aware audio module addresses key challenges in audio-driven talking video generation, providing significant improvements in identity consistency, synchronization, and expression quality.\\n2) Comprehensive quantitative and qualitative evaluations demonstrate that MEMO outperforms current state-of-the-art methods across various metrics and datasets, including out-of-distribution scenarios.\\n3) The paper includes detailed ablation studies to validate the effectiveness of its components, enhancing the credibility and transparency of the proposed method.\\n4) The presentation of the method and results is well-structured, making the complex concepts accessible and easy to understand.' Weaknesses='1) While the paper introduces innovative components, the reliance on diffusion models and existing methodologies such as linear attention and Emotion AdaNorm may reduce the perception of innovation.\\n2) The paper could benefit from a more in-depth comparison with a wider range of baseline models, particularly focusing on how each component of MEMO contributes to performance gains over specific baselines.\\n3) Some aspects of the methodology and the training pipeline could be further elaborated for clearer understanding and reproducibility, particularly the implementation details of the emotion-guidance mechanism.' Questions='1) Can the memory-guided temporal module and emotion-aware audio module be integrated into other types of generative models beyond diffusion models?\\n2) How does the model perform with audio inputs in languages other than those included in the training dataset?\\n3) What is the computational efficiency of MEMO in terms of training and inference time compared to other methods, and how does this impact its applicability for real-time applications?'\n",
      "Success: Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths='1) The introduction of a memory-guided temporal module and emotion-aware audio module addresses key challenges in audio-driven talking video generation, providing significant improvements in identity consistency, synchronization, and expression quality.\\n2) Comprehensive quantitative and qualitative evaluations demonstrate that MEMO outperforms current state-of-the-art methods across various metrics and datasets, including out-of-distribution scenarios.\\n3) The paper includes detailed ablation studies to validate the effectiveness of its components, enhancing the credibility and transparency of the proposed method.\\n4) The presentation of the method and results is well-structured, making the complex concepts accessible and easy to understand.' Weaknesses='1) While the paper introduces innovative components, the reliance on diffusion models and existing methodologies such as linear attention and Emotion AdaNorm may reduce the perception of innovation.\\n2) The paper could benefit from a more in-depth comparison with a wider range of baseline models, particularly focusing on how each component of MEMO contributes to performance gains over specific baselines.\\n3) Some aspects of the methodology and the training pipeline could be further elaborated for clearer understanding and reproducibility, particularly the implementation details of the emotion-guidance mechanism.' Questions='1) Can the memory-guided temporal module and emotion-aware audio module be integrated into other types of generative models beyond diffusion models?\\n2) How does the model perform with audio inputs in languages other than those included in the training dataset?\\n3) What is the computational efficiency of MEMO in terms of training and inference time compared to other methods, and how does this impact its applicability for real-time applications?'\n",
      "Finished gpt4o on data/test_data/few_shot/test_data_2025_summary_prompts_fewshot.jsonl, saved => results/gpt4o/test_data_2025_summary_prompts_fewshot_gpt4o_results.jsonl\n"
     ]
    }
   ],
   "source": [
    "# 1) Build file lists. Each set has multiple JSONL files\n",
    "zero_shot_files = [\n",
    "    \"data/test_data/zero_shot/test_data_2024_abstract_prompts.jsonl\",\n",
    "    #\"data/test_data/zero_shot/test_data_2024_full text_prompts.jsonl\",\n",
    "    #\"data/test_data/zero_shot/test_data_2024_summary_prompts.jsonl\",\n",
    "    \"data/test_data/zero_shot/test_data_2025_abstract_prompts.jsonl\",\n",
    "    #\"data/test_data/zero_shot/test_data_2025_full text_prompts.jsonl\",\n",
    "    #\"data/test_data/zero_shot/test_data_2025_summary_prompts.jsonl\",\n",
    "]\n",
    "one_shot_files = [\n",
    "    \"data/test_data/one_shot/test_data_2024_abstract_prompts_oneshot.jsonl\",\n",
    "    #\"data/test_data/one_shot/test_data_2024_full text_prompts_oneshot.jsonl\",\n",
    "    \"data/test_data/one_shot/test_data_2024_summary_prompts_oneshot.jsonl\",\n",
    "    \"data/test_data/one_shot/test_data_2025_abstract_prompts_oneshot.jsonl\",\n",
    "    #\"data/test_data/one_shot/test_data_2025_full text_prompts_oneshot.jsonl\",\n",
    "    \"data/test_data/one_shot/test_data_2025_summary_prompts_oneshot.jsonl\",\n",
    "]\n",
    "few_shot_files = [\n",
    "    \"data/test_data/few_shot/test_data_2024_abstract_prompts_fewshot.jsonl\",\n",
    "    #\"data/test_data/few_shot/test_data_2024_full text_prompts_fewshot.jsonl\",\n",
    "    \"data/test_data/few_shot/test_data_2024_summary_prompts_fewshot.jsonl\",\n",
    "    \"data/test_data/few_shot/test_data_2025_abstract_prompts_fewshot.jsonl\",\n",
    "    #\"data/test_data/few_shot/test_data_2025_full text_prompts_fewshot.jsonl\",\n",
    "    \"data/test_data/few_shot/test_data_2025_summary_prompts_fewshot.jsonl\",\n",
    "]\n",
    "\n",
    "# Combine them if you want to run everything at once\n",
    "all_prompt_files = zero_shot_files + one_shot_files + few_shot_files\n",
    "# all_prompt_files = zero_shot_files\n",
    "\n",
    "# 2) Evaluate with GPT-4o\n",
    "print(\"=== Running GPT-4o Inference ===\")\n",
    "evaluate_model_on_files(\n",
    "    model_name=\"gpt4o\",\n",
    "    model_inference_func=run_gpt4o_inference,\n",
    "    input_files=all_prompt_files,\n",
    "    output_dir=\"results/gpt4o\"\n",
    ")\n",
    "\n",
    "# # 3) Evaluate with Llama-3 8B\n",
    "# print(\"\\n=== Running Llama-3 8B Inference ===\")\n",
    "# # We already created the llama_pipeline above\n",
    "# evaluate_model_on_files(\n",
    "#     model_name=\"llama3\",\n",
    "#     model_inference_func=run_llama3_inference,\n",
    "#     input_files=all_prompt_files,\n",
    "#     output_dir=\"results/llama3\"\n",
    "# )\n",
    "\n",
    "# print(\"Done! All results saved in `results/` subfolders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
