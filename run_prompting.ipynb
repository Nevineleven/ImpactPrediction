{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, ValidationError\n",
    "from typing import List\n",
    "from openai import OpenAI\n",
    "import re\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 1. Pydantic model for the paper review\n",
    "##################################################\n",
    "\n",
    "class PaperReview(BaseModel):\n",
    "    Soundness: int\n",
    "    Presentation: int\n",
    "    Contribution: int\n",
    "    Rating: int\n",
    "    Confidence: int\n",
    "    Strengths: str\n",
    "    Weaknesses: str\n",
    "    Questions: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2. Helper functions\n",
    "##################################################\n",
    "\n",
    "def load_jsonl(file_path: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Loads a JSONL file and returns a list of dictionaries.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def load_json(file_path: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Loads a standard JSON (not JSONL) file; returns a list or dict\n",
    "    depending on structure. Adjust as needed if your file structure differs.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def fetch_example_by_paper_id(examples: List[dict], paper_id: str) -> dict:\n",
    "    \"\"\"\n",
    "    Given a list of examples that presumably have {\"paper_id\": <str>, \"messages\": [...]},\n",
    "    returns the one matching the given paper_id. If not found, returns None.\n",
    "    \"\"\"\n",
    "    for ex in examples:\n",
    "        # Each ex presumably has some structure that includes\n",
    "        # the \"paper_id\" in ex itself or inside ex[\"messages\"] ...\n",
    "        #\n",
    "        # If your data includes paper_id outside of ex[\"messages\"],\n",
    "        # you'd do something like ex[\"paper_id\"] == paper_id.\n",
    "        #\n",
    "        # Otherwise, you need to see how you stored the paper_id\n",
    "        # in the JSONL. For this example, let's assume\n",
    "        # ex[\"paper_id\"] is at the top level.\n",
    "        if ex.get(\"paper_id\") == paper_id:\n",
    "            return ex\n",
    "    return None\n",
    "\n",
    "def parse_paper_review(response_content: str) -> PaperReview:\n",
    "    \"\"\"\n",
    "    Takes the raw string content from a model's response,\n",
    "    parses it as JSON, and then validates against PaperReview.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed_dict = json.loads(response_content)\n",
    "        return PaperReview(**parsed_dict)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Invalid JSON returned by the model:\", e)\n",
    "        raise\n",
    "    except ValidationError as ve:\n",
    "        print(\"Pydantic validation error:\", ve)\n",
    "        raise\n",
    "\n",
    "def extract_json_string(raw_content: str) -> str:\n",
    "    match = re.search(r\"```(?:json)?(.*?)```\", raw_content, flags=re.DOTALL)\n",
    "    if match:\n",
    "        # Return only the inner portion of the code fence\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return raw_content.strip()\n",
    "\n",
    "def build_few_shot_context(\n",
    "    examples: List[dict],\n",
    "    shot_ids: List[str],\n",
    "    new_example: dict\n",
    ") -> List[dict]:\n",
    "    \"\"\"\n",
    "    Creates a 'few-shot' prompt by concatenating the\n",
    "    demonstration examples (system + user + assistant from each shot)\n",
    "    followed by the new system + user from the new_example.\n",
    "    \n",
    "    Returns a messages list that you can pass to the chat completion.\n",
    "    \"\"\"\n",
    "    # 1) Start with an empty list\n",
    "    messages = []\n",
    "\n",
    "    # 2) For each demonstration example (one-shot or multi-shot),\n",
    "    #    add system, user, and assistant roles as a demonstration\n",
    "    for shot_id in shot_ids:\n",
    "        demo_ex = fetch_example_by_paper_id(examples, shot_id)\n",
    "        if not demo_ex:\n",
    "            print(f\"WARNING: No example found for shot ID {shot_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Add demonstration messages (full conversation: system, user, assistant)\n",
    "        # Typically you want to instruct the model that the assistant message\n",
    "        # is an \"example\" of how to respond.\n",
    "        for msg in demo_ex[\"messages\"]:\n",
    "            # We often transform the roles slightly for few-shot:\n",
    "            # e.g. \"system\" -> \"system\", \"user\" -> \"user\", \"assistant\" -> \"assistant\"\n",
    "            # or sometimes we embed them in a system message. \n",
    "            # For simplicity, let's keep them as separate turns.\n",
    "            messages.append(msg)\n",
    "\n",
    "    # 3) Finally, add the \"system\" + \"user\" from the new_example,\n",
    "    #    but do NOT include the new_example's \"assistant\" because\n",
    "    #    we want the model to generate a new answer.\n",
    "    #    new_example[\"messages\"][0] is system\n",
    "    #    new_example[\"messages\"][1] is user\n",
    "    #    new_example[\"messages\"][2] is assistant (the 'gold' we won't include)\n",
    "    new_system_msg = new_example[\"messages\"][0]\n",
    "    new_user_msg   = new_example[\"messages\"][1]\n",
    "\n",
    "    messages.append(new_system_msg)\n",
    "    messages.append(new_user_msg)\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "test_2024 = load_jsonl(\"data/test_data_2024_summary_prompts.jsonl\")\n",
    "test_2025 = load_jsonl(\"data/test_data_2025_summary_prompts.jsonl\")\n",
    "\n",
    "# Suppose we also have \"unused_data_2024.json\" with additional examples\n",
    "with open(\"data/unused_data_2024.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    unused_data_2024 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ZERO SHOT EXAMPLE ===\n",
      "Zero shot review:\n",
      " Soundness=4 Presentation=4 Contribution=4 Rating=8 Confidence=4 Strengths=\"The paper effectively integrates two NAS methodologies to leverage their respective strengths: weight-entanglement's reduced memory usage and gradient-based optimization's performance. The comprehensive experiments and benchmarks provide strong empirical validation for the proposed approach.\" Weaknesses='The integration of TangleNAS into other existing frameworks and its application to real-world problems might require additional investigation. The practical impact in terms of training time, rather than memory or accuracy alone, could be further explored.' Questions='How does TangleNAS handle potential compatibility issues between different operation types when superimposing weights with zero-padding? What are the specific limitations of TangleNAS in terms of scalability to even larger architecture spaces?'\n"
     ]
    }
   ],
   "source": [
    "# 3.2) Zero Shot Example\n",
    "print(\"=== ZERO SHOT EXAMPLE ===\")\n",
    "# We'll pick one from the test_2024 data, e.g. the first\n",
    "zero_shot_example = test_2024[0]\n",
    "zero_shot_messages = zero_shot_example[\"messages\"][:2]  # system, user\n",
    "try:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=zero_shot_messages\n",
    "    )\n",
    "    raw_content = completion.choices[0].message.content\n",
    "    json_str = extract_json_string(raw_content)\n",
    "    review = parse_paper_review(json_str)\n",
    "    print(\"Zero shot review:\\n\", review)\n",
    "except Exception as e:\n",
    "    print(\"Zero-shot error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ONE SHOT EXAMPLE ===\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'messages'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# We use the paper_id \"o1TKGCrSL7\" from the unused_data_2024 as the demonstration\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Then we also pick a new test example to evaluate zero shot on\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# In a typical approach, \"o1TKGCrSL7\" is the demonstration, and the \"new\" test example\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# is the query. For clarity, let's just pick the second item in test_2024 as the query\u001b[39;00m\n\u001b[1;32m      8\u001b[0m new_example_for_query \u001b[38;5;241m=\u001b[39m test_2024[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m one_shot_messages \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_few_shot_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munused_data_2024\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshot_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mo1TKGCrSL7\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# single demonstration ID\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_example\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_example_for_query\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 86\u001b[0m, in \u001b[0;36mbuild_few_shot_context\u001b[0;34m(examples, shot_ids, new_example)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# Add demonstration messages (full conversation: system, user, assistant)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# Typically you want to instruct the model that the assistant message\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# is an \"example\" of how to respond.\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdemo_ex\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;66;03m# We often transform the roles slightly for few-shot:\u001b[39;00m\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;66;03m# e.g. \"system\" -> \"system\", \"user\" -> \"user\", \"assistant\" -> \"assistant\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;66;03m# or sometimes we embed them in a system message. \u001b[39;00m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;66;03m# For simplicity, let's keep them as separate turns.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m         messages\u001b[38;5;241m.\u001b[39mappend(msg)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# 3) Finally, add the \"system\" + \"user\" from the new_example,\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m#    but do NOT include the new_example's \"assistant\" because\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m#    we want the model to generate a new answer.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m#    new_example[\"messages\"][0] is system\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m#    new_example[\"messages\"][1] is user\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m#    new_example[\"messages\"][2] is assistant (the 'gold' we won't include)\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'messages'"
     ]
    }
   ],
   "source": [
    "# 3.3) One Shot Example\n",
    "print(\"\\n=== ONE SHOT EXAMPLE ===\")\n",
    "# We use the paper_id \"o1TKGCrSL7\" from the unused_data_2024 as the demonstration\n",
    "# Then we also pick a new test example to evaluate zero shot on\n",
    "# In a typical approach, \"o1TKGCrSL7\" is the demonstration, and the \"new\" test example\n",
    "# is the query. For clarity, let's just pick the second item in test_2024 as the query\n",
    "\n",
    "new_example_for_query = test_2024[1]\n",
    "one_shot_messages = build_few_shot_context(\n",
    "    examples=unused_data_2024,\n",
    "    shot_ids=[\"o1TKGCrSL7\"],   # single demonstration ID\n",
    "    new_example=new_example_for_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=one_shot_messages\n",
    "    )\n",
    "    raw_content = completion.choices[0].message.content\n",
    "    json_str = extract_json_string(raw_content)\n",
    "    review = parse_paper_review(json_str)\n",
    "    print(\"One shot review:\\n\", review)\n",
    "except Exception as e:\n",
    "    print(\"One-shot error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4) Few Shot Example\n",
    "print(\"\\n=== FEW SHOT EXAMPLE ===\")\n",
    "# We'll use the paper_id \"o1TKGCrSL7\", \"JzvIWvC9MG\", and \"5rrYpa2vts\" from unused_data_2024\n",
    "# as the demonstrations. Then pick another new test example as the query.\n",
    "if len(test_2024) > 2:\n",
    "    new_example_for_query = test_2024[2]\n",
    "    few_shot_messages = build_few_shot_context(\n",
    "        examples=unused_data_2024,\n",
    "        shot_ids=[\"o1TKGCrSL7\", \"JzvIWvC9MG\", \"5rrYpa2vts\"],\n",
    "        new_example=new_example_for_query\n",
    "    )\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=few_shot_messages\n",
    "        )\n",
    "        raw_content = completion.choices[0].message.content\n",
    "        json_str = extract_json_string(raw_content)\n",
    "        review = parse_paper_review(json_str)\n",
    "        print(\"Few shot review:\\n\", review)\n",
    "    except Exception as e:\n",
    "        print(\"Few-shot error:\", e)\n",
    "else:\n",
    "    print(\"Not enough test_2024 data to do few-shot example.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
