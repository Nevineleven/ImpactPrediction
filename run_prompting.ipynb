{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, ValidationError\n",
    "from typing import List\n",
    "from openai import OpenAI\n",
    "import re\n",
    "# import torch\n",
    "\n",
    "# For Hugging Face usage (e.g., Llama 3)\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 1. Pydantic model for the paper review\n",
    "##################################################\n",
    "\n",
    "class PaperReview(BaseModel):\n",
    "    Soundness: int\n",
    "    Presentation: int\n",
    "    Contribution: int\n",
    "    Rating: int\n",
    "    Confidence: int\n",
    "    Strengths: str\n",
    "    Weaknesses: str\n",
    "    Questions: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# 2. Helper functions\n",
    "##################################################\n",
    "\n",
    "def load_jsonl(file_path: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Loads a JSONL file and returns a list of dictionaries.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def save_jsonl(data: List[dict], file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves a list of dicts to JSONL, one JSON object per line.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for d in data:\n",
    "            f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def extract_json_string(raw_content: str) -> str:\n",
    "    \"\"\"\n",
    "    Sometimes the model might enclose JSON in triple backticks.\n",
    "    This regex tries to extract the JSON portion if present.\n",
    "    Otherwise returns the full raw_content.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"```(?:json)?(.*?)```\", raw_content, flags=re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return raw_content.strip()\n",
    "\n",
    "def parse_paper_review(response_content: str) -> PaperReview:\n",
    "    \"\"\"\n",
    "    Takes the raw string content from a model's response,\n",
    "    parses it as JSON, and then validates against PaperReview.\n",
    "    Raises exceptions if invalid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed_dict = json.loads(response_content)\n",
    "        return PaperReview(**parsed_dict)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Invalid JSON returned by the model:\\n{e}\\nContent:\\n{response_content}\") from e\n",
    "    except ValidationError as ve:\n",
    "        raise ValueError(f\"Pydantic validation error:\\n{ve}\\nContent:\\n{response_content}\") from ve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def run_gpt4o_inference(messages: List[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Given a list of messages in the OpenAI chat format, \n",
    "    calls the GPT-4 'o' model (whatever alias you have) \n",
    "    and returns the text content of the top choice.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages\n",
    "        )\n",
    "        raw_content = completion.choices[0].message.content\n",
    "        json_str = extract_json_string(raw_content)\n",
    "        review = parse_paper_review(json_str)\n",
    "        print(\"Zero shot review:\\n\", review)\n",
    "    except Exception as e:\n",
    "        print(\"Zero-shot error:\", e)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Llama 3.1 8B usage from Hugging Face\n",
    "# # For a quick pipeline-based approach:\n",
    "# llama_pipeline = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=\"meta-llama/Llama-3.1-8B-Instruct\",  # or your actual Llama 3 model path\n",
    "#     trust_remote_code=True,\n",
    "#     device=0 if torch.cuda.is_available() else -1  # for GPU if available\n",
    "# )\n",
    "\n",
    "# def run_llama3_inference(messages: List[dict]) -> str:\n",
    "#     \"\"\"\n",
    "#     A very naive approach for Llama: we just concatenate the messages \n",
    "#     into a single prompt, because the huggingface pipeline doesn't \n",
    "#     natively handle the 'role-based' chat format by default.\n",
    "\n",
    "#     For a more sophisticated approach, you'd implement the chat logic \n",
    "#     or use a library that supports it. This is just a demonstration.\n",
    "#     \"\"\"\n",
    "#     # Simple approach: system + user => combine them as prompt\n",
    "#     # (If you have multiple system/user pairs, you can decide how to handle them)\n",
    "#     combined_prompt = \"\"\n",
    "#     for m in messages:\n",
    "#         role = m[\"role\"]\n",
    "#         content = m[\"content\"]\n",
    "#         if role == \"system\":\n",
    "#             combined_prompt += f\"[SYSTEM]\\n{content}\\n\"\n",
    "#         elif role == \"user\":\n",
    "#             combined_prompt += f\"[USER]\\n{content}\\n\"\n",
    "#         else:\n",
    "#             combined_prompt += f\"[{role.upper()}]\\n{content}\\n\"\n",
    "\n",
    "#     # Generate with pipeline\n",
    "#     outputs = llama_pipeline(combined_prompt, max_new_tokens=1000)\n",
    "#     # The pipeline returns a list of dicts with \"generated_text\".\n",
    "#     # We'll just return the first\n",
    "#     text_out = outputs[0][\"generated_text\"]\n",
    "#     return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 4. Main Evaluate Function\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def evaluate_model_on_files(\n",
    "    model_name: str,\n",
    "    model_inference_func,\n",
    "    input_files: List[str],\n",
    "    output_dir: str\n",
    "):\n",
    "    \"\"\"\n",
    "    - For each input_file in input_files, load the data\n",
    "    - For each line (prompt), call model_inference_func(messages)\n",
    "    - Parse output as PaperReview\n",
    "    - Save the results in output_dir/<basename>_results.jsonl\n",
    "    \"\"\"\n",
    "\n",
    "    for file_path in input_files:\n",
    "        data = load_jsonl(file_path)\n",
    "        results = []\n",
    "        for entry in data:\n",
    "            paper_id = entry.get(\"paper_id\", \"unknown_id\")\n",
    "            messages = entry.get(\"messages\", [])\n",
    "\n",
    "            # Call the model\n",
    "            try:\n",
    "                raw_content = model_inference_func(messages)\n",
    "                # Attempt to parse PaperReview\n",
    "                json_str = extract_json_string(raw_content)\n",
    "                review = parse_paper_review(json_str)\n",
    "\n",
    "                results.append({\n",
    "                    \"paper_id\": paper_id,\n",
    "                    \"raw_response\": raw_content,\n",
    "                    \"review\": review.dict()\n",
    "                })\n",
    "            except Exception as e:\n",
    "                # If an error or invalid JSON, store the error\n",
    "                results.append({\n",
    "                    \"paper_id\": paper_id,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "\n",
    "        # Build output path\n",
    "        base_name = os.path.basename(file_path)\n",
    "        out_name = base_name.replace(\".jsonl\", f\"_{model_name}_results.jsonl\")\n",
    "        out_path = os.path.join(output_dir, out_name)\n",
    "        save_jsonl(results, out_path)\n",
    "        print(f\"Finished {model_name} on {file_path}, saved => {out_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Build file lists. Each set has multiple JSONL files\n",
    "zero_shot_files = [\n",
    "    \"data/test_data/zero_shot/test_data_2024_abstract_prompts.jsonl\",\n",
    "    \"data/test_data/zero_shot/test_data_2024_full text_prompts.jsonl\",\n",
    "    \"data/test_data/zero_shot/test_data_2024_summary_prompts.jsonl\",\n",
    "    \"data/test_data/zero_shot/test_data_2025_abstract_prompts.jsonl\",\n",
    "    \"data/test_data/zero_shot/test_data_2025_full text_prompts.jsonl\",\n",
    "    \"data/test_data/zero_shot/test_data_2025_summary_prompts.jsonl\",\n",
    "]\n",
    "one_shot_files = [\n",
    "    \"data/test_data/one_shot/test_data_2024_abstract_prompts_oneshot.jsonl\",\n",
    "    \"data/test_data/one_shot/test_data_2024_full text_prompts_oneshot.jsonl\",\n",
    "    \"data/test_data/one_shot/test_data_2024_summary_prompts_oneshot.jsonl\",\n",
    "    \"data/test_data/one_shot/test_data_2025_abstract_prompts_oneshot.jsonl\",\n",
    "    \"data/test_data/one_shot/test_data_2025_full text_prompts_oneshot.jsonl\",\n",
    "    \"data/test_data/one_shot/test_data_2025_summary_prompts_oneshot.jsonl\",\n",
    "]\n",
    "few_shot_files = [\n",
    "    \"data/test_data/few_shot/test_data_2024_abstract_prompts_fewshot.jsonl\",\n",
    "    \"data/test_data/few_shot/test_data_2024_full text_prompts_fewshot.jsonl\",\n",
    "    \"data/test_data/few_shot/test_data_2024_summary_prompts_fewshot.jsonl\",\n",
    "    \"data/test_data/few_shot/test_data_2025_abstract_prompts_fewshot.jsonl\",\n",
    "    \"data/test_data/few_shot/test_data_2025_full text_prompts_fewshot.jsonl\",\n",
    "    \"data/test_data/few_shot/test_data_2025_summary_prompts_fewshot.jsonl\",\n",
    "]\n",
    "\n",
    "# Combine them if you want to run everything at once\n",
    "all_prompt_files = zero_shot_files + one_shot_files + few_shot_files\n",
    "\n",
    "# 2) Evaluate with GPT-4o\n",
    "print(\"=== Running GPT-4o Inference ===\")\n",
    "evaluate_model_on_files(\n",
    "    model_name=\"gpt4o\",\n",
    "    model_inference_func=run_gpt4o_inference,\n",
    "    input_files=all_prompt_files,\n",
    "    output_dir=\"results/gpt4o\"\n",
    ")\n",
    "\n",
    "# # 3) Evaluate with Llama-3 8B\n",
    "# print(\"\\n=== Running Llama-3 8B Inference ===\")\n",
    "# # We already created the llama_pipeline above\n",
    "# evaluate_model_on_files(\n",
    "#     model_name=\"llama3\",\n",
    "#     model_inference_func=run_llama3_inference,\n",
    "#     input_files=all_prompt_files,\n",
    "#     output_dir=\"results/llama3\"\n",
    "# )\n",
    "\n",
    "# print(\"Done! All results saved in `results/` subfolders.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
