{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncPage[Model](data=[Model(id='gpt-4o-mini-audio-preview-2024-12-17', created=1734115920, object='model', owned_by='system'), Model(id='dall-e-3', created=1698785189, object='model', owned_by='system'), Model(id='dall-e-2', created=1698798177, object='model', owned_by='system'), Model(id='gpt-4o-audio-preview-2024-10-01', created=1727389042, object='model', owned_by='system'), Model(id='gpt-4o-audio-preview', created=1727460443, object='model', owned_by='system'), Model(id='gpt-4o', created=1715367049, object='model', owned_by='system'), Model(id='o1-mini-2024-09-12', created=1725648979, object='model', owned_by='system'), Model(id='gpt-4o-mini-realtime-preview-2024-12-17', created=1734112601, object='model', owned_by='system'), Model(id='o1-preview-2024-09-12', created=1725648865, object='model', owned_by='system'), Model(id='o1-mini', created=1725649008, object='model', owned_by='system'), Model(id='o1-preview', created=1725648897, object='model', owned_by='system'), Model(id='gpt-4o-mini-realtime-preview', created=1734387380, object='model', owned_by='system'), Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal'), Model(id='gpt-4o-mini-audio-preview', created=1734387424, object='model', owned_by='system'), Model(id='gpt-4o-2024-08-06', created=1722814719, object='model', owned_by='system'), Model(id='gpt-4o-realtime-preview-2024-10-01', created=1727131766, object='model', owned_by='system'), Model(id='babbage-002', created=1692634615, object='model', owned_by='system'), Model(id='gpt-4o-mini-2024-07-18', created=1721172717, object='model', owned_by='system'), Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system'), Model(id='gpt-4o-mini', created=1721172741, object='model', owned_by='system'), Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system'), Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal'), Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system'), Model(id='davinci-002', created=1692634301, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system'), Model(id='omni-moderation-2024-09-26', created=1732734466, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-0125', created=1706048358, object='model', owned_by='system'), Model(id='gpt-4o-realtime-preview-2024-12-17', created=1733945430, object='model', owned_by='system'), Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai'), Model(id='gpt-4o-realtime-preview', created=1727659998, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal'), Model(id='text-embedding-3-small', created=1705948997, object='model', owned_by='system'), Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal'), Model(id='omni-moderation-latest', created=1731689265, object='model', owned_by='system'), Model(id='gpt-4o-2024-11-20', created=1739331543, object='model', owned_by='system'), Model(id='gpt-4o-2024-05-13', created=1715368132, object='model', owned_by='system'), Model(id='text-embedding-3-large', created=1705953180, object='model', owned_by='system')], object='list')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# 2) Create OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    # organization=\"org-xxx\",           # Optional, if you belong to multiple orgs\n",
    "    # project=\"proj-xxx\",              # Optional, if using project-based keys\n",
    ")\n",
    "\n",
    "# 3) Test the connection by listing available models\n",
    "models = client.models.list()\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Define a structured schema for the response\n",
    "class PaperSummary(BaseModel):\n",
    "    background: str\n",
    "    gap: str\n",
    "    hypothesis: str\n",
    "    method: str\n",
    "    conclusion: str\n",
    "\n",
    "def summarize_paper_text(title_text: str, abstract_text: str, full_text: str):\n",
    "    \"\"\"\n",
    "    Summarize the paper into four bullet points:\n",
    "      1) background\n",
    "      2) gap\n",
    "      3) hypothesis\n",
    "      4) method\n",
    "      5) conclusion\n",
    "    Returns a structured JSON object conforming to PaperSummary.\n",
    "    \"\"\"\n",
    "    # 3.1) Create your prompt (system & user instructions)\n",
    "    system_msg = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\" You are a helpful assistant that extracts a structured summary from the paper. \n",
    "                   Please return only valid JSON that matches the specified schema.\n",
    "\n",
    "                   1. Carefully read the full text, paying particular attention to:\n",
    "                    - The background or prior work that sets the stage for the research\n",
    "                    - The specific gap or problem the paper addresses\n",
    "                    - The hypothesis or main claim/idea the authors propose to solve that gap\n",
    "                    - The method or approach (including any experimental setup, data, or algorithmic details) \n",
    "                    - The main conclusion or findings of the paper\n",
    "   \n",
    "                    2. The summary must be thorough enough that a reviewer could evaluate:\n",
    "                    - Soundness (i.e., clarity and rigor of the methods)\n",
    "                    - Presentation (writing clarity and structure)\n",
    "                    - Contribution (novelty and significance) and have enough detail to assess strengths, weaknesses, and potential questions.\n",
    "\n",
    "                    3. Do NOT omit important details from the method—like dataset, experimental steps, or critical algorithmic components—if they are present in the paper.\n",
    "\n",
    "                    4. You are NOT an interpreter or critic of the text. You must produce an **objective** and **factual** summary. Avoid personal opinions or expansions beyond the paper’s content.\n",
    "\n",
    "                    5. If something is unclear or not stated, do NOT fabricate. \n",
    "                    - Instead note briefly “not explicitly stated” or “insufficient details” within that field.\n",
    "                   \"\"\"\n",
    "    }\n",
    "    user_msg = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"\n",
    "        Title: {title_text}\n",
    "        Abstract: {abstract_text}\n",
    "        Full Text: {full_text}\n",
    "        \n",
    "        Provide:\n",
    "        - background\n",
    "        - gap\n",
    "        - hypothesis\n",
    "        - method\n",
    "        - conclusion\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "    # 3.2) Make the completion request with the new parse(...) method\n",
    "    # IMPORTANT: Use a model that supports structured output. \n",
    "    # e.g. \"o1-2024-12-17\", \"gpt-4o-2024-08-06\", or \"o3-mini-2025-1-31\", etc.\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[system_msg, user_msg],\n",
    "        response_format=PaperSummary  # Tells the model we want a JSON object matching PaperSummary\n",
    "    )\n",
    "\n",
    "    # 3.3) The parsed result is accessible via completion.choices[0].message.parsed\n",
    "    paper_summary = completion.choices[0].message.parsed\n",
    "    return paper_summary\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    title = \"A Novel Method in Quantum Computing\"\n",
    "    abstract = \"In this paper, we explore a new approach to quantum entanglement.\"\n",
    "    full_text = \"Full text containing detailed methodology, results, etc...\"\n",
    "\n",
    "    summary = summarize_paper_text(title, abstract, full_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background='Entanglement is a fundamental concept in quantum mechanics which plays a crucial role in quantum computing and information processing. Previous research has focused on various methods to generate and manipulate entangled states, as these are key for achieving quantum supremacy and practical quantum technologies.' gap='Despite progress in the creation and manipulation of entangled states, existing methods are often limited in scalability and efficiency. Many approaches struggle with maintaining coherence and stability over time, which are vital for practical applications.' hypothesis='The authors propose that a new method for generating quantum entanglement, potentially utilizing novel materials or mechanisms, could overcome current limitations and enhance scalability and efficiency in quantum computing.' method='The paper presents a theoretical framework for a new quantum entanglement generation method. The proposed approach is based on [details not explicitly stated], which involves [insufficient details]. The authors also discuss potential experimental setups and simulations that illustrate the feasibility of this method.' conclusion='The authors conclude that their novel method shows promise for improving the scalability and efficiency of entangled state generation in quantum computing, although experimental validation is needed. Their results indicate potential pathways for overcoming current limitations in the field.'\n",
      "Background:  Entanglement is a fundamental concept in quantum mechanics which plays a crucial role in quantum computing and information processing. Previous research has focused on various methods to generate and manipulate entangled states, as these are key for achieving quantum supremacy and practical quantum technologies.\n",
      "Gap:         Despite progress in the creation and manipulation of entangled states, existing methods are often limited in scalability and efficiency. Many approaches struggle with maintaining coherence and stability over time, which are vital for practical applications.\n",
      "Hypothesis:  The authors propose that a new method for generating quantum entanglement, potentially utilizing novel materials or mechanisms, could overcome current limitations and enhance scalability and efficiency in quantum computing.\n",
      "Method:      The paper presents a theoretical framework for a new quantum entanglement generation method. The proposed approach is based on [details not explicitly stated], which involves [insufficient details]. The authors also discuss potential experimental setups and simulations that illustrate the feasibility of this method.\n",
      "Conclusion:  The authors conclude that their novel method shows promise for improving the scalability and efficiency of entangled state generation in quantum computing, although experimental validation is needed. Their results indicate potential pathways for overcoming current limitations in the field.\n"
     ]
    }
   ],
   "source": [
    "print(summary)  # This will print a PaperSummary object\n",
    "# You can access the fields individually:\n",
    "print(\"Background: \", summary.background)\n",
    "print(\"Gap:        \", summary.gap)\n",
    "print(\"Hypothesis: \", summary.hypothesis)\n",
    "print(\"Method:     \", summary.method)\n",
    "print(\"Conclusion: \", summary.conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5 papers so far...\n",
      "Processed 10 papers so far...\n",
      "Processed 15 papers so far...\n",
      "Processed 20 papers so far...\n",
      "Error summarizing paper 5KojubHBr8: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Xftkx2RCfzMHxshpuGkJcgeX on tokens per min (TPM): Limit 30000, Requested 36042. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Skipping this paper and waiting 15 seconds before continuing...\n",
      "Processed 25 papers so far...\n",
      "Processed 30 papers so far...\n",
      "Error summarizing paper OGtnhKQJms: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Xftkx2RCfzMHxshpuGkJcgeX on tokens per min (TPM): Limit 30000, Requested 31537. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Skipping this paper and waiting 15 seconds before continuing...\n",
      "Error summarizing paper 1hhja8ZxcP: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Xftkx2RCfzMHxshpuGkJcgeX on tokens per min (TPM): Limit 30000, Requested 35833. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Skipping this paper and waiting 15 seconds before continuing...\n",
      "Processed 40 papers so far...\n",
      "Error summarizing paper KjOAHlKMF5: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Xftkx2RCfzMHxshpuGkJcgeX on tokens per min (TPM): Limit 30000, Requested 30945. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Skipping this paper and waiting 15 seconds before continuing...\n",
      "Processed 45 papers so far...\n",
      "Processed 50 papers so far...\n",
      "Processed 55 papers so far...\n",
      "Error summarizing paper zwU9scoU4A: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Xftkx2RCfzMHxshpuGkJcgeX on tokens per min (TPM): Limit 30000, Requested 34868. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Skipping this paper and waiting 15 seconds before continuing...\n",
      "Processed 60 papers so far...\n",
      "Processed 65 papers so far...\n",
      "Processed 70 papers so far...\n",
      "Processed 75 papers so far...\n",
      "Processed 80 papers so far...\n",
      "Processed 85 papers so far...\n",
      "Processed 90 papers so far...\n",
      "Error summarizing paper AgM3MzT99c: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Xftkx2RCfzMHxshpuGkJcgeX on tokens per min (TPM): Limit 30000, Requested 34990. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Skipping this paper and waiting 15 seconds before continuing...\n",
      "Processed 95 papers so far...\n",
      "Error summarizing paper Ki39vo5x1T: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Xftkx2RCfzMHxshpuGkJcgeX on tokens per min (TPM): Limit 30000, Requested 36430. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Skipping this paper and waiting 15 seconds before continuing...\n",
      "Processed 105 papers so far...\n",
      "Error summarizing paper fvhJu0FODp: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Xftkx2RCfzMHxshpuGkJcgeX on tokens per min (TPM): Limit 30000, Requested 31061. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Skipping this paper and waiting 15 seconds before continuing...\n",
      "Processed 110 papers so far...\n",
      "Error summarizing paper kxpswbhr1r: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Xftkx2RCfzMHxshpuGkJcgeX on tokens per min (TPM): Limit 30000, Requested 40981. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Skipping this paper and waiting 15 seconds before continuing...\n",
      "Processed 120 papers so far...\n",
      "Processed 125 papers so far...\n",
      "Processed 130 papers so far...\n",
      "Processed 135 papers so far...\n",
      "Error summarizing paper SNGXbZtK6Q: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Xftkx2RCfzMHxshpuGkJcgeX on tokens per min (TPM): Limit 30000, Requested 30941. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Skipping this paper and waiting 15 seconds before continuing...\n",
      "Error summarizing paper kNPcOaqC5r: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Xftkx2RCfzMHxshpuGkJcgeX on tokens per min (TPM): Limit 30000, Requested 37906. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Skipping this paper and waiting 15 seconds before continuing...\n",
      "Processed 145 papers so far...\n",
      "Error summarizing paper dLrhRIMVmB: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Xftkx2RCfzMHxshpuGkJcgeX on tokens per min (TPM): Limit 30000, Requested 32742. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Skipping this paper and waiting 15 seconds before continuing...\n",
      "Processed 155 papers so far...\n",
      "Processed 160 papers so far...\n",
      "Error summarizing paper D2eOVqPX9g: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Xftkx2RCfzMHxshpuGkJcgeX on tokens per min (TPM): Limit 30000, Requested 36180. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Skipping this paper and waiting 15 seconds before continuing...\n",
      "Error summarizing paper Jos5c7vJPP: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Xftkx2RCfzMHxshpuGkJcgeX on tokens per min (TPM): Limit 30000, Requested 33799. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Skipping this paper and waiting 15 seconds before continuing...\n",
      "Processed 170 papers so far...\n",
      "Processed 175 papers so far...\n",
      "Processed 180 papers so far...\n",
      "Processed 185 papers so far...\n",
      "Processed 190 papers so far...\n",
      "Processed 195 papers so far...\n",
      "Processed 200 papers so far...\n",
      "Processed 205 papers so far...\n",
      "Processed 210 papers so far...\n",
      "Processed 215 papers so far...\n",
      "Processed 220 papers so far...\n",
      "Processed 225 papers so far...\n",
      "Processed 230 papers so far...\n",
      "Error summarizing paper 7UhxsmbdaQ: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Xftkx2RCfzMHxshpuGkJcgeX on tokens per min (TPM): Limit 30000, Requested 30502. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Skipping this paper and waiting 15 seconds before continuing...\n",
      "Processed 235 papers so far...\n",
      "Created instruction-finetuning data at: instruction_finetuning_data.jsonl\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "##############################################################\n",
    "# 2. Building the instruction prompt text\n",
    "##############################################################\n",
    "def build_instruction_prompt(background, gap, hypothesis, method, conclusion):\n",
    "    \"\"\"\n",
    "    Construct the instruction prompt that includes the 4 bullet points\n",
    "    plus the rating categories we need from the user.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\\\n",
    "Background: {background}\n",
    "Gap: {gap}\n",
    "Hypothesis: {hypothesis}\n",
    "Method: {method}\n",
    "Conclusion: {conclusion}\n",
    "\n",
    "Categories (1–5):\n",
    "- Soundness: The rigor of the methods for the stated problem\n",
    "- Presentation: The clarity of writing and organization\n",
    "- Contribution: The paper’s novelty or added value to the domain\n",
    "\n",
    "Additionally:\n",
    "- Rating (1–10): Overall recommendation for acceptance\n",
    "- Confidence (1–5): How confident the reviewer is in their assessment\n",
    "\n",
    "Please provide:\n",
    "1) Soundness\n",
    "2) Presentation\n",
    "3) Contribution\n",
    "4) Rating\n",
    "5) Confidence\n",
    "6) Explanation (strengths, weaknesses)\n",
    "7) Questions\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "##############################################################\n",
    "# 3. Constructing gold-standard response from OpenReview fields\n",
    "##############################################################\n",
    "def build_gold_standard_response(review_data):\n",
    "    \"\"\"\n",
    "    Given the 'reviews' sub-fields, build the textual answer.\n",
    "\n",
    "    We'll look for:\n",
    "      - soundness\n",
    "      - presentation\n",
    "      - contribution\n",
    "      - rating\n",
    "      - confidence\n",
    "      - strengths\n",
    "      - weaknesses\n",
    "      - questions\n",
    "\n",
    "    The final text should match the example structure:\n",
    "\n",
    "    Soundness: 2 (fair)\n",
    "    Presentation: 2 (fair)\n",
    "    ...\n",
    "    Explanation:\n",
    "    Strengths:\n",
    "    ...\n",
    "    Weaknesses:\n",
    "    ...\n",
    "    Questions:\n",
    "    ...\n",
    "    \"\"\"\n",
    "    # Some values are optional if not provided. We'll default them if missing.\n",
    "    soundness_val = review_data.get(\"soundness\", {}).get(\"value\", \"N/A\")\n",
    "    presentation_val = review_data.get(\"presentation\", {}).get(\"value\", \"N/A\")\n",
    "    contribution_val = review_data.get(\"contribution\", {}).get(\"value\", \"N/A\")\n",
    "    rating_val = review_data.get(\"rating\", {}).get(\"value\", \"N/A\")\n",
    "    confidence_val = review_data.get(\"confidence\", {}).get(\"value\", \"N/A\")\n",
    "\n",
    "    # For strengths/weaknesses, we might store them under \"strengths\" and \"weaknesses\"\n",
    "    strengths_val = review_data.get(\"strengths\", {}).get(\"value\", \"None given\")\n",
    "    weaknesses_val = review_data.get(\"weaknesses\", {}).get(\"value\", \"None given\")\n",
    "    questions_val = review_data.get(\"questions\", {}).get(\"value\", \"None\")\n",
    "\n",
    "    # Format them in the example style\n",
    "    response_text = f\"\"\"Soundness: {soundness_val}\n",
    "Presentation: {presentation_val}\n",
    "Contribution: {contribution_val}\n",
    "Rating: {rating_val}\n",
    "Confidence: {confidence_val}\n",
    "\n",
    "Explanation:\n",
    "Strengths:\n",
    "{strengths_val}\n",
    "\n",
    "Weaknesses:\n",
    "{weaknesses_val}\n",
    "\n",
    "Questions:\n",
    "{questions_val}\n",
    "\"\"\"\n",
    "    return response_text\n",
    "\n",
    "##############################################################\n",
    "# 4. Main script to process the JSON data\n",
    "##############################################################\n",
    "def create_instruction_finetuning_data(\n",
    "    input_json_path,\n",
    "    output_jsonl_path\n",
    "):\n",
    "    \"\"\"\n",
    "    1. Load the OpenReview-like JSON data.\n",
    "    2. For each paper, call summarize_paper_text(...) to get 4 bullet points.\n",
    "    3. Build the prompt using build_instruction_prompt(...).\n",
    "    4. Build the gold response from the 'reviews' sub-fields.\n",
    "    5. Store them into a JSON lines file with fields: {\"prompt\": \"...\", \"response\": \"...\"}\n",
    "    \"\"\"\n",
    "\n",
    "    with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    output_lines = []\n",
    "    skipped_papers = []  # Track any papers that fail\n",
    "    for i, (paper_id, paper_info) in enumerate(data.items(), start=1):\n",
    "            \n",
    "        try:\n",
    "            # We can retrieve the 'title', 'abstract', 'full_text' if exist\n",
    "            title_val = paper_info.get(\"title\", {}).get(\"value\", \"\")\n",
    "            abstract_val = paper_info.get(\"abstract\", {}).get(\"value\", \"\")\n",
    "            full_text_val = paper_info.get(\"full_text\", {}).get(\"value\", \"\")\n",
    "\n",
    "            # Summarize with your LLM or a placeholder\n",
    "            summary = summarize_paper_text(\n",
    "                title_val, abstract_val, full_text_val\n",
    "            )\n",
    "\n",
    "            background = summary.background\n",
    "            gap = summary.gap\n",
    "            hypothesis = summary.hypothesis\n",
    "            method = summary.method\n",
    "            conclusion = summary.conclusion\n",
    "\n",
    "            # Build the instruction prompt\n",
    "            prompt_text = build_instruction_prompt(\n",
    "                background,\n",
    "                gap,\n",
    "                hypothesis,\n",
    "                method,\n",
    "                conclusion\n",
    "            )\n",
    "\n",
    "            # Now build the gold standard response from the reviews\n",
    "            reviews_info = paper_info.get(\"reviews\", {})\n",
    "            # We might just pass the entire dictionary to build_gold_standard_response\n",
    "            response_text = build_gold_standard_response(reviews_info)\n",
    "\n",
    "            # Store them in your finetuning dataset\n",
    "            item = {\n",
    "                \"paper_id\": paper_id,\n",
    "                \"prompt\": prompt_text,\n",
    "                \"response\": response_text\n",
    "            }\n",
    "            output_lines.append(item)\n",
    "\n",
    "        except Exception as e:\n",
    "            # If there's an error, skip this paper\n",
    "            print(f\"Error summarizing paper {paper_id}: {e}\")\n",
    "            print(\"Skipping this paper and waiting 15 seconds before continuing...\")\n",
    "            skipped_papers.append(paper_id)\n",
    "            time.sleep(15)\n",
    "            continue\n",
    "\n",
    "        # Every 10 papers, wait 1 minute\n",
    "        if i % 5 == 0:\n",
    "            print(f\"Processed {i} papers so far...\")\n",
    "\n",
    "    # Save to JSONL\n",
    "    with open(output_jsonl_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        for line_item in output_lines:\n",
    "            out_f.write(json.dumps(line_item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Created instruction-finetuning data at: {output_jsonl_path}\")\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# 5. Usage example\n",
    "##############################################################\n",
    "if __name__ == \"__main__\":\n",
    "    input_json_path = \"matched_papers_reviews_test.json\"   # path to your input\n",
    "    output_jsonl_path = \"instruction_finetuning_data_test.jsonl\"\n",
    "\n",
    "    create_instruction_finetuning_data(input_json_path, output_jsonl_path)\n",
    "\n",
    "    print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
