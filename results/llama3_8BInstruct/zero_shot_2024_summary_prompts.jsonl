{"paper_id": "B0OwtVEejJ", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The authors provide a clear and well-structured paper with a good hypothesis and a well-defined method. The experiments are comprehensive and the results are presented in a clear and concise manner.\",\n  \"Weaknesses\": \"The paper assumes some knowledge of NAS methods and weight-entanglement, which might make it difficult for non-experts to understand. The authors could provide more details about the implementation of TangleNAS.\",\n  \"Questions\": \"How does TangleNAS compare to other NAS methods in terms of computational complexity? Are there any potential drawbacks to using weight-entanglement in larger search spaces?\"\n}"}
{"paper_id": "ZlEtXIxl3q", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Effective use of contrastive losses, improvement over existing methods, data-efficiency and robustness.\",\n  \"Weaknesses\": \"None mentioned in the summary.\",\n  \"Questions\": \"How generalizable is this method to other domains beyond protein fitness predictions?\"\n}"}
{"paper_id": "6yJuDK1DsK", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Effective lifelong test-time adaptation, significant computational efficiency, robustness in dynamic environments, no need for source data access\",\n  \"Weaknesses\": \"None mentioned\",\n  \"Questions\": \"How does FEATHER handle catastrophic forgetting in more complex scenarios?\"\n}"}
{"paper_id": "lt6xKGGWov", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper proposes a novel feature selection method, MINERVA, that effectively captures sophisticated relationships between features and targets. It is well-structured and easy to follow.\",\n  \"Weaknesses\": \"None mentioned in the summary.\",\n  \"Questions\": \"How does MINERVA perform on real-world datasets, not just synthetic ones?\"\n}"}
{"paper_id": "Bvrc6kobWd", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 3,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"Original approach to understanding margins in contrastive learning, thorough experimental evaluation, clear conclusion.\",\n  \"Weaknesses\": \"Limited to a specific type of contrastive learning (generalized contrastive learning loss), reliance on specific datasets.\",\n  \"Questions\": \"How generalizable are the findings to other types of contrastive learning? Are there any potential limitations or drawbacks to scaling gradients based on positive sample angles and logits?\"\n}"}
{"paper_id": "FJjHQS2DyE", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"Well-structured paper, clear introduction and conclusion, provides a novel approach to UDA under label shift, empirical validation on benchmark datasets.\",\n  \"Weaknesses\": \"Methodology may be complex for some readers, limited discussion on potential issues with pseudo-labels.\",\n  \"Questions\": \"How does CASA handle scenarios with multiple label shifts? Can the authors provide more insights on the theoretical target risk bound?\"\n}"}
{"paper_id": "xelrLobW0n", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Proposes a novel benchmark (TRACE) and approach (RCL) for evaluating and enhancing continual learning capabilities of LLMs, introduces new evaluation metrics, and demonstrates promising results.\",\n  \"Weaknesses\": \"The paper relies on a limited number of experimental settings and LLMs, and the generalizability of the findings is unclear.\",\n  \"Questions\": \"How generalizable are the findings across different LLMs and tasks? What are the potential limitations of the RCL approach in real-world scenarios?\"\n}"}
{"paper_id": "bYQkOPvgDw", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Effective handling of label noise in GNNs, robustness on both homophilous and heterophilous graphs, high noise-rate scenarios, and no reliance on the label smoothness assumption.\",\n  \"Weaknesses\": \"None mentioned in the summary\",\n  \"Questions\": \"\"\n}"}
{"paper_id": "sKPzAXoylB", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Original approach to address both catastrophic forgetting and loss of plasticity, effective in both representation learning and reinforcement learning tasks, leverages utility-based perturbations to dynamically adjust to importance of learned representations.\",\n  \"Weaknesses\": \"None mentioned in the summary, but potential drawbacks of the approach, such as computational overhead or hyperparameter sensitivity, could be a limitation.\",\n  \"Questions\": \"How does the proposed method compare to other approaches that address catastrophic forgetting and loss of plasticity? Are there any potential applications of this method outside of continual learning?\"\n}"}
{"paper_id": "H8Qg1IIMaR", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Original research, thorough investigation, robust methodology, significant implications for AI robustness.\",\n  \"Weaknesses\": \"None mentioned, but some may consider the study's focus on a specific type of attack as a limitation.\",\n  \"Questions\": \"How generalizable are the findings to other types of models or tasks? What are the potential mitigation strategies for these models' vulnerabilities?\"\n}"}
{"paper_id": "tth2qXY7RU", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Effective solution for quantization challenges, significant improvements in both accuracy and hardware efficiency, flexible adaptability to diverse data distributions.\",\n  \"Weaknesses\": \"None explicitly mentioned in the summary.\",\n  \"Questions\": \"How does SuFP perform on more complex models or larger datasets?\"\n}"}
{"paper_id": "fjf3YenThE", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Improves convergence rates and applicability, removes limitations on the number of random directions, competitive performance on sparse optimization tasks.\",\n  \"Weaknesses\": \"None mentioned\",\n  \"Questions\": \"How does the proposed algorithm perform in more complex scenarios beyond adversarial attacks and portfolio optimization?\"\n}"}
{"paper_id": "H9DYMIpz9c", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Efficient data distillation technique for autoregressive tasks, effective in reducing data size while maintaining model performance, potential for large model scalability.\",\n  \"Weaknesses\": \"Limited to autoregressive tasks, computational inefficiencies in traditional distillation methods not fully addressed.\",\n  \"Questions\": \"How does FARZI handle large discrete token spaces in practice? Can the method be extended to other types of sequential data?\"\n}"}
{"paper_id": "4kLVvIh8cp", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Proposes a novel algorithm (PNLSVI) that offers statistically optimal instance-dependent regret bounds for non-linear function approximation, generalizes pessimism-based linear approaches, and achieves minimax optimality in linear approximation contexts.\",\n  \"Weaknesses\": \"None mentioned in the summary.\",\n  \"Questions\": \"How does PNLSVI compare to existing methods in terms of computational efficiency and scalability? What are the potential applications of PNLSVI in real-world problems with large state-action spaces?\"\n}"}
{"paper_id": "CSpWgKo0ID", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Well-structured methodology, good use of behavioral game theory, clear results and implications for future applications.\",\n  \"Weaknesses\": \"Limited scope in terms of game scenarios and LLMs tested, no clear generalizability to other interactive settings.\",\n  \"Questions\": \"How do the results generalize to more complex social settings? Can the behavioral game theory framework be applied to other types of AI models?\"\n}"}
{"paper_id": "gwbQ2YwLhD", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper provides a thorough investigation of the impact of data scale on structure learning algorithms, and the authors provide both theoretical proofs and experimental results to support their claims. The study has the potential to significantly impact the field of structure learning and has a clear and well-structured presentation.\",\n  \"Weaknesses\": \"The paper could benefit from a more detailed discussion of the implications of their findings for real-world applications, and some of the theoretical proofs may be challenging for non-experts to follow.\",\n  \"Questions\": \"How do the authors plan to address the issue of scale sensitivity in practice, and what are the potential solutions for mitigating its effects?\"\n}"}
{"paper_id": "eIYDKNqXuV", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"Efficient and scalable for large and complex datasets, competitive performance with state-of-the-art methods, significant advantage in speed and efficiency.\",\n  \"Weaknesses\": \"Accuracy can be limited by the initial K-Means step, may require iterative refinement of cluster representations.\",\n  \"Questions\": \"How does the method handle datasets with very complex structures or very large numbers of clusters?\"\n}"}
{"paper_id": "bAMPOUF227", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper presents a clear and well-structured hypothesis, a novel framework (SuperContext), and robust experiments on 16 datasets across 9 tasks. The results show significant improvements in LLMs' performance, particularly in reducing hallucinations.\",\n  \"Weaknesses\": \"The paper assumes that task-specific SLMs can enhance LLMs' performance, which might not hold true for all tasks or domains.\",\n  \"Questions\": \"How does the proposed framework handle potential overfitting to the task-specific SLMs? Are the results generalizable to other LLMs or tasks?\"\n}"}
{"paper_id": "eNoiRal5xi", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Combines parameter space perturbation with data space perturbation to improve model generalization, provides a robust solution to the domain generalization problem, statistically significant improvements in DG benchmark datasets.\",\n  \"Weaknesses\": \"Lacks theoretical guarantees for generalization to unknown domains, primarily focuses on aligning the loss landscape between source and perturbed domains.\",\n  \"Questions\": \"How does UDIM perform in scenarios with very limited domain information? Can the approach be adapted to other machine learning tasks?\"\n}"}
{"paper_id": "uU0Adp7Sfo", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"Provides a unified framework, overcomes limitations of pure-competition GANs, and leads to high-quality data generation.\",\n  \"Weaknesses\": \"None mentioned in the summary.\",\n  \"Questions\": \"How does the CCGAN framework generalize to more complex data distributions?\"\n}"}
{"paper_id": "dKju7tbe6D", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Effective combination of iterative and parallel computation, state-of-the-art zero-shot performance, and strong generalization and efficiency.\",\n  \"Weaknesses\": \"None mentioned in the summary.\",\n  \"Questions\": \"How does the proposed IPRM approach handle more complex visual reasoning tasks? What are the computational requirements for training and deploying IPRM?\"\n}"}
{"paper_id": "DL7JWbdGr3", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Effective use of pre-training paradigms, improved performance over state-of-the-art models, adaptability to new diseases, and enhanced training efficiency.\",\n  \"Weaknesses\": \"None mentioned in the summary, but potential weaknesses could be the reliance on pre-trained models, the need for large datasets, and the difficulty in generalizing to diseases with different characteristics.\",\n  \"Questions\": \"How does the model handle data heterogeneity across different diseases? What are the computational requirements for pre-training and fine-tuning the models?\"\n}"}
{"paper_id": "1PaDPHDhwe", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Proposes a novel post-processing technique for mitigating algorithmic biases, effectively manages the trade-off between robust and average accuracies, and allows for a comprehensive evaluation of debiasing algorithms.\",\n  \"Weaknesses\": \"The approach may not be applicable to all types of datasets and models, and the evaluation metric of robust coverage may not capture all aspects of debiasing algorithms.\",\n  \"Questions\": \"How does the proposed technique compare to existing methods that require additional training, and what are the limitations of the robust coverage metric in evaluating debiasing algorithms?\"\n}"}
{"paper_id": "iI7hZSczxE", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Proposes a novel approach to disentangled representation learning for time series data with correlated attributes, demonstrates improved disentanglement and robustness, and shows potential in addressing real-world generalization challenges.\",\n  \"Weaknesses\": \"None explicitly mentioned, but the paper assumes real-world data exhibits correlated attributes, which might not always be the case.\",\n  \"Questions\": \"How does the proposed method compare to other disentanglement methods in terms of computational efficiency? Are there any potential applications of this method beyond energy consumption disaggregation?\"\n}"}
{"paper_id": "qrGjFJVl3m", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Significant contribution to the field of Large Vision-Language Models, excellent results on various visual-language tasks, and multilingual support.\",\n  \"Weaknesses\": \"None mentioned in the paper\",\n  \"Questions\": \"How does the proposed Qwen-VL series compare to other proprietary models? What are the potential limitations of the current implementation and how can they be addressed?\"\n}"}
{"paper_id": "qup9xD8mW4", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"Original and innovative method for behaviour distillation in reinforcement learning, extensive experimentation and evaluation, competitive performance in traditional dataset distillation.\",\n  \"Weaknesses\": \"The paper assumes a fixed dataset is not necessary for reinforcement learning, which might be a limitation for certain tasks, the method requires significant computational resources.\",\n  \"Questions\": \"How does HaDES compare to other dataset distillation methods in reinforcement learning? Can the method be applied to other types of reinforcement learning tasks?\"\n}"}
{"paper_id": "Zw8YxUWL4R", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Introduces a novel approach to text-to-image synthesis, provides better control and expressiveness, and demonstrates improved performance in reconstruction quality and editability.\",\n  \"Weaknesses\": \"The paper assumes a pre-trained Stable Diffusion model, which might limit its applicability to other models.\",\n  \"Questions\": \"How does the proposed method generalize to other text-to-image synthesis models? Can the approach be extended to other applications beyond image synthesis?\"\n}"}
{"paper_id": "3NXhwkZGjz", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed method is innovative, well-structured, and effectively addresses the SFUDA problem. The use of hypothesis consolidation and pseudo-labeling enhances the reliability of the adaptation process. The method's flexibility and ability to integrate with existing approaches are notable strengths.\",\n  \"Weaknesses\": \"The paper assumes a certain level of expertise in domain adaptation and semi-supervised learning, which might make it challenging for non-experts to follow. The evaluation metrics and comparison to other state-of-the-art methods are not thoroughly discussed.\",\n  \"Questions\": \"How does the method handle cases where the source and target domains have significantly different data distributions? Can the method be adapted for other types of domain adaptation problems?\"\n}"}
{"paper_id": "Fn655mJ4bv", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Effective interpretation of structured output models, consideration of correlations among output variables, computational efficiency and scalability.\",\n  \"Weaknesses\": \"Limited information about the energy-based approach and the Gumbel-Softmax trick.\",\n  \"Questions\": \"How does the method handle cases where the number of output variables is very large? Can the method be applied to other types of models?\"\n}"}
{"paper_id": "VyMW4YZfw7", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Simplification of complex GNN models, cleaner theoretical analysis, highlighting the impact of evaluation conventions.\",\n  \"Weaknesses\": \"Limited discussion on the potential drawbacks of traditional nonparametric methods.\",\n  \"Questions\": \"How do the proposed methods generalize to more complex graph structures? Can the simplicity of the proposed methods be further improved?\"\n}"}
{"paper_id": "09xFexjhqE", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Effective solution to the problem of gradient divergence in RFT, state-of-the-art adversarial robustness, automated approach to hyperparameter tuning.\",\n  \"Weaknesses\": \"None mentioned in the summary\",\n  \"Questions\": \"How does AutoLoRa compare to other state-of-the-art methods in terms of robustness and efficiency?\"\n}"}
{"paper_id": "nmBjBZoySX", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Original idea, well-structured framework, extensive experiments, scalability and adaptability.\",\n  \"Weaknesses\": \"None mentioned\",\n  \"Questions\": \"How does AdaGLT compare to other state-of-the-art methods in terms of computational cost?\"\n}"}
{"paper_id": "di52zR8xgf", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Competitive performance with state-of-the-art black-box models, open and transparent, reproducibility and validation, improvements in image quality and adherence to text prompts.\",\n  \"Weaknesses\": \"Still suggests future work in areas like single-stage generation and incorporation of additional text synthesis improvements.\",\n  \"Questions\": \"How does the model handle varying image aspect ratios, what are the limitations of the current refinement stage?\"\n}"}
{"paper_id": "0b328CMwn1", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Original approach to visual prompting, connection to normalization tuning, extensive experiments, improved performance and efficiency.\",\n  \"Weaknesses\": \"Performance gap between VP and conventional fine-tuning methods, choice of layer depth affects performance.\",\n  \"Questions\": \"How does the choice of layer depth for AP implementation affect performance in different architectures? Are there any potential limitations or drawbacks to the proposed method?\"\n}"}
{"paper_id": "AMDKqZcZbi", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"Biologically inspired neural architecture, combination of rapid learning and retention of previously acquired knowledge, successful in both continual and few-shot learning contexts.\",\n  \"Weaknesses\": \"None explicitly mentioned in the paper, but it may be challenging to generalize the proposed model to other domains or tasks.\",\n  \"Questions\": \"How does the proposed model compare to other existing methods in terms of computational efficiency and scalability? Can the model be further improved by incorporating more advanced techniques from biology or machine learning?\"\n}"}
{"paper_id": "am7BPV3Cwo", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Effective solution to the class-aware bias problem, significant improvements over state-of-the-art methods, robust solution for practical OOD detection challenges.\",\n  \"Weaknesses\": \"None mentioned\",\n  \"Questions\": \"How does the ImOOD framework perform on other types of imbalanced datasets?\"\n}"}
{"paper_id": "R1crLHQ4kf", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"Proposes a novel and efficient detection mechanism for adversarial attacks on ASR systems, demonstrates high accuracy and robustness across multiple models and datasets, provides a new defensive layer for ASR systems.\",\n  \"Weaknesses\": \"Adaptive attacks can reduce detection accuracy, but the method becomes more apparent and easier to filter due to increased noise levels.\",\n  \"Questions\": \"How does the proposed method compare to existing approaches in terms of computational efficiency and adaptability to new attacks?\"\n}"}
{"paper_id": "i7P2mK3x3o", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Strong empirical performance, efficient and accurate learning of optimal transport between samples, broad applications in generative models and distribution interpolation.\",\n  \"Weaknesses\": \"None mentioned in the summary.\",\n  \"Questions\": \"How does Q-flow compare to other flow-based models in terms of computational efficiency and scalability? What are the limitations of using a neural ODE framework for optimal transport?\"\n}"}
{"paper_id": "YIls9HEa52", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"Addresses significant limitations of existing rSLDS models, introduces a novel approach to semi-Markov state process and latent geometry, and demonstrates efficacy on synthetic and real neural data.\",\n  \"Weaknesses\": \"None mentioned in the summary, but the proposed model may be computationally expensive due to the PDE approach.\",\n  \"Questions\": \"How does the irSLDS model perform on other types of neural data, such as fMRI or EEG? Can the model be extended to other applications beyond neural analysis?\"\n}"}
{"paper_id": "koYsgfEwCQ", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper addresses an important gap in unsupervised learning of object-centric representations in dynamic visual scenes. The proposed method, DynaVol, outperforms existing methods and enables scene editing. The paper is well-structured and the writing is clear.\",\n  \"Weaknesses\": \"The paper does not discuss the limitations of the approach or potential future work.\",\n  \"Questions\": \"\"\n}"}
{"paper_id": "o6AN2ZoNw2", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Original approach to open-vocabulary semantic segmentation, state-of-the-art results, scalability, and simplicity of method.\",\n  \"Weaknesses\": \"None mentioned in the summary.\",\n  \"Questions\": \"How does the pseudo-mask generator compare to other methods for generating artificial masks? How does the self-training enhancement step impact the overall performance?\"\n}"}
{"paper_id": "HXZK1Z8tHa", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Effective reduction of network latency, improved trainability, and state-of-the-art performance across multiple benchmarks.\",\n  \"Weaknesses\": \"The paper could provide more detailed analysis and comparison with existing methods, and the potential limitations of the proposed approach are not thoroughly discussed.\",\n  \"Questions\": \"How does the proposed method perform on other image restoration tasks beyond the ones mentioned, and what are the potential applications of the developed model in high-level vision problems?\"\n}"}
{"paper_id": "QO3yH7X8JJ", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The paper addresses a clear gap in existing super-resolution methods, provides a novel and effective solution, and demonstrates its performance through extensive experiments.\",\n  \"Weaknesses\": \"None mentioned in the summary, but potential drawbacks could be the reliance on pre-trained models and the need for optimal noise injection.\",\n  \"Questions\": \"How does the proposed method handle cases where the pre-trained model is not suitable for the specific SR task? Is the PRF concept applicable to other tasks beyond SR?\"\n}"}
{"paper_id": "sNtDKdcI1f", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"Well-structured and clear writing, thorough analysis, and robust experiments.\",\n  \"Weaknesses\": \"The paper's focus on output length might be too narrow, and the study could have benefited from more diverse evaluation metrics.\",\n  \"Questions\": \"How might the findings be generalized to other RLHF applications? Are there any potential biases in the preference datasets used?\"\n}"}
{"paper_id": "HFtrXBfNru", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Effective solution to the problem of representation distortion in evolving graphs, excellent experimental setup, clear contributions.\",\n  \"Weaknesses\": \"The paper could be more rigorous in discussing the limitations of the proposed method and the potential risks of overfitting.\",\n  \"Questions\": \"How does the proposed method handle cases where the graph structure changes rapidly or unpredictably? Are there any plans for future work on applying SMART to other types of graph data?\"\n}"}
{"paper_id": "4u0ruVk749", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Effectively handles unobserved confounders, improves causal inference tasks, promising direction for utilizing diffusion models in complex causal inference scenarios.\",\n  \"Weaknesses\": \"Limited explanation of the limitations of existing generative models, no discussion on potential scalability issues with the proposed method.\",\n  \"Questions\": \"How does the proposed method handle scenarios with multiple unobserved confounders? Can the method be extended to other types of data beyond observational data?\"\n}"}
{"paper_id": "Pa6SiS66p0", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Original research, well-structured, clear presentation, strong experimental design, and effective comparison with state-of-the-art methods.\",\n  \"Weaknesses\": \"None significant mentioned in the summary.\",\n  \"Questions\": \"How does the proposed method handle the issue of modality imbalance in the VGGSound dataset? Are there any plans to extend the method to other datasets or tasks?\"\n}"}
{"paper_id": "rkplYfqUr0", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Robust approach to in-context learning, addresses miscalibration and brittleness, integrates contextual information for personalized classification.\",\n  \"Weaknesses\": \"None mentioned\",\n  \"Questions\": \"How does GEN-Z handle out-of-distribution examples, and how does it compare to other generative prompting frameworks?\"\n}"}
{"paper_id": "pTU2X9mUBe", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Comprehensive and diverse dataset, suitable for various tasks, and fills a significant gap in the field.\",\n  \"Weaknesses\": \"None mentioned in the summary.\",\n  \"Questions\": \"How does the dataset address potential biases in the collected data? How will the dataset be maintained and updated in the future?\"\n}"}
{"paper_id": "uhtQyRrTzY", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Original approach to generating large-scale multi-view datasets, effective in dense vision tasks, scalable method, promising results.\",\n  \"Weaknesses\": \"None mentioned, but potential for limitations in real-world data quality or applicability to specific tasks.\",\n  \"Questions\": \"How does the method handle data quality issues or variations in real-world environments?\"\n}"}
{"paper_id": "WZfatbNdLV", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Significant contribution to the field of alternative splicing, novel use of generative model architectures, accurate predictions and manipulations of RNA splicing outcomes.\",\n  \"Weaknesses\": \"Limited discussion on potential limitations and future directions, reliance on specific pretraining and fine-tuning datasets.\",\n  \"Questions\": \"How does the model generalize to non-human tissues and organisms? What are the potential therapeutic applications of this work?\"\n}"}
{"paper_id": "YkRwadXWHd", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"State-of-the-art performance on multiple datasets, effective multi-modal integration and knowledge distillation framework\",\n  \"Weaknesses\": \"High computational costs of existing dual-stream networks, reliance on video data\",\n  \"Questions\": \"How does the proposed approach compare to other multi-modal frameworks in terms of computational efficiency?\"\n}"}
{"paper_id": "q4Bim1dDzb", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Unified voxelization framework, efficient optimization, robustness across diverse datasets, favorable reconstruction quality\",\n  \"Weaknesses\": \"None mentioned\",\n  \"Questions\": \"None\"\n}"}
{"paper_id": "39cPKijBed", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Effective method to mitigate dataset bias in diffusion models, well-validated through experiments on diverse datasets, and practically applicable.\",\n  \"Weaknesses\": \"None mentioned in the summary.\",\n  \"Questions\": \"None raised in the summary.\"\n}"}
{"paper_id": "aG3EARrrd1", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"The model integrates tree decomposition and adaptive radial basis function exploration, demonstrating improved efficiency and accuracy over traditional RBF and hybrid methods. The adaptive RBF exploration and incremental method for determining RBF centers are notable strengths.\",\n  \"Weaknesses\": \"The paper does not discuss the computational complexity of the proposed model, which may be a concern for large-scale datasets. The model's performance on non-time series datasets is not evaluated.\",\n  \"Questions\": \"How does the model handle concept drift in streaming data? Can the model be extended to handle high-dimensional data?\"\n}"}
{"paper_id": "Ts95eXsPBc", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Innovative approach to incorporating spatial information into transformer models, Adaptive Memory Allocator for flexible memory management, improved performance in spatial reasoning tasks and memory efficiency.\",\n  \"Weaknesses\": \"Limited discussion on the cognitive science basis for incorporating spatial information, potential limitations of the Adaptive Memory Allocator in certain environments.\",\n  \"Questions\": \"How does the spatial information impact the transformer model's ability to generalize to new environments? What are the potential applications of the Adaptive Memory Allocator beyond episodic memory models?\"\n}"}
{"paper_id": "LfhG5znxzR", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Original approach to enhancing neural network interpretability and controllability, innovative use of vector quantization, successful application to various datasets\",\n  \"Weaknesses\": \"None mentioned in the summary\",\n  \"Questions\": \"How will this approach handle different types of neural network architectures? Can the codebook features be used to improve model robustness?\"\n}"}
{"paper_id": "bUGzjiUsIq", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"Proposes a novel framework for partially annotated multi-label classification, effectively handles challenges, demonstrates superior performance, and generalizes well across different datasets and domains.\",\n  \"Weaknesses\": \"None mentioned\",\n  \"Questions\": \"How does the proposed framework handle cases where the partial annotation is biased or incomplete? Can the framework be applied to other types of classification tasks beyond multi-label classification?\"\n}"}
{"paper_id": "uMAujpVi9m", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Proposes a novel approach to mitigate protein-ligand data scarcity, demonstrates state-of-the-art results on multiple tasks, and opens up avenues for using diverse protein structure databases for pretraining.\",\n  \"Weaknesses\": \"None mentioned in the summary\",\n  \"Questions\": \"How does ProFSA compare to other methods in terms of computational efficiency? Are the generated pseudo-complexes representative of real-world protein-ligand interactions?\"\n}"}
{"paper_id": "uqxBTcWRnj", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Proposes a novel transitional representation system, introduces a new framework (TDL) for learning compositional predicates, and provides a clear explanation of its benefits.\",\n  \"Weaknesses\": \"The paper assumes a relatively narrow focus on visual object recognition, which might limit its applicability to other domains.\",\n  \"Questions\": \"How does the TDL framework generalize to more complex and abstract symbolic representations?\"\n}"}
{"paper_id": "o4CLLlIaaH", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Generalizable neural radiance fields, potential for scene interaction, enhances rendering quality and geometry correctness.\",\n  \"Weaknesses\": \"Limited discussion on the impact of the proposed method on real-world applications.\",\n  \"Questions\": \"How does the proposed method compare to other state-of-the-art methods in terms of computational efficiency?\"\n}"}
{"paper_id": "KNzL1nglNB", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Effective solution to the problem of limited labeled data, maintains prediction diversity and reduces dominance of large categories, theoretical analysis is well-done, and empirical evaluation is extensive.\",\n  \"Weaknesses\": \"None mentioned in the summary\",\n  \"Questions\": \"How does LRM perform in extreme cases of limited labeled data? Can it be applied to other areas beyond SSL, UDA, and SHDA?\"\n}"}
{"paper_id": "pvhyBB86Bt", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Well-structured paper with a clear hypothesis, thorough analysis, and sound conclusions. The use of recent results from random matrix theory adds rigor to the study.\",\n  \"Weaknesses\": \"The paper assumes prior knowledge of neural networks and random matrix theory, which might limit its accessibility to a broader audience.\",\n  \"Questions\": \"How generalizable are the findings to other types of neural networks beyond MLPs? Are there any potential implications for practical applications of the proposed theorem?\"\n}"}
{"paper_id": "Mylk5iamJC", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Unified approach to address both CSR and OSR problems, effective combination of generative and discriminative modeling, superior performance to discriminative counterparts and existing specialized methods.\",\n  \"Weaknesses\": \"None mentioned in the summary.\",\n  \"Questions\": \"How well does the proposed method generalize to other domains and tasks beyond image recognition and segmentation?\"\n}"}
{"paper_id": "MsOcVFzv8D", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Provides a robust theoretical foundation for multi-domain text classification, outperforms state-of-the-art methods, and has a clear and well-explained approach.\",\n  \"Weaknesses\": \"The paper assumes a specific type of data (text classification) and the generalizability to other tasks is unclear.\",\n  \"Questions\": \"How does the proposed method handle cases with limited labeled data in the target domain?\"\n}"}
{"paper_id": "fBlHaSGKNg", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Original and well-executed contribution to Semi-Supervised Learning, effective sample selection method, and significant improvement in SSL performance.\",\n  \"Weaknesses\": \"None mentioned in the paper summary.\",\n  \"Questions\": \"How does the approach generalize to other domains and datasets beyond CIFAR-10, CIFAR-100, and SVHN?\"\n}"}
{"paper_id": "HwQ8NVvdmm", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"Effective integration of Hadamard transforms with quantization, significant improvements in computational efficiency and energy consumption, minimal accuracy loss compared to floating-point baselines.\",\n  \"Weaknesses\": \"Limited discussion on the potential impact of quantization noise and its effects on model performance.\",\n  \"Questions\": \"How does the proposed method compare to other quantization techniques in the context of continual learning? What are the implications of the observed minimal accuracy loss on real-world applications?\"\n}"}
{"paper_id": "cZo6pDtDZr", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"novel algorithms, near-optimal sample complexity, efficient sequential testing, suitable for unknown epsilon\",\n  \"Weaknesses\": \"none mentioned\",\n  \"Questions\": \"\"\n}"}
{"paper_id": "F7QnIKlC1N", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Well-structured paper with clear methodology, effective use of deep learning techniques, and promising results on Molecule3D and QM9 datasets.\",\n  \"Weaknesses\": \"Some sections could benefit from more details on the limitations and potential biases of the proposed method.\",\n  \"Questions\": \"How does the MSRSA mechanism compare to other attention mechanisms in graph neural networks? Can the authors provide more information on the computational resources required for training and inference?\"\n}"}
{"paper_id": "8JCn0kmS8W", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Effective use of Large Language Models, successful synthesis of audio content, potential in crafting engaging storytelling audio, human-machine co-creation capability.\",\n  \"Weaknesses\": \"Efficiency and artificial composition remain as potential limitations.\",\n  \"Questions\": \"How does WavJourney compare to other audio generation systems? What are the potential applications of WavJourney in real-world scenarios?\"\n}"}
{"paper_id": "XbLffB0T2z", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Transferable Poisoning achieves significantly improved transferability of poisoning perturbations across different learning paradigms, reducing victim model test accuracy by over 27% compared to existing methods.\",\n  \"Weaknesses\": \"The paper assumes that the adversary and the victim use the same learning algorithm, which might be a limitation of the existing strategies.\",\n  \"Questions\": \"How does the proposed approach compare to other transferable attacks? Are there any potential countermeasures to mitigate the effects of Transferable Poisoning?\"\n}"}
{"paper_id": "4y3GDTFv70", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"Comprehensive theoretical framework explaining emergent abilities in LLMs, clear and well-supported hypothesis, effective use of simulation experiments.\",\n  \"Weaknesses\": \"Lack of real-world data to validate the theoretical framework, may be overly complex for some readers.\",\n  \"Questions\": \"How does the theoretical framework generalize to other language models and tasks? Can the framework be applied to other areas of NLP?\"\n}"}
{"paper_id": "ODSgo2m8aE", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Original and innovative approach to stabilizing GNNs, effective empirical validation, and practical implementation through JacoLip.\",\n  \"Weaknesses\": \"The paper could benefit from more detailed theoretical analysis of the proposed method and potential applications beyond node classification and link prediction tasks.\",\n  \"Questions\": \"How does the proposed method handle complex graph structures and large-scale graph data?\"\n}"}
{"paper_id": "RlfD5cE1ep", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"The paper presents a clear and well-structured argument, the analysis of non-contrastive dynamics is thorough, and the findings contribute to the understanding of self-supervised learning.\",\n  \"Weaknesses\": \"The paper assumes a high-dimensional limit, which might not be representative of all scenarios, and the numerical simulations could benefit from more detailed results.\",\n  \"Questions\": \"How generalizable are the findings to other non-contrastive methods, and what are the implications of the sixth-order dynamics on the stability of self-supervised learning?\"\n}"}
{"paper_id": "18TfucMNTr", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"Effective approach to addressing saddle points, improves training outcomes for deep learning models, demonstrates stability and reduced variability.\",\n  \"Weaknesses\": \"Limited discussion on potential drawbacks or edge cases of the proposed method.\",\n  \"Questions\": \"How does the regularization term impact the optimization process in complex scenarios?\"\n}"}
{"paper_id": "JGTC6WgO4T", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"Original and well-motivated approach to multi-source black-box domain adaptation, efficient refinement of pseudo labels, competitive performance on benchmark datasets.\",\n  \"Weaknesses\": \"The paper assumes that the source and target domains have adequate correlation, which might not always be the case in real-world scenarios.\",\n  \"Questions\": \"How does the proposed framework perform in cases where the source and target domains have low correlation?\"\n}"}
{"paper_id": "fQHb1uZzl7", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Effectively unifying feature and cost aggregation, robust and efficient estimation of correspondences, improved performance in dense matching tasks.\",\n  \"Weaknesses\": \"None mentioned in the summary.\",\n  \"Questions\": \"How does the UFC method compare to other state-of-the-art methods in terms of computational efficiency? Can the proposed method be applied to other computer vision tasks beyond dense matching?\"\n}"}
{"paper_id": "YjG29CIOP7", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Proposes a robust approach to address task-distribution shift and corruption, demonstrates superior performance on multiple datasets, and enhances the applicability of data-free meta-learning.\",\n  \"Weaknesses\": \"None mentioned in the summary.\",\n  \"Questions\": \"How does the proposed approach compare to other existing methods in terms of computational efficiency? How does the reliability score in ROSY handle cases where multiple models have similar scores?\"\n}"}
{"paper_id": "DmDpu3r8iU", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"Clearly identifies the need to evaluate both 'know what' and 'know why' elements of value recognition and understanding in LLMs, proposes a novel framework for assessment, and highlights the limitations of existing methods.\",\n  \"Weaknesses\": \"The study's focus on GPT-4 and the Schwartz Value Survey might limit its generalizability, and the inherent understanding of risky values like 'Power' is insufficient in LLMs.\",\n  \"Questions\": \"How can the VUM framework be applied to other types of values and contexts? Can the 'know why' capabilities of LLMs be further developed to improve their value alignment?\"\n}"}
{"paper_id": "kXHEBK9uAY", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Effective combination of hierarchical planning and diffusion-based modeling, efficient sampling and gradient propagation, improved generalization for long-horizon tasks\",\n  \"Weaknesses\": \"Potential computational inefficiency of the Sparse Diffuser, limited evaluation of out-of-distribution tasks\",\n  \"Questions\": \"How does the Hierarchical Diffuser handle complex and dynamic environments? Can the method be further optimized for real-time planning?\"\n}"}
{"paper_id": "i7LCsDMcZ4", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Accurate saliency maps and class activation maps generation for SNNs, effective data augmentation, improved generalization ability and performance of SNNs.\",\n  \"Weaknesses\": \"Limited to classification tasks, potential future work in expanding methods to other neural network tasks.\",\n  \"Questions\": \"How do SLTRP and SLRP compare to existing relevance propagation methods? How can the approach be adapted for other event-based applications?\"\n}"}
{"paper_id": "jHdz0CIS2y", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Effective approach to voice conversion, significant improvements over baseline models, applicability across languages and diverse tasks.\",\n  \"Weaknesses\": \"None mentioned in the summary.\",\n  \"Questions\": \"How do the results generalize to other languages and speaker demographics?\"\n}"}
{"paper_id": "NkYCuGM7E2", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Improves traffic flow efficiency, decreases safety penalties, and adapts better to complex scenarios; demonstrates potential of LLMs in AD systems.\",\n  \"Weaknesses\": \"None mentioned in the summary.\",\n  \"Questions\": \"How well would the proposed system perform in edge cases or scenarios with limited data availability?\"\n}"}
{"paper_id": "RtDok9eS3s", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Original and thorough investigation into transformer simplification, well-structured and easy to follow, significant contribution to the field of deep learning.\",\n  \"Weaknesses\": \"Some minor points might have been explored more thoroughly, e.g. how the simplifications generalize to other architectures or larger models.\",\n  \"Questions\": \"How do the simplifications impact the interpretability and robustness of the models?\"\n}"}
{"paper_id": "YCPDFfmkFr", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Original and innovative approach to handling infeasible QP layers, practical implementation, promising experimental results\",\n  \"Weaknesses\": \"None mentioned\",\n  \"Questions\": \"How does the approach generalize to more complex optimization problems, what are the computational costs of the proposed method?\"\n}"}
{"paper_id": "ueTdErd5Ib", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"Proposes a novel approach to contextual stochastic optimization, provides theoretical guarantees, and demonstrates improved robustness and reduced worst-case costs.\",\n  \"Weaknesses\": \"The paper's focus on robustness against worst-case scenarios might limit its applicability in certain contexts where average cost minimization is sufficient.\",\n  \"Questions\": \"How does the proposed method handle non-linear relationships between predictions and decisions? Can the discretization framework be applied to other optimization problems?\"\n}"}
{"paper_id": "sDmjlpphdB", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Original approach to prompt optimization, effective division of problem space, successful experiments on benchmark tasks\",\n  \"Weaknesses\": \"Limited discussion on potential limitations and future work\",\n  \"Questions\": \"How does MoP perform on tasks with very few or no demonstrations? Are there any plans to extend MoP to other domains?\"\n}"}
{"paper_id": "Mdk7YP52V3", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"Well-structured paper with clear hypothesis, method, and conclusion. Provides a comprehensive theoretical framework to explain the failure modes of overparameterized heteroskedastic regression models.\",\n  \"Weaknesses\": \"The paper assumes a strong background in statistical physics and field theory, which may limit its accessibility to a broader audience.\",\n  \"Questions\": \"How do the findings of this study translate to more complex machine learning architectures, and how can the proposed regularization approach be adapted for practical applications?\"\n}"}
{"paper_id": "YGTSLDAPqb", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"Addressing the issue of out-of-distribution (OOD) performance in machine learning models, proposing a novel 'Connect Later' framework that combines self-supervised pretraining and fine-tuning with targeted augmentations.\",\n  \"Weaknesses\": \"The paper does not provide a comprehensive comparison with existing self-supervised pretraining methods, and the evaluation on multiple datasets might be considered limited.\",\n  \"Questions\": \"How do the targeted augmentations used in the 'Connect Later' framework compare to other methods, and are the improvements in OOD performance consistent across different tasks and datasets?\"\n}"}
{"paper_id": "JTcaziw7G1", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"Proposes a novel approach to private information retrieval, uses multi-party computation to ensure security, and demonstrates efficacy through experiments.\",\n  \"Weaknesses\": \"Computational costs may require further optimization.\",\n  \"Questions\": \"How scalable is the proposed method in large-scale settings? Can the method be adapted to other types of data?\"\n}"}
{"paper_id": "lEkFq4RUCX", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Proposes a new metric, Directional Distance Field (DDF), for measuring differences between 3D point clouds, which is more efficient and accurate than existing metrics like EMD and CD. DDF focuses on underlying surface differences rather than point-wise correspondences, enhancing 3D point cloud processing frameworks.\",\n  \"Weaknesses\": \"The paper assumes that the reference points induce local surface geometry of 3D point clouds, but it is unclear how this is achieved. The evaluation of DDF is limited to a few datasets and tasks.\",\n  \"Questions\": \"How do the reference points induce local surface geometry of 3D point clouds? What are the limitations of the DDF metric in other scenarios, such as non-rigid registration or large-scale point clouds?\"\n}"}
{"paper_id": "Fq8tKtjACC", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"Breaks the existing scale-performance relationship, achieves high performance with smaller models, leveraging high-quality data.\",\n  \"Weaknesses\": \"Model's specialization in Python, sensitivity to prompt variations.\",\n  \"Questions\": \"How can the limitations of the model be fully addressed with further refinements in data and methods?\"\n}"}
{"paper_id": "8fQlGQkj0S", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 5,\n  \"Strengths\": \"Provides a new theoretical framework distinguishing between in-context task learning and retrieval modes, explores the impact of pre-training distributions on ICL, and derives risk upper bounds for task retrieval and learning.\",\n  \"Weaknesses\": \"The analysis is limited to in-context prediction using a Bayes-optimal next-token predictor, and the generative models for pre-training data and in-context samples may oversimplify the complexity of real-world data.\",\n  \"Questions\": \"How do the results generalize to other types of language models or tasks? What are the implications of the findings for the design of more efficient in-context learning algorithms?\"\n}"}
{"paper_id": "hRos9WldRK", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Effective method for handling high levels of noise, eliminates dependency on clean validation set, generalizes well to other tasks\",\n  \"Weaknesses\": \"None mentioned in the summary\",\n  \"Questions\": \"How well does the method perform in real-world scenarios with diverse noise types?\"\n}"}
{"paper_id": "ttRSBZiCiu", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Efficient and flexible model, competitive performance in generating high-fidelity audio, effective alternative to existing methods in speech synthesis\",\n  \"Weaknesses\": \"None mentioned in the paper summary\",\n  \"Questions\": \"How does WaveFluid compare to other recent advancements in speech synthesis, such as diffusion probabilistic models and GANs? What are the potential limitations of using adversarial training in WaveFluid?\"\n}"}
{"paper_id": "WYsLU5TEEo", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 8,\n  \"Confidence\": 4,\n  \"Strengths\": \"Effective combination of interpretability and adversarial robustness, competitive IoU scores, and practical advances in image classification tasks.\",\n  \"Weaknesses\": \"Limited discussion on the potential drawbacks of the proposed framework.\",\n  \"Questions\": \"How does the framework handle the trade-off between interpretability and adversarial robustness? What are the potential applications of the framework in real-world scenarios?\"\n}"}
{"paper_id": "KKZaj2QS3G", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"The proposed method CoInception is effective in handling noise-afflicted time series tasks, achieving state-of-the-art performance across multiple benchmarks. The use of a noise-resilient sampling strategy and a scalable encoder design is a significant contribution to the field.\",\n  \"Weaknesses\": \"None explicitly mentioned in the summary.\",\n  \"Questions\": \"\"\n}"}
{"paper_id": "LWDRiFzbHQ", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Proposes a novel method to address the problem of spurious model responses in OOD generalization assessment, introduces Vicinal Risk Proxy (VRP) which incorporates neighboring samples' responses into the risk proxy calculation, shows consistent improvements over baselines across multiple datasets and test scenarios.\",\n  \"Weaknesses\": \"The method's effectiveness relies on the similarity-weighted sum approach which may not work well for all datasets or test scenarios.\",\n  \"Questions\": \"How does the Vicinal Risk Proxy perform on extremely OOD test sets? Are there any scenarios where the VRP might not improve the generalization assessment?\"\n}"}
{"paper_id": "Yp01vcQSNl", "review": "{\n  \"Soundness\": 4,\n  \"Presentation\": 4,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Originality of the idea, clarity of the writing, effectiveness of the proposed method, thoroughness of the experiments.\",\n  \"Weaknesses\": \"None mentioned in the summary.\",\n  \"Questions\": \"How does the DiGT method perform on very large directed graphs? Are there any plans to make the code and datasets publicly available?\"\n}"}
{"paper_id": "LndMyiBl3n", "review": "{\n  \"Soundness\": 3,\n  \"Presentation\": 3,\n  \"Contribution\": 4,\n  \"Rating\": 9,\n  \"Confidence\": 5,\n  \"Strengths\": \"Effective attack method for GNNs, scalable and adaptable, strong performance in degrading GNNs, no reliance on additional information.\",\n  \"Weaknesses\": \"Limited discussion on potential limitations and future work, assumes knowledge of node features and graph structure.\",\n  \"Questions\": \"How does SheAttack compare to other attack methods in terms of efficiency and effectiveness? Can the authors provide more details on the implementation and computation time?\"\n}"}
