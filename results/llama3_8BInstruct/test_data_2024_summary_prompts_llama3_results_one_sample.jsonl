{"paper_id": "Yp01vcQSNl", "review": "System message: You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text.\nUser instruction: Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Directed graphs are crucial for representing various real-world relationships such as social networks and paper citations. Traditionally, graph neural networks (GNNs) have been applied to these datasets to learn node and edge encodings. However, conventional graph transformers, which can potentially outperform GNNs due to their flexible attention mechanism, often fail to incorporate edge directionality effectively. Existing methods frequently rely on the static adjacency matrix or local computations, which do not fully exploit the potential of dynamic attentional relations that can be developed with transformers.\nGap: Current graph transformer architectures largely ignore or inadequately handle edge directionality, undermining their applicability to directed graphs. The core limitation is their reliance on structural assumptions like static adjacency matrices, which restrict their ability to leverage edge directions flexibly in learning tasks.\nHypothesis: The main hypothesis is that a graph transformer architecture that explicitly accounts for edge directionality using dual encodings and dynamic attention mechanisms can significantly enhance performance on learning tasks for directed graphs.\nMethod: The proposed method, Directed Graph Transformer (DiGT), introduces dual encodings representing each node's source and target role in directed edges. These are learned dynamically using a multi-head directional attention mechanism without relying on explicit graph structure. The DiGT leverages a novel attention matrix interpretable as a latent adjacency matrix, incorporating edge channels as a bias and focusing on k-hop neighborhood localization. DiGT also includes techniques to mask attention matrices based on k-hops and combines multiple heads to achieve final predictions. Additionally, the paper explores alternative ways to incorporate directionality into existing graph transformer architectures.\nConclusion: DiGT outperforms state-of-the-art models in graph learning when directionality is a key component, demonstrating that capturing the directional aspect of edges is crucial for certain directed graph datasets. The experiments on new directed graph datasets designed specifically where edge directionality is important further validate the effectiveness and promise of DiGT.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- REQUIRED FORMAT ---\nPlease return the answer in the following JSON format:\n{\n  \"Soundness\": <integer>,\n  \"Presentation\": <integer>,\n  \"Contribution\": <integer>,\n  \"Rating\": <integer>,\n  \"Confidence\": <integer>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\n--- DEFAULT VALUES ---\nIf you cannot determine the value for any field, please use the following default values:\n- Soundness: 5\n- Presentation: 5\n- Contribution: 5\n- Rating: 5\n- Confidence: 5\n- Strengths: \"Not clear\"\n- Weaknesses: \"Not clear\"\n- Questions: \"Not clear\"\n\n--- YOUR ANSWER ---\n{\n  \"Soundness\": 8,\n  \"Presentation\": 9,\n  \"Contribution\": 9,\n  \"Rating\": 9,\n  \"Confidence\": 9,\n  \"Strengths\": \"The authors proposed a novel graph transformer architecture, DiGT, that effectively captures edge directionality using dual encodings and dynamic attention mechanisms. The experiments demonstrate the effectiveness of DiGT on directed graph datasets.\",\n  \"Weaknesses\": \"The paper relies"}
{"paper_id": "LndMyiBl3n", "review": "System message: You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text.\nUser instruction: Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Graph Neural Networks (GNNs) have become popular across various applications. Despite their success, GNNs are vulnerable to adversarial attacks that involve structural modifications, i.e., adding or deleting edges. This vulnerability has led to the development of different types of attacks, categorized based on the attacker's knowledge (white-box, grey-box, black-box). The most strict setting is the restricted black-box attack (RBA), where attackers know only the node features and graph structure, not labels or model parameters.\nGap: Most previous studies on graph adversarial attacks assume conditions like low-rank node embeddings or graph homophily, which limits their applicability, especially for heterophilic graphs. Heterophilic settings, where nodes of different classes are more likely connected, pose a unique challenge for existing models since they often rely on the homophily assumption.\nHypothesis: The authors propose that evaluating the quality of graphs using a modified silhouette score (MSS) can effectively guide attacks under the RBA setting. MSS aims to serve as a more generalizable approach across graphs with varying levels of homophily and heterophily.\nMethod: The authors introduce SheAttack, guided by a Modified Silhouette Score (MSS) that evaluates the quality of node embeddings within graphs. MSS is calculated based on intra-class and inter-class distances, generalized by replacing the minimal inter-class distance with an average, thus widening the objective's scope. SheAttack employs Greedy Randomized Block Coordinate Descent to optimize edge perturbations guided by MSS, providing scalable computations and effectiveness across different graph settings without prior label or model knowledge.\nConclusion: SheAttack demonstrates strong performance in degrading GNNs across homophilic and heterophilic graphs, comparable to white-box attacks without relying on additional split information. It is efficient and scalable, achieving success without prior knowledge of node labels or victim models, and adapts to both poisoned and evasion settings effectively.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- YOUR OUTPUT FORMAT ---\nThe output should be in the following JSON format:\n{\n\"Soundness\": <integer>,\n\"Presentation\": <integer>,\n\"Contribution\": <integer>,\n\"Rating\": <integer>,\n\"Confidence\": <integer>,\n\"Strengths\": \"<string>\",\n\"Weaknesses\": \"<string>\",\n\"Questions\": \"<string>\"\n}\n\n--- YOUR ANSWER ---\n{\n\"Soundness\": 8,\n\"Presentation\": 7,\n\"Contribution\": 8,\n\"Rating\": 7,\n\"Confidence\": 8,\n\"Strengths\": \"SheAttack demonstrates strong performance in degrading GNNs across homophilic and heterophilic graphs, comparable to white-box attacks without relying on additional split information. It is efficient and scalable, achieving success without prior knowledge of node labels or victim models, and adapts to both poisoned and evasion settings effectively.\",\n\"Weaknesses\": \"The study is limited to graph adversarial attacks, and its applicability to other areas is unclear. The performance of SheAttack might be impacted by the choice of the modified silhouette score (MSS).\",\n\"Questions\": \"How does the performance of SheAttack compare to existing methods under different graph settings"}
