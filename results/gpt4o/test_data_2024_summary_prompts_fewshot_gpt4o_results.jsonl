{"paper_id": "B0OwtVEejJ", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel integration of gradient-based and weight-entangled NAS methods, offering significant improvements in terms of memory efficiency and search effectiveness. The systematic approach to adapting gradient-based optimizers to weight-entangled spaces is innovative and well-motivated. The empirical evaluation across diverse search spaces bolsters the credibility and applicability of the proposed method.", "Weaknesses": "While the paper presents a compelling method, the presentation of results and comparisons could be enhanced for clarity and accessibility. Detailed breakdowns of the experiments and configurations are somewhat lacking, which may hinder reproducibility. Additionally, certain technical aspects of the TangleNAS framework, such as the handling of edge cases in weight-entanglement, could be elaborated further.", "Questions": "What are the specific technical challenges faced when adapting gradient-based NAS methods to weight-entangled spaces, and how does TangleNAS address them? How does the proposed method scale when further extending it to more complex search architectures, such as large-scale transformer models?"}}
{"paper_id": "ZlEtXIxl3q", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. Novel application of contrastive loss functions for learning fitness functions in protein engineering, which addresses challenges associated with nonlinearity in existing methods.\n2. Demonstrates improved recovery and ranking of fitness functions in simulations and real-world benchmark tasks.\n3. Provides a more data-efficient and robust method under noisy conditions compared to traditional approaches.", "Weaknesses": "1. Application and validation appear to be limited to specific datasets and tasks, potentially limiting generalization across diverse biological scenarios.\n2. More extensive comparison with a broader range of existing methods would strengthen the validation of claimed advantages.", "Questions": "1. How well does the proposed method perform across different types of nonlinear transformations?\n2. Are there any particular limitations or computational costs associated with implementing contrastive loss functions in practice for this domain?"}}
{"paper_id": "6yJuDK1DsK", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 6, "Confidence": 4, "Strengths": "The approach introduces a lightweight design for lifelong test-time adaptation, making it well-suited for scenarios with limited computational resources. The model does not require access to source data during adaptation, increasing its practicality in real-world settings.", "Weaknesses": "The method's applicability might be limited to specific types of neural networks or setups. There is also potential room for improvement in the clarity and detail of the methodological description, as well as in the evaluation of the proposed solution across more diverse datasets.", "Questions": "How does the method perform on architectures other than convolutional networks? Are there any limitations concerning the type of pre-trained models to which these adapters can be applied?"}}
{"paper_id": "lt6xKGGWov", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The introduction of MINERVA, a feature selection filter using MINE, highlights an innovative approach to capturing complex feature-target dependencies.\nThe method shows promise in providing more accurate feature selections in scenarios with non-linear dependencies as demonstrated on synthetic benchmarks.\nMINERVA effectively combines neural networks with information-theoretic concepts to improve feature selection processes.", "Weaknesses": "The paper primarily demonstrates the method's effectiveness on synthetic datasets, which may raise concerns about its applicability to real-world datasets with diverse characteristics and noise.\nThe computational complexity and scalability of the method for high-dimensional datasets are not thoroughly discussed, which could limit its practical use in real-world applications.\nThe paper does not provide a comparison to a diverse enough set of existing methods to fully establish the superiority of their approach.", "Questions": "How does MINERVA perform on real-world datasets with high-dimensional features and noise?\nWhat is the computational overhead associated with running MINERVA compared to traditional methods?\nCan the method be extended or adapted to evaluate time-series or sequential data where dependencies are temporal?"}}
{"paper_id": "Bvrc6kobWd", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 2, "Rating": 5, "Confidence": 3, "Strengths": "The analysis of the role of margins from a gradient perspective is a novel contribution, offering insights into the elemental effects of margins in contrastive learning. The experimental evaluation on datasets like CIFAR-10, CIFAR-100, STL-10, and TinyImageNet provides empirical support for the theoretical findings.", "Weaknesses": "The theoretical analysis could be more rigorous and detailed. The explanation of the impact of margins on gradients may have been oversimplified, and the connection to practical improvements in SSL needs to be stronger to demonstrate the significance of the contribution. Additionally, some methodological details might lack clarity, affecting the study's reproducibility.", "Questions": "How would this gradient-based analysis of margins generalize across different self-supervised learning tasks beyond the ones tested in the paper? Is there any potential limitation when applying this approach to models with different architectures or training regimes?"}}
{"paper_id": "FJjHQS2DyE", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper addresses the underexplored issue of label distribution shift in unsupervised domain adaptation, providing a novel methodological approach that is supported by empirical results.\n2. The proposed CASA framework presents a theoretical risk bound that is novel and relevant for the domain adaptation community.\n3. The method demonstrates superior performance across multiple benchmark datasets compared to state-of-the-art approaches.", "Weaknesses": "1. The presentation of the theoretical contributions could be further clarified to enhance accessibility for a broader audience, especially those less familiar with the mathematical nuances.\n2. Although empirical results are provided, more detailed ablation studies could strengthen the understanding of the contribution of each component of the CASA framework.\n3. Limited discussion on potential limitations or future work regarding the extension of CASA in more complex scenarios like universal domain adaptation.", "Questions": "1. Could CASA be extended to work effectively in scenarios with multiple domains, such as in domain generalization settings? If so, how?\n2. How sensitive is CASA to the choice of hyperparameters such as those involved in minimizing the target conditional entropy?\n3. What are the computational costs associated with CASA compared to other state-of-the-art methods?"}}
{"paper_id": "xelrLobW0n", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The TRACE benchmark is comprehensive, covering a wide range of tasks including domain-specific tasks, multilingual capabilities, and mathematical reasoning. This diversity is a notable strength that challenges LLMs in various dimensions.\n2. The introduction of new evaluation metrics, such as 'General Ability Delta,' 'Instruction Following Delta,' and 'Safety Delta,' offers a nuanced approach to evaluating the impact of continual learning methods on LLMs.\n3. The paper demonstrates a thorough evaluation of different sequential fine-tuning techniques and introduces a novel RCL approach that highlights the potential benefits of reasoning-augmented learning strategies.", "Weaknesses": "1. While TRACE benchmark integrates various challenging tasks, some might argue that it doesn't inherently solve the issue of existing continual learning benchmarks possibly being simplistic, as it draws heavily on existing datasets without proposing new tasks. \n2. The paper may have insufficient experimental comparisons with a broader set of continual learning methods, potentially limiting the claims about the RCL approach. \n3. The text relies heavily on theoretical discussions about TRACE and RCL without substantial empirical evidence for their benefits, particularly the reasoning augmentation part.", "Questions": "1. How does the RCL approach compare with other state-of-the-art continual learning methods beyond those nascently explored in the paper?\n2. Could the TRACE benchmark be augmented or adapted to better generalize to other types of models or tasks beyond LLMs?\n3. Is the reasoning augmentation approach applicable in real-time, or does it require a substantial computational overhead?"}}
{"paper_id": "bYQkOPvgDw", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper addresses an important issue of label noise in GNNs, which is often overlooked, especially in heterophilous graphs and high noise scenarios.\n2. The proposed PRGNN framework is novel and does not rely on the common label smoothness assumption, potentially enhancing robustness.\n3. PRGNN demonstrates superior performance across various noise types and levels, showcasing its practical utility and effectiveness.", "Weaknesses": "1. The complexity of the probabilistic graphical model-based framework might hinder its applicability or understanding by practitioners not well-versed in graphical models.\n2. The assumption of clean label reconstruction via Bayesian networks needs further empirical validation across diverse graph domains.\n3. Details on hyperparameter selection or sensitivity analysis in PRGNN were not emphasized, which might impact reproducibility or adaptation to other datasets.", "Questions": "1. How does PRGNN perform in real-world applications where true clean labels are not available, and noise distribution might be unknown or non-standard?\n2. What are the computational overheads of using the proposed variational inference approach compared to traditional GNN methods?\n3. Can the authors elaborate on the limitations or failure cases of PRGNN, particularly in extremely high-dimensional graph settings?"}}
{"paper_id": "sKPzAXoylB", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The method tackles the challenging problem of catastrophic forgetting and loss of plasticity in continual learning with an innovative approach, combining utility-based perturbations and gradient descent effectively. The framework is versatile, being applicable to both representation learning and reinforcement learning tasks, which increases its potential impact. The paper successfully demonstrates improved performance over state-of-the-art methods across various datasets, suggesting that UPGD is a robust and effective solution for continual learning environments.", "Weaknesses": "The paper could benefit from a deeper theoretical analysis of the UPGD method, particularly concerning the underlying assumptions and limitations. While the empirical results are promising, the perturbation aspect of the algorithm isn't thoroughly justified or explored, potentially leaving some readers questioning its necessity or optimality. Additionally, the current evaluations are primarily on standard benchmarks, which might not capture the full range of complexities present in real-world streaming data.", "Questions": "How does the utility-based perturbation impact resource requirements compared to traditional methods in terms of computational overhead? Additionally, have the authors considered evaluating their method in more complex, real-world scenarios beyond benchmark datasets?"}}
{"paper_id": "H8Qg1IIMaR", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper highlights a critical vulnerability in LLMs and VLLMs when handling multiple-choice question answering tasks, emphasizing an aspect often overlooked in model evaluations.\n2. The approach of systematically testing permutations across various LLMs and datasets provides a compelling demonstration of the systemic nature of the issue.", "Weaknesses": "1. The analysis primarily focuses on simple permutation attacks; a more comprehensive exploration of other adversarial tactics could provide deeper insights into the robustness of these models.\n2. The paper could benefit from a deeper investigation into why current mitigation strategies are ineffective and potential alternative solutions.", "Questions": "1. What are the specific characteristics of certain datasets or models that contribute to higher vulnerability in permutation attacks?\n2. Is there potential for training or fine-tuning models specifically to enhance resilience to such adversarial attacks? If so, what initial approaches might be explored?"}}
{"paper_id": "tth2qXY7RU", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The proposed SuFP approach addresses a significant gap in existing quantization methods by effectively handling both dense near-zero data and outliers. This method combines multi-region quantization with scalable bias and efficient hardware implementation, demonstrating significant improvements in accuracy, computational capability, and energy efficiency. The evaluation across various domains adds robustness and credibility to the findings.", "Weaknesses": "The paper could benefit from a more detailed analysis of how the proposed method specifically outperforms existing methods like MSFP and BSFP. It's not entirely clear how the multi-region quantization and scalable bias are implemented, and more technical details would help in understanding the complexity and scalability of the approach. The paper relies on extensive comparisons with traditional FP32, FP16, but lacks sufficient comparison with more modern quantization techniques.", "Questions": "How does the SuFP method specifically adapt to different domains such as vision, language, and generative models? Is there any specific tuning required for each domain, or does the method work universally well across different tasks? Could the authors provide more quantitative comparisons with other modern quantization techniques besides FP32 and FP16?"}}
{"paper_id": "fjf3YenThE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The proposed variance-reduced ZO hard-thresholding algorithm effectively addresses limitations in the existing SZOHT algorithm. It broadens the applicability and improves convergence rates, particularly in the context of high-dimensional machine learning tasks involving sparse learning solutions.", "Weaknesses": "The theoretical analysis of the algorithm under certain assumptions like restricted strong smoothness and convexity might limit general applicability. The practical impact could be contingent on these constraints. Furthermore, the presentation could be improved to make the methodology more accessible.", "Questions": "What are the specific conditions or scenarios where this algorithm provides a significant advantage over existing methods? Are there examples where the assumptions of restricted strong smoothness and convexity do not hold, and if so, how does the algorithm perform under those conditions?"}}
{"paper_id": "H9DYMIpz9c", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The method, FARZI, presents an innovative approach to data distillation tailored specifically for autoregressive tasks, efficiently summarizing large datasets while retaining performance. 2. The paper is well-organized with clear explanations and demonstrates the effectiveness of the method across multiple tasks, highlighting substantial reductions in data size with little to no performance loss.", "Weaknesses": "1. The practicality and scalability of FARZI for extremely large models and vocabulary sizes are not entirely clear. The paper could benefit from a deeper exploration of computation time savings and resource efficiency compared to standard training procedures. 2. More in-depth comparisons with other state-of-the-art data summarization techniques could strengthen the argument for FARZI's relative benefits.", "Questions": "1. How does FARZI handle very large vocabulary sizes in autoregressive tasks, and are there any limitations observed in this context? 2. Can the authors provide further insights into the runtime and computational cost benefits of using FARZI compared to full-scale training in practical, large-scale deployments? 3. Is there potential for applying FARZI to other domains beyond language modeling and sequential recommendation, such as vision tasks with sequential dependencies?"}}
{"paper_id": "4kLVvIh8cp", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper makes a compelling contribution by addressing a gap in offline RL with nonlinear function approximation, providing new instance-dependent regret guarantees. It demonstrates both theoretical rigor and practical relevance by proving the statistical optimality of the proposed method in this complex domain.", "Weaknesses": "The algorithm relies on certain uniform data coverage assumptions, which may not align well with practical applications where data coverage is often uneven. The absence of a matching lower bound for general function approximation raises questions about the defined optimality conditions and the real-world applicability of theoretical guarantees.", "Questions": "How does the PNLSVI algorithm compare in performance to other offline RL algorithms when applied to practical datasets with non-uniform coverage?\nWhat types of real-world scenarios or datasets is this proposed algorithm most applicable to?"}}
{"paper_id": "CSpWgKo0ID", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The study effectively highlights the application of behavioral game theory to understand the interactive behaviors of LLMs. It showcases thorough experimentation across a variety of game scenarios, offering valuable insights into how LLMs perform in strategic interactions, which is crucial as these models see more widespread usage in society.", "Weaknesses": "While the paper presents a novel approach to evaluating LLMs within game-theoretical contexts, its notion of 'coordination failure' could benefit from a more rigorous evaluation. The effectiveness of using behavioral game prompts may vary significantly based on the context and framing, which the study may not have fully explored. There also seems to be an assumption that LLMs operating in these experimental settings directly translate to real-world interactions without substantial adaptation or training, which may not be the case.", "Questions": "1. How does the choice of behavioral prompts and their framing influence the LLM's performance across different game scenarios? \n2. Can the coordination and prediction capabilities of LLMs be substantially improved through fine-tuning or reinforcement learning techniques beyond the framework of behavioral prompts? \n3. What implications do the findings have for integrating LLMs into more complex, multi-agent systems where the nature of interaction and coordination is even more intricate than the games studied here?"}}
{"paper_id": "gwbQ2YwLhD", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The study extends the understanding of scale effects in structure learning algorithms, providing theoretical insights and empirical evidence. It addresses a gap in higher-dimensional and non-linear settings, highlighting the potential issues of existing loss functions in identifying correct DAGs.", "Weaknesses": "The findings are limited to specific settings and assumptions, which may not fully generalize to all real-world applications. The reliance on synthetic data for experiments may also limit the applicability of the conclusions to complex, real-world datasets. Additionally, the paper could benefit from more detailed recommendations on mitigating scale effects in practice.", "Questions": "What specific normalization techniques do the authors recommend for mitigating scale effects in structure learning algorithms? Are there particular types of datasets or domains where these findings are especially relevant, or where the proposed mitigation strategies are particularly crucial?"}}
{"paper_id": "eIYDKNqXuV", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 2, "Rating": 5, "Confidence": 4, "Strengths": "The paper offers a combination of methods that improves the computational efficiency of clustering while still capable of handling complex datasets. The presentation of the two-phase methodology shows potential for scalability and lower runtime.", "Weaknesses": "The novelty of combining K-Means with community detection methods isn't strongly emphasized. Initial clustering with K-Means might limit performance, especially on complex datasets. The comparison is limited to a few datasets, lacking broader benchmarking against modern techniques.", "Questions": "How does the method specifically handle the limitations of the initial K-Means clustering step? Is there potential for iterative improvements?"}}
{"paper_id": "bAMPOUF227", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The hybrid framework leverages both the scalability of SLMs and the vast knowledge base of LLMs, thereby improving the generalizability and factuality of LLM outputs, especially in OOD scenarios. The paper provides comprehensive evaluation across multiple datasets and tasks, demonstrating the robustness and applicability of the SuperContext method.", "Weaknesses": "The framework appears conceptually straightforward, with the primary innovation being the integration of confidence-weighted SLM outputs into LLM prompts. There is a reliance on existing models (Llama 2 and ChatGPT) without a detailed dive into the limitations or potential failure cases of the SuperContext methodology in more diverse or adversarial scenarios.", "Questions": "How does the framework handle conflicting predictions from SLMs and LLMs, especially when confidence scores are ambiguous or borderline? Could there be scalability issues if deploying this framework in real-time applications that require quick processing of large volumes of data?"}}
{"paper_id": "eNoiRal5xi", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. UDIM introduces a novel approach by combining parameter and data perturbation to enhance domain generalization, demonstrating a clear improvement over existing methods.\n2. The approach is theoretically grounded, leveraging aspects of SAM optimization while addressing its limitations in domain generalization.\n3. Empirical results on multiple DG benchmarks show consistent improvement, indicating the robustness and efficacy of the method across different datasets.", "Weaknesses": "1. The paper could benefit from a more detailed analysis of the theoretical aspects, especially regarding the bounding of the true DG objective by the proposed framework.\n2. While the use of data perturbation is innovative, the approach and choice of perturbations are not fully explored or justified, leaving room for skepticism about their applicability to diverse domains.\n3. The explanation of how the combined perturbation method specifically addresses the limitations of current SAM variants in unseen domains could be more detailed.", "Questions": "1. How is the data perturbation approach chosen or optimized for different datasets or domain scenarios? Is there a risk of overfitting the perturbation strategy to the training data?\n2. Can UDIM be effectively combined with existing domain alignment or augmentation-based methods to enhance generalization further?\n3. What are the computational overheads introduced by UDIM compared to standard SAM variants, and are there efficiency implications in large-scale applications?"}}
{"paper_id": "uU0Adp7Sfo", "error": "'NoneType' object has no attribute 'model_dump'"}
{"paper_id": "dKju7tbe6D", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel approach by effectively combining iterative and parallel reasoning, which shows improved efficiency in visual reasoning tasks.\n2. The proposed IPRM model demonstrates impressive results across multiple challenging datasets, establishing state-of-the-art performance in some areas.\n3. The architecture is designed to enhance generalization, reducing the number of parameters and computation steps compared to previous models.", "Weaknesses": "1. The integration of iterative and parallel computation may add complexity to the model, potentially making it difficult to interpret or implement easily.\n2. Detailed ablation studies or visualization tools could further highlight the contribution of each component within the IPRM framework, enhancing understanding of its functioning and benefits.\n3. While the approach yields excellent results, real-world applicability in more diverse and less structured datasets than those tested could be explored further.", "Questions": "1. How does the IPRM model handle cases where the visual input may contain noise or irrelevant details that could affect the reasoning accuracy?\n2. Can the authors provide further insights into the trade-offs between parameter size, computational efficiency, and accuracy achieved by IPRM compared to iterative-only or parallel-only methods?\n3. Could additional results on diverse datasets from varied domains further validate the adaptability and robustness of the proposed model?"}}
{"paper_id": "DL7JWbdGr3", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper effectively introduces a novel pre-training paradigm for epidemic forecasting, borrowing concepts from successful pre-training methods in natural language processing and computer vision. The approach is innovative in its application to epidemiology, demonstrating significant improvements in forecasting accuracy across multiple diseases. The integration of self-supervised learning tasks, such as RANDMASK and PEAKMASK, enriches the model's ability to generalize to unseen epidemics.", "Weaknesses": "The proposed model's reliance on diverse, high-quality historical datasets could limit its applicability in regions with sparse or unreliable epidemic data. While the pre-training approach is novel within the domain of epidemiology, the methodological contributions are mostly applications of existing machine learning techniques, which may not provide significant new insights to the wider machine learning community. Additionally, the paper does not detail how the model's performance scales with different input data qualities or how it handles incomplete datasets.", "Questions": "How does the performance of PEMS change with varying amounts of historical data, particularly in low-data settings? What are the limitations of the self-supervised tasks in capturing epidemic dynamics, and do they apply uniformly across different diseases? Could there be further optimizations in the model architecture specifically tailored for epidemic prediction that aren't derived from general-purpose transformer models in NLP?"}}
{"paper_id": "1PaDPHDhwe", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper provides a practical and effective method for improving both robust and average accuracies without requiring additional training. The introduction of a class-specific scaling strategy and the novel evaluation metric (robust coverage) add significant value. The approach is clearly beneficial in scenarios where retraining is not feasible or desired.", "Weaknesses": "The paper may oversimplify the problem by focusing primarily on post-processing methods, potentially overlooking more sophisticated methods that integrate such strategies during the model training process. While the approach is effective, there could be limitations in scenarios where class imbalances or extremely small minority groups exist, as these might impact the efficiency of class-specific scaling and clustering methods.", "Questions": "What datasets were used for evaluation, and how were they chosen to ensure representative data across minority groups? How sensitive is the class-specific scaling approach to the chosen clustering algorithm, and have alternative clustering methods been considered?"}}
{"paper_id": "iI7hZSczxE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper proposes an innovative approach that relaxes the typical independence assumptions in disentangled representation learning, using contrastive learning to address correlated attributes in time series data. This is particularly useful for applications like NILM, where real-world data often exhibit such correlations. The introduction of the Time Disentangling Score (TDS) as a new metric for assessing disentanglement quality is a noteworthy contribution. The experimental results show significant improvements over traditional methods, indicating the effectiveness of the proposed approach.", "Weaknesses": "While the approach is promising, the paper could improve its discussion on the limitations and challenges of the proposed method. The assumption of support independence, although less stringent than total independence, might still be difficult to achieve in practice, and the implications of this should be discussed. Additionally, the paper should provide more clarity on how contrastive learning is implemented in the context of time series data, as well as more detailed ablation studies to understand the contribution of each component of the method. The presentation could be enhanced with clearer explanations of the technical details and a more structured flow of information.", "Questions": "1. How does the relaxation of independence to independence-of-support specifically help in the context of NILM?\n2. Can the proposed approach be adapted to other types of time series data beyond NILM, and what considerations would need to be made for such generalizations?\n3. How sensitive is the model to the choice of hyperparameters in the contrastive loss function, and were any specific validation strategies employed to optimize these?\n4. How does the Time Disentangling Score (TDS) compare to other disentanglement metrics, and why was it necessary to propose a new metric?"}}
{"paper_id": "qrGjFJVl3m", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces the Qwen-VL series that significantly outperforms existing LVLMs across multiple benchmarks in both visual and multilingual tasks.\n2. The three-stage training pipeline, including large-scale pre-training, multi-task pre-training, and supervised fine-tuning, is methodically designed to enhance fine-grained visual understanding and multilingual capabilities.\n3. The introduction of a position-aware vision-language adapter effectively allows for efficient image feature compression, contributing to superior model performance.", "Weaknesses": "1. Despite the significant advances, the paper does not sufficiently discuss the specific challenges encountered during the integration of multilingual modalities, which could be crucial for researchers attempting to build on this work.\n2. The potential limitations or trade-offs of using a large-scale pre-training approach with weakly labeled data are not fully explored, which might impact the model's robustness in practical applications.\n3. The paper could provide more detailed comparisons with proprietary models to better highlight the advances and remaining gaps in performance.", "Questions": "1. How does the model handle situations where weakly labeled data might introduce noise during the large-scale pre-training stage? Are there mechanisms in place to mitigate such noise?\n2. The paper mentions a comprehensive evaluation on various benchmarks, but how does the model's performance compare in real-world applications where there might be a lack of pre-existing multilingual and multimodal datasets?\n3. Are there plans to extend the model to incorporate additional modalities such as audio or to expand its application in more diverse real-world scenarios? If so, how would the architecture need to be adjusted?"}}
{"paper_id": "qup9xD8mW4", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. HaDES offers a novel approach to applying dataset distillation to reinforcement learning tasks by distilling state-action pairs into a synthetic dataset, which is both innovative and practical for reducing data requirements in RL.\n2. The method shows impressive flexibility and robustness across different tasks and RL settings, suggesting it has potential utility in various applications.\n3. The approach is well-situated within existing research, with a clear explanation of how it advances the state-of-the-art in dataset distillation.", "Weaknesses": "1. The requirement for a meta-evolutionary approach and bi-level optimization might introduce complexity that could make implementation challenging for broader adoption.\n2. While the experiments are comprehensive, there could be additional analysis on the impact of different hyper-parameters or architectures on the performance of distilled datasets.\n3. The paper might benefit from a more detailed discussion of potential limitations or challenges in real-world applications, such as scalability or generalization to more complex environments.", "Questions": "1. Can you provide more insight into the hyper-parameter choices and their impact on the effectiveness of HaDES? Are there specific settings that significantly enhance or degrade performance?\n2. How does HaDES perform when faced with more complex or continuous state spaces in RL environments? Are there scalability limitations in such cases?\n3. What are the potential real-world applications of behaviour distillation as developed here, and how does HaDES compare in terms of efficiency and performance to existing RL approaches without distillation?"}}
{"paper_id": "Zw8YxUWL4R", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The proposed P+ space and XTI method offer a novel approach to increasing the expressiveness and control of text-to-image synthesis, which is a valuable contribution to the field of generative models. The paper demonstrates empirical improvements over existing methods like Textual Inversion, which highlights its practical significance.", "Weaknesses": "While the method shows promise, it lacks a detailed theoretical justification for why layer-specific conditioning improves image synthesis control. The paper could benefit from a more in-depth analysis of the impact of the P+ space on different aspects of image quality, such as diversity or fidelity.", "Questions": "How does the choice of specific per-layer tokens in P+ influence image synthesis, and is there a strategy for optimizing these tokens? Additionally, how does this approach generalize across different text-to-image models beyond the Stable Diffusion model used in the experiments?"}}
{"paper_id": "3NXhwkZGjz", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The approach is novel in transforming SFUDA into a semi-supervised learning problem, utilizing multiple hypotheses and their rationale for pseudo-labeling.\n2. The method shows state-of-the-art performance in SFUDA tasks and can be integrated with existing approaches easily.\n3. Provides a systematic way of consolidating hypothesis rationales to improve pseudo-label reliability, which is beneficial for unsupervised adaptation scenarios.", "Weaknesses": "1. Explanation regarding the motivation for using multiple high-quality hypotheses on the same domain is somewhat unclear, as accessing multiple hypotheses pre-trained on the same domain can be challenging in practice.\n2. The approach introduces several hyperparameters ($\\tilde{k}$, $\\tau_1$, and $\\tau_2$) that might not be easily transferable across different tasks, requiring extensive tuning.\n3. Incorrect representation in Eq. (4) might suggest other overlooked mathematical inaccuracies elsewhere.", "Questions": "1. Could you elaborate on the practical considerations and potential challenges of obtaining multiple source hypotheses pre-trained on the same domain?\n2. How would the values of critical hyperparameters like $\\tilde{k}$ be chosen for application to other datasets, and what impact do they have on the model's robustness?\n3. Why does increasing the number of hypotheses seemingly degrade performance, as suggested by results in Table 5, contrary to the expectation that more hypotheses might yield improved pseudo-labels?"}}
{"paper_id": "Fn655mJ4bv", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel method, SOInter, for interpreting structured output models, emphasizing the importance of considering output variable correlations for explanations. This approach shows improved explanation quality over existing methods such as Lime and Kernel-Shap, particularly in scenarios with significant variable correlations. The integration of a globally-trained local interpreter using deep energy-based training is an innovative technique.", "Weaknesses": "While the paper provides a solid foundation, it may lack in providing detailed analysis on the sensitivity of the model to hyperparameters and the robustness of interpretations when applied to various types of data (e.g., images, text). More comprehensive experimentation covering a wider array of structured output models and tasks could strengthen the validation of the proposed approach.", "Questions": "How sensitive is the SOInter method to the choice of hyperparameters such as the number of features to select? Can the approach be applied to other types of data like images or audio, and if so, how would it adapt the feature selection mechanism to non-tabular data? Are there any specific considerations or limitations when using the Gumbel-Softmax trick in this context?"}}
{"paper_id": "VyMW4YZfw7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper provides a compelling case for the use of simpler, traditional nonparametric techniques in SSNC tasks. It presents a clear methodology for utilizing spectral domain methods and highlights the advantages of maintaining model simplicity while achieving competitive performance with complex GNN approaches.", "Weaknesses": "1. The paper may overstate the generalizability of using simpler spectral methods over complex GNN architectures. While the results show promise, additional comparative analysis and broader datasets would strengthen the claims.\n2. The method description lacks depth in explaining the potential limitations or scenarios where simpler models may fall short compared to advanced GNNs.", "Questions": "1. Can the proposed method generalize well to more diverse and challenging datasets beyond the typical benchmarks like Cora and Pubmed?\n2. How does the choice of kernel functions in the spectral domain affect model performance, and is there guidance on selecting the most appropriate kernel?\n3. Would the proposed simpler models be effective in highly dynamic graphs or graphs with rapidly changing structure, where adaptability is critical?"}}
{"paper_id": "09xFexjhqE", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The introduction of a low-rank branch to separately optimize natural and adversarial objectives is an innovative and pragmatic approach to addressing gradient divergence issues in Robust Fine-Tuning (RFT). Additionally, AutoLoRa's heuristic strategies for scheduling learning rates and loss term scalars tackle hyperparameter sensitivity problems efficaciously. The paper presents comprehensive empirical evidence demonstrating the effectiveness of this method across various tasks, showcasing state-of-the-art adversarial robustness. This practical contribution could significantly improve the robustness of pre-trained models used in critical applications.", "Weaknesses": "While AutoLoRa's low-rank branch approach and automated heuristics for learning rate and loss scaling are appealing, the specific implementation details and potential limitations of these heuristic strategies are not thoroughly examined. It remains unclear how these heuristics might perform across diverse architectures or dataset complexities not covered in the exploration. Furthermore, the empirical analysis appears predominantly centered on ResNet models, which raises questions about the generalizability of the findings to other network architectures without further evidence.", "Questions": "1. How does AutoLoRa perform with architectures other than ResNet, such as transformer-based models?\n2. Could the authors elaborate on the choice of heuristic strategies for scheduling learning rates and loss term scalars? How transferable are these strategies to different model architectures or datasets?\n3. Are there any specific limitations or potential failure cases identified for AutoLoRa, particularly in less structured/more complex datasets not included in the studies?"}}
{"paper_id": "nmBjBZoySX", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses a significant limitation in existing Graph Lottery Ticket (GLT) methods by proposing an adaptive and dynamic framework that enables deeper Graph Neural Networks (GNNs) to efficiently identify graph lottery tickets. The proposed method, AdaGLT, offers a scalable and automated solution with layer-adaptive sparsification, which is a notable improvement over existing techniques. The experiments show promising results, indicating the utility and effectiveness of AdaGLT in various graph datasets.", "Weaknesses": "While the paper presents a novel approach, it may lack experimental diversity. The evaluation seems mainly focused on synthetic and potentially smaller datasets without addressing its performance on a wider range of real-world and more complex graph datasets or tasks. Additionally, the theoretical proofs should be closely scrutinized, as the complexity and soundness of integration between pruning and dynamic restoration need further validation.", "Questions": "1. How does AdaGLT perform on larger, more complex graph datasets, such as those with millions of nodes and edges? \n2. What are the potential trade-offs of using AdaGLT compared to traditional methods in terms of computational resources and execution time due to the dynamic nature of its pruning and training integration? \n3. Can AdaGLT be extended to support online or streaming graph scenarios where graph topology changes over time?"}}
{"paper_id": "di52zR8xgf", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper presents a significant enhancement to a widely-used open model, Stable Diffusion, making it competitive with state-of-the-art black-box models.\n2. The use of novel conditioning schemes and a refinement model to improve visual fidelity indicate a substantial advancement in generative model design.\n3. SDXL retains the openness and reproducibility that are crucial for community-driven research and development.", "Weaknesses": "1. While the approach builds upon Stable Diffusion, the originality in terms of novel techniques might not be as groundbreaking as expected, mostly featuring incremental improvements.\n2. The paper could provide deeper insights into the trade-offs or limitations encountered with the larger architecture and additional attention blocks in terms of computational cost, which is critical for practical deployment.\n3. There is less emphasis on how these changes affect training efficiency or the specific scalability considerations.", "Questions": "1. How do the enhancements in architecture and conditioning specifically affect computational cost and speed compared to previous versions of Stable Diffusion?\n2. Are there specific domains or applications where SDXL shows significant advantages over both previous open models and current black-box models?\n3. What are the key limitations of SDXL when handling novel text prompts compared to black-box solutions?"}}
{"paper_id": "0b328CMwn1", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper presents a novel approach, Activation Prompts (AP), which extends the concept of Visual Prompting (VP) by introducing perturbations at intermediate activation map layers. This method shows promise in improving performance and efficiency.\n2. The extensive experimental evaluation over 29 datasets is a significant strength, indicating the robustness and general applicability of the proposed method.\n3. The analysis of the connection between AP and normalization tuning techniques provides a theoretical foundation for the proposed approach, enhancing its soundness.", "Weaknesses": "1. The novelty of the contribution might be seen as incremental, as it builds upon existing concepts in visual prompting and parameter-efficient fine-tuning.\n2. The analysis and comparison with existing methods may not be comprehensive enough, potentially leaving unanswered questions about the specific advantages of AP over other parameter-efficient fine-tuning methods.\n3. The implementation details, such as the exact setup of perturbations in AP, might not be sufficiently detailed, which could limit the reproducibility of the experiments.", "Questions": "1. What specific criteria were used to select the layer depths for AP implementation, and how sensitive is the performance to this selection?\n2. Could the method be extended or adapted to other types of models beyond CNNs and ViTs, and if so, how?\n3. What are the implications of AP on the interpretability of the model's predictions, and does it introduce any new considerations in this area?"}}
{"paper_id": "AMDKqZcZbi", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The proposed model effectively draws inspiration from biological systems, which is a novel approach to tackling the problem of catastrophic forgetting. The introduction of the sWM task provides a better evaluation setup for assessing both rapid learning and memory retention capabilities of models.", "Weaknesses": "The paper lacks a thorough comparison with more recent and advanced continual learning methods beyond just the traditional few-shot and continual learning baselines. This limits the understanding of how well the proposed model performs in a broader context. Additionally, the model's generalizability to tasks other than navigation, which the MESH architecture seems particularly suited for, is not demonstrated.", "Questions": "How does the model's performance change when applied to tasks outside of navigation-focused problems? Will the MESH architecture need adaptations to handle different types of environmental changes or task domains?\nWhat are the computational costs associated with this biologically inspired model compared to traditional continual learning models?\nHow effectively can the method scale with the increasing complexity of tasks, both in terms of task structure and environment dimensionality?"}}
{"paper_id": "am7BPV3Cwo", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper effectively addresses the important problem of out-of-distribution detection under class-imbalanced settings, which is a practical concern in real-world applications.\n2. The proposed ImOOD framework introduces a combination of post-hoc normalization and training-time regularization, offering a novel perspective on OOD detection. This method has demonstrated significant improvements over existing state-of-the-art approaches on relevant benchmarks.\n3. The experiments conducted on multiple benchmark datasets help to validate the effectiveness of the proposed methods, showing clear benefits over traditional OOD detection techniques.", "Weaknesses": "1. The paper's focus on traditional class imbalance could be expanded to include additional challenges and complexities often found in real-world distributions, such as those found in practical deployments.\n2. The explanation surrounding the introduced methods could be clearer, especially the underlying intuition behind the post-hoc normalization process and how it integrates with the class-aware bias addressed in the paper.\n3. Some sections, particularly in the methodology, are dense and could benefit from clearer explanations or additional diagrams to help elucidate key concepts for a broader audience.", "Questions": "1. Is the post-hoc normalization dependent on the choice of the OOD detection threshold, or is it applied uniformly across all samples?\n2. How would ImOOD perform when applied to datasets with more complex or multi-modal distributions? Are there plans to test the method in such settings?\n3. Can the proposed framework be integrated with existing neural architectures, or is it limited to the particular architectures experimented on in the paper?"}}
{"paper_id": "R1crLHQ4kf", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The approach is novel, leveraging statistical characteristics of output token distributions to detect adversarial audio inputs across multiple ASR systems without needing modifications to the data or adversarial training, demonstrating high effectiveness with AUROC exceeding 99%. The paper is well-structured, clearly illustrating methodology and outcomes.", "Weaknesses": "The approach relies on the statistical analysis of token outputs, which may vary across different models and could depend heavily on the architecture of the ASR systems used. The robustness of the proposed detection method against a wider variety of adaptive attack strategies could be further examined.", "Questions": "How does the proposed method perform when tested on a wider range of adversarial attack strategies beyond those considered? Can the method be generalized to other modalities beyond ASR or expanded to detect adversarial attacks in audio classification tasks?"}}
{"paper_id": "i7P2mK3x3o", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces Q-flow, a novel flow-based model for dynamic optimal transport between arbitrary distributions, which offers a fresh approach to distribution transformation beyond the usual normal distribution. The method effectively leverages neural ODEs to articulate optimal transport maps, demonstrating strong empirical performance in various high-dimensional data tasks.", "Weaknesses": "The primary critique is the lack of theoretical analysis regarding the optimality of the transport map, as well as the potential pitfalls of extending this approach to more complex distribution scenarios without adequate justification. Additionally, though empirical studies are robust, a deeper exploration into the method's limitations or scenarios where it might falter would strengthen the findings.", "Questions": "How does the model ensure that the transport map is optimal, and are there strategies proposed for validating the accuracy or convergence of the transport map across diverse datasets? Are there specific computational requirements or limitations when scaling the method to extremely high-dimensional spaces?"}}
{"paper_id": "YIls9HEa52", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces an innovative approach to address the limitations of traditional rSLDS models by integrating a distance-dependent CRP along with latent geometry, which enhances the model's flexibility and applicability to varying neural data. The use of a PDE framework for computational inference is intriguing and suggests a potential improvement in efficiency. The application of the model to synthetic and real electrophysiological data demonstrates its practical relevance and effectiveness in uncovering non-stationary processes.", "Weaknesses": "While the irSLDS model presents a novel integration of several concepts, the details of implementation and the practical impact of the introduced methods could be elaborated further. The model's scalability and efficiency with larger datasets or in different contexts are not thoroughly addressed, leaving questions about its general applicability. The presentation could benefit from clearer explanations of the methodologies, particularly the PDE framework and its integration with the model.", "Questions": "How does the introduction of a latent geometric structure specifically contribute to improved interpretability, and are there any examples or case studies that illustrate this advantage? Can the authors provide further insights into the scalability of the irSLDS model beyond the presented datasets, and how it would perform in more complex neural data scenarios? What are the potential limitations or challenges in extending the model to other types of neural data or alternative domains?"}}
{"paper_id": "koYsgfEwCQ", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 3, "Strengths": "1. The DynaVol method provides a novel approach for addressing 3D consistent scene understanding using object-centric voxelization.\n2. The multi-stage training process and integration of cycle-consistency loss to ensure better learning dynamics is innovative.\n3. Experimental results demonstrate that the method outperforms existing 2D and 3D methods in unsupervised dynamic scene decomposition, providing strong empirical evidence of the method’s efficacy.", "Weaknesses": "1. The methodology seems to rely heavily on structured volumetric data, which may limit its applicability to more complex, unstructured dynamic scenes.\n2. Details regarding the implementation specifics, especially the interaction between different network components, could be elaborated more clearly.\n3. While the paper addresses occlusion issues, the evaluation on dynamic scenes should consider more complex environments and interactions to show general robustness.", "Questions": "1. How does DynaVol handle scenes with rapidly changing dynamics or interactions between multiple objects beyond the assumptions made for voxelization?\n2. Can the approach be scaled to handle more artifacts or information beyond geometric shapes and motion trajectories in the dynamic scenes?\n3. Is there a specific real-world application or scenario where DynaVol has been tested outside of the benchmark synthetic environments?"}}
{"paper_id": "o6AN2ZoNw2", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. P-Seg effectively eliminates the need for annotated image masks, relying instead on pseudo-mask generation, which simplifies the training process and reduces resource dependence.\n2. The method demonstrates strong performance across multiple benchmark datasets, indicating its scalability and robustness.\n3. By utilizing a self-supervised Vision Transformer (ViT) and image-text datasets, the approach aligns language features with pixel-level features in a novel manner.", "Weaknesses": "1. The novelty of the proposed approach may be somewhat limited, as it builds upon existing MaskFormer architecture and uses pseudo-mask generation similarly explored by other methods.\n2. While the results are promising, the experimental comparisons could benefit from more extensive datasets or additional state-of-the-art baselines for a rigorous evaluation.\n3. The method's reliance on large image-text datasets for effective training might pose a challenge for applications with limited data availability.", "Questions": "1. Can the P-Seg method be effectively applied to domains beyond those evaluated, or would it require significant adaptation?\n2. How sensitive is the performance of P-Seg to the choice of the pseudo-mask generator or the self-supervised ViT model?\n3. What detailed insights can be drawn about the alignment of language features to pixel-level features, and how effectively can this approach be generalized to handle complex semantic relationships in diverse images?"}}
{"paper_id": "HXZK1Z8tHa", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The proposed ShareFormer model addresses the key challenges associated with Transformer-based methods in image restoration, effectively balancing between latency reduction and improved trainability.\n2. Introduction of Shared Portion Stripe Attention (SPSA) and residual connections enhances the model's efficiency in handling long-range dependencies while reducing computational overhead.\n3. Comprehensive evaluation and competitive performance benchmarks showcase the potential of ShareFormer across various image restoration tasks such as super-resolution and denoising.", "Weaknesses": "1. The presentation of the methodology and its components needs refining to enhance clarity and understanding, particularly of how Shared Portion Stripe Attention and residual connections operate within the model.\n2. Comparative insights on the ablation of individual components like CSAU and the precise impact on model performance and efficiency are not adequately detailed.\n3. Some explanations, such as the attention sharing mechanism in Fig.2 and Fig.3, are unclear, potentially causing ambiguity in understanding the exact novelty of the method.", "Questions": "1. Can you clarify the precise operation points for shared attention within the model, as there seems to be a discrepancy in the explanation and illustrations?\n2. How does the mechanism of residual connections on the 'Value' in self-attention contribute specifically to enhancing locality bias, as claimed? Is there comparative visualization available to substantiate this claim?\n3. Why were layers 1 and 3 chosen for attention map comparison in Appendix D instead of the directly sequential pairings used in ShareFormer, to demonstrate reduction redundancy more convincingly?\n4. Was the same training patch size used in SR experiments for ShareFormer as compared to baseline models, to ensure the fairness of comparison?"}}
{"paper_id": "QO3yH7X8JJ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The proposed Diff-SR method offers an innovative solution for arbitrary-scale super-resolution by capitalizing on existing pre-trained diffusion models without the need for additional fine-tuning or retraining specific models. It demonstrates significant improvements in image quality across various scales and provides a flexible and efficient approach to super-resolution tasks, validated by thorough experimental results.", "Weaknesses": "The methodology, while innovative, seems to rely heavily on the efficacy of existing pre-trained models, which may have inherent biases or limitations. The details on how the noise level is determined could be further elaborated. Additionally, the approach's reliance on existing diffusion models may limit its applicability to models that fall outside of this framework.", "Questions": "Can the proposed noise injection strategy be easily adapted to other types of pre-trained generative models beyond diffusion models? How does the method determine the optimal noise level for different low-resolution images, and does this require significant computational resources? Could the authors provide more insight into the Perceptual Recoverable Field and its computational requirements?"}}
{"paper_id": "sNtDKdcI1f", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper provides a strong empirical analysis that highlights the potential oversight in RLHF practices regarding output length.\n2. It offers a set of interventions to address length bias, showing a methodological approach to a well-identified problem in RLHF.\n3. The study uses rigorous experimentation across multiple datasets, ensuring that the results are robust and applicable across various contexts.", "Weaknesses": "1. Although the paper identifies the issue of reward models being biased towards longer outputs, it does not fully address how to adjust reward models to completely remove this bias in practice.\n2. The focus on length as the main confounder may leave out other factors that could play a significant role, which might not have been thoroughly analyzed.\n3. The interventions proposed may not completely align with more complex tasks where semantic quality and coherence are critical, as opposed to merely optimizing length.", "Questions": "1. How can reward models be robustly adjusted to balance length optimization with genuine improvement of output quality?\n2. Could there be other overlooked factors in RLHF, besides length, that might contribute to perceived improvements? If so, what impact might they have relative to length?\n3. How generalizable are the findings across different tasks beyond question answering and dialogue, such as creative writing or technical subjects where verbosity may not equate to quality?"}}
{"paper_id": "HFtrXBfNru", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. The paper addresses an important problem in the field of graph neural networks by considering the challenge of evolving graphs and their impact on representation distortion and temporal generalization.\n2. Introducing a self-supervised learning approach to tackle this problem is a novel and promising direction.\n3. The method shows improved performance in estimation of temporal generalization, indicating potential for real-world applications across several domains.", "Weaknesses": "1. The paper lacks a detailed comparison with existing baseline methods for evolving graphs, which might provide a more comprehensive evaluation of the method's strengths and limitations.\n2. The assumptions required for the theoretical analysis and derivations could limit the practical applicability of the proposed method in dynamic environments where graphs might frequently shrink or node attributes significantly change.\n3. More clarity in some sections, such as further explanations of equations or methodological steps, would enhance understanding.", "Questions": "1. How does SMART compare to existing methods designed for evolving graphs or heterophilic graphs when applied to newly scaled or significantly altered networks?\n2. How would the model potentially adapt if modifications occur in features rather than structures over time? Are there any implementations considering edge-weight dynamics?\n3. Could the use of larger, more complex real-world networks potentially reveal additional challenges not captured in the current experimental settings?"}}
{"paper_id": "4u0ruVk749", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses a well-known challenge in causal inference by proposing a novel approach using diffusion models to handle unobserved confounders. The use of diffusion models is an innovative direction that potentially offers more flexibility and stability compared to traditional methods. The empirical evaluation against state-of-the-art methods on both synthetic and benchmark datasets strengthens the validity of the proposed approach, showing improved ITE estimation.", "Weaknesses": "While the method shows promising results, the theoretical justification for why diffusion models are particularly well-suited for modeling unobserved confounders could be further elaborated. Additionally, the details on how the method outperforms others in terms of computational efficiency or scalability are limited. The clarity in the presentation can be improved, as some parts are not thoroughly explained, particularly the connection and transition functions in the diffusion process.", "Questions": "1. How does the diffusion model scale with larger datasets, particularly in terms of computational resources and time? \n2. Can the authors provide more insight into the choice of diffusion model parameters and how sensitive the results are to these choices?\n3. Are there any specific scenarios or datasets where the proposed method fails to perform well compared to traditional models?"}}
{"paper_id": "Pa6SiS66p0", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The proposed benchmark and method for multi-modal continual learning, leveraging the VGGSound dataset, addresses an important gap in the research area by integrating multi-modal approaches to overcome restrictions in unimodal continual learning, particularly under challenging conditions like class and domain incremental learning.", "Weaknesses": "Lack of experimental diversity, with evaluations limited exclusively to the VGGSound dataset, constrains the demonstration of the proposed method's generalizability. Additionally, comparisons are only made against an older baseline from 2018, lacking insights from competing recent methodologies.", "Questions": "What alternate datasets, benchmarks, or state-of-the-art baselines might better illustrate the strengths and weaknesses of SAMM? How does SAMM integrate multi-modal scenarios compared to vision-language continual learning benchmarks?"}}
{"paper_id": "rkplYfqUr0", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. The paper proposes a novel approach to zero-shot text classification by leveraging generative language models with paraphrased label descriptions, which improves robustness to prompt variations.\n2. The use of multiple paraphrases aggregates results and helps in reducing bias and variance, thus enhancing the model's performance.\n3. The evaluation of the method across a broad set of classification tasks and with multiple language model families demonstrates the approach's general applicability.", "Weaknesses": "1. The reliance on manually crafted paraphrases and descriptions, while innovative, introduces subjectivity and potential bias. Further automation or a clearer framework for crafting effective prompts could strengthen the approach.\n2. The paper might not sufficiently distinguish the benefits over few-shot learning approaches, especially considering the effort involved in paraphrase generation.\n3. There is limited exploration of how this approach scales with larger datasets or more complex classification tasks.", "Questions": "1. How does the effectiveness of GEN-Z compare when used in combination with few-shot learning approaches rather than as a standalone zero-shot method?\n2. What guidelines can be provided for selecting or generating paraphrases to ensure consistency and minimize biases in outputs?\n3. Can the method be effectively scaled to multilingual scenarios, and what challenges might arise in such applications?"}}
{"paper_id": "pTU2X9mUBe", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The dataset provided is unprecedented in terms of scale and detail within the domain of last-mile delivery. LaDe not only includes a massive amount of package and courier data but also addresses both pickup and delivery contexts, offering a comprehensive view which is often lacking in existing datasets. The authors have demonstrated the dataset's applicability by using it to benchmark several tasks pertinent to logistics and supply chain management.", "Weaknesses": "While the dataset is extensive, the paper could have further detailed the specific challenges and quality control measures taken during the dataset compilation. Additionally, the discussion regarding privacy concerns and how they were mitigated is somewhat lacking, given the sensitive nature of logistics data. Moreover, the application of the dataset to provide new insights or discoveries beyond the benchmarked tasks could have been explored more deeply.", "Questions": "What are the ethical considerations taken into account while collecting and sharing such a comprehensive dataset, particularly considering personal and sensitive data related to packages and couriers? Are there specific limitations to using LaDe across different geographic regions or logistic scenarios due to its origin from Chinese cities?"}}
{"paper_id": "uhtQyRrTzY", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 5, "Strengths": "1. MIMIC provides an innovative approach to curating large-scale datasets from unannotated real-world videos and synthetic environments, overcoming the reliance on heavily annotated datasets.\n2. The method effectively utilizes computer vision techniques like SIFT and RANSAC to automate dataset creation, which enhances scalability and applicability to real-world scenarios.\n3. The resulting MIMIC datasets demonstrate superior performance in dense prediction tasks compared to traditional annotated datasets, showcasing the approach's effectiveness.\n4. The study highlights the potential for scalability in dataset creation, allowing for even larger datasets and improved model training.", "Weaknesses": "1. The paper could provide more detailed information on the specific preprocessing and automated steps involved in refining and validating created dataset pairs to ensure the highest quality for downstream tasks.\n2. Although the results are promising, the diversity and complexity of the selected real-world video sources could be explored further to ensure performance consistency across various real-world scenarios.\n3. Additional comparative analyses between MIMIC and other self-supervised learning techniques on diverse real-world datasets could have strengthened the argument for its universal applicability and robustness.", "Questions": "1. How generalizable are the created MIMIC datasets across a wider range of vision tasks beyond those tested in the study? Are there potential limitations for specific types of datasets?\n2. What are the computational and storage requirements for auto-generating and processing the MIMIC datasets, and how do they compare to existing annotated datasets such as ImageNet?\n3. Are there any plans to make the MIMIC datasets available to other researchers, and if so, what formats and access methods will be provided?"}}
{"paper_id": "WZfatbNdLV", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents an innovative approach to RNA splicing prediction by leveraging generative model architectures and combining multiple transformers with side information. TrASPr demonstrates outstanding performance improvements over existing models, offering significant advancements in the field of computational biology and splicing analysis. The method’s ability to suggest biologically plausible regulatory mechanisms and generate accurate RNA sequence designs highlights its potential applicability in experimental and therapeutic contexts.", "Weaknesses": "Some aspects of the model’s implementation and integration could benefit from clearer explanation, such as the specific mechanisms of the model’s application to different tissue types and conditions. There might be room for enhancement in describing the Bayesian Optimization framework and more detailed comparisons against competing models. Also, while the results are promising, the practical applicability constraints in experimental scenarios could be considered more thoroughly.", "Questions": "1. What are the specific limitations or challenges encountered when implementing the TrASPr model across diverse tissue types and complex datasets?\n2. Could further elaboration on the Bayesian Optimization framework's integration with TrASPr enhance comprehension of its practical application?\n3. Are there any specific datasets or conditions in which TrASPr performs less optimally, and if so, what modifications or improvements could address these issues?\n4. How does the model handle instances with exceedingly complex splicing mechanisms, and what impact does this have on its predictive accuracy?"}}
{"paper_id": "YkRwadXWHd", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The proposed method demonstrates significant improvement in WER across benchmark datasets, showcasing its effectiveness in improving CSLR performance.\n2. The integration of multiple modalities (video, keypoints, and optical flow) utilizing fusion techniques provides a comprehensive approach to feature extraction.\n3. The hierarchical knowledge distillation framework effectively reduces computational costs while maintaining performance, which is critical for practical CSLR applications.\n4. The work provides a novel combination of multi-modal learning and knowledge distillation for sign language recognition, addressing a significant gap in the field.", "Weaknesses": "1. The novelty of the approach could be questioned as it primarily leverages known techniques, such as multi-modal fusion and knowledge distillation.\n2. The paper could provide more detailed insights into how the fusion of modalities specifically contributes to learning better features.\n3. Evaluation on only three datasets might limit the demonstration of the framework's generalization ability across various sign languages or contexts. Greater diversity in test datasets might better validate the approach.\n4. There is limited discussion on the potential computational limitations or trade-offs encountered during the implementation of the proposed method.", "Questions": "1. How does the proposed method handle complex sequences with high variability in sign languages across different signers or contexts?\n2. How sensitive is the hierarchical knowledge distillation framework to the chosen teacher-student models and their configurations?\n3. Would performance improvements continue if additional modalities are introduced, or is there a threshold beyond which additional modalities do not contribute significantly to performance?\n4. How does the proposed method compare, in terms of computational efficiency and inference time, with other existing CSLR methods?"}}
{"paper_id": "q4Bim1dDzb", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel approach to inverse rendering using a unified voxelization framework, which helps in efficiently encoding scene properties like geometry, materials, and illumination. The use of lightweight MLPs and Spherical Gaussians for illumination modeling contributes to reducing computational cost and achieving good reconstruction quality.", "Weaknesses": "While the approach is innovative, the paper does not detail sufficiently the degree to which the method surpasses existing techniques in various metrics. Additionally, while it mentions efficiency, there is a lack of detailed analysis on memory consumption or potential trade-offs in quality. The method's ability to generalize across diverse real-world datasets might also not be fully addressed.", "Questions": "1. How does the proposed method handle complex real-world lighting conditions more effectively than previous methods?\n2. Is there a specific reason why Spherical Gaussians were chosen over other potential representations for illumination?\n3. The paper mentions robustness over diverse datasets; could more details be provided about the datasets used for benchmarking?"}}
{"paper_id": "39cPKijBed", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The proposed method for time-dependent importance reweighting in diffusion models is innovative and appears to effectively address dataset bias more precisely than existing methods. This approach shows strong experimental results across multiple diverse datasets, suggesting robust practical applicability. Additionally, the method's integration into a stochastic differential equation framework is theoretically interesting.", "Weaknesses": "While the method shows promise, the reliance on a discriminator for density ratio estimation may introduce potential complexities or limitations in certain scenarios. The paper might also benefit from more detailed comparisons with a broader range of existing bias-mitigation techniques beyond the selected baselines, as well as clarifying potential limitations in scalability or computational overhead.", "Questions": "How does the time-dependent density ratio estimation handle cases with extremely imbalanced bias distributions? Are there potential limitations to this approach when scaling to larger datasets? Can the stochastic differential equation framework introduce any computational inefficiencies or challenges? How sensitive is the model's performance to the method chosen for density ratio estimation, and can improvements to this aspect further enhance the method's effectiveness?"}}
{"paper_id": "aG3EARrrd1", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The proposed OSRT model effectively integrates tree decomposition and adaptive RBF exploration, showing improved accuracy and efficiency for online learning in dynamic datasets. It manages both tree and RBF parameters concurrently, providing a novel approach to addressing limitations in traditional RBF networks, which is validated with benchmark datasets.", "Weaknesses": "The paper lacks a detailed discussion on the computational complexity of the proposed method and comparisons with state-of-the-art hybrid models. The explanation of the methodology could be more rigorous, particularly in describing how OSRT dynamically balances growth, pruning, and adapts to changes in streaming data. More clarity on the algorithm's scaling with data size would also be beneficial.", "Questions": "How does the OSRT model ensure robustness against overfitting, especially when adapting RBF centers and balancing tree growth? Can the method be generalized to other types of streaming data beyond the time series considered in this study? What are the specific limitations of OSRT in terms of scalability and computational cost?"}}
{"paper_id": "Ts95eXsPBc", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel extension to transformers by integrating spatial information, which adds a new dimension to episodic memory modeling. The introduction of the Adaptive Memory Allocator is an innovative approach to memory management, allowing for dynamic memory usage that can adapt to different task demands.", "Weaknesses": "While the methodology is well-executed, the paper could benefit from more comprehensive comparisons with existing episodic memory models beyond transformers. The implications of spatial information incorporation are discussed, but further exploration into its limitations would strengthen the paper.", "Questions": "How does the incorporation of spatial information in the transformer models affect the computational complexity of the algorithms? Are there any scenarios where this added complexity might outweigh the benefits?"}}
{"paper_id": "LfhG5znxzR", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper addresses the critical challenge of improving interpretability and control in neural networks, a key area of research in enhancing AI transparency and safety.\n2. Introducing sparse and discrete bottlenecks with vector quantization is a novel approach that balances performance with increased interpretability, providing actionable insights into neural operations.\n3. The methodology is validated across varied datasets, demonstrating the versatility and effectiveness of the proposed approach.", "Weaknesses": "1. While the approach provides interpretability, the impact on model performance across diverse and larger-scale datasets remains to be thoroughly validated.\n2. The reliance on cosine similarity for selecting representative codebook features might not always effectively capture nuanced variations in complex activation patterns.\n3. The paper could benefit from more detailed experimental results and comparisons with state-of-the-art methods to firmly establish its contributions.", "Questions": "1. How does the inclusion of discrete bottlenecks impact the training time and computational requirements of neural networks, especially in large-scale models?\n2. Can the proposed method effectively generalize to other neural architectures beyond Transformers, such as convolutional networks for image processing tasks?\n3. What mechanisms are in place to optimize the size of the codebook and the selection of relevant features to maintain a balance between interpretability and model performance?"}}
{"paper_id": "bUGzjiUsIq", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses a critical problem in multi-label classification: the challenge of learning from partially annotated data. By proposing the PAPG framework, it brings novel insights into integrating reinforcement learning with incomplete annotations to tackle the issue of imbalance and scarcity in positive annotations. The approach is innovative with the use of policy gradient methods to balance exploration and exploitation. The experiments demonstrate the effectiveness of the method by showing a substantial improvement in F1 scores compared to existing methods, indicating the robustness and versatility of the approach across various datasets.", "Weaknesses": "While the method is promising, the soundness of the framework is not thoroughly established. How the reinforcement learning principles are specifically applied to the partially annotated problem could be expounded upon. Clarity in presentation can be improved; details about the Markov Decision Process setup and how local and global rewards interact might not be sufficiently clear to all readers. Additionally, the paper might benefit from a deeper comparison with existing reinforcement and semi-supervised learning methods to highlight distinguishing factors of the PAPG method. There is also some concern regarding the scalability of the model to extremely large datasets.", "Questions": "1. How sensitive is the framework to the choice of hyperparameters affecting the balance between exploration and exploitation?\n2. Could the method be applied to tasks beyond multi-label classification, or is it specifically tailored to address partial annotations in this context only?\n3. How does this approach compare to semi-supervised learning techniques in scenarios where annotations are scarce?"}}
{"paper_id": "uMAujpVi9m", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The innovative approach of generating over 5 million pseudo-complexes allows for large-scale pretraining, which is typically limited by data scarcity.\n2. ProFSA demonstrates significant improvement over existing methods in critical tasks such as pocket druggability prediction, emphasizing its practical applicability.\n3. The method creatively addresses the gap in the availability of protein-ligand complex data by using existing high-resolution atomic protein structures.", "Weaknesses": "1. The complex methodology and setup might be challenging to understand fully, which could hinder reproducibility.\n2. While pseudo-ligands are utilized effectively, there is limited discussion on the potential limitations or inaccuracies introduced by simulating real ligand interactions.\n3. The impact of different choices in segmenting protein structures and selecting pseudo-ligand representations could have been explored more thoroughly.", "Questions": "1. How sensitive is ProFSA to the initial choice of segmentation and alignment methods for protein structures?\n2. Are there any limitations observed when expanding ProFSA to different types of proteins or ligands not previously included in the training data?\n3. Could the alignment of pseudo-ligand representations with pretrained small molecule encoders introduce biases that are not reflective of true ligand-receptor interactions?"}}
{"paper_id": "uqxBTcWRnj", "error": "'NoneType' object has no attribute 'model_dump'"}
{"paper_id": "o4CLLlIaaH", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. The proposed shift from image-based to point-based rendering in NeRFs is a novel approach that addresses several limitations of existing models, such as feature inconsistencies and poor geometric handling in achieving generalization without per-scene optimization.\n2. The integration of innovations like visibility-oriented feature fetching and a nonuniform log sampling strategy improves both rendering speed and quality, offering a comprehensive advancement in the field.\n3. Experimental validations on multiple datasets demonstrate the method's effectiveness in enhancing rendering quality, contributing both to academic understanding and practical applications.", "Weaknesses": "1. The explanation of the implemented techniques, particularly the visibility-oriented feature fetching and geometric priors, lacks depth, making it challenging for readers to fully grasp the intricacies and potential limitations of the method.\n2. There is a reliance on PatchmatchMVS for the initial scaffolding, which may limit the portability and independence of the GPF method.\n3. Testing is limited to only three datasets, which may not sufficiently demonstrate the model's versatility and broad applicability, especially for diverse and real-world scenarios.", "Questions": "1. Could you provide further detail on the visibility-oriented feature fetching mechanism and how geometric priors are specifically incorporated into the method?\n2. What are the potential impacts of using PatchmatchMVS as a dependency for initial point generation, and how might this affect the method's scalability or generalization in more complex scenes?\n3. How does the computational complexity of the proposed GPF approach compare to existing NeRF methods in terms of rendering speed and resource usage?"}}
{"paper_id": "KNzL1nglNB", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The authors introduce an innovative approach—Label-encoding Risk Minimization (LRM)—which provides a novel way to improve generalization in scenarios with limited labeled data.\n2. This method is shown to successfully enhance prediction diversity and reduce bias towards dominant classes in various challenging settings such as SSL, UDA, and SHDA.\n3. The paper includes a theoretical analysis linking LRM with concepts like neural collapse, adding depth to the contribution.", "Weaknesses": "1. The presentation of results could have been clearer, with more emphasis on comparisons across diverse datasets and clearer visualizations.\n2. While the method offers promising results, the experimental setup is somewhat limited and might not generalize across all types of datasets, especially larger and more complex ones.\n3. There is limited discussion on the robustness of LRM against different types of real-world noise and variance in unlabeled data.", "Questions": "1. How does the LRM method perform in real-world applications where the unlabeled data might contain significant noise and outliers? Are there any adjustments or considerations?\n2. Would LRM require any parametric or structural changes for tasks with significantly different feature spaces or output distributions?\n3. Can LRM be integrated with other advanced techniques such as meta-learning or few-shot learning to enhance its capability in label-deficient environments?"}}
{"paper_id": "pvhyBB86Bt", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 2, "Rating": 5, "Confidence": 4, "Strengths": "The study introduces a novel perspective on the stability of Jacobian matrices in neural networks throughout the training process. This is an important area of exploration, given its implications on network performance and training stability. The paper is systematically organized and well-written, aiding in the reader's understanding of complex concepts. The use of recent advancements in random matrix theory enhances the credibility and depth of the analysis.", "Weaknesses": "The study heavily relies on assumptions that may be considered strong, particularly those about the conditions during the entire training process. Theoretical backing for these assumptions appears limited, which could undermine the robustness of the conclusions drawn. Moreover, the empirical evidence provided, though supportive, might not suffice in lieu of a comprehensive theoretical framework. Additionally, the examples and theorems discussed in the paper lack complexity, potentially oversimplifying the real-world applicability of the findings.", "Questions": "Could you provide more details on the specific techniques from random matrix theory utilized in the study, perhaps in the main text? Furthermore, how do the assumptions made about the training conditions hold up in diverse training environments and architectures?"}}
{"paper_id": "Mylk5iamJC", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The proposed method introduces a unified approach that combines generative and discriminative modeling, potentially improving performance for both closed-set and open-set recognition without the need for modifications or prior knowledge.", "Weaknesses": "The novelty of using a Latent Gaussian Mixture Model in this context may be limited, as similar methods have been explored in prior work. The paper's presentation can be improved for clarity, especially in terms of differentiating this work from existing literature.", "Questions": "How does the L-GMM specifically differ from other latent generative models used for recognition tasks, and what unique contributions does it make beyond integrating CSR and OSR?"}}
{"paper_id": "MsOcVFzv8D", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a theoretically grounded approach to multi-domain text classification, addressing a significant gap noted in current methodologies. The use of margin discrepancy within adversarial training provides a robust framework that balances theory with empirical results, demonstrating superior performance over existing methods. The clear connection between the theoretical framework and empirical results is commendable.", "Weaknesses": "The presentation of the material could be improved for readability and clarity, especially for audiences less familiar with the theoretical aspects of domain adaptation. While the theoretical grounding is strong, the novel contributions beyond the adaptation of existing theories to MDTC are somewhat limited. A broader comparison with more diverse datasets could solidify the findings.", "Questions": "1. How does the proposed MDAT method perform in settings with even less labeled data in the target domain compared to the benchmarks used? 2. Are there any specific scenarios or types of datasets where the theoretical advantages of the method do not translate into practical improvements?"}}
{"paper_id": "fBlHaSGKNg", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper proposes a novel approach for sample selection in semi-supervised learning that effectively balances representativeness and diversity, filling an overlooked gap in SSL research concerning efficient sample annotation.\n2. The use of the Generalized Kernel Herding without Replacement (GKHR) algorithm is innovative, enabling efficient minimization of the new criterion, α-MMD.\n3. The method shows promising results across popular datasets, outperforming current active learning methodologies under budget constraints.", "Weaknesses": "1. The paper could include more extensive empirical evaluations across a wider range of datasets to better demonstrate the generalizability and robustness of the proposed method.\n2. The integration of the method with only specific SSL frameworks like FlexMatch and FreeMatch might limit the applicability of the proposed approach to other frameworks that potentially do not easily accommodate the use of the Gaussian kernel with CLIP features.\n3. The approach's reliance on kernel methods could potentially lead to scalability issues when dealing with very large datasets, which is not addressed in the paper.", "Questions": "1. How does the α-MMD criterion directly improve the generalization abilities of the SSL models, compared to traditional methods?\n2. Can the proposed approach be adapted or extended to accommodate and integrate with other SSL frameworks beyond those mentioned in the paper, and if so, how?\n3. Have the authors considered the computational overhead introduced by the kernel-based feature selection methodology, especially in the context of large-scale real-world applications?"}}
{"paper_id": "HwQ8NVvdmm", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel approach to making continual learning more feasible in resource-constrained environments through fully quantized training. The use of Hadamard transforms with quantization addresses the challenge of maintaining accuracy while reducing computational needs. The approach effectively demonstrates a reduction in memory usage and energy consumption, showing potential applicability to edge devices. The evaluation on CIFAR100 and multiple HAR datasets against existing methods shows promising results with minimal loss in accuracy, indicating the method's potential utility in practical applications.", "Weaknesses": "The integration of Hadamard transforms with quantized training, while innovative, may still face challenges related to hardware implementation and real-world deployment, particularly regarding stochastic rounding. The paper might have benefited from a more comprehensive comparison with other quantization and continual learning methodologies to firmly establish the superiority of the proposed method. The lack of exploration into different types of datasets or broader experimentation with diverse architectures limits the generalizability of the results presented.", "Questions": "How does the proposed HDQT compare with existing quantization methods when applied outside of continual learning contexts, such as standalone model training? Are there specific limitations in implementing this approach on current edge device hardware, particularly concerning the stochastic rounding process? Can the approach be adapted to handle more complex neural network architectures or larger scale datasets while maintaining the same levels of efficiency?"}}
{"paper_id": "cZo6pDtDZr", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper addresses significant limitations in previous work related to locally differentially private estimation of collision probability, offering improvements particularly in sample efficiency.\n2. The introduction of novel algorithms provides both practical and theoretical advancements, such as sequential testing which can adapt to privacy parameters not being pre-specified.", "Weaknesses": "1. The presentation lacks depth in explaining the intuition behind certain methodological choices, such as the hashing and bias correction steps.\n2. There is limited experimental validation to support the claims of reduced sample complexity and the practical applicability of the sequential testing algorithm.", "Questions": "1. How does the proposed estimation algorithm perform compared to non-LDP methods in terms of accuracy and sample efficiency?\n2. Can the sequential testing algorithm be generalized or adapted to other problems or domains where privacy parameters are unknown or difficult to define?"}}
{"paper_id": "F7QnIKlC1N", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The proposed GTMGC model based on Graph Transformers and a novel self-attention mechanism demonstrates potential in efficiently predicting 3D ground-state conformation of molecules from 2D structures. The method's innovation in using MSRSA appears promising for preserving both global and local molecular structures, and the model shows improved performance compared to existing methods. Additionally, the study contributes to the field by integrating 3D information with graph neural networks, showcasing its application in property prediction alongside conformational predictions. ", "Weaknesses": "1. The novelty of the MSRSA within the context of existing self-attention frameworks might require further delineation since several graph-based transformers may incorporate adjacency and distance matrices in some form. \n2. The reliance on benchmark datasets like Molecule3D and QM9 might have inherent biases, and demonstrating the model's robustness across more diverse molecular datasets could strengthen the findings.\n3. While the model's performance against RDkit and similar tools is noted, more detailed comparative analyses against state-of-the-art neural network-based models might be necessary to conclusively ascertain competitive advantages. ", "Questions": "1. How sensitive is the model's performance to the choice of the Laplacian Positional Encoding? Would alternatives yield similar results?\n2. In what ways can the MSRSA mechanism be adapted or generalized for other graph-based problems outside of the molecular domain?\n3. Given that the GTMGC model outperformed baselines in conformational prediction, how does it fare in predicting other molecular properties of interest beyond QM9, and what metrics were used for these predictions?"}}
{"paper_id": "8JCn0kmS8W", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents an innovative use of Large Language Models (LLMs) to orchestrate multi-modal audio generation for storytelling, addressing a gap in compositional audio creation from textual descriptions. WavJourney leverages different expert audio models without additional training, and the system demonstrates strong performance on text-to-audio benchmarks, offering potential in interactive and co-creative AI applications.", "Weaknesses": "The approach appears to emphasize application over method innovation, relying heavily on existing LLMs and other audio models rather than introducing novel algorithms. The paper lacks in-depth comparisons with alternative multi-modal or agent-based systems that integrate LLMs with other task-specific models.", "Questions": "What are the limitations in processing or generating more complex or nuanced audio elements? How adaptable is the method to emerging LLMs or audio models? Have there been attempts to compare WavJourney's framework with other agent-based systems or pipelines that incorporate LLMs for multi-modal generation?"}}
{"paper_id": "XbLffB0T2z", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper addresses an important problem of improving the transferability of data poisoning attacks across different learning paradigms, which is relevant in securing machine learning systems.\n2. The proposed method, Transferable Poisoning, is innovative in using both supervised and unsupervised contrastive learning paradigms to generate high-frequency poisoning perturbations.\n3. The experimental results on benchmark datasets such as CIFAR-10, CIFAR-100, and TinyImageNet demonstrate the effectiveness of the method, showing significant improvement in transferability compared to existing methods.", "Weaknesses": "1. The paper could benefit from more detailed explanation of the underlying mechanism that ensures high-frequency perturbations lead to increased transferability across learning paradigms.\n2. While the use of both supervised and unsupervised methods is noted, a deeper analysis or ablation study could help clarify the specific contribution of each part of the approach to the overall performance improvement.\n3. The presentation, while generally clear, might be enhanced by better visual aids or diagrams to illustrate the process of perturbation generation and the role of contrastive learning.", "Questions": "1. How does the proposed method ensure that high-frequency perturbations remain effective across different datasets and not just within the tested benchmarks?\n2. Can this approach be generalized beyond the specific datasets used, such as to text or other modalities?\n3. Is there a scalability consideration when applying the Transferable Poisoning method to larger datasets or models, and how does it affect the time or computational resources required?"}}
{"paper_id": "4y3GDTFv70", "review": {"Soundness": 2, "Presentation": 3, "Contribution": 2, "Rating": 4, "Confidence": 3, "Strengths": "The paper tackles an important question about the emergent abilities of LLMs, providing a novel hypothesis using Bayesian inference which could potentially enhance understanding in this area.", "Weaknesses": "The theoretical framework proposed is complex and may be too abstract to be practically applicable or empirically testable with current tools. There is also a lack of clarity in how the proposed latent space theory can be operationalized and validated beyond synthetic simulations.", "Questions": "Could the authors elaborate on how this theory might be empirically validated using real-world LLMs, beyond synthetic data simulations?"}}
{"paper_id": "ODSgo2m8aE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper addresses an important challenge of bias and stability in GNNs, proposing a novel approach using Lipschitz bounds.\n2. The method is integrated into existing GNN training pipelines, facilitating practical application without significant computational overhead.\n3. Empirical results show improved fairness and stability of GNN outputs, supporting the effectiveness of the approach.", "Weaknesses": "1. The computation of Lipschitz bounds via the Jacobian matrix might not always accurately represent all cases where biases affect GNNs, potentially leading to over- or under-estimations.\n2. It is unclear if the approach generalizes well to all types of graph structures and datasets, as experiments are limited to specific tasks and datasets.\n3. While theoretically informed, the practical impact and scalability to very large graphs remain somewhat unclear.", "Questions": "1. How does the method handle extremely large graph datasets in practice, where the computation of the Jacobian might be computationally intensive?\n2. Are there specific types of biases in the data that JacoLip is more effective at mitigating compared to others?\n3. How does JacoLip compare in performance and computational cost against other methods of bias mitigation specifically designed for GNNs?"}}
{"paper_id": "RlfD5cE1ep", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 3, "Strengths": "The paper provides significant theoretical advancements in understanding the dynamics of non-contrastive learning by incorporating feature normalization into the dynamic analysis. The use of cosine loss is well-motivated and aligns with practical implementations, leading to valuable insights into how normalization influences representation stability. The eigenmode dynamics analysis is a novel approach that effectively demonstrates how higher-order dynamics induced by cosine loss contribute to robust learning. Additionally, the findings are validated through numerical simulations, providing empirical support for the theoretical claims.", "Weaknesses": "One of the weaknesses lies in the presentation, which could be improved for broader accessibility, especially for readers not deeply familiar with advanced dynamic analysis techniques in self-supervised learning. The paper could benefit from clearer explanations and more intuitive descriptions of the mathematical concepts used, making it more approachable to a wider audience. The numerical simulations are relatively limited in scope, primarily focusing on synthetic data and the CIFAR-10 dataset. Providing a more extensive empirical evaluation on diverse datasets could strengthen the practical implications of the findings. Furthermore, the analysis relies heavily on theoretical assumptions, and additional insights into practical considerations, such as computational efficiency and scalability, would enhance the contribution of the work.", "Questions": "1. Have you considered evaluating the proposed theoretical framework using additional datasets beyond CIFAR-10 to further substantiate its applicability to various domains in practical settings?\n2. Are there plans to explore the implications of the findings on more complex architectures beyond ResNet-18 and investigate whether the observed dynamics hold across a broader range of model architectures?\n3. Could you provide more detailed intuition or visualizations to help readers understand the transition between third-order and sixth-order dynamics and how this transition contributes to stabilizing the representation space?"}}
{"paper_id": "18TfucMNTr", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 5, "Confidence": 3, "Strengths": "The paper introduces a continuation method with Gaussian convolution that addresses saddle point issues and stabilizes training in deep networks, showing improvements in convergence and accuracy. It combines theoretical insights with empirical validation on benchmark tasks.", "Weaknesses": "The implementation details are sparse, particularly in how the regularization is effectively applied and tuned. The application to deep learning benchmarks is limited, with only MNIST and neural ODEs considered, which may not reflect broader applicability.", "Questions": "How is the regularization parameter chosen, and does it require significant tuning across different tasks? Can this method be applied to other deep learning tasks beyond MNIST and neural ODEs, such as large-scale datasets?"}}
{"paper_id": "JGTC6WgO4T", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents a novel approach to multi-source black-box domain adaptation by refining pseudo labels, which potentially addresses key challenges in traditional black-box domain adaptation. The proposed framework is theoretically supported and empirically validated on benchmark datasets, showing competitive performance.", "Weaknesses": "The explanation of the method can be quite dense and challenging to follow. There might be a lack of clarity in how the pseudo-label refinery network operates precisely, particularly with the practical implementation and the assumptions made for deriving theoretical support. Furthermore, while the paper appears to perform well on tested datasets, it lacks exploration of whether the proposed method transfers well to very different domains not covered by the benchmarks used.", "Questions": "1. How does the method handle potential conflicts or inconsistencies in pseudo labels when derived from multiple source domains?\n2. Could the authors elaborate more on the computational efficiency of the PRN in practical settings?\n3. What measures are taken to ensure the diversity and relevance of the multiple sources selected for the adaptation process?"}}
{"paper_id": "fQHb1uZzl7", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel unified framework integrating feature and cost aggregation using Transformers, which is a significant contribution to dense matching tasks in computer vision. The method demonstrates robust and efficient performance, particularly under challenging conditions like large intra-class variations. The use of multi-scale predictions and confidence scores for selecting the most accurate predictions is an innovative approach.", "Weaknesses": "The method's real-world applicability might be limited without additional details on computational complexity, especially given the use of Transformer architecture. The approach may require substantial computational resources, which could affect its usability in scenarios where resources are limited. Moreover, there is a possibility that the integration of the two aggregation types might not always lead to improved performance for all types of dense matching tasks.", "Questions": "How does the proposed UFC architecture compare computationally with existing state-of-the-art methods in terms of resources required? Are there any scenarios or datasets where the UFC method's performance does not surpass current methods? If so, what might be the cause of these limitations?"}}
{"paper_id": "YjG29CIOP7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper effectively addresses significant vulnerabilities in DFML such as Task-Distribution Shift (TDS) and Task-Distribution Corruption (TDC), making it a valuable contribution to improving robustness in DFML settings. It introduces innovative approaches, TEAPOT and ROSY, to combat these challenges, offering a promising alternative for situations where data privacy is a concern and only pre-trained models are available. The comprehensive experiments on a range of datasets provide a convincing demonstration of the approach's effectiveness.", "Weaknesses": "The paper could have benefited from clearer explanations or more intuitive descriptions, particularly regarding the intricacies of the proposed methods, such as TEAPOT and ROSY. The reinforcement learning approach to model selection in ROSY, while innovative, might introduce additional computational complexity, which could be a concern for real-world scalability. Additionally, it is unclear how well the method generalizes across different domains without further empirical results. Potential limitations or edge cases where the method might not perform as well should be explored further.", "Questions": "How do the computational costs of implementing ROSY compare to simpler methods or other model selection strategies in similar scenarios? Are there particular domains or problem types where TEAPOT and ROSY show significant performance drop-offs? Could further insights be shared on how TEAPOT decides when to replay old tasks, and how this affects long-term learning stability?"}}
{"paper_id": "DmDpu3r8iU", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper addresses a significant problem, which is understanding the alignment of LLMs with human values, a topic of growing interest and importance.\n2. Introduction of the Value Understanding Measurement (VUM) framework and discriminator-critique gap (DCG) approach shows novel consideration in evaluating 'know what' and 'know why' dimensions of value understanding.", "Weaknesses": "1. There is a lack of clarity in the descriptions of the methodologies employed, particularly surrounding the specific processes of discriminator-critique evaluations, which leaves room for misinterpretation.\n2. Limited practical examples or illustrative explanations of the proposed hypotheses and methodologies within the paper make it difficult to thoroughly understand the applicability and limitations of the VUM framework.\n3. The paper does not sufficiently address potential biases inherent in using pre-annotated datasets from GPT-4 or how these might affect the results.", "Questions": "1. What measures are taken to ensure that the annotation provided by GPT-4 for the dataset is unbiased and truly reflective of human values?\n2. How do the authors plan to extend this work towards practical use cases, or integration into existing evaluation metrics for LLMs beyond the theoretical framework presented in this study?\n3. Could the authors provide more clarity on how scaling laws affect 'know what', and in which precise aspects they impact LLMs differently than 'know why'? Would additional model parameters apart from scaling introduce any variants in observations?"}}
{"paper_id": "kXHEBK9uAY", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The proposed Hierarchical Diffuser improves efficiency and generalization by leveraging a hierarchical planning structure, incorporating both high-level (subgoal generation) and low-level (subgoal achievement) diffusers.\n\nThe method outperforms existing diffusion-based and hierarchical planning methods on standard offline RL benchmarks, demonstrating better training and planning speeds as well as enhanced generalization capabilities for compositional out-of-distribution tasks.\n\nThe hierarchical approach enables efficient planning for longer horizons, addressing the challenges faced by flat, dense diffusion models in these scenarios.", "Weaknesses": "The formulation is not entirely novel, as it primarily builds on existing concepts of diffusion-based and hierarchical planning. The novelty mainly lies in the integration and implementation of these concepts to improve decision-making in long-horizon tasks.\n\nDetails on how different components of the proposed model interconnect and the specific training methodologies for each diffuser could be more thoroughly explained to enhance reproducibility and understanding.\n\nThere is a concern about the handling of sub-goals that do not align perfectly with the low-level goals, particularly in cases where discretized sub-goals may not be directly achievable with given actions.", "Questions": "1. How is the transition between sub-goals managed in cases where actions cannot directly achieve the designated sub-goal due to the environment's constraints?\n\n2. Are there any specific challenges encountered in training the Sparse Diffuser versus the low-level diffuser? If so, how are these addressed?\n\n3. How does the performance of Hierarchical Diffuser compare to state-only diffusion methods like Decision-diffuser? Are there any specific scenarios where Hierarchical Diffuser might underperform?"}}
{"paper_id": "i7LCsDMcZ4", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper presents a novel approach for generating saliency and class activation maps for Spiking Neural Networks (SNNs), which addresses a key gap in the field. 2. The proposed EventRPG method significantly improves the generalization ability of SNNs on event-based tasks, achieving state-of-the-art performance. 3. The two data augmentation techniques, RPGDrop and RPGMix, are innovative and effectively utilize the saliency maps for enhancing classification tasks.", "Weaknesses": "1. The application of the proposed methods is currently limited to classification tasks, which may limit their broader applicability. 2. The approach requires an understanding and integration of complex techniques such as relevance propagation, which may not be straightforward for all researchers to implement. 3. Presentation can be improved, particularly in terms of providing clarity on the integration of the proposed methods with existing frameworks and in-depth comparisons with similar approaches.", "Questions": "1. Can the relevance propagation techniques (SLTRP and SLRP) be extended to other types of neural tasks beyond classification? 2. How do the augmentation methods specifically compare in terms of computational efficiency and resource requirements with more conventional approaches like Mixup or standard geometric transformations? 3. Are there any limitations in the event types that the proposed techniques can handle, and if so, what are the conditions under which they perform optimally?"}}
{"paper_id": "jHdz0CIS2y", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper presents an innovative approach to voice conversion by effectively utilizing entangled speech representations combined with self-synthesized examples for training, thus improving performance in zero-shot voice conversion tasks.\n2. The method demonstrates applicability across various languages and conversion scenarios, showing competency in both zero-shot and cross-lingual voice conversion tasks.\n3. The integration of prosodic information without relying on text, along with self-synthesized examples, significantly enhances speaker similarity and quality, achieving state-of-the-art results.", "Weaknesses": "1. While the method demonstrates effectiveness, the reliance on self-supervised learning models and speaker verification models may introduce limitations in terms of resource availability and required computational power.\n2. The paper could further discuss potential limitations or boundary conditions where the proposed method might not perform optimally.\n3. There is less emphasis on the comparative analysis of how the method holds against a broader set of baseline models, particularly in more diverse linguistic contexts.", "Questions": "1. Could the authors provide more insights on how the self-synthesized examples specifically contribute to improving model performance compared to traditional augmentation techniques?\n2. How generalizable is the proposed method to languages with less training data available for the self-supervised learning models?\n3. Are there specific scenarios or types of speech data where the method might not perform as well, and how could it be adapted to handle such cases?"}}
{"paper_id": "NkYCuGM7E2", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. The integration of LLMs for high-level decision-making in autonomous driving systems introduces a novel approach to leverage human-like reasoning, potentially improving safety and adaptability.\n2. The paper presents a clear framework for utilizing chain-of-thought processes, allowing the LLM to systematically address sub-problems.\n3. Experiments demonstrating improvements in traffic flow and reduced safety penalties add empirical support to the proposed approach.\n4. The paper is well-organized, with a clear presentation of methods and experimental results.", "Weaknesses": "1. The effectiveness and limitations of using LLMs in real-time, safety-critical applications like autonomous driving are not thoroughly explored. Concerns about LLMs' reliability and their potential randomness are not addressed.\n2. The model's ability to deal with rare and long-tail events is not convincingly demonstrated with comprehensive or varied examples.\n3. Comparisons with few baseline methods limit the generalizability and robustness of the conclusions drawn about the system's performance improvements.", "Questions": "1. How does the system ensure consistent and reliable outputs from LLMs given potential randomness in their responses?\n2. Can the authors elaborate on how the proposed framework handles rare and long-tail events, especially in scenarios not covered in the experiments?\n3. What measures are taken to maintain the real-time performance necessary for decision-making in dynamic driving environments with the LLM integration?"}}
{"paper_id": "RtDok9eS3s", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The proposed modifications to transformer blocks provide a noticeable improvement in training efficiency and parameter reduction while maintaining model performance.\n2. The paper clearly articulates the modifications made to the transformer architecture and the rationale behind these changes, providing a thoughtful exploration of how to streamline a widely used architecture.\n3. The study uses comprehensive experiments to validate the efficacy of their approach across different transformer models, including both encoder-only and decoder-only setups.", "Weaknesses": "1. The generalizability of the results to larger models or more diverse datasets remains to be seen, as experiments were primarily conducted on smaller model sizes (~100M parameters) and limited to specific datasets like CodeParrot and GLUE.\n2. The paper could benefit from a more thorough exploration of potential trade-offs in model expressiveness or performance across a broader range of NLP tasks not covered in the current scope.", "Questions": "1. How well do these simplified transformer architectures perform when scaled to larger model sizes or across a wider range of NLP tasks beyond those presented in the paper?\n2. Are there scenarios or applications where the reduced complexity may lead to a noticeable drop in performance compared to traditional transformer architectures?\n3. How do the authors envision these architectural changes impacting the future development and deployment of transformer models, especially in resource-constrained environments?"}}
{"paper_id": "YCPDFfmkFr", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper addresses a critical issue in neural network training involving optimization layers, specifically tackling the problem of infeasibility in quadratic programming layers, crucial for advancing applications in dynamic tasks.\n2. It introduces an innovative method leveraging primal-dual augmented Lagrangian techniques, enabling differentiation even through infeasible quadratic programming layers, thereby enhancing the modeling expressiveness of neural networks.\n3. The implementation of the QPLayer package provides a practical contribution to the field, demonstrating its utility on real tasks and interfacing with common learning frameworks.", "Weaknesses": "1. While the proposed method shows promising results, the lack of comparison with straightforward alternatives like introducing slack variables or penalty terms for handling infeasibility issues may limit the depth of the presented analysis.\n2. The paper might benefit from more detailed explanations or guidelines for selecting hyperparameters within the proposed framework, which could enhance usability for practitioners.\n3. There is limited exploration of potential limitations or edge cases where the proposed method might struggle; more comprehensive testing scenarios could strengthen the claim of its robustness.", "Questions": "1. How does the approach compare with more straightforward methods, like using slack variables or penalties, in terms of efficiency and ease of integration into existing neural network architectures?\n2. Can the authors provide more insights or guidelines on hyperparameter selection within the proposed framework to facilitate its adoption?\n3. Are there specific types of problems or conditions where the proposed method might not perform as well, and could the authors discuss potential solutions to those challenges?"}}
{"paper_id": "ueTdErd5Ib", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel approach to contextual stochastic optimization that integrates learning and optimization to achieve robustness against uncertainty. The method shows theoretical guarantees with bound regret and demonstrates practical advantages in experiments, including in a real-world electricity generation problem.", "Weaknesses": "The presentation of the method and the experimental results could be more detailed. The explanation of the discretization and secondary optimization integration is somewhat lacking in detail, making it difficult to fully grasp the mechanism and effectiveness of the method.", "Questions": "How does the discretization of the feasible region ensure that the theoretical guarantees hold? Are there particular types of optimization problems for which this approach is particularly well-suited or not well-suited?"}}
{"paper_id": "sDmjlpphdB", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The introduction of Mixture-of-Prompts (MoP) as a novel approach to optimize prompts by dividing the problem space into manageable regions is quite ingenious and highlights potential for improved task performance. The paper's results showcasing up to 43% increase in performance compared to previous methods emphasizes the novelty and efficacy of this method.", "Weaknesses": "One of the main weaknesses is the assumption that the problem space can be effectively divided into homogeneous regions manageable by separate experts and the implicit complexity such a division entails in practical applications. The methodology may also face scalability issues as the number of regions and experts grow. Additionally, there is a potential limitation in the specific criterion for clustering tasks and the specificity of logger selection for diverse NLP tasks.", "Questions": "How does MoP handle edge cases where certain tasks or demos do not naturally fit into one of the pre-defined homogeneous regions? Is there a fall-back mechanism in place for cases where the assigned expert underperforms in a certain task region? Additionally, what considerations or safeguards were put in place to balance between the number of experts (and regions) versus computational efficiency?"}}
{"paper_id": "Mdk7YP52V3", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 3, "Strengths": "The paper provides a novel theoretical approach using statistical physics to explain issues in overparameterized heteroskedastic models, offering potential insights applicable to various architectures and datasets. The introduction of a nonparametric free energy framework and the empirical validation of the theoretical findings strengthen the paper's contributions.", "Weaknesses": "The presentation could be improved to provide clearer explanations and insights, especially for readers not familiar with statistical physics or the specifics of field-theoretical approaches. While novel, the theoretical framework might benefit from more intuitive explanations or analogies to make it more accessible to a broader audience.", "Questions": "How well does the proposed nonparametric free energy framework generalize to different types of real-world datasets beyond those tested? Are there any limitations observed with specific types of empirical data or particular neural network architectures?"}}
{"paper_id": "YGTSLDAPqb", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 5, "Confidence": 4, "Strengths": "- The paper addresses a very relevant problem in domain adaptation, particularly concerning OOD data where model performance often degrades significantly, an issue common in fields such as astronomy.\n- The 'Connect Later' framework presents a novel approach by proposing the use of targeted augmentations during the fine-tuning phase, which seems to effectively address the performance issues of existing methods.\n- The proposed method has been validated across multiple domains, demonstrating significant improvements in OOD performance, which could have practical implications beyond the fields studied.\n- The study provides experimental evidence supporting the hypothesis, showcasing improved performance metrics across a variety of benchmark datasets.", "Weaknesses": "- While the paper proposes a novel approach, it lacks a detailed theoretical explanation or justification for why the 'Connect Later' framework should outperform existing methods within the context of domain adaptation.\n- The comparisons made with other methods are interesting; however, the paper could benefit from a more comprehensive analysis covering broader variations of previous techniques to clearly highlight why this method excels.\n- The paper does not delve deeply into the limitations or potential drawbacks such as increased computational costs, potentially complex processes in creating targeted augmentations, or scalability issues in certain applications.\n- More discussion on the potential limitations or biases introduced by the simplistic or domain-specific nature of targeted augmentations might provide a more balanced critique of the approach.", "Questions": "- Could the methodology be applied successfully to other domains or distinct types of data? Particularly, could it be employed on smaller datasets where pretraining might not be feasible due to lack of sufficient data?\n- How labor-intensive is the process of designing targeted augmentations for each specific application, and could this aspect limit the scalability of the method across new domains?\n- To what extent is the observed improvement attributable specifically to the tailored augmentations strategy, versus the intrinsic advantages due to the pretraining approach?\n- How does the approach perform against unsupervised domain adaptation methods that do not rely on labeled source data?"}}
{"paper_id": "JTcaziw7G1", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper addresses an important and current problem of ensuring privacy in information retrieval for augmenting large language models, which is increasingly relevant as data distribution and privacy regulations become more complex.\n2. The proposed PRAG framework presents an innovative combination of multi-party computation and secret sharing techniques, contributing a novel MPC-friendly protocol for inverted file approximate search.\n3. The experiments demonstrate the method's potential by providing privacy without compromising retrieval capabilities, indicating practical utility especially for sensitive data applications.", "Weaknesses": "1. The computational overhead and costs are higher compared to traditional methods, which may limit practical deployment without further optimization.\n2. The method's reliance on synthetic datasets may not fully represent real-world complexities and constraints, potentially affecting generalizability.\n3. Some implementation details of the MPC protocol and handling of potential inefficiencies are not thoroughly explored, which could be crucial for practitioners looking to adopt the method.", "Questions": "1. Can real-world datasets of different structures and scales be tested to further validate the performance and efficiency of PRAG?\n2. What are the prospects for optimizing the computational costs, and does the proposed method scale well with growing data and server networks?\n3. Are there any specific privacy trade-offs that the PRAG approach necessitates, such as potential data utility reductions or increased latency?"}}
{"paper_id": "lEkFq4RUCX", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The proposed Directional Distance Field (DDF) offers a novel way to measure the difference between 3D point clouds, focusing on geometric surface differences rather than point-wise correspondences.\n2. The method showcases improvements in accuracy across multiple 3D tasks such as shape reconstruction, registration, and scene flow estimation.\n3. It provides a more memory-efficient and computationally effective solution compared to traditional metrics like EMD and CD.", "Weaknesses": "1. The explanation of how reference points are generated and used for directional distances could be clearer and more detailed.\n2. Some comparisons with existing metrics might lack depth or could benefit from additional datasets or practical scenarios.", "Questions": "1. How are reference points specifically generated from the point clouds, and what criteria determine their selection?\n2. Could you provide more clarity on how DDF handles different sampling scenarios within 3D point clouds?\n3. How does DDF integrate with existing 3D point cloud processing frameworks, and are there limitations to its implementation?"}}
{"paper_id": "Fq8tKtjACC", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "1. The paper demonstrates a novel approach of utilizing high-quality, 'textbook quality' data to train a smaller neural network achieving superior performance, which challenges the current paradigm of simply increasing scale.\n2. Efficient utilization of resources is shown, achieving state-of-the-art results with significantly less computational power and data volume.\n3. The paper is well-organized and presents its findings clearly, making a compelling case for data quality over quantity.\n4. Offers potential applications in domains where computational resources are limited, promoting more sustainable AI development practices.", "Weaknesses": "1. The model seems to have specific limitations in its application to Python, which might restrict its generalizability to other programming languages without further modifications.\n2. There is sensitivity to prompt variations, which could affect consistent performance, tipping a balance between customization and robust model behavior.\n3. Some critical details concerning data processing and model architecture are proprietary and not completely disclosed, hindering full reproducibility by others.", "Questions": "1. How does the model's performance on non-code or more natural language processing tasks compare to the specialized success in programming tasks?\n2. Could the methodology of utilizing high-quality data be extended effectively to language models in other domains, or is there something unique about the training on code?\n3. Alongside Python, are there plans to extend or adapt phi-1 to support other programming languages or to address the limitations identified with prompt sensitivity?"}}
{"paper_id": "8fQlGQkj0S", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper provides a novel theoretical framework that distinguishes between task learning and task retrieval within in-context learning, a topic that has not been thoroughly analyzed before.\n2. By introducing generative models for pretraining data and in-context samples, the authors offer a new approach to evaluating risk and performance in language models.\n3. The experimental validation on Transformers adds credence to the theoretical findings, showing practical relevance.", "Weaknesses": "1. The generative modeling approach, while insightful, might be overly simplistic for capturing the full complexity of language model pretraining data.\n2. The reliance on Gaussian mixture models assumes a certain level of homogeneity in task distributions that may not align with real-world data complexities.\n3. The paper assumes access to clean task distributions, which might not be practically feasible in many scenarios.", "Questions": "1. How sensitive are the results to the assumption that pretraining data is drawn from noisy task distributions centered around clean tasks?\n2. Can the proposed methodology be extended beyond Gaussian mixture models to more complex distributional assumptions?\n3. How does the approach handle cases where the underlying task distributions are not well-represented by the chosen generative models?"}}
{"paper_id": "hRos9WldRK", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The method provides a novel approach to handle label noise using a meta-learning framework that does not rely on a clean validation set, making it widely applicable. The empirical results demonstrate significant improvement over baseline methods across various datasets, showcasing its efficacy. Additionally, the study's integration with existing techniques highlights its practicality as a complementary solution.", "Weaknesses": "The approach may still face limitations when applied to real-world datasets where noise characteristics differ greatly from synthetic datasets used in experiments. While empirical results are compelling, the reliance on meta-learning frameworks might introduce complexity in model training and deployment.", "Questions": "How does the method performance compare when dealing with datasets exhibiting systematic noise, rather than random noise? Does the meta-learning framework significantly increase computational requirements, and how does it affect training times compared to traditional methods?"}}
{"paper_id": "ttRSBZiCiu", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "WaveFluid presents an innovative approach to audio synthesis by learning a velocity field through adversarial training, which avoids some of the typical constraints found in other models. The claim of achieving comparable audio quality with significantly fewer inference steps is a strong advantage, especially in terms of computational efficiency. The integration of modular design features such as attention blocks enhances the model's performance capabilities.", "Weaknesses": "The method's reliance on modular design and adversarial training can add complexity and may require extensive hyperparameter tuning. While the paper claims competitive performance, detailed comparative analysis against a wider array of baseline models, particularly state-of-the-art GANs, could strengthen the experimental validation of the proposed approach. The paper's explanation of the probabilistic refiner stage could benefit from further clarity and detail.", "Questions": "How does the training and inference speed of WaveFluid compare quantitatively against state-of-the-art flow-based and diffusion models? Are there specific scenarios or datasets where WaveFluid's performance significantly exceeds that of comparable models, and what might be the contributing factors?"}}
{"paper_id": "WYsLU5TEEo", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The proposed framework effectively integrates interpretability and adversarial robustness in neural network classifiers, addressing a significant gap in the current methodologies. The use of GANs for generating counterfactuals and adversarial samples is innovative and shows promising results on classification tasks. The paper illustrates its approach with effective use of specialized GAN components and evaluates performance using relevant datasets, contributing to practical advances in image classification robustness.", "Weaknesses": "The framework is tested only on binary classification tasks, limiting its demonstrated applicability. The paper does not fully explore or demonstrate its performance across multiclass classification tasks, which are common in real-world applications. Additionally, while the framework integrates interpretability and robustness, the extent to which each component individually contributes to performance improvement is not thoroughly dissected, potentially limiting insights into the framework's functional contributions.", "Questions": "How scalable is the proposed framework when applied to multiclass classification tasks? What adjustments would be necessary to adapt it to such contexts? Additionally, although the framework claims to achieve competitive IoU scores compared to traditional methods, more details on specific metrics and comparative baselines would help in evaluating its advantages."}}
{"paper_id": "KKZaj2QS3G", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The method uses a novel combination of dilated convolution and inception blocks, showcasing good architectural design for capturing varied receptive fields.\n2. The application of a noise-resilient sampling strategy using a spectrum-based low-pass filter for robust representation learning is innovative.\n3. The proposed method is shown to outperform existing state-of-the-art approaches and is more parameter efficient, highlighting its scalability.", "Weaknesses": "1. The paper doesn't fully explore the generalizability of the approach across different domains; the results are primarily limited to given benchmark datasets.\n2. Although the method is innovative, the integration and effectiveness of the low-pass filter might not be fully justified beyond certain types of noise.\n3. The combination of these methods seems complex, and it’s not clear if simpler models could achieve similar benefits with better interpretability.", "Questions": "1. How does the noise-resilient sampling strategy handle varying levels of noise, particularly when the noise characteristics are unknown or variable over time?\n2. Were there any specific challenges faced while integrating the inception block with dilated convolutions?\n3. Can the method be adapted for real-time applications, and if so, what are the potential computational constraints?"}}
{"paper_id": "LWDRiFzbHQ", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "- The proposed method, Vicinal Risk Proxy (VRP), offers a novel approach to improve the reliability of out-of-distribution generalization assessments by integrating the surrounding test samples' responses.\n- The paper clearly highlights the weaknesses of current OOD methods and proposes a potentially effective solution to address spurious model responses.\n- The use of multiple datasets and test scenarios to demonstrate the effectiveness of VRP helps in reinforcing the validity of the proposed method.\n- Clear representation and explanation of how VRP smooths out the spurious scores, alongside thorough empirical analysis support the results.", "Weaknesses": "- While the paper introduces a new way to calculate risk proxies, it may require significant computational resources due to the need to evaluate neighboring samples' responses. This could potentially limit its applicability to large datasets or real-time applications.\n- The integration of neighboring sample information, while novel, may introduce complexities or unintended biases depending on the test dataset distribution.\n- It seems that the VRP approach, while promising, might not fully eliminate the reliance on test labels for true generalization evaluation—there is a possibility the proxy might still fall short under certain conditions not explicitly tested in the paper.", "Questions": "- To what extent does the improvement in correlation offered by VRP justify the increased computational cost? Has this been measured or discussed?\n- How does VRP perform across datasets with highly varying distributions, and are there limitations in specific patterns it may not handle well?\n- Given that neighboring test samples are vital to the VRP approach, how does it deal with datasets where such samples are not inherently similar, or where the similarity metrics themselves might be biased or imprecise?"}}
{"paper_id": "Yp01vcQSNl", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The DiGT model introduces a novel approach to handling directionality in graphs using dual encodings and dynamic attention mechanisms, which fills a notable gap in current graph transformer architectures. The method is well-validated on newly designed datasets that highlight the importance of edge directionality.", "Weaknesses": "The method's reliance on novel datasets suggests potential overfitting to specific scenarios where edge directionality is crucial. There might be limited applicability to general undirected graph problems or datasets with less pronounced directionality.", "Questions": "How does DiGT perform on standard benchmarks where directional information is less critical? Could elements of DiGT be integrated into existing graph transformer models for a broader range of graphs?"}}
{"paper_id": "LndMyiBl3n", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. The introduction of the Modified Silhouette Score (MSS) provides a novel approach for assessing silhouette scores in heterogeneous graph environments, which is a valuable contribution to GNN adversarial attack literature.\n2. SheAttack effectively addresses the limitations of homophily assumptions and expands applicability to various graph types, demonstrating its adaptability and efficacy.\n3. The methodology of employing Greedy Randomized Block Coordinate Descent is efficient, making the approach scalable and effective without relying on in-depth prior information about the model being attacked.", "Weaknesses": "1. The novelty of using silhouette scores has limitations, as it primarily builds on existing concepts rather than introducing groundbreaking new methods.\n2. The paper may need further elaboration on the theoretical underpinnings of why Modified Silhouette Score (MSS) effectively generalizes across different graph structures.\n3. While comprehensive, the relations between experimental results and methods used could be more delineated to enhance understanding of performance differentials across datasets.", "Questions": "1. How sensitive is the Modified Silhouette Score to varying parameters, and is there a recommended method for setting these parameters for new unknown datasets?\n2. Can SheAttack be further adapted or extended to attacks beyond structural modifications, such as feature perturbations, and if so, how?\n3. Given the introduction of MSS as a primary tool, are there potential biases or limitations in how MSS may interpret structural changes in differing graph contexts?"}}
