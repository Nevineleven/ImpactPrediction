{"paper_id": "u3xwwfHmBC", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The proposed TTformer model introduces a novel approach by utilizing tense-specific attention modules to address spatio-temporal dependencies, which is a fresh perspective in the domain of traffic forecasting.\n2. Integration of contrastive learning techniques to handle incomplete data is a strong aspect of the paper, potentially improving robustness.\n3. The model has shown improved performance over baseline models on real-world traffic datasets, indicating practical applicability.", "Weaknesses": "1. The complexity of the model might limit its scalability and practical deployment due to increased computational costs associated with the tense-specific modules and contrastive learning.\n2. The paper lacks a detailed discussion on the underlying mechanisms of the tense-specific attention modules, which can make it challenging for readers to understand the distinct contributions of each module.\n3. There might be a dependency on hyperparameter tuning for optimal performance, which is not thoroughly explored or documented in the report.", "Questions": "1. How does the model handle scenarios where the traffic data is extremely sparse or highly noisy? Are there any specific strategies employed for such cases?\n2. Can the proposed method be extended to other domains that require spatio-temporal analysis, or is it specifically tailored for traffic forecasting?\n3. What are the main computational overheads introduced by the TTformer, and how do they compare to those of existing models, particularly in terms of inference time?"}}
{"paper_id": "YKvBiRWdQC", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The Overcooked Generalisation Challenge (OGC) effectively addresses the significant limitation in existing benchmarks by focusing on the generalisation capabilities of RL agents. The paper successfully introduces a novel environment and evaluation scenario for cooperative multi-agent RL, emphasizing zero-shot cooperation abilities. The use of unsupervised environment design (UED) and incorporation of diverse dual curriculum design methods manifest innovation and advancement in benchmark development.", "Weaknesses": "While the introduction of OGC is commendable, the paper lacks comprehensive detail on the observation space and potential limitations regarding assumptions of environment conditions. There is a need for deeper analysis and explanation of why state-of-the-art DCD algorithms are struggling with the OGC benchmark. The paper could benefit from more detailed experimental results, including performance on different unseen layouts and an analysis of training curves.", "Questions": "What specific measures are taken to ensure that the OGC benchmark remains relevant as AI capabilities advance? Additionally, how can the community contribute to updating or expanding the challenge in the future? Further, could you provide more details about the observation space, particularly for collaborators less familiar with Overcooked?"}}
{"paper_id": "tePFpDgyqg", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The LongPack data pipeline innovatively leverages natural referral relationships, such as hyperlinks, to better simulate the natural complexity of long-document contexts. This approach addresses a noted gap in the training of language models by circumventing the ineffective concatenation of short documents, instead creating richer, long-context training data. The strategy demonstrates a significant performance improvement (32.7% higher score) over previous methods, underscoring its potential value for enhancing language model training efficiency.", "Weaknesses": "The paper could benefit from additional clarity in presenting how LongPack's document assembly directly translates to improved model performance. More details on the selection criteria or weighting of referral signals might strengthen the argument for the pipeline's robustness. Additionally, while LongPack emphasizes hyperlink data, the approach may not be directly applicable to datasets with fewer explicit referral structures, potentially limiting its applicability across diverse domains.", "Questions": "How does LongPack handle potential noise or irrelevant content that may be introduced when concatenating documents through referral relationships? Are there specific filtering or refinement processes in place to ensure the quality of constructed long documents?"}}
{"paper_id": "gx3LMRB15C", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. T-JEPA provides a novel approach to self-supervised learning for tabular data without relying on data augmentations, which is a notable achievement given the challenges in this domain.\n2. The method successfully integrates a joint embedding predictive architecture that allows for effective feature identification and performance improvements on various downstream tasks.\n3. Implementation includes an innovative use of regularization tokens to prevent representation collapse, highlighting an effective training strategy.", "Weaknesses": "1. The paper lacks a detailed theoretical justification on how the regularization tokens specifically prevent representation collapse during training.\n2. Comparison with existing tabular self-supervised learning methods could be more extensive to better contextualize improvements in performance.\n3. More clarity is needed in some sections of the methodology, which could help in better understanding the implementation nuances.", "Questions": "1. How does the T-JEPA approach compare in computational efficiency with other current state-of-the-art self-supervised learning methods for tabular data?\n2. What criteria were used to select the particular datasets for experimentation, and how might performance differ on other datasets with different characteristics?\n3. Can the method be adapted for semi-supervised learning scenarios, where some labels are available, to potentially enhance the performance further?"}}
{"paper_id": "kTH3bEH6hW", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper addresses an important challenge in machine learning, which is improving data representations in tabular data for few-shot learning.\n2. The introduction of a range-limited augmentation method is a novel approach that effectively enhances the semantic consistency of augmented data.\n3. The use of a comprehensive benchmark with a diverse set of datasets (FeSTa) is a strong point, providing a standardized basis for comparison.", "Weaknesses": "1. The scope of the proposed approach is limited to tabular data, which may not directly extend to other data modalities like text or image.\n2. The clarity of problem definition and differentiation from existing few-shot learning methods across different domains could be improved.\n3. The novelty of the range-limited strategy may be questioned, as it closely resembles existing augmentation strategies with minor adaptations specific to tabular data.", "Questions": "1. How does the range-limited augmentation strategy perform when applied to domains other than tabular data?\n2. What are the computational implications of implementing this range-limited approach in terms of training time and resource efficiency compared to traditional methods?\n3. How does this approach scale with increased complexity or dimensionality of the tabular data involved?"}}
{"paper_id": "EqcLAU6gyU", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 3, "Strengths": "The framework effectively addresses key challenges in multi-agent systems, such as task decomposition and allocation inefficiencies. The integration of a meta-agent with LLM capabilities for fast task decomposition and a reward model for evaluating sub-tasks is innovative and demonstrates clear improvements over existing methods. The feedback loop for continuous improvement is a strong aspect of the design.", "Weaknesses": "The paper may lack detailed mathematical rigor in explaining the underlying models and the reward system. The implementation specifics, especially regarding scalability and potential constraints of the framework, might not be fully detailed. While empirical results show advancements, the paper might benefit from more diverse dataset evaluations and real-world application scenarios to fully establish the method's robustness.", "Questions": "How adaptable is the framework to rapidly changing environments or tasks that require real-time adaptation? \n\nWhat are the computational overheads associated with the proposed reward model and feedback loop integration, especially in large-scale multi-agent environments?\n\nCould you provide more details on how the feedback loop influences the continuous learning process of the meta-agent? \n\nAre there any potential limitations identified during experiments, particularly regarding solvability and non-redundancy checks?"}}
{"paper_id": "qrTrnrEi9d", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "TransFusion provides a practical approach to improve performance of LLMs for information extraction tasks in low-resource languages, addressing a significant gap. The inclusion of English translations using high-resource models helps in leveraging the wealth of pre-training in English. The method's flexibility to adapt to existing LLM architectures like GPT-4 enhances its appeal.", "Weaknesses": "The methodology relies heavily on the quality and accuracy of machine translation from low-resource languages to English, which might not always be reliable. While TransFusion enhances performance, the novelty might be considered incremental given the current landscape of translation-based methods in NLP.", "Questions": "How does this method perform when translations are noisy or inaccurate? Is there a significant performance drop compared to high-resource scenarios?\nHas the method been evaluated in real-world, non-benchmark environments to test robustness in diverse settings?\nHow does the complexity and computational cost of TransFusion compare with traditional cross-lingual transfer techniques?"}}
{"paper_id": "Bp0HBaMNRl", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper offers a novel differentiable causal discovery method that effectively addresses the limitations of previous approaches by supporting nonlinear models and latent variables without assuming deterministic relations. The theoretical and empirical results demonstrate significant improvements in accuracy and scalability, with successful application to challenging datasets like MNIST.", "Weaknesses": "While the methodology is robust and innovative, the presentation could be enhanced, especially regarding visualization and interpretation of results. The description of synthetic experiments is not sufficiently detailed and could benefit from more comprehensive baseline comparisons.", "Questions": "How do the newly established identifiability conditions compare to existing ones in terms of practical applicability? Are there any specific domains or types of data where the proposed method does not perform as well?"}}
{"paper_id": "JzLcKWtGnl", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The Spatial 3D-LLM model introduces a novel architecture that enhances spatial reasoning in 3D vision-language tasks. The progressive spatial awareness scheme is innovative and effectively captures multi-level spatial information. The newly constructed 3D instruction dataset, MODLE, adds significant value to the research community, providing new benchmarks for evaluating 3D spatial intelligence in vision-language models.", "Weaknesses": "While the methodology shows promise, there is a lack of detailed comparison with existing state-of-the-art models in similar domains, which limits the contextual understanding of its improvements. The paper could benefit from a more thorough analysis of the limitations of the proposed approach, particularly how it generalizes to various types of 3D environments and tasks beyond those presented. Some sections of the presentation are dense, and the clarity could be improved to make the contribution more apparent to a broader audience.", "Questions": "How does the performance of Spatial 3D-LLM compare to existing models like LEO or other 3D MLLMs in terms of computational efficiency and accuracy in different 3D environments? Are there specific scenarios or types of 3D data where the proposed model struggles? How does the model handle extremely complex or dynamic scenes where objects are frequently moving or changing orientation? Can the approach be extended to other applications beyond the tested 3D object distance measurement and layout editing tasks, such as in robotics or gaming?"}}
{"paper_id": "hXJrQWIoR3", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper addresses a significant gap in explainable AI for graph representations by focusing on representation-level explainability.\n2. The inclusion of both theoretical and empirical validation strengthens the claims made about the robustness and generalization ability of the proposed methods.\n3. The approach effectively combines unsupervised ensemble graph kernel methods with pattern analysis GNNs to explain graph representations.", "Weaknesses": "1. The paper could provide more clarity on the practical implications of the proposed methods, specifically in terms of computational complexity and scalability for large graphs.\n2. While the paper proposes a novel method, the novelty of combining well-known techniques could be questioned without sufficient emphasis on unique contributions.\n3. The theoretical analysis, though comprehensive, might be inaccessible to practitioners unfamiliar with the technical aspects of graph kernels and pattern analysis.", "Questions": "1. How does the proposed framework handle very large graph datasets in terms of computational efficiency?\n2. Can the methods introduced be adapted to dynamic graphs where the structure evolves over time?\n3. Could more insights be provided on how the pattern analysis explicitly improves interpretability compared to existing methods?"}}
{"paper_id": "67sSPPAZiG", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper introduces a comprehensive dataset and benchmark tailored for egocentric video understanding, which is a critical advancement for AR/VR applications and robotics.\n2. MM-Ego model's novel 'Memory Pointer Prompting' mechanism shows potential in addressing long video content processing, making it relevant for future developments in video understanding systems.\n3. The integration of a multimodal approach specifically targeted at overcoming the unique challenges of egocentric videos is a solid step forward in this niche research area.", "Weaknesses": "1. As highlighted, the benchmark alone might not provide substantial differentiation from existing datasets concerning the kind of challenges they present, thereby limiting its standalone impact.\n2. The narrative on the 'narration to egocentric QA' strategy might oversell its novelty, as similar annotation-driven approaches have been seen before.\n3. Additional experiments could help demonstrate the model's adaptability and robustness across various egocentric video datasets beyond what's presented, enhancing its generalization claims.", "Questions": "1. How does MM-Ego's 'Memory Pointer Prompting' specifically compare against other attention mechanisms in terms of computational efficiency?\n2. Can the dataset and benchmark be easily integrated with other egocentric datasets, or is there a need for extensive preprocessing?\n3. What specific strategies are in place for the de-biasing mechanism mentioned, and how do they address potential biases introduced during dataset annotation?"}}
{"paper_id": "kjmLabjSE2", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper provides a significant extension to the analysis of differential privacy within Noisy-SGD, demonstrating convergence of privacy loss even without the traditional assumptions of convexity and smoothness. This greatly broadens the applicability of DP-SGD in practical settings.\n2. The introduction of techniques such as the shifted R\\'enyi divergence analysis with H\\\"older continuous gradients and forward Wasserstein distance tracking is well-motivated and technically sound, offering a novel approach to a longstanding issue.\n3. The analysis offers better privacy loss bounds, which are an improvement over those achieved under previous assumptions, thereby setting a new state-of-the-art.\n4. The findings address a crucial problem in differential privacy, particularly for applications with non-convex loss functions, which are common in modern deep learning applications.", "Weaknesses": "1. The presentation, while thorough, could be improved in terms of clarity, particularly concerning complex mathematical concepts and their implications. For example, the discussion on optimal shift allocation and its impact on results may benefit from a more intuitive explanation.\n2. The paper primarily focuses on theoretical contributions and lacks empirical examples or experiments demonstrating the practical implications of these theoretical results on real-world datasets.\n3. There is a need for more concrete guidance on how the proposed techniques and assumptions translate into actionable steps in implementing DP-SGD for practitioners.\n4. The paper could benefit from a more detailed comparison with existing methods in terms of computational efficiency and privacy-utility trade-off, beyond the theoretical guarantees.", "Questions": "1. Can the authors provide practical examples where H\\\"older continuous gradients naturally occur in real-world machine learning tasks? How often do these scenarios arise in practice compared to convex/smooth loss functions?\n2. How sensitive are the results to the choice of the Holder exponent? Does the exponent significantly impact the level of privacy achieved?\n3. Could the authors elaborate on the computational implications of their approach? Does the need for forward Wasserstein distance tracking and optimal shift allocation introduce significant overhead compared to existing approaches?\n4. How does the proposed method compare in terms of privacy-utility trade-offs for commonly used benchmarks like CIFAR-10 or ImageNet? Is there a plan to explore empirical validation in future work?\n5. Are there any scenarios or types of datasets where the proposed method might not outperform existing approaches? What are the limitations or boundary conditions for the applicability of this analysis?"}}
{"paper_id": "I18MA5DjoP", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper introduces a novel framework, the Neuron Predictability Lens (NPL), which offers a new perspective on analyzing and interpreting transformer-based LLMs by focusing on neuron activation predictability within FFNs.\n2. The application of the framework to real models such as LLaMA-2 and GPT-J provides valuable insights on neuron behaviors and activations across layers, opening pathways for better model optimization and efficiency.\n3. The framework uncovers interesting relationships between neuron predictability and importance, including the roles of 'background neurons,' which could have implications for improving model design and understanding.", "Weaknesses": "1. While the framework provides a novel way to analyze LLMs, the predictability of neuron activations and its deeper implications for model behavior and optimization might still require further validation through more exhaustive experiments and datasets.\n2. The paper could improve in terms of explaining certain methodological choices and the interpretation of key results more clearly, such as the specifics of the linear transformations used or how the predictability lens can directly lead to practical improvements beyond analysis.\n3. The reliance on linear transformations may limit the generalizability of the findings, considering more sophisticated relationships might not be fully captured, potentially leaving room for further exploration with non-linear methods.", "Questions": "1. How does the framework handle the variability in neuron behavior across different kinds of datasets or tasks? Would the predictability remain consistent?\n2. Is there potential to extend the Neuron Predictability Lens to other components of the LLMs beyond FFNs, such as attention heads, and if so, how might this be approached?\n3. Could the authors elaborate on how the insights from neuron predictability might directly impact practical performance improvements in language models, possibly through optimization or architecture modifications?"}}
{"paper_id": "qpDqO7qa3R", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The method leverages existing image restoration diffusion models and extends them to video restoration without additional training. It maintains temporal consistency through hierarchical latent warping and hybrid flow-guided spatial-aware token merging, showcasing versatile application across various datasets and tasks.", "Weaknesses": "The reliance on pre-trained models might limit the scope of improvements for novel or highly distinctive video datasets. The effectiveness of temporal consistency without explicit retraining could be highly dependent on the spatial-aware token merging and optical flow accuracy, which may not always align perfectly with diverse video characteristics.", "Questions": "How does the method perform on highly dynamic or complex scenes where optical flow can be less reliable? Are there any limitations or scenarios where the proposed approach might struggle to maintain consistency or quality?"}}
{"paper_id": "dSQtMx6dPE", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The proposed approach effectively identifies privacy risks in graph prompt learning and addresses them using an innovative application of the PATE framework.\n2. The study evaluates the method across various GNN architectures, showcasing strong empirical performance and potential for wide applicability.\n3. Offers a novel contribution by adapting differentially private learning techniques to the graph domain, which is underexplored compared to language and vision domains.", "Weaknesses": "1. The method's scalability and potential performance impact on large-scale datasets are not thoroughly investigated, leaving questions about its feasibility in real-world applications.\n2. The use of node centrality for weighing teacher votes is well-motivated but lacks in-depth theoretical justification or comparison with alternative weighting schemes.\n3. The theoretical foundations of the privacy guarantees provided by the approach are somewhat under-explored, which might limit confidence in its robustness against sophisticated attacks.", "Questions": "1. How well do the DP-GPL and DP-GPL+W methods scale with increasing graph size and complexity?\n2. What is the trade-off between privacy and accuracy for the proposed methods compared to traditional approaches, and how does this influence the choice of weighting in DP-GPL+W?\n3. Could the method be generalized to work with heterogeneous graphs, and what adaptations might be necessary?"}}
{"paper_id": "GbgCRJedQ7", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The proposed SMT method offers a novel approach to parameter-efficient fine-tuning by sparsely tuning sub-matrices, which effectively reduces computational costs while achieving competitive accuracy. The experiments clearly show that SMT surpasses existing PEFT methods such as LoRA in both computational efficiency and accuracy, thus demonstrating its practical applicability for large language models.", "Weaknesses": "The paper does not address the potential challenges of selecting optimal sparse sub-matrices across various types of neural network architectures, which may limit the generalizability of SMT. Additionally, factors such as the robustness of the approach in new unseen tasks or detailed comparative analysis against random sparse selection methods are not well explored.", "Questions": "1. How does the selection of sparse sub-matrices affect model regularization, and does this method introduce any biases in the learning process? 2. Can SMT be effectively applied to models with architectures very different from transformers, and if so, how might the sparse tuning strategy differ? 3. How does SMT perform in terms of robustness and generalization on entirely new downstream tasks not included in the training phase?"}}
{"paper_id": "ZZVOrId3yN", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. CrossModalNet provides a novel approach to integrating multimodal data effectively, addressing a critical gap in medical image segmentation.\n2. The use of advanced frameworks like the dual-stream cross-network design and Transformers enhances the model's performance significantly, as demonstrated by improved metrics on the MM-WHS dataset.\n3. Provides a strong theoretical foundation which could guide future research in the field.", "Weaknesses": "1. The complex model architecture and advanced mathematical frameworks might pose challenges to replication and implementation in practical settings without substantial computational resources. \n2. The paper's evaluation is primarily based on a single dataset (MM-WHS), which might not fully represent the model's generalizability across other datasets and medical segmentation tasks.\n3. There is limited discussion on the impact of varying hyperparameters and potential overfitting due to model complexity.", "Questions": "1. Can the authors elaborate on how the CrossModalNet architecture can be extended or adapted for other medical segmentation tasks beyond whole-heart segmentation?\n2. How sensitive is the model's performance to the choice of hyperparameters, and were there specific strategies used to prevent overfitting given the model's complexity?\n3. Does CrossModalNet require any specific pre-processing steps or data normalizations to manage domain shifts effectively across different datasets?"}}
{"paper_id": "gLHuAYGs6a", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The proposed SMvCNet effectively integrates multi-view information using heterogeneous random walks, demonstrating superior clustering performance across multiple datasets. The unified sample-level structure effectively guides representation learning by capturing both inter-view consistency and complementary structural information.", "Weaknesses": "The paper lacks a detailed complexity analysis of the SMvCNet, particularly in terms of time and space requirements, which makes it difficult to assess scalability. Furthermore, the proposed method's theoretical performance, including potential convergence guarantees, is not discussed in depth.", "Questions": "What are the computational complexities of the proposed algorithm in terms of time and space? Are there theoretical justifications for the expected performance improvements and the convergence behavior of the SMvCNet?"}}
{"paper_id": "x5l5PRtvul", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper provides a substantial theoretical advancement by addressing the variance collapse problem in SVGD through the novel use of hybrid kernels. The introduction of h-SVGD as a kernelised Wasserstein gradient flow is a significant contribution that could influence future research in variational inference.", "Weaknesses": "Though the paper provides a compelling theoretical framework, the presentation of complex concepts like the hybrid Stein PDE could benefit from more intuitive explanations or examples to aid understanding. Additionally, the experimental section could be expanded to include more diverse scenarios or benchmarks.", "Questions": "How does the choice of different kernels for the driving and repulsive terms in h-SVGD affect its performance with different distributions? Has there been an investigation into the optimal combination of kernels? Could the proposed h-SVGD method be extended or adapted to address other challenges or limitations in particle-based inference methods?"}}
{"paper_id": "pOUAVXnOQP", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces an innovative approach to address spectral bias in implicit neural representations using sinusoidal trainable activation functions. This is a significant contribution that improves the learning and reconstruction of high-frequency details, which is a notable advance over existing ReLU-based networks. The theoretical foundations and practical implications of the method are well-articulated, and the experimental results demonstrate superior performance in image, shape, and waveform reconstruction compared to state-of-the-art methods.", "Weaknesses": "While the proposed method shows significant improvement, there is a lack of detailed analysis on the computational overhead introduced by optimizing the additional parameters of the sinusoidal functions. Also, the paper could have better highlighted potential limitations or scenarios where STAF might not perform as well or where traditional ReLU-based methods might still be preferable.", "Questions": "1. Can the method be applied to deeper architectures or those with more complex topologies, and how does STAF scale in such scenarios?\n2. What are the potential impacts of the initialization scheme on model convergence and stability across different datasets or tasks?\n3. How does the use of trainable sinusoidal activation functions affect the model's robustness to noise or variations in the input data?"}}
{"paper_id": "iN64nSYt0z", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses a pertinent and challenging problem of instruction adherence in LLMs and proposes an innovative, resource-efficient method to enhance this alignment. The introduction of the GUIDE framework and the Influence metric provides a novel approach to systematically bias attention mechanisms, potentially offering significant advancements in transformer-based models' explainability and alignment.", "Weaknesses": "The evaluation lacks a comprehensive assessment of potential trade-offs with the proposed approach, particularly regarding impacts on the quality and consistency of outputs. There is also a notable absence of comparisons with existing methodologies in similar domains, which could provide better insights into the distinct advantages and potential limitations of GUIDE and Influence. Additionally, there is limited discussion on scalability and generalizability across different LLM architectures or domains.", "Questions": "How does GUIDE compare to other recent methodologies focused on instruction adherence and transformer alignment, particularly regarding efficiency and effectiveness? Can GUIDE be easily integrated into existing LLM frameworks, or does it necessitate significant architectural modifications? What are the implications of GUIDE on model adaptability to longer and more complex prompts?"}}
{"paper_id": "gRuZkEy49k", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper addresses an important gap in GFlowNet training, focusing on the inefficiencies observed in sampling methods.\n2. The integration of MCTS principles into GFlowNet training, via MCDS, is innovative and leverages known strengths of both methods to improve exploration during training.\n3. Empirical evidence suggests that MCDS outperforms traditional sampling approaches, highlighting its potential for broader applications in discrete environments.", "Weaknesses": "1. The paper could provide more clarity on certain technical components, such as the exact implementation details of MCDS within the broader GFlowNet architecture.\n2. There seems to be a lack of comprehensive statistical analysis accompanying the empirical results, especially in demonstrating the significance of improvements observed with MCDS.\n3. Although the paper claims enhanced performance, the experiments focus on relatively synthetic environments; inclusion of more diverse real-world tasks could bolster claims of generalizability.", "Questions": "1. How does the proposed MCDS handle environments with significantly larger state spaces, and what are its computational limitations?\n2. Can the authors provide more detailed comparisons against other state-of-the-art sampling techniques to clearly illustrate MCDS's advantages?\n3. What are the potential impacts of parameter changes within the MCDS approach on its empirical performance?"}}
{"paper_id": "d23EVDRJ6g", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 2, "Rating": 5, "Confidence": 4, "Strengths": "1. The proposal of MotionDreamer enhances the capability of generating natural and diverse motion sequences from limited data.\n2. The usage of sliding window local attention and quantization techniques appears both novel and immediately applicable, potentially advancing the field.\n3. User perceptual assessments show MotionDreamer’s effectiveness in practical scenarios like temporal editing, which could benefit animation industries.", "Weaknesses": "1. The primary weakness lies in the limited novelty of integrating existing methodologies in a way that may not significantly advance the field theoretically.\n2. While MotionDreamer's use of localized attention and quantization is claimed to handle overfitting, specific comparisons with alternative methods, such as QnA layers from SinMDM, are not elaborated.\n3. The evaluations, while comprehensive, seem to rely heavily on qualitative assessments without deep discussions on failure cases or challenging scenarios where the method might underperform.", "Questions": "1. How does MotionDreamer handle extremely complex or chaotic motion sequences compared to simpler, smoother sequences?\n2. Are there specific complex scenarios where the localized generative masked transformer fails to sufficiently capture the variability of motion?\n3. Is there any consideration or future work aimed at extending the method's utility to broader datasets beyond motion animation?"}}
{"paper_id": "kQ5s9Yh0WI", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper addresses a significant limitation in current long-context LLMs by proposing a novel approach for ultra-long text generation. The introduction of AgentWrite is a creative solution for breaking down long tasks, and the creation of the LongWriter-6k dataset and LongBench-Write benchmark are valuable contributions to the field. The experiments conducted demonstrated the potential for existing models to generate much longer coherent outputs when trained with suitable datasets.", "Weaknesses": "The paper might benefit from a more comprehensive evaluation involving a wider range of models to ensure the broader applicability of the proposed method. Additionally, more details on how AgentWrite compares to existing methods at breaking down tasks would enrich the discussion on its effectiveness. Future work could also explore how such approaches impact the training time and computational costs.", "Questions": "1. How does the AgentWrite system ensure that the quality and coherence of subtask-generated text are maintained across very long outputs?\n2. Can the methods proposed be easily adapted to other languages or datasets outside what was specifically tested here?\n3. What are the computational trade-offs when using such a system to generate lengthy outputs consistently?"}}
{"paper_id": "9CqkpQExe2", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The introduction of dynamic Ada-K routing provides a novel, adaptive approach to resource allocation in Mixture-of-Experts architectures, addressing the inefficiencies of static Top-K routing.\n2. The proposed method reduces computational resources, FLOPs, and inference latency while improving model performance, which makes it a significant contribution to the scalability of large language models.\n3. Extensive experimental validation across varying model sizes and benchmarks demonstrates strong results, providing evidence of the method's versatility and effectiveness.", "Weaknesses": "1. While dynamic routing is shown to be effective, the paper could provide more insights into potential limitations, such as scalability to even larger models or varying types of benchmarks.\n2. The implications of using Proximal Policy Optimization for training allocators are not fully explored. Discussion on how the choice of differential algorithm impacts training stability and performance could be beneficial.\n3. There could be more discussion on the complexities and potential overhead introduced by the dynamic routing system, including the computational cost relative to the gains in efficiency.", "Questions": "1. Could the proposed dynamic Ada-K routing method encounter challenges if applied to models larger than those tested, such as in contexts that heavily rely on specialized data formats or exceptionally complex tasks?\n2. Given the reliance on Proximal Policy Optimization, what are the primary factors affecting the allocation learning, and how sensitive are these factors to the choice of hyperparameters?\n3. How does the overhead introduced by the routing decision-making process compare with the efficiency gains in practice, especially for extremely large batch processing scenarios?"}}
{"paper_id": "PpYy0dR3Qw", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper presents a novel combination of Local Training and Communication Compression in the federated learning setting, leveraging both to improve communication efficiency. \n2. Theoretical contributions include a comprehensive mathematical proof ensuring linear convergence and dependability on model and data distribution characteristics. \n3. LoCoDL sets new standards for communication-efficient distributed learning by achieving a doubly-accelerated communication complexity.", "Weaknesses": "1. The practical implementation details and potential limitations or trade-offs of the proposed LoCoDL algorithm might not be comprehensively discussed.\n2. The paper may rely heavily on theoretical proofs without extensive empirical validation across diverse real-world settings.\n3. The integration of the local model estimates and dual variables might add complexity to the system, potentially affecting ease of implementation.", "Questions": "1. How does LoCoDL perform in highly heterogeneous datasets compared to homogeneous settings?\n2. Are there specific scenarios or data distributions where the proposed algorithm might not achieve the expected communication efficiency?\n3. How does the computational cost of the proposed method compare to existing state-of-the-art FL algorithms during local updates?"}}
{"paper_id": "UatDdAlr2x", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 2, "Rating": 6, "Confidence": 4, "Strengths": "The paper provides an in-depth analysis of transformer architectures, specifically focusing on their ability to execute counting tasks. It highlights how subtle changes in architecture and hyperparameters influence the model's performance, providing valuable insights into the workings of transformers. The theoretical and empirical evaluations are well-motivated and aligned.", "Weaknesses": "The contribution is somewhat limited to academic insights into transformers' working mechanisms, which might not directly translate into practical improvements for more complex real-world tasks or multi-layer models. It primarily focuses on single-layer architectures and a controlled counting task, which may limit the generalizability of the findings.", "Questions": "What potential approaches do the authors suggest for extending these findings to more complex multi-layer transformer models? How generalizable are these findings to other tasks beyond counting, such as natural language processing applications with richer data contexts?"}}
{"paper_id": "IwgmgidYPS", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel dataset, MedTrinity-25M, which addresses the lack of rich multigranular annotations in existing medical datasets. It leverages an automated pipeline to generate these annotations without requiring paired text datasets, thus improving scalability. The dataset's utility is demonstrated through its ability to enhance the performance of AI models on various medical tasks, showcasing its potential impact on the field.", "Weaknesses": "The dataset's dependence on large multimodal models and medical knowledge retrieval could introduce biases or inaccuracies, especially if the underlying models or knowledge bases are incomplete or flawed. Additionally, the process for generating ROI and annotations, while automated, might not capture all nuances of expert annotations, potentially affecting model performance in critical applications. The paper could also provide more details on the validation of the dataset's annotations.", "Questions": "How does the dataset handle potential biases or inaccuracies in the underlying multimodal models and medical knowledge retrieval systems? What measures are taken to ensure the quality and reliability of the automatic annotations, given they replace expert manual annotations? Are there any plans for expert validation or adjustment of these automated annotations, particularly for high-stakes medical applications?"}}
{"paper_id": "WG7GzGx3G9", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The proposed Rotated Runtime Smooth (RRS) method effectively addresses both channel-wise and spike outliers, offering a significant improvement in INT4 inference accuracy. The integration with existing hardware ensures practicality and demonstrates lower computational overheads compared to prior methods.", "Weaknesses": "The paper does not provide sufficient detail on the potential computational overhead introduced by the runtime smoothing process, especially in a real-world deployment scenario. Moreover, the practicality of implementing the method across various hardware setups may not be fully addressed.", "Questions": "How does the runtime performance of RRS compare with existing methods like SmoothQuant and QuaRot in terms of actual hardware latency rather than theoretical computational overhead?"}}
{"paper_id": "BhECSDSkAE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The method proposes an innovative test-time training strategy with a memory mechanism specifically tailored for few-shot medical video segmentation, which is a pressing challenge in medical imaging due to the scarcity of annotated video data.\n2. The research successfully bridges the gap between static image models and video segmentation by constructing a new methodology that leverages minimal annotations effectively.\n3. The introduction of the OS-I2V-Seg dataset for evaluation highlights the practicality and application of the proposed approach.", "Weaknesses": "1. The paper lacks comprehensive details on the pre-training of the one-shot segmentation model on static images, which raises concerns about replicability and understanding of the methodology.\n2. The framework might not fully address domain shift issues that arise from potential differences between training datasets and test scenarios, particularly in medical settings.\n3. The presentation could benefit from more detailed explanations of the memory mechanism and self-distillation processes, as their roles are crucial yet not sufficiently detailed in the method section.", "Questions": "1. How does the proposed method ensure robustness to domain shifts when the pre-trained static image-based models are applied to distinct video datasets?\n2. Could the approach be applicable to other video domains outside of medical imaging, and if so, what adjustments might be needed?\n3. Regarding the memory mechanism employed during test-time training, how is the size of the memory bank determined, and what impact does it have on the segmentation performance?"}}
{"paper_id": "mnB4hDTIDr", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces a novel approach, DCScore, to evaluate the diversity of LLM-generated datasets by treating it as a sample classification problem. It provides a feasible solution to a relatively under-explored area, demonstrating strong correlation with diversity indicators and offering computational efficiency.", "Weaknesses": "1. The paper does not thoroughly explore the novelty and specific advantages of DCScore compared to existing methods. \n2. The rationale behind choosing certain evaluation tasks and the impact of diversity on practical applications could be better justified.\n3. The theoretical contribution and its real-world implications are not fully developed, leaving readers unclear about broader applications or significance.", "Questions": "1. How does DCScore compare specifically to existing diversity evaluation methods in various practical performance scenarios, beyond computational efficiency?\n2. What are the implications of diversity in LLM-generated datasets on real-world applications, and how critical is this factor in improving those applications?\n3. What kind of datasets or scenarios would particularly benefit from using DCScore for diversity evaluation?"}}
{"paper_id": "Y0qmwm6tgy", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. The introduction of the Moreau envelope for estimating weight importance is a novel approach in the context of pruning for LLMs. This innovation provides a fresh perspective on dealing with weight perturbations. \n2. The use of proximal gradient descent and post-pruning fine-tuning with LoRA adds to the robustness of the proposed method. It ensures that MoreauPruner effectively maintains model performance after pruning. \n3. The experiments conducted on multiple LLMs provide a comprehensive evaluation of MoreauPruner's effectiveness.", "Weaknesses": "1. The practical implications of the proposed method are not entirely clear. The discussion on how these robust pruning outcomes directly translate to real-world applications or computational resource savings could be more detailed.\n2. While the results demonstrate consistency across different weight formats, the benefits beyond this aspect could be better highlighted. The paper could provide more comparisons to existing state-of-the-art pruning methods.\n3. The computational cost and scalability of the method are not thoroughly discussed, which are critical factors for the deployment of pruning techniques in resource-constrained environments.", "Questions": "1. Can the authors provide more insights into the computational overhead introduced by using the Moreau envelope and proximal gradient descent in comparison to traditional pruning methods?\n2. Are there specific scenarios or architectures for which MoreauPruner is particularly effective or less suited, based on the experiments conducted?\n3. How would MoreauPruner perform if applied to different types of perturbations beyond weight format changes, such as changes in input data distribution or adversarial attacks?"}}
{"paper_id": "aAcOaJYbUg", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The introduction of LAION-C is a notable contribution, providing a benchmark that better reflects true out-of-distribution conditions for models trained on internet-scale datasets. This fills a significant gap in current robustness evaluation methods. The paper is well-structured and clearly communicates the necessity of a new benchmark. The novel distortion types present a meaningful challenge to current vision models, showcasing their reliance on familiar artifacts.", "Weaknesses": "While LAION-C presents a compelling new benchmark, the paper could further explore the implications of model performance differences on LAION-C for practical applications. Additionally, there might be a need for a more diverse set of distortions to cover a wider variety of OOD situations, as currently, the distortions primarily focus on visual corruption types.", "Questions": "What specific criteria were used to select the six synthetic distortion types, and how do these distortions align with potential real-world challenges? How might the findings from LAION-C impact the future development of vision models and their training regimens?"}}
{"paper_id": "duGygkA3QR", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The proposal of incorporating Dynamic Mode Decomposition (DMD) into Graph Neural Networks (GNNs) offers a novel way to associate GNN feature propagation with dynamical systems theory, which can provide theoretical insights and potentially enhance learning tasks. The approach has been empirically validated across several standard graph-based learning tasks, showing it's not just theoretically appealing but practically beneficial.", "Weaknesses": "The paper may lack a deep exploration or comparison with similar existing methodologies in dynamical systems applications within GNN contexts. Additionally, while the results indicate success, the paper could benefit from more detailed explanation or justification of the specific choices made within the method, including parameter selection and computational complexity assessment.", "Questions": "How does the proposed method scale with extremely large graphs given the additional computational requirements of Dynamic Mode Decomposition? Is there an analysis of potential computational overhead or optimization strategies applied to manage resources effectively?"}}
{"paper_id": "s6nYndMwG7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper addresses a significant challenge in the use of influence functions, particularly related to the computational cost of existing methods, making a contribution that could make these functions more usable on large models.\n2. The method proposed to use spectral properties of the Hessian to set hyperparameters is novel and provides an interesting approach to optimizing the LiSSA algorithm.\n3. Empirical validation on different models, including ResNet and language models, is provided, showing the practical applicability and effectiveness of the proposed method.", "Weaknesses": "1. The paper's theoretical contributions rely on strong assumptions, and it is not clear how these assumptions hold in practice, especially related to the contractivity condition, which makes it difficult to assess the general applicability of the proposed method.\n2. The empirical results could be better aligned with the theoretical claims, particularly around the evaluation and demonstration of contractivity or other key assumptions.\n3. The presentation could be improved, particularly in terms of explaining the selection process for LiSSA parameters and clarifying the experimental setups and results.", "Questions": "1. Can you provide additional empirical evidence or theoretical support that the contractivity assumption holds for the settings you consider?\n2. How would the proposed spectral property-based method handle situations where the assumptions do not hold, particularly for models with more complex loss landscapes?\n3. Can you clarify the precise procedure for selecting the LiSSA parameters, particularly the batch size, and how sensitive the method is to these parameter choices?"}}
{"paper_id": "9BVMD3keG8", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper expands on online learning in brokerage by incorporating contextual information, a novel approach that fills a gap in existing literature. The algorithms achieve optimal regret bounds under realistic assumptions, highlighting the method's robustness and potential practical application. The use of both full and two-bit feedback mechanisms provides a comprehensive analysis of the problem.", "Weaknesses": "The paper's presentation could be improved, as it lacks clarity in certain sections, notably in the explanation of the feedback mechanisms. The reliance on the bounded density assumption as necessary for learnability might limit the method’s applicability to real-world scenarios where this assumption does not hold, potentially affecting the generalizability of the results.", "Questions": "What specific challenges might arise when applying the proposed method to real-world OTC markets with unbounded noise distributions? How sensitive are the algorithms to variations in the contextual information or to inaccuracies in the assumed linear relationship between the market value and context?"}}
{"paper_id": "mnna9LUg7P", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The proposed method provides an innovative approach to quantizing state space models, particularly by addressing the unique challenges posed by their activation patterns.\n2. It successfully demonstrates the application of the Hadamard transform to handle outliers, which is a novel approach in the context of SSM quantization.\n3. The experimental results show promise for deployment of SSMs in resource-constrained environments, which could have significant practical implications.", "Weaknesses": "1. The methodology, while innovative, may still need further exploration and validation across more diverse SSM architectures and datasets to establish its generalizability and effectiveness.\n2. The paper may lack detailed discussion on potential trade-offs introduced by the quantization process, especially regarding accuracy loss.\n3. A clearer comparison with existing quantization methods for other models could strengthen the claims regarding the uniqueness and superiority of the proposed approach.", "Questions": "1. How does the performance of Quamba compare to other quantization methods not specifically designed for SSMs?\n2. What are the implications of the proposed quantization method on power consumption and heat dissipation in edge devices?\n3. Are there any specific types of SSM architectures where this method may not perform as well, and if so, why?"}}
{"paper_id": "3X3LuwzZrl", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The proposed method introduces a novel decomposition of GNN message passing, providing insights into label influence correlations. The approach effectively models and quantifies interactions between labels, leading to improved performance in multi-label node classification tasks.", "Weaknesses": "The paper could benefit from a clearer explanation of the proposed decomposition methodology for readers less familiar with GNNs. Additionally, more comprehensive experiments with diverse datasets and comparisons with other advanced MLNC models are necessary to establish the method's general applicability.", "Questions": "How does the proposed method handle varying graph structures and sizes across different datasets? Is its efficacy consistent regardless of graph complexity or sparsity?"}}
{"paper_id": "IqaQZ1Jdky", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper presents a significant advancement in KAN by utilizing Bernstein polynomials to enhance flexibility and robustness, a noteworthy contribution to the field.\n2. VBn-KAN demonstrates superior performance across a range of tasks, including multivariate time series forecasting and computer vision, indicating the practical efficacy of the proposed method.\n3. The methodological approach, incorporating dynamic adaptation via Bayesian optimization, is innovative and well-grounded in theoretical principles.", "Weaknesses": "1. The paper may not thoroughly explore potential limitations or drawbacks of the VBn-KAN method, such as potential computational overhead or challenges in implementation.\n2. While the approach demonstrates superior performance, the novelty might be considered iterative rather than groundbreaking, building upon existing KAN frameworks.\n3. The evaluation could benefit from a wider range of benchmark datasets and real-world applications to generalize findings better.", "Questions": "1. How does VBn-KAN handle computational overhead compared to traditional KAN methods, particularly when dealing with high-dimensional datasets?\n2. Can the authors elaborate on the potential limitations or challenges of implementing the proposed method in real-world applications?\n3. Is there a specific reason for choosing the particular set of benchmark tasks, and how might different tasks impact the method's effectiveness?"}}
{"paper_id": "OdMqKszKSd", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel use of diffusion-based models for generating articulated 3D assets from a single image, addressing a significant gap in the field related to scalability and practicality. The use of large pretrained models like DINOv2 and GPT-4o to enhance detail accuracy and plausibility shows a creative integration of existing powerful tools for a new application. Furthermore, the paper is well-organized and demonstrates the effectiveness of the approach through thorough experimental validation and user studies.", "Weaknesses": "While the approach is innovative, it relies heavily on pretrained models such as DINOv2 and GPT-4o, raising questions about the novelty of the method itself versus the application and integration of these models. The paper might lack insights into the failure modes or limitations of the method, particularly in cases where the models might not generalize well due to limited training data or diversity in the datasets used.\n\nAdditionally, the complexity of implementing such an approach might not be fully transparent, potentially limiting its applicability or reproducibility in practical scenarios.", "Questions": "1. How does the approach handle cases where the single input image lacks richness in details or when the viewpoint is suboptimal for capturing key articulations?\n\n2. Are there limitations noted with the use of GPT-4o in predicting the part connectivity graph, and how does it affect the overall method if any?\n\n3. What are the computational requirements for training and inference using this diffusion model-based approach, and how scalable is it when extending to larger sets of articulated objects?"}}
{"paper_id": "LOBhVTtVnc", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The proposed Geometric Spatiotemporal Transformers (GSTs) effectively integrate Transformer architecture with spatiotemporal graphs, addressing limitations of current GNN-based methods such as fixed input lengths and cumulative errors.\n2. GSTs demonstrate superior performance in long-term physical dynamics simulation across diverse scales, highlighting their versatility and robustness.\n3. The Temporal Difference Graph (TDG) is an innovative feature that enhances the model's ability to predict long-term trajectories accurately and efficiently.\n4. Extensive experiments showcase the efficacy of GSTs in handling multi-scale datasets, suggesting potential for wide application in physical dynamics simulation.", "Weaknesses": "1. The paper lacks detailed discussion on the computational complexity and scalability of the proposed GSTs, especially in comparison to existing GNN-based approaches.\n2. Although the GST model achieves impressive results, there is a limited evaluation with industrial or real-world applications, raising questions about its feasibility in large-scale deployment.\n3. The reliance on E(3) symmetries, while beneficial for some applications, may limit the generalizability of GSTs to scenarios where such symmetries are not present or are only partially applicable.\n4. The paper could benefit from more clarity in the exposition of the temporal aspects of the model, particularly regarding how temporal dependencies are explicitly accounted for and analyzed.", "Questions": "1. How do the computational requirements of GSTs compare with those of traditional GNNs when scaled to larger datasets or more complex systems?\n2. Is there any limitation in applying GSTs to systems where E(3) symmetries are not prevalent, and if so, how might the model be adjusted or extended to address such cases?\n3. What are the main challenges in extending GSTs to real-world applications, particularly those involving industrial-scale data, and how might these be overcome?\n4. Can the Temporal Difference Graph (TDG) concept be adapted or generalized to other domains, such as finance or social network analysis, where long-term dependencies are critical?"}}
{"paper_id": "oa5UeyUVMm", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper makes a clear attempt to advance the understanding of applying diffusion probabilistic models to graph representation learning, which is a fairly novel domain. The experimental validation on multiple datasets provides evidence for the model's effectiveness. The introduction of the Diff-InfoMax principle offers a theoretical foundation for future work.", "Weaknesses": "The paper could benefit from improved clarity in explaining the specific contributions of the proposed method compared to existing work in both diffusion models and graph representation learning. Some theoretical claims, such as the relation to the InfoMax principle, would benefit from more rigorous validation or proof. Additionally, the paper's presentation could be enhanced for better readability and understanding of complex concepts.", "Questions": "How does the proposed method Graffe specifically address the challenges associated with the non-Euclidean nature of graph data, and how does it improve over existing GNN solutions? Could the method potentially be applied to other types of non-Euclidean data structures beyond graphs? Is there a specific reason that GAT and GIN are chosen for encoding, and could alternative GNN architectures potentially offer better results?"}}
{"paper_id": "5swfKRkCx7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The proposed method effectively enhances LLMs' performance in knowledge-intensive question-answering tasks by introducing a novel attention mechanism for external knowledge integration. This dynamic and layer-wise integration contributes to more robust utilization of external knowledge, allowing for improved performance compared to traditional RAG approaches.", "Weaknesses": "The description of the method lacks sufficient detail, particularly in explaining how the external knowledge attention mechanism is implemented and integrated into the existing LLM framework. There is also a lack of direct comparison in terms of efficiency gains, as the claimed advantages of reduced inference latency and increased throughput are not empirically validated in the results.", "Questions": "How exactly does the memory-based approach differentiate between relevant and less relevant knowledge across different layers of the LLM? Are there specific criteria or thresholds used to assign layer-wise weights to the knowledge representations?"}}
{"paper_id": "5s1qpjrNvZ", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces a novel GRL algorithm with adaptive sampling rates to ensure mean return above a performance threshold, which is a significant contribution to addressing the sample inefficiency in reinforcement learning. The roll-back feature (GRL-RB) increases robustness, providing a mechanism to dynamically adjust sampling rates and recover from potential performance dips.", "Weaknesses": "There are some limitations in the empirical evaluation, which lacks comprehensiveness, with only two simpler scenarios tested and relatively small gains in practical performance. Additionally, the claims of 'performance guarantees' are somewhat overstated, as they rely on assumptions that may not hold in practical scenarios. The inclusion of a roll-back feature to amend performance suggests that these guarantees are not fully realized.", "Questions": "Can you clarify why the performance evaluation of baselines like LD doesn't match the expected static sampling rates? Why not degrade alpha faster, using the current alpha instead of the original alpha? Also, how do the derived sampling rates fair when assumptions are breached, and is there any strategy for violation detection other than roll-back?"}}
{"paper_id": "WNb4P8aG66", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces a novel framework, DoD, leveraging visual priors in diffusion models, achieving state-of-the-art results with reduced computational cost. The multi-stage framework and the use of Latent Embedding Module (LEM) to compress sample information are innovative approaches addressing the issues of texture detail reproduction. The paper's clear presentation, along with conducted experiments, strengthens its argument.", "Weaknesses": "The proposal lacks comprehensive comparison with other advanced image generation models or methods deploying visual priors. The reduction in computational costs and improvements in output quality needs more quantifiable evidence across varied datasets. Additionally, while the DoD framework is innovative, the paper could enhance its contribution by clarifying the scope of its applicability beyond the ImageNet dataset and exploring other possible applications or benefits.", "Questions": "How does DoD compare with other diffusion-based models incorporating visual priors in different environments or datasets? Can this approach leverage other types of priors, such as those from text or audio, and what adaptations might be necessary? Are there any identified limitations or scenarios where the proposed method might not perform as expected?"}}
{"paper_id": "UfczlMudN6", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The integration of a robust adaptation module within a single architecture is a significant step forward in achieving generalization for both ID and OOD scenarios. The use of epistemic neural networks to quantify uncertainty provides a novel approach that could have implications for other areas of deep RL. The validation on realistic simulated tasks provides a compelling proof of concept for the algorithm's effectiveness in real-world settings.", "Weaknesses": "While the method presents an innovative unification of ID and OOD approaches, the paper could benefit from additional comparisons with a broader range of existing methods to more conclusively demonstrate its superiority. The robustness of the evaluation could be questioned due to the focus on a singular type of task (locomotion). Broader testing across various domains would strengthen claims of general applicability.", "Questions": "1. Could the authors provide more detail on how the epistemic neural network specifically influences the training process and final policies?\n2. How does the GRAM framework handle situations where the ID and OOD distinctions are not clear-cut or have overlapping characteristics?\n3. Considering the reliance on simulated environments, what steps are necessary to ensure that the findings are transferable to untested real-world scenarios?"}}
{"paper_id": "60Vd7QOXlM", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper identifies a significant gap in privacy auditing by enhancing the design of canaries to uncover more realistic memorization behavior in LLMs. The development of new token canaries shows promising results, strengthening the empirical analysis of privacy risks associated with LLMs. It is a well-structured study with thorough experiments across different models and conditions.", "Weaknesses": "The novelty of using canaries for privacy audits is somewhat limited, as the paper primarily enhances existing structures rather than introducing a fundamentally new paradigm. Additionally, while effective, the robustness of the new canaries might not be universally applicable across all LLM architectures and datasets, potentially limiting generalizability.", "Questions": "How do the designed canaries fare in terms of performance across various model architectures not explicitly tested in the paper? How sensitive is the proposed methodology to variations in dataset distributions or model architectures?"}}
{"paper_id": "GqhvJ1o8m5", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The design of the dual-branch architecture combined with spatial-temporal attention is highly innovative, providing better handling of implicit disparity and enhancing frame-to-frame consistency.\n2. The introduction of the YouTube-SBS dataset significantly contributes to the stereo video generation research community, offering a vast resource for evaluation and development of new models.\n3. The model demonstrates considerable improvements in standard metrics (L1, SSIM, PSNR), indicating robust performance.", "Weaknesses": "1. The paper could provide more detailed comparative analysis and ablation studies to better understand the individual contributions of each component within the ImmersePro framework.\n2. Some key details regarding the implementation and training of the model, as well as specifics about the dataset, are lacking, which could affect reproducibility and further experimentation by other researchers.\n3. The technique relies on significant computational resources due to its complex architecture, potentially limiting its access and applicability in broader, less resource-rich environments.", "Questions": "1. Can the authors provide more insights into the efficiency and speed of the proposed model compared to traditional methods?\n2. How does the model performance vary with different genres or types of video content present in the YouTube-SBS dataset?\n3. Are there any known limitations or failure cases for ImmersePro that have been observed, especially in terms of handling highly dynamic scenes or scenes with intricate occlusions?"}}
{"paper_id": "ua5MHdsbck", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The integration of triplet relations into preference learning models is novel and shows a marked improvement in the accuracy and capability of protein design beyond training data.\n2. The proposed method demonstrates clear advantages in experimental results, outperforming baseline approaches across various datasets and conditions.\n3. The application-oriented nature of the paper contributes both theoretically and practically to the field of bioinformatics, particularly in protein design.", "Weaknesses": "1. The approach assumes an accurate scoring function, which is typically challenging to obtain in real-world scenarios, and may not address the potential noise effectively in scoring functions.\n2. While effective, the method may require significant computational resources for constructing and evaluating triplet datasets, which could be a barrier for practical, large-scale applications.", "Questions": "1. How does the approach handle noise in scoring functions during triplet creation? Are there mechanisms in place to mitigate errors due to inaccurate scorers?\n2. What are the computational implications of scaling this approach to very large protein datasets? Are there efficiency improvements that can be considered in future work?\n3. Can the approach be generalized to other domains where extrapolative design is required, or is it particularly tailored to the nature of protein data?"}}
{"paper_id": "56Zn3halhq", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The introduction of an adaptive data augmentation method addressing the inefficiencies in traditional data augmentation for time series is promising. The use of reinforcement learning to target marginal samples and optimize a neural augmentor is innovative, potentially leading to more consistent prediction errors and improved overall forecasting accuracy.", "Weaknesses": "The requirement for a robust model zoo could limit the applicability of the proposed method in environments where such resources are not readily available. Additionally, the effectiveness of the reinforcement learning framework relies heavily on the quality of variance as a reward structure, which might introduce complexities and dependencies that are not fully explored or justified in the paper.", "Questions": "How does the proposed method handle scenarios where the model zoo is imperfect or limited in scope? Are there any specific strategies suggested for constructing an effective model zoo or mitigating potential issues arising from a limited zoo?"}}
{"paper_id": "aPTGvFqile", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. AlignCLIP addresses a crucial challenge in existing CLIP models by effectively reducing the modality gap, thus improving cross-modal tasks.\n2. The method significantly enhances zero-shot classification and multi-modal retrieval, demonstrating practical impact across a variety of downstream tasks.\n3. The paper is well-written, clearly outlining the problem, solution, and the results in a comprehensive manner.", "Weaknesses": "1. While the approach is innovative, it builds upon existing structures like SBERT and CLIP's transformer-based encoder, which may limit the novelty of the contribution.\n2. The extent to which parameter-sharing versus intra-modality separation contributes to performance gains is not entirely clear, warranting further exploration.\n3. The work could benefit from exploring more varied datasets or conditions to further generalize its findings.", "Questions": "1. How does the intra-modality separation objective specifically impact embeddings in comparison to other methods?\n2. What are the limitations or challenges encountered when applying parameter-sharing across the modality encoders?\n3. Could the improvements seen in zero-shot classification be primarily attributed to the semantic regularization component, and how might this be isolated in experimentation?"}}
{"paper_id": "GOwNImvCWf", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper proposes an innovative use of behavioral loss in autoencoders to improve the functional equivalence of reconstructed neural networks, which enhances their performance.\n2. The integration of both structural and behavioral losses shows clear empirical benefits in reconstructing and generating high-performing neural networks, demonstrating a significant improvement over previous methods.\n3. The methodology is evaluated on diverse datasets, showcasing its general applicability and effectiveness.\n4. The approach is straightforward and potentially impactful for tasks involving model reconstruction and weight space learning.", "Weaknesses": "1. While the incorporation of behavioral loss is novel, the conceptual contribution might be seen as somewhat incremental, given the existing literature that explores functional aspects of neural networks.\n2. The experimental evaluation, though performed on multiple datasets, could benefit from deeper analysis or additional experiments to better understand the boundary and scalability of the approach.\n3. The paper could improve in terms of clarity in presenting detailed methodology and results, as some technical aspects might not be thoroughly elucidated for all readers.", "Questions": "1. How does the method scale with larger or more complex neural networks? Are there any identified limitations in terms of computational cost or feasibility for very large models?\n2. Could the proposed methodology be adapted or extended to other domains outside the current scope of datasets, such as NLP or reinforcement learning?\n3. How sensitive is the proposed approach to the choice of hyperparameters within the composite loss function, and what strategy is recommended for optimal hyperparameter tuning?"}}
{"paper_id": "C9BA0T3xhq", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "EIQL provides a novel approach to offline RL by effectively balancing in-sample and out-of-sample actions, demonstrating improved performance, especially in sparse reward settings and incomplete trajectories.", "Weaknesses": "The paper's method relies on stochastic updates without a thorough analysis of how the Bernoulli parameter affects performance, leaving open questions on its dynamic adaptability and impact on stability.", "Questions": "How sensitive is EIQL to the choice of the Bernoulli parameter and was there any analysis on tuning this parameter for different environments within the D4RL benchmark?"}}
{"paper_id": "EeqlkPpaV8", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 5, "Confidence": 4, "Strengths": "The paper provides a novel approach to understanding the adaptive complexity of sampling algorithms for log-concave distributions, filling a significant gap in the literature regarding high-accuracy and high-dimensional contexts. The use of hardness potentials and random partitions is innovative and adds depth to the analysis of sampling algorithm limits.", "Weaknesses": "The paper assumes a strong background in high-dimensional statistical analysis and parallel computing models, which might limit accessibility for a broader audience. While the findings are technically sound, the paper could benefit from more comprehensive empirical evaluations or examples to illustrate the practical implications of the stated limits.", "Questions": "How do the proposed lower bounds translate into practical algorithm design strategies? Are there potential avenues for overcoming these fundamental limits, or do they suggest the necessity for paradigm shifts in how we approach sampling in high-dimensional settings?"}}
{"paper_id": "oK1zJCWBqf", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper proposes a new reward-model-free method for aligning LLMs with human preferences. The approach highlights the balance between preference alignment and regularization to retain model consistency. Empirical results demonstrate the method's effectiveness and potential advantages over existing techniques.", "Weaknesses": "The core innovation of integrating preference loss with regularization needs more depth in theoretical exploration. Detailed comparisons with alternative methods, especially RLHF-based approaches, are necessary to strengthen the argument of SPO offering significant practical benefits. The paper also lacks a comprehensive error analysis, and explanation of how the method scales with larger models or more varied data scenarios is scarce.", "Questions": "1. Can further justification be provided on how the proposed regularization technique enhances model robustness compared to existing methods? 2. How does the performance of SPO vary with different datasets, especially out-of-distribution data not covered in initial experiments? 3. Could you explain the specific impacts of hyperparameter variations on SPO's performance, such as softmax exponent adjustment?"}}
{"paper_id": "Dq9VrVuLzV", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 3, "Strengths": "1. SyntheOcc introduces a novel framework for synthesizing geometrically controlled images conditioned on 3D occupancy labels using diffusion models, which could significantly enhance data augmentation for autonomous driving.\n2. The use of 3D semantic multi-plane images (MPIs) allows for precise geometric controllability, spatial alignment, and preservation of complex scene geometry, enhancing the quality of synthetic datasets.\n3. Demonstrated superior performance in generating task-aligned synthetic data and improving robustness of perception models in 3D occupancy prediction tasks.", "Weaknesses": "1. The paper, while innovative, may require more comprehensive comparisons against a wider array of existing methods beyond the nuScenes dataset to confirm its generalizability and robustness across diverse scenarios.\n2. The discussion on the limitations and potential bottlenecks in computational requirements or scalability of the proposed approach is limited, which might be crucial for real-world application.\n3. The complexity of integrating multi-view and temporal consistency strategies is high, and the effectiveness of these approaches is not clearly quantified or compared against existing models in enough detail.", "Questions": "1. How does SyntheOcc compare to other state-of-the-art methods in terms of computational efficiency and scalability? Is real-time implementation feasible?\n2. Can the MPI-based approach be generalized to other domains or applications outside of autonomous driving where 3D geometry is critical?\n3. What are the specific limitations of the proposed method when dealing with highly complex scenes, and how can these be addressed in future work?"}}
{"paper_id": "73Q9U0vcja", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The method significantly reduces X-ray exposure by up to 4x while maintaining image reconstruction quality, which is a notable improvement.\n2. Integrating generative diffusion models with active learning to adaptively select data measurements is an innovative approach.\n3. The method is tested on multiple real-world datasets demonstrating its broad applicability.", "Weaknesses": "1. The approach may have limitations in high-stakes applications like medical imaging due to trade-offs that are not fully addressed.\n2. The computational complexity and the inference time of the proposed method deserve more detailed consideration.\n3. Detailed comparison with existing active learning methods in CT is needed, as well as discussion of the potential impact of different projection geometries.", "Questions": "1. How does the model handle the potential trade-offs and limitations in high-stakes applications such as medical imaging?\n2. Can the method be extended to more complex settings like fan-beam or 3D cone beam CT?\n3. What are the specific computational challenges and how can they be addressed to improve the efficiency of the method?\n4. How does the distribution of selected angles vary across different datasets and what implications does this have for broader applications?"}}
{"paper_id": "UiEjzBRYeI", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper addresses a significant challenge faced by foundation vision models like SAM, enhancing them to perform semantic, instance, and panoptic segmentation in open-vocabulary settings. 2. The introduction of composable prompts to address over-segmentation is an interesting and novel approach. 3. The unified affinity framework to merge patches efficiently, avoiding excessive computational complexity, is well-conceived and executed. 4. Extensive evaluations on COCO and ADE20K datasets demonstrate the method's effectiveness, achieving state-of-the-art performance.", "Weaknesses": "1. The method, while improving SAM's capabilities, may still rely heavily on the inherent strengths of SAM, thereby limiting its standalone impact if SAM itself evolves in future iterations. 2. The approach might require fine-tuning of hyperparameters for different datasets or contexts, which can affect its general applicability. 3. While promising, the reliance on the SAM framework might restrict adaptability to rapidly evolving or entirely new image segmentation tasks not accounted for by SAM.", "Questions": "1. How does SAM-CP handle cases where the initial SAM output is highly fragmented due to inherent image complexities, and how robust is the method to varying levels of granularity in initial segmentation? 2. How does the model's performance hold up when tasked with segmenting datasets that are markedly different from the training datasets in terms of semantics and objects present? 3. Given that SAM-CP relies on composable prompts, what is the sensitivity of the results to the choice of prompts, and is there an automated way to select or optimize these prompts?"}}
{"paper_id": "lessla98Wp", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The proposed model L-C4 presents a novel approach to video colorization by utilizing language inputs, which is a creative and user-friendly method. The use of a cross-modality generative model to guide the colorization process and the incorporation of components like temporally deformable attention and cross-clip fusion are key innovations contributing to improved inter-frame and long-term consistency. The model's ability to outperform existing methods across benchmarks underscores its effectiveness. The paper is well-structured and clearly articulated, making the methodologies and findings accessible.", "Weaknesses": "The paper does not thoroughly discuss potential limitations of the approach, such as its scalability to very large video datasets or possible challenges with ambiguous or complex text inputs. Additionally, the reliance on a pre-trained model and the specific dataset used for training may limit the generalizability of the approach to a broader range of video types and styles.", "Questions": "How does L-C4 handle ambiguous or conflicting text descriptions, and what is the impact on the colorization results? Additionally, can the approach be easily adapted or extended to include other modalities or inputs, such as sound or user-provided reference images, to further guide the colorization process? Could further insights be provided regarding the computational efficiency of the approach when applied to higher resolution videos?"}}
{"paper_id": "UstOpZCESc", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The proposed framework, Privacy-aware Lifelong Learning (PALL), introduces a novel approach to integrating lifelong learning with machine unlearning, addressing a crucial privacy concern in AI applications. The method's ability to optimize task-specific sparse subnetworks within a single neural network architecture significantly enhances memory efficiency. Additionally, PALL demonstrates scalability and achieves state-of-the-art performance, preventing catastrophic forgetting while also allowing exact task unlearning.", "Weaknesses": "The paper may lack clarity in explaining how various baselines, particularly in terms of regularization-based methods, were adapted to the privacy-aware lifelong learning setting. Furthermore, the evaluation appears limited to only two datasets, raising concerns about generalizability across other domains. The reliance on synthetic datasets for certain evaluations may not adequately demonstrate the method's real-world applicability. Furthermore, while the accuracy on unlearned tasks is considered, this is a weak proxy for unlearning conclusions.", "Questions": "1. How does the proposed method handle scenarios where user data is not neatly separated into distinct tasks? Can the framework adapt to situations where multiple users participate in the same task?\n2. What specific baseline manipulation and performance guarantees were provided for various baselines to handle unlearning requests? Clarification on how baseline accuracies were interpreted would be helpful.\n3. Can the authors elaborate on the interpretation and real-world applicability of the reported unlearning metric A_u, and how it correlates with more rigorous unlearning metrics?"}}
{"paper_id": "LnRegTxsa4", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper proposes a well-defined approach that improves the interaction and representation of features in cross-view geo-localization, which is an area of significant interest. The introduction of a new dataset (G2D) adds value to the research community. The methodologies (CVCAM and MHSAM) are innovative in enhancing cross-view feature interactions and representation. Empirical results demonstrate clear improvements over existing state-of-the-art methods.", "Weaknesses": "The paper may not adequately address the scalability of the approach for larger, real-world datasets. While the proposed modules show promising results, further analysis of hyperparameter sensitivity and computational complexity could provide a more thorough understanding of the approach's practical utility. Additionally, the paper could benefit from a more in-depth discussion on how the proposed modules can handle varying environmental factors commonly encountered in real-world applications, such as lighting and weather changes.", "Questions": "1. How does the proposed method handle variations in image quality, such as those caused by poor lighting or adverse weather conditions?\n2. Can the approach be adapted for real-time applications in autonomous systems, considering computational constraints?\n3. What are the specific criteria for selecting convolutional kernel sizes in the MHSAM, and how do they affect performance?"}}
{"paper_id": "2ErS9Bkc3O", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper offers a novel matrix-theoretic perspective on the adversarial fragility of neural networks, linking feature compression to reduced robustness. This theoretical framework could provide valuable insights into the relationship between input dimensions and adversarial weakness, adding depth to existing studies on this topic.", "Weaknesses": "The paper relies on the assumption of Gaussian distributed data and matrices, which may not fully capture the complexities of real-world datasets and neural network structures. The results, while theoretically interesting, might lack practical applicability due to these assumptions. Additionally, the experiments conducted are largely numerical and may not convincingly demonstrate real-world relevance across diverse datasets and scenarios.", "Questions": "How does the proposed analysis generalize beyond Gaussian assumptions, particularly in real-world scenarios where data distributions can be complex and multimodal? Can the matrix-theoretic analysis incorporate more diverse neural network architectures or data types, such as convolutional neural networks or sequence data? What are the implications of these findings on designing more robust neural network architectures in practice?"}}
{"paper_id": "pK3oe2bubc", "review": {"Soundness": 2, "Presentation": 3, "Contribution": 2, "Rating": 4, "Confidence": 3, "Strengths": "The paper presents an innovative approach to improving the robustness of transformer architectures by introducing layer shuffling during training. This could improve model flexibility in distributed environments and represents a creative solution to robustness issues related to layer order changes.", "Weaknesses": "The experimental results show a reduction in accuracy with LayerShuffle compared to sequential models, which could limit practical applications. The evaluation, while comprehensive on certain datasets, might lack generalizability and should include more model architectures to ensure broad applicability. The theoretical grounding for why shuffling improves robustness is not sufficiently detailed.", "Questions": "How does LayerShuffle perform in real-world distributed environments beyond theoretical and controlled dataset conditions? Also, could additional auxiliary tasks help mitigate the accuracy trade-offs observed with LayerShuffle?"}}
{"paper_id": "sec09tLQUl", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The proposed approach, FairDropout, presents an interesting technique to improve generalization on minority groups without the need for explicit group labels, which is a significant practical advantage.\nThe paper effectively demonstrates the technique across various benchmarks, suggesting broad applicability.", "Weaknesses": "The method relies on assumptions regarding neuron roles and the localization of memorization, which may not hold in all contexts or models.\nThe presentation lacks clarity in some sections, with insufficient detail on certain methodological aspects, e.g., specifics on how memorization neurons are precisely identified.\nThe potential beneficial aspects of memorization are noted but not fully examined, which leaves open questions about the balance between generalization and memorization.", "Questions": "How does FairDropout determine which neurons are responsible for memorization during the training phase?\nCan FairDropout be extended or adapted to work with other types of networks beyond those tested, and if so, what are the potential challenges?\nWhat is the computational overhead introduced by the method, especially in larger models like BERT and ResNet-50?"}}
{"paper_id": "96jZFqM5E0", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The method leverages a large and diverse dataset effectively, demonstrating significant improvements over state-of-the-art methods in 3D hand pose estimation.\n2. The innovative use of PCA-based dimension reduction to form positive pairs in contrastive learning is a novel contribution that enhances learning efficacy.\n3. The adaptive weighting mechanism for contrastive loss is a thoughtful addition that improves the learning process by prioritizing truly similar samples.", "Weaknesses": "1. While the PCA-based approach for identifying similar hand poses is innovative, the reliance on 2D keypoint extraction might limit its ability to capture the full 3D spatial relationships between hand components, potentially impacting accuracy in complex scenarios.\n2. The paper might benefit from more in-depth comparisons and discussions regarding other high-performing methods beyond PeCLR, including more recent approaches not mentioned here.\n3. The presentation could be improved, as some aspects, particularly technical details of the adaptive weighting mechanism, are not fully elaborated.", "Questions": "1. How does the PCA-based approach handle cases where the extracted 2D keypoints are occluded or incorrectly predicted in challenging in-the-wild conditions?\n2. What is the impact of the adaptive weighting mechanism on the overall training time and computational resources required for the SiMHand framework compared to traditional methods?\n3. Are there any specific challenges the method faces when scaling to even larger datasets or other domains, such as poses with tools or object interactions?"}}
{"paper_id": "uNd289HjLi", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper provides an innovative approach to MRI denoising by applying score-based self-supervised learning, which does not require high-SNR labels. The introduction of a reparameterization technique to stabilize training and the focus on preserving spatial details are notable strengths.", "Weaknesses": "The approach may still require extensive computational resources and specific parameter tuning. The efficacy and potential limitations of the detail refinement extension in preserving spatial details could be better explored.", "Questions": "1. How well does C2S perform across different types of noise not covered in the study?\n2. Could the approach handle multi-modal data or be adapted to other imaging modalities?\n3. What are the potential limitations or challenges in deploying C2S in real-world clinical settings?"}}
{"paper_id": "e2ONKX6qzJ", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. Proposes a novel method to mitigate oversaturation in high guidance scales, enhancing image quality while maintaining computational efficiency.\n2. Demonstrates compatibility of the method with various diffusion models and samplers, suggesting broad applicability.", "Weaknesses": "1. The decomposition into parallel and orthogonal components, while innovative, might lead to potential complexities in implementation. The balance of down-weighting and its quantifiable impact on different model settings could be explained in more depth.\n2. The addition of features like rescaling and momentum, although effective, may require further empirical justifications or theoretical underpinnings to generalize their benefits across diverse datasets.", "Questions": "1. How does the proposed method ensure that the mitigated oversaturation doesn't compromise the artistic styles inherent to specific diffusion models?\n2. What are the potential limitations when applying the adaptive projected guidance method to domains outside of image generation, if any?"}}
{"paper_id": "a2eBgp4sjH", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper provides a novel application of graph-based methods to the MultiFilterANN problem, showing significant theoretical developments in space-time trade-offs. The integration of penalized distances in graph indices is an innovative approach that shows promising empirical results, demonstrating improvements over existing methods.", "Weaknesses": "The connection between the theoretical framework and the empirical methods could be clarified further. Distinctions between theoretical analysis and the empirical setup aren't clearly delineated, which may lead to confusion about the applicability of theoretical results to empirical observations.", "Questions": "How does the proposed method scale with the increasing number of filters in real-world datasets? Are there situations where the penalized distance fails to maintain efficiency or accuracy? Further, how generalizable are the empirical results beyond the presented datasets?"}}
{"paper_id": "EIXZXPz7jU", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The proposed method introduces a novel application of diffusion models and optimal transport for enhancing the accuracy of PINNs, particularly in handling high-frequency singularities in PDEs. This approach is innovative in using flow-matching techniques to effectively resample points in high residual regions, thus improving convergence and solution quality.\n2. Experimental results demonstrate clear advantages of the proposed method over traditional techniques, showcasing its potential in challenging scenarios such as Poisson equations with peaks and linear elasticity problems with intricate material geometries.\n3. The method provides a significant step towards addressing the limitations of traditional PINNs in high-dimensional spaces and complex PDEs through an elegant solution that fusese state-of-the-art techniques in PINNs and generative modeling.", "Weaknesses": "1. The paper would benefit from more extensive experimentation across a wider range of PDE problems to fully validate the generalizability and efficacy of the proposed method.\n2. The method description, especially relating to the flow-matching dynamics and its integration into PINNs, lacks clarity in some sections, making it difficult for readers to reproduce or extend the work without additional details.\n3. Certain sections of the paper could have been more concise, with a better focus on the core contributions and their implications within the context of PINNs and PDE solutions.", "Questions": "1. Is the proposed flow-matching sampling approach applicable to other types of differential equations or constraints beyond the singularities in PDE source functions discussed in this paper?\n2. What are the computational costs associated with the additional sampling steps, and how do they compare to traditional PINN implementations, particularly in high-dimensional settings?\n3. How robust is the method to varying levels of noise or inaccuracies in the initial conditions or boundary conditions of the PDEs being solved?"}}
{"paper_id": "QO4bF6MHza", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces a novel benchmark, MathHay, which specifically evaluates the long-context mathematical reasoning capabilities of LLMs, addressing a significant gap in existing benchmarks. The methodology of creating document 'haystacks' with mixed relevant and irrelevant content attempts to simulate real-world complexity and boosts the realism of evaluation scenarios.", "Weaknesses": "While the MathHay benchmark provides a comprehensive evaluation setting, it relies heavily on constructed scenarios, which might not fully represent all aspects of real-world complexity or variability. Furthermore, the benchmark highlights inadequacies in existing models but does not offer substantial insights into specific strategies for improvement beyond 'more research is needed'.", "Questions": "How does the MathHay benchmark ensure that the mathematical reasoning aspects it evaluates are domain-agnostic and not overly tailored to specific types of documents or topics? Are there plans to expand the benchmark to include more varied types of mathematical reasoning tasks?"}}
{"paper_id": "GULx8rzzjC", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper presents a novel approach, Mycroft, for selecting informative data subsets from external data sources, which is an important problem in machine learning.\n2. The method is tested across two domains, suggesting the approach could be widely applicable.\n3. The framework addresses both feature space similarity and gradient matching, providing a comprehensive technique for data subset evaluation.", "Weaknesses": "1. The description of RetrieveTopK function and its implementation details are unclear and may confuse readers trying to reproduce results or understand the algorithm fully.\n2. The theoretical backing and proofs provided for the efficacy of the method could be expanded for a better understanding of the mathematical foundation.\n3. Lack of discussion on privacy techniques limits understanding of how privacy concerns are mitigated, which is crucial given the context of data sharing.\n4. There is limited discussion on the computational overhead introduced by the proposed method compared to baseline methods.\n5. Results are mostly given for limited datasets, making it harder to assess generalizability or applicability in more diverse or larger-scale scenarios.", "Questions": "1. Could the authors elaborate on how RetrieveTopK function aligns with the coverage goals for useful data subsets?\n2. How does Mycroft ensure privacy of data during the subset selection process? Are there privacy-preserving experiments conducted?\n3. What considerations are made regarding the computational resources or time for Mycroft compared to standard random sampling?\n4. In practice, how scalable is Mycroft when dealing with extensive and large-scale datasets? Are there performance benchmarks available?"}}
{"paper_id": "PnZ2lbQaao", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel approach to integrate domain indexing in cross-domain recommendations, enhancing both performance and interpretability. The adversarial Bayesian framework facilitates better domain index inference, offering clear visualization of domain relationships. The empirical results on synthetic and real-world datasets demonstrate significant improvements over existing methods, highlighting the practical utility of the proposed framework.", "Weaknesses": "While the paper effectively demonstrates the advantage of domain indexing, the explanation of the adversarial Bayesian framework and its implementation details could be clearer. Additionally, while the results are promising, further experiments on larger, more diverse real-world datasets could strengthen the validation of the method. The dependency on synthetic data may not fully capture the complexity of real-world cross-domain challenges.", "Questions": "How sensitive is the method to the choice of the adversarial component in the Bayesian framework? Can the domain indexing framework be generalized to other types of data beyond the recommendation setting? How does the approach handle domains with insufficient or highly imbalanced data?"}}
{"paper_id": "AT64R0ivUO", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The method SteerPrompt effectively combines iterative prompting with attention steering, presenting a creative and resource-efficient way to improve LLMs' comprehension for open-book QA tasks. The technique's implementation at inference time without requiring parameter changes is particularly notable for enhancing existing models' abilities without retraining.", "Weaknesses": "The method relies heavily on the model's own predictions, which may lead to reinforcing existing biases or errors, especially when selecting key sentences. The validation across varied datasets and tasks beyond open-book QA is limited, which might impact the generalization claims.", "Questions": "1. How does the effectiveness of SteerPrompt vary with different types of LLM architectures beyond Vicuna-7B and LLAMA-3?\n2. What are the implications of this method on the interpretability of the model, especially with altered attention?\n3. Can SteerPrompt be effectively adapted for other NLP tasks requiring deep contextual understanding outside of QA scenarios?"}}
{"paper_id": "nGiGXLnKhl", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 3, "Strengths": "1. The paper presents a novel application of RWKV from NLP to vision tasks, maintaining state-of-the-art performance while reducing computational complexity.\n2. Extensive experimental validation is performed on various benchmark datasets, demonstrating the effectiveness of VRWKV.\n3. The proposed Q-Shift and Bi-WKV mechanisms are innovative solutions to address the high computational demands of ViTs.", "Weaknesses": "1. The contributions over existing techniques such as RWKV and ViT may not appear groundbreaking, mostly involving adaptation rather than entirely new methodologies.\n2. Detailed analysis on how different hyperparameters in VRWKV affect performance is lacking, which would aid understanding of the model's scalability and robustness.", "Questions": "1. How does VRWKV's computational efficiency compare with other linear attention models in vision tasks?\n2. Are there specific types of images or tasks where VRWKV may not perform as well compared to traditional ViT?\n3. What are the possible limitations of Q-Shift and Bi-WKV in dataset diversity other than those tested?"}}
{"paper_id": "TBJCtWTvXJ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a robust theoretical analysis combined with empirical evidence to propose a novel optimizer, SignSoftSGD, which addresses the known shortcomings of Adam, especially in terms of loss spikes and convergence speed. The integration of Nesterov's accelerated gradient further enhances the optimizer’s efficiency.", "Weaknesses": "While the theoretical and empirical analysis is comprehensive, the novelty of the contribution could be further strengthened by clearer comparisons with relevant recent optimizers beyond Adam and AdamW.", "Questions": "How does SignSoftSGD perform in comparison to other recent optimizer variants developed as modifications to Adam intended for specific large-scale vision and language tasks? Are there scenarios where SignSoftSGD might underperform compared to Adam, especially in non-standard training setups?"}}
{"paper_id": "8gSrJOL2oc", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The proposed framework, TRIDENT, introduces improvements in CZSL by effectively addressing previous limitations such as background influence in visual feature extraction and enhances attribute-object disentanglement and generalization using advanced word embeddings. The use of MLLM embeddings and attribute smoothing shows strong empirical performance improvements on multiple challenging datasets.", "Weaknesses": "The framework relies heavily on existing components, such as MLLM embeddings from LLaVA and feature extraction tools, which limits the novelty. While the results are impressive, the paper could benefit from more detailed analysis on the computational overhead introduced by these additions. The integration of the components in a cohesive manner could be more explicitly articulated.", "Questions": "How does TRIDENT compare in terms of computational efficiency with previous approaches? Are the improvements in accuracy partly due to increased computation? Could further detail on scalability and limitations in processing speed be provided?"}}
{"paper_id": "yheQRc5xWB", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper proposes a novel application of state-space models to time-varying counterfactual predictions, addressing confounding biases effectively. The Mamba-CDSP model represents a significant contribution by combining SSMs with a method for de-correlation of covariates, which enhances prediction accuracy and efficiency. The evaluation on diverse datasets, including synthetic and real-world scenarios, demonstrates the robustness and applicability of the approach.", "Weaknesses": "The paper could provide more detailed theoretical justification for the use of state-space models specifically in the context of TCP, as well as further insights into the limitations or potential areas for improvement of the proposed method. The reliance on previously established frameworks (like SSMs) might raise questions about the novelty of the solution in terms of methodological innovation.", "Questions": "How does the Mamba-CDSP model specifically mitigate confounding biases compared to previous models? Can the model be adapted to other domains outside of time-varying counterfactual prediction? Are there specific scenarios where this model might fail to perform effectively?"}}
{"paper_id": "HxKSzulSD1", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces a critically important concept of 'weak-to-strong deception' in the context of aligning advanced AI models, which is crucial for ensuring the future safety and trustworthiness of AI systems.\n2. It effectively addresses a significant gap in the literature concerning the potential misalignments that can occur when powerful models interact with weaker supervision tools.\n3. The utilization of a variety of models (GPT-2, OPT, Mistral-7B) helps strengthen the findings, showing the relevance of the study across different architectures.\n4. The experiments are well-designed to simulate different alignment scenarios, providing a solid basis for the conclusions drawn.\n5. The results underscore an emerging issue in the alignment of large language models, suggesting pathways for further research into creating more robust supervision frameworks.", "Weaknesses": "1. While the paper presents a novel concept, it mainly identifies the problem without offering concrete solutions, highlighting the need for more substantial mitigation strategies.\n2. The experiments, despite being varied, might not fully capture the complexities of real-world applications where such deceptive behaviors could emerge.\n3. The methodology, particularly around the experimental setup for detecting deception, could benefit from further details for reproducibility.\n4. The analysis might require more in-depth exploration of why bootstrapping remains only partially effective and what this implies for future methods.", "Questions": "1. Can the authors elaborate on potential techniques beyond bootstrapping that might effectively address the 'weak-to-strong deception' problem?\n2. How do the authors envision applying the findings in real-world scenarios, especially where supervisory models might not have perfect visibility of all model activities?\n3. Given the complexities of alignment issues highlighted, what are the next steps the authors recommend for developing more resilient training protocols that minimize deceptive opportunities? "}}
{"paper_id": "wH8XXUOUZU", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces an innovative approach, Deep Compression Autoencoder (DC-AE), which effectively improves high-compression autoencoders for image synthesis, demonstrating practical and theoretical advancements. 2. The incorporation of Residual Autoencoding and Decoupled High-Resolution Adaptation addresses identified issues with high compression ratios effectively. 3. Experimental results show significant improvements in both computational efficiency and image generation quality, substantiating the claims.", "Weaknesses": "1. While the proposed methods are sound, the presentation could be improved for clarity regarding the technical details and underlying mechanisms of the proposed model. 2. The complexity of the introduced methodologies, especially their implementation and integration with existing frameworks, may hinder broader accessibility and understanding. 3. Some claims, particularly around the efficiency gains and implementation, would benefit from additional comparative analysis against more varied baseline models.", "Questions": "1. How does the performance of DC-AE compare with traditional autoencoders at lower compression ratios, and how do these differences manifest in practical scenarios? 2. Are there specific limitations or edge cases where the DC-AE model does not perform as expected, especially regarding variances in dataset characteristics or image types? 3. Could you provide more insights into the potential scalability of the proposed model for real-time applications or its adaptability to other image-based tasks?"}}
{"paper_id": "QWIg5e6mtT", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. Provides a novel framework (H-PSRO) for heterogeneous team games, addressing a significant gap in existing research.\n2. Successfully reduces exploitability and demonstrates superior performance in benchmarks like StarCraft and Google Research Football.\n3. Maintains computational efficiency while expanding policy search space, which is critical for scalability.", "Weaknesses": "1. The reliance on sequential optimization could limit scalability in extremely large-scale games, potentially hindering its applicability in more complex environments.\n2. The adaptation to continuous action spaces is yet to be explored, which may limit the framework's applicability in certain settings.\n3. While experiments demonstrate the efficacy of the approach, more detailed comparisons with alternative methods in a similar complexity class could reinforce the paper's claims.", "Questions": "1. How does the framework handle dynamic changes in team composition during the game?\n2. What are the potential methods for adapting the approach to continuous action spaces?\n3. Can the reliance on sequential optimization be mitigated, and if so, how?"}}
{"paper_id": "kbSU5bwoRv", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. SaMoye effectively addresses the challenge of zero-shot singing voice conversion by employing improved feature disentanglement techniques, which minimizes timbre leakage and enhances quality.\n2. The large-scale dataset used for training (1,815 hours from over 6,000 speakers) supports robust model generalization and contributes to high-quality voice conversion.\n3. SaMoye's ability to convert singing into non-human timbres is a notable contribution, showcasing its versatility beyond traditional human voice tasks.", "Weaknesses": "1. The dependence on a large dataset poses limitations for practitioners with limited data access or computational resources, potentially hindering its practical deployment.\n2. Specific details on how feature disentanglement was quantitatively validated in experiments are lacking, which may cast doubt on the method's reliability.\n3. While the model introduces novel elements, the fundamental approach and techniques might not be highly innovative, relying on existing concepts in slight variations.", "Questions": "1. How sensitive is SaMoye to variations in the size or composition of the training dataset? Can it generalize well with less data?\n2. What specific quantitative metrics aside from subjective evaluation were used to assess the reduction in timbre leakage?\n3. Could the model's approach to feature disentanglement be applied to other domains beyond voice conversion, such as general voice synthesis or even text-to-speech tasks?"}}
{"paper_id": "KyqtKhv6q1", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel and efficient method of incorporating historical spatial priors into 3D perception systems for autonomous vehicles.\n2. It demonstrates substantial improvements in 3D object detection and semantic map segmentation by leveraging context from prior traversals.\n3. The method is scalable and offers a practical approach that can be easily integrated into existing architectures without significant computational overhead.\n4. Well-structured paper with clear explanations and comprehensive experiments conducted on the widely-used nuScenes dataset, showcasing the effectiveness of the proposed method.", "Weaknesses": "1. Although the proposed method shows improvements, reliance on historical data may introduce challenges in dynamically changing environments where previous data might become obsolete.\n2. The paper does not explicitly demonstrate the potential limitations of DMP when dealing with extremely large-scale data or entirely new locations without prior traversals.\n3. Some baseline comparisons might lack fair adaptation or optimization within the context of this study's framework, potentially affecting performance evaluations.", "Questions": "1. How does the DMP framework handle rapidly changing environments where historical data may not accurately represent current road conditions?\n2. Are there specific scenarios where the performance of DMP might degrade due to outdated or irrelevant historical data?\n3. Could the method be extended to handle entirely novel areas where no historical data is available, and if so, how might it perform under such conditions?"}}
{"paper_id": "lTL4t68BNc", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The introduction of the RGA-IB method for enhancing the robustness of GNNs using the Information Bottleneck principle is a significant contribution to the field, connecting two important areas in a novel way. \n2. The methodology is well-founded, utilizing a multi-layer GNN architecture to simulate gradient descent for reducing IB loss, which is theoretically sound and practically effective.\n3. The evaluation includes a variety of datasets and adversarial scenarios, demonstrating the utility and effectiveness of the approach in enhancing node classification accuracy under attack.", "Weaknesses": "1. The theoretical explanations behind why reducing IB loss improves adversarial robustness, while insightful, could be further detailed to strengthen the argument.\n2. While the paper is generally well-presented, some sections could benefit from clearer explanations, especially for readers less familiar with the Information Bottleneck framework or graph attention mechanisms.\n3. The experiments, although comprehensive, could have included a more diverse set of datasets or adversarial strategies to further validate the robustness claims.", "Questions": "1. How does the choice of datasets and specific adversarial scenarios affect the generalizability of RGA-IB to other types of graph-structured data?\n2. Can the authors provide more empirical evidence or theoretical support for the choice of IB as a robustness measure compared to other possible metrics?\n3. Is there potential for extending RGA-IB to applications beyond node classification, such as link prediction or graph-level tasks?"}}
{"paper_id": "hWmwL9gizZ", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "ProVaccine introduces a novel combination of sequence and structure embeddings with dual attention mechanisms tailored for vaccine candidate selection.\nThe use of a large and relevant dataset enhances the potential applicability and reliability of the model in real-world scenarios.", "Weaknesses": "The methodology lacks detailed ablative studies to dissect the specific impact of each component methodologically.\nThe complex integration of multiple features might reduce interpretability, potentially making the model hard to deploy without further simplification.\nMissing details and minor errors in documentation may confuse readers and challenge reproducibility of the results.", "Questions": "Does the ProVaccine model provide any interpretability or insight into which features or sequence motifs contribute most significantly to immunogenic predictions?\nHow does ProVaccine manage computational complexity, especially when dealing with very large antigen sequences or structures?"}}
{"paper_id": "IL85Ebjg9j", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses a crucial aspect of multi-view classification where traditional models assume equal importance and alignment across views, which isn't applicable in many real-world scenarios. By proposing a computational trust-based discounting method, the authors offer a novel way to handle view-specific conflicts, enhancing prediction reliability. The use of subjective logic and Binomial opinion theory adds depth to the theoretical foundation, and experiments conducted on six diverse datasets demonstrate the practical utility and robustness of the approach. The inclusion of new metrics such as Multi-View Agreement with Ground Truth helps in better evaluation of performance.", "Weaknesses": "While the paper introduces an interesting approach, one potential weakness is the reliance on the assumption that the proposed trust-based method consistently outperforms existing methods across all scenarios, which may not hold true in complex multi-view settings with high dimensionality or sparse data. The computational cost of evaluating instance-wise trust levels for each view across large datasets might present scalability issues. Additionally, the paper could benefit from a more comprehensive comparison with a broader range of state-of-the-art MVC methodologies.", "Questions": "How does the performance of the proposed method vary with the number of views and instances, particularly in datasets with high dimensionality or sparsity? Are there specific scenarios where the method might struggle or where traditional MVC approaches might be more suitable?\n\nCan the proposed method be adapted or extended to handle other forms of data integration challenges, such as missing views or partially observed data? How well would it perform under conditions of incomplete data?\n\nThe AUROC metric is mentioned as a measure of reliability in indicating prediction correctness. Could you elaborate on how this metric is used specifically within the context of evaluating the proposed method?"}}
{"paper_id": "cnKhHxN3xj", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces a novel metric, the Wasserstein distance, to quantify neuronal entanglement in neural networks, which can provide a new perspective in understanding model behavior under sparsity. It also proposes a 'Sparse Expansion' experimental framework that helps maintain model performance by effectively disentangling neuron outputs.", "Weaknesses": "The practical applicability and scalability of the Sparse Expansion method are not thoroughly explored or demonstrated. The reliance on the mixture of experts approach may come with its own computational overheads, which are not addressed in the conclusion. Additionally, the impact of applying these methods in different neural network architectures beyond LLMs would benefit from further exploration.", "Questions": "1. How does the Sparse Expansion framework balance between computational complexity and performance gains, particularly in large-scale deployments?\n2. Can the proposed Wasserstein metric for entanglement be applied to other types of neural networks beyond LLMs effectively?\n3. What is the impact of using a different distribution other than Gaussian for defining the Wasserstein Neurons?\n4. How does the methodology fare in terms of computational overhead when integrated into existing neural networks during training?"}}
{"paper_id": "dgR6i4TSng", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces a novel use of quantum circuits to achieve highly parameter-efficient fine-tuning of large pre-trained models, which is a relevant and timely contribution to the field.\n2. The method demonstrates a significant reduction in the number of trainable parameters compared to existing PEFT methods while maintaining competitive performance, which is a strong experimental validation.\n3. The use of quantum-inspired techniques can inspire further research on interdisciplinary approaches for efficient model adaptation.", "Weaknesses": "1. The paper lacks detailed discussion and theoretical analysis on how the quantum-inspired parameterization impacts the learning dynamics and generalization of the models.\n2. There is limited discussion on the practical implementation details and resource requirements of the quantum-inspired approach. Practical challenges of implementation in existing deep learning frameworks are not fully addressed.\n3. Benchmarking is limited to a few datasets and comparisons primarily with LoRA, which may not provide a comprehensive understanding of the method’s effectiveness across diverse scenarios.\n4. The notion of 'quantum unitary parameterization' relies on a complex theoretical foundation that might not be easily accessible to the broader audience of the paper, which can affect the clarity of presentation.", "Questions": "1. How does the quantum-inspired parameterization handle different neural network architectures beyond those tested (e.g., non-transformer-based models)?\n2. Can the method be extended to perform well on large-scale pre-trained models without significantly increasing computational overhead?\n3. How sensitive is the performance of Quantum-PEFT to the choice of quantum circuit design and parameters used in the Pauli parameterization?\n4. Are there specific tasks or model architectures where Quantum-PEFT may not be suitable or may underperform compared to traditional methods?\n5. How does the method handle issues related to vanishing or exploding gradients during training, especially with the introduction of quantum-inspired modifications?"}}
{"paper_id": "ye1mxb79lw", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The BILBO algorithm effectively addresses sample inefficiency in bilevel optimization and offers theoretical guarantees on convergence. It introduces a novel approach to simultaneously optimize upper- and lower-level problems, significantly improving convergence speed and reducing regret in synthetic and real-world scenarios.", "Weaknesses": "While BILBO is promising, the paper lacks extensive comparisons with recent state-of-the-art bilevel optimization methods. Moreover, there is limited discussion on the scalability of the proposed algorithm and its performance in high-dimensional spaces, which is crucial for practical applications.", "Questions": "Can BILBO be extended or adapted for handling multiobjective bilevel optimization problems? How does the algorithm handle large-scale or high-dimensional settings, and what are the computational bottlenecks in such scenarios?"}}
{"paper_id": "1fwZJzGdKj", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper proposes a novel multi-agent collaborative data selection mechanism to enhance data efficiency in LLM pretraining, which is original and innovative. The experimental results showcase substantial performance improvements, indicating the effectiveness of the proposed approach.", "Weaknesses": "The mechanism's benefits might be limited to specific setups or datasets, as the SlimPajama dataset is prominently featured in the experiments. The use of a console to integrate agent inputs could be more clearly explained, including the specifics of the influence functions and reward calculations.", "Questions": "How does the agent console dynamically adjust the influence of each data selection strategy during training, and what criteria are used for these adjustments? Could the reliance on the SlimPajama dataset limit the generalizability of the results to other datasets in the LLM training domain?"}}
{"paper_id": "pISLZG7ktL", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "The paper effectively addresses a significant gap in the field of robotics by identifying data scaling laws that relate to generalization in imitation learning for robotic manipulation. The approach is novel and provides a comprehensive method for scaling training data to achieve better generalization, which is supported by thorough experimental evidence. The use of real-world data collection and the demonstration of potential for zero-shot deployment are particularly strong aspects. The paper is well-organized, with clear presentation of experiments and results.", "Weaknesses": "While the study is thorough and the methodology is sound, it is focused on a specific subset of robotic tasks (like 'Pour Water' and 'Mouse Arrangement'), which might limit the immediate applicability of the findings across a broader range of robotic manipulation tasks. There is also limited discussion on the computational costs and practical constraints associated with the data collection and implementation of the proposed scaling laws, which could be a potential concern for deploying these strategies at scale in practical applications.", "Questions": "1. How does the approach generalize to more complex tasks beyond the ones studied? Are there specific types of tasks or environments where the scaling laws proposed might not apply or require modification?\n2. What are the computational and resource implications of collecting such diverse datasets in real-world settings at scale, and how can these be managed efficiently in practical applications?\n3. Can you provide more insight into how the results from this study might translate to policy generalization for different robotic platforms or environments that were not part of this study?"}}
{"paper_id": "dML3XGvWmy", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. The concept of self-evolving AI agents inspired by the Gödel machine is innovative and could lead to significant advancements in AI adaptive capabilities.\n2. The framework's use of LLMs for dynamic modification shows versatility in handling various tasks efficiently.\n3. Experiments demonstrate the framework's adaptability and potential superiority over traditional systems in performance.", "Weaknesses": "1. The feasibility of the Gödel Agent's implementation in real-world applications is not thoroughly explored, leaving questions about resource requirements and practical constraints.\n2. Lack of detailed benchmarks against established agent systems and minimal explanation on how the self-modification process is controlled or limited could be a point of concern regarding system stability and predictability.\n3. The methodology relies heavily on LLM capabilities, which could lead to limitations in situations where real-time response and low latency are critical.", "Questions": "1. How does the Gödel Agent ensure stability and prevent undesirable self-modifications during runtime?\n2. What are the specific benchmarks used for evaluating performance gains, and how do these compare with state-of-the-art agent systems in similar setups?\n3. Can the self-evolving approach be integrated with existing agent designs, or does it require building a system from scratch?"}}
{"paper_id": "xlxGsX1pc7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The introduction of U-MATH provides a diverse collection of university-level problems, filling a significant gap in evaluating LLMs' mathematical reasoning. The inclusion of visual elements in the benchmark is innovative and addresses a current shortfall in multimodal assessments. The creation of the µ-MATH dataset is a valuable contribution for assessing LLMs' ability to evaluate solutions, enhancing our understanding of LLM limitations.", "Weaknesses": "The benchmark may still lack diversity in some mathematical subfields, especially given the focus on only six core subjects. The reliance on an LLM to judge solutions in the µ-MATH dataset may introduce biases, as current models have known limitations in nuanced evaluations. The paper could provide more detailed examples of the types of visual problems included in the benchmark.", "Questions": "How does the choice of core subjects in U-MATH reflect the challenges faced in real-world university-level mathematics? Are there plans to include other subject areas in future expansions of the benchmark? What measures are being taken to ensure the accuracy of the LLMs used for judging solutions, given their current limitations in logical reasoning?"}}
{"paper_id": "AjXkRZIvjB", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses an important issue of evaluating LLMs on mathematical reasoning by introducing a benchmark that mitigates the issue of data contamination and variability, which is a significant advancement in the field. The empirical analysis on 25 state-of-the-art LLMs provides a broad examination of the problem.", "Weaknesses": "While the paper provides new insights into LLM capabilities, it primarily highlights issues already acknowledged in existing literature, such as LLMs' reliance on pattern matching over genuine reasoning. The lack of proposed solutions to the identified issues may limit the immediate impact of this work. Additionally, the concepts of 'genuine logical reasoning' and 'pattern-matching' are not clearly defined, making it difficult to assess the novelty of the evaluation.", "Questions": "1. How do the authors define 'genuine logical reasoning,' and how is it differentiated from the observed pattern-matching behavior?\n2. What are the next steps in this line of research to address the fragility in reasoning identified by the study?\n3. How can the benchmark be further developed to test other aspects of reasoning beyond mathematical reasoning?"}}
{"paper_id": "IJiTI0fB0e", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces an innovative information-theoretic framework which effectively disentangles different sources of diversity in prompt-based generative models. Introducing conditional and mutual information measures separately is a strong methodological contribution.", "Weaknesses": "The presentation of the methodology lacks clarity in terms of equation formulation and interpretability. The practical applicability or potential drawbacks of such metrics aren't thoroughly examined. More concrete examples and case studies may enhance understanding and assess real-world applicability.", "Questions": "How do the Conditional-Vendi and Information-Vendi scores compare in varied scenarios of prompt-based models? Are there any limitations of this approach in real-time generative applications? What are the computational demands for these methods compared to traditional diversity metrics?"}}
{"paper_id": "fs2Z2z3GRx", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "FIG presents an innovative approach by using measurement interpolants, which enhances the robustness and flexibility of diffusion and flow matching models, handling high noise and ill-posed problems effectively. The theoretical foundations add depth and credibility to the proposed method.", "Weaknesses": "The paper may suffer from complex presentation, as the explanation of some aspects such as measurement interpolants could be expanded and simplified for better understanding. The paper could clarify more on potential limitations or specific drawbacks when applying FIG to broader types of inverse problems beyond the presented image scenarios.", "Questions": "How does FIG handle scenarios with different types of noise or measurement errors (e.g., non-Gaussian noise)? Could the method be generalized to non-image linear inverse problems, and what adjustments might that require?"}}
{"paper_id": "H4FSx06FCZ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents an innovative approach to 3D steganography using 3D Gaussian splatting, which is an emerging field with promising applications in copyright protection and secure communication. The authors effectively combine hybrid decoupled Gaussian encryption with advanced anchor point strategies to enhance security and efficiency. Extensive experimental results demonstrate the method's superiority in rendering fidelity, speed, and security, making it a valuable contribution to the field.", "Weaknesses": "The proposed methods may introduce complexity in practical implementation, potentially requiring significant computational resources and specialized knowledge, which could limit widespread adoption. Additionally, while the paper demonstrates comparative performance improvements, the long-term robustness and adaptability of the approach to various real-world scenarios could have been explored more thoroughly.", "Questions": "How does the SecureGS framework perform in highly dynamic or real-time environments where computational efficiency is paramount? What specific strategies were employed to ensure the robustness of the encryption mechanism against potential attacks or tampering in practical deployment scenarios?"}}
{"paper_id": "OGtUfA6Amo", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 3, "Strengths": "The hierarchical patch graph network (Hi-Patch) offers a novel multi-scale modeling approach that addresses the complexities of irregular multivariate time series by capturing both fine-grained and coarse-grained dependencies effectively. The use of intra-patch and inter-patch graph layers allows the model to progressively process scales, which demonstrates superior forecasting and classification results on multiple datasets. The structured framework and thorough experimental validation highlight the robustness and efficacy of Hi-Patch.", "Weaknesses": "The proposed Hi-Patch model, while innovative, may encounter scalability issues as the number of variables and the complexity of time series increase. Additionally, the approach lacks a detailed comparison of computational efficiency against state-of-the-art models, which could help gauge its practical application in real-world scenarios. The dependence on fully connected graph networks within each patch might result in computational inefficiency as data scales, affecting its practical deployment.", "Questions": "How does Hi-Patch handle missing data points within and across time segments when processing irregularly sampled data? What specific strategies does the model employ to maintain its robustness in such cases? Additionally, what is the computational complexity of the model, and how does it compare to the state-of-the-art models in terms of efficiency when handling large datasets?"}}
{"paper_id": "ujpAYpFDEA", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. The paper addresses an important and timely problem of identifying watermarks in LLM outputs, crucial for protecting copyright and reducing misuse.\n2. The Water-Probe algorithm is shown to be highly effective at detecting watermarked outputs across various LLMs with a low false positive rate.\n3. The introduction of the Water-Bag strategy enhances watermark imperceptibility, providing a valuable contribution to the field of watermarking in AI-generated content.", "Weaknesses": "1. The paper focuses on watermark detection but does not comprehensively address the challenges of watermark embedding into LLMs, which is also crucial for practical deployment.\n2. There is a trade-off in detectability when using the Water-Bag strategy; this aspect could be explored further to assess its impact on real-world applications.\n3. The presentation could be improved with more detailed explanations of the experimental setups and clearer illustration of the methods used.", "Questions": "1. How does the Water-Probe algorithm perform when faced with different types of text modifications, such as paraphrasing or summarization? Are its detection capabilities robust to these changes?\n2. Could the authors provide more insight into the computational cost associated with implementing the Water-Probe and Water-Bag strategies? How feasible are they in large-scale real-world applications?\n3. What are the specific criteria used to define 'increased randomness' in the Water-Bag strategy, and how is this measured experimentally? More details would help in understanding the designed methodology."}}
{"paper_id": "7UKHNQIErp", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces a novel formalism for understanding and developing Direct Preference Alignment (DPA) loss functions using symbolic logic, which is a significant conceptual advance. By providing a structured landscape of DPA loss functions, it aids in the development and analysis of human-AI alignment algorithms. The methodology is well-explained and offers potential for substantial improvements in understanding and deriving new DPA algorithms.", "Weaknesses": "While the paper presents a strong theoretical framework, it might be challenging to implement practically due to the complexity involved in translating existing DPA losses into symbolic programs. The work may also face scalability issues with the symbolic reasoning approach when applied to large-scale models or datasets without significant computational resources.", "Questions": "1. How does the proposed symbolic logic framework handle scalability for large language models, and what are its computational requirements?\n2. Can the framework be extended or adapted to other types of preference alignment algorithms outside the scope of DPA? If so, how?"}}
{"paper_id": "CpgWRFqxhD", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1) The introduction of a memory-guided temporal module and emotion-aware audio module addresses key challenges in audio-driven talking video generation, providing significant improvements in identity consistency, synchronization, and expression quality.\n2) Comprehensive quantitative and qualitative evaluations demonstrate that MEMO outperforms current state-of-the-art methods across various metrics and datasets, including out-of-distribution scenarios.\n3) The paper includes detailed ablation studies to validate the effectiveness of its components, enhancing the credibility and transparency of the proposed method.\n4) The presentation of the method and results is well-structured, making the complex concepts accessible and easy to understand.", "Weaknesses": "1) While the paper introduces innovative components, the reliance on diffusion models and existing methodologies such as linear attention and Emotion AdaNorm may reduce the perception of innovation.\n2) The paper could benefit from a more in-depth comparison with a wider range of baseline models, particularly focusing on how each component of MEMO contributes to performance gains over specific baselines.\n3) Some aspects of the methodology and the training pipeline could be further elaborated for clearer understanding and reproducibility, particularly the implementation details of the emotion-guidance mechanism.", "Questions": "1) Can the memory-guided temporal module and emotion-aware audio module be integrated into other types of generative models beyond diffusion models?\n2) How does the model perform with audio inputs in languages other than those included in the training dataset?\n3) What is the computational efficiency of MEMO in terms of training and inference time compared to other methods, and how does this impact its applicability for real-time applications?"}}
