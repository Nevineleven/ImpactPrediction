{"paper_id": "B0OwtVEejJ", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper proposes an innovative approach to integrate weight-entanglement and gradient-based neural architecture search (NAS), which merges two previously separate sub-fields. This integration enhances performance, training properties, and performance efficiency while maintaining memory efficiency. The availability of code increases the transparency and reproducibility of the research.", "Weaknesses": "The paper might lack detailed theoretical analyses or benchmarks that conclusively prove the superiority of the proposed approach over existing methods across various domains. The compatibility and efficiency on diverse tasks and datasets need more comprehensive empirical evidence.", "Questions": "How does the proposed method generalize to other architecture search spaces or tasks beyond those tested? Are there specific scenarios where the integration of weight-entanglement with gradient-based methods might not be beneficial?"}}
{"paper_id": "ZlEtXIxl3q", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces a novel approach using contrastive loss functions in the context of global epistasis models, offering an improvement over the traditional MSE loss. The technique is validated through consistent improved performance on benchmark tasks, demonstrating its practical utility.", "Weaknesses": "The approach's reliance on a single type of loss function may limit its applicability across different types of biological data. The paper could benefit from a broader evaluation across various datasets to confirm the generality of the proposed method.", "Questions": "How does the proposed method perform on other types of biological sequence data beyond the benchmark tasks mentioned? Can this approach be generalized to other domains outside of protein engineering?"}}
{"paper_id": "6yJuDK1DsK", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The approach introduces a novel lightweight method for lifelong test-time adaptation that significantly reduces the number of additional parameters needed. FEATHER is capable of functioning without access to source data and can be integrated with existing methods, offering robustness against error accumulation.", "Weaknesses": "The paper doesn't extensively detail the comparison of computational costs with other methods. The performance claims, while impressive, rely heavily on specific benchmark datasets, which may not represent all possible real-world scenarios for test-time adaptation.", "Questions": "How does FEATHER perform on non-image data or models other than CNNs? What are the implications of potential limitations in settings outside the tested benchmark datasets? How effective is FEATHER in adapting over extremely long periods or highly dynamic distributional changes?"}}
{"paper_id": "lt6xKGGWov", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces a novel supervised feature selection approach leveraging neural estimation of mutual information, allowing for the evaluation of feature subsets as ensembles. This approach captures sophisticated feature-target relationships and demonstrates potential for improved feature selection accuracy.", "Weaknesses": "The paper does not provide extensive details on the experimental setup, datasets used, or comparisons with comprehensive benchmarks, which makes it difficult to fully assess the effectiveness and generalizability of the proposed method. Additionally, the computational complexity and scalability of the method are not discussed.", "Questions": "1. How does the proposed method compare with existing feature selection methods on large, real-world datasets?\n2. What are the computational costs associated with the neural estimation approach, and how does it scale with large feature sets?\n3. Are there specific types of feature-target relationships where this method excels compared to traditional methods?"}}
{"paper_id": "Bvrc6kobWd", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper provides a novel perspective on understanding margins in contrastive learning by using gradient analysis, which could enhance theoretical insights into the method. Experimental results show improved generalization performance across datasets, indicating practical applicability.", "Weaknesses": "The explanation of the methodology, particularly in relation to how gradient effects are decomposed and analyzed, could be clearer. The theoretical insights need stronger empirical connections. The impact of additional factors beyond emphasis on positive samples and gradient scaling seems underexplored.", "Questions": "How do the proposed gradient-based insights translate into practical guidelines for model training? Are there specific scenarios or datasets where the proposed method is most beneficial?"}}
{"paper_id": "FJjHQS2DyE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper presents a novel approach to improve unsupervised domain adaptation by minimizing conditional symmetric support divergence, which is a significant improvement over traditional methods relying on covariate shift assumptions. The introduction of a theoretical target risk bound provides a strong theoretical basis for the approach, enhancing its credibility.", "Weaknesses": "The methodology may require complex implementation efforts due to the new concepts introduced, such as the conditional symmetric support divergence and the theoretical target risk bound. The empirical evaluation, though promising, might be limited to specific UDA benchmark tasks, which may not fully demonstrate the generalizability of the approach across various real-world applications.", "Questions": "How does the proposed method perform across a wider variety of real-world datasets beyond the standard UDA benchmarks? Are there any potential limitations in scalability of the approach due to increased computational or implementation complexity?"}}
{"paper_id": "xelrLobW0n", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 6, "Confidence": 4, "Strengths": "The introduction of TRACE as a new benchmark addresses a gap in evaluating continual learning capabilities of LLMs. The approach highlights the potential issues of catastrophic forgetting and explores solutions to mitigate it, such as the Reasoning-augmented Continual Learning (RCL).", "Weaknesses": "While TRACE provides a new benchmark, it remains uncertain how it accounts for prior exposure of models to similar datasets during instruction tuning, which could affect the validity of results. The paper could benefit from a more extensive comparison with existing benchmarks and techniques in continual learning.", "Questions": "How does TRACE ensure that it challenges models without recalling prior exposure during instruction tuning? Could you elucidate how the reasoning tasks were chosen to mitigate the loss of general abilities in the LLMs?"}}
{"paper_id": "bYQkOPvgDw", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper presents an innovative probabilistic graphical model to address label noise in graph neural networks, which is a less explored area compared to graph perturbation and attack.\n2. The proposed PRGNN framework appears effective in high noise-rate scenarios and on heterophilous graphs, addressing common weaknesses of existing methods.\n3. The paper includes extensive experiments to validate the robustness and effectiveness of the proposed models.", "Weaknesses": "1. The reliance on a probabilistic graphical model may introduce complexity, and it is unclear how it compares to simpler or alternative approaches without such a model.\n2. It would be beneficial to have more clarity on how the model particularly addresses and performs better on heterophilous graphs, beyond standard benchmarks.\n3. The paper does not clarify whether the proposed methods are applicable in inductive settings or only transductive ones.", "Questions": "1. Could the authors provide more insights into potential challenges or limitations when extending the PRGNN framework to inductive learning scenarios?\n2. Given the extensive experiments, what are the main conditions or constraints under which PRGNN significantly outperforms other methods?\n3. Are there any real-world datasets or benchmarks that inherently contain the types of noise addressed in the paper, beyond synthetically generated data?"}}
{"paper_id": "sKPzAXoylB", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The approach addresses two key issues of continual learning (catastrophic forgetting and loss of plasticity) simultaneously, which is innovative. UPGD demonstrates strong performance across various tasks in a challenging streaming learning setup. The method shows superior results in reinforcement learning experiments, indicating its robustness.", "Weaknesses": "The paper may lack clarity in the explanation of the utility-based perturbations and how they are applied. Additionally, the setup of experiments might benefit from more detailed descriptions to allow reproducibility and ensure the robustness of comparisons against other methods.", "Questions": "How sensitive is UPGD to the choice of hyperparameters, and are there any recommendations for their selection? Can the method be extended or adapted to other neural network architectures, and how well would it generalizes to different application domains?"}}
{"paper_id": "H8Qg1IIMaR", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper identifies a novel and specific vulnerability in current language and vision-language models that could impact their deployment.", "Weaknesses": "The scope of the vulnerability analysis could be expanded to include more diverse datasets and scenarios to firmly establish the generality of the findings.", "Questions": "Have potential mitigation strategies for permutation sensitivity been explored, or are there suggestions for architectural changes that might reduce this vulnerability?"}}
{"paper_id": "tth2qXY7RU", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel quantization technique, Super Floating-Point (SuFP), that targets the balance between accuracy and computational efficiency without sacrificing performance. The proposed method achieves impressive improvements in computational capacity and energy efficiency. The use of multi-region piecewise quantization and tensor-wise scalable bias is an innovative approach. The paper provides extensive experimental results demonstrating significant advantage over state-of-the-art approaches.", "Weaknesses": "The paper does not thoroughly discuss potential limitations of SuFP, such as the trade-offs involved in its implementation and possible edge-case scenarios where it may not perform as expected. The practical implementation challenges of deploying SuFP in existing systems could be further elaborated.", "Questions": "Can the SuFP technique be adapted or generalized for other emerging domains or models beyond vision, language, and generative models? What are the potential impacts of SuFP on the hardware landscape beyond AI accelerators?"}}
{"paper_id": "fjf3YenThE", "error": "'NoneType' object has no attribute 'model_dump'"}
{"paper_id": "H9DYMIpz9c", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper proposes a novel data distillation approach that significantly reduces dataset size while maintaining model performance. Utilizes efficient reverse-mode differentiation and latent-space factorization, showing impressive empirical results across tasks. The reduction in data size required for similar performance offers valuable implications for scaling large autoregressive models.", "Weaknesses": "Potential scalability issues for larger datasets or models may not be fully explored. The paper could benefit from a more detailed analysis of computational efficiency and training time comparisons to models trained from scratch.", "Questions": "How well does Farzi scale when applied to even larger models and datasets beyond those tested? What are the computational trade-offs in terms of training time and resources compared to conventional training with full datasets?"}}
{"paper_id": "4kLVvIh8cp", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper proposes a new oracle-efficient algorithm, PNLSVI, that is designed for offline reinforcement learning with non-linear function approximation, addressing a gap in the current literature.\n2. The algorithm is innovative with three key components and achieves a minimax optimal instance-dependent regret, which is a significant theoretical contribution.\n3. Extends prior results from simpler function classes to a more general framework, thus increasing applicability in the field.", "Weaknesses": "1. Although the theoretical contributions are significant, the lack of empirical evaluation to substantiate the practical benefits of the proposed algorithm limits its impact.\n2. The dependencies of the regret bound on the function class complexity are tight, but it would be beneficial to know how these apply in practical scenarios versus theoretical ones.\n3. Some assumptions in the paper might not be realistic or widely applicable, which could hinder the practical implementation of the algorithm.", "Questions": "1. Can the authors provide some insights or examples where the assumptions needed for this algorithm hold in real-world applications?\n2. Is there an intuitive explanation for the need for the variance-based weighted regression scheme and how it specifically benefits the proposed method?\n3. Are there further plans to extend this work to include empirical results or adapt existing datasets to test the algorithm's effectiveness?"}}
{"paper_id": "CSpWgKo0ID", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper explores an important and novel area of research by applying behavioral game theory to analyze the social behavior of large language models (LLMs). The results provide new insights into the behaviors of LLMs in various game-theoretic settings, highlighting differences in their strategies and adaptations.", "Weaknesses": "The methodology for creating and testing game strategies from LLM outputs is not entirely clear and might lack robustness. The games selected, while representative, may not fully capture the range of social scenarios that LLMs might encounter. Furthermore, the results and conclusions could benefit from more detailed analysis and broader evaluation criteria.", "Questions": "1. How exactly were the strategies operationalized from the LLM outputs? What factors influenced the choice of specific games for the experiments?\n2. Were there any significant differences in behavior between different versions of GPT, and what might account for these differences?\n3. How might the robustness of LLM strategies be improved to enhance their performance in coordination games like the Battle of the Sexes?\n4. Could additional social scenarios or games further illuminate the LLMs' behavioral tendencies?"}}
{"paper_id": "gwbQ2YwLhD", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper provides theoretical insights into the impact of variable scale on structure learning algorithms using square-based and log-likelihood-based losses. Theoretical findings are backed up by extensive empirical experiments on both synthetic and real-world data, enhancing the reliability and applicability of the conclusions drawn.", "Weaknesses": "The paper may not introduce a fundamentally new approach or algorithm, but rather focuses on the theoretical implications of existing methods under certain conditions. The work primarily critiques and extends prior findings, which may limit its novelty and broader impact.", "Questions": "How would incorporating these findings into existing structure learning algorithms improve their accuracy or robustness in practice? Are there potential methods or preprocessing steps that could mitigate the scale effects highlighted in the paper?"}}
{"paper_id": "eIYDKNqXuV", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. The algorithm effectively handles complex manifold data and autonomously determines the number of clusters. 2. Demonstrates competitive performance on real datasets with known ground-truth labels, particularly in terms of the normalized mutual information (NMI) score. 3. It boasts impressive computational efficiency, making it suitable for large-scale datasets.", "Weaknesses": "1. The innovation primarily lies in the combination of existing methods, such as K-Means and a community detection algorithm, which might limit the novelty. 2. It is unclear how the proposed method performs on datasets without clear ground-truth labels. 3. The paper does not discuss potential limitations or drawbacks of the algorithm.", "Questions": "1. How does Village-Net Clustering perform when applied to datasets without known ground-truth labels? 2. What are the potential limitations or constraints of using this algorithm in practice?"}}
{"paper_id": "bAMPOUF227", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a simple yet effective framework that significantly enhances the reliability of Large Language Models (LLMs) for generalizability and factuality. It provides a comprehensive suite of resources, including datasets and model outputs, and demonstrates how LLMs can benefit from discriminative models, which is an innovative approach. The proposed plug-in method is empirically validated and shows improved performance on a variety of tasks.", "Weaknesses": "While the paper presents a novel approach, it could expand further on the theoretical groundwork of how the integration of task-specific models benefits LLMs. Additionally, there might be scalability concerns or limitations regarding the computational efficiency of the proposed method when applied to very large-scale LLMs.", "Questions": "How does the proposed approach affect the scalability and computational resources required for very large scale LLMs? Are there any limitations to the types of tasks or models that can benefit from this framework?"}}
{"paper_id": "eNoiRal5xi", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel approach called Unknown Domain Inconsistency Minimization (UDIM) which combines parameter and data perturbations for effective domain generalization, potentially leading to improved performance over existing Sharpness-Aware Minimization (SAM) techniques. Theoretical validation is provided alongside empirical evidence across multiple benchmarks, demonstrating its effectiveness in unseen domain scenarios.", "Weaknesses": "The approach may rely heavily on the assumption that perturbed data accurately represents unseen domains, which might not be universally applicable. The computational cost associated with the additional perturbations and loss landscape alignments could also be significant, potentially impacting the method's applicability in resource-constrained environments.", "Questions": "How does the method perform in terms of computational efficiency compared to existing SAM-based techniques? Are there specific cases where UDIM might not provide significant benefits over traditional methods?"}}
{"paper_id": "uU0Adp7Sfo", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces a novel approach CCGAN, which combines collaborative and competitive mechanisms in GANs to enhance data generation quality and stability. The theoretical grounding provides a strong basis for its claims, particularly regarding equilibrium and training convergence. The experimental results across multiple datasets demonstrate the practical applicability and improvements over baseline models.", "Weaknesses": "While the concept is well-established, the paper could benefit from a clearer explanation of the impact of CCGAN's collaborative component, especially compared to existing multi-agent GAN frameworks. The presentation would be improved with better visualizations and examples to concretely demonstrate the qualitative improvements claimed. Additionally, more detailed experimental insights and comparisons with diverse GAN frameworks would strengthen the empirical evidence.", "Questions": "How does CCGAN perform in terms of computational overhead compared to conventional GANs? Is there a trade-off? What are the specific challenges in tuning hyperparameters for CCGAN, and how might they differ from standard GANs? How does the proposed method handle diverse dataset characteristics, such as those with more complex or chaotic structures?"}}
{"paper_id": "dKju7tbe6D", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel approach that leverages both iterative and parallel computation for reasoning tasks, effectively combining benefits from both computation types. It demonstrates strong reasoning and generalization capabilities, achieving state-of-the-art performance on challenging datasets such as CLEVR-Humans. The approach also aids interpretability by allowing for visualization of computation steps.", "Weaknesses": "The explanation of key components of the proposed system, particularly the parallel reasoning mechanism, lacks clarity, making it difficult to grasp how parallel processing is achieved. Some design decisions appear non-intuitive and require further elaboration. The visualizations, although insightful, could be expanded upon and better explained to highlight the unique contributions of the iterative and parallel mechanisms.", "Questions": "How exactly does the parallel reasoning mechanism work within the proposed framework? Can you provide more detailed visualizations or examples to clarify the impact of the iterative and parallel processing components on performance and interpretability?"}}
{"paper_id": "DL7JWbdGr3", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper tackles a significant problem in epidemic prediction by leveraging self-supervised learning (SSL) to utilize data from past epidemics. It shows promising results with PEM outperforming previous state-of-the-art methods across various datasets, including Covid-19, highlighting efficiency and adaptability.", "Weaknesses": "The methodology, while sound, lacks novelty as it primarily applies existing SSL concepts to a new domain. More detailed experimental comparisons with baseline methods would strengthen the claims of superior performance. The clarity of the approach could be enhanced by providing more details on the SSL task design and baselines.", "Questions": "How does the proposed model handle significantly varied dynamics in the datasets used for pre-training? What specific aspects of epidemic dynamics are captured that contribute to the PEM's improved performance?"}}
{"paper_id": "1PaDPHDhwe", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses a relevant issue in the field of distributionally robust optimization by introducing a simple yet effective scaling strategy that can be directly applied to debiasing algorithms. The empirical results demonstrate that the approach can improve both robust and average accuracies without additional training. The introduction of a unified metric to summarize the trade-off between accuracies is a valuable contribution for evaluating existing methods.", "Weaknesses": "While the proposed scaling techniques are practical and easy to implement, their simplicity may limit the novelty of the contribution. The paper would benefit from a deeper theoretical analysis of why these methods work and how they can be generalized. The presentation could be improved with clearer explanations of technical details and more comprehensive experimental results. Comparison with more recent state-of-the-art methods would enhance the evaluation section.", "Questions": "1. How does the performance of the proposed scaling techniques vary across different datasets and tasks? \n2. Can the instance-wise adaptive scaling technique be adjusted or extended to handle specific types of biases in different domains, such as computer vision or NLP?\n3. Are there any limitations or potential pitfalls in using the unified metric for evaluating robust optimization methods?"}}
{"paper_id": "iI7hZSczxE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper presents a novel approach to learning disentangled representations for time series data, specifically targeting electricity usage, which is a relevant problem for energy optimization and reducing carbon footprint. The introduction of the Time Disentangling Score (TDS) as a metric for evaluating disentanglement quality is a valuable addition to the field.", "Weaknesses": "The abstract does not provide specific details on how the method is implemented or evaluated, making it difficult to assess the complete methodology's efficacy. The connection between the proposed method and its practical applications could be elaborated upon, particularly in terms of how it significantly advances current approaches.", "Questions": "How does the proposed disentanglement approach specifically improve over existing methods that assume attribute independence in real-world applications? Could you elaborate on the practical implications and potential limitations of using the TDS metric?"}}
{"paper_id": "qrGjFJVl3m", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The Qwen-VL series sets new performance records on visual-centric benchmarks with generalist models of similar scales. 2. The integration of a visual receptor and a multilingual multimodal cleaned corpus demonstrates an innovative approach to enhancing model capabilities. 3. Public availability of the models encourages research and further exploration in the field.", "Weaknesses": "1. The abstract lacks clarity on specific novel contributions, focusing instead on results and performance. 2. There might be limited information on how the approach compares to baseline models in terms of methodology, rather than just performance metrics.", "Questions": "1. How does the performance of Qwen-VL compare to baseline models in terms of architectural efficiency and training resources? 2. What specific challenges were encountered when aligning the image-caption-box tuples, and how were they addressed?"}}
{"paper_id": "qup9xD8mW4", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel concept of behavior distillation for reinforcement learning, which does not require access to expert data, addressing a significant gap in the domain. HaDES method shows remarkable results in distilling information into just four state-action pairs while achieving competitive performance, demonstrating robustness and versatility. The experimental results are compelling, indicating potential advancements in training multi-task agents and neural architecture search.", "Weaknesses": "The practical applicability of the proposed method may be limited since behavior distillation without expert data might not always be feasible depending on the complexity of tasks. While the theoretical contributions are significant, further exploration and validation on a broader range of domains would strengthen the claims. The paper could expand on the discussion about potential challenges and limitations of this method in specific RL contexts.", "Questions": "What are the specific limitations of HaDES in more complex and high-dimensional tasks, and how does it handle diversity in state-action spaces? Can the method be extended to environments where the state space is less clearly defined or continually evolving?"}}
{"paper_id": "Zw8YxUWL4R", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces a novel approach, the Extended Textual Conditioning space (P+), which enhances control over text-to-image diffusion models. The proposed Extended Textual Inversion (XTI) offers improved expressive and precise modeling, with faster convergence than existing methods. The method shows better reconstruction and editability without the need for balancing these goals, which is a significant advantage.", "Weaknesses": "The effectiveness and advantages of the proposed XTI method need more theoretical grounding; the improvements are primarily demonstrated through experimental results. The distinction and representation of the extended space and the per-layer tokens might require further clarification. In-depth analysis of how the unique properties of P+ lead to improved object-style mixing would strengthen the work.", "Questions": "How does the proposed P+ space enhance the synthesis process, and what are the specific benefits over existing textual conditioning methods? Can the authors provide more insights into the theoretical basis for the observed improvements in expressiveness and convergence?"}}
{"paper_id": "3NXhwkZGjz", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel approach to SFUDA by introducing multiple prediction hypotheses and consolidating these to identify the most likely correct pseudo-labels. This method shows state-of-the-art performance and can be easily integrated with existing models to enhance their effectiveness.", "Weaknesses": "The necessity and practical applicability of the multiple hypotheses approach may require further clarification, given that creating multiple high-quality hypotheses from the same domain seems challenging and wasteful. Additionally, some performance metrics do not reach state-of-the-art levels in all cases, indicating room for improvement in specific tasks.", "Questions": "How are the optimal values for $\\tilde{k}$, $\\tau_1$, and $\\tau_2$ determined, and how might these values vary across different datasets or tasks?"}}
{"paper_id": "Fn655mJ4bv", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper proposes a novel technique for interpreting structured output models, which is a relevant and complex problem that has not been extensively addressed in prior research. The introduction of an energy-based training process adds a new perspective to understanding structured outputs, allowing the method to capture correlations and provide more nuanced explanations.", "Weaknesses": "The presentation of the methodology could be improved, as certain components are not thoroughly explained, possibly affecting reproducibility. Additionally, while the paper mentions various datasets, specific details about the evaluation process and comparison with existing methods seem to be lacking.", "Questions": "How does the proposed method compare quantitatively with other state-of-the-art interpretation techniques on similar tasks? Are there specific types of structured output models where this technique might not be applicable or effective?"}}
{"paper_id": "VyMW4YZfw7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper challenges current conventions in GNN design by suggesting that simpler non-deep learning methods may achieve similar or better results, which is a valuable perspective.", "Weaknesses": "The work might lack novelty as it largely revisits and applies existing nonparametric estimation techniques rather than introducing new methods. The experimental validation to support claims about evaluation convention shifts could be more comprehensive.", "Questions": "How do the simpler spectral methods perform on new, more complex SSNC tasks compared to recently developed GNN architectures?"}}
{"paper_id": "09xFexjhqE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel approach by disentangling the optimization paths for adversarial and natural objectives, potentially addressing a known issue in Robust Fine-Tuning (RFT). The proposed AutoLoRa method is of significant practical importance as it simplifies the tuning process by automatically converting pre-trained models for adversarially robust downstream tasks without extensive hyperparameter tuning. Extensive empirical evaluations show promising improvements.", "Weaknesses": "The explanation for the automated scheduling of hyperparameters lacks depth, and the empirical results in Table 5 do not clearly illustrate its advantages. Some details, like the constant factor in formulae, remain unexplained and lack validation. There is a lack of diverse architectural evaluation beyond ResNet, which limits generalizability claims.", "Questions": "How does the AutoLoRa method ensure stability across different backbone architectures, beyond ResNet? Could the authors provide more insight into the choice and impact of the constant factor in the learning rate scheduling?"}}
{"paper_id": "nmBjBZoySX", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces an adaptive, dynamic, and automated framework for identifying Graph Lottery Tickets (AdaGLT), which addresses the key limitations of existing GLT paradigms. \n2. The proposed method provides theoretical guarantees for mitigating over-smoothing and improving sparse structures in deep GNN scenarios.\n3. Extensive experiments across various graph datasets showcase the superiority of AdaGLT over state-of-the-art models, especially in deep GNN use cases.", "Weaknesses": "The main disadvantage highlighted is the scalability issue when applying to deeper and larger-scale GNNs, but AdaGLT seems to partially address this. Nonetheless, it remains to be confirmed how well the method scales compared to simpler alternatives.\nThe need for extensive computations was not clearly articulated in the experiments, which remains a key consideration for practical applications in resource-constrained environments.", "Questions": "How does AdaGLT compare to simpler sparse training methods in terms of computational cost and efficiency, especially in very large-scale graphs? Are there specific scenarios or graph types where AdaGLT may not be as effective?"}}
{"paper_id": "di52zR8xgf", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a significant improvement in the architecture of the latent diffusion model, leveraging a larger UNet and novel conditioning schemes that show dramatic improvements over previous versions.\n2. SDXL achieves competitive results with state-of-the-art black-box image generators, indicating its effectiveness.\n3. The introduction of a refinement model further enhances the visual fidelity of generated images.", "Weaknesses": "1. The abstract lacks details on the potential computational cost implications of the larger model and additional components, which could be relevant for practical deployments.\n2. There is limited discussion on the limitations or potential trade-offs involved with the new architecture, such as training time, resource requirements, or scalability.", "Questions": "1. What are the computational cost implications of using a three times larger UNet and additional conditioning schemes?\n2. How does SDXL perform on diverse datasets that may contain biases, and are there any measures in place to address them?"}}
{"paper_id": "0b328CMwn1", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The introduction of activation prompting (AP) offers a novel way to enhance visual prompting by utilizing intermediate activation maps, addressing some limitations of traditional input-level VP.\n2. The paper provides a comprehensive theoretical and empirical analysis across 29 datasets, showcasing the performance and efficiency benefits of AP over conventional methods.", "Weaknesses": "1. The novelty of the AP approach may be perceived as incremental, given the extensive research already conducted in parameter-efficient model adaptation techniques.\n2. Comparisons with pre-existing methods should also consider varying settings like different backbones, as this may affect the fairness of the comparisons.\n3. Despite extensive experiments, the impact of the proposed AP method on real-world tasks with substantial performance gaps remains somewhat unclear.", "Questions": "1. How does the performance of AP scale with larger models beyond CNNs and ViTs, or in more diverse real-world applications?\n2. Can the activation prompting (AP) method be easily integrated with other existing parameter-efficient techniques to leverage their respective strengths?"}}
{"paper_id": "AMDKqZcZbi", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper proposes an innovative approach by drawing inspiration from biological neural systems, specifically utilizing content-addressable heteroassociative memory and a spatially invariant convolutional network architecture. The proposed model demonstrates strong empirical performance, outperforming existing ANN baselines in continual and few-shot learning tasks. The new task, sequential Morris Water Maze (sWM), introduces a challenging benchmark for evaluating rapid and continual learning capability.", "Weaknesses": "The paper may lack sufficient discussion on the scalability of the proposed approach to more complex or varied tasks beyond the specific navigation task used. The reliance on specific biologically inspired architectures might limit the generalizability of the findings to broader machine learning applications. The evaluation is limited to comparisons with existing ANN baselines and may benefit from a broader set of benchmarks.", "Questions": "How does the proposed model handle tasks that are not inherently spatial in nature? Can the principles outlined be applied effectively to such tasks? \n\nWhat are the computational overheads associated with implementing a content-addressable heteroassociative memory compared to traditional neural network architectures?\n\nAre there any specific limitations or assumptions inherent in the choice of architectures that should be considered when applying this model to real-world problems?"}}
{"paper_id": "am7BPV3Cwo", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper provides a novel framework, ImOOD, which effectively addresses the issue of class imbalance in OOD detection, offering a strong theoretical foundation. The proposed method consistently outperforms state-of-the-art approaches across several benchmarks, indicating its robustness and effectiveness.", "Weaknesses": "The paper may lack evaluation on diverse types of OOD datasets, which could test the method's applicability in broader scenarios. Some aspects of the proposed framework, like the specifics of the post-hoc normalization and training-time regularization, could benefit from more in-depth explanation.", "Questions": "How does ImOOD perform on OOD datasets with varying degrees of class imbalance? Are there any limitations or assumptions of ImOOD that might affect its generalizability to other datasets or domains?"}}
{"paper_id": "R1crLHQ4kf", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper presents an innovative and effective method for detecting adversarial attacks in ASR systems using probability distribution characteristics.\n2. The method shows high accuracy in distinguishing adversarial examples, with AUROC scores above 99%.\n3. The approach is comprehensive, addressing the robustness of detection methods under adaptive attacks.", "Weaknesses": "1. The paper could expand on the types of adversarial attacks tested and include more diverse attack methodologies.\n2. Some clarity is needed on the complexity and real-time applicability of the proposed detection method within live ASR systems.", "Questions": "How does the proposed detection approach perform with real-time ASR systems in practical, dynamic environments compared to static datasets?"}}
{"paper_id": "i7P2mK3x3o", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces a flow-based model to transport data between arbitrary distributions P and Q, potentially broadening the utility of such models in generative tasks. Notably, the approach extends beyond conventional normalizing flow methods by allowing flows to map between non-normal distributions and enables several downstream tasks, demonstrating versatility in applications like mutual information estimation and image translation.", "Weaknesses": "The theoretical claims, like ensuring optimal transport, might be unsubstantiated without further empirical validation, particularly with real-world benchmarks. The paper might suffer from insufficient comparison to existing methods in practical scenarios and could benefit from strengthening the empirical evaluation to clearly demonstrate the proposed method’s advantages.", "Questions": "The abstract implies improved performance metrics on tasks like density estimation and images translation. How do these performances quantitatively compare to state-of-the-art techniques in these areas? Additionally, is the choice of flow models based on certain theoretical advantages over alternative approaches, or is it empirical?"}}
{"paper_id": "YIls9HEa52", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces an innovative improvement over the recurrent switching linear dynamical system (rSLDS) model by addressing its limitations, such as fixed state number and latent structure. The proposed model incorporates a semi-Markov discrete state process with latent geometry, which is a significant advancement. The use of partial differential equations (PDE) theory to derive a semi-parametric formulation for dynamical sufficient statistics is noteworthy, showing a strong theoretical foundation.", "Weaknesses": "The abstract lacks detail on the empirical performance improvements of the irSLDS model compared to existing models, making it difficult to assess the practical impact. There is also limited discussion on the computational requirements and scalability of the model, which could impact its applicability. Furthermore, the clarity and organization of the presentation could be enhanced to improve understanding.", "Questions": "How does the performance of the proposed irSLDS model compare quantitatively with existing models in terms of accuracy and computational efficiency? What are the specific limitations in terms of computational resources and time when scaling the model to larger datasets or more complex neural activity patterns?"}}
{"paper_id": "koYsgfEwCQ", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces a novel 3D scene generative model, DynaVol, which unifies geometric structures for object-centric learning with a differentiable volume rendering framework.\n2. The method uniquely integrates object-centric voxelization and canonical-space deformation for dynamic scene decomposition.\n3. Demonstrates significant performance improvements over existing unsupervised methods for scene decomposition.", "Weaknesses": "1. The description of the methodology could be clearer, especially concerning the different stages of the process, such as warmup vs. dynamics stages.\n2. The definition and role of certain components, such as 'L_point', are not sufficiently explained.\n3. The segmentation results appear somewhat messy in the results presented, with a limited number of real video experiments conducted.", "Questions": "1. Can you clarify the distinct roles of the warmup stage versus the dynamics stage in your method?\n2. How is the number of objects N determined, and could connected components be used to automatically choose N?\n3. Would it be feasible to utilize an invertible mapping instead of the cycle-consistent deformation networks for forward and backward dynamics?"}}
{"paper_id": "o6AN2ZoNw2", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces a simple and effective approach for open-vocabulary semantic segmentation utilizing pseudo-mask and language training. It achieves state-of-the-art results on standard benchmarks without the need for fine-tuning, and it shows scalability with data. Additionally, the promise to release code and demos will aid in further research.", "Weaknesses": "The novelty of the method is somewhat limited, as it closely resembles existing approaches like ZegFormer, and there are significant omissions in baseline comparisons, such as OVSegmentor and TCL. Furthermore, critical ablations on design choices are missing, which reduces the insights into why the proposed method works. Presentation could be improved by providing more details on the evaluation protocol, pseudo-mask generation, and self-training procedure.", "Questions": "How does the proposed method compare directly with recent state-of-the-art methods in open-vocabulary segmentation not included in the comparisons? What specific textual prompts or templates are used for class and background labels? What is the effect of the choice of K in K-Means for pseudo-mask generation? How critical is setting K = 8, and how does it affect performance?"}}
{"paper_id": "HXZK1Z8tHa", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces ShareFormer, a novel Transformer architecture that aims to improve latency and trainability in high-resolution image restoration tasks. The use of shared attention maps to improve inference speed and the incorporation of residual connections to facilitate optimization are strong contributions. The results are supported by numerous experimental validations, and the plan to open-source the code and models is commendable.", "Weaknesses": "The introduction of shared attention maps, while innovative, may not be entirely novel as similar concepts have been explored in other works. The paper lacks some detailed ablation studies to thoroughly isolate and prove the impact of individual components like CSAU. The text could benefit from clearer explanations and more detailed analysis of certain design choices.", "Questions": "Is the concept of sharing attention maps entirely novel to this paper, or have similar ideas been proposed in prior works? Could you provide more evidence or visualization to support the claim of amplified locality bias due to the residual connections on 'Value'? How do you ensure fairness in comparison with other models given potential differences in training methodologies, such as patch size during progressive learning?"}}
{"paper_id": "QO3yH7X8JJ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. Novel approach in leveraging existing pre-trained DGMs without additional training efforts, addressing a key limitation in image super-resolution.\n2. Introduction of the Perceptual Recoverable Field (PRF) as a theoretical insight for balancing noise injection, offering clear innovation and practical utility.\n3. Extensive experiments showcasing superior performance and adaptability across diverse environments, indicating robustness of the method.", "Weaknesses": "1. While the concept of using pre-trained models is advantageous, the dependency on noise injection requires careful calibration which might limit ease of use or repeatability without specific guidelines.\n2. Although extensive experiments are claimed, the lack of detailed quantitative and visual results for various arbitrary-scale super-resolution tasks can be a limitation for evaluating effectiveness comprehensively.", "Questions": "1. How does the choice of noise level adapt across different scales, and are there guidelines for selecting these parameters pragmatically?\n2. Can the proposed Diff-SR method seamlessly integrate with all existing DGMs or are there specific prerequisites for DGMs to be compatible with this method?\n3. The paper mentions a trade-off between low-level fidelity and high-level signature. Could more insights or examples be provided to illustrate this trade-off within the application scope?"}}
{"paper_id": "sNtDKdcI1f", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper addresses a noteworthy issue in the field of RLHF, highlighting the correlation between output length and perceived helpfulness, offering important insights into model behavior.\n2. The authors provide a thorough analysis of reward models across multiple datasets, contributing to the ongoing discussion about the optimization of RLHF models. \n3. By experimentally testing interventions that can decouple length from reward, this study pushes forward our understanding and potential methodologies, although partially, in refining RLHF without length bias.", "Weaknesses": "1. While the study successfully identifies the issue of length bias, the proposed interventions to mitigate it do not uniformly solve the problem across all tested scenarios, indicating a need for further research.\n2. The analysis, while comprehensive, may benefit from more intuitive presentation models and clarity in how figures and data are described, to enhance readability and comprehension.\n3. The findings, although impactful, provide limited actionable strategies for developers wanting to address this bias in practical applications, necessitating further research into concrete solution paths.", "Questions": "1. Do the findings from your analysis suggest specific systemic changes that developers could implement immediately to counteract the length bias issue in RLHF?\n2. How generalizable are the results concerning length bias to other modalities or are they primarily confined to RLHF in language models? Would visual or multimodal RLHF tasks exhibit similar patterns?\n3. Reflecting on your findings that lengthier responses might contribute to rewards, what implications do you anticipate for RLHF research focusing on other qualitative attributes like relevance or engagement?\n4. Are there alternative metrics or approaches proposed in your study to gauge 'helpfulness' without conflating it with response length, and how effective might they be? "}}
{"paper_id": "HFtrXBfNru", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses an important issue with GNNs related to representation distortion as graphs evolve over time, providing a theoretical basis for the problem and introducing a novel method for mitigating it through self-supervised graph reconstruction.", "Weaknesses": "The theoretical assumptions and conditions under which the bound is derived might be too strong or unrealistic in practical scenarios. The method comparison could be expanded to include more diverse baselines to strengthen empirical claims.", "Questions": "How sensitive is the Smart method to different types of graph evolutions beyond the ones covered in the experiments?"}}
{"paper_id": "4u0ruVk749", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper introduces a novel use of diffusion models for estimating individualized treatment effects by modeling unobserved confounders.\n2. The authors demonstrate consistent improvements over state-of-the-art methods using standard evaluation metrics on both synthetic and benchmark datasets.", "Weaknesses": "1. The assumptions and setup for the diffusion model related to unobserved confounders may need further elaboration and justification.\n2. The explanations given in the abstract about the process and significance might be challenging for readers to fully grasp without prior knowledge of diffusion models.", "Questions": "1. Can the authors provide more intuition or visual explanation on how the reverse diffusion process effectively captures unobserved confounders?\n2. How sensitive is the proposed model to the choice of priors or the nature of the apriori knowledge used?\n3. Are there specific scenarios where the proposed methodology is expected to have limitations?"}}
{"paper_id": "Pa6SiS66p0", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 2, "Rating": 5, "Confidence": 3, "Strengths": "The paper proposes a novel benchmark for multi-modal continual learning and demonstrates that using multiple modalities can help mitigate catastrophic forgetting in neural networks. The approach of integrating and aligning information from different modalities using relational structural similarities is innovative. The paper also sets a strong baseline for future research in this area.", "Weaknesses": "The evaluation is primarily conducted on a single dataset (VGGSound), and there is a lack of comparison with more recent and varied baselines. Additionally, while the paper provides a benchmark, the contribution in terms of proposing a new learning technique or method is somewhat limited. There is also a need for more empirical evidence across different datasets to substantiate the claims about the effectiveness of multi-modal learning in continual learning scenarios.", "Questions": "Can you provide a comparison of your method with more recent continual learning approaches? How does your benchmark compare with existing ones in terms of usability and coverage of different modalities? What are the specific challenges in extending your approach to other modalities, such as language?"}}
{"paper_id": "rkplYfqUr0", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel generative prompting framework for zero-shot text classification that addresses issues of miscalibration and brittleness in LMs. The approach integrates contextual information about labels, which is a significant contribution. The empirical results are impressive, with consistent outperforming of zero-shot and few-shot baselines across multiple benchmarks.", "Weaknesses": "One potential weakness is the reliance on the quality of natural language label descriptions, which may vary in practice. The method might also need more evaluation on diverse linguistic or domain-specific tasks to establish broader applicability. Moreover, the complexity of setting up detailed label descriptions might hinder its usability in some applications.", "Questions": "Can the approach be extended to other NLP tasks beyond text classification? How sensitive is the performance to the quality or variability of label descriptions used in the generative framework?"}}
{"paper_id": "pTU2X9mUBe", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents the first large-scale, publicly available dataset focused on last-mile delivery, which is a significant contribution to logistics and spatio-temporal data research. The dataset's comprehensiveness and diversity make it a valuable resource for multiple research communities. The clear documentation of its characteristics and open accessibility are strong points.", "Weaknesses": "While the dataset is a valuable contribution, the paper lacks detail on addressing data privacy concerns robustly. Furthermore, while the paper benchmarks baseline models, it would benefit from a more detailed analysis of the dataset's unique insights or potential challenges in applying existing models.", "Questions": "How does the dataset specifically handle privacy concerns regarding courier and client data? Are there plans to regularly update this dataset to reflect changes in logistics operations or urban environments?"}}
{"paper_id": "uhtQyRrTzY", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The proposed dataset-curation approach allows large-scale multi-view dataset generation without the need for additional annotations, which is a significant step forward in representation learning. Experiments show that representations from the MIMIC datasets outperform those learned from expensive, annotated datasets, indicating effectiveness and scalability.", "Weaknesses": "The presentation could be improved for clarity, and additional contexts or motivations could be provided for why certain choices were made, such as the specific tasks used for evaluation. More details on the limitations of the proposed method would also strengthen the paper.", "Questions": "How does the proposed method generalize to other tasks beyond those specifically discussed in the paper? What are the potential biases introduced by the automatically generated datasets?"}}
{"paper_id": "WZfatbNdLV", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces TrASPr, an innovative predictive model using transformer architecture for tissue-specific RNA splicing prediction, which stands out in its performance compared to recent models. The combined use of predictive modeling and generative approaches positions the work as a novel contribution to the RNA splicing field.", "Weaknesses": "The reliance on Bayesian Optimization with a custom loss function for RNA outcomes could lead to concerns about the generality and reproducibility of the results. Furthermore, more comprehensive comparisons and benchmarks with established tools in the field would strengthen the claims of superiority.", "Questions": "How does TrASPr's interpretability in identifying regulatory features compare with traditional models? Could the method for designing RNA splicing outcome through generative modeling be applied to other biological sequences with similar success?"}}
{"paper_id": "YkRwadXWHd", "error": "'NoneType' object has no attribute 'model_dump'"}
{"paper_id": "q4Bim1dDzb", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a unified voxelization framework, dubbed UniVoxel, for inverse rendering which accelerates the process significantly. The integration of spherical Gaussians for modeling illumination provides an elegant solution that meshes well with other scene attributes. Experimental results demonstrate significant speed-ups in training time while maintaining good reconstruction quality.", "Weaknesses": "The paper could benefit from a clearer explanation of how UniVoxel directly compares to existing unified voxelization frameworks outside of time efficiency. There could be deeper consideration of how different scene complexities affect the framework and its limitations in terms of handling varying lighting and material conditions.", "Questions": "Does the use of spherical Gaussians impose any specific limitations in terms of the range or complexity of scenes that UniVoxel can handle effectively? How does the performance vary across different types of scenes or under varying lighting conditions?"}}
{"paper_id": "39cPKijBed", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper introduces an innovative approach to mitigate dataset bias in diffusion models using time-dependent importance reweighting. This addresses a crucial aspect of fairness in generative models.\n2. Strong theoretical backing is provided, linking the new method to traditional score-matching and establishing convergence to an unbiased distribution.\n3. The experimental results show significant performance improvements across various datasets and bias settings, demonstrating the practical applicability of the method.", "Weaknesses": "1. The concept of time-dependent density reweighting can be complex and may be challenging for practitioners to implement without extensive understanding of the theoretical aspects.\n2. The experiments, while thorough, primarily focus on standard datasets, and the applicability to more diverse or complex distributions may need further exploration.", "Questions": "1. How does the proposed method affect the computational efficiency of the diffusion model training process?\n2. Can this method be generalized to other types of generative models beyond diffusion models?"}}
{"paper_id": "aG3EARrrd1", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper introduces a novel predictive statistical model called the Online Sparse Residual Tree (OSRT) that dynamically adapts to streaming data.\n2. OSRT incorporates both online tree decomposition and adaptive radial basis function exploration, which enhances its capability to handle large, evolving datasets efficiently.\n3. The method maintains the latest trends in data by updating its final layer based on the most recent data.\n4. Empirical results suggest that OSRT outperforms existing online radial basis function methods in terms of efficiency and accuracy.", "Weaknesses": "1. The paper does not clearly outline the limitations and potential drawbacks of the proposed OSRT model compared to other methods.\n2. There is a lack of theoretical analysis supporting the model's performance and convergence guarantees, which is critical for understanding its applicability in different contexts.\n3. While the paper claims efficiency and accuracy improvements over existing methods, it would benefit from providing more detailed comparisons and quantitative results.\n4. The explanations and presentation could be clearer, particularly in describing the integration and operational nuances of the model components.", "Questions": "1. How does the OSRT model's computational complexity scale with respect to the size and dimensionality of the input data streams?\n2. Are there specific scenarios or dataset characteristics where OSRT might not perform well compared to existing online RBF methods?\n3. What are the parameters and hyperparameters of the model, and how sensitive is the model to these settings?"}}
{"paper_id": "Ts95eXsPBc", "error": "'NoneType' object has no attribute 'model_dump'"}
{"paper_id": "LfhG5znxzR", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces an innovative approach to make neural networks more interpretable by introducing codebook features, offering a novel way to discretize hidden states. It showcases that networks can maintain performance even with this quantization, opening avenues for network behavior control. The proposed method successfully overcomes the superposition problem in a finite state machine dataset and demonstrates its potential in guiding language model generation.", "Weaknesses": "The method's effectiveness largely hinges on manual intervention to identify relevant codes, which may limit its scalability or broad applicability. There is a lack of detailed analysis on the potential trade-offs between model expressiveness and interpretability introduced through quantization. Additionally, considerations on practical integration and deployment in existing neural architectures are minimally addressed.", "Questions": "How scalable is the proposed method for very large-scale neural networks, and what are its implications on computational efficiency? Are there specific limitations in terms of depth or size of the network that might affect the quantization approach? Could there be an automated way to associate codes with specific semantic concepts, reducing the need for manual intervention?"}}
{"paper_id": "bUGzjiUsIq", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper addresses a novel and challenging problem of partially annotated multi-label classification, which is relevant in many practical scenarios where obtaining full annotations is difficult.\n2. The proposed PAPG framework effectively combines reinforcement learning and supervised learning, showcasing improvement over existing methods in handling class imbalance and limited annotations.\n3. The introduction of local and global rewards is an interesting approach to deal with class imbalance.", "Weaknesses": "1. The motivation and differentiation from existing PU learning methods should be clearer, as the problem might appear similar to PU learning initially.\n2. Details on how the proposed rewards and iterative training strategy specifically address the scarcity of positive annotations and label imbalance are not thoroughly explained, leaving some ambiguity about the approach's effectiveness.\n3. The paper could benefit from a more thorough discussion on how the method handles label correlations, a critical aspect of multi-label classification that is not deeply explored.", "Questions": "1. How does PAPG ensure better handling of label correlations compared to other multi-label classification methods?\n2. Can the proposed method be directly compared to existing PU learning methods for effectiveness analysis?\n3. Are there limitations in the proposed PAPG framework when applied to datasets with a higher degree of label sparsity?"}}
{"paper_id": "uMAujpVi9m", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel pretraining approach that significantly improves pocket representation by leveraging high-resolution protein structures and pretrained small molecule representations. By simulating ligand-receptor interactions through the creation of over 5 million complexes, it effectively addresses the limited availability of real pocket-ligand complex data. The method, ProFSA, achieves state-of-the-art performance across multiple tasks such as druggability prediction and ligand binding affinity prediction.", "Weaknesses": "The paper may lack a detailed description of the potential limitations or assumptions in their pocket-ligand interaction modeling approach. Additionally, while the approach shows strong empirical results, more theoretical analysis on why the method achieves such performance is desirable. Furthermore, the scalability and practical applicability of generating such a large synthetic dataset need careful evaluation.", "Questions": "How does the method ensure that the synthetic complexes generated accurately simulate real ligand-receptor interactions? Are there specific benchmarks or evaluations that confirm the quality of the synthetic pairs compared to real data? What are the computational costs associated with generating and processing 5 million complexes?"}}
{"paper_id": "uqxBTcWRnj", "error": "'NoneType' object has no attribute 'model_dump'"}
{"paper_id": "o4CLLlIaaH", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The proposed Generalizable Neural Point Field (GPF) introduces a novel paradigm shift by employing point-based rather than image-based rendering, which addresses significant issues related to occlusions and geometric discontinuities.\n2. The paper presents a compelling improvement in rendering speed and reconstruction quality through its nonuniform log sampling strategy and learnable kernel for feature aggregation.\n3. The ability to achieve superior geometries, view consistencies, and rendering quality in both generalization and fine-tuning settings suggests strong empirical performance over existing methods.", "Weaknesses": "1. The reliance on geometric priors and neural features could limit the generalizability if such priors are inaccurate or difficult to obtain, potentially affecting the robustness of the approach in varied contexts.\n2. The evaluation is limited to three datasets, which may not fully demonstrate the approach's adaptability or limitations across diverse scenarios or more complex environments.\n3. While there is a mention of preliminary proving of the new paradigm's potential, the experimental results need to be more thorough to solidify this claim.", "Questions": "1. How does the dependence on geometric priors for visibility modeling affect the method's performance in dynamically changing scenes where priors may not be accurate?\n2. Can the authors provide insights into potential computational overhead introduced by the learnable kernel and the nonuniform log sampling strategy compared to traditional NeRF approaches?\n3. How scalable is the Generalizable NeRF approach with respect to large-scale datasets or real-time applications, given the shift to point-based rendering?"}}
{"paper_id": "KNzL1nglNB", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The proposed LRM is a novel approach that draws from neural collapse to manage scenarios with insufficient labels. The theoretical analysis of the relationship between LRM and ERM is a strong point. Empirical demonstrations show LRM's effectiveness across various scenarios such as semi-supervised learning and domain adaptation.", "Weaknesses": "The paper lacks sufficient experimental evidence to demonstrate generalizability across diverse datasets. Limited clarity in presentation may hinder understanding, particularly on how the label encodings are estimated and aligned. The applicability of LRM as a plugin could be more clearly illustrated.", "Questions": "What are the computational overheads when using LRM compared to traditional methods? How does LRM handle scenarios with completely unlabeled target datasets aside from domain adaptation? Can LRM be effectively integrated into existing deep learning frameworks?"}}
{"paper_id": "pvhyBB86Bt", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses a critical aspect of neural network training: the stability of the Jacobian matrix throughout training, rather than only at initialization. The theoretical framework is impressive, utilizing advanced results from random matrix theory to extend stability results. The study contributes to the understanding of why initialization schemes are crucial, which is a valuable addition to the existing literature. Moreover, the paper demonstrates the applicability of the framework to various neural network scenarios, including sparse networks and non-iid initialization.", "Weaknesses": "The theoretical nature of the paper might limit its accessibility to those without a strong background in random matrix theory. While the paper provides a solid theoretical framework, practical implications and experiments to demonstrate the real-world effectiveness of these theoretical findings are limited. The assumptions made for stability during training might be too restrictive, as it requires conditions that are not thoroughly justified theoretically.", "Questions": "How can the results be generalized or applied to real-world neural network architectures beyond theoretical implications? Do the assumptions about stability during training hold in practical scenarios, and how can they be empirically validated?"}}
{"paper_id": "Mylk5iamJC", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper introduces an innovative approach to simultaneously address both closed-set recognition (CSR) and open-set recognition (OSR) using a Latent Gaussian Mixture Model (L-GMM).\n2. The framework is well-designed with a collaborative training algorithm that uniquely combines elements of generative and discriminative models.\n3. Experimental results demonstrate improved performance over traditional discriminative models in both CSR and OSR tasks.", "Weaknesses": "1. The presentation of the formalization of open-set recognition based on learning theory could be clearer, as some parts appear dense and could be better explained.\n2. The explanation of the collaborative training algorithm could benefit from more detail or examples to enhance reader understanding.\n3. Limited discussion on the generalizability of the approach to other types of data beyond image recognition and segmentation.", "Questions": "1. How sensitive is the L-GMM model to the choice of hyperparameters, and what strategies do you recommend for tuning them effectively?\n2. Can this approach be effectively adapted to other domains beyond image recognition, such as text or audio data, and what modifications might be needed?"}}
{"paper_id": "MsOcVFzv8D", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper provides a theoretical foundation for Multi-domain text classification (MDTC), which is a significant leap over existing empirical work.\n2. The proposed margin discrepancy-based adversarial training (MDAT) approach is well-aligned with the theoretical insights provided and shows promising results compared to state-of-the-art methods.\n3. Conducting empirical studies on two MDTC benchmarks supplemented the theoretical findings and validated the efficacy of the proposed method.", "Weaknesses": "1. While the paper claims novel theoretical contributions, the analysis seems heavily derived from existing work by Zhang et al., 2019, potentially limiting its originality.\n2. The presentation of theoretical results lacks clarity in emphasizing new insights versus adaptations of existing results to MDTC.\n3. A deeper discussion on the limitations and potential pitfalls of the proposed approach would strengthen the paper.", "Questions": "1. How does the proposed MDAT method perform in scenarios where the domain divergence is high?\n2. Can the proposed margin discrepancy-based adversarial training approach be adapted to other multi-domain problems beyond text classification?"}}
{"paper_id": "fBlHaSGKNg", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces a novel sample selection methodology for SSL, addressing budget constraints through a modified Frank-Wolfe algorithm and the innovative use of the alpha-Maximum Mean Discrepancy criterion. The proposed method, UPA, is shown to consistently improve SSL method performance, offering a significant contribution to improving low-budget learning scenarios.", "Weaknesses": "The potential complexity of the proposed methodology may pose implementation challenges, and there is a need for further clarification on the choice and impact of the alpha-MMD criterion. The empirical evaluation, while promising, could benefit from broader testing across more varied data sets to fully establish generalization claims.", "Questions": "How does the alpha-MMD criterion specifically enhance generalization, and are there specific types of data or models where UPA faces limitations? Can UPA be easily integrated with other existing SSL techniques?"}}
{"paper_id": "HwQ8NVvdmm", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper proposes a novel approach using Hadamard transforms to facilitate low-precision training via integer-only operations, which potentially reduces computational demands significantly for continual learning on edge devices. The empirical results demonstrating minimal accuracy degradation with significant bit-width reduction are promising and suggest practical applications in privacy-sensitive and low-latency environments.", "Weaknesses": "The method's generalizability to a wide range of datasets and tasks is not thoroughly established; more diverse datasets and neural architectures should be considered to strengthen the claims. The practical implementation of stochastic rounding and tiled matrix multiplication on typical hardware platforms may introduce complexities not addressed in the paper. The connection between the proposed technique and the alleviation of privacy concerns remains somewhat indirect.", "Questions": "How well does the proposed method integrate into existing machine learning workflows on edge devices, particularly in terms of hardware compatibility and real-world deployment? Could the authors clarify how stochastic rounding is implemented and its computational implications? Are there specific settings or types of models where the proposed method yields more pronounced benefits?"}}
{"paper_id": "cZo6pDtDZr", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents novel algorithms for estimating and testing collision probability with improved sample complexity, satisfying local differential privacy, and offering nearly optimal sample complexity. These are significant improvements and advancements in the field.", "Weaknesses": "The presentation could be clearer, particularly for readers unfamiliar with the field. More background information and explanations of technical terms would enhance comprehension.", "Questions": "How do the proposed algorithms compare in real-world scenarios beyond experimental settings? Are there any specific applications where these improvements could have a significant impact?"}}
{"paper_id": "F7QnIKlC1N", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper introduces GTMGC, a novel approach using Graph-Transformer for predicting the 3D configurations of molecules from 2D structures, which is a significant step forward in molecular modeling.\n2. The proposed Molecule Structural Residual Self-Attention mechanism is a noteworthy contribution, potentially useful in other molecular modeling tasks.\n3. Comprehensive evaluation on Molecule3D and QM9 datasets underscores the effectiveness of the method.", "Weaknesses": "1. While the method shows improved performance, details on computational efficiency compared to established methods like RDkit are sparse.\n2. The novelty of the approach may be limited as it relies on integrating existing technologies rather than introducing fundamentally new concepts.\n3. There could be more clarity around the limitations and assumptions underlying the proposed method, such as any biases present in the training data.", "Questions": "1. How does the proposed GTMGC model compare in terms of computational efficiency with traditional DFT approaches, particularly on larger molecules?\n2. Can the Molecule Structural Residual Self-Attention mechanism be adapted for other types of molecular modeling tasks beyond what's tested in this paper?\n3. Were there any specific failure modes identified during testing on the Molecule3D and QM9 datasets?"}}
{"paper_id": "8JCn0kmS8W", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. WavJourney introduces a novel framework combining large language models and audio models to generate storytelling audio content, expanding the possibilities of audio generation beyond domain-specific conditions.\n2. The framework demonstrates state-of-the-art results in text-to-audio generation benchmarks, indicating its effectiveness and innovation.\n3. The introduction of a new multi-genre story benchmark enhances the scope for future research and evaluation.", "Weaknesses": "1. The methodology primarily leverages existing models and frameworks, potentially limiting the originality of the technical contribution.\n2. Further details on the evaluation process and comparison with alternative approaches or frameworks using LLMs would strengthen the paper's claims.", "Questions": "1. How does the system's performance vary with different configurations of large language models, and what is the impact of using open-source LLM alternatives?\n2. Can the approach be extended to accommodate additional or alternative audio modalities, and what challenges might arise in such extensions?\n3. What measures are in place to objectively evaluate user engagement and storytelling effectiveness beyond subjective evaluations?"}}
{"paper_id": "XbLffB0T2z", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses an important limitation in current availability data poisoning strategies by focusing on attack transferability across different learning paradigms, which is less explored. It provides a novel algorithm that leverages supervised and unsupervised learning paradigms to enhance transferability, showing promising results across various learning algorithms and extensive experiments on benchmark datasets.", "Weaknesses": "The assumption that frequency-level characteristics are indicative of successful transferability is intriguing but lacks a deeper theoretical explanation. The paper could also benefit from a stronger justification for the choice of specific algorithms in generating poisoning attacks. Additionally, while extensive experiments are conducted, the scalability of the method to larger datasets or more complex learning paradigms needs further exploration.", "Questions": "1. How does the proposed method perform on larger scale datasets or more complex models? 2. Could the approach be adapted to other modalities beyond image datasets? 3. What are the computational costs associated with using both supervised and unsupervised paradigms in generating poisoning perturbations?"}}
{"paper_id": "4y3GDTFv70", "error": "'NoneType' object has no attribute 'model_dump'"}
{"paper_id": "ODSgo2m8aE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses an important challenge in graph neural networks related to biases and fairness, offering a theoretical exploration of Lipschitz bounds as a stabilizing factor. It effectively combines robust statistics with deep learning, potentially leading to more fair and stable GNNs. The experiments conducted support the theoretical claims made.", "Weaknesses": "The complexity of calculating Lipschitz bounds for non-Euclidean spaces such as graphs is not entirely resolved, and the methodology may be limited to theoretical analyses without practical implementation details. The paper may lack comprehensive evaluations or considerations of alternative approaches.", "Questions": "How does the proposed Lipschitz bound calculation compare to existing methods in terms of efficiency and accuracy in practice? What are the potential trade-offs when applying these bounds in real-world scenarios?"}}
{"paper_id": "RlfD5cE1ep", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper presents a novel analysis of non-contrastive learning dynamics by incorporating feature normalization. The introduction of cosine loss to understand the role of feature normalization is an interesting addition that sheds light on preventing representation collapse in non-contrastive learning frameworks.", "Weaknesses": "The paper could benefit from a more comprehensive comparison with existing literature, especially with recent contributions that also consider the role of normalization techniques. The theoretical exploration appears sound but lacks empirical validation on more complex models, which limits the practical impact of the findings.", "Questions": "How does the proposed analysis compare with other derivations using implicit regularization techniques in similar frameworks? Are there practical experiments beyond linear networks to validate the claims better?"}}
{"paper_id": "18TfucMNTr", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper addresses an important problem in deep learning, namely the challenges posed by local minima and saddle points during training.\n2. The concept of optimization by continuation and its theoretical insights are valuable contributions that enhance understanding of training dynamics in deep learning models.\n3. The proposed regularization strategy appears to be a novel way to improve optimization process.", "Weaknesses": "1. The paper might lack sufficiently rigorous empirical validation on large-scale or diverse datasets to demonstrate generalizability.\n2. The treatment of saddle points might not fully account for complexities found in high-dimensional neural network settings, potentially limiting applicability.\n3. The paper does not seem to adequately compare its approach to other established methods for overcoming optimization challenges in deep learning.", "Questions": "1. How does the proposed continuation method compare with other common optimization techniques like stochastic gradient descent momentum (SGDM) or Adam optimizer in terms of computational efficiency and convergence?\n2. Could the authors provide more detailed experimental results on different architectures, such as CNNs or transformers, to evaluate the general applicability of their method?\n3. What are the specific assumptions made about the loss landscapes of neural networks when applying this continuation approach, and how might they be relaxed or modified for broader applicability?"}}
{"paper_id": "JGTC6WgO4T", "error": "'NoneType' object has no attribute 'model_dump'"}
{"paper_id": "fQHb1uZzl7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents a novel Transformer-based architecture integrating feature and cost aggregation for dense matching tasks. It effectively exploits self- and cross-attention mechanisms to improve accuracy, demonstrating significant improvements on standard benchmarks.", "Weaknesses": "The abstract lacks detail on the specific results and comparisons. It also does not mention how the proposed method compares in terms of computational complexity or efficiency against existing approaches.", "Questions": "How does the proposed method's computational complexity compare to existing state-of-the-art methods? Are there specific types of dense matching tasks where the approach does not perform well?"}}
{"paper_id": "YjG29CIOP7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper addresses a significant gap in the DFML literature by proposing methods to handle Task-Distribution Shift (TDS) and Task-Distribution Corruption (TDC), crucial aspects for improving robustness.\n2. The introduction of TEAPOT and ROSY represents innovative approaches that diversify pseudo task distributions and adaptively manage model selection, respectively.\n3. The experimental validation confirms the advantages of TEAPOT and ROSY for enhancing robustness against distributional challenges.", "Weaknesses": "1. The paper relies heavily on assumptions that may not hold in all practical scenarios, such as the risk of TDC through model poisoning attacks being representative without simple preventative measures.\n2. The computational complexity associated with the reinforcement learning based ROSY method may limit its applicability to real-world situations.\n3. While experiments showcase the method's effectiveness, the focus on 4-layered convolutional networks raises questions about generalizability to more complex architectures and diverse real-world datasets.", "Questions": "1. How does the proposed TEAPOT method compare to existing continual learning methodologies in terms of computational overhead and model accuracy?\n2. Can the ROSY framework be efficiently scaled to accommodate larger and more varied deep learning models, considering its use of reinforcement learning?\n3. How does the defense strategy of ROSY handle cases where model poisoning is less apparent or disguised through subtle, incremental changes?"}}
{"paper_id": "DmDpu3r8iU", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses a crucial issue of value alignment in LLMs, introducing the Value Understanding Measurement (VUM) framework, which is an innovative approach to quantitatively assessing 'know what' and 'know why' aspects of LLM understanding of human values. The development of a dialogue dataset based on the Schwartz Value Survey and the evaluation of five representative LLMs provide solid empirical grounding for their claims.", "Weaknesses": "There is a lack of clarity on certain methodological aspects, such as the inputs for models in different steps of the evaluation process, and the specifics of prompts used in certain experiments. The evaluation might be limited by the unknown parameters and training details of the LLMs considered, which might affect the robustness of the claims related to scaling laws and model capabilities. The presented human study lacks details, which raises concerns about the validation of the critique model's ability to distinguish outputs beyond superficial explanations.", "Questions": "1. Can you clarify what specific inputs were used for the models at each step of the VUM framework evaluation? 2. What were the specific prompts used in the experiment involving relevance to values, and where are the results discussed? 3. Could you provide more detailed insights into the human study methodology and results you mention in the appendix?"}}
{"paper_id": "kXHEBK9uAY", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The proposed Hierarchical Diffuser demonstrates an effective solution to combine hierarchical planning strategies with diffusion-based methods, showing superior performance on challenging offline reinforcement learning tasks.\nThe model successfully addresses the computational inefficiencies typically associated with diffusion-based planners by adopting a 'jumpy' planning strategy, leading to faster training and planning stages as verified through empirical evaluations.\nThe paper provides an interesting exploration of the generalization capabilities of their method on out-of-distribution tasks, illustrating its potential beyond standard benchmarks.", "Weaknesses": "The concept of integrating hierarchical methods with diffusion-based models, while effective, may lack substantial novelty, as elements of this approach draw from existing methods within the field. \nFurther details on the implementation specifics, such as parameter settings or computational overhead in real-world scenarios, are needed to thoroughly evaluate practical applicability and consistency across different environments.\nThe evaluation could be enhanced by comparing against a broader range of state-of-the-art methodologies, particularly those employing recent advancements in hierarchical and diffusion models.", "Questions": "How does the Hierarchical Diffuser handle environments with significantly different distributions or noise levels? Are there assumptions about the underlying dynamics or noise in order for the jumpy planning strategy to be effective?\nCan the method be extended to handle multi-agent environments, or is it inherently limited to single-agent settings due to the architectural choices made in the model's design?"}}
{"paper_id": "i7LCsDMcZ4", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "This paper presents a novel approach to generating saliency maps for Spiking Neural Networks (SNNs), addressing a notable gap in event-based vision using a bio-inspired sensor. The introduction of Spiking Layer-Time-wise Relevance Propagation and Spiking Layer-wise Relevance Propagation demonstrates a methodological innovation that contributes to improved performance in object and action recognition tasks. The results show state-of-the-art performance, highlighting the effectiveness of the proposed method.", "Weaknesses": "While the proposed method and results are promising, the lack of comparison with pre-existing event datasets such as N-ImageNet is a limitation. The method could also benefit from broader comparative studies with other saliency-based augmentation techniques besides conventional vision methods like Grad-CAM.", "Questions": "How does the proposed method compare to other saliency-based augmentation techniques specifically designed for event camera applications? Additionally, could the method be applied to larger datasets such as N-ImageNet for a more comprehensive performance evaluation?"}}
{"paper_id": "jHdz0CIS2y", "error": "'NoneType' object has no attribute 'model_dump'"}
{"paper_id": "NkYCuGM7E2", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The integration of LLMs into autonomous driving represents an innovative approach, addressing the challenge of rare event generalization and interpretability through commonsense reasoning.\n2. The paper is well-structured, with clear illustrations that enhance understanding of the proposed methodology.\n3. Extensive experimentation validates the approach, showing improvements over baseline methods.", "Weaknesses": "1. The paper lacks a detailed analysis of the potential safety risks and consistency issues associated with using LLMs in real-time safety-critical systems like autonomous driving.\n2. The scalability and real-time applicability of the proposed method are not thoroughly evaluated, raising concerns about the efficiency of the approach under practical constraints.\n3. Comparisons with only one other RL algorithm limit the strength of demonstrated improvements over existing methods.", "Questions": "1. How does the proposed method ensure the robustness of LLM outputs given the inherent randomness in LLMs, especially in critical driving scenarios?\n2. Can the authors elaborate on the real-time computational demands of integrating LLMs into the decision-making process of autonomous vehicles?\n3. What mechanisms are in place to ensure consistent behavior across different versions of LLMs used for decision-making?"}}
{"paper_id": "RtDok9eS3s", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The research presents a novel approach to simplifying Transformer architecture, which leads to faster training throughput and reduced parameter use without loss of performance or training speed. The combination of signal propagation theory and empirical observations provides a solid foundation to the proposed modifications, making it an insightful contribution to the field.", "Weaknesses": "The evaluation was conducted on relatively smaller models (~100M parameters) and a limited number of tasks. This raises questions about the generalizability of the results to larger, more complex models and a broader set of tasks. Moreover, the practical implications of removing certain components (e.g., skip connections) might not be fully addressed, particularly regarding long-term stability and scalability.", "Questions": "1. How does the simplified model perform on larger, more complex transformer architectures commonly used in industry? 2. What are the potential long-term effects of removing components like skip connections on model stability during training? 3. Would the simplifications proposed be effective across various types of tasks, such as those requiring complex reasoning or multi-modal integration?"}}
{"paper_id": "YCPDFfmkFr", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses an important problem of differentiating through infeasible QPs, which is crucial for optimization layers in neural networks. The approach is innovative and leverages primal-dual augmented Lagrangian techniques effectively. The provision of open-source software, QPLayer, adds significant practical value to the research, allowing easy integration into modern learning frameworks.", "Weaknesses": "The presentation could be clearer, especially for readers not familiar with optimization layers, as some sections are complex and could be expanded with more explanations. The feasibility analysis might benefit from additional empirical validations across diverse datasets and practical scenarios, which would further substantiate the claims.", "Questions": "How does the proposed method compare in performance with traditional optimization techniques in extensive real-world applications? Are there limitations to the differentiability approach that might affect computational efficiency or stability under certain conditions? Could the method be generalized for non-convex QP layers?"}}
{"paper_id": "ueTdErd5Ib", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel method for addressing contextual stochastic optimization problems, focusing on balancing expected cost minimization with robustness against worst-case scenarios. The approach involves discretizing the feasible region and employing a secondary optimization problem, providing theoretical guarantees for bounding regret. The method shows competitive results and significantly reduces worst-case costs in experiments.", "Weaknesses": "The paper's presentation could be clearer, particularly in explaining the discretization and secondary optimization processes in a more accessible manner. Further experimental validation on a broader set of problems would strengthen the claims.", "Questions": "How sensitive is the method to the choice of discretization and the specific parameters controlling the user-defined cost threshold? Are there specific contexts where the method may not perform as well?"}}
{"paper_id": "sDmjlpphdB", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel approach to prompt design by employing a Mixture-of-Expert paradigm, which successfully addresses the limitation of demo-free instruction models. The proposed Mixture-of-Prompts (MoP) method significantly enhances performance on benchmark NLP tasks, suggesting its practical applicability and effectiveness. The incorporation of semantic similarity in demo assignment and the use of a joint search strategy further strengthen their approach.", "Weaknesses": "The dependency on the initial clustering method and its potential impact on the performance requires further discussion and analysis. There is a need for more comprehensive experimentation across a wider variety of datasets and tasks to ensure the generality of the approach.", "Questions": "How does the choice of clustering algorithm impact the overall performance of the MoP approach? Is there a significant variation in results based on different clustering techniques?"}}
{"paper_id": "Mdk7YP52V3", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. This paper provides a novel theoretical perspective on the instability issues observed in heteroskedastic neural regression models, particularly highlighting the importance of regularization. \n2. The derivation of the nonparametric free energy and the identification of phase transitions provide a deeper understanding of the underlying phenomena in overparameterized models.", "Weaknesses": "1. The paper may rely heavily on theoretical concepts that could be difficult for practitioners not familiar with statistical physics to grasp fully. \n2. More extensive empirical validation could strengthen the presented theoretical claims, especially in diverse real-world scenarios.", "Questions": "1. How might the proposed regularization optimization scheme be applied in practice in current machine learning frameworks?\n2. Are there specific types of data or problem domains where the described phase transitions are more critical to consider?"}}
{"paper_id": "YGTSLDAPqb", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses an important problem of domain adaptation, particularly in scenarios where unlabeled target data is available but not effectively utilized through standard approaches. The proposed method, Connect Later, is innovative in combining generic pretraining with fine-tuning using targeted augmentations to effectively reduce out-of-distribution (OOD) errors across multiple real-world datasets. The results are significant, achieving state-of-the-art in certain cases, and the paper is well-structured, making it easy to follow the proposed methodology and experiments.", "Weaknesses": "One potential limitation is the dependency on the specific domain knowledge to design targeted augmentations, which might limit the applicability of the proposed method to domains where such knowledge is not readily available. Additionally, the paper could benefit from a broader comparison with other domain adaptation or transfer learning techniques to situate its contributions more clearly within the field.", "Questions": "Can the Connect Later framework be applied in cases where there is minimal or no domain knowledge available to inform the design of targeted augmentations?"}}
{"paper_id": "JTcaziw7G1", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel approach using multi-party computation for secure information retrieval that maintains privacy.\n2. It effectively addresses the issue of performing retrieval-augmented generation without exposing queries or database contents, which is crucial for sensitive data handling.\n3. The introduction of a new MPC protocol for inverted file approximate search is a significant technical contribution.", "Weaknesses": "1. The practicality of the approach in real-world large-scale applications could be questioned, given the potential for high computational and communication costs.\n2. There is limited information on performance benchmarks or comparisons with existing solutions, which makes it challenging to assess efficiency.", "Questions": "1. How does the proposed system scale with very large datasets, and what are the computational and time overheads involved?\n2. Can the approach be integrated easily with existing large language models and data pipelines without significant modifications?\n3. What is the impact on accuracy and relevance of the retrieved documents when using the MPC-based retrieval method?"}}
{"paper_id": "lEkFq4RUCX", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The proposal of a new distance metric (DDF) for comparing 3D point clouds is a novel idea that can advance existing methodologies significantly. \n2. The experiments demonstrate that DDF outperforms existing metrics across multiple tasks, indicating its versatility and effectiveness.\n3. The proposed method is computationally and memory efficient, making it practical for real-world applications.", "Weaknesses": "1. The generation process of reference points and some technical details related to the shared identical weight operation are not thoroughly explained in the paper. \n2. While the experimental results are promising, more insights into the limitations or potential downsides of DDF compared with other metrics would strengthen the paper.", "Questions": "1. How are the reference points determined or generated, and how does this affect the results?\n2. Can you elaborate on how the identical weights are shared in the equation related to directional distances? Is there any scenario where this might not hold effectively?"}}
{"paper_id": "Fq8tKtjACC", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The model phi-1 demonstrates impressive performance on coding tasks like HumanEval and MBPP despite its relatively small size. It showcases the potential of training efficiency by achieving reasonable accuracy with constrained computational resources and a small parameter count.", "Weaknesses": "The paper lacks information on the specific strategies or novel techniques used in refining phi-1, which are crucial to fully understanding its success. There is also limited explanation on how 'textbook quality' data significantly improves performance and how synthetic data was generated to enhance the learning process.", "Questions": "How was the 'textbook quality' dataset curated, and what criteria were used to ensure its quality? What specific role did synthetically generated textbooks and exercises play in the model's training, and could more detail be provided on the GPT-3.5 synthetic data generation process?"}}
{"paper_id": "8fQlGQkj0S", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper provides a novel and rigorous analysis of in-context learning focusing on both task retrieval and task learning. It reveals an interesting phenomenon regarding the risk bounds in task retrieval, which could have significant implications for the design of in-context learning systems.", "Weaknesses": "The paper’s generative models may be too simplified to fully capture the complexities of real-world in-context learning scenarios. Additionally, the paper may not sufficiently address potential limitations or challenges in applying these findings to practical implementations.", "Questions": "How do the theoretical findings translate to practical improvements in Transformer model implementations? Are there any specific conditions under which the unique risk bound behavior is most pronounced?"}}
{"paper_id": "hRos9WldRK", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The proposed Learning to Bootstrap (L2B) method provides a novel approach to handling noisy labels by leveraging joint reweighting and implicit relabeling. 2. The method is both innovative and practical, offering compatibility with existing noisy label learning methods and leading to significant improvements without additional computational costs. 3. Extensive experimental results demonstrate the method's efficacy across multiple benchmark datasets, highlighting its robustness and practical applicability.", "Weaknesses": "1. Although the paper demonstrates improvements on benchmark datasets, the novelty of the approach might be limited since the concept of bootstrapping and reweighting has been explored in different contexts before. 2. The reliance on a clean validation set assumption, as mentioned in section 3.1, contradicts the claims of not needing validation samples. This inconsistency could raise questions about the practical application of the method. 3. It's unclear how the method generalizes across diverse real-world datasets beyond those tested, given potential variations in the nature of label noise.", "Questions": "1. How does the method handle systematic label noise, particularly given that much of the focus is on random noise scenarios? Are there specific adaptations required? 2. Can more clarifications be provided regarding the training process when alpha and beta do not necessarily sum to one? How does this affect the stability of the learned weightings?"}}
{"paper_id": "ttRSBZiCiu", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper introduces an innovative approach to speech synthesis using the WaveFluid model, which directly learns a velocity field through adversarial training.\n2. By leveraging reparameterization techniques, the proposed method effectively reduces memory footprint and improves training efficiency.\n3. The competitive performance of the model is demonstrated, showing improved sample quality compared to previous vocoders within limited inference steps.", "Weaknesses": "1. The evaluation seems limited to specific settings, and more comparative analysis with state-of-the-art models across various benchmarks could strengthen the claims.\n2. The paper's focus on specific architectural improvements may limit its broader applicability to other domains of generative modeling.\n3. Details on training and hyperparameter settings, as well as potential limitations of the proposed approach, are not sufficiently discussed.", "Questions": "How does the proposed WaveFluid model compare with cutting-edge vocoders in terms of computational resources required during inference? Are there plans to expand this approach to broader generative modeling tasks beyond speech synthesis?"}}
{"paper_id": "WYsLU5TEEo", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel unified framework using GANs to generate counterfactual examples that simultaneously improve interpretability and robustness of neural image classifiers. The framework is innovative in its dual focus, addressing two critical issues in image classification tasks. The method demonstrates effectiveness in producing descriptive saliency maps and improved adversarial robustness, which are validated through semantic segmentation and adversarial attack scenarios.", "Weaknesses": "The approach may require further validation across a broader range of datasets and tasks to establish its general applicability. The explanation of how the unified model works, particularly the integration of the classifier and discriminator, could benefit from additional clarity. There is also a need for comparative studies with other state-of-the-art methods addressing interpretability or adversarial robustness to thoroughly evaluate the competitive edge of the proposed framework.", "Questions": "How does the framework perform on multiclass classification tasks beyond the binary tasks evaluated? What are the computational costs associated with training the unified model compared to traditional classifier and adversarial training methods? Can the method be extended to other domains or application areas beyond image classification?"}}
{"paper_id": "KKZaj2QS3G", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces a novel sampling strategy for noise-resilient representation learning, and proposes an efficient encoder architecture using dilated convolution within an Inception block.", "Weaknesses": "The lack of previous work explicitly addressing noise in time series is claimed, but not well corroborated with comparative analysis. The proposed method's novelty could be overstated given basic concepts like low-pass filtering for noise reduction.", "Questions": "How does the proposed encoder with dilated convolution compare to other lightweight architectures in terms of computational efficiency? Are there specific scenarios or datasets where this method significantly underperforms?"}}
{"paper_id": "LWDRiFzbHQ", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 5, "Confidence": 4, "Strengths": "The paper presents a novel approach to improve out-of-distribution generalization assessment by integrating neighboring test samples into correctness indications.", "Weaknesses": "The approach may add complexity and computational overhead due to the reliance on neighboring sample analysis. Additionally, it might not sufficiently address challenges involving severe distribution shifts.", "Questions": "How does the method's performance compare with other state-of-the-art techniques when applied to real-world datasets showing severe distributional shifts? Additionally, can VRP be optimized to reduce potential computational overhead?"}}
{"paper_id": "Yp01vcQSNl", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel graph transformer architecture that explicitly considers edge directionality, addressing a significant gap in existing graph learning models. The use of dual encodings and a directional attention module demonstrates an innovative approach to learning with directed graphs. The empirical results on newly proposed directional graph datasets show state-of-the-art performance, underscoring the effectiveness of the method.", "Weaknesses": "While the approach is sound and presents a novel perspective, the paper would benefit from a more thorough exploration of potential limitations or challenges in applying the approach to a wide range of graph types. The introduction of new datasets is useful, but there could be a potential bias in designing datasets that naturally favor the proposed method.", "Questions": "How does the proposed method perform on non-directional graphs, or when directionality is only mildly present? Are there any plans to test the method's applicability to real-world datasets where directionality might not be as pronounced?"}}
{"paper_id": "LndMyiBl3n", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses a relevant and practical problem of restricted black-box attacks on Graph Neural Networks and proposes a novel attack method, SheAttack, using the Modified Silhouette Score (MSS) as an objective measurement. The approach is theoretically underpinned and shows promising results in experimental validation across both synthetic and real-world datasets.", "Weaknesses": "The method relies on several hyperparameters, the impact of which isn't fully explored or justified, potentially affecting the robustness and general applicability of the proposed solution. Moreover, while theoretically informed, the innovative aspects of the MSS and its implementation might require further elaboration to fully appreciate its significance over existing methods.", "Questions": "How sensitive is the SheAttack performance to the choice of hyperparameters? Are there particular settings or initialization strategies that mitigate performance variability? Further, could the modified silhouette approach be generalized to attack other types of networks beyond GNNs?"}}
