{"paper_id": "B0OwtVEejJ", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper bridges the gap between weight sharing in NAS and weight entanglement by proposing a novel method that allows for efficient gradient-based search in weight-entangled spaces. The findings highlight enhanced performance, improved training properties, and superior anytime performance while maintaining memory efficiency.", "Weaknesses": "The integration of weight-entanglement with gradient-based NAS might not be entirely novel given that some related techniques in NAS have previously explored similar avenues. Additionally, the paper may lack experimental diversity by not examining various architecture spaces or datasets.", "Questions": "How does the proposed approach perform compared to state-of-the-art non-weight-entangled NAS methods across different domains? Are there specific limitations or datasets where the method does not perform well?"}}
{"paper_id": "ZlEtXIxl3q", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper presents a novel technique using contrastive loss functions, demonstrating improved performance in estimating fitness functions where traditional methods like MSE fall short. It successfully leverages a theoretically sound approach with practical validation.", "Weaknesses": "The application and validation appear somewhat limited, focusing primarily on the proposed benchmarks without extensive exploration across different or diverse datasets. The foundational concepts, while innovative, might require more clear explanations for broader audiences.", "Questions": "How does the performance of the proposed method vary across different types of biological sequences and fitness landscapes not covered in the benchmarks?"}}
{"paper_id": "6yJuDK1DsK", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 5, "Confidence": 3, "Strengths": "FEATHER introduces a lightweight approach to lifelong test-time adaptation, requiring significantly fewer additional parameters compared to existing methods. It effectively separates source and target domain knowledge, reducing error accumulation.", "Weaknesses": "The paper claims significant computational and memory savings but lacks detailed empirical analysis to support these claims. The approach primarily targets CNNs, limiting its applicability across diverse model architectures.", "Questions": "How does FEATHER perform in terms of memory and computation compared to other methods? Can the approach be extended to models other than CNNs, such as Transformers?"}}
{"paper_id": "lt6xKGGWov", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents a novel approach to supervised feature selection using neural networks to estimate mutual information, which can capture sophisticated relationships between features and targets. This departure from traditional methods promises more accurate feature selection.", "Weaknesses": "The paper's performance claims rely primarily on examples rather than comprehensive experimental validation. There is no mention of benchmark comparisons or evaluations on standard datasets. Additionally, the abstract does not clarify the computational feasibility or efficiency of the proposed method, which could be a limitation in practical applications.", "Questions": "How does the proposed method compare to state-of-the-art feature selection techniques in terms of computational efficiency and accuracy on standard benchmarks? Can you provide more detailed results from real-world datasets beyond the examples?"}}
{"paper_id": "Bvrc6kobWd", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel perspective in understanding margins in contrastive learning by analyzing gradients. The experimental results showcasing the improved generalization performance are particularly strong, indicating the practical significance of the findings.", "Weaknesses": "While the new gradient analysis perspective is compelling, the paper might lack clarity in certain sections, particularly in how gradient effects are translated into practical improvements. Additionally, it is unclear how the proposed methods compare with other state-of-the-art approaches not discussed in the paper.", "Questions": "What are the specific mechanisms by which margins affect gradients, and how do these differ from other methods without margins? Is there a potential impact on the computational efficiency or training time when implementing the proposed emphasis and scaling strategies?"}}
{"paper_id": "FJjHQS2DyE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces a novel approach to domain adaptation using conditional adversarial support alignment (CASA), which addresses limitations of previous methods under label distribution shift. It proposes a theoretical target risk bound that justifies the effectiveness of their approach, and provides empirical evidence of improved performance over state-of-the-art methods.", "Weaknesses": "The paper could provide more in-depth comparison and analysis of results to better understand the scenarios in which CASA outperforms other methods. The novelty of some components may be incremental, and further clarification on the approach's limitations and assumptions would strengthen the work.", "Questions": "How does CASA perform in scenarios with varying degrees of label distribution shift? Can the authors provide more detailed visualizations or case studies of its application?"}}
{"paper_id": "xelrLobW0n", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces TRACE, a novel benchmark specifically designed for evaluating continual learning in aligned large language models. TRACE spans multiple challenging tasks in diverse domains such as multilingual capabilities, code generation, and mathematical reasoning, offering a comprehensive tool to assess the continual learning abilities of such models. The unified format for all datasets allows for seamless evaluation. The empirical results presented demonstrate a significant decline in performance, underlining the challenges of achieving a balance between task-specific performance and the retention of initial model abilities.", "Weaknesses": "The impact and novelty of the proposed benchmark are not entirely clear. While TRACE aims to address continual learning, it's uncertain how much more challenging or representative it is compared to existing benchmarks. Additionally, the paper does not provide extensive comparisons with existing continual learning methodologies, which would help in validating its claims about the benchmark's uniqueness and the efficacy of the proposed Reasoning-augmented Continual Learning (RCL) approach.", "Questions": "How does TRACE compare to existing continual learning benchmarks in terms of specificity and difficulty? Are there any results comparing TRACE directly with other benchmarks? What other continual learning methods were considered and why were they not included in the comparison?"}}
{"paper_id": "bYQkOPvgDw", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper proposes a novel approach to address label noise in graph neural networks, which is a less explored area compared to graph perturbation.\n2. The use of a probabilistic graphical model framework is a strong methodological contribution and well-justified.\n3. Extensive experiments demonstrate the robustness of the method under various noise conditions and graph types.", "Weaknesses": "1. The explanation regarding why uncertainty modeling is necessary is not very clear. The benefits over simpler models are not well articulated.\n2. It is unclear how the approach specifically avoids the pitfalls of label smoothness assumptions prevalent in other methods.\n3. The focus is primarily on synthetic noise scenarios; real-world applications and benchmarks are not sufficiently highlighted.", "Questions": "1. How does the method perform in real-world applications where label noise naturally occurs?\n2. What are the computational costs associated with the probabilistic graphical model compared to more traditional approaches?\n3. Can the method be adapted for inductive learning settings?"}}
{"paper_id": "sKPzAXoylB", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel approach, Utility-based Perturbed Gradient Descent (UPGD), which effectively addresses both catastrophic forgetting and loss of plasticity in continual learning. UPGD is shown to improve performance across a challenging streaming learning setup with strong empirical results. The idea of applying perturbations to the gradient update, which vary based on unit utility, is innovative and well-supported by experiments.", "Weaknesses": "While the approach shows promise, the paper could provide more detailed analysis and justification for the choice of perturbation scales. Additionally, there may be limitations in the diversity of tasks used for evaluation, which could affect the generalizability of the method's effectiveness. More clarity on how UPGD compares with other methods in diverse scenarios would enhance the paper.", "Questions": "How does the performance of UPGD vary with different perturbation scales and scheduling? Can the authors provide further insights into how UPGD handles tasks with drastically different characteristics compared to the ones used in the current experiment setup?"}}
{"paper_id": "H8Qg1IIMaR", "review": {"Soundness": 2, "Presentation": 2, "Contribution": 2, "Rating": 4, "Confidence": 4, "Strengths": "The paper highlights a crucial vulnerability in popular language models, potentially impacting their deployment in real-world applications.", "Weaknesses": "The experiment methodology and the broader implications of permutation sensitivity need further explanation. It is not clear how natural variations in prompt ordering affect the models' performance.", "Questions": "What datasets and model architectures were tested to arrive at the conclusion regarding permutation sensitivity?"}}
{"paper_id": "tth2qXY7RU", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The introduction of the Super Floating-Point (SuFP) data type and quantization method addresses limitations in existing approaches by improving memory footprint and logic efficiency without sacrificing accuracy. The experimental results show superior performance and efficiency gains, highlighting significant improvements in computational capability and energy efficiency.", "Weaknesses": "The paper does not delve deeply into the potential trade-offs or limitations of the proposed approach, particularly how it may perform across different real-world scenarios beyond the benchmark tests provided. Additionally, the paper may benefit from further discussion on the complexity of implementing SuFP in existing systems.", "Questions": "Can the authors provide more details on how SuFP handles edge cases in data distributions? Are there any specific application scenarios where SuFP might not be the best choice?"}}
{"paper_id": "fjf3YenThE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The introduction of generalized variance reduction to mitigate conflicts between zeroth-order gradients and hard-thresholding shows promise for improved algorithm performance. The approach is demonstrated in practical applications such as portfolio optimization and black-box adversarial attacks.", "Weaknesses": "The paper may lack clarity in some sections and could benefit from more comprehensive experimental validation. The explanation of how variance reduction specifically mitigates the mentioned conflicts might need elaboration for greater clarity.", "Questions": "How does the proposed method practically compare with other zeroth-order gradient methods across different scenarios? Could the results on adversarial attacks be extended to larger datasets or varied attack models?"}}
{"paper_id": "H9DYMIpz9c", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces Farzi, an original method for data distillation that shows potential in reducing the size of training datasets significantly while maintaining performance. The approach leverages efficient mathematical optimizations and factorization techniques, showing strong empirical results on both recommendation and language modeling tasks.", "Weaknesses": "The scalability of the proposed method to larger datasets and models is not extensively explored. Additional clarity on the computational efficiency gains and application scope could strengthen the paper.", "Questions": "How does Farzi perform when scaled to larger models such as those encountered in industrial applications?"}}
{"paper_id": "4kLVvIh8cp", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces an oracle-efficient algorithm (PNLSVI) for offline RL with non-linear function approximation, offering innovative components such as a variance-based weighted regression and a pessimistic value iteration approach. It extends results from linear and differentiable functions to a general framework, claiming to be the first statistically optimal algorithm for nonlinear offline RL. This is a significant advancement in the field.", "Weaknesses": "The paper lacks numerical experiments to validate the theoretical contributions, which could strengthen the practical applicability of the proposed algorithm. Additionally, there are strong assumptions (e.g., uniform data coverage) that may not hold in real-world scenarios. The need for an accompanying lower bound for non-linear cases to demonstrate optimality fully is also a potential shortcoming.", "Questions": "How does the algorithm perform in practical scenarios, and can the assumptions like uniform data coverage be relaxed or justified with concrete examples? Are there specific non-linear function classes or instances where the algorithm's benefits are particularly pronounced?"}}
{"paper_id": "CSpWgKo0ID", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The study provides insights into LLM behavior in social settings using game theory, offering a novel approach to understanding AI interactions. The presentation of results is clear and detailed.", "Weaknesses": "The paper might lack sufficient depth in comparing alternative strategies or models. The focus seems limited to specific games, which may not fully represent the wider scope of LLM capabilities. Clarification on methodological approaches could be improved.", "Questions": "How generalizable are the findings to other models or more complex social scenarios? What are the implications of LLMs' behavior for practical applications in human-LLM interactions?"}}
{"paper_id": "gwbQ2YwLhD", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses an important issue in structure learning by examining the impact of variable scale on the accuracy of DAG models. The theoretical findings are supported by empirical evidence from both synthetic and real-world datasets.", "Weaknesses": "The scope of the findings might be limited by the focus on scale and square-based losses. The contribution, while valuable, may not represent a significant breakthrough in the field. The novelty might be limited unless clear connections to broader applications are made.", "Questions": "How do the proposed conditions for minimal square-based losses in $d$-dimensional cases generalize beyond the initial assumptions? Are there practical guidelines for mitigating the impact of scale in real-world applications?"}}
{"paper_id": "eIYDKNqXuV", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces a novel unsupervised clustering algorithm emphasizing computational efficiency and performance compared to state-of-the-art methods. It successfully determines the optimal number of clusters autonomously.", "Weaknesses": "The novelty of the algorithm might be limited due to the use of existing techniques. It's unclear how the proposed method compares with a more extensive set of modern clustering techniques beyond K-Means and DBSCAN. More experimental results and comparisons could strengthen the claims.", "Questions": "How does the proposed method perform relative to other modern clustering algorithms not included in the experiments? What is the impact of parameter choices on the algorithm's performance?"}}
{"paper_id": "bAMPOUF227", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces a novel framework that effectively enhances the reliability of LLMs by using task-specific fine-tuned models during inference. It provides comprehensive resources, including datasets, prompts, and model outputs, and demonstrates improved performance on generalizability and factuality tasks.", "Weaknesses": "The method's novelty may be limited due to its resemblance to existing techniques. Additionally, the paper does not thoroughly explore the role of confidence or provide a detailed ablation study on this aspect. It could benefit from more direct comparisons with similar methods to highlight unique contributions more effectively.", "Questions": "How does the proposed method directly compare to existing frameworks like Hugging Face's approach? Can you provide further insights into the significance of confidence in the framework's performance?"}}
{"paper_id": "eNoiRal5xi", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces the concept of Unknown Domain Inconsistency Minimization (UDIM), which is a novel approach that combines both parameter and data perturbation. This approach addresses a key issue in domain generalization and successfully outperforms existing SAM variants. The theoretical validation showing that merging SAM with UDIM creates an upper bound for domain generalization tasks adds strength to the paper's claims.", "Weaknesses": "While UDIM shows empirical success, there may be concerns regarding the practicality and computational cost of generating perturbed domains and aligning loss landscapes, particularly as this can be computation-heavy. There might also be concerns about how well these simulated domains represent real unknown domains that the model might encounter.", "Questions": "How computationally intensive is the proposed UDIM method, and what impact does it have on training time? How well do the empirically crafted unknown domains generalize to real-world scenarios?"}}
{"paper_id": "uU0Adp7Sfo", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces an innovative approach by combining competition and collaboration in GANs, which could potentially address some existing limitations in generative models. The theoretical backing and experimental validation across four datasets are strengths of the work.", "Weaknesses": "The presentation could be clearer, especially in relating the theoretical explanations to practical outcomes. Some aspects of the argumentation are complex, possibly making them less accessible.", "Questions": "How does the introduced CCGAN compare in terms of computational efficiency with existing GAN models? Are there specific scenarios where CCGAN performs less effectively?"}}
{"paper_id": "dKju7tbe6D", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces a novel reasoning mechanism combining iterative and parallel computation, which is beneficial for dynamic multi-step reasoning and concurrent execution of independent operations. The approach exhibits stronger reasoning capabilities and generalization with fewer parameters and computation steps, achieving state-of-the-art zero-shot performance on CLEVR-Humans.", "Weaknesses": "The technical details of IPRM, especially regarding the implementation of parallel reasoning and operation composition, are not sufficiently clear. It is difficult to discern how the approach distinctly facilitates parallel operations as claimed. The paper also lacks detailed examples or visualizations to illustrate how IPRM's reasoning process unfolds.", "Questions": "How does the Operation Composition stage enforce parallel processing? Can more examples or visualizations be provided to illustrate the reasoning steps and usage of parallel computing?"}}
{"paper_id": "DL7JWbdGr3", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel approach to epidemic time-series forecasting by leveraging pre-trained models, which is a significant innovation in the context of public health data analysis. The method shows improved performance over state-of-the-art models in various tasks and contexts, which demonstrates its practical relevance and potential impact.", "Weaknesses": "The paper may lack sufficient comparison with existing approaches in terms of computational efficiency and resource requirements during pre-training and fine-tuning. There is also some ambiguity regarding the handling of diverse datasets and the specific implementation details of self-supervised tasks.", "Questions": "How does the model ensure fair performance improvement over traditional data-driven models, specifically in sparse data scenarios? What specific self-supervised tasks are leveraged, and how were they tailored to capture epidemic dynamics across different diseases?"}}
{"paper_id": "1PaDPHDhwe", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses an important issue related to debiasing and dataset bias in machine learning. It proposes a scalable and efficient method that utilizes class-specific scaling to improve robust accuracy without sacrificing average accuracy significantly.", "Weaknesses": "The paper could provide more theoretical justification or insights into why the proposed method works well. The reliance on empirical results without deeper insights into the mechanism or theoretical underpinnings might limit the perceived depth of the contribution.", "Questions": "How does the approach perform under different levels of data imbalance or with different debiasing algorithms? What are the specific limitations of the class-specific scaling strategy?"}}
{"paper_id": "iI7hZSczxE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces a novel approach to disentangled representation learning for time series, which can enhance interpretability and generalization in the context of home appliance electricity usage. The introduction of the Time Disentangling Score (TDS) is a significant contribution as it provides a way to gauge disentanglement quality.", "Weaknesses": "The reliance on smooth bijections and contrastive learning may have limitations in terms of complexity and interpretability. It's not entirely clear how well the method generalizes to a variety of time series data beyond the specific application demonstrated. More experiments on diverse datasets would strengthen the validity of the approach.", "Questions": "How does the method perform on other types of time series data apart from home appliance electricity usage? Could the approach be generalized to non-energy domains?"}}
{"paper_id": "qrGjFJVl3m", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a new series of models that successfully enhance vision-language integration, setting new benchmarks and demonstrating superior performance on real-world dialogue tasks. The release of the models for public use encourages further research and development in the field.", "Weaknesses": "While the models perform well, the novelty in the architectural design and training method seems limited, as similar multi-stage and multi-resolution strategies have been used in prior work. The ablation studies and some claims in the related work section require more detailed explanations and stronger justification.", "Questions": "What specific innovations in the training pipeline or model architecture distinguish Qwen-VL models from existing vision-language models? How does the alignment of image-caption-box tuples differ from previous methods, and what impact does it have on model performance?"}}
{"paper_id": "qup9xD8mW4", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces a novel concept of 'behaviour distillation' for reinforcement learning, addressing a gap where traditional dataset distillation falls short due to the lack of fixed datasets in RL. The proposed method, HaDES, demonstrates the potential for synthesizing minimal datasets that achieve competitive performance. The method shows versatility by generalizing across different architectures and hyperparameters and applying to multi-task agents in a zero-shot fashion. Visualization of synthetic datasets provides insights into tasks, offering an interpretability advantage. The paper is well-written and the methods are clearly explained.", "Weaknesses": "The motivation for introducing 'behaviour distillation' over traditional expert dataset access in RL is not clearly justified. The potential benefits or specific problems addressed by distilling without expert data remain unclear, which could affect the perceived impact and applicability of the approach. Details on optimizations used in HaDES could be more thoroughly discussed, as well as comparisons with other existing techniques for similar tasks. Additional insights into the performance differences between standard RL and data-driven RL approaches would strengthen the paper.", "Questions": "What is the specific motivation for behavior distillation when traditional methods could utilize expert datasets directly? How does the performance of HaDES compare in situations where expert data is available? Can the authors provide more details on the optimization techniques used in HaDES and how these compare to existing work, such as BPTT?"}}
{"paper_id": "Zw8YxUWL4R", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel concept of Extended Textual Conditioning space, P+, which enhances control and expressiveness in text-to-image models. The method achieves significant improvements in reconstruction and editability over existing baselines without balancing these goals. The paper is well-written, and the experiments conducted are thorough, showcasing the effectiveness and unique properties of the proposed approach.", "Weaknesses": "The specific implementation details of the Extended Textual Inversion (XTI) and how it functions in practice are somewhat vague. While the empirical results are impressive, the paper lacks a theoretical understanding of why the proposed method outperforms existing solutions. The distinctiveness of 'per-layer prompting' requires further clarification to fully appreciate its contribution to the field.", "Questions": "How does the method balance the complexity of detailed per-layer prompting with computational efficiency? Could the authors provide more theoretical insights or analyses into why the P+ space is more effective for inversion and editability compared to existing methods? Are there any limitations or edge cases where the proposed method does not perform well?"}}
{"paper_id": "3NXhwkZGjz", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "This paper provides a novel approach to the challenging task of Source-Free Unsupervised Domain Adaptation (SFUDA) by leveraging multiple prediction hypotheses and their rationales. The proposed three-step adaptation process, including model pre-adaptation, hypothesis consolidation, and semi-supervised learning, has shown state-of-the-art performance. It can also be integrated into existing approaches.", "Weaknesses": "The motivation behind using multiple hypotheses is unclear, especially when all are pre-trained on the same domain. The utility and accessibility of these multiple high-quality hypotheses in practical scenarios are questionable. The setting might be computationally expensive. The explanation for the chosen number of optimal hypotheses and related parameters seems dataset-specific and may not generalize well. Some datasets show non-state-of-the-art performance.", "Questions": "How does the method handle the selection of the optimal number of hypotheses k and values for τ1 and τ2 for various tasks beyond the experimental dataset? Additionally, why does increasing the number of hypotheses potentially degrade performance instead of improving it?"}}
{"paper_id": "Fn655mJ4bv", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses a novel aspect of structured output model interpretation, offering promising insights into feature importance and correlations. The energy-based training process for the interpreter function is a strong innovative aspect.", "Weaknesses": "The paper's approach may be limited in the context of images where region-based explanations might be more appropriate than independently assessed features. The learning algorithm's robustness concerning hyperparameters is not well documented, raising concerns about its stability and utility.", "Questions": "How does the learning algorithm handle variations in hyperparameters such as tau and k? Are there any considerations for feature correlation within the interpreter, and has robustness against noise been assessed?"}}
{"paper_id": "VyMW4YZfw7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper provides a valuable perspective on the potential over-engineering of GNNs and suggests that simpler spectral methods might achieve comparable performance. It addresses an important issue by examining the role of evaluation conventions in new GNN designs' perceived performance improvements.", "Weaknesses": "The paper may not provide a comprehensive theoretical foundation or sufficient empirical evidence to conclusively support its claims. Additionally, the scope of the ablative study and the range of graph types evaluated might be limited.", "Questions": "What are the specific graph types evaluated in the study? How do the authors ensure that the proposed simpler methods generalize well across various graph datasets?"}}
{"paper_id": "09xFexjhqE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper provides a well-motivated solution to the problem of gradient divergence in Robust Fine-Tuning (RFT) by introducing a low-rank (LoRa) branch. The proposed method, AutoLoRa, automates the conversion of pre-trained feature extractors into adversarially robust models, offering significant practical utility without extensive hyperparameter tuning.", "Weaknesses": "The paper does not sufficiently explain the choice of the constant factor in the proposed method and lacks diversity in the network architectures evaluated. Some parts of the presentation, such as Table 4, are confusing and contain typographical errors. More detailed ablation studies, particularly on the automated hyperparameter scheduling, would strengthen the contribution.", "Questions": "How robust is AutoLoRa across different types of network architectures beyond ResNet? Can the constant factor used in the formula be adjusted for potentially better performance, and if so, what would be the impact?"}}
{"paper_id": "nmBjBZoySX", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces an innovative framework (AdaGLT) for Graph Neural Networks that addresses existing scalability and elasticity issues. The integration of pruning and restoration, dynamic sparsity adaptation, and automatic tuning of parameters are significant advancements over traditional methods. Rigorous theoretical proofs and extensive experiments enhance the credibility of the proposed method.", "Weaknesses": "The paper may still be considered incremental as it builds upon existing graph lottery ticket frameworks. The depth and breadth of experimentation, although extensive, may not conclusively cover all potential real-world applications or sufficiently large graphs.", "Questions": "How does AdaGLT perform on extremely large graphs and in real-time scenarios? Is the framework applicable to other forms of neural networks beyond GNNs?"}}
{"paper_id": "di52zR8xgf", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "Stable Diffusion XL demonstrates significant improvements in image synthesis quality with novel conditioning schemes and larger model architecture. The refinement model further enhances the output quality, achieving competitive results with state-of-the-art black-box models.", "Weaknesses": "The abstract does not provide specific details about the conditioning schemes or the refinement process, leaving questions about the generalizability and underlying innovations of these contributions.", "Questions": "How do the novel conditioning schemes impact the model's ability to handle diverse text prompts compared to previous versions? What are the specific innovations in the refinement model that contribute to improved visual fidelity?"}}
{"paper_id": "0b328CMwn1", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. Introduction of a new concept 'Activation Prompt' (AP) that expands the capabilities of Visual Prompting by applying universal perturbations to activation maps, rather than just input data. 2. Provides a thorough experimental analysis comparing AP against conventional VP and other PEFT techniques across multiple datasets and models. 3. Offers theoretical insights into the limitations of VP and the advantages of AP, enhancing understanding of these techniques.", "Weaknesses": "1. Although the paper proposes a novel concept, the improvement of AP over existing techniques like VP may not justify the claims of superior performance without further evidence or examples. 2. The theoretical analysis, while interesting, may be too complex or abstract for practical application without additional context or clarification.", "Questions": "1. How does the implementation of AP affect the computational efficiency in real-world applications, beyond theoretical considerations? 2. Are there specific scenarios or tasks where AP significantly outperforms VP, and could these be highlighted more clearly in the paper?"}}
{"paper_id": "AMDKqZcZbi", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses a crucial problem in machine learning: enabling rapid and continual learning similar to biological systems. The integration of biologically-inspired memory architectures with modern neural network approaches is innovative. The proposed model shows enhanced performance compared to baseline models, indicating the potential of the approach.", "Weaknesses": "The contribution, while innovative, may be specific to the domain of navigation tasks and not necessarily generalizable to other types of machine learning problems. Further empirical validation across diverse tasks would strengthen the claims. The comparison with existing models could be more comprehensive.", "Questions": "How does the proposed model perform on tasks outside the navigation domain? Are there plans to extend the benchmark to other types of tasks? What are the computational demands of the proposed memory policy compared to simpler baselines?"}}
{"paper_id": "am7BPV3Cwo", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces a novel generalized statistical framework, ImOOD, to tackle the problem of OOD detection on imbalanced datasets. The proposed method shows significant improvements over state-of-the-art approaches on several benchmarks.", "Weaknesses": "The paper could benefit from more clarity in explaining the theoretical framework and the methodology employed. Additionally, the scope of the study might be limited due to its focus on specific datasets and scenarios that may not fully represent all real-world situations.", "Questions": "How does the proposed ImOOD framework perform in scenarios where both in-distribution and out-of-distribution datasets are imbalanced? Are there plans to expand the testing to more diverse datasets beyond CIFAR and ImageNet?"}}
{"paper_id": "R1crLHQ4kf", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The proposed adversarial example detection strategy is straightforward yet effective, making it applicable to various ASR systems.\n2. Includes a comprehensive evaluation involving different state-of-the-art ASR systems and language datasets.\n3. The method shows high performance in differentiating adversarial examples, with over 99% AUROC against clean data.", "Weaknesses": "1. The paper might not sufficiently cover the performance of the method against a broader range of sophisticated or adaptive adversarial attacks.\n2. There is a lack of direct comparison with existing adversarial detection methods in the literature, which would strengthen the evaluation.\n3. While robustness to adaptive attacks is mentioned, more details and extensive experimentation in this area could be beneficial.", "Questions": "1. How does the method perform across different types of adversarial examples beyond those tested, particularly newer or more adaptive attacks?\n2. Could you provide more details on the computational overhead introduced by the detection mechanism?"}}
{"paper_id": "i7P2mK3x3o", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces a novel flow-based model for transporting between distributions that are only accessible via finite samples, which can be a practical extension to existing models. The approach has potential applications in various domains such as mutual information estimation and image translation, which is empirically evaluated.", "Weaknesses": "The paper might not have made clear connections between the proposed model and existing methods or adequately discussed the limitations. Additionally, the presentation might need improvements in terms of clarity and structure. There could be a lack of theoretical exploration or proofs regarding the proposed model’s soundness and effectiveness.", "Questions": "How does the proposed model compare with existing state-of-the-art methods for similar tasks? Would there be a theoretical justification of the transportation cost minimization approach employed?"}}
{"paper_id": "YIls9HEa52", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper presents a novel model, irSLDS, that addresses some of the limitations of the rSLDS by incorporating a semi-Markov process and latent geometry. The use of PDE theory to derive an efficient formulation for dynamical sufficient statistics is innovative. The model is validated on synthetic data and shows promising results on real-world electrophysiological data, demonstrating its potential for uncovering non-stationary processes in neural activity.", "Weaknesses": "The paper lacks extensive comparative evaluations with other alternative models, which makes it difficult to assess the significance of the improvements offered by irSLDS. The explanation of the model and its advantages over existing approaches could be clearer, as some technical details are not sufficiently motivated. Additionally, the paper does not explore the scalability and computational efficiency of the proposed method, which could impact its applicability to larger datasets.", "Questions": "How does the irSLDS model perform in comparison to other state-of-the-art models beyond rSLDS? Has the scalability and computational efficiency of the proposed method been evaluated on larger datasets? What are the limitations of the irSLDS model in terms of the types of data or scenarios where it may not perform well?"}}
{"paper_id": "koYsgfEwCQ", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper proposes a novel approach combining 3D geometry with unsupervised learning for dynamic scene decomposition, outperforming existing methods.", "Weaknesses": "The explanation of methodologies and certain terms (e.g., episodes, camera poses) lack clarity, making it difficult to fully understand the framework and evaluate its generalizability.", "Questions": "How is the assumed number of objects (N) determined, and could the connected components framework be used to automatically identify N?"}}
{"paper_id": "o6AN2ZoNw2", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The proposed P-Seg framework demonstrates state-of-the-art performance across widely accepted benchmarks without the need for complex modifications, suggesting strong practical utility. The method's ability to generalize without fine-tuning and its scalability with additional data and self-training highlight its robustness and potential for real-world applications. The availability of the code and demo will facilitate future research and practical adoption.", "Weaknesses": "The paper lacks detailed ablations and discussions around certain design choices, such as the use of pseudo-masks from publicly available datasets. This may limit the understanding of which components contribute most significantly to the model's performance. Furthermore, there is a need for comparison with more recent or different baseline methods to solidify claims of state-of-the-art results.", "Questions": "How does the model handle different scales of data during training, and what specific datasets were used for benchmarking? Could there be limitations in performance for specific types of images or vocabulary domains? Will there be any constraints or considerations when implementing this in real-time settings?"}}
{"paper_id": "HXZK1Z8tHa", "error": "'NoneType' object has no attribute 'model_dump'"}
{"paper_id": "QO3yH7X8JJ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper proposes a novel approach to utilize existing pre-trained diffusion models for arbitrary-scale super-resolution (ASSR) tasks without needing additional training. This can potentially save significant computational resources compared to existing methods. The introduction of the Perceptual Recoverable Field (PRF) metric to balance noise level and output quality is an innovative contribution that offers deep insights into managing trade-offs in such models.", "Weaknesses": "The paper relies heavily on the adjustment of noise levels, which might require fine-tuning or trial-and-error in practical scenarios, limiting its plug-and-play capability. Additionally, while there is mention of extensive experiments, the abstract does not provide details on scalability or comparison with a wide range of benchmarks in diverse conditions, leaving some potential performance aspects open to question.", "Questions": "How does the method perform when tested on different datasets or conditions that are outside the training distribution of the pre-trained DGMs? Are there any guidelines or heuristics provided for selecting the appropriate noise level in practical situations?"}}
{"paper_id": "sNtDKdcI1f", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper provides a comprehensive evaluation on multiple settings and datasets for RLHF optimization for helpfulness. It effectively explores and confirms the phenomenon that optimizing for response length significantly influences RLHF results, which is an important analysis that has not been comprehensively covered previously.", "Weaknesses": "The presentation could be improved. The figures and tables are not entirely self-contained; readers have to frequently cross-reference with the text to understand abbreviations and extract key takeaways, which interrupts the flow and ease of understanding.", "Questions": "1. Does optimizing solely for length still teach the model nontrivial aspects of helpfulness? Would a baseline comparison where a model generates extended outputs without structure perform worse?\n2. What does 'weighted reward gain' precisely measure in terms of observing improvements beyond length increments?\n3. Is there a qualitative understanding of what dimensions contribute to the remaining reward improvements not credited to length?\n4. What would be the implications of this study on other model optimization tasks like harmlessness or honesty?\n5. What actionable insights or recommendations can practitioners derive from this study to regulate length in RLHF processes? The interventions attempted appear to be partly successful, but clear guidelines or strategies are not highlighted."}}
{"paper_id": "HFtrXBfNru", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper provides a novel theoretical analysis of representation distortion in graph neural networks as graphs evolve over time. It introduces Smart, a simple yet effective baseline leveraging self-supervised graph reconstruction, which shows promising results on both synthetic and real-world datasets.", "Weaknesses": "The theoretical results, while compelling, are based on certain strong assumptions that might not hold in all practical scenarios. Additionally, there's a lack of comparison with other state-of-the-art methods beyond a basic linear regression baseline.", "Questions": "How does Smart perform compared to other advanced baseline models in terms of accuracy and efficiency? Are there specific scenarios or graph types where Smart might not perform well?"}}
{"paper_id": "4u0ruVk749", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel approach by incorporating diffusion models in the estimation of individualized treatment effects, which addresses the challenge of unobserved confounders in a unique manner. The empirical results demonstrate improvements over state-of-the-art methods, indicating practical significance.", "Weaknesses": "The methodology might rely heavily on assumptions regarding the generative process of the latent space that could limit the applicability of the approach in diverse real-world scenarios. Additionally, there is a lack of clarity in some parts of the explanation, which could make the convoluted parts of the proposed methodology less accessible to the audience.", "Questions": "What are the limitations in terms of computational complexity when using diffusion models for ITE estimation, compared to more traditional methods? How sensitive is the method to the assumptions made on the apriori knowledge for the generation process of the latent variable?"}}
{"paper_id": "Pa6SiS66p0", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces a novel benchmark for multi-modal continual learning, which is an underexplored area. The findings highlight the advantage of using multiple modalities to achieve more robust representations and reduced forgetting, providing valuable insights for the field.", "Weaknesses": "The paper lacks extensive experimental validation across multiple datasets, as most experiments are conducted on VGGSound. The novelty and implications of the proposed method compared to existing multi-modal benchmarks could be further clarified.", "Questions": "How does the performance of the proposed method compare to recent single and multi-modal continual learning approaches? Are there plans to extend experiments to other datasets to validate the benchmark's effectiveness?"}}
{"paper_id": "rkplYfqUr0", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The introduction of Gen-Z for zero-shot classification addresses real limitations in current language model prompts by improving robustness against variations. This contribution is backed by strong empirical results across various benchmarks. Furthermore, the inclusion of context in label descriptions adds a novel dimension to zero-shot classification.", "Weaknesses": "The approach heavily relies on the availability and quality of label descriptions, which might not be feasible in all scenarios. Moreover, while the customization to include author's or reader's context is intriguing, it raises potential concerns about scalability and generality across different tasks or languages.", "Questions": "What specific strategies can be employed when high-quality label descriptions are difficult to obtain? How does the proposed method scale to a larger number of label classes, particularly in more complex or multilingual settings?"}}
{"paper_id": "pTU2X9mUBe", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a large-scale, diverse, and comprehensive last-mile delivery dataset that is publicly available. This addresses a significant gap in the availability of real-world datasets for this field. The datasets' characteristics allow for a wide range of research possibilities in logistics, supply chain management, and spatio-temporal data mining.", "Weaknesses": "The paper does not provide sufficient detail on how privacy is protected in the dataset. Also, while the dataset is verified on several tasks using classical baseline models, there is no indication of how it performs with state-of-the-art models. Additionally, the data might not sufficiently cover dynamic aspects such as traffic conditions.", "Questions": "What measures have been taken to ensure user privacy in the dataset? Are there plans to include more dynamic variables such as real-time traffic data in future versions of the dataset? How does the dataset compare against existing public datasets in terms of usage scenarios?"}}
{"paper_id": "uhtQyRrTzY", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel method for curating large-scale, multi-view datasets from real-world videos and simulated environments without additional annotations. This method is shown to enhance performance on dense geometric tasks and object understanding tasks compared to datasets involving more expensive data curation processes.", "Weaknesses": "Details on how the datasets are curated and the specific implementation of masked image modeling objectives could be clearer. More information on potential limitations when scaling or applying the method to various environments would be beneficial.", "Questions": "How does the method handle varying qualities of input video data, such as different resolutions or noise levels, and does this impact pretraining effectiveness?"}}
{"paper_id": "WZfatbNdLV", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel approach to RNA splicing prediction using generative models and Bayesian Optimization, which could significantly advance the understanding of splicing mechanisms. The use of TrASPr and its integration with a costumed BO algorithm adds value to the RNA field, particularly in designing therapeutic targets. The work directly addresses prediction and design of RNA splicing outcomes condition-specific, which is crucial for applied RNA biology.", "Weaknesses": "The paper does not sufficiently compare its model against existing solutions beyond claiming superiority, which could hinder the validity of its performance claims. Also, there is a typographical error in 'costume' instead of 'custom,' and the explanation of the generative process and model evaluation could be clearer.", "Questions": "How does TrASPr specifically outperform other models in terms of metrics like accuracy or computational efficiency? Can this model be adapted for real-time prediction of AS in clinical settings?"}}
{"paper_id": "YkRwadXWHd", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel hierarchical knowledge distillation framework that reduces computational burden while maintaining high performance in continuous sign language recognition tasks. Extensive experiments demonstrate state-of-the-art results across several benchmark datasets.", "Weaknesses": "The paper relies heavily on existing methods such as multi-modal fusion and knowledge distillation, contributing less in terms of groundbreaking new techniques. Performance claims require further validation against more diverse baselines, particularly considering the architectural advantages leveraged in the proposed method.", "Questions": "How does the proposed model perform in more complex environments beyond standard benchmarks? Are weight values for loss terms dataset-sensitive, and how do they affect performance? Will code be released for reproducibility?"}}
{"paper_id": "q4Bim1dDzb", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The proposed method presents a novel approach to inverse rendering, with an efficient unified voxelization framework that significantly reduces training time while maintaining good reconstruction quality. The use of Spherical Gaussians for illumination modeling is compelling, offering a seamless integration into the voxelization process. The extensive experiments on diverse scenes strengthen the proposal's validity.", "Weaknesses": "The core contribution of the method relies on leveraging existing ideas from grid-based representations and shallow neural networks, which may not represent a substantial conceptual breakthrough. The improvements in modeling illumination might have limitations due to only comparative metrics rather than absolute superiority. More quantitative comparisons against the state of the art could solidify the contributions further.", "Questions": "How does the method handle variations in scene complexity and illuminative conditions across different datasets? Can the proposed Spherical Gaussian approach be adapted to other tasks within the realm of graphics and rendering, and if so, how easily?"}}
{"paper_id": "39cPKijBed", "error": "'NoneType' object has no attribute 'model_dump'"}
{"paper_id": "aG3EARrrd1", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces a novel model for online learning with promising results in efficiency and accuracy over existing methods. The design of an adaptive structure that grows based on data influx and utilizes efficient RBF approximation showcases innovation.", "Weaknesses": "The paper lacks clear motivation for some methodological choices, and there might be insufficient experimental validation to fully establish OSRT's superiority across diverse datasets. Some mathematical derivations and explanations appear incomplete or unclear, affecting reader comprehension.", "Questions": "What specific datasets were used in the evaluation, and how do they reflect real-world scenarios intended for OSRT? Are there any known limitations or scenarios where OSRT might underperform compared to other methods?"}}
{"paper_id": "Ts95eXsPBc", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces Spatially-Aware Transformer models which are innovative as they include spatial context in episodic memory, thus enhancing memory utilization efficiency and accuracy in downstream tasks. The Adaptive Memory Allocator is a novel approach to optimizing memory utilization based on reinforcement learning. Experiments demonstrate the model's advantages in diverse environments.", "Weaknesses": "The title of the paper is misleading as it suggests a focus on spatial dimensions in transformers but centers around episodic memory management. Overclaims about contributions are made, suggesting novelty in areas that have been previously studied. Certain experiments, such as action-conditioned image generation and some reinforcement learning settings, lack practicality and broad applicability.", "Questions": "Could the authors provide more details on how their method compares to recent embodied agents with advanced networks? Are there experiments with larger-scale tasks or more complex settings to test the proposed models' scalability?"}}
{"paper_id": "LfhG5znxzR", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents a novel approach to making neural network hidden states sparse and discrete by using a vector quantization bottleneck, which adds to the interpretability and control of neural networks. It demonstrates that this method operates with only modest performance degradation and provides a means to control neural network behavior by activating specific codes. The method is validated on various datasets, and the code and models are open-sourced, enhancing the work's reproducibility and accessibility.", "Weaknesses": "The paper might be lacking in extensive quantitative evaluation to comprehensively assess how the proposed approach affects neural network generalization across diverse tasks beyond the provided datasets. Potential limitations or scalability considerations when applying this method to larger models or more complex tasks are not explored in depth.", "Questions": "How does the introduction of the vector quantization bottleneck affect the training time and computational resources required for large scale models? Are there any strategies proposed to identify the codes representing complex or abstract concepts beyond basic categorical features?"}}
{"paper_id": "bUGzjiUsIq", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 5, "Confidence": 3, "Strengths": "1. The paper addresses a challenging and understudied problem: partially annotated multi-label classification. 2. Proposes a novel framework (PAPG) combining reinforcement learning and supervised learning techniques. 3. Uses innovative ideas like local and global rewards to tackle class imbalance issues.", "Weaknesses": "1. The paper lacks a detailed explanation of how the proposed method directly addresses the scarcity of positive samples and class imbalance. 2. More empirical comparisons with state-of-the-art methods could strengthen the claims of effectiveness. 3. The paper assumes some familiarity with advanced topics like reinforcement learning, which might limit accessibility.", "Questions": "1. How does PAPG perform in comparison to traditional fully supervised approaches on similar tasks? 2. Are there specific domains where PAPG would not perform well? 3. How were decisions made regarding the reward structures in the reinforcement learning algorithm?"}}
{"paper_id": "uMAujpVi9m", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper proposes a novel pretraining approach leveraging high-resolution atomic protein structures and pretrained small molecule representations, achieving state-of-the-art results in multiple tasks. ProFSA's method innovatively mitigates the data scarcity problem in protein-ligand complex structures, offering a significant advancement in pretraining methodologies.", "Weaknesses": "The paper does not address potential limitations or shortcomings of the proposed approach explicitly. There could be more detailed discussion on how the approach can be generalized beyond the specific tasks tested.", "Questions": "How would the method handle cases where the protein structure or ligand properties are substantially different from those in the training data? Are there any scalability issues or challenges in applying this methodology to larger or more diverse datasets?"}}
{"paper_id": "uqxBTcWRnj", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel framework for Transitional Dictionary Learning that can learn symbolic knowledge by reconstructing input as a combination of parts with implicit relations, presenting a unique approach to compositional understanding. The proposed game-theoretic diffusion model and the use of the EM algorithm for visual decomposition are innovative. The experiments demonstrate significant improvements over state-of-the-art unsupervised methods. The proposed metrics align well with human evaluations, providing strong validity to the approach.", "Weaknesses": "The method has not been tested on real-world visual datasets, which may limit its applicability and demonstrate its robustness in practical scenarios. The need for more concrete human evaluations and ethical considerations in experimentation is critical. Additionally, references to earlier works and appropriate citations could be improved for better comprehensiveness.", "Questions": "Have you considered testing the proposed framework on real-world visual data or more complex datasets beyond abstract compositional ones? How were the human evaluation criteria set and adhered to ethical guidelines?"}}
{"paper_id": "o4CLLlIaaH", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces a novel approach with the Generalizable neural Point Field (GPF) that provides an alternative paradigm for neural radiance fields. The proposed model demonstrates improvements in geometry and rendering quality, addressing significant issues in existing methods.", "Weaknesses": "The complexity of the new approach may introduce scalability issues. Additionally, the paper appears to lack in-depth experimental analysis across a wider array of datasets, and some technical explanations could be clearer.", "Questions": "How does the point-based approach compare in rendering speed against traditional image-based methods in real-world applications? What specific challenges are anticipated in scaling up the proposed method for larger datasets, and how does the structure mitigate the known issues of existing NeRF models?"}}
{"paper_id": "KNzL1nglNB", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The proposed method, LRM, offers an innovative approach to the label insufficient scenarios by utilizing neural collapse insights and aligning estimated label encodings with ground-truth encodings. It can serve as a versatile plugin for existing models, potentially enhancing their performance in challenging real-world situations such as semi-supervised learning and unsupervised domain adaptation.", "Weaknesses": "The presentation of the material could be improved for greater clarity, and more comprehensive experiments on a wider range of datasets and settings would strengthen the paper. Additionally, the implications of neural collapse in the context of LRM could be more elaborately discussed to fully appreciate its impact.", "Questions": "How does LRM handle the case when the prediction means do not accurately reflect the ground-truth label distributions? Can the authors provide more detailed insights into the theoretical analysis comparing LRM with ERM?"}}
{"paper_id": "pvhyBB86Bt", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper provides an extension to existing theories on neural network initialization by analyzing the behavior of the Jacobian during training, which is valuable for understanding stability in deep learning models. It leverages recent results in random matrix theory, showing a solid methodological foundation.", "Weaknesses": "The paper's assumptions may limit its applicability, and clarity on certain technical aspects and assumptions could be improved to make it more accessible.", "Questions": "Could the authors clarify the conditions under which their general theorem holds, especially concerning the assumptions about sparse networks and non-iid initialization?"}}
{"paper_id": "Mylk5iamJC", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper provides a novel formalization for open-set recognition, offering a unified goal for CSR and OSR. The introduction of L-GMM with a collaborative training algorithm shows promising results in handling both CSR and OSR tasks.", "Weaknesses": "The practical application of the theoretical formalization could be explored more thoroughly. The integration of CSR and OSR performance isn't deeply analyzed in terms of trade-offs.", "Questions": "How does the proposed model perform across various datasets, and what are the specific limitations in scenarios with significantly different distributions of known and unknown classes?"}}
{"paper_id": "MsOcVFzv8D", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The paper provides the first known theoretical analysis of MDTC, addressing a significant gap in the literature. It introduces a novel margin discrepancy-based adversarial training approach, grounded in a theoretical framework. The paper's empirical results demonstrate superior performance over state-of-the-art benchmarks, indicating the practical efficacy of the proposed method.", "Weaknesses": "The reliance on theoretical frameworks from prior works such as [Zhang et al, 2019] may limit the perceived originality of the contribution. The paper might benefit from clearer differentiation between this work and existing literature, particularly in terms of novelty and distinct contributions.", "Questions": "What specific measures have been taken to ensure the robustness and generalizability of the proposed MDAT approach across different MDTC scenarios?"}}
{"paper_id": "fBlHaSGKNg", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses the crucial task of efficient sample selection for annotation in Semi-Supervised Learning under budgetary constraints. It introduces a novel methodology, UPA, which leverages a modified Frank-Wolfe algorithm and a novel criterion, alpha-MMD. The proposed method is demonstrated to consistently improve the performance of popular SSL methods, indicating its practicality and effectiveness.", "Weaknesses": "Details on the computational cost, especially in relation to the proposed modifications and criterion, are not extensively discussed. The empirical evaluation, although promising, might suffer from a limited scope if only specific datasets are considered. Furthermore, the reliance on existing SSL methods without a standalone validation of UPA's core components could be seen as a potential limitation.", "Questions": "How does the modified Frank-Wolfe algorithm specifically cater to the needs of SSL tasks in comparison to its traditional use cases? Are there plans to explore how UPA can be generalized or tailored to other machine learning contexts beyond the current experiments?"}}
{"paper_id": "HwQ8NVvdmm", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The proposed technique effectively enables low-precision training by leveraging Hadamard transforms and integer-only matrix multiplications, addressing the challenges of model accuracy degradation due to quantization. The paper demonstrates promising results on standard datasets, showcasing minimal accuracy loss while significantly reducing computational requirements.", "Weaknesses": "The paper could provide more extensive comparisons with other quantization techniques and its real-world applications on edge platforms. Additionally, while the method addresses computational constraints, the implementation complexity and potential limitations in various use cases are less explored.", "Questions": "What are the potential implementation challenges for deploying this technique on existing edge hardware? How does the proposed method compare with other state-of-the-art quantization approaches in terms of both efficiency and accuracy on a wider range of tasks?"}}
{"paper_id": "cZo6pDtDZr", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents algorithms that significantly improve the sample complexity for estimating collision probability under local differential privacy, improving upon previous work by a factor of 1/α². The provision of the first sequential testing algorithm for collision probability also marks a significant contribution to the field.", "Weaknesses": "The presentation, while clear in conveying the main algorithmic contributions, could benefit from a more extensive discussion of the implications and potential applications of the work. More contextual information regarding the practical significance and applicability of collision probability in various domains could enhance understanding.", "Questions": "How do the proposed improvements in sample complexity translate into practical applications? Are there specific domains or types of problems where these advancements would be particularly beneficial?"}}
{"paper_id": "F7QnIKlC1N", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 5, "Confidence": 4, "Strengths": "1. The introduction of a novel network based on Graph-Transformer for predicting molecular conformations is a strength.\n2. The proposal of the Molecule Structural Residual Self-Attention mechanism adds value to the field.\n3. The method achieves significant results, outperforming state-of-the-art methods.", "Weaknesses": "1. The novelty of the self-attention mechanism may not be as groundbreaking, as it could be similar to previously developed methods.\n2. Comparison against similar transformer-based molecular modeling methods is lacking.\n3. There are concerns about the simplicity of improvements over a baseline model, which could rely heavily on the model's capacity rather than architectural innovations.", "Questions": "1. How does the proposed approach compare against other recent transformer-based molecular modeling methods in terms of model size and complexity?\n2. Can you provide a more detailed analysis of how the proposed MSRSA mechanism contributes to the improvement in performance?\n3. Are there any results or insights on the interpretability of the learned representations or model parameters?"}}
{"paper_id": "8JCn0kmS8W", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces a novel framework, WavJourney, that leverages Large Language Models for the innovative task of generating storytelling audio content. It successfully bridges different audio models to create harmonious audio from text. The experimental results demonstrate state-of-the-art performance, and the availability of the new multi-genre story benchmark adds significant value.", "Weaknesses": "There may be concerns about the reliance on existing audio generation models. The methodology might be seen more as an integration of existing technologies rather than a fundamentally new model innovation. It is unclear how robust the system is to variances in text input or how it compares with other potential methods without adequate ablation studies.", "Questions": "How does the system perform with different levels of text complexity or detail in descriptions? Can you provide more details on the types of errors or failures that occur within the framework?"}}
{"paper_id": "XbLffB0T2z", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The authors identify a significant limitation in existing data poisoning attacks and propose a method to enhance the transferability of these attacks across different learning paradigms. The paper includes extensive experiments demonstrating the effectiveness of the proposed method on benchmark image datasets.", "Weaknesses": "The originality of the approach might be limited, as it appears to build on existing methods without substantial innovation. The paper also seems to lack some clarity in its main mechanism, and there might be concerns about the proper configuration of experiments related to data augmentations in contrastive learning settings.", "Questions": "How does the proposed method compare in terms of computational cost with existing poisoning strategies, particularly on larger datasets? What specific measures are taken to ensure the reproducibility of results, given the concerns about data transformation configurations?"}}
{"paper_id": "4y3GDTFv70", "review": {"Soundness": 2, "Presentation": 3, "Contribution": 2, "Rating": 3, "Confidence": 3, "Strengths": "The paper addresses the important topic of understanding the behavior of LLMs and provides a potential framework for categorizing language ambiguity.", "Weaknesses": "The explanation for emergent abilities in LLMs seems trivial and lacks depth. The argument does not sufficiently advance understanding of the phenomena.", "Questions": "How does the proposed categorization of language directly contribute to explaining the specific emergent behaviors of LLMs?"}}
{"paper_id": "ODSgo2m8aE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses an important problem of bias and fairness in GNNs by applying the Lipschitz bound concept, offering a theoretical analysis and experimental validation.", "Weaknesses": "The paper may lack a detailed comparison with other methods and does not fully explore the computational implications of applying Lipschitz bounds in GNNs.", "Questions": "How does the proposed Lipschitz bound method compare in performance and computational efficiency with other bias mitigation techniques in GNNs?"}}
{"paper_id": "RlfD5cE1ep", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "This paper provides an interesting extension to the existing theories of non-contrastive learning by considering the cosine loss and feature normalization. It offers a more in-depth understanding of the dynamics involved in non-contrastive learning and emphasizes the role of feature normalization in preventing collapse.", "Weaknesses": "The paper does not consider relevant literature on related work that uses cosine loss, potentially missing important comparisons. The identification of regimes is not well-substantiated and lacks mathematical rigor. Experiments are conducted on simplified models, possibly limiting the generalizability of the findings. Additionally, some figures are difficult to interpret.", "Questions": "The term 'Thermodynamic limit' is used without sufficient explanation for those unfamiliar with it in the context of optimization. How does it relate to the Neural Tangent Kernel regime? What are the specifics of its role in this context? Also, is there more insight on how the exponential moving average in BYOL relates to the findings about dynamics and stability?"}}
{"paper_id": "18TfucMNTr", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses an important problem in training deep neural networks by considering noisy training as a type of optimization by continuation, which is an innovative perspective. The approach is shown to be effective in several test problems.", "Weaknesses": "The paper may not provide enough theoretical backing or insights into why the proposed method reduces the risk of being trapped in local minima, and the experiments might not cover a wide variety of architectures or datasets.", "Questions": "How does the proposed method compare with other state-of-the-art techniques for avoiding local minima in deep neural networks in terms of performance and computational efficiency?"}}
{"paper_id": "JGTC6WgO4T", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The proposed method addresses a significant limitation in unsupervised domain adaptation, extends it to a multi-source setting, and provides a novel approach by refining pseudo labels with a Pseudo label Refinery Network.", "Weaknesses": "The presentation might lack clarity in some aspects, especially when explaining the theoretical foundations and the practical implementations of the proposed method. It is not clear how the method copes with potentially poor quality of sources.", "Questions": "How does the proposed method handle cases where one or more of the source domains provide poor-quality pseudo labels compared to others?"}}
{"paper_id": "fQHb1uZzl7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces a novel approach that combines feature and cost aggregation using a Transformer-based network. The method is simple and effective, leveraging attention mechanisms to unify those techniques. Evaluations on standard semantic and geometric matching benchmarks show significant improvements over existing methods.", "Weaknesses": "The paper could better clarify whether benchmark comparisons use the same datasets for previous state-of-the-art methods. The discussion on the performance in optical flow tasks is vague, lacking details. Additionally, the approach may struggle with images having small overlapping regions due to noise in the cost score matrix.", "Questions": "Can the authors provide more details on the datasets used in comparison to previous works? How does the method perform specifically on optical flow tasks? Are there any failure cases, especially with images having a small overlap?"}}
{"paper_id": "YjG29CIOP7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces robust data-free meta-learning (DFML) techniques for the first time and shows extensive experiments to support its approaches. TEAPOT and ROSY handle task-distribution shift and task-distribution corruption effectively, providing a strategy to maintain memory of old tasks and a model selection defense mechanism, respectively.", "Weaknesses": "There might be concerns about the practical implications of task-distribution corruption risks and the complexity of the proposed RL-based model selection method. The paper may rely heavily on established ideas from continual learning research, and the experiments are limited to 4-layered convolutional networks, lacking exploration with heterogeneous architectures.", "Questions": "Given possible simple data checks, is TDC a significant threat? How scalable are the proposed methods TEAPOT and ROSY to models with more complex architectures? What are the key differences in performance and computational cost compared to existing DFML methods?"}}
{"paper_id": "DmDpu3r8iU", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 5, "Confidence": 3, "Strengths": "This paper introduces a novel framework for assessing the understanding of human values by LLMs, addressing a critical area of concern in AI ethics. The Value Understanding Measurement (VUM) framework provides a systematic approach to evaluate 'know what' and 'know why', potentially offering new insights into value alignment in AI.", "Weaknesses": "The methodology, particularly regarding how the measures are operationalized and tested, needs more clarity. The paper lacks detailed explanations of the evaluation criteria and the role of GPT-4 in both generating and critiquing outputs. The findings, while interesting, heavily rely on assumptions about the capabilities and architecture of GPT models without empirical support.", "Questions": "How can the VUM framework be applied to models beyond GPT-4, and are there plans to validate the findings with human evaluators? Additionally, how do the authors plan to address potential biases in the Schwartz Value Survey when applied within diverse cultural contexts?"}}
{"paper_id": "kXHEBK9uAY", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces an innovative planning method that combines hierarchical and diffusion-based approaches for effective trajectory modeling. The 'jumpy' planning strategy efficiently manages long-horizon tasks and reduces computational cost, as demonstrated by empirical evaluations showing superior performance and efficiency.", "Weaknesses": "The paper might lack novelty in formulating the hierarchical planning method using diffusion models. Potential limitations include challenges in the effectiveness of solving sub-goals independently without precise feedback mechanisms between the levels.", "Questions": "How does the hierarchical model ensure that the jumpy sub-goals lead to optimal trajectories in all cases? Are there any limitations in generalizing to drastically different distributions?"}}
{"paper_id": "i7LCsDMcZ4", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces novel methods (SLTRP & SLRP) for extracting CAMs and saliency maps from SNNs, which contributes significantly to the field of event-based videos. EventRPG shows state-of-the-art results across multiple datasets, showcasing improved augmentation techniques.", "Weaknesses": "There is room for improvement in presenting complex ideas more clearly to a broader audience. Some claims need further experimental validation, particularly on more extensive datasets, which are common benchmarks in the field.", "Questions": "How do the proposed methods compare in performance and computational complexity to existing saliency map extraction techniques in non-event-based SNNs?"}}
{"paper_id": "jHdz0CIS2y", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel training strategy using self-synthesized examples, improving voice conversion models by reducing information loss related to accent and emotion. The framework is versatile, applicable to zero-shot voice conversion and cross-language tasks without text. Extensive comparisons show state-of-the-art results in terms of naturalness, speaker similarity, and intelligibility.", "Weaknesses": "The reliance on self-synthesized examples is innovative, yet the long-term improvements and potential limitations of such a strategy need further exploration. While the paper claims state-of-the-art performance, additional details on the comparison benchmarks would enhance credibility.", "Questions": "How does the framework ensure preservation of subtle speech characteristics over iterative training? Are there tasks where this method does not perform as well? How does it handle extremely varied linguistic inputs?"}}
{"paper_id": "NkYCuGM7E2", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 5, "Confidence": 3, "Strengths": "The paper introduces a novel approach by integrating Large Language Models (LLMs) into autonomous driving systems, leveraging their common sense reasoning. Extensive experiments reportedly demonstrate the superiority of the proposed method over baseline approaches in complex multi-vehicle scenarios.", "Weaknesses": "The soundness of relying on LLMs, which traditionally struggle with low-level control, is questioned. Issues of trustworthiness and consistency are raised, especially given the randomness inherent in LLM outputs. There's also a concern about whether the approach can handle real-time processing needs. Comparison in the evaluation is limited to one RL algorithm and one MPC, which may not sufficiently demonstrate superiority.", "Questions": "How does the approach handle the inherent randomness of LLMs, especially in safety-critical driving scenarios? What are the specific characteristics of the tested scenarios, and do they comprehensively represent real-world complexity and diversity? How resilient is the system to model updates (e.g., GPT version changes) affecting decision consistency?"}}
{"paper_id": "RtDok9eS3s", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a simplified version of the transformer block that maintains training speed and performance while reducing complexity, which is a significant contribution to the field of deep learning. The use of signal propagation theory to justify these simplifications adds to the robustness and soundness of the proposed approach. The reduction in parameters and increase in training throughput are quantifiable benefits that demonstrate the practical impact of the research.", "Weaknesses": "The paper does not address potential limitations or performance issues that could arise from the simplification of transformer blocks in certain tasks or larger models. The scope of the experiments could be broader to include more varied datasets and tasks to fully validate the general applicability of the approach. Additionally, while the theoretical justifications are sound, more empirical evidence on diverse benchmarks would strengthen the paper.", "Questions": "How do the proposed simplifications affect the performance of transformer models on a wider range of tasks beyond those tested in the paper? Are there scenarios where these simplifications might degrade model performance? Is there a limit to the model size or complexity where these simplifications are no longer effective or beneficial?"}}
{"paper_id": "YCPDFfmkFr", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper addresses the important problem of designing optimization layers for neural networks, with a focus on convex quadratic programming (QP) layers. It presents a novel approach for differentiating through both feasible and infeasible QP solutions, demonstrating the potential to enhance predictive performance in learning tasks. The development of a C++ software package, QPLayer, is a notable contribution for the community.", "Weaknesses": "The presentation of the paper could be improved, particularly in terms of clarity and organization. Some technical details may be challenging for readers not deeply familiar with optimization in neural networks, and the benefits of the proposed method over existing straightforward techniques are not extensively validated.", "Questions": "How does the proposed approach compare to simple techniques like introducing slack variables and penalties in terms of performance and advantages? Is there any consideration of the impact on convergence for iterative algorithms using the proposed QP layers? Could the paper improve readability by providing more background or foundational information within the text?"}}
{"paper_id": "ueTdErd5Ib", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper proposes a novel method for contextual stochastic optimization problems that integrates learning with optimization to ensure robustness under uncertain conditions. The method is shown to outperform existing approaches in terms of worst-case cost, particularly in a realistic application of electricity generation.", "Weaknesses": "The paper lacks detailed analysis and comparison with a wider range of existing methods. Some theoretical aspects may need further clarification or justification.", "Questions": "How sensitive is the proposed method to the choice of discretization and threshold parameters? Are there specific scenarios where the method may perform poorly?"}}
{"paper_id": "sDmjlpphdB", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces a novel Mixture-of-Expert approach to improve prompt-based language model performance, achieving significant improvements over existing methods on benchmark NLP tasks.", "Weaknesses": "The clustering-based approach for demo assignment may have issues with scalability or coverage, and the overall impact of the novelty compared to existing auto-prompting techniques is questionable.", "Questions": "How does the method ensure that the clusters and corresponding experts are sufficiently comprehensive and representative of task diversity?"}}
{"paper_id": "Mdk7YP52V3", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper tackles the significant issue of model instability in heteroskedastic neural regression models using the fascinating perspective of statistical physics. It provides a theoretically grounded explanation for the observed phenomena and suggests a more efficient regularization strategy.", "Weaknesses": "The paper may lack detailed empirical validation and quantitative measures for the proposed free energy and regularization impacts. Some conceptual aspects might not be entirely original and are possibly extensions of existing established theories.", "Questions": "Does the proposed regularization hold quantitatively under specific controlled experiments? How novel are the derivations and theoretical frameworks presented in the context of existing literature?"}}
{"paper_id": "YGTSLDAPqb", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper tackles the challenging problem of out-of-distribution generalization and domain adaptation, which is a significant issue in practical deployment of machine learning models. The proposal of 'Connect Later' leverages targeted augmentations in fine-tuning, improving OOD error across several real-world datasets. The improvements in astronomical time-series classification and wildlife species identification demonstrate the approach's effectiveness. The paper is well-written and logically structured, making it accessible.", "Weaknesses": "The paper lacks a thorough comparison between different self-supervised pretraining methods on the same datasets, as different methods were used for different datasets, which complicates direct comparison of the results. Additionally, the importance of targeted augmentations over general augmentations is not sufficiently isolated, and it would be beneficial to include a baseline where general augmentations are used during fine-tuning. Finally, the conclusions regarding the role of pretraining in distribution shifts could be more robust if tested on larger varieties of datasets and clearly demonstrated the assumptions from prior studies.", "Questions": "How does the 'Connect Later' method perform if general augmentations are used during the fine-tuning stage instead of targeted ones? Can the proposed method's benefits over traditional self-supervised pretraining be attributed solely to the targeted augmentation strategy?"}}
{"paper_id": "JTcaziw7G1", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel approach to combining secure multi-party computation with information retrieval, aiming to enhance the privacy of data accessed by large language models.", "Weaknesses": "The proposed method may face practical performance challenges due to the complexity of multi-party computation. The scalability of this approach and the capability to handle very large datasets is a potential concern.", "Questions": "How does the communication complexity of the proposed MPC protocol compare to existing methods in practical scenarios?"}}
{"paper_id": "lEkFq4RUCX", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 8, "Confidence": 3, "Strengths": "The proposed metric, DDF, provides a novel and efficient way to quantify dissimilarity between 3D point clouds, addressing limitations of existing metrics. Extensive experiments demonstrate high accuracy and efficiency across various tasks such as shape reconstruction and scene flow estimation.", "Weaknesses": "The paper abstract lacks detailed explanation on how reference points are generated and how the proposed method handles complex or highly variable point clouds effectively.", "Questions": "How are reference points generated for different point clouds, and is there a mechanism to ensure robustness in diverse scenarios?"}}
{"paper_id": "Fq8tKtjACC", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The model demonstrates impressive performance despite its smaller size compared to competitors, indicating efficient use of resources. The use of \"textbook quality\" data and synthetic exercises produced impressive results, which contributes to the field of efficient AI by achieving strong performance metrics with fewer resources.", "Weaknesses": "The paper lacks detailed information about the training process and dataset specifics, which makes it difficult to fully assess the generalizability and reproducibility of the results. There may also be a lack of comprehensive comparison with a wider range of existing models.", "Questions": "How well does phi-1 perform across a wider range of benchmarks beyond HumanEval and MBPP to ensure its robustness and versatility? What are the implications of the emergent properties of the model and how are they measured?"}}
{"paper_id": "8fQlGQkj0S", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper proposes a generative model for analyzing in-context learning, providing risk upper bounds and identifying a unique phenomenon where increased in-context samples can worsen task retrieval. The theoretical results are validated through numerical experiments.", "Weaknesses": "The paper may rely on specific assumptions that limit the generalizability of the results. The implications of the findings may not be fully explored, especially the practical consequences of the revealed phenomenon in task retrieval.", "Questions": "How do the results extend to more complex or real-world datasets beyond the scenarios tested? How significant is the identified phenomenon in practical applications of in-context learning?"}}
{"paper_id": "hRos9WldRK", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 3, "Strengths": "The proposed method, Learning to Bootstrap (L2B), shows a novel approach to reweighting that adapts dynamically, improving robustness against noisy labels. It is compatible with existing noisy label learning methods. Extensive experiments demonstrate competitive performance on several benchmarks, positioning it as a robust addition to the field. The method offers practical applicability being tested on standard datasets.", "Weaknesses": "The experiments are primarily conducted on image datasets, and it could benefit from broader application domains to establish generalizability. The complexity of the dynamic adjustment mechanism might pose implementation challenges in different settings. More details on the mathematical underpinning of the model's assumptions and limitations could enhance understanding.", "Questions": "How does the L2B method compare in terms of computational efficiency with existing methods? Are there particular scenarios or types of noise where L2B may perform worse? What are the limitations when extending this method to domains other than image classification?"}}
{"paper_id": "ttRSBZiCiu", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "Innovative application of probability flow model. Competitive results compared to previous vocoders. Efficient training with reduced memory footprint.", "Weaknesses": "Limited comparison to other state-of-the-art models. Lack of detailed explanation on the two-stage model. Incomplete analysis on scalability of the approach.", "Questions": "How does the model perform on other audio synthesis tasks beyond speech? Are there any limitations in scalability with larger datasets?"}}
{"paper_id": "WYsLU5TEEo", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel unified framework that addresses both interpretability and adversarial robustness, which are crucial aspects of neural image classifiers. It effectively leverages GANs to generate counterfactual samples that serve dual purposes. The approach is shown to produce descriptive saliency maps and improve model robustness against adversarial attacks.", "Weaknesses": "While the proposed framework is innovative, the paper could benefit from a more thorough explanation of how the framework compares to existing methods in terms of both interpretability and robustness. The paper focuses on two specific tasks, which may limit the generalizability of the results to other domains or types of neural networks.", "Questions": "How does the performance of the unified framework compare with state-of-the-art methods focusing exclusively on interpretability or robustness? Are there any anticipated limitations when applying the framework to more complex datasets or broader classification tasks?"}}
{"paper_id": "KKZaj2QS3G", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces a novel sampling strategy and a new encoder architecture which shows strong experimental results compared to state-of-the-art methods, particularly in the fields of forecasting, classification, and anomaly detection.", "Weaknesses": "There is a lack of consideration of existing methods that address noise in time series data. Some claims in the paper are basic signal processing techniques that are presented as novel contributions.", "Questions": "How does the proposed method compare to traditional noise-handling techniques in time series analysis? Are there specific limitations or scenarios where this approach would not be suitable?"}}
{"paper_id": "LWDRiFzbHQ", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The proposed approach, VRP (vicinal risk proxy), addresses the problem of evaluating model generalization on out-of-distribution data without ground truths. The approach is conceptually innovative, leveraging neighboring test samples to give a holistic model accuracy estimate. The integration with existing indicators to improve correlation with out-of-distribution accuracy is a strong point.", "Weaknesses": "The paper may lack empirical validation on diverse datasets or additional real-world applications, making it hard to generalize the proposed method's effectiveness widely. There might also be concerns about computational efficiency, as the calculation of vicinal scores could potentially be resource-intensive.", "Questions": "How does VRP perform in a variety of real-world applications beyond typical benchmark datasets? Are there any scalability concerns with the method when applied to large-scale datasets, and how does VRP handle different types of out-of-distribution scenarios?"}}
{"paper_id": "Yp01vcQSNl", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel graph transformer architecture that explicitly accounts for edge directionality, which addresses an important gap in existing graph transformers. It also proposes new directional graph datasets, contributing to the practical aspects of the research.", "Weaknesses": "The paper could benefit from clearer explanations and more detailed evaluations of alternative approaches. It is also not clear how well the model can generalize beyond the proposed datasets.", "Questions": "How does the method perform on existing benchmark datasets beyond the newly introduced directional graph datasets? Are there any limitations in terms of scalability or computational requirements?"}}
{"paper_id": "LndMyiBl3n", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. Introduction of the Modified Silhouette Score (MSS) as a measure of graph quality is a noteworthy contribution.\n2. The proposal of SheAttack, an efficient attack method that effectively reduces node distinguishability without needing prior knowledge of labels or the victim model, shows innovation.\n3. The paper provides a comprehensive evaluation through experiments on both synthetic and real-world graphs.", "Weaknesses": "1. The paper's explanation of its theoretical contributions could be clearer, as the abstract relies heavily on jargon which might be difficult for general readership.\n2. The practical applicability of SheAttack under various real-world adversarial conditions wasn't fully demonstrated in the abstract.\n3. The assumptions or limitations of using MSS as an objective were not detailed in the abstract.", "Questions": "1. How does the Modified Silhouette Score quantify graph quality better than previous metrics in practical scenarios?\n2. Are there any specific limitations or computational constraints of the SheAttack method when applied to very large graphs?\n3. How does the method handle varying levels of homophily in graphs, as suggested by your findings?"}}
