{"paper_id": "u3xwwfHmBC", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces a novel approach, Tri-Tense Former (TTformer), which effectively captures spatio-temporal relationships in traffic data using three tense-specific attention modules. This approach appears innovative and more aligned with real-world traffic scenarios, potentially improving forecasting accuracy. Additionally, the contrastive learning with negative filtering technique enhances the model's robustness against incomplete data.", "Weaknesses": "The paper may not adequately address how the proposed tense-specific attention modules integrate with existing techniques for traffic forecasting, or how generalizable this method is across different datasets. The robustness of the model is discussed, but practical challenges in real traffic systems, such as handling abrupt changes or noise in data, may need further exploration.", "Questions": "1. How does TTformer compare with other recent innovations in traffic forecasting models that address spatio-temporal dependencies?\n2. Can the technique of contrastive learning with negative filtering handle sudden anomalies in traffic flow data effectively?\n3. What datasets were used to evaluate the model, and how well does the TTformer perform across various geographical and contextual traffic scenarios?"}}
{"paper_id": "YKvBiRWdQC", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel benchmark for studying zero-shot cooperation in reinforcement learning, focusing on generalisation to new partners and levels. It provides a comprehensive environment to evaluate and improve AI models' ability to cooperate with unseen agents, which has significant implications for real-world applications. The integration with dual curriculum design methods and the availability as an open-source tool adds substantial value to the broader research community.", "Weaknesses": "The challenge primarily focuses on the Overcooked-AI environment, which might limit its applicability to other domains. Furthermore, the paper highlights that state-of-the-art methods fail in this setting, but does not provide insights into potential solutions or improvements, leaving the practical value of the study somewhat exploratory.", "Questions": "What specific characteristics of the Overcooked-AI environment make it particularly suitable for evaluating zero-shot cooperation, and how might these be generalized to other domains? Are there plans to expand the challenge to incorporate different environments or variations to enhance generalizability? What are the potential approaches to overcome the limitations observed with current state-of-the-art methods?"}}
{"paper_id": "tePFpDgyqg", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper identifies the significance of long distance referrals in natural long documents for improving long-context training. The proposed LongPack data pipeline effectively constructs long documents by leveraging hyper-link connections, enhancing the quality and scalability of training data. The experiments demonstrate significant improvements over previous methods, indicating a practical impact.", "Weaknesses": "The approach relies heavily on web page data and hyperlink connections, which might not generalize well to other types of short documents lacking such explicit relationships. The paper could benefit from a more thorough exploration of alternative data sources and structures. Additionally, more detailed analysis on the potential limitations and biases introduced through hyperlink-based document construction is needed.", "Questions": "1. How well does LongPack perform with other types of documents that do not have hyperlink connections, such as books or proprietary textual data?\n2. Are there any strategies considered for mitigating potential biases introduced by specific hyperlink patterns on certain websites?\n3. Can this approach be adapted or extended to incorporate semantic relationships beyond hyperlink connections to further improve long-context model training?"}}
{"paper_id": "gx3LMRB15C", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel augmentation-free approach to self-supervised learning for tabular data, overcoming a significant challenge in constructing data augmentations. The proposed T-JEPA method utilizes a Joint Embedding Predictive Architecture, making it innovative and effective for feature extraction without augmentations. The experimental results clearly demonstrate that T-JEPA enhances classification and regression tasks compared to traditional methods, showing its potential in real-world applications.", "Weaknesses": "While the approach is innovative, the paper may benefit from more detailed comparisons with a broader range of datasets and standard SSL methods to comprehensively evaluate its generalization ability and robustness. Additionally, the introduction of regularization tokens requires further exploration and analysis to deeply understand their impact across varying datasets and models.", "Questions": "1. How does T-JEPA compare to other SSL models on more complex and diverse tabular datasets? \n2. Could the introduction of regularization tokens have specific limitations in certain types of structured data? \n3. What are the potential impacts on computational efficiency and scalability when applying T-JEPA on large-scale datasets?"}}
{"paper_id": "kTH3bEH6hW", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces an innovative augmentation technique for contrastive learning in tabular domains, which is particularly relevant given the high cost of labeling in these settings. The establishment of a comprehensive benchmark, FeSTa, with 42 tabular datasets and 31 algorithms, is a significant contribution that will benefit the research community by providing a standard for comparison. The approach shows strong performance, outranking existing methods, which underscores its potential effectiveness.", "Weaknesses": "The paper appears to focus heavily on the empirical evaluation across the benchmark but might lack detailed theoretical insights into why the proposed augmentation method performs well. Also, the reliance solely on predefined feature-specific ranges for augmentation could limit its adaptability to diverse kinds of tabular data.", "Questions": "How does the predefined feature-specific range selection process work in practice, and how sensitive is the performance to these choices? Is there a mechanism in place for automatic or adaptive determination of these ranges to enhance applicability across different datasets and domains?"}}
{"paper_id": "EqcLAU6gyU", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a clear and well-structured framework for agent-oriented planning in multi-agent systems, introducing key principles such as solvability, completeness, and non-redundancy. The incorporation of a feedback loop to enhance robustness and effectiveness demonstrates thoroughness in design.", "Weaknesses": "While the framework is well-conceived, the novelty might be limited as it builds on existing concepts without introducing entirely new methodologies. The paper could benefit from deeper exploration of potential limitations or challenges in real-world implementations.", "Questions": "How does the proposed framework handle unforeseen scenarios or queries not initially considered during the planning phase? Are there specific case studies or real-world scenarios where the framework has been tested and validated?"}}
{"paper_id": "qrTrnrEi9d", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel framework, TransFusion, which effectively addresses the issue of performing information extraction tasks in low-resource languages by leveraging translations to English. This approach is innovative and practical, significantly improving cross-lingual transfer capabilities. The results showcasing improvements in performance on a diverse set of languages and datasets highlight the method's applicability and robustness.", "Weaknesses": "The reliance on translations to English may limit the performance if the translation is not accurate or if it loses context, especially in highly context-dependent IE tasks. Moreover, the strategy still depends on access to high-quality translation services, which might not be available for all low-resource languages. Details on computation efficiency and the scalability of the TransFusion approach are not fully explored.", "Questions": "How does the accuracy of the translations impact the performance of the model on low-resource languages? Are there any specific translation services or models that need to be used to achieve the reported results with TransFusion? Can this method be generalized or adapted for other NLP tasks beyond information extraction?"}}
{"paper_id": "Bp0HBaMNRl", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel approach to causal discovery with latent variables that is both theoretically sound and practically applicable. It addresses a significant gap in existing methods by relaxing previous assumptions and introducing a differentiable algorithm suitable for nonlinear latent hierarchical models. The results demonstrate improved accuracy and scalability, which are crucial for handling high-dimensional data like images.", "Weaknesses": "The paper might lack details on the method's performance across diverse datasets outside the high-dimensional image domain. Additionally, while the theoretical advancements are notable, the complexity of the proposed approach might limit its accessibility or implementation by practitioners without deep expertise in the field.", "Questions": "What are the potential limitations of the proposed algorithm when applied to domains outside high-dimensional image data? How does the algorithm handle potential issues of overfitting given its complexity, especially in real-world data with noise and other inconsistencies?"}}
{"paper_id": "JzLcKWtGnl", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper proposes a novel approach to enhance spatial perception and reasoning in 3D vision-language tasks by introducing a progressive spatial awareness scheme. The introduction of the 3D instruction dataset and novel tasks such as 3D object distance measurement and 3D layout editing are significant contributions to the field. The experimental results demonstrate superior performance across various tasks, indicating the effectiveness of the proposed method.", "Weaknesses": "The paper does not sufficiently address the flexibility and scalability of the proposed Spatial 3D-LLM across diverse 3D environments or scenes that might vary significantly in scale and complexity. Furthermore, the integration of the proposed model with existing LLM architectures and the computational overhead associated with the enhanced spatial awareness scheme need further clarification.", "Questions": "How does the Spatial 3D-LLM handle inconsistencies in spatial data from different sources or sensors? Can the model effectively integrate with existing LLM architectures without significant retraining or adaptation? What measures are taken to ensure scalability and efficiency as the complexity of 3D scenes increases?"}}
{"paper_id": "hXJrQWIoR3", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper tackles the pertinent problem of explainability in graph representation learning, which is underexplored. The use of graph pattern analysis and pattern counting for explainable representations is innovative. Theoretical analyses provided for robustness and generalization add to the soundness of the method.", "Weaknesses": "The approach may have limitations regarding scalability due to the high-dimensional pattern counting vectors. There's a lack of clarity on how node features are adequately integrated into the framework.", "Questions": "How does the method scale with very large graphs? Are there specific limitations on the size of graphs that can be effectively processed using this framework? How does the method perform when node features are highly informative compared to structural information alone?"}}
{"paper_id": "67sSPPAZiG", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The research makes a significant contribution by generating one of the largest egocentric QA datasets comprising 7M samples and providing a challenging benchmark for egocentric video understanding.\n2. The introduction of a novel multimodal architecture with 'Memory Pointer Prompting' offers an innovative approach to comprehending extended video content effectively, addressing the challenge of recognizing and memorizing details in long videos.\n3. The proposed de-biasing evaluation method is a valuable addition to mitigate language biases in models, enhancing the reliability of evaluations.", "Weaknesses": "1. The data generation process for QA samples, although automated, might need further validation to ensure quality and relevance, given the scale at which it was performed.\n2. The effectiveness of the 'Memory Pointer Prompting' mechanism may require more detailed analysis and comparison with existing methods to substantiate claims of superior performance.", "Questions": "1. How was the quality of the automatically generated QA data ensured, and were any human evaluations conducted to validate the generated data?\n2. Can the proposed techniques and architectures be generalized to other types of video data beyond egocentric videos, or are they tailored specifically for egocentric contexts?\n3. How does the model performance with the new de-biasing evaluation method compare to traditional evaluation techniques, and are there specific metrics that highlight its advantages?"}}
{"paper_id": "kjmLabjSE2", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper provides a significant advance in the analysis of Differential Privacy for Noisy-SGD by proving convergent Rényi DP bounds for non-convex non-smooth losses. This broadens the applicability of DP guarantees to a wider class of optimization problems. The improvement in privacy bounds over state-of-the-art results for smooth strongly convex losses shows the potential impact of this work.", "Weaknesses": "The presentation could be improved to make the innovative elements more accessible to a broader audience. Some readers may find the technical depth daunting, particularly the intricate mathematical analysis of the shifted divergence and Wasserstein distance tracking.", "Questions": "What are the practical implications of applying the Hölder continuous gradient assumption? Would most real-world applications fulfill this assumption? How does the privacy-utility trade-off compare to previous methods in empirical settings?"}}
{"paper_id": "I18MA5DjoP", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The introduction of the Neuron Predictability Lens (NPL) provides a novel mechanism to analyze the internals of Transformer-based LLMs, specifically focusing on neuron predictability. The experimental validation on models such as LLaMA-2 and GPT-J is a significant strength, lending credibility and applicability to the proposed framework. The analysis, which includes both global investigation of FFN contributions and local examination of neuron interpretability, enhances understanding of model behavior and could inform future improvements in efficiency and effectiveness.", "Weaknesses": "While the concept of neuron predictability is intriguing, the practical implications and applications of these findings are not entirely clear. The exploration of 'background neurons' and their role could be expanded further. The presentation, while comprehensive, lacks certain detailed explanations which might be necessary for full comprehension, particularly for readers who are not deeply familiar with the inner workings of LLMs.", "Questions": "What are the practical implications of understanding 'neuron predictability' in LLMs? How does the discovery of 'background neurons' contribute to advancing the field or improving the models in practical applications? Are there possible limitations or challenges anticipated with the implementation of the Neuron Predictability Lens in other models or tasks? "}}
{"paper_id": "qpDqO7qa3R", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces an innovative method that leverages pre-trained image restoration diffusion models for zero-shot video restoration, avoiding the need for retraining and demonstrating strong generalization capabilities. The hierarchical latent warping strategy and the hybrid correspondence mechanism efficiently use spatial, optical flow, and feature matching information, highlighting the robustness and versatility of the approach.", "Weaknesses": "While the proposed method shows significant improvements and flexibility, details on the computational cost and potential limitations in processing time for high-resolution videos are not clear. The paper could also benefit from an exploration of scenarios where this approach may not work as expected.", "Questions": "How does the approach handle videos with complex motion or scenes with rapid temporal changes given the reliance on keyframes and local frames? Are there instances where the method struggles with synchronization between frames?"}}
{"paper_id": "dSQtMx6dPE", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper identifies a critical privacy issue in the emerging area of graph prompt learning and provides a novel solution with differential privacy guarantees. The proposed methods, DP-GPL and DP-GPL+W, appear to be well-designed and effectively address the identified problem while preserving utility.", "Weaknesses": "The presentation of the paper could be improved for clarity, especially regarding technical details of the proposed solutions. Further, while the contribution is notable, it may have limited immediate applicability due to the specialized nature of graph prompts and privacy.", "Questions": "How do DP-GPL and DP-GPL+W techniques specifically account for the small sample size issue that hinders DP-SGD, and do they introduce any notable trade-offs compared to more standard approaches?"}}
{"paper_id": "GbgCRJedQ7", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel approach, Sparse Matrix Tuning (SMT), which addresses the performance gap between parameter-efficient fine-tuning methods and full fine-tuning while significantly reducing memory costs. The method is thoroughly validated across multiple large language model tasks, proving its effectiveness compared to existing techniques like LoRA and DoRA. The fact that SMT does not exhibit performance degradation as the number of trainable parameters increases is a notable strength.", "Weaknesses": "While the paper effectively demonstrates the superiority of SMT over existing methods, the abstract does not provide a detailed explanation of the methodology. The practicality of consistently identifying the most significant sub-matrices for gradient updates could be further validated with more diverse datasets and tasks. Additionally, the contribution in terms of theoretical foundations or insights into why SMT works better than other PEFT methods could be expanded.", "Questions": "1. How does SMT handle different task complexities, particularly when there are very few trainable parameters or very complex tasks? \n2. Can the SMT method be generalized to other architectures beyond large language models like LLaMA? \n3. Are there specific criteria or heuristics for determining the significant sub-matrices, and how computationally intensive is this selection process?"}}
{"paper_id": "ZZVOrId3yN", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents an innovative architecture called CrossModalNet that effectively addresses the challenge of multimodal data fusion in medical image segmentation. The introduction of rigorous mathematical analysis and the Cross-Modal Information Flow (CMIF) metric provide a strong theoretical foundation. The use of Joint Adversarial Domain Adaptation (JADA) further enhances the model's robustness in handling domain shifts. Extensive experiments demonstrate the superior performance of the proposed approach.", "Weaknesses": "While the paper provides a rigorous theoretical foundation, practical implementation details and potential limitations of CrossModalNet in real-world applications are not fully explored. The generalization to other datasets beyond MM-WHS is not addressed.", "Questions": "1. Can the proposed CrossModalNet architecture be easily adapted to other types of multimodal medical datasets beyond MM-WHS?\n2. How does the model perform in terms of computational efficiency and real-time application potential?\n3. Are there any specific limitations or challenges in applying Joint Adversarial Domain Adaptation (JADA) to other medical image segmentation tasks?"}}
{"paper_id": "gLHuAYGs6a", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces a novel multi-view clustering approach utilizing heterogeneous random walks, which is innovative in the context of capturing high-order sample structures across views. The use of a unified sample-level structure to inform both within-view and cross-view structure learning is a notable advancement. The method demonstrates improved performance over several real-world datasets.", "Weaknesses": "The paper may not delve deeply into the complexity of the proposed algorithm, leaving some aspects of scalability and computational efficiency unclear. Additionally, while the approach is innovative, the incremental contribution over existing multi-view clustering techniques might not be groundbreaking.", "Questions": "How does the proposed method scale with increasing numbers of views and larger datasets? Are there specific limitations when applying this method to diverse types of multi-view data beyond what was tested? How sensitive is the clustering performance to the choice of parameters in the random walk strategy?"}}
{"paper_id": "x5l5PRtvul", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel application and theoretical development of h-SVGD, demonstrating its ability to alleviate variance collapse without additional computational cost or requiring factorization of the target density. The work includes significant theoretical contributions, such as the solution to the hybrid Stein partial differential equation and characterizing a special case where h-SVGD acts as a kernelised Wasserstein gradient flow.", "Weaknesses": "The paper suggests a bias in the mean field limiting distribution, as h-SVGD does not converge to the target distribution. It is unclear how significant this bias is in practice and how it affects various applications. Presentation, while comprehensive, could benefit from clearer organization of results and more intuitive explanations of complex theoretical aspects.", "Questions": "How does the bias in the mean field limiting distribution impact practical applications of h-SVGD? Could the convergence issues be mitigated, and are there specific settings where the bias might have minimal effects? What are the specific computational advantages or limitations h-SVGD introduces compared to other SVGD variants?"}}
{"paper_id": "pOUAVXnOQP", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The introduction of Sinusoidal Trainable Activation Functions (STAF) presents a novel approach to overcoming the limitations of ReLU networks' spectral bias. The paper demonstrates that STAF can achieve superior performance in reconstructing high-frequency details of continuous signals, with faster convergence and better PSNR compared to state-of-the-art methods like KAN, WIRE, SIREN, and Fourier features.", "Weaknesses": "The presentation of the paper could be improved with clearer explanations of the technical details and empirical evidence provided. Additionally, while the results are promising, it would help to understand any potential limitations or challenges in applying STAF to a broader range of signal types and applications.", "Questions": "How does STAF perform when applied to signals with varying levels of noise or in different domains beyond computer graphics? Are there specific scenarios where STAF could be less effective than other methods?"}}
{"paper_id": "iN64nSYt0z", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The proposed GUIDE method appears simple yet effective in enhancing alignment of LLM outputs with user instructions. The introduction of the Influence metric helps in understanding the propagation and impact of instructions, which could be valuable for further research on improving LLM alignment.", "Weaknesses": "The paper lacks detailed explanation on the application and robustness of the method across various domains and types of instructions. The described method's scalability and integration into existing LLM frameworks are not sufficiently discussed.", "Questions": "How does GUIDE handle situations where instructions are ambiguous or conflicting? Is the approach applicable across all types of LLMs or only specific architectures? More clarity on how Influence is quantitatively measured would be helpful."}}
{"paper_id": "gRuZkEy49k", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel adaptation of MCTS for GFlowNet training, leveraging connections between GFlowNets and maximum-entropy RL. It provides theoretical insights and proves that MCTS can be adapted for optimal flow calculation in GFlowNet contexts. The methodology is validated through extensive experiments across diverse domains, demonstrating significant improvement over standard sampling techniques.", "Weaknesses": "The presentation could be improved for better clarity and comprehension, especially in explaining complex concepts related to GFlowNets and MCTS. While the experiments are extensive, more details on parameter tuning and the exact setup could help in reproducing the results.", "Questions": "How do the computational costs of using MCTS compare with standard on-policy sampling methods in GFlowNet training? Can the approach be generalized to continuous domains, or is it inherently limited to discrete spaces?"}}
{"paper_id": "d23EVDRJ6g", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper addresses a crucial issue in the application of generative masked models for animation tasks, where large datasets may not be available. MotionDreamer provides a novel approach of embedding motion into quantized tokens, along with a sliding window local attention in a masked transformer, leading to superior results in animation generation. The proposed method is shown to outperform state-of-the-art GAN or Diffusion-based methods in terms of faithfulness and diversity.", "Weaknesses": "The paper could provide more details on the specifics of the distribution regularization method and the codebook construction, which are crucial components of MotionDreamer. Additionally, while the results are promising, further real-world applications and datasets showcasing broader applicability would strengthen the paper.", "Questions": "1. How does the distribution regularization method work in detail, and what are its limitations?\n2. Are there specific limitations or scenarios where MotionDreamer might not perform as well as GAN or Diffusion-based methods?\n3. How does the method cope with extremely long or short motion sequences, and is there a breakdown of performance based on motion duration?"}}
{"paper_id": "kQ5s9Yh0WI", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper provides an innovative approach to enhancing the generation length of LLMs by decomposing tasks into subtasks.\nThe introduction of the AgentWrite pipeline is a valuable contribution that enables existing models to exceed previous output length limitations.\nThe creation of a new dataset, LongWriter-6k, and a benchmark, LongBench-Write, helps in systematically evaluating and advancing long-output LLM capabilities.", "Weaknesses": "The primary contribution relies significantly on increasing the amount of long-output data available during training, which is an incremental rather than a novel solution.\nThe effectiveness of the approach may be limited by the capability to source or generate quality long-output training data in other domains.\nThe reliance on existing models and datasets may limit the generalization of the results to different architectures or language models.", "Questions": "How does the performance of the proposed method compare to alternative approaches in different linguistic or domain contexts?\nIs there empirical evidence on how well the subtasks generated by AgentWrite maintain coherence when integrated back into longer outputs?\nAre there plans to extend the dataset or benchmark to include more diverse or multilingual scenarios?"}}
{"paper_id": "9CqkpQExe2", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel Ada-K routing strategy that dynamically adjusts the number of experts activated for each token, thus optimizing computational resources without compromising model performance.\n2. Extensive evaluations demonstrate significant improvements over conventional MoE-based LLMs, including over 25% reduction in FLOPs and more than 20% inference speedup.\n3. The strategy implements a learnable and lightweight allocator module that is versatile and broadly applicable across mainstream MoE-based LLMs.\n4. Detailed analysis provides insights into model behavior and adaptive MoE system design, which could be very useful for further development in this field.", "Weaknesses": "1. The paper may lack clarity in presenting technical details, particularly in the implementation of the dynamic allocator module, which could affect reproducibility.\n2. More discussion on the limitations or potential downsides of using the Ada-K routing strategy, such as any trade-offs in latency or hardware requirement impacts, would be beneficial.\n3. Insights into specific conditions or scenarios where the proposed method might underperform, compared to static Top-K routing, are missing.", "Questions": "1. How does the Ada-K routing strategy handle cases where the cost of switching experts might outweigh the benefits of adaptive allocation?\n2. Are there specific types of tasks or datasets where Ada-K routing showed less improvement compared to static Top-K, and what factors contributed to this?\n3. How robust is the method to variations in token distribution, especially for edge cases with highly imbalanced or rare distributions?"}}
{"paper_id": "PpYy0dR3Qw", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel algorithm, LoCoDL, which effectively combines local training and compression to improve communication efficiency in distributed optimization and federated learning. The approach shows a theoretical doubly-accelerated communication complexity, which is well-grounded in theory. The empirical results are convincing and demonstrate the significant practical advantages of the algorithm over existing methods.", "Weaknesses": "The presentation of the paper could be improved for clarity, especially in the explanation of how the unbiased compressors are integrated into the algorithm. Furthermore, the abstract does not provide details on the experimental setup, which makes it difficult to assess the breadth of the empirical validation.", "Questions": "1. What datasets and experimental conditions were used to validate the performance improvements of LoCoDL claimed in the paper? \n2. How does LoCoDL handle non-convex optimization problems, if at all? \n3. Are there specific scenarios or types of federated learning applications where LoCoDL would not perform well?"}}
{"paper_id": "UatDdAlr2x", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper offers a detailed theoretical and empirical investigation into how simple transformer architectures can implement various counting strategies. It provides insights into the dependencies between model architecture and hyperparameters, highlighting the significant impact of small design choices on the transformer's solution space. The dual analysis of strategy emergence in theory and practice is particularly strong.", "Weaknesses": "The paper may not introduce entirely novel architectural innovations or tasks, focusing instead on analyzing existing components under constrained conditions. While the analysis is rigorous, the practical applicability of the findings to more complex real-world tasks remains to be shown. There is also potential for more thorough evaluation on diverse tasks beyond the histogram counting.", "Questions": "How do the findings in this paper generalize to complex tasks faced by transformers in real-world applications? Do the counting strategies identified have implications for enhancing the interpretability or efficiency of larger models used in practice?"}}
{"paper_id": "IwgmgidYPS", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "The paper presents an impressive and comprehensive dataset, MedTrinity-25M, which covers a vast range of medical images and annotations, significantly expanding available resources for medical AI. The use of a novel automated pipeline to generate multigranular annotations without reliance on paired text descriptions is innovative and addresses a major bottleneck in dataset creation. The dataset has the potential to support a wide variety of tasks, contributing notably to the development of foundation models in the medical domain.", "Weaknesses": "The paper relies heavily on the quality and performance of domain-specific expert models for grounding and producing ROI descriptions, which may introduce biases depending on the models' limitations. Additionally, while the dataset is proposed as a resource for pre-training, the generalization of findings across diverse real-world medical contexts may require further validation outside controlled testing environments. Furthermore, the paper does not provide much detail on the specific challenges faced during dataset compilation or how they were resolved.", "Questions": "1. How were the domain-specific expert models selected and validated for their role in the grounding process of ROI identification?\n2. Could the authors elaborate on potential limitations encountered in terms of bias or ethical considerations when creating MedTrinity-25M? \n3. Are there any plans for continual updates or expansions of the dataset to incorporate emerging medical imaging modalities or diseases?"}}
{"paper_id": "WG7GzGx3G9", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel approach to handling outliers in activations for INT4 quantization by proposing the Rotated Runtime Smooth (RRS) method. This method effectively utilizes the Runtime Smooth operation to eliminate channel-wise outliers and the Rotation operation to manage spike outliers, thus improving inference performance significantly. The results, such as improving WikiText-2 perplexity from 57.33 to 6.66, demonstrate the effectiveness of the approach.", "Weaknesses": "While the approach appears sound, the presentation could be clearer, especially regarding how the Rotation operation specifically addresses spike outliers. Additionally, the implementation details for practical deployment in large scale systems are not extensively covered, which could be important for scaling the solution.", "Questions": "1. How does the proposed RRS method impact the overall computational efficiency and latency compared to previous approaches?\n2. Are there any specific cases where the RRS might not perform as well, or certain types of outliers where it is less effective?\n3. Can this method be generalized to other types of networks or tasks beyond those tested in the paper, such as different architectures or non-language model tasks?"}}
{"paper_id": "BhECSDSkAE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel task of one-shot medical video object segmentation by leveraging static image datasets, which is a valuable contribution to the field of medical imaging where annotated video data is scarce. The proposed approach effectively uses labeled images with minimal annotations to achieve segmentation in videos, which is demonstrated to be effective through extensive experimentation. The introduction of a new dataset, OS-I2V-Seg, also provides a useful resource for future research.", "Weaknesses": "While the method and results appear promising, the paper may lack details on potential limitations or specific challenges encountered during implementation that could impact real-world application. The effectiveness of the proposed memory mechanism and self-distillation approach might require further validation across different datasets to ensure generalization.", "Questions": "1. How does the proposed method perform on different medical imaging modalities beyond the ones tested in the paper?\n2. Are there any specific limitations or failure cases identified during experimentation with the proposed framework?\n3. Can the memory mechanism and self-distillation approach be adapted for other domains, or are they specifically tailored for medical imaging and videos?"}}
{"paper_id": "mnB4hDTIDr", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses an underexplored area of diversity evaluation in LLM-generated datasets, proposing a novel approach that is grounded in theoretical principles. DCScore is demonstrated to have lower computational costs, making it an efficient alternative to existing methods. The experimental validation of DCScore shows its effectiveness and better correlation with various diversity pseudo-truths.", "Weaknesses": "The abstract does not provide detailed information on the datasets used for experimentation or the specific diversity pseudo-truths considered. Further clarity on these aspects would strengthen the understanding of the method's practical applicability.", "Questions": "What specific datasets were used in the experiments, and how were the diversity pseudo-truths defined? Could the authors provide examples or case studies where DCScore has successfully improved model performance through enhanced dataset diversity evaluation?"}}
{"paper_id": "Y0qmwm6tgy", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper identifies a critical instability issue in existing one-shot gradient pruning algorithms and proposes a robust solution with MoreauPruner. The rigorous experimental evaluation across multiple large language models shows the effectiveness of the proposed method in improving pruning stability. The introduction of the Moreau envelope for estimating weight importance is a novel contribution to the field of model pruning.", "Weaknesses": "While the method is robust against perturbations, the paper could benefit from a clearer explanation of the underlying theoretical mechanisms that enable MoreauPruner's robustness. Some sections of the paper may require more detailed descriptions or graphical illustrations to improve the clarity of presentation, particularly for readers less familiar with optimization techniques.", "Questions": "1. How does the computational complexity of MoreauPruner compare to existing pruning methods, particularly in large-scale settings?\n2. Are there specific types of weight perturbations or model architectures where MoreauPruner performs suboptimally?\n3. Can MoreauPruner be easily extended to other types of neural networks beyond those evaluated in the study?"}}
{"paper_id": "aAcOaJYbUg", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces LAION-C, a new OOD benchmark specifically designed to address the limitations of prior benchmarks in the context of modern web-scale datasets. The benchmark includes novel distortion types and is evaluated across state-of-the-art models. The inclusion of psychophysical experiments provides a unique comparison of model performance to human observers, demonstrating the relevance and depth of the work.", "Weaknesses": "There is a potential limitation in the generalizability of the findings, as the benchmark focuses on distortions that are novel but might still occur naturally or be biased by the dataset selection. Some of the psychophysical findings may not generalize beyond specific platform limitations or experimental conditions.", "Questions": "How do the newly introduced distortion types address specific limitations found in ImageNet-C, and how do they ensure they remain OOD in rapidly changing web-scale datasets? Are there plans to continuously evolve or adapt LAION-C to ensure it remains relevant as datasets grow in size and complexity?"}}
{"paper_id": "duGygkA3QR", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "1. The paper introduces an innovative approach by integrating Dynamic Mode Decomposition (DMD) with Graph Neural Networks (GNNs), leveraging dynamical systems theory to improve GNN capabilities.\n2. Extensive experiments on various tasks including directed graphs, large-scale graphs, long-range interactions, and spatial-temporal graphs highlight the model's versatility and superior performance.\n3. The theoretical foundation is strong, with established connections between the DMD-estimated operator and the original dynamic operator.\n4. The proposal to incorporate domain-specific constraints like symmetry addresses practical applications and enhances model relevance to real-world systems.", "Weaknesses": "1. While the use of DMD is well-motivated, the paper might need to further clarify the computational overhead introduced by incorporating DMD into GNNs, especially in large-scale applications.\n2. The empirical results are promising, but more comparisons with existing state-of-the-art models in specific tasks could strengthen claims of superiority.\n3. The integration details of domain-specific constraints are briefly mentioned but could use further practical examples or case studies for better understanding.", "Questions": "1. Can the authors provide more detailed insights into the computational efficiency of the DMD-enhanced GNNs compared to traditional GNNs?\n2. How does the performance vary across different types of graph structures, especially those with varying densities and node degrees?\n3. Are there specific types of graphs or datasets where the DMD-GNN approach may not perform as well, and if so, what adaptations might be suggested?"}}
{"paper_id": "s6nYndMwG7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper provides a novel approach to selecting hyperparameters for LiSSA based on spectral properties of the Hessian, which could improve the practicality of influence functions for large models. The empirical results that confirm the new method's effectiveness are valuable for the machine learning community.", "Weaknesses": "The paper may lack a thorough discussion on the computational overhead introduced by evaluating the spectral properties. Additionally, the potential generalization of these findings to models not covered in the study is not discussed in detail.", "Questions": "How does the computational cost of the proposed spectral evaluation compare to the traditional hyperparameter tuning process? Can the applicability of the new hyperparameter choices be extended to other approximation methods beyond LiSSA and PBRF?"}}
{"paper_id": "9BVMD3keG8", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper provides a well-formulated approach to tackle the online learning problem of brokerage by characterizing regret bounds for different feedback scenarios. The theoretical results are robust and offer strong bounds that are optimal up to logarithmic factors. The problem setup reflects real-world trading, and the results have potential practical implications.", "Weaknesses": "While the paper is theoretically sound, it may lack clarity in some parts of the presentation, potentially making it harder for non-expert readers to grasp the concepts. Additionally, empirical validation would enhance the impact of the theoretical findings.", "Questions": "How does the proposed algorithm perform in practice with real trading data? Can the method be adapted for non-linear relationships between contextual information and market values? What are the implications of lifting the bounded density assumption in realistic market environments?"}}
{"paper_id": "mnna9LUg7P", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper addresses a significant challenge in deploying State Space Models (SSMs) for cloud and edge applications by providing an effective quantization method. The proposed 8-bit per-tensor quantization methodology is novel and demonstrates clear improvements in both model efficiency and preservation of accuracy. The results are impressive, showing substantial efficiency gains with minimal impact on performance, suggesting high practical relevance.", "Weaknesses": "While the paper effectively demonstrates the advantage of the proposed quantization method on certain hardware (Nvidia Orin Nano 8G), it would be beneficial to see results across a broader array of hardware platforms. Additionally, more detailed comparison with other quantization techniques could further establish the unique advantages of the proposed method.", "Questions": "How does the proposed quantization method compare with other existing quantization techniques specifically designed for Transformers or other neural network structures? Can the method be easily adapted to other SSM architectures, and are there any foreseeable limitations in doing so? Furthermore, how does this approach handle varying hardware constraints across different platforms beyond those tested?"}}
{"paper_id": "3X3LuwzZrl", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel model, Label Influence Propagation (LIP), which effectively addresses the complex influences between labels in non-Euclidean graph data for multi-label node classification. The abstraction of message passing into propagation and transformation, and the creation of a label influence graph, adds a new dimension to the field, leading to significant improvements over state-of-the-art methods. Comprehensive evaluations on benchmark datasets further validate the model's effectiveness.", "Weaknesses": "While the model introduces a new concept in handling label influences, the paper may lack sufficient qualitative analysis explaining the specific scenarios or datasets where LIP sees the most improvement. A deeper exploration of model interpretability could be beneficial. Additionally, while the presentation is mostly clear, parts of the methodology and results could benefit from more detailed explanations and visual aids, aiding readers unfamiliar with the specific graph types used.", "Questions": "1. How does the label influence graph construction handle sparse datasets where label co-occurrence might be limited?\n2. Are there specific limitations with certain types of graphs or node features where LIP underperforms compared to other models?\n3. How scalable is LIP concerning computational complexity when handling extremely large graphs with high dimensional label sets?"}}
{"paper_id": "IqaQZ1Jdky", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper proposes a novel framework for Kolmogorov-Arnold Networks, utilizing Bernstein polynomials as the function basis, which is both theoretically supported by the Weierstrass approximation theorem and demonstrates robustness through uniform convergence.\n2. By incorporating learnable activation functions on edges, the approach enhances both the accuracy and interpretability of neural networks compared to traditional MLPs.\n3. Comprehensive experiments across diverse fields, including time series forecasting, computer vision, and function approximation, demonstrate the method's superior performance over conventional approaches and other KAN variants.", "Weaknesses": "1. The abstract does not provide detailed comparisons or specific results from the experiments, which would help in understanding the practical impact of the method more clearly.\n2. The reliance on Bernstein polynomials, while robust, may introduce additional complexity in implementation and optimization, which is not addressed in the paper.\n3. While the concept of learnable activation functions on edges is promising, the scalability of this approach in larger networks or datasets is not discussed.", "Questions": "1. How does the implementation of learnable activation functions on edges scale with the size of the network, especially in larger datasets or deeper architectures?\n2. Can the framework for VBn-KAN be easily adapted to other types of neural networks, such as convolutional or recurrent networks, beyond the fields tested?\n3. What are the specific computational overheads introduced by using Bernstein polynomials compared to traditional activation functions?"}}
{"paper_id": "OdMqKszKSd", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel approach to generating 3D articulated objects from a single image, which is both practical and scalable. The use of a diffusion model to account for the variability in part shapes and motion is innovative. The proposed coarse-to-fine pipeline with part connectivity graph and part abstraction enhances the quality of the generated objects. The method significantly outperforms existing state-of-the-art techniques in terms of realism and reconstruction quality.", "Weaknesses": "The paper might lack clarity in certain sections, particularly in the explanation of the diffusion model and its implementation details. Additionally, there may be insufficient exploration of the limitations or failure cases of the proposed method. The presentation of experimental results, while strong, could benefit from more detailed qualitative analysis or visual examples.", "Questions": "How does the diffusion model specifically handle the ambiguity in part shape and motion from single-view images? Are there any specific limitations or scenarios where the proposed method struggles to generate accurate articulated objects? Could the authors provide more visual examples of the generated objects compared to the input images to demonstrate the effectiveness of their approach?"}}
{"paper_id": "LOBhVTtVnc", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents an innovative approach by leveraging a novel Transformer-based architecture for dynamics prediction tasks, which is different from the traditional GNN methods.\nThe introduction of the Temporal Difference Graph (TDG) is a key innovation that effectively mitigates cumulative errors in long-term predictions, showcasing improved performance across various physical systems.\nThe experimental validation is comprehensive, covering multiple challenging physical systems, and the results demonstrate state-of-the-art performance.", "Weaknesses": "The paper might lack detailed ablation studies or comparison of its components to ensure clear understanding of what contributes most to the improved performance.\nSome technical details or hyperparameters related to the GSTs and TDG might not be thoroughly explained, which could make reproducibility challenging without the complete methodology.", "Questions": "How does the computational complexity of GSTs compare with that of traditional GNN-based methods, especially in large-scale simulations?\nWhat specific mechanisms are in place within the GST framework to handle extensive input sequences efficiently while preserving computational resources?\nAre there specific scenarios where the proposed method underperforms significantly compared to existing methods, and what are the possible reasons?"}}
{"paper_id": "oa5UeyUVMm", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel application of diffusion probabilistic models to graph representation learning, an area that has been relatively unexplored. The theoretical foundation provided, proving that the denoising objective maximizes conditional mutual information, enhances the soundness and depth of the study. Empirical results indicating state-of-the-art performance on a wide variety of datasets further support the effectiveness of the proposed method.", "Weaknesses": "While the contribution and theoretical framework are significant, the presentation could be enhanced for greater clarity, particularly in explaining the proof of the theoretical results and the details of the experimental setup. This might make the content more accessible to a broader audience.", "Questions": "Are there specific challenges or limitations faced when applying diffusion models to graph data compared to traditional vision applications that the study did not address in detail? How does the proposed method handle large-scale graphs in terms of computational efficiency?"}}
{"paper_id": "5swfKRkCx7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The proposed method introduces a novel approach to integrate external knowledge in LLMs via an additional attention head, which potentially resolves knowledge comprehension and reasoning challenges. The memory-based mechanism for dynamic knowledge integration is innovative and shows promise across both general and specific-domain QA tasks.", "Weaknesses": "The paper might lack a detailed analysis of how the additional attention head interacts with the existing LLM architecture, which could be crucial for understanding its robustness and scalability. The memory mechanism's complexity and computational overhead are not addressed, raising concerns about its practicality in real-time applications.", "Questions": "How does the proposed external knowledge attention head affect the overall computational performance of the LLM? Can this method be generalized to other NLP tasks outside of QA?"}}
{"paper_id": "5s1qpjrNvZ", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel approach to guided reinforcement learning by introducing a method to calculate the sampling rate of the guide policy, guaranteeing performance thresholds. It provides a robust and effective algorithm that simplifies integration with existing methods. The theoretical performance guarantee is a significant contribution to the guided RL field.", "Weaknesses": "The paper relies on certain assumptions for the performance guarantee, which might limit its applicability in more complex or dynamic environments. Additionally, the presentation could be improved to make the methodology and results more accessible and clear to readers.", "Questions": "How sensitive is the proposed method to variations in the assumed conditions for the guarantee? Were there any specific domains or applications where GRL-RB showed significant advantages over standard methods?"}}
{"paper_id": "WNb4P8aG66", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The proposed Diffusion on Diffusion (DoD) framework effectively addresses the limitations of conventional class-guided diffusion models by introducing a novel multi-stage generation approach with visual priors. The method demonstrates significant improvements in efficiency and performance, achieving superior results on ImageNet-$256 \\times 256$ with reduced training costs.", "Weaknesses": "The presentation of the paper could be improved for clarity, especially explaining the latent embedding module and its impact on discarding redundant information. More detailed comparisons with the baseline models could enhance understanding of the results.", "Questions": "How does the latent embedding module specifically operate in terms of architecture and computational cost? Are there any limitations of the DoD method when applied to other datasets or domains beyond ImageNet?"}}
{"paper_id": "UfczlMudN6", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel framework that addresses both in-distribution and out-of-distribution generalization challenges in deep reinforcement learning within a unified architecture. The introduction of a robust adaptation module and a joint training pipeline is innovative and demonstrates improved generalization performance in realistic tasks with quadruped robots.", "Weaknesses": "The paper may lack detailed theoretical analysis or proofs to strongly support the claims about the generalization capabilities of the proposed algorithm. Additionally, the abstract does not mention any comparative experiments against existing state-of-the-art methods, which would be useful to benchmark the performance improvements.", "Questions": "How does the proposed framework compare to existing state-of-the-art reinforcement learning methods in terms of generalization performance? Are there specific real-world deployment trials of the algorithm, and if so, what were the results?"}}
{"paper_id": "60Vd7QOXlM", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "1. The paper introduces significantly more effective canaries for privacy auditing of language models, surpassing prior state-of-the-art approaches.\n2. The approach is applicable across various realistic threat models and demonstrates strong empirical results.\n3. The method provides a substantial advancement in privacy auditing by achieving nontrivial success where prior methods failed.", "Weaknesses": "1. The abstract does not provide detailed explanations on how the canaries are generated or the specific methodologies applied.\n2. More insights into the computational complexity and scalability of the proposed solution would be beneficial.", "Questions": "1. How are the new canaries generated, and what specific attributes make them more effective than those used previously?\n2. Are there any limitations or specific types of language models for which this auditing technique may not work as effectively?"}}
{"paper_id": "GqhvJ1o8m5", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel framework for generating stereo videos from single-view ones, using a dual-branch architecture that leverages spatial-temporal attention mechanisms without requiring explicit disparity maps. This approach reduces potential errors associated with traditional disparity estimation models. The introduction of the YouTube-SBS dataset is a significant contribution, providing a large-scale benchmark for stereo video generation models. The quantitative improvements over existing methods demonstrate the effectiveness of the proposed framework.", "Weaknesses": "While the technical advancements and large dataset are valuable, the paper would benefit from a more detailed analysis of how the spatial-temporal attention mechanisms specifically contribute to the improvements in results. Additionally, more information on the comparative baselines and evaluation protocols would enhance the transparency and replicability of the experiments.", "Questions": "1. How does the spatial-temporal attention mechanism specifically improve the quality of stereo video generation compared to traditional methods? \n2. Are there any limitations in the scope of the YouTube-SBS dataset, such as bias towards certain types of videos or scenes, that could affect the generalizability of the results? \n3. Can the proposed framework be extended or adapted for real-time stereo video generation, or is it currently limited to offline processing?"}}
{"paper_id": "ua5MHdsbck", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel approach to protein design, focusing on the importance of capturing triplet relations over traditional pair-based methods. The proposed progressive search method is innovative, leveraging concepts from preference alignment in large language models.\nThe evaluation on AAV and GFP proteins demonstrates significant improvements in extrapolation tasks, indicating the method's practical effectiveness.", "Weaknesses": "The abstract does not provide detailed insights into the mechanisms or architecture of the proposed model, which makes it harder to evaluate its uniqueness and potential challenges.\nFurther details on the comparative performance to existing state-of-the-art methods would strengthen the contribution claim.", "Questions": "How does the introduction of triplet relations influence computational complexity compared to pair-based methods?\nWhat are the specific challenges faced in implementing preference alignment models for protein design, and how were they addressed in the study?\nAre there any limitations identified in the current approach regarding certain types of proteins or specific design goals?"}}
{"paper_id": "56Zn3halhq", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel learnable data augmentation method, AutoTSAug, which effectively addresses the challenge of capturing specific patterns in time series forecasting. The use of reinforcement learning and variational masked autoencoders is innovative and demonstrates a cutting-edge approach to augmenting marginal samples. The empirical analysis for identifying which parts of the data to augment adds robustness to the method.", "Weaknesses": "Although the proposed method is promising, the paper could benefit from more detailed experimental results comparing AutoTSAug directly with other state-of-the-art time series forecasting augmentation techniques. Additionally, the specific implementation details and settings of the reinforcement learning approach could be expanded upon.", "Questions": "1. How does AutoTSAug specifically determine and differentiate 'marginal samples' from the rest of the dataset?\n2. Can the approach be extended to domains beyond time series data effectively, or does it rely specifically on the nature of time series forecasting datasets?\n3. Are there any limitations or scenarios where AutoTSAug might not be as effective in improving forecasting performance?"}}
{"paper_id": "aPTGvFqile", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses a significant limitation of CLIP embeddings, namely the modality gap, and proposes a solution in the form of AlignCLIP. The approach is coherent and yields improvements in zero-shot and fine-tuning evaluations, highlighting the importance of addressing the modality gap in cross-modal tasks.", "Weaknesses": "While the paper proposes an interesting solution, it lacks clarity on how the semantically-regularized separation objective function and shared learnable parameters are implemented and evaluated. Additionally, more detailed experimental results and comparisons with other modality alignment methods would strengthen the paper's claims.", "Questions": "How does AlignCLIP compare with other state-of-the-art methods specifically targeting the modality gap in CLIP embeddings? Can the authors provide more details on the architecture changes and how they impact performance besides experimental results?"}}
{"paper_id": "GOwNImvCWf", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents a novel insight into the realm of neural network weights by identifying shortcomings in the current use of autoencoders for weight space learning and proposing an innovative behavioral loss to address this gap. The novel combination of structural and behavioral losses effectively improves model reconstruction performance, which is a significant contribution to weight-based representation learning.", "Weaknesses": "While the paper introduces the concept of a behavioral loss, it may lack comprehensive theoretical backing or evidence on why this specific behavioral component is essential in weight space. The abstract does not provide detailed information on how behavioral loss is technically implemented and assessed across experiments. More detailed experiments and comparisons with existing methods would strengthen the claims.", "Questions": "1. How is the behavioral loss formulated, and how does it specifically contribute to the performance improvements compared to existing reconstruction losses?\n2. Can this approach be generalized to all types of neural networks, or are there specific limitations in terms of model architecture or application that were observed?\n3. Are there any specific downstream tasks where this approach significantly outperforms others, or does it provide a general improvement across the board?"}}
{"paper_id": "C9BA0T3xhq", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel approach for offline reinforcement learning that addresses the exploration-exploitation trade-off by extending the Bellman update to actions not in the dataset. This approach could potentially enable better utilization of static datasets and improve performance in environments with sparse rewards or incomplete trajectories.", "Weaknesses": "The paper might lack a clear comparison with other state-of-the-art offline RL methods in diverse environments. Details on error bounds and theoretical guarantees of the proposed approach might also be insufficiently discussed.", "Questions": "What are the specific limitations of the EIQL approach compared to traditional methods in terms of computational complexity? How does the new algorithm handle scenarios where the extrapolated actions might lead to worse policies?"}}
{"paper_id": "EeqlkPpaV8", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses an important problem of parallelized sampling in large-data applications, providing novel insights into the adaptive complexity of sampling. The use of a chain-like structure with random partition and classical smoothing techniques is innovative.", "Weaknesses": "The paper might be very technical, making it difficult to understand without a strong background in the area. The implications of the results and their application to real-world problems are not clearly articulated.", "Questions": "What are the practical applications of these theoretical findings? How do these results compare to existing sampling methods in terms of efficiency and accuracy?"}}
{"paper_id": "oK1zJCWBqf", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The proposed SPO method offers a novel way to align generative models with human preferences without relying on a reward model, which can simplify implementation. The theoretical foundation linking SPO to the Bradley-Terry model underlines a solid theoretical motivation.", "Weaknesses": "The abstract lacks detailed empirical results to substantiate the claims of simplification and alignment precision. Furthermore, more information on how SPO performs in practical applications compared to existing methods is needed.", "Questions": "How does SPO perform compared to reward model-based methods in terms of computational resources required and alignment quality? Are there any scenarios where SPO may not be as effective?"}}
{"paper_id": "Dq9VrVuLzV", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper proposes an innovative approach to generating synthetic annotated datasets by using a diffusion model conditioned on 3D occupancy labels. This approach addresses the challenge of high human labor in generating dense 3D annotations and provides a scalable solution. The method effectively integrates 3D semantic multi-plane images for accurate geometrical conditioning, resulting in photorealistic and accurate dataset synthesis. Extensive evaluations demonstrate the model's capability to serve as an effective data augmentation tool for enhancing perception models in autonomous driving.", "Weaknesses": "While the paper presents a robust solution, it relies heavily on the quality and accuracy of the input semantic multi-plane images (MPIs), which may not always be available or accurately annotated in real-world scenarios. The scalability of the approach to very large scenes and diverse real-world conditions could be further explored.", "Questions": "How does the performance of models trained on SyntheOcc-generated datasets compare with those trained on real-world datasets in a variety of environmental conditions? Are there specific limitations to the types of occupancy labels or scenes that SyntheOcc can accurately synthesize?"}}
{"paper_id": "73Q9U0vcja", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces an innovative methodology by combining generative diffusion models with active learning to address inverse problems in imaging. This approach is novel and demonstrates significant improvements in reducing data acquisition requirements while enhancing image reconstruction quality. The use case in X-ray computed tomography is compelling and showcases the practical application of the method.", "Weaknesses": "The paper could benefit from more extensive experimental validation across a broader range of datasets and use cases to further fortify the claims of generalizability and efficacy of the proposed method. Additionally, reliance on domain-specific data for pre-training the diffusion model might limit its applicability to well-represented domains.", "Questions": "How does the proposed method perform in domains with limited training data, and what are the limitations regarding the diversity and representativeness of the pre-trained diffusion model in such scenarios? Can this approach be generalized to other types of imaging inverse problems beyond X-ray computed tomography?"}}
{"paper_id": "UiEjzBRYeI", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper proposes a novel methodology, SAM-CP, to enhance the capabilities of the Segment Anything model (SAM) by introducing composable prompts for semantic-aware segmentation. This approach effectively allows for versatile segmentation across open and closed domains, achieving state-of-the-art results in open-vocabulary segmentation. The framework is well-designed, efficient in handling multiple semantic classes, and demonstrates broad applicability.", "Weaknesses": "While the methodology and results are promising, the paper could improve in presentation clarity, specifically in the detailed explanation of the composable prompts and their integration with the SAM model. Some parts may require additional technical details or visual aids to help comprehend complex concepts more easily.", "Questions": "Can the SAM-CP framework be adapted to other types of segmentation tasks beyond semantic, instance, and panoptic segmentation? What are the primary limitations when applying this model to more complex, real-world datasets?"}}
{"paper_id": "lessla98Wp", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces a novel application of language-guided video colorization, which addresses the limitation of previous methods by utilizing a cross-modality generative model.\n2. The inclusion of instance-aware text embeddings and temporally deformable attention offers a creative solution to achieving color consistency while allowing for creative color applications.\n3. The use of extensive experimental results adds credibility to the claims of superior performance and illustrates the model's effectiveness in achieving consistent and creative colorization.", "Weaknesses": "1. While the proposed model appears innovative, the reliance on pre-trained models might limit the novelty of the approach, as much of the success could leverage existing properties of these models.\n2. The abstract hints at complex components such as the cross-modality pre-fusion module and temporally deformable attention, yet does not describe their working in detail, which could be crucial to fully understanding the mechanism.\n3. Concerns about computational overhead or complexity introduced by these new modules are not addressed, which might impact practical applications.", "Questions": "1. How does the proposed L-C4 model handle different types of language variability (e.g., synonymous descriptions) when guiding the colorization process?\n2. Can the authors provide more insights into the computational efficiency of their model, particularly how the proposed modules affect processing speed compared to existing methods?\n3. What are the limitations of this language-based approach in scenarios where the language descriptions may be ambiguous or insufficient to contextually guide the colorization process? How does the model handle such cases?"}}
{"paper_id": "UstOpZCESc", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses a novel and challenging problem by integrating lifelong learning with privacy-aware unlearning. PALL allows for both forward knowledge transfer and exact unlearning within a cohesive framework, which is crucial for compliance with data privacy regulations. The use of task-specific sparse subnetworks and episodic memory rehearsal is innovative and shows promise. The scalability is empirically validated across various architectures, demonstrating the practical applicability of the proposed solution.", "Weaknesses": "The framework's dependency on episodic memory might introduce scalability concerns when dealing with very large datasets. Additionally, while the empirical results demonstrate the efficacy of the approach, the paper could benefit from more detailed analysis on the trade-offs between memory consumption and unlearning efficacy. Moreover, it remains unclear how the approach would handle more complex data structures beyond image classification.", "Questions": "How does PALL handle memory constraints, especially given the need for episodic memory rehearsal in large-scale datasets? Can the method be effectively applied to other domains, such as NLP, which might have different requirements for knowledge representation and privacy constraints? The balance between retaining useful knowledge and unlearning sensitive data is essential; how sensitive is PALL to these competing objectives as task complexity grows?"}}
{"paper_id": "LnRegTxsa4", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "1. The introduction of the Cross-view and Cross-attention Module (CVCAM) and Multi-head Spatial Attention Module (MHSAM) provides a novel approach to improving cross-view object geo-localization. 2. The proposed method effectively addresses existing challenges such as edge noise and insufficient transfer of information between views. 3. The creation of the G2D dataset enhances the resources available for the 'Ground→Drone' localization task.", "Weaknesses": "1. The paper might omit specific architectural details or mathematical formulations that would help in replicating the results. 2. There might be a limited exploration of how the proposed modules generalize to other types of cross-view localization tasks beyond 'Ground→Drone'. 3. The evaluation is primarily based on only two datasets, which may not fully capture the model's robustness across diverse scenarios.", "Questions": "1. How does the proposed method perform on other cross-view tasks beyond 'Ground→Drone'? 2. Can the CVCAM and MHSAM modules be integrated with other localization frameworks, or are they tightly coupled with the current architecture? 3. Could further clarity be provided on the specific types of convolutional kernels used in the MHSAM and their impact on performance?"}}
{"paper_id": "2ErS9Bkc3O", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper provides a rigorous matrix-theoretic explanation for the adversarial fragility of deep neural networks, contributing to the theoretical understanding of a significant issue in machine learning. The analytical results are precise and align with existing feature-compression-based explanations.", "Weaknesses": "The paper primarily focuses on theoretical analysis without empirical validation, which is crucial for verifying the practical applicability of the findings. The presentation could be improved for better clarity to a broader audience not well-versed in matrix theory.", "Questions": "Have the theoretical results been compared with empirical observations in any real-world datasets or neural network architectures? How might these findings influence the design of more robust neural networks?"}}
{"paper_id": "pK3oe2bubc", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses the important issue of robustness in neural networks by allowing vision transformers to adapt to arbitrary layer orders. The approach to randomize attention module execution order during training is innovative and holds potential for distributed neural networks. The ability to merge trained models and prune layers at test time without significant performance loss is a notable achievement.", "Weaknesses": "The presentation could be clearer, with more detailed explanations of the proposed training approaches. While the adaptability of the model is impressive, the 20% reduction in accuracy is a significant trade-off that may limit the applicability of the approach. The evaluation of the approach may benefit from testing across a broader set of vision transformer models.", "Questions": "1. How does the proposed training method perform when the model size is scaled up or down? \n2. Are there specific types of vision transformer architectures that benefit more from this approach? \n3. Can this method be applied to other types of neural networks beyond vision transformers, and if so, how would it be adapted?"}}
{"paper_id": "sec09tLQUl", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper effectively addresses an important issue of spurious feature exploitation in deep learning, proposing a novel method called FairDropout. The methodology is grounded in recent research findings about neuron memorization, and it shows significant improvements across multiple domains, including vision, language, and healthcare.", "Weaknesses": "The approach may be limited in scenarios where spurious feature memorization is not confined to a small subset of neurons. Additionally, the paper does not clearly address potential trade-offs in terms of computational overhead or the impact of dropout on the overall model performance.", "Questions": "How does FairDropout perform in extremely unbalanced datasets, where minority classes are significantly smaller than majority classes? Is there a threshold for the number of neurons to be dropped for the method to remain effective while maintaining overall model performance?"}}
{"paper_id": "96jZFqM5E0", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper proposes a novel framework, SiMHand, for pre-training 3D hand pose estimation using a large-scale dataset of hand images from in-the-wild videos. It introduces an innovative contrastive learning method that adaptively weights the loss based on inter-sample distance. The experimental results show significant performance gains over previous state-of-the-art methods on multiple datasets.", "Weaknesses": "The paper might benefit from further exploration of potential limitations, such as the dependency on a large volume of training data. Additionally, while the proposed method outperforms existing approaches, the paper could provide more analysis on computational complexity and real-time applicability of the framework.", "Questions": "1. Can the proposed framework be applied to other domains beyond 3D hand pose estimation?\n2. How sensitive is the performance of the proposed method to the choice of hyperparameters, particularly in terms of contrastive loss weighting?\n3. Are there any challenges in implementing this framework on lower-resource computing devices?"}}
{"paper_id": "uNd289HjLi", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel self-supervised framework, Corruption2Self (C2S), for MRI denoising that addresses key limitations of current methods, such as oversmoothing of fine features. The introduction of a generalized ambient denoising score matching (GADSM) loss represents a significant contribution by effectively modeling high-SNR images from noisy data. Furthermore, the incorporation of a noise level reparameterization and detail refinement extension demonstrates a comprehensive approach to enhancing training stability and balancing noise reduction with feature preservation. The extension to multi-contrast denoising is also a valuable addition, leveraging cross-contrast information to improve performance. The method's performance is thoroughly evaluated on well-known datasets, demonstrating competitive results against state-of-the-art approaches.", "Weaknesses": "The primary potential weakness lies in the novelty of the core techniques, such as the GADSM loss, where additional clarifications on the theoretical underpinnings or comparative analysis with similar state-of-the-art methods could enhance the study. Furthermore, the paper might benefit from wider exploration or investigation of C2S in varied clinical MRI settings or across additional datasets to further substantiate the robustness and generalizability of the approach.", "Questions": "How does the GADSM loss compare with other denoising score matching or self-supervised loss functions in terms of computational complexity and training stability? Are there scenarios or types of MRI acquisitions where C2S might not perform as expected or where additional adjustments to the framework may be necessary? Can the approach be adapted or scaled to real-time processing demands in clinical environments?"}}
{"paper_id": "e2ONKX6qzJ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "The paper addresses a critical issue in classifier-free guidance (CFG) for diffusion models, specifically oversaturation and unrealistic artifacts at high guidance scales. The proposed Adaptive Projected Guidance (APG) method effectively decomposes the CFG update term to mitigate these issues, which is novel and insightful. The approach is easy to implement and adds minimal computational overhead, making it highly practical. The extensive experimental validation demonstrates significant improvements in FID, recall, and saturation scores, showcasing the method's effectiveness across various models and samplers.", "Weaknesses": "While the APG method appears to be highly effective, the abstract does not provide detailed information about the diversity of datasets or model types tested. Additionally, the connection between CFG and gradient ascent is described as an insight but may require further elaboration in the full paper for complete clarity.", "Questions": "1. How does APG perform across different types of diffusion models besides the ones tested? Is there a theoretical limitation to its applicability?\n2. Are there specific conditions or types of input data where APG might not perform as well as CFG?\n3. Can the momentum method introduced for rescaling be fine-tuned for different applications, and if so, how does this impact model performance?"}}
{"paper_id": "a2eBgp4sjH", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The paper provides a comprehensive theoretical foundation for MultiFilterANN, including space-time tradeoffs and limitations of existing algorithms. It extends known graph-based ANN algorithms to handle filter constraints effectively. The empirical evaluation shows the competitiveness and versatility of the proposed approach, adapting seamlessly to multiple filters. Additionally, the release of novel datasets is a valuable contribution to the research community.", "Weaknesses": "While the theoretical insights are strong, the paper might benefit from more extensive empirical comparisons across a wider variety of datasets and configurations to further validate the generalizability of the proposed approach. Additionally, details on the practical implementation aspects of integrating filter constraints within graph indices could be more elaborately discussed.", "Questions": "1. How does the performance of the proposed approach scale with a very large number of filters compared to existing solutions?\n2. Are there specific types of datasets or label distributions where the proposed method outperforms or underperforms more evidently?\n3. Can the penalized distance function used for filter constraints be adapted or tuned to further improve performance, and what guidelines exist for such tuning?"}}
{"paper_id": "EIXZXPz7jU", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces an innovative sampling point selection method using diffusion models, effectively tackling the problem of singularities in the source functions of PDEs for PINNs. The approach offers a novel use of optimal transport coupling to increase sampling in high residual areas, enhancing both accuracy and computational efficiency.", "Weaknesses": "The presentation could be improved for clarity and accessibility, especially in explaining the complex mathematical techniques to a broader audience. Detailed implementation steps and hyperparameter choices for the diffusion models and flow-matching techniques may not be thoroughly covered in the abstract.", "Questions": "How is the effectiveness of the sampling method measured quantitatively in comparison to existing methods? Are there specific limitations or conditions under which the proposed method outperforms normalizing flow-based sampling, and can these be highlighted with examples?"}}
{"paper_id": "QO4bF6MHza", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The introduction of MathHay provides a novel benchmark specifically designed to evaluate the long-context mathematical reasoning abilities of large language models, filling an important gap in current evaluation methods.\nThe paper provides extensive experiments and analysis on eight top-performing LLMs, offering valuable insights into their current capabilities and limitations in mathematical reasoning over long contexts.", "Weaknesses": "While the benchmark is introduced with a valuable purpose, the paper could have been strengthened by providing more detailed insights or proposed methods for improving LLMs' performance on MathHay.\nThe paper focuses heavily on evaluation but does not propose new methodologies for enhancing the mathematical reasoning abilities of LLMs over long contexts.", "Questions": "What are the specific types of mathematical reasoning tasks included in the MathHay benchmark?\nAre there aspects of the benchmark that specifically isolate reasoning errors from retrieval errors?\nHave the authors considered extending MathHay to include real-world applications or scenarios that necessitate long-context mathematical reasoning?"}}
{"paper_id": "GULx8rzzjC", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 3, "Strengths": "The proposed method, Mycroft, effectively addresses the significant challenge of data acquisition by allowing model trainers to evaluate and utilize data with a constrained sharing budget. This approach is innovative in utilizing feature space distances and gradient matching to identify informative data subsets, thereby maximizing model performance with minimal data exposure. The experimental results are robust, showing convergence to full-information performance across tasks and domains.", "Weaknesses": "While the paper presents strong empirical results, there may be a lack of clarity in the mathematical formulation and assumptions underlying Mycroft's approach. The reliance on feature space distances and gradient matching could be sensitive to the choice of dimensionality reduction techniques or types of models used.", "Questions": "How does Mycroft handle potential biases present in the small data subsets it identifies? Are there any safeguards against overfitting to these subsets, especially in high-variance tasks? Also, is Mycroft applicable to other domains beyond those tested in the paper?"}}
{"paper_id": "PnZ2lbQaao", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The proposed adversarial Bayesian framework, Domain Indexing Collaborative Filtering (DICF), addresses the cold-start problem in cross-domain recommendation systems. The approach not only improves recommendation performance but also offers interpretability, which is a valuable aspect in understanding how cross-domain knowledge is transferred. The framework is evaluated on both synthetic and real-world datasets, demonstrating its applicability and effectiveness.", "Weaknesses": "The concept of domain indexing, while interesting, may require further explanation to clarify its novelty compared to existing methods in the literature. Additionally, the framework relies on the assumption that the adversarial Bayesian approach can generalize well across different domains, which may not hold in all scenarios. More extensive experimentation with diverse datasets would strengthen the validation of the proposed method.", "Questions": "How does the proposed framework handle highly dynamic domains where item features or user preferences change rapidly? Could the interpretability aspect be further elaborated, perhaps with a case study or visualization, to illustrate how domain indices are inferred and utilized? Are there any limitations or assumptions in the framework that might affect its scalability or applicability in larger, more complex domains?"}}
{"paper_id": "AT64R0ivUO", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The proposed SteerPrompt method effectively addresses a critical limitation in large language models by improving reading comprehension through steering attention scores, thereby enhancing the model's ability to focus on essential contextual information. The method shows significant improvement in open-book QA performance without altering model parameters.", "Weaknesses": "The paper might benefit from a more thorough exploration of different types of contexts to determine how versatile and adaptable the SteerPrompt method is across varied datasets and tasks beyond open-book QA.", "Questions": "1. How does SteerPrompt handle extremely long contexts where critical information is sparsely distributed?\n2. Are there any known limitations or potential edge cases where SteerPrompt might not perform well?"}}
{"paper_id": "nGiGXLnKhl", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a variant of the RWKV model adapted for vision tasks, demonstrating significant improvements over existing models like ViT. The method's ability to efficiently handle high-resolution images without relying on window operations is a notable contribution. The evaluations provide strong empirical evidence of improved performance in both classification and dense prediction tasks, alongside faster speeds and reduced memory usage.", "Weaknesses": "While the paper presents strong empirical results, more details on the architectural modifications from RWKV to VRWKV could enhance understanding. It's crucial to ensure transparency in what specific changes enable the observed performance gains. Additionally, the method should be validated across a broader range of vision tasks to fully establish its generalizability.", "Questions": "1. What are the specific architectural changes made from the RWKV to VRWKV, and how do they contribute to improved performance in vision tasks?\n2. How does VRWKV perform on more complex vision tasks beyond image classification and dense prediction, such as object detection or segmentation?\n3. Can the VRWKV architecture be extended or adapted for use in multimodal tasks involving both vision and language?"}}
{"paper_id": "TBJCtWTvXJ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel optimizer, SignSoftSGD (S3), which builds on the strengths of Adam while addressing its known weaknesses. The theoretical analysis provided shows that S3 achieves optimal convergence rates under general nonconvex stochastic conditions. The experimental results are comprehensive, demonstrating that S3 consistently outperforms or matches AdamW in various tasks, even with a larger learning rate. The integration of memory-efficient Nesterov's accelerated gradients is a notable enhancement.", "Weaknesses": "While the proposed optimizer shows promise, the novelty may be considered incremental as it builds on existing ideas like SignSGD and Nesterov's acceleration, adapting them into a new optimizer rather than introducing entirely new concepts. Further experiments in diverse domains beyond vision and language tasks could help strengthen the generalizability of the claims. Additionally, the theoretical insights could be further expanded to provide more detailed intuition about the optimizer's behavior.", "Questions": "1. How does the proposed S3 optimizer perform in domains other than vision and language tasks?\n2. Can the theoretical analysis provide more detailed insights or intuitions on why S3 avoids loss spikes better than Adam?\n3. How sensitive is S3 to the choice of learning rate compared to other optimizers like Adam and SGD?"}}
{"paper_id": "8gSrJOL2oc", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper proposes a novel framework that effectively addresses key limitations in compositional zero-shot learning by mitigating background impact, utilizing advanced word embeddings from Multimodal Large Language Models, and applying attribute smoothing to reduce overconfidence in models, enhancing generalization.", "Weaknesses": "The proposed methods, while innovative, may add complexity to the model that requires careful tuning and validation across diverse datasets. There is limited discussion on potential drawbacks or computational overhead introduced by the FAA modules and the new embedding strategies.", "Questions": "How does the introduction of Multimodal Large Language Model embeddings affect the computational efficiency of the model compared to baseline methods? Are there scenarios where TRIDENT might underperform due to its complexity, especially on smaller or less diverse datasets?"}}
{"paper_id": "yheQRc5xWB", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel approach using state-space models to tackle the challenges of time-varying counterfactual prediction, achieving both effective prediction and high efficiency. The concept of covariate-based decorrelation is innovative and addresses confounding bias while preserving covariate information. Extensive experiments demonstrate that the proposed model outperforms existing baselines significantly.", "Weaknesses": "The paper could improve in presentation clarity, particularly in explaining the technical details of the proposed decorrelation method and its implementation. Additionally, while the paper claims efficiency, specific comparative metrics on running time in various conditions are not clearly reported.", "Questions": "How does the proposed Mamba-CDSP model specifically compare to state-of-the-art LSTM or Transformer based approaches in terms of absolute running time for long sequences? Can the decorrelation approach be generalized to other models beyond state-space models? Are there specific scenarios or datasets where the model does not perform as well and what are the potential causes?"}}
{"paper_id": "HxKSzulSD1", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses a relevant and emerging problem in the field of AI alignment, specifically focusing on the interactions between weak and strong models in a superalignment context. The investigation into weak-to-strong deception is novel and provides valuable insights into potential risks associated with strong AI models. The extensive experiments performed in various scenarios add credibility to the findings.", "Weaknesses": "The paper does not clearly outline the methodology used for identifying and quantifying the weak-to-strong deception phenomenon, which could limit reproducibility and understanding of the results. The effectiveness of the proposed bootstrapping technique to mitigate deception is limited, and further exploration into alternative solutions would strengthen the contribution of the paper.", "Questions": "1. How exactly was the weak-to-strong deception quantified, and what criteria were used to distinguish between aligned and misaligned behaviors?\n2. Are there specific characteristics of the intermediate models used in bootstrapping that contribute to its partial success, or could any model potentially serve this purpose?\n3. How do the findings generalize to other types of alignment tasks beyond the multi-objective alignment case studied?"}}
{"paper_id": "wH8XXUOUZU", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "The paper introduces innovative techniques to address the challenge of maintaining reconstruction accuracy at high spatial compression ratios in autoencoder models for diffusion models. The proposed methods, Residual Autoencoding and Decoupled High-Resolution Adaptation, are well-explained and demonstrate significant speedup and improved performance over existing methods. The empirical results on ImageNet show impressive improvements in both training and inference speedup, validating the effectiveness of the approach.", "Weaknesses": "The paper might need more clarity on potential limitations or scenarios where the proposed approach may not perform as well. While speedup and performance improvements are highlighted, a deeper analysis of the trade-offs involved in very high compression ratios could strengthen the work.", "Questions": "How does the performance of DC-AE compare on datasets other than ImageNet? Are there specific applications or scenarios where the compression might degrade reconstruction quality? Could the training strategy of Decoupled High-Resolution Adaptation be extended or modified for other types of generative models?"}}
{"paper_id": "QWIg5e6mtT", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper addresses a significant gap in the existing literature by tackling the challenge of inadequate policy expressiveness in heterogeneous team games.\n2. The introduction of H-PSRO is novel and potentially impactful for solving complex team coordination problems.\n3. Theoretical guarantees for monotonic improvement in team rewards and lower exploitability are well-substantiated.\n4. Empirical results demonstrating H-PSRO's superiority in convergence and performance compared to non-heterogeneous baselines are positive and promising.", "Weaknesses": "1. The paper's presentation could be improved, as some parts may require additional clarity, especially regarding methodological details.\n2. While the empirical results are compelling, further validation across a wider range of games and settings would strengthen the generalizability claims.\n3. The experimental setup details are limited, which hinders reproducibility and nuanced understanding of the experiments conducted.", "Questions": "1. Can the proposed H-PSRO framework be easily extended to games with more than two teams?\n2. Are there specific game settings or domains where H-PSRO might not outperform existing methods, and if so, what are the limitations?\n3. How does the computational complexity of H-PSRO compare to Team PSRO in practice, given the sequential optimization steps?"}}
{"paper_id": "kbSU5bwoRv", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel zero-shot singing voice conversion model that effectively combines content, timbre, and pitch features, demonstrating successful conversion even to non-human timbres. The use of a large-scale dataset with over 1,815 hours of singing and 6,367 speakers is significant and supports the model's performance. The proposed method shows strong results in both objective and subjective tests, highlighting its robustness.", "Weaknesses": "Although the model is innovative and performs well, the paper could provide more in-depth analysis of the system's limitations and potential areas for improvement. Additionally, the presentation of results could be clearer, with more structured explanations of the experiments and their outcomes.", "Questions": "How does the model ensure that the semantics of the original singing are preserved accurately in the zero-shot conversion? Could there be potential issues with semantically complex or culturally specific songs? Can the model be extended or adapted to convert speaking voices as well, and if so, what modifications would be necessary?"}}
{"paper_id": "KyqtKhv6q1", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel framework, Differentiable Map Priors, that introduces the concept of leveraging historical spatial information to improve autonomous vision systems. This approach integrates with existing 3D perception systems without incurring significant computational overhead, providing a practical enhancement in performance. The empirical results demonstrate significant and consistent improvements in key tasks such as 3D object detection and semantic map segmentation on a recognized benchmark dataset, nuScenes.", "Weaknesses": "While the integration of map priors is a practical and impactful contribution, the novelty of the approach itself may be limited as it builds upon existing perception systems. There is also minimal information provided in the abstract regarding how the integration occurs or the theoretical foundations supporting the model improvements. More details would be useful to fully understand the mechanism and scalability of the approach.", "Questions": "How does the framework specifically leverage historical traversal data to improve performance, and what are the potential limitations in different environments where history may be less available? Are there any domain adaptations or assumptions that need to be considered when applying Differentiable Map Priors across different geographic locations or mapping datasets?"}}
{"paper_id": "lTL4t68BNc", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper proposes a novel approach, Robust Graph Attention inspired by Information Bottleneck (RGA-IB), which brings a new perspective to enhancing the robustness of attention-based Graph Neural Networks (GNNs) through minimizing IB loss. This methodology is supported by extensive experiments that demonstrate significant improvements in accuracy under adverse adversarial attacks, showcasing the robustness and practicality of the proposed approach. The work provides a fresh linkage between the Information Bottleneck principle and robustness in GNNs, potentially inspiring future research along similar lines.", "Weaknesses": "The paper primarily claims robustness improvements through lowering IB loss, but the exact trade-offs or potential limitations, such as increased computational demands or any specific types of attacks where the method might not be as effective, are not thoroughly discussed. Some assumptions made in linking IB loss directly with robustness might require deeper theoretical reinforcement or detailed experimental justification to enhance confidence further. Moreover, clarity around the interpretability and scalability of the proposed method in larger or more complex graph datasets would benefit practitioners.", "Questions": "1. How does the proposed RGA-IB method perform in terms of computational efficiency and scalability, especially for large-scale graphs?\n2. Are there any specific types of adversarial attacks where the method does not perform significantly better than existing techniques?\n3. While the focus is on node classification accuracy under adversarial conditions, does RGA-IB affect other tasks like link prediction or graph-level classification in GNNs?"}}
{"paper_id": "hWmwL9gizZ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. ProVaccine introduces an innovative dual attention mechanism into deep learning for immunogenicity prediction, enhancing accuracy and generalizability significantly.\n2. The study compiles a comprehensive dataset encompassing over 9,500 antigen sequences and structures, thus establishing a valuable resource for future research.\n3. Extensive experimental validation across various metrics showcases the model's strengths and potential in advancing vaccine design.", "Weaknesses": "1. While the methodology is robust, the paper could have further delved into potential limitations of ProVaccine or scenarios where it might underperform.\n2. The integration of the dataset and its potential limitations such as data imbalance or source representation bias are not discussed thoroughly.", "Questions": "1. How does ProVaccine handle data imbalance across different antigen sources in the compiled dataset?\n2. Can the dual attention mechanism be effectively adapted or scaled for other types of protein prediction tasks?\n3. How does ProVaccine ensure interpretability and transparency in its predictions, particularly in a post-hoc validation setting?"}}
{"paper_id": "IL85Ebjg9j", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses an important issue in multi-view classification by introducing a trust-based discounting method to resolve conflicts between views. The approach is evaluated on six real-world datasets, demonstrating its effectiveness, and introduces a new metric that considers ground truth in evaluating predictions.", "Weaknesses": "The method assumes some prior knowledge about the trustworthiness of individual views, which may not always be practical or easy to ascertain in real-world scenarios. Additionally, the paper might not fully explore the limitations of the proposed approach or compare it with all possible existing alternatives.", "Questions": "1. How is the instance-wise probability-sensitive trust estimated in practice, and what assumptions do we need to make about the availability of such information? \n2. Can the approach be easily extended or adapted to account for more complex relationships between views, such as non-linear dependencies? \n3. How does the performance vary when different types of conflicts arise that involve dependencies among more than two views?"}}
{"paper_id": "cnKhHxN3xj", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper proposes a novel methodology to measure neuronal entanglement using the Wasserstein distance, providing fresh insight into the interpretability of large language models under weight sparsity conditions. The experimental framework for disentangling polysemantic neurons as a mixture of experts is innovative and contributes significantly to understanding and optimization within neural networks.", "Weaknesses": "While the introduction of Wasserstein Neurons and the approach appear strong, the paper could benefit from clearer explanations of technical details, especially for broader accessibility. The presentation might be overly technical for audiences not deeply familiar with advanced neural network interpretability methods.", "Questions": "1. How does the proposed method scale with increasingly larger language models, given the computational cost of calculating Wasserstein distances?\n2. Are there specific tasks or types of language models where this method is particularly advantageous?\n3. Can the approach be generalized to other architectures beyond large language models, such as in convolutional neural networks?"}}
{"paper_id": "dgR6i4TSng", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces an innovative use of quantum computations for parameter-efficient fine-tuning, which is a novel approach in improving parameter efficiency in transfer learning. The use of quantum unitary parameterization for achieving a logarithmic increase in trainable parameters demonstrates significant advancements in parameter efficiency over traditional PEFT methods such as LoRA. This could have important implications for future machine learning models that require efficient resource usage.", "Weaknesses": "The integration of quantum computing principles may present complexity that is challenging to comprehend for those not deeply versed in quantum mechanics, potentially limiting its accessibility to a broader machine learning audience. While initial results show promise, the reliance on quantum computations raises questions about the practical implementation and accessibility of this approach on a wider scale.", "Questions": "How does Quantum-PEFT compare in terms of computational cost and time efficiency against conventional methods when scaled up for large dimensions in real-world applications? Are there any constraints on hardware or accessibility when implementing Quantum-PEFT in practice across standard computing environments?"}}
{"paper_id": "ye1mxb79lw", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "The paper introduces a novel algorithm for bilevel Bayesian optimization that is sample-efficient, which is crucial for problems characterized by noisy, constrained, and derivative-free settings. BILBO's ability to utilize confidence bounds for constructing trusted solution sets is innovative and shows theoretical robustness with the presentation of sublinear regret bounds. The empirical evaluations on synthetic and real-world problems substantiate the theoretical claims.", "Weaknesses": "The paper may rely heavily on theoretical assumptions that are common in Bayesian optimization but may not hold in all practical applications. Potential limitations with respect to different simulator setups for the decoupled upper- and lower-level evaluations were not thoroughly explored.", "Questions": "1. How well does BILBO adapt to different noise levels in the optimization landscape?\n2. Can BILBO be effectively extended to solve multi-objective bilevel problems?\n3. What are the potential computational overheads associated with maintaining and querying the trusted sets?"}}
{"paper_id": "1fwZJzGdKj", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces a novel multi-agent collaborative approach to data selection for LLM pretraining, addressing the often overlooked conflicts between existing data selection methods. The framework demonstrates a significant performance gain in multiple LLM benchmarks, suggesting its effectiveness in improving data efficiency and convergence speed. The use of an agent console for dynamic integration of information is a notable strength.", "Weaknesses": "The paper could provide more detailed explanations on how different data selection criteria are reconciled by the agent console, and the specific mechanics involved in the updating of prioritization rules by each agent. More clarity on the experimental setup and baselines used for comparison could strengthen the evaluation.", "Questions": "1. How are conflicts between different data selection methods handled in scenarios where they have substantially opposing criteria?\n2. Could you elaborate on the structure and decision-making process of the agent console?\n3. How were the specific benchmarks chosen for the experiments, and do they cover a wide enough range of language model applications? What are the limitations when extending this method to other domains or datasets?"}}
{"paper_id": "pISLZG7ktL", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a comprehensive study on data scaling in robotic manipulation, identifying key insights about the importance of environment and object diversity over sheer quantity of demonstrations. This can have significant implications for data collection in robotics, providing practical guidelines to achieve high success rates efficiently. The empirical evaluation is robust, leveraging a large dataset across many environments and objects to support the findings.", "Weaknesses": "While the study identifies important scaling laws and offers a novel data collection strategy, it primarily focuses on single-task settings and may not fully address complexities in multi-task or more dynamic environments. The focus on imitation learning may also limit generalizability to reinforcement learning contexts or other forms of robot learning.", "Questions": "How well do the data scaling laws identified translate to more complex or multi-task scenarios in robotic manipulation? Could the findings on the importance of environment and object diversity be applied effectively in those cases as well?"}}
{"paper_id": "dML3XGvWmy", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 3, "Strengths": "The Gödel Agent presents an innovative concept of self-evolving agents inspired by the Gödel machine. By allowing agents to modify their logic dynamically through LLMs, the framework targets continuous self-improvement, leading to enhanced performance, efficiency, and generalizability.", "Weaknesses": "The paper might lack sufficient details on how the dynamic adaptations are monitored and guided to prevent undesirable evolution. Moreover, the scalability and practical implementation challenges of deploying such self-improving agents remain unclear.", "Questions": "1. How does Gödel Agent ensure that the self-modifications remain aligned with the high-level objectives?\n2. What mechanisms prevent the system from evolving into sub-optimal or unsafe configurations?\n3. Could this framework be extended to areas beyond mathematical reasoning and agent tasks?"}}
{"paper_id": "xlxGsX1pc7", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The introduction of the U-MATH benchmark provides a significant contribution to the evaluation of mathematical skills in LLMs by addressing the gaps of diversity and complexity in existing datasets. The use of unpublished university-level problems adds a novel and challenging aspect. Including multimodal problems is a progressive step in understanding LLMs' capabilities across different formats. The open-sourcing of U-MATH, µ-MATH, and evaluation code enhances reproducibility and encourages further research in the field.", "Weaknesses": "The reliance on LLMs to judge the correctness of solutions could introduce biases or errors if these models are not yet robust in understanding complex problem-solving. While the paper highlights the challenges in evaluation, it does not propose improvements or solutions to enhance LLM judging capabilities, which could limit its impact on advancing this technology. The dataset's utility will largely depend on how it is adopted and expanded upon by the research community.", "Questions": "How does the use of LLM as a judge compare to traditional evaluation methods by human experts in terms of reliability and accuracy? Are there plans to expand on this dataset with even more diverse problem sets and multimodal elements? What criteria were used to select the six core subjects for U-MATH, and do these subjects largely represent university-level mathematics curricula across different institutions?"}}
{"paper_id": "AjXkRZIvjB", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces GSM-Symbolic, a novel benchmark to evaluate LLMs' mathematical reasoning more rigorously. It provides insightful analysis into the limitations of LLMs, demonstrating significant performance variations and fragility in reasoning capabilities when faced with altered numerical values or additional clauses.", "Weaknesses": "The presentation could be improved for clarity, particularly in explaining how the GSM-Symbolic benchmark differs from GSM8K and why these differences are significant. More details on the methodology and implementation of the benchmark would strengthen the paper.", "Questions": "How does GSM-Symbolic specifically address the limitations of the GSM8K benchmark? Are there plans to extend GSM-Symbolic to evaluate reasoning in other domains beyond mathematics?"}}
{"paper_id": "IJiTI0fB0e", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper tackles an important problem of quantifying diversity in text-conditioned generative models by distinguishing between prompt-induced and model-induced diversity.\nThe use of matrix-based information measures and the decomposition of existing metrics into Conditional-Vendi and Information-Vendi scores is a rigorous approach.\nTheoretical results provide a strong foundation and interpretation for the proposed diversity quantification method.", "Weaknesses": "While the theoretical framework is solid, the practical applicability and implementation details may not be fully demonstrated.\nThe paper might benefit from more comprehensive experimental validation across different model architectures and datasets.", "Questions": "How sensitive is the proposed method to different types of generative models or variations in the text prompts?\nWould there be specific scenarios where the proposed scores might not align with intuitive perceptions of diversity?"}}
{"paper_id": "fs2Z2z3GRx", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel approach called Flow with Interpolant Guidance (FIG) for solving linear inverse problems in image reconstruction.\n2. The approach is theoretically justified and experimentally shows strong performance, particularly in challenging scenarios with high noise or ill-posedness.\n3. The paper provides empirical evidence that FIG improves upon state-of-the-art methods.", "Weaknesses": "1. The paper does not provide details on the theoretical derivations or assumptions that justify the use of measurement interpolants in reverse-time sampling.\n2. There is limited discussion on the computational cost or resource requirements of the proposed method compared to existing methods.\n3. While code release is promised, it is not clear if the code has been tested in a broad range of settings beyond what is mentioned in the paper.", "Questions": "1. Can the authors provide more details on the theoretical justification for the interpolant guidance used in FIG? \n2. Are there any specific limitations or cases where FIG does not perform well compared to other methods?\n3. How does FIG handle variations in measurement noise, and is there any parameter tuning required?"}}
{"paper_id": "H4FSx06FCZ", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses a notable gap in existing 3D NeRF and GS steganography methods by introducing a novel framework, SecureGS, designed for better security and efficiency. It leverages a hybrid decoupled Gaussian encryption mechanism and proposes innovative security measures such as density region-aware anchor growing. Extensive experiments are provided to demonstrate the superiority of SecureGS over existing methods in aspects like rendering fidelity, speed, and security.", "Weaknesses": "While the proposed method appears innovative, the paper lacks a detailed discussion on potential limitations or biases inherent to the SecureGS framework. Additionally, it might benefit from a wider variety of use cases and comparisons with more state-of-the-art models within the steganography domain. The scalability of the method to large-scale scenes is unclear.", "Questions": "1. How does SecureGS perform when scaled to extremely large and detailed 3D scenes? Are there computational constraints? \n2. Can the proposed method be adapted or extended to work with other forms of 3D representations beyond 3D Gaussian splats?\n3. What are the specific challenges faced in the proposed decoupled Gaussian encryption mechanism compared to traditional methods?"}}
{"paper_id": "OGtUfA6Amo", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper proposes a novel hierarchical patch graph network, Hi-Patch, that effectively captures multi-scale information for irregular multivariate time series through the use of interconnected graph layers. The approach is well-motivated, and the method outperforms existing state-of-the-art models across multiple datasets, proving its efficacy in both forecasting and classification tasks. The inclusion of code in an open repository is also a strong point, promoting transparency and reproducibility.", "Weaknesses": "The paper could provide more insights into the potential computational costs associated with the hierarchical patch graph network, especially considering the multi-scale analysis, which might increase the model's complexity. The scalability of the approach when applied to larger datasets or more complex IMTS could also be discussed further.", "Questions": "How does the Hi-Patch network handle missing data within the IMTS datasets, especially considering irregular sampling rates? Are there any specific mechanisms in place to deal with such cases within the framework, or are these preprocessed?"}}
{"paper_id": "ujpAYpFDEA", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses an important gap in the current understanding of watermarking techniques in LLMs by focusing on their imperceptibility. The introduction of the Water-Probe algorithm provides a novel approach to detecting watermarks, and the proposed Water-Bag strategy seems to offer a significant improvement in making watermarks less detectable. The experiments effectively demonstrate the vulnerabilities of existing watermark techniques, highlighting the relevance and timeliness of this research.", "Weaknesses": "The paper could provide more detailed information on how the Water-Probe algorithm was developed and validated, which would help in assessing its generalizability and applicability to various LLMs. While addressing imperceptibility, it might not comprehensively consider how the proposed methods impact the overall system performance, such as execution time and resource usage.", "Questions": "1. How does the Water-Probe algorithm perform across various LLM architectures, and is its effectiveness consistent across different scales of models?\n2. Can the Water-Bag strategy be integrated with existing watermarking solutions without significant modifications, or does it require substantial changes to existing systems?\n3. Were there any benchmark tasks used to assess the impact of these watermarking strategies on the performance of LLMs beyond imperceptibility?"}}
{"paper_id": "7UKHNQIErp", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper provides a novel formalism for characterizing preference losses, offering a deeper understanding of different DPA loss functions. By identifying symbolic forms for common DPA variants, the work facilitates a systematic exploration of the DPA loss landscape, potentially advancing human AI alignment. The framework is well-articulated and could guide future research in preference learning.", "Weaknesses": "While the paper offers a comprehensive framework, it may be challenging for practitioners to immediately apply these theoretical insights without substantial expertise in symbolic reasoning and formal methods. The paper might benefit from concrete examples or demonstrations on real-world datasets to illustrate the practical utility of the proposed framework.", "Questions": "How does the proposed framework compare to other formal approaches in terms of computational complexity and applicability to large-scale problems? Can the authors provide specific examples or case studies where the new loss functions derived using their framework have led to improvements in practice?"}}
{"paper_id": "CpgWRFqxhD", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. MEMO addresses critical challenges in audio-driven talking video generation by introducing innovative memory-guided and emotion-aware modules, enhancing identity consistency, expression alignment, and audio synchronization.\n2. The use of a large-scale dataset without relying on traditional facial inductive biases showcases the robustness and flexibility of the approach.\n3. The extensive experiments and human evaluations provide strong evidence of improved performance over state-of-the-art methods.", "Weaknesses": "1. While the method is promising, the details on managing computational costs for memory and multi-modal attention modules in large-scale deployments are not extensively covered.\n2. The paper could provide more specific insights into the limitations and potential failure modes of MEMO in diverse scenarios or out-of-distribution samples.", "Questions": "1. Can the memory-guided module handle very long sequences effectively, and what are its limitations in terms of sequence length?\n2. How does MEMO perform in real-time applications, and what optimizations are necessary to achieve satisfactory performance?\n3. Are there specific audio types or environmental conditions where MEMO might underperform compared to existing methods?"}}
