{"paper_id": "B0OwtVEejJ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The proposed TangleNAS method effectively combines the strengths of gradient-based and weight-entangled NAS paradigms, providing a novel approach to optimize more complex and larger search spaces with reduced memory requirements. The method is well-validated through comprehensive experiments across various search spaces and demonstrates consistent improvements in both any-time performance and accuracy, indicating its robustness and general applicability.", "Weaknesses": "The integration of gradient-based NAS methods within weight-entangled spaces, while innovative, may face challenges in terms of scalability for increasingly large and complex architectures. Additionally, the method relies on zero-padding for weight-sharing, which might introduce some inefficiencies or inaccuracies in certain cases.", "Questions": "How does TangleNAS handle extremely large search spaces where zero-padding might significantly increase computation? Are there any situations where the two-stage methods might still be preferable over TangleNAS? How can the balance between maintaining performance and reducing memory usage be further optimized?"}}
{"paper_id": "ZlEtXIxl3q", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces an innovative application of contrastive losses to address the challenges posed by global epistasis in learning fitness functions. This approach is shown to be both data-efficient and robust against noise, which is critical in practical applications for protein sequencing. The use of both simulated and real-world datasets strengthens the validity of the claims.", "Weaknesses": "While the application of contrastive loss is novel and valuable, the presentation of results could be improved for clarity. The paper could benefit from additional details on how the approach compares with other contemporary methods beyond MSE, to further contextualize its advantages.", "Questions": "How does the proposed method scale with larger biological datasets? Are there any limitations on the types of nonlinearities that the contrastive loss can effectively handle?"}}
{"paper_id": "6yJuDK1DsK", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. FEATHER introduces a novel lightweight adapter approach for test-time adaptation, significantly reducing the need for parameter adaptation while maintaining or improving performance.\n2. The method eliminates the requirement for source data during test-time adaptation, which is an important advantage in practical scenarios.\n3. The experimental validation on diverse and challenging datasets demonstrates the method's efficacy and robustness under dynamic conditions.\n4. FEATHER's design is particularly suited for deployment on edge devices due to its computational efficiency.", "Weaknesses": "1. Although the approach is efficient, the paper lacks clarity in explaining the technical details, particularly regarding the specific configurations of the adapters and how they are initialized.\n2. The presentation of results could benefit from more comprehensive discussions of the implications of findings and comparisons with relevant baselines beyond quantitative metrics.\n3. There is a potential over-reliance on specific datasets, and the generalizability of results to other types of distributions or domains is not fully explored.", "Questions": "1. How does FEATHER handle scenarios where the test-time distribution shift is significantly larger than what is typically encountered in the benchmark datasets?\n2. Can the adapter configuration in FEATHER be optimized further, or are there inherent limitations in parameter reduction?\n3. Are there any specific types of distribution changes where FEATHER's approach might struggle, due to either technical or theoretical limitations?"}}
{"paper_id": "lt6xKGGWov", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces MINERVA, a feature selection filter leveraging the Mutual Information Neural Estimator (MINE) to effectively capture complex, non-linear dependencies between features and targets. The method shows improved accuracy in synthetic benchmarks, indicating its ability to handle sophisticated feature-target relationships exceptionally well.", "Weaknesses": "While MINERVA demonstrates improved accuracy in synthetic datasets, the testing might not cover real-world scenarios with complex noise and multiple confounding variables. The presentation could benefit from more clarity and detail in methodology and experiment results to fully assess the method's limitations.", "Questions": "How does MINERVA perform on real-world datasets compared to synthetic ones, especially in the presence of noise and confounding factors?\nAre there any limitations in terms of the computational cost or scalability of the proposed method when applied to larger datasets?\nHow does the method ensure robustness when estimating mutual information for high-dimensional feature subsets, which may introduce more variability?"}}
{"paper_id": "Bvrc6kobWd", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper provides a novel perspective on contrastive learning by delving into the gradient-level analysis of margins, offering a new understanding of how margins affect representation learning. The experimental evaluation across multiple datasets like CIFAR-10, CIFAR-100, STL-10, and TinyImageNet provides a strong basis for the claims.", "Weaknesses": "The explanation of the results could benefit from more clarity, especially regarding the specific impact of margin adjustments on gradient modification. The connection to practical applications and potential improvements of contrastive learning frameworks could be more explicitly articulated.", "Questions": "1. How does the proposed gradient analysis of margins compare to existing decision-boundary-based approaches quantitatively?\n2. Are there any specific settings or domains where this gradient-level understanding of margins has shown limitations or less effectiveness?\n3. Can this gradient-based perspective be extended to other self-supervised learning frameworks beyond contrastive learning?"}}
{"paper_id": "FJjHQS2DyE", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The proposed CASA framework addresses a crucial aspect of domain adaptation under label shift, which is often overlooked in existing methodologies.\n2. The theoretical grounding provided for the target risk bound enhances the credibility of the approach.\n3. Empirical evaluation across multiple benchmark datasets demonstrates the efficacy of CASA in handling label distribution shifts effectively.", "Weaknesses": "1. While theoretically sound, the practical implementation details of CASA might be complex, particularly in terms of aligning conditional feature supports, which could limit its ease of application.\n2. The presentation could be improved for better clarity, especially in the theoretical sections, which may be difficult for practitioners unfamiliar with the underlying mathematical concepts.", "Questions": "1. How does CASA handle cases where pseudo-labeling significantly misclassifies target samples, particularly in highly imbalanced label shift scenarios?\n2. Can the CASA framework be adapted or extended to handle dynamic environments where label distributions change over time or online UDA settings? If so, what would be the main challenges?"}}
{"paper_id": "xelrLobW0n", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. The paper addresses a significant gap in the evaluation of LLMs for continual learning by introducing the TRACE benchmark and providing a thorough empirical evaluation.\n2. The RCL approach is well-motivated and shows promise in mitigating catastrophic forgetting in LLMs.\n3. Introduction of new metrics such as 'General Ability Delta,' 'Instruction Following Delta,' and 'Safety Delta' for more comprehensive evaluation is innovative and provides useful insights.", "Weaknesses": "1. While promising, the RCL method may require more extensive validation across a broader range of tasks and models to establish its general effectiveness.\n2. The TRACE benchmark, although diverse, might still miss some real-world scenarios or tasks that LLMs encounter.\n3. The reliance on GPT-4 for reasoning annotation may introduce biases inherent to that model, which should be addressed or quantified.", "Questions": "1. How does the RCL approach compare to other advanced continual learning strategies not discussed in the paper?\n2. Are there any specific domains or types of tasks where TRACE and RCL show limitations in evaluation performance?\n3. What are the potential biases introduced by using GPT-4 for reasoning path annotations, and how does the paper address these?"}}
{"paper_id": "bYQkOPvgDw", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The PRGNN framework proposes a novel application of probabilistic graphical models to address label noise in Graph Neural Networks, moving away from the common label smoothness assumption. This makes it applicable to both homophilous and heterophilous graphs, which are not adequately addressed by existing methods. The use of variational inference and contrastive loss for leveraging unlabeled data is innovative and shows robustness across various noise types and rates.", "Weaknesses": "While the method is theoretically sound, the presentation could be improved, particularly in terms of clarity and the detailed description of the methodology. More comprehensive experimental comparisons with a wider variety of existing methods would strengthen the results. The framework's reliance on several components (e.g., three GNNs) could be complex and computationally expensive.", "Questions": "How does the computational complexity of PRGNN compare to traditional GNNs, and is there any trade-off in terms of scalability for larger graphs? Can the proposed method be adapted to other types of noise or datasets, and what modifications would be necessary to apply it to non-graph structured data?"}}
{"paper_id": "sKPzAXoylB", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses a significant gap in continual learning by proposing a method that addresses both catastrophic forgetting and loss of plasticity simultaneously. The use of utility-based perturbed gradient descent offers a novel approach to managing the importance of neural network units. The method is tested on a variety of datasets, including MNIST, CIFAR-10, and ImageNet, demonstrating its applicability across multiple domains. Additionally, the extension to reinforcement learning tasks highlights its versatility. The evaluation showcases improved performance over traditional methods, particularly in non-stationary environments.", "Weaknesses": "While the method is innovative, the presentation of the technique and its implementation details could be clearer, particularly in how utility is specifically calculated and applied to the gradient updates. The experiments, while comprehensive, might not fully capture all potential edge cases in streaming learning environments. The paper could benefit from more detailed comparisons with closely related work and a clearer discussion on computational complexity and scalability of the proposed method.", "Questions": "1. How is the utility of each weight specifically calculated, and how does this computation scale with larger, more complex networks?\n2. Can UPGD be integrated with other continual learning strategies, such as memory replay, or does it function optimally only in isolation?\n3. Beyond the datasets tested, how would the method perform in domains like natural language processing where tasks and data streams can be highly non-stationary?\n4. Is there a way to quantify the trade-off between plasticity and protection against forgetting in terms of performance improvement?"}}
{"paper_id": "H8Qg1IIMaR", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses a critical vulnerability in large language models, shedding light on their susceptibility to simple adversarial attacks in multiple-choice question answering tasks. It presents a comprehensive experimental evaluation across multiple datasets and models, offering valuable insights into the robustness of these systems.", "Weaknesses": "The paper highlights a major issue but does not propose effective mitigation strategies, leaving a gap in addressing how these vulnerabilities could be resolved. The current solutions evaluated, such as majority voting and calibration, are found ineffective, raising concerns about practical applicability.", "Questions": "What are the potential avenues for future research or alternative strategies that could be investigated to mitigate the identified vulnerabilities in LLMs and VLLMs against such permutation attacks?"}}
{"paper_id": "tth2qXY7RU", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces the Super Floating-Point (SuFP) quantization method, which shows significant improvements in computational capability and energy efficiency without compromising accuracy. The method's innovative approach of multi-region piecewise quantization with tensor-wise scalable bias is particularly useful for maintaining performance across various domains. The hardware implementation demonstrates the practical applicability of the approach, showing a substantial reduction in memory footprint and enhancement in throughput.", "Weaknesses": "The presentation could be improved with more detailed explanations of the underlying concepts, particularly the specifics of how the multi-region piecewise quantization and tensor-wise scalable bias are implemented. Additionally, the paper would benefit from a more comprehensive comparison with a wider range of existing quantization methods to further substantiate its claims.", "Questions": "How does the SuFP method deal with extreme outliers in data distributions, and what are the trade-offs in bit precision across different regions? Could the authors provide more clarity on the scalability of SuFP to even larger models and its impact on training time?"}}
{"paper_id": "fjf3YenThE", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a significant advancement in zeroth-order optimization by addressing the limitations of existing algorithms concerning hard-thresholding and random directions. The variance-reduction approach not only theoretically supports improved convergence but also enhances practical applicability across varied high-dimensional tasks, such as adversarial attacks and portfolio optimization. The method's implementation demonstrates competitive performance compared to existing solutions.", "Weaknesses": "While the theoretical analysis is strong, the paper could benefit from more extensive experimental validation across a broader range of real-world datasets and applications beyond those mentioned. The presentation could be clearer in sections explaining the core algorithm details, especially for readers less familiar with complex mathematical notations.", "Questions": "1. How sensitive is the proposed algorithm's performance to the tuning of hyperparameters compared to SZOHT and other baseline methods?\n2. Can this variance-reduction technique be seamlessly integrated or adapted for other optimization paradigms outside high-dimensional settings?\n3. What are the computational overheads of utilizing historical gradients in extremely high-dimensional spaces, and how does this scale with increasing problem size?"}}
{"paper_id": "H9DYMIpz9c", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces an innovative approach, FARZI, for data distillation specifically tailored to autoregressive tasks, addressing the unique challenges posed by such tasks. The method demonstrates remarkable efficiency in reducing dataset size while maintaining or even improving model performance. This is particularly beneficial for the training of large models, potentially reducing computational and environmental costs significantly. The use of reverse-mode differentiation and Hessian-Vector Products is a novel aspect that contributes to the overall efficiency of the method.", "Weaknesses": "The paper might lack detailed clarity in some areas regarding the explanation of FARZI's components and the specific implementation details. While the results are promising, it's important to consider the extent of the generalizability of FARZI across different datasets and domains, which may not have been thoroughly covered. Further empirical validation could strengthen the claims.", "Questions": "1. Can FARZI be effectively generalized to other types of sequences beyond language modeling and sequential recommendation?\n2. How does the latent space factorization specifically contribute to regularization within the context of FARZI, and are there specific design choices that were critical in achieving the reported performance?\n3. Are there any specific limitations or scenarios where FARZI might not perform optimally, which were observed during the experiments?"}}
{"paper_id": "4kLVvIh8cp", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The PNLSVI algorithm is a novel approach in the offline RL space featuring non-linear function approximation with instance-dependent regret guarantees, which is an unaddressed area until now. It advances the field by proposing a statistically optimal method that works efficiently with a complex function class context. Its empirical validation and theoretical underpinnings provide significant contributions to developing RL algorithms that remain efficient in larger state-action spaces.", "Weaknesses": "The presentation could be improved to make complex ideas more accessible to a broader audience, possibly by including additional clarifications or examples. Additionally, while the theoretical contributions are strong, real-world benchmarks or applications demonstrating the algorithm's effectiveness in diverse environments would strengthen the paper.", "Questions": "What is the algorithm's empirical performance in terms of scalability in real-world applications? Are there any computational limitations when dealing with very large state-action spaces due to the computations like the D2-divergence? How does the choice of the particular regression and bonus oracles affect the algorithm's performance?"}}
{"paper_id": "CSpWgKo0ID", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper makes a significant contribution by integrating concepts from behavioral game theory into the study of LLMs, addressing how these models operate in social interaction contexts involving repeated interactions. The experimental design, using a broad set of game scenarios and robust checks, provides compelling insights into LLMs' strategic behavior. This could inform future development and application of LLMs in social settings.", "Weaknesses": "While the study offers valuable insights, it primarily focuses on a specific set of repeated games and lacks exploration of diverse strategic environments or deeper analyses into the decision-making processes of LLMs in complex scenarios. Additionally, the paper doesn't fully explore the implications of its findings for real-world applications, making the leap from games to practical use unclear.", "Questions": "How do the findings of this paper translate into potential improvements in LLM design for real-world applications? Are there other game scenarios or setups where similar behavior could be expected from LLMs? Do the authors have any insights on architectural changes or updates needed in LLMs to enhance coordination abilities?"}}
{"paper_id": "gwbQ2YwLhD", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper addresses an important problem in structure learning by analyzing the influence of data scale on the performance of structure learning algorithms.\n2. Theoretical proofs and experimental validations provide a strong foundation for the claims made by the authors, showcasing a comprehensive analysis.\n3. The study provides valuable insights into the behavior of different loss functions, contributing to better understanding and potential improvements in the field of structure learning.", "Weaknesses": "1. While the paper provides theoretical insights, the practical implications and solutions for addressing the identified issues could be more thoroughly explored.\n2. The presentation could benefit from clearer explanations of the existing approaches and how they are impacted by scale, particularly in non-linear settings.\n3. The experiments, although supportive, may not cover a wide enough range of real-world scenarios and datasets to fully establish the general applicability of the findings.", "Questions": "1. Can you provide further clarification on how practitioners can apply your findings? Are there specific normalization techniques you recommend?\n2. How do the scale issues identified in high-dimensional settings compare to those in low-dimensional settings? Are there unique challenges or findings specific to high-dimensional cases?\n3. Were there any specific limitations or challenges in the experiments that might have impacted the results, particularly in real-world datasets?"}}
{"paper_id": "eIYDKNqXuV", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The proposed Village-Net Clustering algorithm offers a novel combination of K-Means and a community detection method for improved clustering of complex datasets. The approach is computationally efficient, automatically determines the number of clusters, and shows scalability advantages over traditional clustering methods.", "Weaknesses": "The initial reliance on K-Means could limit accuracy on non-convex datasets, as the method might inherit its sensitivity to initialization and assumption of spherical clusters. While scalability and efficiency are strong points, the paper could provide more extensive comparisons or ablations against other clustering methods regarding accuracy improvements.", "Questions": "Can the accuracy limitations of the initial K-Means clustering step be mitigated by iterative refinement techniques? How does the Village-Net approach handle datasets with very high dimensionality, and is the cluster quality maintained in such scenarios?"}}
{"paper_id": "bAMPOUF227", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel framework, SuperContext, effectively combining SLMs with LLMs to enhance out-of-distribution task performance and reduce hallucinations. Extensive evaluation across various tasks demonstrates significant improvement over baseline methods. The integration of task-specific information shows promise in enhancing generalization.", "Weaknesses": "The approach assumes the availability of well-curated task-specific SLMs which might be limited for some applications. The computational complexity related to integrating multiple models during inference could pose challenges in real-time applications. The paper lacks detailed analysis on the possible drawbacks of mixing outputs from different models.", "Questions": "1. How scalable is the SuperContext framework when applied to very large datasets or real-time applications?\n2. Are there specific tasks where the integration does not prove beneficial? If so, what are the common characteristics of those tasks?\n3. How does the framework handle conflicting outputs from SLMs and LLMs during integration?"}}
{"paper_id": "eNoiRal5xi", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces a novel idea of combining parameter space and data space perturbations to improve domain generalization. The approach is grounded in theoretical insights from PAC-Bayesian theory and is evaluated on several benchmarks, showing statistically significant improvements.", "Weaknesses": "The paper does not provide detailed theoretical guarantees for the generalization to unknown domains, which could strengthen the proposed method. Some aspects of the data space perturbation process might require more detailed explanation or justification. Additionally, the presentation of the method could be improved for clarity.", "Questions": "How does UDIM handle scenarios where the unknown domains have significant dissimilarities compared to the source domain? Are there specific types of data or domain shifts where UDIM performs particularly well or struggles? Can more insights be provided into the process of simulating unknown domains through data space perturbation?"}}
{"paper_id": "uU0Adp7Sfo", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 3, "Strengths": "The paper introduces a novel framework (CCGAN) that successfully integrates both competitive and collaborative aspects within GANs, leading to more stable and high-quality data generation. The theoretical foundation is robust, ensuring equilibrium and reducing training collapse risks. Empirical results are strong across multiple datasets, demonstrating the model's practical effectiveness.", "Weaknesses": "While the theoretical aspect is well-developed, the explanation of the collaborative-competitive mechanism could be clearer and more detailed, especially in how it fundamentally changes the training dynamics. Additionally, insights into the choice and derivation of the regularization term would enhance the understanding of its impact.", "Questions": "1. How does the novel regularization term specifically break the mutual dependency between the generator and discriminator, and is this universally applicable across different data types?\n2. Were there any specific challenges or limitations identified during the empirical testing that suggest future directions for improvement?\n3. Can the CCGAN framework be easily extended to other types of GANs, such as CycleGAN or StyleGAN, without significant modifications?"}}
{"paper_id": "dKju7tbe6D", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel integration of iterative and parallel computation strategies within a single architecture, leading to improved efficiency and generalization in visual reasoning tasks. The evaluation on multiple benchmarks demonstrates strong performance, particularly in zero-shot learning and transfer learning scenarios. The proposed method shows a well-balanced capacity to handle mutually independent operations while maintaining step-by-step processing.", "Weaknesses": "The scalability of the IPRM approach to more complex datasets and tasks remains to be fully explored. Additionally, while the model claims to use fewer parameters, the complexity of Operation Formation and Execution may lead to practical implementation challenges. The paper relies on a specific set of benchmarks that may not cover all aspects of visual reasoning.", "Questions": "How does the IPRM handle dependencies that are not immediately recognizable in the initial steps? Are there datasets where the benefits of parallel and iterative processing are not as significant? Additionally, can the method be applied to other domains beyond visual reasoning, such as natural language processing, where similar reasoning strategies may be beneficial?"}}
{"paper_id": "DL7JWbdGr3", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The approach leverages large, heterogeneous datasets to improve epidemic forecasting, showcasing the utility of pre-training methods in this domain.\n2. The model demonstrates significant improvements in prediction accuracy and efficiency over existing state-of-the-art models.\n3. It highlights the adaptability and applicability of transfer learning techniques in epidemic prediction tasks.", "Weaknesses": "1. The paper may face challenges in ensuring the consistency and quality of heterogeneous historical data, which can affect the pre-training process.\n2. The robustness of the approach in scenarios with vastly differing epidemic characteristics is not thoroughly discussed.\n3. Specific details about the implementation and computational costs of the pre-training process could be expanded.", "Questions": "1. How does the model handle differences in data quality across diverse datasets during pre-training?\n2. Are there any limitations identified regarding the model's performance across different classes of epidemics not included in the training datasets?\n3. Could the pre-training framework be extended beyond epidemic forecasting to other areas in public health?"}}
{"paper_id": "1PaDPHDhwe", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The proposed class-specific scaling method provides a novel approach to balancing robust and average accuracy without additional training. The introduction of a new evaluation metric, robust coverage, offers a comprehensive understanding of debiasing algorithms.", "Weaknesses": "The paper does not fully explore the limitations or potential downsides of the class-specific scaling method across different datasets. It lacks extensive experimental validation and comparisons with a broader range of existing methods.", "Questions": "How does the proposed class-specific scaling method perform across different domains and datasets? Are there any limitations in terms of scalability or applicability to other types of machine learning models?"}}
{"paper_id": "iI7hZSczxE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper addresses a significant challenge in disentangled representation learning for time series data: the assumption of attribute independence. By proposing DIOSC, a method that relaxes this assumption via contrastive learning, the authors provide a novel approach for achieving disentangled representations in the presence of correlated attributes. The introduction of the Time Disentangling Score (TDS) as a new metric to evaluate disentanglement quality is a valuable contribution. The method demonstrates substantial improvements in generalization performance on downstream tasks like NILM, highlighting its practical relevance.", "Weaknesses": "The paper relies heavily on the novel assumption of independence-of-support, which may need further theoretical exploration to fully validate its efficacy. The implementation details concerning the contrastive learning aspects could be more thoroughly described to allow for replication. Additionally, while the TDS is proposed as a new metric, its effectiveness and robustness compared to existing metrics in different settings should be further explored.", "Questions": "1. How does DIOSC's contrastive learning approach handle varying levels of correlation between different attributes in time series data?\n2. How sensitive is the Time Disentangling Score (TDS) to the choice of parameters within DIOSC?\n3. Can the authors provide more insight into the computational overhead introduced by the self-attention and contrastive learning components compared to standard methods?"}}
{"paper_id": "qrGjFJVl3m", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "The paper presents the Qwen-VL series, which significantly enhances the capabilities of LVLMs in visual-language tasks. The meticulous design of the model architecture, including a visual receptor and a three-stage training pipeline, allows the series to achieve superior performance in fine-grained visual understanding and multilingual support. The comprehensive evaluation and new benchmarks set demonstrate the model's effectiveness in various real-world applications.", "Weaknesses": "While the models achieve superior performance, the paper does not discuss the potential computational resource demands or efficiency aspects related to training and deploying such extensive model architectures. Additionally, the approach to handling challenges like maintaining alignment between visual and linguistic modalities across multilingual contexts could be elaborated further.", "Questions": "1. What are the computational resource requirements for training the Qwen-VL series, and how do they compare to those of existing benchmark models?\n2. How does the model architecture ensure effective alignment between visual and linguistic modalities in the context of multilingual data?\n3. Are there plans to make the dataset or training framework used for the Qwen-VL series publicly available to facilitate further research?"}}
{"paper_id": "qup9xD8mW4", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 3, "Strengths": "1. The paper introduces HaDES, a novel approach to behavior distillation, leveraging meta-evolutionary strategies to synthesize minimal datasets for reinforcement learning tasks.\n2. The authors effectively address a significant limitation in transferring dataset distillation to reinforcement learning by proposing a method independent of a fixed dataset, which expands the applicability of distillation techniques.\n3. Comprehensive experiments across various environments, including continuous and discrete tasks, demonstrate the generality and effectiveness of HaDES, positioning it as a versatile tool in both reinforcement learning and supervised tasks.", "Weaknesses": "1. While the paper proposes an innovative approach, the dependence on a specific evolutionary strategy could limit generalization to other strategies not explored in the paper.\n2. The method's reliance on synthetic datasets raises questions about the interpretability and transparency of the distilled state-action pairs, particularly their real-world applicability and insight into the underlying RL tasks.\n3. Further details on comparative performance against a broader range of baseline methods would provide a clearer perspective on the competitive nature of HaDES beyond traditional dataset distillation.", "Questions": "1. How does HaDES perform when compared with other RL-specific distillation techniques or state-of-the-art RL algorithms beyond the tested environments?\n2. Can the approach be extended or modified to incorporate real-world complexities, such as when facing partial observability or non-stationary environments?\n3. What are the potential limitations in using HaDES synthesized datasets in policy training, especially regarding exploration and sample efficiency beyond the presented tasks?"}}
{"paper_id": "Zw8YxUWL4R", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel approach with the P+ conditioning space that allows for more control and expressiveness in text-to-image synthesis. The introduction of Extended Textual Inversion (XTI) is an innovative method that improves upon existing frameworks by enabling faster convergence and better reconstruction quality. The experimental results are well-presented, and the paper provides clear insights into the potential applications of its method.", "Weaknesses": "The paper could have included more diverse datasets in its experiments to better generalize the findings. Additionally, while the concept of layer-specific conditioning tokens is compelling, the paper does not deeply explore potential computational limitations or challenges in training stability that might arise from this increased complexity.", "Questions": "How does the proposed P+ space and XTI approach handle different types of textual inputs, particularly those with high semantic complexity or ambiguity? Are there any limitations in terms of the types or lengths of text prompts that can be effectively used with this model?"}}
{"paper_id": "3NXhwkZGjz", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The proposed method effectively addresses the challenges posed by Source-Free Unsupervised Domain Adaptation (SFUDA) by innovatively using multiple prediction hypotheses and rationale analysis to improve pseudo-label reliability, enhancing model adaptation without access to source data. The approach is flexible and can be integrated into existing UDA methods, thereby broadening its applicability and impact.", "Weaknesses": "The method relies on the assumption that rationale representations can reliably indicate the correctness of hypotheses, which may not hold in all scenarios. The complexity of constructing rationale centroids and managing multiple hypotheses might introduce computational overhead, which is not extensively discussed. Scalability to large datasets and different domain shifts might also be a concern.", "Questions": "How does the complexity of managing multiple hypotheses affect the computational efficiency of the model? Are there any specific scenarios where the rationale-based method does not perform as expected? Can the method be effectively scaled to larger datasets with more complex domain shifts?"}}
{"paper_id": "Fn655mJ4bv", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel and effective method for interpreting structured output models by utilizing correlations among output variables. The proposed SOInter method improves explanation quality over existing techniques like Lime and Kernel-Shap and maintains computational efficiency, making it suitable for large-scale applications.", "Weaknesses": "The presentation could be improved, particularly regarding the clarity of how the method is implemented and evaluated. While the method is demonstrated on both synthetic and real datasets, more details on the datasets and experimental setup would enhance the understanding and implications of the results.", "Questions": "What specific datasets were used for evaluating the method, and how were they chosen? Are there any specific limitations in the types of structured output models or datasets where SOInter might not perform well? How does the two-phase training process balance between pre-training and fine-tuning to ensure optimal interpreter performance?"}}
{"paper_id": "VyMW4YZfw7", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper proposes a simplified approach to semi-supervised node classification (SSNC) using nonparametric methods, which are less complex than existing GNN models, yet achieve competitive performance. This simplicity allows for cleaner theoretical analysis and provides insights into the potential over-complexity of recent GNN models. The experiments on common benchmarks like Cora, Citeseer, and Pubmed demonstrate the effectiveness of these simpler models.", "Weaknesses": "While the paper highlights an interesting direction by harnessing established nonparametric methods, it may not provide as substantial a methodological innovation compared to more complex GNN models. Additionally, the focus on the spectral domain might limit the general applicability of the method without further extensions or adaptations to non-spectral graph data.", "Questions": "1. How will this simplified approach handle larger and more complex graphs outside of common SSNC benchmarks? 2. Could the authors provide more detailed explanations or evidence of how much the evaluation conventions, such as dataset splits, have impacted previous model performance evaluations?"}}
{"paper_id": "09xFexjhqE", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The introduction of a low-rank branch (AutoLoRa) for separating natural and adversarial objective optimizations in RFT is innovative and addresses key challenges in adversarial fine-tuning, resulting in improved stability and robustness. The automatic scheduling strategy for learning rates and loss term scalars further enhances convergence and reduces sensitivity to hyperparameter tuning, providing practical advantages.", "Weaknesses": "The paper could benefit from more detailed comparison with direct baselines to highlight specific advantages and limitations of the proposed method. Additionally, while the low-rank branching is novel, more insights into its potential limitations or scenarios where it underperforms relative to other methods could enrich the discussion.", "Questions": "How does the proposed AutoLoRa method perform across different architectures beyond ResNet models? Are there any specific types of tasks or data where the AutoLoRa approach is less effective or requires further modifications? What are the potential trade-offs in terms of computational efficiency or complexity when implementing the low-rank branch method?"}}
{"paper_id": "nmBjBZoySX", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The proposed AdaGLT introduces innovative concepts like layer-adaptive sparsification and integrates pruning with training, which significantly enhances the flexibility and applicability of GNNs in deeper networks.\n2. Theoretical proofs and extensive experimental evaluations underline the effectiveness of the approach in mitigating over-smoothing and achieving superior performance compared to state-of-the-art methods.\n3. The method's ability to automate pruning scheduling reduces the need for manual intervention, making it more user-friendly and efficient for large-scale implementations.", "Weaknesses": "1. While the methodology is well-articulated, the implications of the theoretical proofs might not be fully understandable to audiences without a deep mathematical background.\n2. The generalizability of the approach across extremely large real-world graphs that may exceed the tested datasets is not fully established in the results presented.\n3. No explicit mention of potential limitations or challenges encountered when applying AdaGLT to different types of graphs, such as dynamic or evolving graphs.", "Questions": "1. How does AdaGLT handle varying types of graph dynamics, such as graphs that change over time or evolve continually in real-time applications?\n2. Is there an impact on the performance of AdaGLT when applied to heterogeneous graphs where node or edge types might vary significantly?\n3. Can the approach be extended or adapted to work with other neural network architectures outside the GNN domain, such as RNNs or CNNs?"}}
{"paper_id": "di52zR8xgf", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a meaningful enhancement to the Stable Diffusion model with a novel approach that includes a larger UNet backbone, multiple attention blocks, and a second text encoder, resulting in improved performance.\n2. The introduction of innovative conditioning schemes and a refinement model enhances image quality and text adherence, offering a competitive alternative to black-box models.\n3. The open model approach facilitates greater transparency, reproducibility, and potential for community-driven improvements.", "Weaknesses": "1. The paper, while presenting significant improvements, does not provide detailed comparisons with a broader range of state-of-the-art models beyond Midjourney, which could provide further validation of its competitive edge.\n2. While offering improvements, the model still requires future work in areas like single-stage generation and further text synthesis enhancements, indicating that it may not yet be a complete solution.", "Questions": "1. How does the performance of SDXL compare across different image resolutions and aspect ratios? Are there any limitations in specific scenarios?\n2. What are the specific quantitative metrics used to evaluate the improvements in SDXL over the previous versions and compared to black-box models?\n3. Can the model's refinement stage be adapted or utilized independently within other generative model frameworks to enhance image fidelity?"}}
{"paper_id": "0b328CMwn1", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces Activation Prompts (AP), a novel method that improves upon Visual Prompting (VP) by enhancing model performance and efficiency. This approach extends the concept of introducing perturbations by applying them at intermediate activation map layers, which is both innovative and effective. The paper's theoretical analysis provides a solid connection between AP and normalization techniques, and the extensive experiments over 29 datasets showcase the robustness and generalizability of the method.", "Weaknesses": "While the approach is compelling, the paper may lack a thorough examination of potential limitations and failure cases of Activation Prompts, especially in diverse model architectures or uncommon datasets. Additionally, the choice of intermediate layers for AP implementation is noted to significantly affect performance, but the paper might not sufficiently guide practitioners on selecting optimal layers.", "Questions": "1. How does AP perform on tasks or datasets significantly different from those in the conducted experiments?\n2. Can the method of choosing appropriate layers for AP implementation be systematically streamlined or automated?\n3. What are the potential downsides or limitations when introducing APs in very deep networks with numerous layers?"}}
{"paper_id": "AMDKqZcZbi", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The model is inspired by a well-documented biological system, making the proposed architecture conceptually sound and innovative.\n2. The approach addresses the dual challenges of rapid learning and retention efficiently, solving the problem of catastrophic forgetting.\n3. The method is well-presented and includes the development of a new evaluation task, the sequential Morris Water Maze (sWM), which contributes to its originality.", "Weaknesses": "1. While the evaluation demonstrates effectiveness, it may be limited by the specificity of the sWM task; thus, the generalizability of the approach across other tasks and environments is not fully explored in the paper.\n2. The paper may lack a thorough comparison with a broader range of cutting-edge continual learning methods.", "Questions": "1. How does the proposed MESH architecture perform on real-world continual learning challenges beyond the conceptual task of the sWM?\n2. What is the computational complexity of the model during both training and inference phases, particularly compared to other state-of-the-art models?\n3. Can the biologically inspired aspects of the model be further adapted to other types of neural architectures or learning paradigms?"}}
{"paper_id": "am7BPV3Cwo", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper effectively addresses the critical challenge of OOD detection in the context of imbalanced datasets, a problem that is prevalent in real-world applications but often overlooked. The introduction of the ImOOD framework provides a novel combination of post-hoc normalization and training-time regularization, showcasing significant improvements over existing methods. The evaluation on standard benchmarks like CIFAR10-LT, CIFAR100-LT, and ImageNet-LT demonstrates the robustness and applicability of the approach.", "Weaknesses": "The presentation could be improved, as some of the technical details regarding the implementation of the post-hoc normalization and training-time regularization might be difficult to follow for practitioners not deeply familiar with the statistical framework. Another potential issue is the reliance on labeled data for the implementation of post-hoc normalization, which might limit the practicality of the method in situations where labeled data is scarce. Some aspects of the methodology could benefit from more thorough experimental validation.", "Questions": "1. How sensitive is the ImOOD framework to variations in the degree of class imbalance? Are there scenarios where the method may not perform as expected?\n2. Does the regularization technique require any specific modifications if applied to architectures significantly different from the ones used in the experiments (such as non-CNN models)?\n3. Can the post-hoc normalization be adapted to unsupervised settings, or does it inherently rely on labeled ID data? If so, how would the framework perform under these conditions?"}}
{"paper_id": "R1crLHQ4kf", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel approach to detecting adversarial attacks on ASR systems by analyzing probability distributions over output tokens. The method is shown to be effective across multiple ASR systems and datasets, demonstrating high accuracy in distinguishing between benign and adversarial inputs. Furthermore, the approach does not rely heavily on computationally expensive data modifications or adversarial training, making it a more efficient solution.", "Weaknesses": "While the method demonstrates high accuracy in detection, the reliance on statistical characteristics of probability distributions may limit its effectiveness against sophisticated adaptive attack strategies that could potentially mask their adversarial nature. Additionally, the paper might lack clarity on the scalability of this method when applied to real-time systems or larger datasets.", "Questions": "How does the method perform in real-time scenarios where computational efficiency is crucial? Are there any specific limitations when applying this approach to more complex, real-world ASR systems? Additionally, how does the detection method handle adversarial examples crafted using advanced techniques specifically designed to evade statistical analysis?"}}
{"paper_id": "i7P2mK3x3o", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces Q-flow, a novel flow-based neural ODE model for directly transporting between arbitrary distributions without relying on a normal distribution intermediary. This approach is innovative in its attempt to minimize transport costs directly, and it has strong empirical performance in high-dimensional data applications like mutual information estimation and image-to-image translation.", "Weaknesses": "The model could be resource-intensive to train, given the continuous-time formulation and the need to minimize both KL divergence and Wasserstein-2 cost, which may not scale well with extremely high-dimensional data. The paper may lack detailed exploration of the limitations or potential edge cases where the approach might not perform well.", "Questions": "How does the model handle extremely high-dimensional datasets, considering computational and memory constraints? Are there specific scenarios or types of distributions where Q-flow might fail or underperform compared to existing methods?"}}
{"paper_id": "YIls9HEa52", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The irSLDS model presents a novel enhancement over traditional rSLDS by incorporating flexible state cardinality and latent geometry, addressing significant limitations. The integration of a distance-dependent CRP and PDE framework offers a robust mathematical foundation, which enhances the model's applicability to trial-varying neural data. The validation of the model on both synthetic and real data demonstrates its practical impact and capability to uncover non-stationary neural dynamics.", "Weaknesses": "The paper could improve clarity in the presentation of mathematical formulations, as parts may be challenging for readers unfamiliar with advanced statistical models and processes. While the use of variational Laplace-EM and parallel computing methods enhances computational performance, an explicit comparison of computational costs with other models is lacking. Additionally, the paper might benefit from a more detailed discussion on the limitations and potential pitfalls of the proposed model.", "Questions": "How does the computational efficiency of the irSLDS model compare quantitatively with existing rSLDS models during inference tasks? Moreover, can the irSLDS model be adapted to different types of neural datasets without significant modifications, and what would be the expected challenges during such adaptations?"}}
{"paper_id": "koYsgfEwCQ", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. DynaVol presents a novel approach using object-centric voxelization and a three-stage training process, enabling superior performance in decomposing 3D dynamic scenes compared to existing 2D and 3D methods.\n2. The model effectively handles occlusion and supports dynamic scene editing, contributing to advancements in understanding and manipulating 3D environments.\n3. The use of cycle-consistency loss and volumetric slot attention enhances learning of 3D consistent representations.", "Weaknesses": "1. The presentation of the methodology could be improved for better clarity, particularly in explaining the technical implementation details and network components.\n2. The paper does not provide extensive experimental results across a diverse range of datasets, which would help demonstrate the generalizability and robustness of the approach.\n3. There might be computational challenges associated with the proposed approach, and these challenges are not thoroughly addressed in the paper.", "Questions": "1. How does DynaVol manage computational efficiency, particularly given the complex network components and multi-stage training process?\n2. Can the approach be extended to handle more complex or larger-scale scenes with higher numbers of dynamic objects?\n3. What are the specific limitations in terms of scene complexity or object occlusions that the model might face, and how does it compare quantitatively to state-of-the-art models in these scenarios?"}}
{"paper_id": "o6AN2ZoNw2", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper proposes an innovative approach for open-vocabulary semantic segmentation that eliminates the need for extensive annotated masks, using pseudo-mask generation and language supervision. This method not only achieves state-of-the-art results but also demonstrates scalability and robustness compared to resource-intensive counterparts.\nThe use of self-supervised ViT for mask generation and language features from noisy image-text datasets aligns well with open-vocabulary needs, providing a practical alternative.\nEffective use of pseudo-mask generator and image-text contrastive loss to align pixel and language features is a compelling aspect of the methodology.", "Weaknesses": "While the proposed framework is efficient, the paper could provide more clarity in certain methodological details, such as specific hyperparameter settings and architecture configurations.\nThe reliance on noisy datasets for language features could sometimes introduce variability in results; additional analysis on this could enhance the understanding of the method's robustness.\nAlthough achieving state-of-the-art results, the impact of this method on real-world applications could be further explored, analyzing its adaptability across a wider range of datasets.", "Questions": "1. How sensitive is the method to the quality and size of the noisy image-text datasets used for language supervision? Would there be significant changes in performance with different datasets?\n2. Are there specific examples or scenarios where P-Seg does not perform as well compared to models using annotated masks, and how does the framework address such cases?\n3. Is there a detailed analysis available on the computation resources needed and time efficiency compared to traditional training methods with annotated masks?"}}
{"paper_id": "HXZK1Z8tHa", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel method, ShareFormer, which provides a balanced solution to the trade-offs in Transformer-based image restoration. The proposed Shared Portion Stripe Attention (SPSA) reduces computational overhead while maintaining performance, and the residual connections improve trainability. The method is comprehensively evaluated across multiple benchmarks and demonstrates state-of-the-art results, highlighting its effectiveness and potential for broad applicability.", "Weaknesses": "While the method shows promise, the explanation of how the residual connections specifically improve trainability and introduce local inductive bias could be elaborated. Additionally, more clarity on how the shared attention maps maintain the quality of results compared to conventional mechanisms would strengthen the paper. Some aspects of possible limitations in diverse real-world scenarios and the impact on model generalization are not thoroughly addressed.", "Questions": "1. How are specific architectural parameters tuned in ShareFormer to optimize the trade-off between latency and performance? 2. What are potential limitations when extending ShareFormer to high-level vision tasks, given its design for image restoration? 3. Are there any constraints concerning dataset characteristics that could affect the model's performance?"}}
{"paper_id": "QO3yH7X8JJ", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The proposed Diff-SR method effectively leverages pre-trained diffusion generative models, avoiding resource-intensive fine-tuning or retraining steps, and shows impressive flexibility in arbitrary-scale super-resolution. The method's novel noise injection strategy using the Perceptual Recoverable Field (PRF) is innovative and appears to be well-founded. Extensive experiments validate its superior performance across a range of datasets and scales.", "Weaknesses": "The presentation could be improved, particularly in conveying detailed explanations about the underlying mechanics of the noise injection process and how PRF is defined and computed. Additionally, comparisons with a broader range of state-of-the-art methods would strengthen the claims of superiority.", "Questions": "How exactly is the Perceptual Recoverable Field (PRF) quantified, and what criteria are used to determine the optimal noise level? Are there scenarios where the proposed noise injection strategy may not perform well in terms of maintaining high-level fidelity?"}}
{"paper_id": "sNtDKdcI1f", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The study provides important insights into the biases of RLHF, specifically highlighting the tendency of reward models to favor longer outputs. The use of multiple open-source datasets, comprehensive analysis of output length correlations, and thoughtful interventions in the RLHF pipeline contribute to the study's thoroughness.", "Weaknesses": "The paper acknowledges that while length is a significant factor, it does not fully explain the improvements seen with RLHF. However, it could explore more aspects or underlying factors that contribute to the perceived improvements in helpfulness. Additionally, the focus on length might overshadow other potential biases or factors influencing reward models.", "Questions": "1. Beyond length, are there other superficial features in outputs that might similarly skew rewards? How can they be quantified?\n2. Were there any unexpected results when implementing the length penalty or altering preference data, especially in terms of model performance on tasks beyond those analyzed? \n3. How does the implementation of these interventions affect the performance of the reward model across various types of tasks, like creative tasks versus factual tasks?"}}
{"paper_id": "HFtrXBfNru", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The proposed method, SMART, effectively addresses the challenge of evolving graph data by minimizing representation distortion through self-supervised learning, which is a novel contribution to the field of graph neural networks.\n2. The use of self-supervised graph reconstruction techniques to improve temporal generalization estimation is innovative, demonstrating outstanding performance on both synthetic and real-world datasets.\n3. The paper provides a comprehensive evaluation, including theoretical lower bounds on representation distortion and metrics such as MAPE, which highlight the critical role of structure graph reconstruction.", "Weaknesses": "1. While the paper presents a novel framework, the presentation could be improved for clarity. Some aspects of the adaptive feature extractor and graph reconstruction processes might benefit from a more detailed explanation.\n2. The reliance on synthetic and predefined datasets may not fully capture the complexity and variability of real-world evolving graphs, potentially limiting the generalizability of the results.", "Questions": "1. How sensitive is the SMART method to the choice of hyperparameters, such as those used in the self-supervised graph reconstruction?\n2. Can the SMART framework be easily adapted to other types of temporal graph data beyond the ones evaluated in the study, like those with dynamic topology changes or intermittent data arrivals?\n3. What are the computational overheads associated with implementing SMART, particularly in terms of training time and resource consumption?"}}
{"paper_id": "4u0ruVk749", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces an innovative approach to handle unobserved confounders using diffusion models, improving the estimation of Individual Treatment Effects (ITE). The methodology is well-founded on strong theoretical principles, and the experimental results demonstrate an advantage over a wide array of state-of-the-art models. The integration of diffusion processes to reconstruct latent confounders offers a fresh perspective in causal inference, marking a significant contribution to the field.", "Weaknesses": "While the theoretical framework is strong, the paper's presentation lacks clarity in some parts, particularly in detailing the diffusion process and its practical implementation aspects. The reader may find it challenging to grasp the intricacies of the model due to dense mathematical explanations. Additionally, the scalability of such models in real-world applications with large datasets remains unexplored in the paper.", "Questions": "1. How does the diffusion model's computational cost compare to traditional generative models in practice?\n2. Are there any limitations observed during experiments when using this approach on large-scale, real-world datasets?\n3. Can the proposed diffusion model be integrated into existing causal inference frameworks to enhance their performance?"}}
{"paper_id": "Pa6SiS66p0", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper addresses a significant issue in continual learning by proposing a novel multi-modal approach which leverages the integration of audio and visual modalities to reduce catastrophic forgetting. It presents a well-structured benchmark for evaluating multi-modal continual learning on the VGGSound dataset, which sets a foundation for further research in this area. The method, SAMM, demonstrates improved performance over baseline methods and highlights the enhanced stability-plasticity trade-off offered by multi-modal learning.", "Weaknesses": "While the proposed method shows potential, the paper could benefit from a deeper exploration of the limitation and challenges of implementing multi-modal learning systems, possibly including computational costs and data requirements. Additionally, clearer comparisons with other existing multi-modal systems for continual learning would enhance the impact of this work. The presentation could be more engaging with additional visualizations or diagrams to clearly convey the performance improvements and method structure.", "Questions": "1. How does the proposed SAMM method handle scenarios where one modality might provide contradictory information compared to another? \n2. Has the study considered the computational overhead added by using multiple modalities and how does it compare to traditional unimodal approaches? \n3. Are there specific tasks or domains where this multi-modal approach significantly underperforms or would not be suitable?"}}
{"paper_id": "rkplYfqUr0", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. The proposed GEN-Z framework introduces a novel approach to zero-shot text classification by utilizing generative rather than discriminative prompting, which addresses challenges related to miscalibration and inefficiencies from prompt variations.\n2. GEN-Z demonstrates robust performance across 19 classification tasks and multiple open-source language model families, providing evidence of the method's generalizability and effectiveness.\n3. Aggregating results from multiple paraphrases of label descriptions is a creative strategy to reduce variance, strengthening the approach’s robustness.", "Weaknesses": "1. The requirement for manually crafting label descriptions and generating paraphrases could limit the scalability of the method, as it involves human intervention, which can be resource-intensive.\n2. While the approach shows robustness in specific tasks, the reliance on particular language models and descriptions may not generalize well to entirely new or unseen domains without significant re-engineering.\n3. The reliance on ChatGPT for generating paraphrases introduces dependencies on external models, which could affect reproducibility and consistency across different implementations.", "Questions": "1. How does GEN-Z perform on non-standard NLP tasks or when faced with highly unstructured data?\n2. Can the approach be adapted for multilingual applications, and if so, what challenges are foreseen in such adaptations?\n3. Is there a systematic method to generate label descriptions and paraphrases without manual intervention? If so, how does it impact performance compared to manually crafted descriptions?"}}
{"paper_id": "pTU2X9mUBe", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper successfully addresses a significant gap in the logistics and supply chain research community by proposing a large-scale, comprehensive last-mile delivery dataset. The LaDe dataset is diverse, capturing a broad scope of delivery scenarios, which will likely catalyze advancements in algorithm development for logistics tasks. The dataset's comprehensive nature, covering both pick-up and delivery processes across multiple cities, is a considerable contribution to the field.", "Weaknesses": "While the dataset is comprehensive, the paper could improve by offering more detailed analysis or case studies that demonstrate the dataset's practical utility in addressing specific logistics problems. Furthermore, the benchmarking tasks, although essential, are relatively standard, and the paper could enhance its contributions by introducing novel metric or task insights derived from LaDe data. Handling sensitive information, particularly privacy concerns, is also not clearly addressed.", "Questions": "1. How does the dataset ensure the privacy and security of the individuals involved (e.g., couriers and recipients), given its comprehensive nature?\n2. Can the dataset be adapted for use in different cultural or regulatory contexts, given its origin in Chinese cities?\n3. Are there plans to continuously update or expand the dataset to capture trends or seasonal shifts in last-mile delivery operations?"}}
{"paper_id": "uhtQyRrTzY", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "1. The proposed MIMIC method effectively addresses the scalability issue of existing dense prediction approaches by using unannotated real-world videos, providing superior performance in dense tasks.\n2. The paper introduces a novel dataset curation approach that doesn't rely on 3D annotations, making it applicable to a wide range of real-world data sources.\n3. The method shows significant improvements in performance compared to models trained with expensive annotated datasets, demonstrating its effectiveness and potential impact on the field.", "Weaknesses": "1. The dependence on keypoint detection methods like SIFT may have limitations in terms of scalability or effectiveness in varying environmental conditions.\n2. Details on the computational cost and efficiency of dataset curation using MIMIC are limited, which is crucial for evaluating the practicality of the approach.\n3. The paper may need more in-depth comparison with other state-of-the-art self-supervised dense task methods to establish relative performance across various scenarios.", "Questions": "1. How does MIMIC handle videos with highly dynamic scenes or significant occlusions which could affect keypoint matching and alignment?\n2. Can the MIMIC approach be adapted for use with other feature detection algorithms beyond SIFT?\n3. What are the potential impacts or challenges of using MIMIC in real-world deployment scenarios with different camera types or varying recording conditions?"}}
{"paper_id": "WZfatbNdLV", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "The TrASPr model effectively integrates advanced transformer architecture with biological insights, improving the accuracy of RNA splicing prediction. Its ability to outperform state-of-the-art models in tissue-specific splicing tasks and its utility in designing RNA sequences demonstrate significant novelty and practical applicability. The use of Bayesian Optimization further adds to its strengths in experimental settings.", "Weaknesses": "While the model shows impressive results, the complexity of the approach might limit its accessibility to broader audiences not familiar with deep learning techniques or RNA biology intricacies. Details on the interpretability of the model, especially concerning how it identifies regulatory features, could have been expanded.", "Questions": "1. How does the model handle previously unseen tissue conditions, and what are its limitations in generalizing beyond the training data? \n2. Can the TrASPr model be adapted for splicing event prediction in species other than humans and mice, or is additional data required for such adaptability? \n3. How does the model address the effect of environmental factors or stresses reflected in splicing outcomes across different tissues?"}}
{"paper_id": "YkRwadXWHd", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 3, "Strengths": "The paper introduces a method that effectively improves continuous sign language recognition by integrating multiple modalities: video, keypoints, and optical flow. The hierarchical knowledge distillation framework reduces computational burdens while maintaining accuracy. The approach shows state-of-the-art results on multiple benchmark datasets, illustrating strong practical performance and potential impact.", "Weaknesses": "The integration of the multi-modal approach, though effective, may face challenges in scenarios where one of the modalities provides noisy or less reliable data. Additionally, the reliance on complex fusion techniques might limit the model's generalizability to other tasks or simpler scenarios.", "Questions": "How does the model perform when one of the modalities is significantly weaker or missing? Can the approach still provide competitive results with incomplete multi-modal data? Are there any explorations regarding the interpretability of the fused modality outputs?"}}
{"paper_id": "q4Bim1dDzb", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper proposes a novel approach, UniVoxel, that effectively combines geometry, materials, and illumination modeling within a unified voxelization framework, thus significantly improving the optimization efficiency of inverse rendering. The integration of Spherical Gaussians for local incident light modeling is an innovative technique that eliminates the need for complex ray tracing, making the method robust across various lighting conditions. The framework achieves a substantial reduction in training time while maintaining high rendering quality, demonstrating its practical utility for real-world applications.", "Weaknesses": "The paper might lack sufficient comparative analysis against all recent methods to establish the superiority of UniVoxel across diverse scenarios. The description of how the semantic field integrates varying material and illumination properties could have been more detailed to enhance understanding. Additionally, despite the promising results, the potential limitations of applying UniVoxel to more complex scenes or under significantly varied lighting conditions haven’t been fully explored.", "Questions": "How well does the UniVoxel framework perform in extremely complex scenes with intricate geometry and dynamic lighting conditions? Is there an inherent limitation to the types or scales of scenes it can effectively handle? Furthermore, could the method be adapted to further reduce computational requirements, potentially enabling real-time applications?"}}
{"paper_id": "39cPKijBed", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The proposed time-dependent importance reweighting (TIW) method provides a novel approach to mitigate dataset bias in diffusion models, which is a crucial advancement in generative modeling tasks.\n2. The method is theoretically sound and demonstrates significant improvements over existing techniques in experiments across multiple datasets, showing its adaptability and practical relevance.\n3. TIW addresses the challenges in density ratio estimation by incorporating a time-dependent discriminator, which enhances the model's convergence to an unbiased distribution.", "Weaknesses": "1. The presentation of the method could be clarified with more detailed descriptions of the proposed framework, especially regarding the implementation of the time-dependent discriminator and how it integrates with the diffusion process.\n2. While the experimental results are promising, the specific settings and configurations of baseline comparisons could be more explicitly detailed to strengthen the claims of superiority.\n3. The paper could benefit from a broader discussion of potential limitations and future directions for research in this area.", "Questions": "1. How does the time-dependent discriminator scale with larger and more complex datasets, and are there computational trade-offs compared to traditional methods?\n2. Can the proposed framework be extended to other types of generative models beyond diffusion models, or is it highly specialized?\n3. What are the potential impacts of the method on the computational efficiency and training time of diffusion models compared to standard approaches?"}}
{"paper_id": "aG3EARrrd1", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The OSRT model addresses a significant challenge in online learning by integrating tree-based structures with RBF networks for improved efficiency in handling large, streaming datasets. The model's dynamic adaptability and concurrent optimization of tree depth and RBF parameters are notable strengths. Empirical validation on benchmark time series datasets demonstrates its effective performance.", "Weaknesses": "The methodology appears to lack clarity in certain aspects, such as the exact mechanisms for adaptive RBF center determination and pruning criteria. The paper could benefit from more comprehensive comparisons with a broader range of existing methods to thoroughly gauge the model's advantages. Additionally, the model's specific limitations, such as potential scalability issues or computational burden, could be better addressed.", "Questions": "How does the OSRT model specifically handle non-stationary data streams, and are there scenarios where its performance may degrade? Could the authors provide more insights into practical implementation challenges of OSRT for real-world applications, particularly in terms of computational resources?"}}
{"paper_id": "Ts95eXsPBc", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The proposed Spatially-Aware Transformers (SAT) effectively integrate spatial information into episodic memory models, leading to improved performance in spatial reasoning tasks. The introduction of the Adaptive Memory Allocator (AMA) adds a novel mechanism for optimizing memory allocation, making it a significant contribution to the field.", "Weaknesses": "While the integration of spatial information is innovative, the paper may lack detailed analysis comparing different spatial embedding techniques. Additionally, the presentation of the experimental results could benefit from more comprehensive visualizations and clarity.", "Questions": "1. How does the SAT model compare to existing state-of-the-art models in terms of computational efficiency? 2. What specific environments and tasks were used to evaluate the SAT, and were these representative of real-world applications?"}}
{"paper_id": "LfhG5znxzR", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. The introduction of sparse and discrete bottlenecks makes neural network activations more interpretable while maintaining performance levels. \n2. The paper presents a novel way to apply vector quantization through codebooks to enhance understanding and control over neural networks.\n3. The proposed approach is validated across different types of neural networks and datasets, showing its versatility and potential for wide applicability.", "Weaknesses": "1. The method may introduce additional complexity in properly tuning and managing codebook implementations such as size selection and code vector granularity.\n2. The paper may lack an extensive comparative analysis against other interpretability and controllability methods for neural networks, which would bolster the claim of its effectiveness.\n3. While the approach is demonstrated to maintain performance, the trade-off between interpretability and potential minor performance degradation is not deeply explored.", "Questions": "1. How does the size of the codebook influence the trade-off between interpretability and model performance?\n2. Are there specific neural network architectures or application domains where this method excels or falls short?\n3. What strategies are employed to mitigate the potential loss of fine-grained information when using large codebooks for sparse representations?"}}
{"paper_id": "bUGzjiUsIq", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper addresses a significant issue in multi-label classification with partially annotated datasets using a novel integration of reinforcement learning and supervised learning. The proposed PAPG framework demonstrates improved performance and generalizes well across various datasets, illustrating its robustness and potential impact in the field.", "Weaknesses": "While the method shows promising results, the complexity and practical implementation details of the PAPG framework might pose challenges for practitioners. Additionally, more insights into the limitations or potential drawbacks of the proposed approach would strengthen the paper.", "Questions": "How does the PAPG framework handle extremely large label sets or datasets with very minimal annotations? Are there any scalability concerns with the proposed approach when applied to real-world, large-scale datasets?"}}
{"paper_id": "uMAujpVi9m", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel approach to pretraining pocket representations that addresses data scarcity issues using pseudo-complex generation and aligns these with small molecule encoders. The proposed ProFSA method shows significant improvements in benchmarks related to druggability prediction and ligand binding affinity, highlighting its practical applicability in biomedical research.", "Weaknesses": "While the method shows promise, its reliance on the quality of the segmentation of protein structures and pseudo-ligand representation alignment could introduce variability. Additionally, the paper could benefit from improved clarity in its presentation style to aid comprehension.", "Questions": "How does the segmentation approach determine the boundaries of drug-like fragments in protein structures? Are the results heavily dependent on the initial segmentation criteria? What are the potential limitations of using pseudo-ligand representations in the context of highly specific ligand-receptor interactions?"}}
{"paper_id": "uqxBTcWRnj", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper offers a novel approach to integrating neural networks with symbolic systems through a Transitional Dictionary Learning framework, which effectively bridges the gap between neural and symbolic representations. Its innovative use of unsupervised learning to generate intermediate representations is a significant contribution. The introduction of new evaluation metrics, clustering information gain and heuristic shape score, provides useful tools for assessing model performance. The research demonstrates significant improvements over existing methods in compositional pattern discovery.", "Weaknesses": "While the proposed method shows promise, the integration and practical applicability need more empirical validation in more diverse and complex real-world datasets. The presentation could be improved in terms of clarity, especially in explaining the technical details of the game-theoretic diffusion model and the metrics introduced. Furthermore, the explanation on how the transitional representations specifically contribute to better symbolic reasoning could be expanded.", "Questions": "How well does the method scale with larger, more intricate datasets beyond the tested compositional visual object datasets? Are there specific domains or types of data where this transitional representation system might face limitations? Could more insight be provided into the interpretability of the learned predicates in terms of their semantic meaning or utility in downstream tasks?"}}
{"paper_id": "o4CLLlIaaH", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The proposed shift from image-based to point-based rendering provides a novel approach to solving critical issues like occlusion and geometric inconsistencies in generalized NeRFs.\n2. The introduction of visibility-oriented feature fetching and log sampling strategy demonstrates a strong innovation in rendering speed and quality.\n3. Experimental validation on multiple datasets showcases the effectiveness and potential of the method in enhancing rendering quality and scene interaction.", "Weaknesses": "1. The transition from image-based to point-based methods may involve complexities that are not fully addressed, particularly in relation to scalability and computational demands.\n2. While the method shows promise, detailed analysis on how it compares to existing methods beyond benchmark performance metrics could be beneficial.\n3. Potential limitations in diverse scene settings or specific edge cases are not discussed thoroughly.", "Questions": "1. How does the GPF method handle scenarios with extremely complex interactions between light and surfaces, which are challenging for traditional NeRFs?\n2. What are the specific computational overheads introduced by the point-based approach compared to image-based methods?\n3. Can the method be adapted or extended to handle dynamic scenes where objects move or change over time?"}}
{"paper_id": "KNzL1nglNB", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The proposed LRM method offers a novel approach to handling scenarios with limited labeled data by leveraging label encodings to enhance prediction diversity and decrease bias towards substantial categories. The theoretical analysis linking LRM with neural collapse is compelling. The method demonstrated improved performance over traditional methods like ERM and EntMin in extensive experimental settings.", "Weaknesses": "The paper could benefit from clearer exposition in places, especially regarding the mathematical formulation of LRM and its theoretical analysis. Additionally, while the approach is empirically validated, more datasets and practical applications would strengthen the claims. Potential constraints in computational resources for large-scale use are not detailed.", "Questions": "How does LRM specifically ensure diversity in predictions? Are there scenarios where it might inadvertently perform lower than existing methods, particularly in balanced datasets? Could the authors provide more insights into the computational complexity and efficiency of the LRM method under real-world constraints?"}}
{"paper_id": "pvhyBB86Bt", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses an important and under-explored aspect of deep neural network training by examining Jacobian stability. The use of random matrix theory to extend the analysis of Jacobian behavior beyond initialization is a valuable contribution. The study provides insights into the behavior of Jacobians throughout training and in the context of sparsity, which is beneficial for understanding network stability.", "Weaknesses": "The assumption that stability at initialization ensures stability during training may not hold in all scenarios, especially in cases with substantial network modifications or dynamic architectures. The paper could benefit from more empirical evidence across diverse architectures beyond MLPs to strengthen its claims. Additionally, the impact of different non-iid initialization protocols on the stability of the Jacobian could be further explored.", "Questions": "How does the proposed stability theorem explain scenarios where the initial Jacobian stability does not seem to prevent issues like training divergence in practice? Are there specific conditions or architectures where the theorem may not apply? Can the results be generalized to more complex architectures such as convolutional neural networks or transformers, where depth and structural differences might affect Jacobian behavior differently?"}}
{"paper_id": "Mylk5iamJC", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper proposes an innovative framework using Latent Gaussian Mixture Models (L-GMM) that bridges a significant gap in CSR and OSR by employing a unified generative approach. It effectively combines generative and discriminative modeling, demonstrating superior performance in both open-set and closed-set recognition tasks. The approach is novel as it utilizes generative models to address both CSR and OSR problems without requiring any modifications or prior knowledge of unknown classes.", "Weaknesses": "The presentation of the paper, while coherent, could be improved in clarity, especially in how the methodology is detailed and the transitions between sections are handled. While the methodology is promising, it might lack some detailed comparisons or benchmark results against a broader range of state-of-the-art models, which could strengthen claims of superiority.", "Questions": "How does L-GMM handle extremely large datasets, and what are its computational requirements compared to other models? Are there any limitations in scalability or efficiency when the model is applied to real-world scenarios with numerous unknown classes? Could you provide more detailed results on the specific datasets used for CSR and OSR experiments?"}}
{"paper_id": "MsOcVFzv8D", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a theoretically sound approach by incorporating margin discrepancy into the MDTC framework, providing a robust foundation and advancing the theoretical understanding of multi-domain text classification. The MDAT method offers state-of-the-art performance across multiple datasets and effectively utilizes adversarial training to minimize domain divergence.", "Weaknesses": "The presentation needs improvement, particularly in clearly explaining complex theoretical concepts to make them accessible to a wider audience. While the proposed method is validated on MDTC benchmarks, a wider range of datasets could further verify the generalizability and applicability of the approach.", "Questions": "1. How does the introduction of margin discrepancy in the adversarial framework specifically enhance the model's ability to generalize across different domains in practice?\n2. Are there any specific limitations or assumptions in the theoretical framework that could affect its applicability to other domain adaptation tasks outside MDTC?\n3. How sensitive is the method to the choice of hyperparameters, particularly those influencing margin discrepancy, and what guidelines are there for selecting them?"}}
{"paper_id": "fBlHaSGKNg", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper addresses a significant gap in semi-supervised learning by proposing a novel sample selection criterion, α-MMD, which effectively enhances model performance under annotation budget constraints.\n2. The integration of the UPA method with existing SSL frameworks like FlexMatch and FreeMatch demonstrates strong empirical results across multiple datasets, indicating the practical applicability of the approach.\n3. The use of Generalized Kernel Herding without Replacement (GKHR) is both innovative and contributes to the methodological strength of the work, achieving a balance between representativeness and diversity in sample selection.", "Weaknesses": "1. The paper could benefit from a more detailed discussion on potential limitations of the UPA method, especially in scenarios with extremely constrained budgets.\n2. While the empirical results are promising, there is a lack of theoretical analysis or guarantees regarding the convergence or optimality of the proposed method, which could strengthen the study.", "Questions": "1. Can the α-MMD criterion be adapted or extended to other types of kernels or feature extraction methods beyond CLIP?\n2. How does the proposed UPA method perform compared to state-of-the-art active learning methods in terms of computational efficiency and time complexity?"}}
{"paper_id": "HwQ8NVvdmm", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper effectively addresses a critical challenge in combining continual learning with quantization for edge devices, offering a novel solution through Hadamard Domain Quantized Training (HDQT). The method demonstrates impressive efficiency improvements with minimal accuracy loss, making it very relevant for real-world applications. The results are well-supported with experiments on both CIFAR100 and HAR datasets.", "Weaknesses": "The paper may need more detailed exploration of potential limitations or scenarios where HDQT's performance might degrade. Additionally, while the integration of Hadamard transforms with quantization is innovative, it might face challenges in hardware compatibility or implementation complexity on different edge devices.", "Questions": "What specific hardware environments were used for testing the proposed method, and how well does HDQT adapt to different edge devices with varying computational resources? Would HDQT still maintain its efficiency and accuracy benefits if implemented on widely used commercial edge devices?"}}
{"paper_id": "cZo6pDtDZr", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces innovative algorithms for estimating collision probability and conducting sequential hypothesis testing under local differential privacy (LDP). These algorithms improve upon sample complexity issues in existing methods and offer solutions when the privacy parameter is unknown. The distributed user-server protocol for private estimation and the adaptive nature of the sequential testing algorithm are particularly noteworthy advances.", "Weaknesses": "The presentation of the results could be clearer and more structured, which might help in better understanding the practical implications and comparisons with existing methods. More detailed empirical validation across a wider range of scenarios and datasets could strengthen the conclusions further.", "Questions": "How does the proposed sequential testing algorithm handle practical constraints such as communication overhead between users and the server? Would there be any scenarios where the distributed nature of the estimation algorithm might introduce biases or inaccuracies, and how would these be mitigated?"}}
{"paper_id": "F7QnIKlC1N", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "The paper presents a novel and efficient method for predicting molecular ground-state conformations, a crucial task in chemistry. The introduction of the Molecule Structural Residual Self-Attention mechanism is innovative, offering a meaningful contribution to molecular representation learning. The use of the GTMGC model demonstrates significant improvements over existing state-of-the-art methods, suggesting the robustness and effectiveness of the approach. The paper is well-structured, with clear explanations and thorough experimental validation on benchmark datasets.", "Weaknesses": "While the proposed method shows strong results, the reliance on specific datasets (Molecule3D and QM9) may limit the generalizability of the model to other datasets or real-world applications. Additionally, the need for adequate computational resources to train such a model might be a potential barrier to entry for some researchers.", "Questions": "How does the model perform on other datasets not mentioned in the paper, and how well does it generalize to larger or more diverse molecules? Could you provide more insight into the computational requirements for training the GTMGC model, particularly in terms of time and resources needed?"}}
{"paper_id": "8JCn0kmS8W", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "WavJourney introduces a novel approach that integrates Large Language Models with expert audio models to create comprehensive audio from textual descriptions. The paper successfully demonstrates the model's ability to synthesize realistic and engaging audio content aligned with semantic and temporal conditions, achieving state-of-the-art results on text-to-audio benchmarks. The framework's compositional and interpretable design facilitates human-machine co-creation, which is a significant step forward in audio generation from text.", "Weaknesses": "The paper mentions potential limitations in the efficiency of the system and the artificial nature of the composition, which might limit its real-world application without further optimization. Additionally, while the approach is demonstrated on certain benchmarks, the generalization to other untested scenarios might pose challenges.", "Questions": "1. How does WavJourney handle complex or ambiguous text prompts that could result in multiple valid audio compositions? 2. Can the framework be extended to support real-time audio generation, or does it inherently require significant preprocessing time to compile the audio script? 3. What are the specific computational demands of the system, and how does it manage resource constraints when orchestrating multiple audio models?"}}
{"paper_id": "XbLffB0T2z", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces a novel approach to enhancing transferability of poisoning attacks, addressing a key limitation in existing methods.\n2. The method is demonstrated to achieve significant reductions in test accuracy across multiple benchmark datasets, highlighting its effectiveness.\n3. Combines both supervised and unsupervised learning paradigms to generate perturbations, offering a robust attack strategy.", "Weaknesses": "1. The presentation of the paper could be improved, particularly in detailing the methodology to ensure clarity and reproducibility.\n2. Limited discussion on potential defenses or mitigation strategies against the proposed attack, which is crucial for understanding its real-world implications.\n3. More extensive experiments could strengthen the conclusions, particularly with additional datasets or more diverse learning paradigms.", "Questions": "1. How does the proposed method perform against more recent or advanced learning algorithms that might employ different regularization or augmentation techniques?\n2. Could the authors elaborate on the specific challenges or limitations faced when iterating between supervised and unsupervised targets during perturbation generation?\n3. What are the potential ethical considerations or implications of developing such a strong poisoning attack?"}}
{"paper_id": "4y3GDTFv70", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper provides a novel theoretical framework linking the emergent abilities of LLMs to Bayesian inference on sparse joint distributions. \n2. It offers a new perspective by distinguishing between unambiguous and ε-ambiguous languages, which is insightful for understanding language processing by LLMs.\n3. Simulation experiments conducted on synthetic data provide empirical support for the theoretical claims made.", "Weaknesses": "1. The presentation could be improved, as the complex theoretical concepts might be difficult to understand without thorough background knowledge.\n2. The paper relies heavily on the assumption that LLMs act as universal density approximators, which might not be straightforward to validate across all language models and use cases.\n3. The synthetic nature of validation experiments may not fully capture the complexities encountered in real-world data.", "Questions": "1. How might this framework translate to practical improvements in LLM design or training strategies?\n2. Are there plans to extend the validation beyond synthetic experiments to more complex, real-world scenarios or datasets?\n3. Can the theory be generalized across different types of LLM architectures, or does it assume specific characteristics present only in certain models?"}}
{"paper_id": "ODSgo2m8aE", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses an important gap in the stability and fairness of GNNs trained on biased data, with the novel application of Lipschitz bounds tailored for GNNs. The method is elegantly integrated into existing GNN frameworks using the plug-and-play JacoLip approach, making it practical and appealing for real-world applications. Empirical results across various datasets add credibility to the proposed solution.", "Weaknesses": "While the theoretical foundation of using Lipschitz bounds in GNNs is solid, the paper could further elaborate on specific real-world cases where biases are significantly mitigated. The presentation of technical details and experiments could be improved for clarity, as some readers may find it challenging to follow without additional context or illustrations.", "Questions": "1. How does the integration of JacoLip affect the training time for larger and more complex graphs compared to traditional GNNs? \n2. Have the authors considered potential limitations or scenarios where Lipschitz bounds might not be effective in stabilizing the outputs of GNNs?"}}
{"paper_id": "RlfD5cE1ep", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper provides an insightful analysis of the role of feature normalization in non-contrastive learning, offering a theoretical understanding of its stabilizing effects. It extends existing analyses with a practical dimension by incorporating feature normalization into the evaluation, which aligns with real-world practices. Numerical simulations verify the theoretical claims, improving the understanding of non-contrastive learning dynamics.", "Weaknesses": "The paper primarily builds on and extends previous analyses rather than presenting a completely novel framework. While the results are promising, they are largely verified on synthetic data or CIFAR-10, a relatively simple dataset, potentially limiting generalizability to more complex scenarios. Some details of the experimental setup and comparisons to alternative methods might require further elaboration.", "Questions": "1. Can the proposed theoretical framework be extended to more complex datasets beyond CIFAR-10?\n2. How does the induced sixth-order dynamics specifically contribute to stability compared to third-order systems in L2 loss approaches?\n3. Are there practical scenarios or applications where feature normalization might not contribute significantly, and if so, how would the model perform in such cases?"}}
{"paper_id": "18TfucMNTr", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper presents a continuation method utilizing Gaussian convolution, which is a promising technique to address the challenges posed by local minima and saddle points in deep learning optimization. The approach demonstrated improved convergence and stability, with faster convergence rates and higher accuracy than traditional methods. This suggests an effective strategy to enhance deep network training.", "Weaknesses": "While the method shows promise, the paper lacks a detailed comparison with other state-of-the-art optimization techniques beyond traditional methods. It is also unclear how scalable the proposed approach is to larger datasets and more complex models than those used in the experiments. The presentation of results could be more thorough, including more quantitative analysis and benchmarks.", "Questions": "1. Can the proposed regularization method be generalized to other forms of continuation methods or non-Gaussian convolution approaches?\n2. How does the method perform on larger and more complex datasets or models, such as ImageNet or large-scale natural language processing tasks?\n3. What are the specific computational requirements and overheads when implementing this continuation strategy in practical scenarios?"}}
{"paper_id": "JGTC6WgO4T", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper addresses an overlooked area in unsupervised domain adaptation by focusing on multi-source black-box domain adaptation. The Label Space-Induced Pseudo Label Refinement (LPR) framework is a novel contribution, effectively utilizing multiple source domains for pseudo-label refinement. The method is theoretically grounded and empirically validated on multiple benchmark datasets, demonstrating its effectiveness in diverse settings.", "Weaknesses": "While the method is innovative, it could benefit from a more detailed exploration of how variances in pseudo label quality across different source domains impact performance. The presentation could be improved to make complex concepts more accessible. Additionally, the empirical evaluation lacks diversity in dataset types beyond the benchmarks used, potentially limiting the generalizability of findings.", "Questions": "1. How does the quality of source predictions affect the proposed LPR framework's performance?\n2. Are there certain characteristics of source or target domains that can lead to suboptimal pseudo label refinement?\n3. How scalable is the PRN approach when applied to large-scale datasets with multiple diverse source domains?"}}
{"paper_id": "fQHb1uZzl7", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel architecture that successfully integrates feature and cost aggregation within a Transformer-based framework for dense image matching, demonstrating superior performance over existing methods. The use of self- and cross-attention mechanisms within a coarse-to-fine hierarchical strategy is innovative and results in robust correspondence estimation under challenging conditions.", "Weaknesses": "While the architecture shows improved performance, the contribution could be seen as an incremental advancement given the existing body of work utilizing Transformer architectures for computer vision tasks. The dependence on computationally expensive Transformer operations might raise concerns regarding the scalability and real-time applicability of the proposed method.", "Questions": "How does the proposed UFC architecture perform in terms of computational efficiency and resource requirements compared to existing methods? Are there any specific scenarios in the benchmarks where UFC particularly excels or struggles?"}}
{"paper_id": "YjG29CIOP7", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "1. The paper addresses significant challenges in the field of data-free meta-learning (DFML), specifically task-distribution shift (TDS) and task-distribution corruption (TDC).\n2. The proposed solutions, TEAPOT and ROSY, offer innovative methods to handle these challenges through memory-based learning and reinforcement learning-based model selection.\n3. Extensive experiments demonstrate the effectiveness and robustness of the approach across multiple datasets, indicating strong empirical support.", "Weaknesses": "1. While the approach is innovative, the complexity of the method might pose implementation challenges for practitioners unfamiliar with reinforcement learning and policy gradients.\n2. The details of the interpolation and memory maintenance strategies could be expanded to facilitate replication by other researchers.", "Questions": "1. How sensitive are TEAPOT and ROSY to the hyperparameters used, particularly in different datasets or in varying task distributions?\n2. How does the approach scale to very large-scale systems with thousands of pre-trained models, and are there any computational efficiency considerations to be addressed?"}}
{"paper_id": "DmDpu3r8iU", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper tackles an important and timely issue in the field of LLMs—alignment with human values—which is critical for their safe deployment. The formulation of the Value Understanding Measurement framework provides a structured way to assess not only the categorization of values but also the reasoning behind these categorizations, which is novel.", "Weaknesses": "While the VUM framework is a step forward, it is not clear whether the use of the Schwartz Value Survey captures the full range of ethical and cultural complexities inherent in value understanding. The reliance on GPT-4 annotations might introduce biases, and the evaluation on five LLMs might not be comprehensive enough to generalize the findings.", "Questions": "How does the VUM framework address potential biases introduced by using GPT-4 generated annotations in the dataset? Are there plans to expand the range of values or datasets used to validate the framework further?"}}
{"paper_id": "kXHEBK9uAY", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel hierarchical approach to diffusion-based generative modeling for offline reinforcement learning, achieving significant improvements in planning efficiency and generalization to long-horizon tasks. Hierarchical Diffuser effectively combines high-level subgoal generation with low-level plan execution, allowing for improved training and planning speeds. The empirical results demonstrate the superiority of this method over existing non-hierarchical and hierarchical methods on standard benchmarks.", "Weaknesses": "The paper, while thorough, could provide more discussion on potential limitations of the proposed hierarchical approach in different task environments or datasets not included in the experiments. Furthermore, the presentation could be enhanced by more detailed explanations of the hierarchical structure and clearer diagrammatic representations of the framework.", "Questions": "1. How does the method perform when applied to real-world tasks beyond offline RL benchmarks? \n2. What are the implications of choosing different segment lengths for subgoals in the Sparse Diffuser? \n3. Could the hierarchical framework be adapted to include more than two levels of planning? If so, how would this affect computational efficiency?"}}
{"paper_id": "i7LCsDMcZ4", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces novel relevance propagation techniques, SLTRP and SLRP, tailored to SNNs, effectively bridging a gap in event-based data classification. The proposed methods, integrated into the EventRPG framework, enhance the generalization capabilities of SNNs, demonstrating impressive performance improvements on event-based object and action recognition tasks. The paper is well-structured and clearly presents the methodology and experimental results.", "Weaknesses": "The methods currently focus exclusively on classification tasks, potentially limiting their applicability across other types of neural network tasks. There is also a possibility that the proposed augmentation techniques could alter the interpretability of the saliency maps, which may warrant a deeper exploration.", "Questions": "How can the proposed relevance propagation methods be extended to other types of tasks beyond classification in SNNs? Can the current methods be adapted or modified to maintain the interpretability of saliency maps while applying data augmentation techniques?"}}
{"paper_id": "jHdz0CIS2y", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "SelfVC proposes an innovative approach that utilizes entangled speech representations for effective voice conversion, overcoming many limitations of previous methods, including cross-lingual and zero-shot scenarios. The use of self-synthesized examples for iterative refinement is a notable innovation, leading to improvements in speaker similarity and conversion quality. The approach is applicable to multiple languages, which significantly extends its utility.", "Weaknesses": "While the paper introduces a strong conceptual method, some aspects of the presentation could be clearer, particularly regarding the detailed implementation of the iterative refinement process for readers not specialized in the area. Moreover, although the model performs well, the paper could benefit from a more thorough analysis of why the self-synthesized examples specifically lead to such significant improvements compared to other existing refinement techniques.", "Questions": "How does the iterative refinement strategy precisely interact with the self-synthesized examples to enhance speaker similarity? Is there any potential risk of overfitting to the synthesized examples, and how does the model mitigate such risks in various language contexts? Additionally, how scalable is this method in terms of computational resources required for training on large multilingual datasets?"}}
{"paper_id": "NkYCuGM7E2", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces an innovative approach by incorporating Large Language Models (LLMs) as the decision-making component within autonomous driving systems. This method potentially addresses key challenges in current systems, such as adaptability to complex scenarios and interpretability, which are critical for developing advanced autonomous driving solutions. The experimental results indicate improvements in traffic flow efficiency and safety, highlighting the feasibility of LLMs in this context.", "Weaknesses": "While the integration of LLMs is a novel idea, the paper lacks comprehensive details on how the LLMs are specifically tailored or trained for high-level decision-making in autonomous driving. Additionally, the scalability and real-time performance considerations in practical deployment scenarios are not thoroughly addressed. The experiments are conducted under unspecified conditions, leaving questions about the generalizability of the approach to diverse real-world settings.", "Questions": "Can you provide more details on how the LLMs are trained or adapted specifically for the decision-making tasks in autonomous driving contexts? How does the system ensure real-time performance and scalability when deployed in real-world scenarios with a large number of vehicles? Could you elaborate on the experimental setup and the diversity of scenarios tested to ensure robustness across different driving environments?"}}
{"paper_id": "RtDok9eS3s", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a thorough investigation into simplifying transformer blocks, which addresses a significant aspect of model design --- computational efficiency. The modifications proposed demonstrate impressive performance gains with reduced training times and parameter counts. The experiments are well-structured, utilizing both encoder and decoder model variations, which reinforces the reliability of results across transformer applications.", "Weaknesses": "While the simplification approach is experimentally validated, there is limited discussion on possible trade-offs in terms of generalization or when facing out-of-domain data. The paper could also benefit from clearer visual aids or diagrams to help convey the structural changes proposed, enhancing comprehension for readers unfamiliar with intricate transformer details.", "Questions": "What are the potential effects of the proposed simplifications on transformer models when applied to less common or novel tasks beyond the benchmark datasets used? Additionally, could the simplification of certain components possibly degrade the model's ability to learn more abstract or complex representations?"}}
{"paper_id": "YCPDFfmkFr", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The proposed method effectively addresses the problem of infeasibility in QP layers during neural network training. By using primal-dual augmented Lagrangian techniques, the authors offer a robust solution that broadens the range of layer architectures that can be used, increasing the model's expressive power. The development of QPLayer provides a practical toolkit that integrates well with existing learning frameworks.", "Weaknesses": "While the method is innovative, the application scope demonstrated in the experiments appears limited to structured tasks like Sudoku. The paper could benefit from a broader range of real-world applications to showcase the method's efficacy. Furthermore, the details on computational overhead and efficiency in comparison to traditional QP methods are limited.", "Questions": "1. How does the proposed method perform in more complex real-world scenarios beyond structured tasks like Sudoku?\n2. What is the computational cost of implementing the ECJ framework compared to existing primal-feasible approaches, particularly in large-scale networks?\n3. How does the approach handle extreme cases of infeasibility, and is there a threshold beyond which the method's assumptions break down?"}}
{"paper_id": "ueTdErd5Ib", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel approach that integrates learning with optimization, which enhances robustness against uncertainty in contextual stochastic optimization problems. The proposed discretization framework is theoretically sound and demonstrates significant improvements in robustness over traditional methods. The experimental results, especially on real-world scenarios such as electricity generation, underscore the method's practical utility.", "Weaknesses": "While the method shows strong performance in terms of robustness, the applicability might be limited by the computational complexity of the discretization process, especially in high-dimensional spaces. The approach also assumes the user can define a meaningful cost threshold, which might not always be straightforward in real-world applications.", "Questions": "How does the computational complexity of the discretization process scale with higher-dimensional problems? Is there an efficient way to determine the user-defined cost threshold in scenarios where domain expertise is lacking?"}}
{"paper_id": "sDmjlpphdB", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel and effective approach for prompt optimization by extending beyond demo-free instructions and employing a Mixture-of-Experts strategy. The MoP framework is well-validated through significant improvements in benchmark NLP tasks with up to 43% performance increase, demonstrating the method's power. The division of problem space into homogeneous regions with specialized experts is a genuine contribution to the field of automatic prompting.", "Weaknesses": "The paper may lack comprehensive analysis on the computational cost added by incorporating a Mixture-of-Experts approach, particularly in terms of scalability to larger LLMs and tasks. Additionally, it would benefit from a discussion on potential limitations or scenarios where MoP might underperform compared to simpler models.", "Questions": "1. How does MoP handle extremely large datasets where the clustering step itself might become computationally expensive?\n2. Are there scenarios where MoP could potentially underperform compared to more straightforward prompt strategies?\n3. Is the efficacy of the MoP framework consistent across different domains and not just the benchmark NLP tasks presented?"}}
{"paper_id": "Mdk7YP52V3", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel theoretical framework using statistical physics to explain the overfitting issues in overparameterized heteroskedastic regression models. The introduction of a nonparametric free energy framework is innovative and provides a comprehensive explanation for the behavior of these models across different regularization strengths. The empirical validation on real-world data supports the theoretical claims.", "Weaknesses": "The reliance on complex field-theoretical concepts may hinder accessibility for readers without a strong background in statistical physics. The presentation could be improved by providing more intuitive explanations or visual aids to help elucidate the theoretical framework. Additionally, the paper would benefit from a clearer description of the practical implications and limitations of the proposed framework.", "Questions": "1. How scalable is the nonparametric free energy framework when applied to larger datasets or more complex models? 2. Can this theoretical approach be extended to other types of models beyond heteroskedastic regression neural networks? 3. What are the potential challenges in implementing the suggested regularization tuning approach in practice?"}}
{"paper_id": "YGTSLDAPqb", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The 'Connect Later' framework presents a methodical approach to addressing the domain adaptation challenge by integrating targeted data augmentations after self-supervised pretraining, effectively enhancing OOD performance. The validation across diverse datasets showcases the framework's robustness and applicability.", "Weaknesses": "While the idea of domain-specific augmentations is promising, the paper does not fully explore the potential variability in augmentation strategies needed for different domains, which may limit generalizability. The process for designing these targeted augmentations lacks sufficient clarity and general guidelines.", "Questions": "1. What criteria are used to design the targeted augmentations for the fine-tuning phase, and how are they adapted to different domains?\n2. Can the 'Connect Later' framework be easily adapted to new domains without significant manual intervention?\n3. What are the computational and resource overheads associated with implementing this two-phase training approach compared to conventional strategies?"}}
{"paper_id": "JTcaziw7G1", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel method for securely augmenting large language models with private information retrieval capabilities using multi-party computation (MPC). The approach ensures privacy preservation during information retrieval from distributed servers, which is a significant advancement in maintaining data confidentiality. The method is rigorously tested using both synthetic and real data, showcasing its efficacy in retrieval accuracy and speed.", "Weaknesses": "While the approach is innovative, the computational cost is high, suggesting the need for further optimization. The paper might also lack some details regarding the implementation specifics necessary for practical deployment, which could hinder reproducibility.", "Questions": "1. What are the specific limitations in terms of computational cost, and how could these be potentially addressed in future work?\n2. How scalable is the proposed method when applied to large-scale real-world scenarios, and what impacts do you anticipate in terms of network latency?\n3. Could the approach be further optimized to reduce dependency on communication complexity without compromising privacy?"}}
{"paper_id": "lEkFq4RUCX", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The proposed DDF metric is potentially more efficient and effective than existing methods like EMD and CD, particularly for 3D point cloud tasks. \n2. By focusing on the underlying surface differences rather than point-wise correspondences, the method offers improved memory efficiency and potentially more accurate evaluations.\n3. The evaluation across multiple tasks (shape reconstruction, rigid registration, scene flow estimation, and feature representation) demonstrates the metric's versatility and practical applicability.", "Weaknesses": "1. The explanation of how reference points induce local surface geometry could have been more detailed, possibly with illustrative figures.\n2. While the method claims enhanced computational efficiency, quantitative comparisons regarding time complexity with existing metrics (EMD and CD) would strengthen this claim.\n3. The method might still face challenges with complex or degenerate surfaces where directional distances are less informative.", "Questions": "1. How sensitive is the DDF metric to the choice of reference points? Are there strategies to ensure consistent results across different point clouds?\n2. Can this surface-based approach adapt to handle dynamic scenes, where point clouds may change over time, such as in scene flow estimation?\n3. Are there any potential limitations or failure cases where DDF might not provide reliable measurements, particularly compared to other metrics?"}}
{"paper_id": "Fq8tKtjACC", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces a novel approach focusing on high-quality data to train language models, potentially achieving competitive performance with smaller architectures which reduces computational costs. The results demonstrate that careful data curation can impact performance significantly.", "Weaknesses": "The method might be overly reliant on the quality of the synthetic data generated, and the approach may not easily generalize to other coding languages or tasks beyond Python. There is also limited exploration of the broader applicability of the proposed method.", "Questions": "1. How do the authors plan to address the gap in performance across other languages and tasks outside Python?\n2. Can the approach be adapted to applications beyond code generation, and how might the data curation process change in different domains?\n3. What specific criteria were used to filter the web sources, and how might these criteria be rigorously evaluated for effectiveness?"}}
{"paper_id": "8fQlGQkj0S", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel theoretical framework that effectively distinguishes between task learning and task retrieval in in-context learning (ICL). By utilizing generative models and Gaussian mixture models, the study provides clear risk upper bounds and characterizes the behavior of task learning and retrieval under varying conditions. The findings are validated with numerical computations and experiments, which enhances the credibility of the proposed framework.", "Weaknesses": "The presentation of the paper could be improved, particularly in terms of explicating complex theoretical concepts for broader accessibility. Additionally, while the proposed models and risk analyses are robust, the experimental validation could be more extensive, including evaluations on more diverse datasets to strengthen the generalizability of the results.", "Questions": "1. How sensitive are the proposed risk upper bounds to different configurations of the Gaussian mixture model, especially regarding task diversity representation?\n2. What analogous insights can be drawn from this framework when applied to other domains or models beyond language models?\n3. Have the authors considered potential variations in task retrieval risk patterns with different types of pretraining data distributions beyond the assumed noisy task distributions?"}}
{"paper_id": "hRos9WldRK", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper proposes a robust method, Learning to Bootstrap (L2B), which significantly enhances model robustness against label noise without depending on a clean validation set. Its integration into existing noisy label learning techniques and empirical validation on well-known datasets like CIFAR-10, CIFAR-100, ISIC2019, and Clothing 1M showcase its practical applicability. The dynamic reweighting mechanism that accounts for both real and pseudo-labels offers substantial innovation in handling label noise.", "Weaknesses": "The reliance on the meta-learning framework may introduce additional computational overhead, potentially limiting scalability. The proposed approach may encounter challenges in effectively tuning the meta-learning parameters across varied real-world scenarios, potentially affecting its generalizability. The paper may lack sufficient analysis of the influence of varying noise levels on the performance of the proposed method.", "Questions": "1. How does the proposed method handle extreme cases of label noise where a substantial portion of the dataset is mislabeled?\n2. What is the computational cost of the L2B method compared to traditional approaches, and how does it scale with larger datasets?\n3. Has the effect of different types of label noise (e.g., class-dependent vs. instance-dependent noise) been considered, and how does L2B perform under these conditions?"}}
{"paper_id": "ttRSBZiCiu", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel approach to speech synthesis that effectively reduces computational overhead while maintaining high audio quality. The two-stage adversarial training mechanism is original and shows competitive results, particularly the achievement of comparable quality with only one inference step. The presentation of the model is clear, and the modular design with attention blocks enhances performance.", "Weaknesses": "The paper may lack detailed comparisons against the latest state-of-the-art models in certain scenarios or datasets. Potential limitations could include the robustness of the model against varied audio input conditions and the adaptability of the model to other audio synthesis tasks.", "Questions": "How does the performance of WaveFluid compare across different datasets and audio contexts? Are there specific scenarios where the model struggles that were not addressed in the experiments? Can the framework be easily extended to other domains beyond speech synthesis?"}}
{"paper_id": "WYsLU5TEEo", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The proposed framework effectively combines interpretability and adversarial robustness in neural classifiers, using GANs to generate counterfactuals which serve dual purposes. The integration of these features is innovative, addressing a gap where both aspects are typically handled separately. The framework's ability to achieve competitive IoU scores and resist adversarial attacks is a significant empirical strength.", "Weaknesses": "While the results are promising, the method's effectiveness may be context-specific and dependent on the datasets chosen. The paper could provide more clarity on how well the method generalizes across broader types of data. The complexity of the approach, involving specialized architectures and multiple training phases, might limit accessibility and ease of replication.", "Questions": "1. How does the framework perform with datasets and models beyond the ones specifically tested, particularly in non-image domains?\n2. Could the use of the discriminator's output for measuring prediction uncertainty be further explored or quantified in terms of its accuracy or reliability?\n3. Were there any specific challenges or limitations encountered in the application of the cycle-consistent loss, and how were they addressed?"}}
{"paper_id": "KKZaj2QS3G", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a well-structured method for time series analysis that efficiently addresses noise resilience with a novel sampling strategy and an encoder design that ensures robustness and scalability. The proposed CoInception framework demonstrates strong performance across different types of tasks and datasets, showcasing its versatility and adaptability. The integration of Dilated Convolution with Inception blocks is well-justified, and the hierarchical triplet loss is an innovative approach to enforce consistency in noisy environments.", "Weaknesses": "The paper might benefit from a more extensive discussion on the potential limitations of the CoInception framework, particularly regarding the applicability of the noise-resilient sampling strategy to time series with very different spectral properties. Additionally, the computational overhead associated with the use of Inception blocks and the hierarchical triplet loss could be further clarified.", "Questions": "How does the complexity of the CoInception framework, specifically the use of Inception blocks, affect the training time and resource requirements compared to simpler architectures? Is the noise-resilient sampling strategy adaptable for real-time applications where noise characteristics can change dynamically?"}}
{"paper_id": "LWDRiFzbHQ", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel method, the Vicinal Risk Proxy (VRP), which effectively integrates responses from neighboring samples to improve the reliability of generalization assessments, addressing the limitations of existing unsupervised risk measurements.\n2. The methodology is well-grounded and demonstrates a strong correlation with model accuracy on out-of-distribution test sets, providing a practical solution for evaluating model generalization without the need for ground truth labels.\n3. Experimental results across multiple datasets and scenarios highlight the robustness and applicability of the proposed approach, establishing its value over traditional isolated sample-based measures.", "Weaknesses": "1. While the VRP method offers a significant improvement, the paper could benefit from a clearer presentation of the underlying mathematical framework and the computational complexity involved in computing the vicinal assessments.\n2. The presentation of the paper could be improved, especially in terms of explaining the vicinal assessment process with diagrams or illustrative examples to aid reader understanding.\n3. There is limited discussion on the potential scalability issues when applying the VRP to very large datasets, as the approach relies on neighboring sample relationships.", "Questions": "1. How does the VRP method handle cases where the density of nearby samples is very low, potentially impacting the accuracy of the similarity-weighted sum used in the risk proxy?\n2. Are there any specific limitations or edge cases identified where VRP might not provide reliable generalization assessments compared to traditional methods?\n3. Could the authors elaborate on how the choice of similarity metric impacts the performance of VRP, and whether different metrics were tested during experimentation?"}}
{"paper_id": "Yp01vcQSNl", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces an innovative approach to enhancing graph transformer models by effectively incorporating edge directionality, which is crucial for many real-world applications involving directed graphs. The dual encodings and dynamic attention mechanisms are novel contributions that improve the flexibility and applicability of graph transformers. The extensive evaluation on directed graph datasets highlights the method's effectiveness.", "Weaknesses": "The presentation could be improved for better clarity, particularly in explaining the implementation details and the mathematics behind dual encodings and the dynamic attention framework. Some parts of the proposed architecture might require additional computational resources, which could hinder the scalability of the solution.", "Questions": "How does the proposed method handle scalability issues when applied to large-scale directed graphs? Are there any trade-offs in terms of computational efficiency versus accuracy when using DiGT on very large datasets? Could there be scenarios where static adjacency matrices might still be beneficial or necessary for certain tasks?"}}
{"paper_id": "LndMyiBl3n", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents SheAttack, which effectively addresses the limitations of existing graph adversarial attacks by functioning well under both homophilic and heterophilic graph settings. It introduces the innovative use of a Modified Silhouette Score (MSS) to guide edge perturbations without requiring label or model knowledge, making it broadly applicable and scalable. The method demonstrates high efficiency even under the restricted black-box attack setting.", "Weaknesses": "While the modified silhouette score is an innovative aspect, the paper may need a more thorough theoretical analysis to justify the choice of using an average inter-class distance as opposed to the minimal one. Additionally, the reliance on certain assumptions in calculating MSS could limit the method's adaptability in some unexpected graph types not yet tested.", "Questions": "1. Can the Modified Silhouette Score (MSS), as proposed, potentially be adapted or improved to handle more complex graph structures where node features are highly dynamic?\n2. How did the authors ensure the reproducibility of the results, and is there a benchmark or public dataset used for these evaluations that can be accessed by other researchers for verification?\n3. Would further experimentation under varied adversarial conditions provide additional insights into the method's limits?"}}
