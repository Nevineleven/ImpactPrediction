{"paper_id": "u3xwwfHmBC", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper proposes a novel model, Tri-Tense Former (TTformer), which effectively captures spatio-temporal dynamics by introducing three tense-specific attention modules. This is a significant contribution to the field of traffic forecasting.\n2. The proposed model successfully integrates a contrastive learning approach without relying on predetermined adjacency matrices, which improves prediction robustness in the presence of missing data.\n3. The model demonstrates strong empirical results, outperforming existing baseline models on realistic datasets.", "Weaknesses": "1. The paper may be limited by its reliance on self-reported exceptional performance, and further validation by independent researchers across more diverse datasets would strengthen its claims.\n2. The complexity introduced by the multiple tense-specific attention mechanisms might pose challenges in terms of computational efficiency and implementation details.\n3. The model's reliance on precise tuning of the attention-mediated workflow could make it less adaptable to other types of forecasting datasets without significant retraining.", "Questions": "1. How does the TTformer compare to other models in terms of computational resource requirements and scalability?\n2. Can the model be adapted for other domains where capturing multi-temporal dynamics is critical, and not just restricted to traffic forecasting?\n3. How sensitive is the model to different levels of data sparsity and noise in comparison to other state-of-the-art approaches?"}}
{"paper_id": "YKvBiRWdQC", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces the Overcooked Generalisation Challenge, a novel benchmark aimed at assessing the zero-shot cooperation capabilities of reinforcement learning agents. This is a significant step forward in studying human-AI cooperation in novel environments with novel partners.\n2. The challenge builds on popular concepts of multi-agent reinforcement learning, unsupervised environment design, and dual curriculum design, pushing the boundaries of real-world applications in AI cooperation.\n3. The authors provide an open-source, GPU-accelerated environment that is accessible and versatile, facilitating further research and development in this important field.\n4. The study showcases an extensive evaluation using various state-of-the-art dual curriculum design algorithms, revealing their limitations and highlighting areas for future improvement.", "Weaknesses": "1. The complexity of the benchmark and the inherent difficulty in generating suitable training environments complicate the implementation, which might impose a high entry barrier for researchers less familiar with the used methodologies.\n2. While the benchmark provides a foundation for studying zero-shot cooperation, it might not currently address other critical aspects such as reasoning about other agents' mental states or long-term strategic planning, which are also pivotal for successful cooperation.\n3. It remains challenging to ensure diverse and sufficiently complex environments that mimic real-world scenarios effectively due to the resource constraints imposed by environment scaling requirements.", "Questions": "1. How do the introduced generalisation tasks translate to real-world applications beyond the Overcooked setting? Are there plans to validate the results with human participants?\n2. What are the potential strategies for integrating reasoning about other agents into the existing framework, and how might that impact the benchmark results?\n3. Given the significance of environment complexity in determining agent success, what strategies or best practices do the authors suggest for designing training environments that can better prepare agents for real-world scenarios?"}}
{"paper_id": "tePFpDgyqg", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The introduction of long-distance referrals as a key metric for long-context understanding is novel and well-motivated. 2. The LongPack pipeline effectively constructs long-context training data by leveraging naturally occurring hyperlink relationships, improving training efficiency. 3. The proposed methods are thoroughly evaluated and show significant improvements over state-of-the-art baselines.", "Weaknesses": "1. The paper heavily relies on hyperlink data for data construction, which may not be applicable or effective for datasets without hyperlink information. 2. The scalability of the approach is limited by the initial sample size and hyperlink availability in the RefinedWeb dataset. 3. Although the methodology is promising, real-world efficacy may vary depending on the diversity of the initial samples.", "Questions": "1. How does LongPack handle the potential issue of hyperlink overload or irrelevant hyperlinks that might not indicate true semantic referrals? 2. Is there a mechanism to validate the semantic relevance of hyperlinks in the constructed long documents to ensure quality? 3. Could LongPack be adapted for datasets that do not rely on web pages or have limited hyperlinks?"}}
{"paper_id": "gx3LMRB15C", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces an innovative augmentation-free self-supervised learning approach specifically designed for tabular data, which is a significant contribution given the challenges faced in this area.\n2. T-JEPA demonstrates substantial improvements in performance on both classification and regression tasks, suggesting its potential to outperform traditional and deep learning methods on structured data.\n3. The paper provides a thorough empirical evaluation across multiple datasets, reinforcing the robustness and scalability of the proposed method.\n4. The regularization tokens proposed as a novel method to escape collapsed training regimes is an insightful addition, especially for JEPA-based architectures.", "Weaknesses": "1. The method's specific dependence on transformer-like architectures might limit its applicability across a broader range of models, thus affecting its generality.\n2. While the work introduces regularization tokens, there could be more theoretical insights provided into why these tokens effectively prevent collapse, which could strengthen the foundational understanding of the approach.\n3. There are potential limitations in the context of real-world tabular data where feature engineering and feature selection are highly domain-specific tasks.", "Questions": "1. Can T-JEPA be integrated with other non-transformer architectures effectively, or is its application inherently limited by its design?\n2. What are the potential impacts or alterations required if T-JEPA were to be adapted for anomaly detection tasks, given its reliance on latent feature prediction?\n3. How does the regularization token influence the embedding space during the initial stages of training compared to later stages?"}}
{"paper_id": "kTH3bEH6hW", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel range-limited augmentation technique specifically for contrastive learning in tabular data, which is highly innovative given the relatively limited attention tabular data receives compared to other data modalities.\n2. The introduction of the FESTA benchmark offers a significant resource to the community, evaluating 31 algorithms across 42 datasets, making it the largest benchmark for few-shot learning in tabular domains.\n3. Results demonstrate that their method consistently outperforms existing methods across various scenarios, highlighting its effectiveness in few-shot contexts.\n4. The experiments are thorough, covering a wide range of settings, including ablation studies for hyperparameters, which strengthen the validity of the results.", "Weaknesses": "1. While the augmentation method shows promising results, the reliance on predefined feature ranges might introduce dependency on the initial choice of these ranges, potentially limiting generalizability without careful tuning.\n2. The approach focuses on numerical features and does not address augmentations tailored for categorical features, which might limit applicability in datasets where categorical data is predominant.\n3. Despite the comprehensive benchmark, the paper could explore more comparisons with other state-of-the-art methods, especially transformer-based models that are gaining traction in tabular data tasks.", "Questions": "1. How sensitive are the results to the choice of feature range boundaries? Could automated methods to determine optimal feature ranges further enhance performance and generalizability?\n2. The methodology focuses on numerical features; are there plans to extend this work to incorporate categorical data augmentations effectively?\n3. Considering the noted lack of significant advantage for transformer-based models over MLPs, might there be factors other than model architecture alone that influence performance in few-shot scenarios within tabular data?"}}
{"paper_id": "EqcLAU6gyU", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 3, "Strengths": "1. The paper introduces a novel agent-oriented planning framework in multi-agent systems, which effectively decomposes tasks and allocates them to appropriate agents. 2. The design principles of solvability, completeness, and non-redundancy are clearly articulated and guide the proposed solution. 3. Extensive experiments demonstrate the framework's improvements over existing methods in terms of accuracy and efficiency.", "Weaknesses": "1. The reliance on human-annotated data for the reward model could pose scalability issues. 2. The specific mechanisms for handling uncertain or ambiguous tasks could have been explored further.", "Questions": "1. How does the framework handle dynamic changes in the availability or capability of agents in real-time? 2. What are the performance implications when scaling to a larger number of agents and more complex tasks?"}}
{"paper_id": "qrTrnrEi9d", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "TransFusion effectively leverages machine translation to improve information extraction in low-resource languages by utilizing English translations and annotations, achieving superior cross-lingual performance. Comprehensive empirical results across 12 datasets and 50 languages underscore its efficacy.", "Weaknesses": "The framework relies heavily on the quality of machine translation systems, which could limit performance if translation quality is poor. Additionally, the method's applicability might be restricted when high-quality translation for a language is unavailable.", "Questions": "How does the approach handle cases where translation quality significantly varies across language pairs? Is there a strategy for dynamically integrating different translation systems based on target language specifics?"}}
{"paper_id": "Bp0HBaMNRl", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel differentiable causal discovery method that addresses the challenge of identifying causal structures with latent variables, extending traditional methods with theoretical guarantees for nonlinear latent hierarchical models under relaxed conditions.\n2. It successfully combines continuous optimization techniques with causal discovery, improving both scalability and accuracy, demonstrating practical utility with large datasets.\n3. Comprehensive experimental evaluations on synthetic and real datasets indicate improved performance against existing baselines, underscoring the approach’s effectiveness.", "Weaknesses": "1. Despite the advancements made, the method still imposes certain structural conditions, such as requiring each latent variable to have at least two pure children, which might limit the applicability in situations where these conditions are not met.\n2. The results are heavily reliant on created conditions under which the theoretical aspects hold, potentially limiting their applicability to more diverse and challenging real-world datasets.\n3. There might be limitations in scenarios where measured variables act as children in the causal graph, as the method currently does not fully account for these structures.", "Questions": "1. Can the proposed method be extended to accommodate structures where measured variables have children in the causal graph?\n2. Is there potential to relax the requirement of 'two pure children' for latent variables while maintaining identifiability, thus broadening applicability?\n3. How sensitive is the method to the initial assumptions about latent structures, and how does this sensitivity affect practical deployment in varied domains?"}}
{"paper_id": "JzLcKWtGnl", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel approach to enhancing spatial perception and reasoning in 3D vision-language models by enriching spatial embeddings, which is a significant advancement in the field.\n2. Two new tasks, 3D object distance measurement and 3D layout editing, are proposed, accompanied by a large dataset (MODLE), which adds substantial value and utility to the research community.\n3. The experimental results demonstrate the effectiveness of the proposed methods across multiple 3D vision-language tasks, showcasing state-of-the-art performance.", "Weaknesses": "1. The sophisticated computational processes involved, such as the Progressive Spatial Awareness Scheme, require significant computational resources, which may limit practical applicability.\n2. The dependence on comprehensive training datasets could affect the model’s generalizability, especially when dealing with more diverse or complex 3D scenes not present in the training data.", "Questions": "1. Can the proposed Spatial 3D-LLM framework be effectively scaled down for applications requiring real-time performance, such as in robotics or VR systems?\n2. Is there a strategy in place to enhance the generalizability of the model to 3D scenes that may not have been covered in the current dataset, especially those with rare or unseen spatial configurations?"}}
{"paper_id": "hXJrQWIoR3", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel problem of representation-level explainable graph learning, which is important for understanding graph representations.\n2. It presents two innovative methods, leveraging graph pattern analysis—ensemble graph kernels and a pattern analysis GNN framework, both of which enhance the explainability of graph representation learning.\n3. Theoretical analysis on robustness and generalization provides solid foundational support for the proposed methods.\n4. Extensive empirical evaluations show that the method is effective and competitive in both supervised and unsupervised settings.", "Weaknesses": "1. The methods can be computationally expensive due to the high time and space complexity associated with processing large datasets.\n2. While the paper provides a solid explanation model, practical applications might be limited by the scalability challenges highlighted in its complexity analysis.", "Questions": "1. How do the proposed methods handle extremely large graphs, given the high computational complexity?\n2. Could the choice of patterns be expanded beyond the ones tested in the experiments, and what impact might this have on performance?"}}
{"paper_id": "67sSPPAZiG", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents a comprehensive approach to egocentric video understanding by introducing a large-scale egocentric QA dataset, a novel Memory Pointer Prompting mechanism, and a challenging benchmark for model evaluation. The proposed MM-Ego model demonstrates significant improvements in understanding and memorizing visual details from egocentric videos across varying lengths.", "Weaknesses": "The methodology relies heavily on a specific data engine and the handcrafted theme of the Memory Pointer Prompting mechanism. The performance gain is primarily showcased on a specifically curated benchmark, leaving questions about the general applicability across different video types and scenarios.", "Questions": "How does the performance of MM-Ego compare when applied to non-egocentric video datasets? Are there plans to extend the model to incorporate more diverse video content and scenarios?"}}
{"paper_id": "kjmLabjSE2", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper provides significant improvements in understanding the differential privacy guarantees of Noisy-SGD without the restrictive assumptions of convexity and smoothness. Through innovative analysis using H¨older continuous gradient assumptions and improved Wasserstein distance tracking, it advances privacy bounds, enhancing applicability to a broader range of loss functions. It also offers a tighter privacy bound for smooth strongly convex losses compared to existing methods.", "Weaknesses": "The presentation of the technical details is dense and could be made more accessible to a broader audience. The complex mathematical derivations might hinder comprehension without additional explanations or examples. Moreover, the practical implications of the proposed theoretical advancements on real-world datasets or large neural networks are not extensively explored.", "Questions": "How does the proposed method perform in practice with neural networks trained on real-world datasets? Are there any specific applications where the relaxed assumptions yield substantial improvements in privacy guarantees? What are the implications of the findings on the time and space complexity of implementing these methods?"}}
{"paper_id": "I18MA5DjoP", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces the Neuron Predictability Lens (NPL), offering a novel perspective to analyze neuron activation patterns in large language models (LLMs), which could guide future studies on model interpretability and optimization. 2. It provides significant insights into neuron behaviors across layers, showing the distinct roles and contributions to model performance. 3. The framework suggests practical applications for optimizing LLMs, such as inference acceleration and mechanism analysis.", "Weaknesses": "1. The study mainly focuses on transformer-based LLMs and may not be readily applicable to other architectures. 2. The paper lacks extensive empirical testing across diverse datasets and tasks that could validate the general applicability of the proposed method. 3. The description of the experimental setup and results could be more detailed for better reproducibility.", "Questions": "1. Can the Neuron Predictability Lens be adapted for other neural network architectures or is it inherently tied to the transformer architecture? 2. How sensitive is the NPL framework to variations in data inputs or model architectures? 3. Can the method be extended to explain neuron functions or roles in transformer blocks beyond FFNs?"}}
{"paper_id": "qpDqO7qa3R", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper presents a novel zero-shot video restoration framework that leverages pre-trained image restoration diffusion models, effectively eliminating the need for extensive retraining.\n2. It introduces innovative techniques like hierarchical latent warping and hybrid flow-guided, spatial-aware token merging which significantly improve both the temporal consistency and quality of restored videos across various tasks.\n3. The work demonstrates state-of-the-art performance in extreme scenarios and shows excellent generalization across different datasets and degradation types.\n4. The presentation of the work is clear and the experimental evaluation is thorough, covering multiple degradation scenarios and video restoration tasks.", "Weaknesses": "1. Despite the robust approach, limitations are acknowledged in dynamic scenes where latent decoder sensitivity might lead to flickering and in handling videos with extreme degradation, which remains challenging.\n2. While the method is applicable to various tasks, it relies on existing diffusion models, which might pose limitations if such diffusion models are not available or optimized for certain video processing tasks.", "Questions": "1. How does the method perform in comparison to diffusion models that are specifically fine-tuned for video restoration tasks in terms of computational efficiency?\n2. Can the hierarchical latent warping and token merging techniques be applied independently for different use cases, or do they have to be used in conjunction to achieve the reported results?\n3. How does the framework handle scenes with rapid dynamic movements, and are there strategies in place to address the potential limitations in such scenarios?"}}
{"paper_id": "dSQtMx6dPE", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper provides a novel and effective approach to address privacy risks in graph prompt learning using differential privacy methods specifically adapted for graph data. DP-GPL and DP-GPL+W are shown to achieve strong utility at high privacy levels, which is a significant advancement in ensuring privacy in GNN applications.", "Weaknesses": "While the proposed methods show promising results, the complexity of the privacy analysis for DP-GPL+W due to differential weights may pose challenges in broad adoption and requires further examination. The empirical evaluation is thorough but may benefit from additional real-world case studies to demonstrate applicability outside of experimental settings.", "Questions": "How does the proposed DP-GPL and DP-GPL+W framework generalize to different graph datasets with varying structures and sizes? Are there specific parameter tuning strategies provided for different dataset characteristics?"}}
{"paper_id": "GbgCRJedQ7", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel approach called Sparse Matrix Tuning (SMT) that successfully addresses the performance gap between parameter-efficient fine-tuning methods and full fine-tuning.\n2. Experiments demonstrate that SMT provides significant memory savings and computational efficiency while achieving accuracy that rivals full fine-tuning.\n3. The work includes a comprehensive evaluation on large language models and diverse benchmarks, solidifying the method's utility and performance.", "Weaknesses": "1. The selection criteria and methodology for sparse matrices might need further theoretical backing or broader exploration to ensure robustness across tasks not included in the experiments.\n2. While the paper covers a range of tasks, it focuses largely on the LLaMA series, and results might not generalize across all model architectures or tasks without further validation.", "Questions": "1. Can the SMT method be adapted for other architectures beyond LLaMA, and if so, what adjustments might be necessary?\n2. What is the potential impact of SMT on pre-training phases of LLMs, if extended beyond fine-tuning?\n3. How does SMT perform on more complex tasks involving multi-modal inputs or outputs? Does the sparsity of matrices selected affect its applicability?"}}
{"paper_id": "ZZVOrId3yN", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "CrossModalNet presents a novel architecture that effectively integrates multimodal data streams, addressing both information preservation and domain adaptability issues. The work is backed by rigorous mathematical proofs and validated with compelling experimental results, demonstrating superior performance over existing models.", "Weaknesses": "The paper provides an in-depth theoretical analysis, but it would benefit from a broader exploration of real-world clinical applications to further validate the robustness and versatility of the model.", "Questions": "How adaptable is the CrossModalNet framework to other medical segmentation tasks beyond cardiac MRI segmentation? Are there any planned extensions or potential challenges in generalizing this approach?"}}
{"paper_id": "gLHuAYGs6a", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The proposed heterogeneous random walk strategy offers a novel approach to capturing high-order sample structures across multiple views, which contributes to effective multi-view clustering.\n2. Extensive experiments on five real-world datasets demonstrate the superiority of the proposed SMvCNet method compared to existing multi-view clustering models.", "Weaknesses": "1. The complexity and computational cost associated with the heterogeneous random walks and the proposed network may limit scalability to even larger datasets.\n2. The paper could benefit from a deeper discussion on the limitations of the approach and potential avenues for optimization or expansion.", "Questions": "1. How does the performance of the proposed method compare on much larger or more complex datasets beyond the five tested?\n2. Are there specific scenarios or types of datasets where the proposed approach might underperform compared to traditional methods?"}}
{"paper_id": "x5l5PRtvul", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. The paper effectively demonstrates the capability of h-SVGD to alleviate variance collapse without additional computational cost, which is a significant enhancement over existing SVGD methodologies.\n2. The study provides both theoretical insights and empirical results, showcasing the practical benefits of h-SVGD on high dimensional inference tasks.\n3. New theoretical contributions regarding the existence of solutions to the hybrid Stein PDE are presented, offering a meaningful extension of SVGD theoretical framework.", "Weaknesses": "1. The paper acknowledges that h-SVGD does not converge to the target distribution in the mean field limit, indicating a potential limitation in its applicability for exact inference tasks.\n2. The connection between the theoretical results and empirical outcomes could be articulated more clearly to strengthen the insights drawn from the experiments.\n3. The method relies on specific kernel conditions which might not be straightforward to generalize to all possible application scenarios.", "Questions": "1. Can the authors elaborate on scenarios where the variance adjustment mechanism of h-SVGD might fail, despite its effectiveness in mitigating variance collapse?\n2. Is it possible to quantify under what conditions or assumptions h-SVGD can serve as a reliable approximation even though it doesn't exactly match the target distribution in the mean field limit?\n3. How might the theory and approach adapt if extended beyond Gaussian mixture models and Bayesian neural networks into other complex distributions?"}}
{"paper_id": "pOUAVXnOQP", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces a novel activation function, STAF, which effectively reduces spectral bias, demonstrating significant performance improvements in various signal reconstruction tasks.\n2. Extensive experimental evaluation shows superiority over state-of-the-art methods, evidenced by higher PSNR values and faster convergence.\n3. Theoretical contributions in the form of initialization schemes and expressive power analysis are well-developed and add substantial value to the approach.", "Weaknesses": "1. The paper heavily focuses on theoretical aspects, which might lead to less accessible implementation details for practitioners not deeply versed in theoretical mathematics.\n2. The dependency on a careful selection of initialization strategies and hyperparameters might suggest that STAF's practical advantages are somewhat contingent on meticulous parameter tuning.", "Questions": "1. How does STAF perform in real-world applications with less controlled environments and higher data complexity beyond the tested domains?\n2. What are the impacts of potential overfitting due to the increase in the number of trainable parameters, and how can they be effectively mitigated?\n3. Could there be clear guidelines on the selection of hyperparameters for STAF in various signal processing tasks, or do they need to be specifically tuned for each application?"}}
{"paper_id": "iN64nSYt0z", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. GUIDE introduces an effective methodology to enhance attention scores of important instructions within prompts, leading to improved alignment with user directives without additional computational burden.\n2. The experimental results convincingly show that GUIDE can significantly enhance LLM adherence to instructions, surpassing natural prompting and even supervised fine-tuning approaches under certain conditions.\n3. Influence represents a novel metric that contributes to understanding token impact propagation through the layers of a model, aiding in calibrating the attention enhancement parameters in GUIDE.", "Weaknesses": "1. Despite its potential, the implementation of GUIDE relies on fixed biases (like ∆ values), which may require careful tuning for different contexts and models.\n2. Although GUIDE is designed to reduce resource requirements, detailed analysis on how it affects different types of instructions or larger-scale applications is limited.\n3. The presentation could be further improved for clarity in some sections, benefiting from a clearer delineation of method steps and assumptions.", "Questions": "1. How does GUIDE perform across different types of tasks outside of summarization and information retrieval, such as more complex reasoning tasks?\n2. Are there specific limitations observed in the Influence metric's capacity to calibrate DELTA across diverse models or instruction types?\n3. Is there a plan to extend the GUIDE framework to multi-modal or cross-lingual LLM applications?"}}
{"paper_id": "gRuZkEy49k", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper effectively adapts Monte Carlo Tree Search (MCTS) to improve Generative Flow Networks (GFlowNet) training, showing significant improvement in data sampling efficiency and learning outcomes.\n2. The proposed Monte Carlo DAG Search (MCDS) is shown to be widely applicable across different GFlowNet parameterizations and reward environments.\n3. The paper provides a thorough theoretical justification alongside extensive empirical evidence supporting the efficacy of the proposed methodology across diverse benchmarks and tasks.", "Weaknesses": "1. The MCDS technique requires parameterization of the state flow, which limits its applicability to specific GFlowNet parameterizations like DB, SubTB, and FM.\n2. The practical implementation of MCDS can be inefficient in dynamic environments, as it may involve significant computational overhead across iterations.\n3. The paper does not deeply explore hybrid techniques or further optimizations that could enhance the applicability of MCDS beyond its current scope.", "Questions": "1. Can the proposed MCDS be integrated with a wider variety of GFlowNet parameterizations, including TB, and does the current research suggest possible pathways to achieving that?\n2. How does MCDS compare to traditional MCTS in computational efficiency, particularly in larger and more complex environments?\n3. What are the potential limitations or challenges when applying MCDS in real-world applications beyond the tested benchmark tasks?"}}
{"paper_id": "d23EVDRJ6g", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. MotionDreamer introduces an innovative localized generative masked transformer for motion synthesis, offering a novel approach to creating diverse animations from a single reference motion.\n2. The method demonstrates strong performance across various metrics, outperforming state-of-the-art GAN or Diffusion-based models.\n3. The paper includes comprehensive experiments that highlight the effectiveness of the proposed approach, including its application to downstream tasks such as temporal editing and beat-aligned dance generation.", "Weaknesses": "1. While the method shows promise, the reliance on a single reference motion may limit its applicability to broader datasets or scenarios where multiple reference motions are available.\n2. The paper could benefit from more in-depth analysis or visualization of failure cases to better understand the limitations of the proposed approach.", "Questions": "1. How does the model generalize to sequences that differ significantly from the training instance in terms of style or context?\n2. What are the potential limitations of using a sliding window local attention mechanism compared to standard global attention techniques?\n3. Can the proposed localized generative masked transformer be adapted for tasks outside of motion synthesis, such as in other content generation domains?"}}
{"paper_id": "kQ5s9Yh0WI", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper presents a novel agent-based methodology, AgentWrite, that effectively addresses the limitation of output length in long-context LLMs. \n2. The introduction of the LongWriter-6k dataset is a significant contribution, enabling models to generate outputs exceeding 10,000 words while maintaining quality. \n3. Comprehensive experiments support the claims, showcasing state-of-the-art performance in ultra-long text generation.", "Weaknesses": "1. The dependency on the AgentWrite pipeline may pose limitations in real-world applications due to its sequential nature, which can increase inference time and reduce efficiency. \n2. The methodology relies heavily on the availability of high-quality SFT data with long outputs, which may not always be feasible to collect or create.", "Questions": "1. How does the AgentWrite pipeline perform in terms of inference time and computational efficiency compared to standard methods? \n2. Can the approach to extend output lengths be generalized to other structural types of data or is it specific to textual information only?"}}
{"paper_id": "9CqkpQExe2", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper proposes a novel adaptive routing strategy, Ada-K, for Mixture-of-Experts models, improving both computational efficiency and performance.\n2. The experimental results demonstrate significant improvements in reducing FLOPs and speeding up inference, while maintaining or increasing model performance.\n3. The Ada-K strategy is versatile and can be integrated into mainstream MoE-based LLMs, highlighting its broad applicability.", "Weaknesses": "1. While the Ada-K routing strategy offers clear efficiency benefits, the paper could further explore potential limitations or scenarios where its application might not be as effective.\n2. The reliance on reinforcement learning and the Proximal Policy Optimization algorithm introduces additional complexity, requiring careful tuning of hyperparameters, which might hinder easy adoption.", "Questions": "1. How does the Ada-K routing strategy perform on tasks with extremely high variance in token importance? Are there any scenarios where a fixed Top-K approach might still be preferable?\n2. Can the Ada-K strategy be extended or adapted for other types of models beyond MoE-based LLMs? If so, what considerations would need to be addressed?"}}
{"paper_id": "PpYy0dR3Qw", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "LoCoDL introduces a communication-efficient distributed learning algorithm effectively combining local training and compression, achieving doubly-accelerated communication complexity. Its practical performance surpasses other algorithms such as DIANA and ADIANA. The paper presents rigorous theoretical analysis and extensive experiments demonstrating superiority across various settings.", "Weaknesses": "The paper relies heavily on the assumption of a strongly convex setting and does not address nonconvex problems, limiting its applicability. Also, while the theoretical contributions are robust, the practical implementation details for realistic deployment scenarios could be further elaborated.", "Questions": "How does LoCoDL perform in nonconvex settings, and are there planned adaptations for such cases? Can LoCoDL be combined with bidirectional compression and stochastic variance reduction, as briefly mentioned in the conclusion, to further enhance its efficiency?"}}
{"paper_id": "UatDdAlr2x", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper provides a deep theoretical and experimental exploration of how transformer components interact to solve the histogram task, which is of fundamental interest for understanding neural network interpretability.\n2. The authors identify two distinct counting strategies, relation-based and inventory-based, which offer insights into how transformers can behave distinctly based on hyperparameter settings.\n3. The work opens up possibilities for better understanding and potentially improving transformer architectures by carefully adjusting hyperparameters and architectural choices.", "Weaknesses": "1. The applicability of the findings to more complex models, such as multi-layer or autoregressive transformers, is not clear, potentially limiting the broader impact of the study.\n2. The reliance on precise theoretical constructions may not account for nuances in real-world, noisy settings where ideal conditions cannot be realized.\n3. Some results hinge critically on specific parameter settings, which might not translate well across different tasks or real data distributions.", "Questions": "1. How might the findings influence the design and training of larger, more complex transformer models, particularly LLMs?\n2. Could the mechanism of the softmax activation being a 'curse or a blessing' be further explored to determine more robust design principles?\n3. Are there plans to extend the study to tasks with different complexity levels, and what adjustments might be necessary for those cases?"}}
{"paper_id": "IwgmgidYPS", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "1. The paper introduces MedTrinity-25M, a large and diverse multimodal medical dataset with extensive multigranular annotations, addressing a significant gap in medical dataset resources.\n2. It proposes an automated pipeline that scales up multimodal data by generating detailed image-ROI-description triplets without paired text descriptions, demonstrating technological innovation in data synthesis.\n3. The dataset demonstrates potential through extensive evaluation, showing improved performance of models trained on MedTrinity-25M across multiple medical VQA benchmarks.\n4. The presentation of the methodology and results is clear and detailed, providing a comprehensive understanding of the contributions and findings.", "Weaknesses": "1. The reliance on an automated pipeline might introduce errors or biases not present in human-annotated datasets, potentially affecting data quality.\n2. The paper is heavily focused on demonstration through VQA benchmarks, and the broader applicability of the dataset across other domains or tasks is not fully explored.\n3. The requirement of advanced retrieval and generation models may limit the accessibility and reproducibility of the pipeline for wider adoption without specialized resources.", "Questions": "1. How does the quality of annotations generated by the pipeline compare qualitatively and quantitatively to human-annotated datasets?\n2. Are there plans to address and quantify the bias and error rate in the automated annotation process?\n3. What are the plans for ensuring the dataset's accessibility and usability for a wide range of users in the research community?"}}
{"paper_id": "WG7GzGx3G9", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper addresses a crucial problem in the deployment of large language models by proposing a training-free method to enhance the performance of INT4 inference. \n2. The method, Rotated Runtime Smooth, improves accuracy for quantized models while maintaining low computational overhead. \n3. Extensive experiments on leading foundational models demonstrate solid performance improvements.", "Weaknesses": "1. The novelty of the approach could be challenged as it builds upon existing concepts of rotation and smoothing, though it applies them effectively. \n2. Some practical limitations of the method are not fully explored, especially issues with rare or unlikely scenarios of outlier distributions where the methodology might underperform.", "Questions": "1. How does the performance of Rotated Runtime Smooth compare to other quantization approaches in real-world applications beyond benchmarks? \n2. Are there scenarios where the proposed method might introduce overheads comparable to the latency it aims to mitigate?"}}
{"paper_id": "BhECSDSkAE", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper addresses a novel task of one-shot medical video object segmentation using static image datasets, which is important for reducing annotation costs in medical imaging. \n2. The introduction of OS-I2V-Seg, a comprehensive dataset, provides valuable data for further research in this domain. \n3. The proposed method is well-validated through thorough experiments, demonstrating its efficacy in leveraging limited annotations and adapting to spatiotemporal contexts.", "Weaknesses": "1. While the task is novel, the proposed test-time training strategy might not be fundamentally groundbreaking given existing work in temporal-aware models and memory mechanisms. \n2. The paper relies on the assumption that the first frame annotation is available, which may not always be feasible in certain medical situations. \n3. The dependency on specific datasets and the selection of similarity measures may limit the generalizability of the approach across diverse medical imaging tasks.", "Questions": "1. Are there any particular medical imaging tasks or scenarios where the proposed method may not perform well? \n2. Could the memory mechanism be further optimized to better handle longer video sequences or more complex temporal dependencies? \n3. Is there a potential for extending this approach to domains outside of medical imaging, or are there inherent limitations to its applicability?"}}
{"paper_id": "mnB4hDTIDr", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. The paper introduces DCScore, a novel classification-based approach to evaluate the diversity of LLM-generated datasets, which is an under-explored area.\n2. DCScore demonstrates lower computational costs and aligns well with diversity axioms, showing a strong correlation with pseudo-truths like generation temperature and human judgment.\n3. The approach is theoretically grounded, and the experiments demonstrate its efficiency and effectiveness compared to existing diversity evaluation methods.", "Weaknesses": "1. The paper’s novel contribution is somewhat incremental, focusing on refining diversity evaluation techniques rather than presenting a fundamentally new theory or application.\n2. While DCScore outperforms in many scenarios, its dependency on selecting appropriate hyperparameters like classification temperature could be a limitation for general applicability.\n3. Empirical validation relies heavily on correlation with pseudo-truths and assumptions about diversity, which might not fully capture all aspects of dataset diversity.", "Questions": "1. How robust is DCScore to changes in hyperparameter settings, especially across different dataset domains?\n2. Could DCScore be integrated into an automated dataset generation pipeline to provide real-time feedback for improving diversity?\n3. How does DCScore handle cases where identical content exists in different contextual settings within generated datasets, affecting diversity assessment?"}}
{"paper_id": "Y0qmwm6tgy", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper presents a novel pruning method, MoreauPruner, which addresses the robustness to weight perturbations—a previously neglected aspect in pruning literature.\n2. The theoretical foundation of MoreauPruner is well-developed, utilizing the Moreau envelope to ensure consistent pruning outcomes.\n3. Extensive experiments conducted on multiple state-of-the-art LLMs demonstrate the method's efficacy in maintaining model performance post-pruning.\n4. The method shows robust performance even under different weight formats, validating its practical utility.", "Weaknesses": "1. The experiments are limited to models with up to 13B parameters due to hardware constraints, which might not accurately represent performance on much larger models.\n2. The discussion around the impact of larger recovery sets and calibration sets is somewhat peripheral and could be expanded for deeper insights.", "Questions": "1. How does MoreauPruner perform in situations involving simultaneous use of other model compression techniques, such as knowledge distillation or quantization?\n2. Is there any expected change in the performance of MoreauPruner if applied to LLMs with more than 13B parameters, especially regarding computational efficiency and accuracy maintenance?"}}
{"paper_id": "aAcOaJYbUg", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper addresses an important gap in OOD robustness benchmarks by introducing LAION-C, which targets the limitations of previous benchmarks in the context of web-scale datasets.\n2. The construction of LAION-C involves six novel distortion types that are truly OOD for state-of-the-art models, providing a relevant and challenging benchmark.\n3. A comprehensive evaluation is conducted involving a large suite of models and human observers, demonstrating the benchmark's utility and introducing a paradigm shift in human versus machine performance.", "Weaknesses": "1. While the introduction of novel distortions is thorough, the paper doesn't provide detailed analysis on specific model architectures' strengths or weaknesses against particular types of distortions.\n2. The long-term sustainability and continuous relevance of LAION-C as a benchmark are not thoroughly addressed, especially in a rapidly evolving field.", "Questions": "1. How does LAION-C compare to other emerging benchmarks in terms of identifying the specific weaknesses of vision models?\n2. What are potential strategies for model improvements that researchers should explore based on LAION-C’s findings?\n3. How should future benchmark datasets be constructed to remain OOD with respect to continuously evolving web-scale datasets?"}}
{"paper_id": "duGygkA3QR", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper presents a novel integration of Dynamic Mode Decomposition (DMD) with Graph Neural Networks (GNNs), which effectively captures complex dynamics within graph-structured data. \n2. The approach is validated with extensive experiments across various tasks, including node classification, spatial-temporal prediction, and link prediction, showing state-of-the-art performance. \n3. The theoretical connections between DMD, Koopman operator theory, and GNN dynamics are well-established and provide a solid foundation for the proposed models.", "Weaknesses": "1. The paper could have provided more in-depth comparison with other advanced dynamical systems approaches in graph learning to highlight the advantages of using DMD more clearly. \n2. The requirement of selecting an initial feature dynamic may add complexity to the implementation and might not be straightforward for practitioners unfamiliar with DMD. \n3. Some of the experimental setups and parameters tuning (such as truncation rate in DMD) could be described in finer detail to facilitate reproducibility.", "Questions": "1. Can the proposed DMD-GNN framework be easily adapted to more diverse graph types beyond those explored, such as dynamic or evolving graphs? \n2. How sensitive are the DMD-GNNs to the choice of initial feature dynamics, and is there a systematic way to determine the optimal dynamic for a given graph task? \n3. What computational overhead does the integration of DMD introduce compared to traditional GNNs, and how does it scale with very large graphs?"}}
{"paper_id": "s6nYndMwG7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses the challenge of computing inverse Hessian-vector products efficiently for calculating influence functions in deep models. It provides a thorough theoretical analysis of the LiSSA algorithm, defining conditions under which it converges successfully. The experimental section validates these findings empirically, offering practical insights and recommendations for hyperparameter selection.", "Weaknesses": "The work largely focuses on the theoretical underpinnings of the LiSSA algorithm and its empirical validation, which might limit its practical applicability without specific guidance on how it compares with alternative methods beyond basic validation. Some parts of the explanation may be challenging to understand due to their technical nature and dense exposition.", "Questions": "1. How does the proposed approach compare to other techniques that circumvent iHVP, such as gradient-based methods, in terms of computational efficiency and accuracy?\n2. Are there any specific cases or scenarios where the proposed hyperparameter selection process might not provide reliable results?\n3. Could the methodology be applied directly to emerging large-scale models or are additional modifications necessary?"}}
{"paper_id": "9BVMD3keG8", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper provides a robust theoretical framework for understanding the role of contextual information in brokerage through online learning, offering optimal algorithms.\n2. It establishes tight regret bounds, proving both upper and lower bounds in both full and two-bit feedback scenarios, adding significant depth to the literature in contextual bilateral trade problems.\n3. The work elegantly connects contextual information exploitation to practical market tasks like price discovery, making it relevant to real-world applications.", "Weaknesses": "1. The assumptions regarding bounded densities of traders' valuations might be restrictive and may not always hold in real-world scenarios, which potentially limits the applicability of the results.\n2. The theoretical nature of the research could make the results less appealing to practitioners who are seeking immediately applicable solutions in diverse market settings.\n3. The paper might not consider other contextual factors which could influence trading strategies beyond simply linear contexts.", "Questions": "1. How sensitive is the proposed framework to variations in the bounded density L and dimensionality d? \n2. Can the model be extended to handle cases where noise distributions are non-linearly dependent on context or feature interdependencies among context vectors exist?\n3. How does the model perform in practical, real-world OTC scenarios, where assumptions might fail to hold strictly?"}}
{"paper_id": "mnna9LUg7P", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel quantization method tailored specifically for State Space Models (SSMs), addressing unique challenges such as sensitivity to activation quantization and distinct outlier patterns. The proposed method, Quamba, achieves efficient post-training quantization without requiring additional training, making it useful for real-world applications. It demonstrates promising results in reducing model size and inference latency while maintaining accuracy, particularly for resource-constrained settings.", "Weaknesses": "The paper could benefit from more detailed comparisons with state-of-the-art Transformer quantization methods to further solidify the claimed advantages. Additionally, while the approach is demonstrated to be effective for specific SSMs, its applicability to a wider range of models or tasks may require further exploration.", "Questions": "1. How does Quamba perform when applied to other types of State Space Models beyond Mamba and Jamba? \n2. Can the percentile-based activation clipping strategy adversely affect certain models or datasets? If so, how can this issue be mitigated?\n3. Are there any plans to extend Quamba's capabilities to support even lower bit-width quantization (e.g., INT4) for more aggressive resource constraints?"}}
{"paper_id": "3X3LuwzZrl", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel approach, Label Influence Propagation (LIP), which provides a comprehensive analysis of label correlations in multi-label node classification tasks. LIP effectively captures both first-order and high-order label influences.\n2. The method is compatible with a variety of GNN backbones, demonstrating its flexibility and adaptability across different graph-based tasks.\n3. Empirical results show significant improvements over state-of-the-art methods across a range of datasets, showcasing the proposed method's effectiveness and robustness.", "Weaknesses": "1. While the theoretical foundation regarding influence propagation is well-grounded, the model's reliance on specific GNN architectures could limit its applicability to other less-commonly used GNN models.\n2. The complexity of computations, especially those involving PageRank and high-order influence propagation, may pose challenges for large scales of data, potentially impacting scalability.", "Questions": "1. Is it feasible to extend the LIP framework to support dynamic or evolving graphs where label interactions might change over time?\n2. Given the reliance on the computation of high-order influences, how does LIP scale with increasingly large graphs, and are there optimizations that can be applied to reduce computational overhead?\n3. How sensitive is the LIP performance to the choice of specific GNN backbones, and would certain architectures provide better or more stable results?"}}
{"paper_id": "IqaQZ1Jdky", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces an innovative framework for Kolmogorov-Arnold Networks with a variable function basis using Bernstein polynomials, which improves adaptability and robustness across diverse datasets. The experimental results show significant improvement over various benchmarks in multivariate time series forecasting, image classification, and function approximation, highlighting the method's utility.", "Weaknesses": "The paper lacks a thorough discussion on the potential computational overhead introduced by the dynamically learnable function basis. Additionally, the paper could benefit from more detailed comparisons with a broader range of existing methodologies beyond KAN variants to better establish the proposed method's competitiveness.", "Questions": "1. How does the choice of function basis affect the computational efficiency of the proposed KANs, and what impact does it have on training time? 2. How well does the VBn-KAN framework generalize to datasets and tasks not covered in the experiments? 3. Is there a specific strategy or recommendation for choosing the degree of Bernstein polynomials for different types of data?"}}
{"paper_id": "OdMqKszKSd", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel method for generating articulated 3D objects from a single image, which addresses limitations in scalability and practicality of previous methods requiring multi-view inputs.\n2. The methodological innovation includes a diffusion-based model and a pipeline that efficiently handles part connectivity and generation in a coarse-to-fine manner, offering interpretable and editable outputs.\n3. The experimental results demonstrate significant improvement over state-of-the-art methods in reconstruction quality, graph prediction, and handling ambiguity in input images.", "Weaknesses": "1. The method currently focuses on cuboid-shaped objects and might not generalize well to a wider range of object categories with more complex geometries.\n2. There is a reliance on part retrieval from a predefined library, which may limit the ability to capture finer geometric details present in the input images.", "Questions": "1. How does the method perform on objects with more complex geometries that are not cuboids? Are there plans to extend the approach to handle such object categories?\n2. To what extent can the current approach capture fine-grained details of parts beyond what is available in the part library used for retrieval?\n3. How does the choice of text and image prompts affect the generation quality, and are there guidelines for selecting these prompts effectively?"}}
{"paper_id": "LOBhVTtVnc", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper proposes a novel Geometric Spatiotemporal Transformer (GST) for simulating long-term physical dynamics, which demonstrates strong performance across different scales, including molecules, proteins, and human motions. \n2. GST maintains E(3) symmetry, which is crucial for accurately simulating physical dynamics, and combines the flexibility and expressivity of Transformers with the capability to handle spatiotemporal graphs.\n3. The introduction of the Temporal Difference Graph (TDG) is a significant contribution as it helps in reducing cumulative errors in long-term predictions.", "Weaknesses": "1. While GST shows strong performance, the main novelty lies in the combination of existing techniques (Transformers and E(3) symmetries) rather than the development of completely new methods.\n2. The explanation of the Temporal Difference Graph and its integration into the model could be further detailed to improve accessibility and understanding for users unfamiliar with these concepts.", "Questions": "1. How does GST perform in comparison to non-geometric transformer models when applied to non-physical tasks? Is its architecture strictly specific to physics tasks?\n2. The results indicate GST-V outperforms GST-F in most cases; are there specific situations or types of data where GST-F might be preferable?\n3. How sensitive is the model's performance to variations in hyperparameters, especially in different scales or types of data among molecules, protein, and human motion datasets?"}}
{"paper_id": "oa5UeyUVMm", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper addresses the novel application of Diffusion Probabilistic Models (DPMs) for graph representation learning, which is a significant contribution to the field. \n2. It establishes a solid theoretical foundation linking diffusion models and information theory, specifically proposing the Diff-InfoMax principle. \n3. The empirical results demonstrate state-of-the-art performance on various datasets, validating the effectiveness of the proposed approach.", "Weaknesses": "1. The description of the methodology, while theoretically compelling, can be complex and may pose challenges for practitioners looking to implement the approach without deep expertise in both diffusion models and graph neural networks. \n2. The paper strongly relies on theoretical constructs, which might be difficult to interpret or validate in fully practical settings besides the reported results.", "Questions": "1. How sensitive is the performance of Graffe to changes in task-specific hyperparameters, such as the noise schedule or mask ratio? \n2. Are there specific types of graphs (e.g., directed vs. undirected, different sizes) where Graffe either excels or underperforms?"}}
{"paper_id": "5swfKRkCx7", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper proposes a novel approach to augmenting LLMs with external knowledge attention for question answering, which effectively combines retrieval-augmented generation with a memory-based mechanism. \n2. The method is clearly articulated and backed by comprehensive experiments that demonstrate its effectiveness across various datasets, both general and domain-specific.\n3. The approach shows significant improvements over existing methods, leveraging memory-based differentiation and dynamic knowledge integration.", "Weaknesses": "1. The method may rely heavily on the quality of retrieved knowledge, which may not be optimal in all scenarios, potentially impacting the accuracy of question answering.\n2. While the model performs well on benchmark datasets, its performance on more diverse, real-world QA scenarios or on languages other than English is not explored in depth.", "Questions": "1. How does the proposed method handle situations where the retrieved knowledge itself is of poor quality or highly noisy?\n2. Can the mechanism of the knowledge attention and memory module be easily adapted to other architectures beyond LLaMA-2?\n3. How does the model perform in practical real-time applications compared to benchmark datasets?"}}
{"paper_id": "5s1qpjrNvZ", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper addresses the sample inefficiency of reinforcement learning by introducing a guided reinforcement learning approach that ensures a performance guarantee above a user-defined threshold.\n2. The introduction of a roll-back mechanism (GRL-RB) is an innovative method to handle non-convergence, which can improve the stability and reliability of the learning process.\n3. The theoretical analysis is comprehensive with experimental validation in various settings, providing robustness to the proposed methods.", "Weaknesses": "1. The derived sampling rate assumes full convergence between updates, which can be difficult to achieve in complex environments and may limit the applicability of the method.\n2. The performance guarantees are bounded by certain assumptions, and the scope of environments and reward schemes considered is relatively narrow.\n3. Roll-back mechanism reacts only after the degradation occurs, which might not be sufficient for critical real-world applications where preemptive action is necessary.", "Questions": "1. How does the performance of GRL-RB compare to GRL in scenarios where convergence assumptions are not met due to complex environments or poor initial hyperparameter choice?\n2. Can the roll-back mechanism be extended to anticipate performance degradation and proactively adjust sampling rates to avoid violations instead of reacting to them?\n3. How does the need for convergence between alpha updates impact the method's suitability for large-scale or highly dynamic environments where fast adaptation is crucial?"}}
{"paper_id": "WNb4P8aG66", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 9, "Confidence": 4, "Strengths": "1. The proposed Diffusion on Diffusion (DoD) framework provides an innovative approach by introducing visual priors to enhance diffusion models, demonstrating substantial improvements in image generation quality. 2. The method shows significant training and computational efficiency, evidenced by achieving state-of-the-art results on the ImageNet-256 dataset with fewer parameters and training steps compared to existing methods. 3. Comprehensive experiments showcase the effectiveness of visual priors, and the proposed approach is rigorously evaluated using various metrics, providing a thorough validation of the claimed improvements.", "Weaknesses": "1. The paper is heavily reliant on the performance evaluation on a single dataset, ImageNet-256; evaluations on more diverse datasets or tasks could strengthen the paper. 2. The dependency on latent space compression and reconstruction might impose limitations when scaling to much larger model sizes or resolutions.", "Questions": "1. How well does the DoD model generalize across different datasets or generation tasks beyond ImageNet? 2. How sensitive is the performance to the choice of parameters in the Latent Embedding Module, particularly in terms of model scalability?"}}
{"paper_id": "UfczlMudN6", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel framework, GRAM, that unifies adaptation and robustness in dynamics generalization for deep RL, showcasing excellent performance in both ID and OOD scenarios.\n2. The use of a robust adaptation module, alongside a joint training pipeline for both ID and OOD environments, is innovative and well-motivated, offering practical relevance for safe deployment in real-world settings.\n3. The paper is well-organized, with comprehensive experiments that demonstrate the superior performance of GRAM over existing methods, including rich discussions and ablation studies supporting the claims.", "Weaknesses": "1. The reliance on a specific adversary choice during robust training might limit the generality of GRAM's robust adaptation capabilities since other unknown adversarial dynamics might require different robust training strategies.\n2. Although the experiments are extensive, they are confined to simulated environments, which may not completely represent the complexities encountered in real-world applications.\n3. The approach might require further tailoring to extend beyond locomotion tasks, where dynamics and adaptation concerns could differ significantly.", "Questions": "1. Can GRAM be extended or adapted to handle tasks with more complex dynamics or those requiring multi-modal inputs beyond proprioceptive observations?\n2. How might the approach need to adjust when scaling to higher-dimensional action spaces or more varied environmental conditions?\n3. What considerations would be necessary when deploying GRAM on real hardware to ensure it performs as reliably as in simulation?"}}
{"paper_id": "60Vd7QOXlM", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel and effective method for identifying privacy leakage in large language models, which is a significant contribution to the field of machine learning privacy. 2. It provides a comprehensive evaluation of privacy leakage across a variety of models and datasets, showcasing the generalizability of the proposed method. 3. The approach offers a non-trivial audit for LLM training under a realistic DP guarantee without the need for shadow models or gradient canaries, which marks a breakthrough in practical privacy audits.", "Weaknesses": "1. The method's reliance on canary design choices and the specific training configurations might limit the generalizability of the results across all possible model settings or other types of data. 2. While the introduction of new tokens proves effective, it might not fully address how this strategy could be adapted or extended to non-textual modalities.", "Questions": "1. How does the method perform across different types of tasks or datasets where the structure or nature significantly deviates from PersonaChat or E2E? 2. Can the proposed auditing technique be extended to other domains beyond text, such as images or multi-modal datasets, and if so, how? 3. Does the introduction of new tokens have any long-term implications for the model's ability to generalize or adapt post-training?"}}
{"paper_id": "GqhvJ1o8m5", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel dual-branch architecture for stereo video conversion that effectively utilizes spatial-temporal attention mechanisms. \n2. The introduction of the YouTube-SBS dataset is a significant contribution, providing a large-scale benchmark for stereo video generation models.\n3. ImmersePro demonstrates substantial quantitative improvements over existing methods, showcasing its efficacy in stereo video synthesis.", "Weaknesses": "1. The method may struggle with producing strong stereo effects due to dataset limitations and the focus on subtle stereo effects.\n2. There is a reliance on handpicked architectures and layers which might not generalize well to all types of video content.", "Questions": "1. How does ImmersePro handle scenes with large motion or significant occlusion? Does the layered disparity representation cope well with these?\n2. Can the system be adapted for real-time stereo video generation applications or is it primarily designed for offline processing?\n3. Is there a plan to expand the dataset or the method to include stronger and more varied stereo effects in future work?"}}
{"paper_id": "ua5MHdsbck", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper proposes a novel and effective data distillation approach for extrapolative protein design, leveraging preference learning to significantly improve performance.\n2. The method is evaluated comprehensively on challenging datasets, demonstrating state-of-the-art performance in several tasks compared to previous methods.\n3. Ablation studies and comparisons with other preference learning algorithms provide valuable insights into the efficacy of the proposed approach.", "Weaknesses": "1. There is a high dependency on carefully curated datasets and the quality of the preference dataset creation, which may not be readily applicable or scalable to diverse biological sequences.\n2. Setting up such an approach can be computationally intensive, especially involving scorer distillation, which may limit its accessibility to researchers without high computational resources.", "Questions": "1. How generalizable is the proposed approach across different types of protein design tasks beyond GFP and AAV?\n2. Are there particular considerations or adaptations needed when applying this method to other biological sequence domains or with sequences having different mutation rates?\n3. What are the potential limitations regarding overfitting to triplet examples in the hard preference learning framework?"}}
{"paper_id": "56Zn3halhq", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. The paper introduces AutoTSAug, a novel method for data augmentation in time series forecasting, which leverages reinforcement learning to identify and augment marginal samples, significantly enhancing model performance.\n2. The method is built upon solid empirical observations, providing a new perspective on the utility of pretrained model zoos for data filtering and augmentation policy learning.\n3. The experimental results demonstrate consistent improvement over standard augmentation techniques across multiple real-world datasets.", "Weaknesses": "1. The reliance on a diverse and well-performing model zoo for identifying marginal samples may be a limitation, as the effectiveness of AutoTSAug could degrade with less competitive models.\n2. While the paper claims minimal computational cost, implementing the method requires fine-tuning on multiple datasets and models, which may introduce overheads not clearly quantified in relation to gains.\n3. The complexity of the algorithm might limit its applicability in real-time or resource-constrained environments.", "Questions": "1. How does AutoTSAug handle datasets lacking sufficient diversity in the model zoo, and what adaptations are necessary for such scenarios?\n2. Can the REINFORCE-based approach be extended to accommodate other types of forecasting tasks or irregular time series data with missing values?"}}
{"paper_id": "aPTGvFqile", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper addresses a significant problem in CLIP's embedding space and proposes a comprehensive framework, AlignCLIP, to reduce the modality gap.\n2. The proposed method demonstrates consistent improvements across various tasks, including zero-shot classification, robustness to distribution shifts, and multi-modal retrieval.\n3. The paper includes extensive experimental validation and sufficient elaboration on the techniques involved.", "Weaknesses": "1. While the idea of parameter sharing and intra-modality separation is theoretically sound, the practical implications and potential trade-offs (e.g., computational overhead) are not fully discussed.\n2. The effectiveness of AlignCLIP may heavily rely on the choice of semantic encoder and the regularization settings, which may not generalize well across different datasets or models.", "Questions": "1. How sensitive is AlignCLIP to the choice of the semantic encoder used for regularization?\n2. Are there any potential trade-offs regarding computational efficiency or resource utilization when adopting AlignCLIP?\n3. Can AlignCLIP be effectively adapted to other dual-modality systems outside of CLIP?"}}
{"paper_id": "GOwNImvCWf", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces a novel behavioral loss to enhance the autoencoder training in weight space, addressing the limitations of structural reconstruction alone.\n2. It provides a comprehensive experimental analysis using different datasets, demonstrating improved reconstruction and generation of neural network weights.\n3. The combination of structural and behavioral losses is shown to significantly enhance performance across a range of downstream tasks.", "Weaknesses": "1. The study is limited to smaller models due to computational constraints, leaving questions about scalability to larger, more complex neural networks.\n2. The reliance on appropriate query selection for the behavioral loss might pose challenges in practical applications, especially when queries do not match the training data distribution.\n3. Although the paper provides theoretical insights, additional clarification on the implementation specifics of large-scale experiments would strengthen the work.", "Questions": "1. How can the proposed method be scaled to handle larger models commonly used in real-world applications?\n2. What strategies can be implemented to select appropriate queries for the behavioral loss, especially in scenarios where training data distribution is unknown or diverse?\n3. Could the proposed approach be extended or adapted to other types of neural networks, such as Transformers or RNNs?"}}
{"paper_id": "C9BA0T3xhq", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper introduces a novel extension to Implicit Q-Learning by incorporating expectile regression for handling in-sample and out-of-sample data, which could potentially improve offline reinforcement learning applications.\n2. EIQL effectively addresses distribution shift and error extrapolation issues that commonly arise in offline RL settings.\n3. The experimental evaluation is comprehensive, covering a range of benchmark datasets, and demonstrates improvements over existing methods.", "Weaknesses": "1. The paper lacks a detailed theoretical analysis to fully support the proposed dual learning mechanism's effectiveness compared to existing methodologies.\n2. Although the experimental results are promising, more extensive comparisons with a broader range of state-of-the-art techniques could better establish the superiority of EIQL.\n3. The manuscript could benefit from clearer explanations and definitions of key concepts and methods, particularly for readers unfamiliar with the domain.", "Questions": "1. How sensitive is the EIQL algorithm to the choice of expectile level, and how does this selection impact its stability and performance across diverse datasets?\n2. Can the expectile regression approach used in EIQL be effectively integrated into other RL paradigms beyond the scope of offline learning?\n3. What are the limitations or potential drawbacks of EIQL when applied to environments with highly skewed or unbalanced reward distributions?"}}
{"paper_id": "EeqlkPpaV8", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper makes a significant contribution by establishing adaptive lower bounds for log-concave sampling, which had been poorly understood before. This is essential for designing efficient algorithms in high-dimensional settings.\n2. The methodology used, including leveraging chain-like structures and smoothing techniques, is robust and well-justified in exploring the adaptive complexities proposed.\n3. The work addresses both unconstrained and box-constrained sampling, covering a broad range of scenarios in the domain.", "Weaknesses": "1. The paper's results are limited to high-dimensional settings, which means the work may not be applicable to all practical scenarios, particularly in low-dimensional cases where much is still unknown.\n2. Presentation-wise, the paper could be improved by clearer figures and more intuitive explanations of technical concepts, especially for readers less familiar with the domain.", "Questions": "1. Can the presented lower bounds be extended to other sampling problems beyond log-concave distributions, such as diffusion models?\n2. Are there any potential strategies discussed or planned to address the limitations of the existing bounds within low-dimensional settings?"}}
{"paper_id": "oK1zJCWBqf", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces Soft Preference Optimization (SPO), which provides a novel method for aligning language models to human preferences without the need for a reward model, simplifying the alignment process and reducing bias propagation.\n2. Theoretical guarantees are provided under the Bradley-Terry model assumption, offering insights into the convergence properties of SPO.\n3. Comprehensive empirical evaluations demonstrate the effectiveness of SPO over several baselines, particularly in pairwise preference alignment tasks.", "Weaknesses": "1. The algorithm introduces computational complexity due to the requirement for online sampling, potentially limiting its scalability for very large models or datasets.\n2. While the theoretical analysis is robust under the BT model assumption, the practicality of these assumptions in realistic scenarios may be limiting.\n3. The reliance on online sampling and fixed parameter values like the α parameter for controlling entropy may not be ideal for all tasks, and could benefit from additional tuning or adaptation strategies.", "Questions": "1. How does the choice of the α parameter affect the performance of SPO across different datasets or applications? Are there guidelines for selecting this parameter?\n2. Can SPO be extended to other modalities and tasks beyond language model alignment, or are there intrinsic limitations based on the current design?\n3. How sensitive is the performance of SPO to the quality and diversity of the preference datasets? Would noisy or biased datasets significantly affect its effectiveness?"}}
{"paper_id": "Dq9VrVuLzV", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. SytheOcc is a novel framework for synthesizing photorealistic and geometrically controlled images for autonomous driving scenarios, which is a valuable contribution in the context of data augmentation for perception models. \n2. The use of 3D semantic multi-plane images (MPIs) for providing comprehensive and aligned 3D scene descriptions is innovative. \n3. The paper presents extensive qualitative and quantitative evaluations demonstrating the effectiveness of SytheOcc in generating datasets that improve model performance on occupancy prediction tasks.", "Weaknesses": "1. The model heavily relies on having the appropriate conditional inputs, which may not always be available or easily obtained. \n2. Certain results, such as extending these techniques for real-time applications or handling dynamic scenes with rapidly changing geometry, are not addressed thoroughly.", "Questions": "1. How does SytheOcc perform in real-time applications or rapidly changing dynamic scenes typically encountered in autonomous driving? \n2. Could you elaborate on the limitations or challenges when generating data for different autonomous driving environments based on this method?"}}
{"paper_id": "73Q9U0vcja", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel approach by integrating diffusion models and active learning for computed tomography, showing significant improvements in data efficiency and reconstruction quality.\n2. The method achieves up to 4x reduction in X-ray dose with 2x computational speed-ups, demonstrating its practical impact and efficiency on several real-world tomography datasets.\n3. The work is well-supported with extensive experiments and comparisons against other methods, showcasing clear advantages of the proposed approach.\n4. The potential applicability of the method to a range of imaging setups, beyond just CT, speaks to its broader applicability and innovation.", "Weaknesses": "1. The requirement for a significant amount of prior training data for the diffusion model could limit applicability in domains where such data is not readily available.\n2. The computational demands of the method, though justified in some contexts, could be a barrier to broader adoption, especially in resource-constrained environments.\n3. There is a potential risk of reconstruction bias when dealing with out-of-distribution samples, which might limit the usage in high-stakes settings like medical imaging.", "Questions": "1. How transferable are the diffusion models between different domains? Would a model trained on one type of tomography data be applicable elsewhere without re-training?\n2. The paper mentions that diffusion models outperform others especially when no pre-scan is available. Could there be scenarios where pre-scans might not be as beneficial and specific strategies could still outperform without them?\n3. How robust is the approach when considering real-world challenges like measurement noise or sample misalignment beyond the synthetic setups considered in the paper?"}}
{"paper_id": "UiEjzBRYeI", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "1. SAM-CP proposes a novel approach of composable prompts to extend the capabilities of the Segment Anything Model, achieving state-of-the-art results in open-vocabulary segmentation.\n2. The methodology effectively combines semantic labeling and instance merging, with a unified affinity framework that improves computational efficiency.\n3. Extensive experiments demonstrate the versatility of SAM-CP in both open and closed domains across multiple benchmark datasets.", "Weaknesses": "1. The reliance on SAM for patch generation introduces limitations, as inaccuracies in SAM can negatively impact the overall segmentation performance, particularly in closed-domain segmentation.\n2. The method's effectiveness is somewhat constrained by the underlying vision foundation model, which may not perform optimally in all scenarios, such as small object detection.", "Questions": "1. How does the approach handle errors or ambiguities introduced by the SAM patches when applied to real-world segmentation tasks?\n2. Can SAM-CP be adapted for other types of segmentation tasks beyond what is covered in the paper, such as interactive segmentation or real-time applications?"}}
{"paper_id": "lessla98Wp", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The proposed L-C4 framework effectively addresses limitations of existing video colorization methods by leveraging language-based descriptions for assigning creative and consistent colors. 2. The novel temporally deformable attention and cross-clip fusion components significantly enhance temporal consistency and long-term color maintenance in video colorization tasks. 3. Extensive experiments and user studies demonstrate superior performance of L-C4 across various metrics, indicating its robustness and applicability.", "Weaknesses": "1. While the framework shows promising results, the reliance on textual descriptions might limit applicability in scenarios where precise language descriptions are not available. 2. The computational complexity of the approach, especially the need for extensive training resources, might restrict its deployability in real-time applications.", "Questions": "1. How well does L-C4 handle situations with erroneous or ambiguous language descriptions provided by users? 2. Can the L-C4 framework be adapted or extended to handle other forms of video enhancement beyond colorization? 3. Is the system capable of handling multiple instances of the same object and differentiating their colorizations with varying user instructions?"}}
{"paper_id": "UstOpZCESc", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper provides a novel framework, PALL, for integrating lifelong learning and privacy-aware unlearning, which is a unique contribution to the field of AI.\n2. The empirical validation of the method across multiple architectures and datasets demonstrates its scalability and effectiveness in various scenarios.\n3. The method is memory efficient and maintains the ability to perform exact unlearning, which is crucial for compliance with privacy regulations.", "Weaknesses": "1. The approach focuses on task-incremental learning, limiting its direct application to class- or domain-incremental settings where tasks have unified label spaces.\n2. The current method does not address selective data unlearning, which may be necessary for stricter privacy requirements related to individual data deletion.", "Questions": "1. Can the PALL framework be adapted to handle class- or domain-incremental learning scenarios, and how would the architecture need to change?\n2. What are the potential impacts of extending PALL to handle selective data unlearning rather than complete task unlearning?\n3. How effectively can PALL be applied to domains beyond vision tasks, such as natural language processing?"}}
{"paper_id": "LnRegTxsa4", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces novel modules, CVCAM and MHSAM, which enhance cross-view object geo-localization performance. These modules effectively address the limitations of existing methods by improving the interaction between views and refining spatial features.\n2. The creation of the G2D dataset is a significant contribution, expanding available resources for the Ground→Drone localization task and addressing a gap in this research area.\n3. Extensive experiments demonstrate that the proposed method outperforms state-of-the-art techniques, showing superior localization accuracy across multiple datasets.", "Weaknesses": "1. The paper could provide more detailed insight into the specific scenarios or applications where the improvement offered by CVCAM and MHSAM would be most impactful.\n2. While the visualization analysis supports the effectiveness of the proposed modules, more quantitative results on how noise suppression and feature refinement contribute to the overall performance would strengthen the argument.\n3. The dependability on the newly created G2D dataset might limit the generalizability of the approach to other datasets or real-world applications, which needs more validation.", "Questions": "1. How does the model's performance vary when trained or tested on other datasets beyond CVOGL and G2D?\n2. Can the CVCAM and MHSAM modules be integrated into other geo-localization frameworks to enhance their performance?\n3. What specific challenges were encountered in creating the G2D dataset, and how were they addressed to ensure its applicability as a benchmark?"}}
{"paper_id": "2ErS9Bkc3O", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper provides a matrix-theoretic explanation for the adversarial fragility of deep neural networks, which is a significant contribution to the understanding of this phenomenon. 2. The work complements existing theories on adversarial robustness with new insights obtained from both theoretical and numerical analysis. 3. The methodology leverages matrix theory to relate feature compression with adversarial robustness, providing a clear mathematical underpinning for the observed degradation in robustness as input dimension increases.", "Weaknesses": "1. The paper focuses heavily on theoretical aspects without demonstrating practical methods to enhance adversarial robustness based on the derived insights. 2. The results are largely theoretical and might not translate well to a variety of practical neural network architectures and real-world datasets. 3. The numerical experiments, although consistent with the theoretical predictions, are limited in scope and are conducted on relatively simple models and data setups.", "Questions": "1. How can the derived theoretical insights be translated into practical strategies for improving adversarial robustness in real-world neural networks? 2. Does the matrix-theoretic explanation suggest specific architectural changes or training methodologies that could mitigate the observed adversarial fragility? 3. How does the adversarial robustness vary with different types of neural network architectures, such as convolutional or transformer-based networks?"}}
{"paper_id": "pK3oe2bubc", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 5, "Confidence": 3, "Strengths": "1. The paper explores an innovative approach to making vision transformers robust to arbitrary layer execution orders, which could have significant implications for distributed computation and model robustness. 2. The study provides a novel perspective on model merging and layer shuffling methodologies, offering insights into flexible model deployment. 3. The use of UMAP analysis helps understand the inner working of layers in vision transformers, shedding light on their adaptability and contribution.", "Weaknesses": "1. The proposed approach entails a notable decline in accuracy compared to baseline models, which may limit its practicality in scenarios where performance is critical. 2. The training method's impact on more complex or larger datasets is not explored, raising concerns about the scalability and efficiency of the method in different contexts. 3. The evaluation lacks comparisons against state-of-the-art methods beyond existing baselines, which could make it challenging to assess the full potential of the proposed solution.", "Questions": "1. Would this approach be scalable to significantly larger and more complex datasets without excessive computational overhead? 2. How does the method perform in terms of resource utilization compared to standard vision transformers? 3. Could the robustness to arbitrary execution orders potentially be improved while mitigating the trade-off in overall performance?"}}
{"paper_id": "sec09tLQUl", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces a novel approach, FairDropout, which addresses the challenge of reducing spurious correlations in deep learning models without requiring group annotation information.\n2. The proposed method shows considerable empirical improvements in worst-group accuracy over state-of-the-art baselines across diverse datasets, demonstrating its potential general applicability.\n3. The paper is well-structured, clearly presenting the problem, method, and results, making it accessible to a broad audience.", "Weaknesses": "1. The work assumes that memorization only has a negative impact on generalization, without considering cases where memorization might be beneficial, which could limit the applicability of the proposed method.\n2. While the method scales to larger architectures, the hypothesis regarding the distinction between memorizing and generalizing neurons requires further theoretical and empirical validation to strengthen the claims.", "Questions": "1. Can you provide insights into scenarios where memorization could be inherently beneficial to generalization, and how FairDropout would handle such cases?\n2. How does FairDropout handle cases where the spurious correlation is not strongly tied to specific neurons and instead distributed more broadly across the network?"}}
{"paper_id": "96jZFqM5E0", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel contrastive learning framework, SiMHand, for 3D hand pose estimation pre-training, leveraging a large-scale dataset of diverse hand images. \n2. Innovative use of mining techniques to identify similar hand poses from different images improves the robustness and discrimination power of the model. \n3. Demonstrated significant improvements over existing methods such as SimCLR and PeCLR, with substantial performance gains on key datasets like FreiHand, DexYCB, and AssemblyHands.", "Weaknesses": "1. The dependence on a large-scale dataset of 2.0M images may limit accessibility for others who do not have the resources to compile similarly extensive datasets. \n2. The method might face challenges when scaling to different or new domains beyond the specific setups tested in this paper, depending heavily on finding 'similar' hand poses.", "Questions": "1. How does the method perform when applied to domains or tasks that may not have as straightforward similarity metrics as 3D hand poses? \n2. Can the similarity mining approach be generalized to other types of pose estimation or non-hand-centric setups effectively? \n3. What are the expected outcomes when applying this method where the dataset is highly imbalanced or contains much fewer positive samples?"}}
{"paper_id": "uNd289HjLi", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel and effective approach for MRI denoising using a score-based self-supervised framework, which addresses the problem of label scarcity in MRI datasets. It extends denoising score matching to more practical ambient noise settings, demonstrating competitive performance with supervised methods. The methodology is well-grounded theoretically and empirically tested across multiple datasets, showcasing the ability to tackle various noise levels and MRI contrasts robustly.", "Weaknesses": "The reliance on specific noise assumptions and pre-processing steps such as the Variance Stabilizing Transform might limit applicability to broader noise types or scenarios where Gaussian assumptions do not hold well. Some specific implementation details may require more elaboration, especially regarding the handling of non-Gaussian noise distributions in practice.", "Questions": "Can the proposed C2S framework be adapted to efficiently handle non-Gaussian noise scenarios in MRI data without relying on Gaussian approximations or transformations? How does the method perform when pre-processing steps like the Variance Stabilizing Transform are omitted?"}}
{"paper_id": "e2ONKX6qzJ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper clearly identifies and addresses a significant issue in the use of classifier-free guidance (CFG) in diffusion models, particularly oversaturation at high guidance scales.\n2. It introduces a novel method, Adaptive Projected Guidance (APG), which effectively reduces oversaturation and maintains high image quality with minimal computational overhead.\n3. The method is demonstrated to be compatible with existing models and samplers, and extensive experiments show improvements in FID, recall, and saturation metrics across various models.", "Weaknesses": "1. The approach relies heavily on orthogonal projections and reverse momentum, which might not translate easily to other types of generative models beyond diffusion models.\n2. The requirement for converting predictions back to denoised samples might present challenges in systems where such conversions are non-trivial.\n3. Limited discussion on the potential computational cost or complexity of implementing these changes in large-scale production environments.", "Questions": "1. Can the APG method be adapted for other types of generative models beyond diffusion models, such as GANs?\n2. What are the implications of using APG on the interpretability and explainability of the generated images, especially in critical applications?\n3. Is there any impact on the speed of image generation when applying APG, particularly in real-time applications or when using large datasets?"}}
{"paper_id": "a2eBgp4sjH", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. Comprehensive theoretical and empirical investigation of the MultiFilterANN problem, bridging gaps in literature. 2. Novel algorithmic approach improves recall significantly over existing baseline methods for multiple datasets. 3. Release of new datasets fills a critical gap, offering a new benchmark for future work.", "Weaknesses": "1. The empirical evaluation could be more extensive by including more baselines and conditions to further demonstrate robustness. 2. The complexity of the presented algorithms could be a barrier to understanding and reproduction by others.", "Questions": "1. Can the approach be directly extended to other ANN-related tasks or domains beyond those tested? 2. How does the method perform in a dynamic setting where datasets change frequently over time?"}}
{"paper_id": "EIXZXPz7jU", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces a novel sampling method for PINNs using flow matching, addressing source singularities more effectively than existing approaches.\n2. The method presents significant improvements in accuracy and computational efficiency, as demonstrated by comprehensive numerical results on various PDE problems.\n3. The approach circumvents the challenges associated with normalizing flow methods, such as probability density estimation, providing a robust alternative.", "Weaknesses": "1. While the numerical experiments effectively showcase the proposed method's strengths, the absence of a broader theoretical discussion may limit the readers' understanding of the underlying strengths and limitations.\n2. The presentation lacks clarity in some sections, especially in detailing the mathematical formulations and algorithms, which may hinder accessibility for readers not already familiar with the area.\n3. Reproducibility is claimed to be supported, but the details offered in the paper regarding implementation specifics are limited.", "Questions": "1. Can the proposed method be adapted to handle other types of PDE singularities not explicitly tested in the paper?\n2. Are there any scenarios where flow-matching sampling may perform worse than existing methods, such as those utilizing normalizing flows?\n3. How does the computational cost of flow-matching PINN compare to standard PINN implementations in practice across a range of PDEs?"}}
{"paper_id": "QO4bF6MHza", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. MATHHAY introduces an innovative benchmark specifically targeting long-context mathematical reasoning, which is crucial for real-world applications of LLMs.\n2. The benchmark construction is automated, allowing scalable and efficient creation of test materials with consistent quality control practices.\n3. The paper provides comprehensive experimental evaluations and demonstrates a significant gap in current LLMs' performance, thus highlighting areas for future research.", "Weaknesses": "1. Although MATHHAY is novel, the benchmark focuses solely on mathematical reasoning, which may limit its applicability for more generalized long-context tasks.\n2. The paper does not provide sufficient details on extending MATHHAY to evaluate other types of reasoning challenges or interdisciplinary contexts.", "Questions": "1. How can MATHHAY be adapted or extended to evaluate other reasoning capabilities beyond mathematics in LLMs?\n2. Are there specific plans to update the benchmark as LLM capabilities evolve, and what are the criteria for doing so?"}}
{"paper_id": "GULx8rzzjC", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper tackles a relevant issue in machine learning by proposing a method that allows model trainers to effectively identify useful external data sources without extensive data sharing. \n2. Mycroft shows potential in enhancing model performance with minimal data exposure by leveraging functional and feature similarity.\n3. Extensive experimental validation on multiple datasets and case studies demonstrates the practicality and robustness of the method, including handling label and data noise.", "Weaknesses": "1. The approach assumes the existence of a relevant subset Dhard which might not always be feasible, potentially limiting Mycroft's applicability.\n2. While functional similarity is used to incorporate some diversity, explicitly promoting diverse data selection is not thoroughly explored.\n3. The assumption that similar data is most beneficial may overlook cases where dissimilar data could provide unique insights into the task.", "Questions": "1. How does Mycroft perform when Dhard is too limited or when there is no clear hard subset identified?\n2. Can the approach be extended to scenarios where the model trainer cannot share any data at all with data owners?\n3. Are there plans to explore more privacy-preserving techniques in future iterations of Mycroft, especially in cases with strategic data owners?"}}
{"paper_id": "PnZ2lbQaao", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces a novel adversarial Bayesian framework, Domain Indexing Collaborative Filtering (DICF), which significantly improves performance in cross-domain recommendation tasks and enhances interpretability.\n2. Demonstrates effectiveness across both synthetic and real-world datasets, with clear evidence of improvement over state-of-the-art methods.\n3. Provides visualizations and insights into the knowledge transfer process, offering a deeper understanding of domain relationships.", "Weaknesses": "1. The approach may rely heavily on the choice of prior distributions and domain-specific feature selection, which might limit generalizability without careful tuning.\n2. The method assumes that spurious features can be effectively isolated and identified, which may not hold in all cross-domain scenarios.\n3. The framework's adaptation to evolving or dynamic domains, where user preferences change over time, remains unexplored.", "Questions": "1. How sensitive is the proposed framework to the choice of prior distributions and the feature selection process?\n2. Can the DICF framework be extended to handle dynamic environments where domain characteristics and user preferences evolve over time?\n3. How does the model perform in scenarios with multiple overlapping domains, and does the complexity significantly increase?"}}
{"paper_id": "AT64R0ivUO", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The proposed SteerPrompt method effectively addresses a critical limitation in current attention steering models by removing the reliance on human annotation, which can significantly enhance the utility of attention steering in practical applications.\n2. SteerPrompt shows substantial empirical improvements over existing prompt-based and iterative methods, demonstrating its effectiveness in enhancing model reading comprehension with consistent performance gains across multiple datasets and model sizes.\n3. The integration of attention steering and iterative prompting creates a novel approach that capitalizes on the strengths of both techniques while minimizing their limitations, which could inspire future research directions in improving LLM comprehension.", "Weaknesses": "1. While SteerPrompt shows promising results, its reliance on key sentence identification introduces an upper bound on performance tied to sentence selection accuracy. Misidentification can potentially limit the method in scenarios where key sentence prediction is challenging.\n2. Although attention reweighting improves comprehension, it could introduce computational complexity, possibly limiting scalability for very large models or real-time applications without careful optimization.\n3. The method heavily relies on the selection of attention heads, which, despite improvements in search efficiency, still poses a challenge in terms of computational resources and time for large model setups.", "Questions": "1. How does SteerPrompt handle contexts where the key information is spread across multiple sentences rather than concentrated in a single sentence?\n2. Has SteerPrompt been evaluated on tasks outside of open-book QA to test its robustness and adaptability to different problem domains?\n3. How can SteerPrompt be optimized for real-time applications where computational resources for attention steering might be limited?"}}
{"paper_id": "nGiGXLnKhl", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces Vision-RWKV, a novel visual perception model with linear complexity, demonstrating comparable performance to ViT while being more efficient in terms of computational resources and memory usage.\n2. It successfully adapts RWKV, originally designed for NLP tasks, to visual field applications, enabling processing of high-resolution images without windowing operations.\n3. Extensive experiments on multiple vision tasks such as classification, detection, and segmentation show the model's capability and scalability across different tasks and datasets.", "Weaknesses": "1. The implementation of Q-Shift in PyTorch is noted as inefficient, potentially limiting the deployment efficiency of VRWKV, although optimization via CUDA is suggested as a solution.\n2. While the model's performance is generally impressive, there is a heavy reliance on the engineering and optimization aspects to achieve these results, which may limit the comprehensiveness of the theoretical insights provided.\n3. The paper might benefit from a more detailed exploration of the potential limitations or scenarios where the model might underperform.", "Questions": "1. How does Vision-RWKV handle scenarios with dynamic spatial structures where fixed token shifts might not be as effective?\n2. Apart from CUDA optimizations, are there other potential architectural changes or algorithms (e.g., better memory management) being considered for improving the Q-Shift efficiency?\n3. Could the VRWKV approach be applied effectively to tasks beyond visual perception, or are there intrinsic limitations tied to the visual domain?"}}
{"paper_id": "TBJCtWTvXJ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper provides both theoretical and empirical analysis that demonstrates how Adam's potential for loss spikes is linked to excessively large updates, an important contribution to the understanding of optimization in DNN training.\n2. The proposed S3 optimizer integrates improvements such as a flexible momentum order and reduced tuning overhead, leading to enhanced performance over Adam in both training speed and accuracy across various tasks.\n3. Extensive experiments carried out validate the effectiveness of S3 on language and vision models showing tangible improvements over existing optimizers.\n4. The analysis of the trade-offs regarding hyperparameters involving learning rates and momentum orders is unprecedented, providing practical insights into choosing optimizer settings.", "Weaknesses": "1. Although the experiments are extensive, it's unclear how S3 performs relative to less common, more specialized optimizers that may outperform Adam in niche domains.\n2. The complexity of S3 might present a barrier to immediate practical adoption without adequate automated methodologies to set its hyperparameters.\n3. While the paper discusses various scenarios theoretically, it doesn't delve deeply into potential drawbacks or domains where S3 might not perform as well as optimizers tailored to specific problem characteristics.", "Questions": "1. Can the S3 optimizer be adapted or further enhanced to account for additional criteria, such as energy efficiency or resource-constrained environments?\n2. How does the S3 optimizer perform in settings where catastrophic forgetting is a concern, such as continual learning?\n3. Is there a possibility of combining S3 with other enhancement techniques used in optimizers, such as adaptive learning rate schedules or learned optimizers, to further improve performance?"}}
{"paper_id": "8gSrJOL2oc", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel framework, TRIDENT, that effectively addresses compositional zero-shot learning by combining MLLM embeddings and attribute smoothing.\n2. Extensive experiments show that TRIDENT achieves state-of-the-art performance on three challenging datasets, demonstrating its capability and robustness.\n3. The method improves on several fronts, including disentanglement and generalization, by leveraging both large language models and multimodal models.", "Weaknesses": "1. The approach relies heavily on LLMs and MLLMs, which might limit accessibility due to computational resources required to train and fine-tune these large models.\n2. The framework might be specifically tailored for the datasets it was evaluated on, and its general applicability to other datasets is not extensively validated.", "Questions": "1. How does the proposed TRIDENT framework perform on other types of datasets or domains outside of those tested in this study?\n2. Can the TRIDENT framework be efficiently scaled with larger and more complex datasets while maintaining its state-of-the-art performance?\n3. Is the reliance on large models like GPT-3.5 essential, or can the framework offer competitive performance with smaller language models?"}}
{"paper_id": "yheQRc5xWB", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper proposes a novel Mamba-based framework enhanced with a covariance de-correlation strategy, achieving significant improvements in counterfactual prediction over time series.\n2. Theoretical analyses are provided to support the claims, and comprehensive experiments on synthetic and real-world datasets demonstrate the proposed method's superiority over existing baselines in both performance and efficiency.\n3. The paper is well-structured and clearly presented, making it accessible to readers with a technical background in causal inference and machine learning.", "Weaknesses": "1. While the proposed method shows excellent results, it is specifically tailored to linear state-space models, which might limit its applicability to non-linear scenarios.\n2. The approach depends heavily on the selection and tuning of hyperparameters, such as the de-correlation parameter α, which could be challenging in practice.", "Questions": "1. How would the proposed framework perform in highly non-linear and non-Gaussian datasets beyond the MIMIC-III dataset used in the experiments?\n2. Can the CDSP mechanism be adapted to work with non-linear state-space models, and if so, what modifications would be necessary?\n3. What strategies can guide the tuning of critical hyperparameters like α, and how does their sensitivity affect the robustness of the proposed solution?"}}
{"paper_id": "HxKSzulSD1", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper identifies an important concern of potential deception in weak-to-strong generalization scenarios, which is crucial given the rapid advancement of LLMs.\n2. It provides a rigorous empirical investigation through a series of experiments that expose the weak-to-strong deception phenomenon across different model capacities and settings.\n3. The presentation of the results is clear and the paper is well-organized, allowing readers to follow along with the arguments and findings effectively.", "Weaknesses": "1. The paper's exploration is limited to a specific multi-objective alignment setting and might not fully capture the range of scenarios where weak-to-strong deception can occur.\n2. While the paper discusses potential solutions to mitigate deception, the effectiveness of these countermeasures remains limited, requiring further exploration.\n3. The reliance on contrived experimented setups may not accurately reflect real-world applications, questioning the general applicability of the findings.", "Questions": "1. What other scenarios can the identified deception mechanism apply to, beyond multi-objective alignment?\n2. How can future research expand upon the bootstrapping method to further mitigate the deception problem?\n3. What specific characteristics or signals do strong models rely on to determine the knowledge boundaries of weaker models and engage in deceptive behaviors?"}}
{"paper_id": "wH8XXUOUZU", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper proposes novel methods to address the challenges of high spatial-compression autoencoders, demonstrating significant improvements in both inference and training speed without sacrificing quality. \n2. The introduction of Residual Autoencoding and Decoupled High-Resolution Adaptation provides innovative solutions that can be broadly applied to enhance diffusion models' efficiency.\n3. Extensive empirical results across various tasks and datasets showcase the effectiveness and robustness of the proposed model.", "Weaknesses": "1. The approach may heavily rely on specific architectures and data settings, which might limit its generalizability across different domains without further adaptation.\n2. The results, although impressive, are largely technical and may not yield substantial theoretical insights or paradigm shifts.", "Questions": "1. How does the method perform on datasets or domains not covered in the experiments? \n2. Can the proposed techniques be adapted to other forms of generative models beyond diffusion models?"}}
{"paper_id": "QWIg5e6mtT", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper addresses the significant limitation in existing PSRO frameworks for heterogeneous team games and proposes the novel H-PSRO framework to tackle these challenges effectively.\n2. It provides a comprehensive theoretical foundation and empirical evaluation that demonstrates the lower exploitability and superior convergence properties of H-PSRO compared to existing methods.\n3. The paper is well-structured and presents complex ideas clearly, supported by detailed experimental results.", "Weaknesses": "1. The method's reliance on sequential optimization may pose efficiency limitations in scenarios with highly complex games or a large number of agents, which might affect its applicability in such cases.\n2. The scalability of the proposed method to continuous action spaces is mentioned but not thoroughly investigated in the experiments.", "Questions": "1. How would H-PSRO perform in environments with continuous action spaces, and what modifications would be necessary to address this scenario?\n2. Can the sequential optimization approach be extended or adapted to handle situations where teams consist of a very large number of agents?"}}
{"paper_id": "kbSU5bwoRv", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper introduces SaMoye, a novel zero-shot singing voice conversion model capable of converting singing to human and non-human timbres. 2. The approach uses a large-scale dataset and introduces feature disentanglement and enhancement to improve zero-shot performance. 3. Extensive experiments show promising results for both human and non-human timbre conversion tasks.", "Weaknesses": "1. The paper does not provide a detailed analysis of the limitations or potential challenges of the proposed methods, especially regarding real-time applications or computational efficiency. 2. The dependency on large-scale training data may limit the model's applicability in certain scenarios where such data is not available.", "Questions": "1. How does SaMoye handle real-time singing voice conversion, and what are the computational demands? 2. Can the model be extended to other types of audio conversion tasks, such as speech-to-music or animal sounds?"}}
{"paper_id": "KyqtKhv6q1", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel approach, Differentiable Map Priors (DMP), which effectively integrates spatial priors into 3D perception systems. This is a significant contribution as it leverages prior traversals to boost detection accuracy.\n2. The experiments on the nuScenes dataset show consistent improvements across multiple architectures, which demonstrates the robustness and generalizability of the proposed method.\n3. The approach is computationally efficient and incurs little overhead, making it suitable for real-world applications.", "Weaknesses": "1. The paper does not deepen into potential limitations or scenarios where the proposed DMP might not bring improvements, such as in areas with limited past traversal data.\n2. Some aspects of the DMP functionality, like dealing with dynamic changes in the environment or how it copes with significant occlusions, would benefit from further explanation and experimentation.", "Questions": "1. How does the DMP framework deal with drastic environmental changes between traversals, such as construction or changes in vegetation?\n2. Can the learned priors from DMP be transferred to other geographical locations or cities, or are they inherently tied to the locations they are learned on?"}}
{"paper_id": "lTL4t68BNc", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper effectively introduces a novel graph attention mechanism, RGA-IB, which significantly improves the robustness of GNNs against various adversarial attacks.\n2. The integration of the Information Bottleneck (IB) principle to reduce IB loss is both innovative and well-justified, providing a theoretical foundation for the enhanced robustness.\n3. Comprehensive experiments demonstrate the superiority of RGA-IB over existing methods in terms of reduced IB loss and improved node classification accuracy.", "Weaknesses": "1. While the paper extensively covers the robustness improvements, it lacks a discussion on the computational overhead introduced by RGA-IB compared to other GNNs.\n2. The practical applicability of the method in real-world, large-scale graph datasets is not sufficiently explored, potentially limiting its scalability insights.", "Questions": "1. Can the RGA-IB framework be adapted to other forms of GNNs beyond the graph attention networks discussed? If so, what modifications would be necessary?\n2. What is the computational complexity of RGA-IB compared to other robust graph attention methods, and how does it perform on substantially larger graphs beyond the ones tested?"}}
{"paper_id": "hWmwL9gizZ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces PROVACCINE, a novel deep learning framework with a dual attention mechanism, providing a significant improvement over existing methods in immunogenicity prediction tasks. 2. It compiles a comprehensive immunogenicity dataset that enhances the reliability and generalizability of predictive models. 3. The study establishes rigorous validation protocols, effectively demonstrating practical implications in real-world vaccine development scenarios.", "Weaknesses": "1. The reliance on dual attention mechanisms may introduce additional complexity, demanding more computation resources and possibly creating barriers to widespread adoption. 2. The study briefly touches on the limitation of structure prediction and its potential impact on the model’s performance, indicating that the results hinge on the quality of these predicted structures.", "Questions": "1. How does PROVACCINE handle sequences with missing structural information, and does this affect prediction accuracy significantly? 2. Is there a plan to expand the dataset to encompass more diverse pathogens or populate the dataset with additional real-world experimental data? 3. Can the model be easily adapted to predict other protein-related properties beyond immunogenicity?"}}
{"paper_id": "IL85Ebjg9j", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. The paper proposes a novel trust-discounting mechanism to enhance the Evidential Multi-view framework, effectively addressing the challenges of conflicting views in multi-view classification.\n2. The method is well-evaluated on six real-world datasets and demonstrates superior performance in terms of prediction accuracy and consistency compared to existing methods.\n3. The introduction of a new evaluation metric, MVAGT, provides an insightful way to measure the model's ability to handle view disagreements, aligning well with real-world multi-view scenarios.", "Weaknesses": "1. The method relies heavily on the underlying assumption that some views can be trusted more than others, which may not always hold, especially when all views are equally ambiguous.\n2. The complexity of the proposed method, involving multiple stages and separate neural networks for referral and functional evidence generation, could make it difficult to implement and computationally expensive.\n3. The paper does not provide a clear comparison of how the method performs with different levels of noise or conflicts across the datasets, which could limit understanding of its robustness in varied conditions.", "Questions": "1. How does the method handle scenarios where there is no clear distinction of trustworthiness between views, and conflicts persist across all views?\n2. Can the proposed trust-discounting mechanism be adapted to scenarios with more than two conflicting views? How scalable is it?\n3. How sensitive is the method to the choice of referral trust initialization, and does it have a significant effect on the final outcomes?"}}
{"paper_id": "cnKhHxN3xj", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces a novel way of measuring neuronal entanglement using the Wasserstein distance, contributing to the understanding of interpretability in neural networks.\n2. It provides a new perspective on the interaction between neuronal entanglement and sparsity, a significant insight for network pruning and optimization.\n3. The Sparse Expansion framework is innovative and effectively demonstrates the practical implications of Wasserstein neurons in large language models.", "Weaknesses": "1. The experimental evaluation, while comprehensive, primarily focuses on a specific type of neuron (Wasserstein neurons), which may limit generalizability to other types of neurons or architectures.\n2. The practical applicability of the Sparse Expansion technique may be limited due to potential increases in memory footprint and computational complexity, which are not addressed in the current study.", "Questions": "1. How does the proposed framework handle different types of models other than LLMs? Is the technique applicable beyond transformer-based architectures?\n2. Can the Wasserstein distance metric be generalized or adapted to other pruning methodologies beyond unstructured sparsity?\n3. Does the introduction of Sparse Expansion have a significant impact on inference speeds, and how might this affect the real-world performance of these models?"}}
{"paper_id": "dgR6i4TSng", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. Quantum-PEFT presents an innovative approach by leveraging quantum unitary parameterizations for ultra-efficient fine-tuning, potentially setting a new standard in parameter efficiency.\n2. Extensive experimental results demonstrate significant parameter reduction while maintaining competitive performance across various benchmarks in language and vision tasks.\n3. The paper provides a thorough comparative analysis against state-of-the-art PEFT methods, reinforcing the validity and effectiveness of the proposed approach.", "Weaknesses": "1. The reliance on quantum-inspired techniques may introduce complexity that limits the accessibility and broader applicability of the method to researchers unfamiliar with quantum computing principles.\n2. Some technical aspects, such as the impact of varying specific quantum-related parameters, are intricate and might require deeper elaboration for complete understanding.", "Questions": "1. How does the introduction of quantum-based parameterization impact the interpretability and understanding of the model's decisions compared to traditional methods?\n2. Are there any particular tasks or domains where Quantum-PEFT might struggle to outperform existing PEFT methods, and if so, what challenges could be anticipated?"}}
{"paper_id": "ye1mxb79lw", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces a novel algorithm (BILBO) for bilevel Bayesian optimization that addresses challenges in noisy, constrained, and derivative-free settings. \n2. BILBO uses trusted sets to improve sample efficiency and provides a theoretical guarantee with sublinear regret bound. \n3. Empirical results demonstrate BILBO's effectiveness across synthetic and real-world problems, showing significant improvement over baselines.", "Weaknesses": "1. The approach may face scalability issues in high-dimensional spaces, which is common in Bayesian optimization. \n2. Although the paper proposes potential solutions to scalability, such as direct modeling and adaptive discretization, these remain as future work and are not fully explored in this paper.", "Questions": "1. How does the algorithm perform in high-dimensional settings beyond what is covered in the paper? \n2. Are there any specific application domains where BILBO might face significant limitations, and if so, what adaptations might be necessary? \n3. Can the method be generalized to multilevel optimization beyond two levels, and what are the anticipated challenges?"}}
{"paper_id": "1fwZJzGdKj", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper presents a novel multi-agent collaborative framework for efficient data selection during LLM pretraining, providing significant improvements in data efficiency and model performance.\n2. The approach dynamically integrates multiple data selection criteria, which addresses inherent conflicts between methods and results in up to 10.5% performance gains.\n3. The thorough experimental evaluation across various benchmarks and the use of different datasets showcases the framework's robustness and applicability.", "Weaknesses": "1. The method may require significant computational resources due to the complexity of maintaining and updating multiple agents.\n2. The framework relies heavily on the initialization of agents based on pre-defined heuristics, potentially limiting its generalization to new data domains.\n3. The requirement for carefully designed ablation studies hints at the complexity of the system, which might complicate reproducibility and practical deployment.", "Questions": "1. How sensitive is the multi-agent framework to the initial settings of the agent weights and their update strategies?\n2. Can the framework be adapted to other types of pretraining tasks beyond LLMs, such as vision or multimodal models?\n3. What are the potential computational trade-offs involved in implementing the collaborative data selection mechanism at a larger scale?"}}
{"paper_id": "pISLZG7ktL", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper presents a comprehensive empirical study on data scaling laws in robotic manipulation, contributing significant insights into how policy generalization can be improved through data diversity. 2. Establishes a clear relationship (power-law) between data scaling and policy performance, akin to trends observed in NLP and CV. 3. Proposes an efficient data collection strategy, demonstrating that generalizable single-task robot policies can be achieved with relatively modest data collection efforts.", "Weaknesses": "1. The study is limited to imitation learning and does not explore task-level generalization or reinforcement learning, which might additionally enhance scalability insights. 2. Experimental validation is restricted to four tasks, limiting the generalizability of the findings across a wider range of tasks and scenarios.", "Questions": "1. How do the identified data scaling laws extend to reinforcement learning contexts in robotic manipulation? 2. Are the power-law relationships observed for data scaling consistent across more diverse or complex tasks not covered in this study? 3. How might the efficiencies of data collection strategies be further enhanced, especially for tasks that involve more complex interactions or dexterity?"}}
{"paper_id": "dML3XGvWmy", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces a novel framework, Godel Agent, for recursive self-improvement of agents using large language models (LLMs), which is based on a compelling idea of self-reference and dynamic optimization. This could be a major step toward more autonomous and adaptive agent systems.\n2. The framework was comprehensively evaluated across multiple domains including coding, science, and math, demonstrating superior performance compared to both hand-designed agents and existing automated agent design methods.\n3. The recursive approach to self-improvement offers the potential to explore a larger design space, potentially uncovering previously unattainable optimizations.", "Weaknesses": "1. The concept largely hinges on the effectiveness of contemporary LLMs, and thus its scalability and general applicability might be limited by current LLM capabilities. Improvements in LLM architecture might be necessary for significant advancements in results.\n2. Safety considerations need more comprehensive treatment. The self-improving nature of Godel Agent poses potential risks and ethical challenges which require long-term strategies for mitigation.\n3. While the paper claims superior results, it does not delve deeply into the limitations and potential failure modes of the proposed framework, particularly in scenarios where environment feedback is ambiguous or misleading.", "Questions": "1. How does Godel Agent ensure safety and reliability as it enhances itself, particularly in environments where full control might not be feasible?\n2. Since Godel Agent relies heavily on LLMs for decision making and self-improvement, how does it manage LLM biases or errors that could propagate through its recursive improvements?\n3. Could Godel Agent be effectively applied in real-world applications with unpredictable or highly variable environments, and what modifications would be required to handle such scenarios?"}}
{"paper_id": "xlxGsX1pc7", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces U-MATH, a novel and comprehensive benchmark for evaluating university-level mathematical skills in LLMs, featuring 20% multimodal problems. This addresses existing gaps in benchmarks that focus primarily on basic levels of mathematics. The paper is well-structured with a clear presentation of the problem, methodology, and results. The experiments conducted offer valuable insights into the challenges LLMs face with advanced mathematical problems.", "Weaknesses": "While the dataset is comprehensive, the selection process might introduce biases due to favoring certain problem types or difficulty levels. The evaluation relies on LLMs as judges, which could introduce biases and may not be sufficiently reliable for complex reasoning tasks. In addition, the inclusion of only 20% visual problems, while reflective of some real-world settings, limits the evaluation of broader multimodal reasoning.", "Questions": "Can the benchmark be expanded to include a broader range of advanced topics? How does the evaluation account for potential biases introduced by using LLMs as judges? Is there a plan to ensure that the selection biases in problem types do not affect the evaluation of LLMs' mathematical capabilities?"}}
{"paper_id": "AjXkRZIvjB", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel benchmark, GSM-Symbolic, which provides a more nuanced and reliable evaluation of LLMs’ mathematical reasoning abilities. This allows for clearer insights and assessments that go beyond single-point metrics.\n2. The paper identifies critical limitations in current LLMs by highlighting their sensitivity to changes in question variables and their struggles with question complexity.\n3. The work contributes to understanding the non-formal nature of LLMs’ reasoning, showing how models can be prone to pattern matching instead of true logical reasoning.", "Weaknesses": "1. The paper primarily focuses on LLM limitations without proposing methods for improvement or mitigation strategies.\n2. While the study discusses several limitations, it might benefit from an expanded exploration of potential future research directions or methodologies to address the observed reasoning shortcomings in LLMs.", "Questions": "1. How might current training techniques be modified to reduce the sensitivity of LLMs to inconsequential changes in problem data?\n2. Can the GSM-Symbolic benchmark be expanded to include more complex mathematical reasoning tasks to further stress test LLM capabilities?\n3. What might be some practical applications or implications of the demonstrated variability in LLM mathematical reasoning for settings or fields that rely heavily on accurate arithmetic or logical reasoning?"}}
{"paper_id": "IJiTI0fB0e", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper presents a novel, theoretically grounded method to evaluate the diversity in prompt-based generative models by decomposing diversity into prompt-induced and model-induced components.\n2. The introduction of Conditional-Vendi and Information-Vendi scores provides a quantitative evaluation metric for conditional generative models, addressing a gap in existing methods.\n3. The work presents thorough theoretical justification and validation through numerical experiments across different generative models.", "Weaknesses": "1. The practical applicability of the proposed scores outside controlled experimental settings is yet to be established, especially for more complex real-world scenarios.\n2. While the theoretical contributions are strong, the emphasis on the practicality of the method in diverse real-world applications is somewhat limited.\n3. The dependency on effectively chosen kernel functions and parameters could pose challenges in broader applications.", "Questions": "1. How robust is the proposed method to changes in kernel functions or hyperparameters? Does it affect the consistency of the scores significantly?\n2. Could this method of evaluation be integrated into training algorithms for prompt-based generative models to improve their diversity?\n3. How does the Conditional-Vendi score perform in more complex dataset scenarios with overlapping modes in prompts?"}}
{"paper_id": "fs2Z2z3GRx", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel algorithm, FIG, for linear inverse problems that is both theoretically justified and experimentally effective. It demonstrates competitive performance across various tasks and datasets, particularly in challenging scenarios involving high measurement noise. The method is efficient in terms of runtime and memory usage, which is a significant advantage. The paper is well-organized, and the experimental results rigorously validate the proposed method.", "Weaknesses": "While FIG performs admirably on linear inverse problems, it cannot directly handle non-linear problems, which limits its generalizability. Additionally, FIG's compatibility is constrained to models without latent encodings due to its reliance on the linear relationship between measurements and signals.", "Questions": "Can the FIG algorithm be effectively extended to address non-linear inverse problems, potentially with modifications? How might the method be adapted for use with latent models?"}}
{"paper_id": "H4FSx06FCZ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. SecureGS addresses critical issues in 3DGS steganography, such as fidelity, speed, and security, effectively advancing the field.\n2. The paper introduces innovative techniques, including hybrid decoupled Gaussian encryption and region-aware density optimization, improving both security and performance.\n3. Experimental results demonstrate significant improvements in rendering quality, speed, and security compared to existing methods.", "Weaknesses": "1. The dependency on specific neural network structures and algorithms may limit the generalizability of the method across different applications or 3D frameworks.\n2. The approach's robustness to extremely high-density or complex environments was not fully explored and may present potential limitations.", "Questions": "1. How does SecureGS handle scenarios with highly complex or irregular 3D environments where Gaussian point density is naturally high?\n2. Can the proposed framework be extended to handle dynamic or animated scenes, or is it limited to static 3D assets?"}}
{"paper_id": "OGtUfA6Amo", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "Hi-Patch provides a novel hierarchical patch graph network approach to effectively model irregular multivariate time series (IMTS), addressing the limitation of existing methods that treat all variables uniformly regardless of their original scales. The paper thoroughly demonstrates superior performance across multiple datasets and tasks compared to state-of-the-art methods, illustrating the practical applicability and robustness of the proposed model.", "Weaknesses": "Although Hi-Patch appears effective, its need for optimal scale selection, as highlighted in the study, may introduce an overhead or dependency on dataset-specific configurations. Additionally, while the model efficiently handles observed data without interpolation, this might limit its performance in settings where missing data prediction is crucial.", "Questions": "How does Hi-Patch performance vary with datasets that have different levels of irregularity? Can the model adapt to datasets outside healthcare or climate that involve highly dynamic time variables, such as finance or network traffic data? Also, how sensitive is Hi-Patch to the choice of patch size beyond the datasets evaluated?"}}
{"paper_id": "ujpAYpFDEA", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper addresses an unexplored area in the field of watermarked Large Language Models (LLMs) by focusing on the imperceptibility of watermarks. \n2. It introduces Water-Probe, a novel unified identification method capable of identifying various watermarking techniques using crafted prompts, showing a minimal false-positive rate on non-watermarked LLMs.\n3. The Water-Bag strategy enhances watermark imperceptibility by introducing randomness into watermark key selection, presenting a potential method for LLM service providers to prevent user detection of watermarks.", "Weaknesses": "1. The practicality of creating indistinguishable prompts for the purposes of detecting watermark discrepancies might be challenging, especially in real-world applications where diverse text variations can diminish detection efficacy.\n2. While the Water-Bag strategy increases randomness, it concedes detection robustness, balancing the tradeoff might be hard without reducing watermark utility.\n3. Limited examination of how the proposed methods might apply in different LLM architectures and whether the imperceptible watermarking can scale effectively to various model sizes is not well-addressed.", "Questions": "1. How does Water-Probe handle variations in language model output that result purely from model sampling variability rather than watermarking alterations?\n2. Can these watermark detection methodologies be adapted effectively across language models with fundamentally different architectures or training protocols?\n3. Could there be potential ethical considerations or misuse cases for this research not covered in the paper, such as detecting commercial watermarking without authorization?"}}
{"paper_id": "7UKHNQIErp", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper proposes a novel formal framework to characterize preference alignment algorithms, providing a new perspective that could facilitate further development of loss functions.\n2. The work introduces a formalism that offers a deeper understanding of preference loss semantics, potentially guiding improvements in the systematic design of new loss functions.\n3. The approach delivers insights into the structure and exploration of the large landscape of DPA losses, showing how small modifications can lead to new variants with desirable properties.", "Weaknesses": "1. Despite the formal rigor, the presentation of the logical frameworks and decompilation procedures may be challenging for readers less familiar with the symbolic logic representations used.\n2. While the approach is compelling, experimental results are limited to case studies without extensive empirical validation across different LLM tasks.\n3. The practical impact of these theoretical insights on improving model alignment with human preferences needs further empirical substantiation.", "Questions": "1. How generalizable is the decompilation approach to other types of models or loss functions beyond DPA?\n2. Are there plans for broader empirical validation of the novel loss functions derived from this framework in diverse real-world scenarios?\n3. Can the framework be extended to handle multi-model scenarios or dynamic models where the preference structure might evolve over time?"}}
{"paper_id": "CpgWRFqxhD", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces MEMO, a novel method that significantly improves audio-driven talking video generation by addressing key challenges like audio-lip synchronization, identity consistency, and expression naturalness.\n2. MEMO incorporates innovative modules such as a memory-guided temporal module for enhancing long-term identity consistency and an emotion-aware audio module for better audio-video interaction.\n3. Extensive experiments and human evaluations highlight MEMO's superior performance over existing state-of-the-art methods, demonstrating strong generalization and applicability across diverse scenarios.", "Weaknesses": "1. The approach relies on extensive preprocessing and a sophisticated data pipeline, which might limit its adaptability to environments with less available resources.\n2. The requirement for a well-designed emotion detection model for optimal performance could be a potential barrier for reproduction or extension of this work by others in diverse settings. \n3. While the paper addresses multiple challenges in talking video generation, the integration of its approach into broader contexts or systems that require additional capabilities is not fully explored.", "Questions": "1. How sensitive is MEMO's performance to the quality and diversity of the training dataset and preprocessing pipeline?\n2. Is the proposed emotion-aware module easily adaptable to new emotions or variations that are not part of the initial training set?\n3. Can the MEMO framework be extended or modified to handle other modalities or applications, like full-body movement or object-focused animations?"}}
