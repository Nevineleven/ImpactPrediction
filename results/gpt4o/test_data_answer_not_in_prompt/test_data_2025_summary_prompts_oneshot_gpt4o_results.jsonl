{"paper_id": "u3xwwfHmBC", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel model, TTformer, that effectively combines tense-specific attention mechanisms with contrastive learning to capture complex spatio-temporal dependencies and improve traffic forecasting accuracy.\n2. The application of contrastive learning and the use of tense-specific attentions are innovative approaches that significantly enhance the model's robustness to incomplete data.\n3. Empirical results on real-world datasets demonstrate substantial performance improvements over existing baseline models.", "Weaknesses": "1. The paper may lack a detailed explanation of how specific design choices, such as the use of tense-specific modules, contribute to performance improvements compared to conventional methods.\n2. More comparative analysis with a broader range of state-of-the-art models would strengthen the position of TTformer as a leading solution in this domain.", "Questions": "1. How does TTformer handle scenarios with highly dynamic and unpredictable traffic patterns in real-time applications?\n2. Can the proposed model be generalized or adapted for other spatio-temporal forecasting tasks outside traffic prediction domains?"}}
{"paper_id": "YKvBiRWdQC", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces the Overcooked Generalisation Challenge (OGC), which is a significant advancement for assessing RL agents' generalization abilities in zero-shot cooperation scenarios. The utilization of diverse training levels through unsupervised environment design (UED), in combination with state-of-the-art DCD methods, adds value by pushing the boundaries of current RL evaluations. The paper is well-structured and clearly explains the novel additions such as OvercookedUED and Overcooked Mutator.", "Weaknesses": "Despite the promising framework, the evaluation demonstrates that current DCD algorithms struggle with the tasks presented in the OGC, highlighting a potential gap between the complexity of the challenge and the capabilities of existing methods. There is also a lack of detailed analysis on why these state-of-the-art algorithms performed poorly, which could have provided insights into specific areas for improvement.", "Questions": "What specific design choices in the Overcooked Generalisation Challenge led to the observed struggles of existing DCD algorithms? Are there particular aspects of the novel levels or partner dynamics that amplify this difficulty? Additionally, how does the challenge specifically model the human-like variability and cooperation dynamics that might be encountered in actual human-AI interactions?"}}
{"paper_id": "tePFpDgyqg", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The LongPack pipeline introduces an innovative approach to constructing long-context documents by leveraging natural referral signals, addressing a key gap in long-context training data.\n2. The experimental results show a significant improvement in model performance, achieving a 32.7% higher score than previous methodologies.\n3. The paper is well-organized and clearly presents the significance of maintaining long-distance referral density for long-context LLM training.", "Weaknesses": "1. The reliance on web page hyperlinks as referral signals may limit the applicability of the method to document domains that inherently lack such structure.\n2. The paper does not provide a detailed analysis of potential limitations regarding the scalability and computational requirements of the LongPack pipeline.", "Questions": "1. How does the LongPack approach handle documents or data sources that do not naturally contain referral signals like hyperlinks?\n2. Are there plans to validate the effectiveness of LongPack in different domains or types of text data beyond web pages?"}}
{"paper_id": "gx3LMRB15C", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper presents a novel augmentation-free self-supervised learning method specifically tailored for tabular data, addressing the challenges of meaningful data augmentation in this domain. \n2. T-JEPA demonstrates significant performance improvements in both classification and regression tasks compared to traditional models, showcasing its practical utility.\n3. The inclusion of regularization tokens to prevent representation collapse is a thoughtful addition that strengthens the method's robustness.", "Weaknesses": "1. While the method shows improvements over Gradient Boosted Decision Trees in certain settings, it is unclear how it compares to other advanced models or techniques specially designed for tabular data.\n2. The paper primarily focuses on comparing against models trained directly on original features, which might not fully capture the competitive landscape. Additional comparisons with recent state-of-the-art in tabular model designs would be beneficial.", "Questions": "1. How does T-JEPA compare with other self-supervised methods for tabular data in terms of computational efficiency and scalability for large-scale datasets?\n2. Are there specific types of tabular data or feature distributions where T-JEPA performs exceptionally well or poorly?\n3. Can the introduction of regularization tokens be further optimized or adapted for different types of tabular datasets?"}}
{"paper_id": "kTH3bEH6hW", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The proposed range-limited augmentation strategy shows significant improvements in preserving task-relevant information for tabular data in few-shot learning scenarios. \n2. The comprehensive evaluation using the FeSTa benchmark across multiple datasets demonstrates the robustness of the method.\n3. The approach is simple yet effective, indicating potential for practical application in real-world scenarios where labeled data is limited.", "Weaknesses": "1. The paper lacks a detailed exploration of the theoretical aspects or limitations of using predefined feature-specific ranges, which might limit the generalizability of the method.\n2. The method may require manual tuning of feature ranges, which could be challenging in practice for different datasets or when there is a lack of expert knowledge about the data.", "Questions": "1. How sensitive is the method to the selection ratio and the predefined ranges for features? \n2. Can the method be extended or adapted to work with other types of structured data beyond tabular data, and if so, how effectively?"}}
{"paper_id": "EqcLAU6gyU", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel agent-oriented planning framework that significantly enhances task decomposition and allocation in multi-agent systems.\n2. The integration of a reward model and feedback loop enhances the adaptability and efficiency of the framework.\n3. The framework has demonstrated superior empirical performance, indicating strong potential for real-world applications.", "Weaknesses": "1. The framework's reliance on the capabilities of large language models could pose limitations in scenarios with insufficient computational resources.\n2. The details of the baseline methods used for comparison in experiments are not fully discussed, which may impact the reproducibility and verification of the claims.", "Questions": "1. Can the framework be easily adapted to address specific domains or task types where domain-specific knowledge is critical?\n2. How does the framework handle task decomposition and allocation in highly dynamic environments where agent capabilities or available resources change frequently?"}}
{"paper_id": "qrTrnrEi9d", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The TransFusion method provides a novel and effective way to enhance multilingual information extraction tasks, especially for low-resource languages, by leveraging translation and annotation fusion.\n2. The method demonstrates significant performance improvements over current state-of-the-art LLMs in handling low-resource data, indicating practical value in multilingual NLP applications.\n3. The paper is clear and well-organized, making it easy to understand the methodology and results.", "Weaknesses": "1. The reliance on high-quality translations might limit the applicability of the approach in scenarios where such translations are unavailable or inaccurate.\n2. There is limited discussion on the scalability of this approach to numerous languages or domains with varying resource availability.", "Questions": "1. How does TransFusion perform when applied to languages that are structurally very different from English, and are there any limitations observed in such cases?\n2. Are there any plans to extend the current framework to directly combine low-resource language data annotations without relying solely on translations?"}}
{"paper_id": "Bp0HBaMNRl", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel differentiable causal discovery method that effectively handles nonlinear latent variables without assuming deterministic relations, significantly advancing the field.\n2. Strong theoretical groundwork is laid with new identifiability results, enhancing the method's applicability to various real-world scenarios.\n3. The method shows excellent performance and scalability in experiments, making it a valuable addition to causal inference techniques, especially given its application to high-dimensional image data.", "Weaknesses": "1. The presentation could be improved; certain sections may require more clarity, especially for readers not deeply familiar with causal discovery and related theoretical concepts.\n2. Although the paper demonstrates significant results, the complexity of the method might make it challenging for practitioners to quickly apply the techniques in various domains without a deep understanding of the model intricacies.", "Questions": "1. How does the performance of the proposed method compare in scenarios with varying degrees of nonlinearity and noise levels? Are there specific types of data or contexts where it performs inadequately?\n2. Can the method be efficiently adapted and scaled to integrate more complex observational datasets from domains beyond imaging, such as in genomics or large-scale text data?"}}
{"paper_id": "JzLcKWtGnl", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel approach that effectively enhances spatial perception and reasoning in 3D vision-language tasks through a progressive spatial awareness scheme. It demonstrates significant performance improvements over state-of-the-art techniques by leveraging enriched spatial information at multiple levels. The presentation of the method is clear and well-structured, making complex concepts accessible.", "Weaknesses": "The method relies heavily on the progressive spatial awareness scheme that may not be easily adaptable to other types of 3D vision models or datasets. Furthermore, the paper does not provide a thorough ablation study to isolate the contribution of each component within the proposed model.", "Questions": "1. How well does the Spatial 3D-LLM perform when integrated with different language model backbones?\n2. Can the proposed progressive spatial awareness scheme be generalized to non-3D tasks or incorporated into existing 2D models?"}}
{"paper_id": "hXJrQWIoR3", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper successfully addresses a gap in explainable graph representation learning by proposing novel methods that enhance interpretability.\n2. Comprehensive experiments on real-world datasets validate the effectiveness and robustness of the proposed framework, demonstrating significant improvements over baselines.\n3. The combination of theoretical analysis and practical experimentation provides a strong foundation for the proposed methods.", "Weaknesses": "1. While the paper focuses on theoretical analyses and experiments, the practical applicability of the methods in diverse graph domains could be further explored.\n2. The complexity of the proposed framework, which involves multiple steps of kernel-based pattern analysis and GNN adaptation, might present scalability issues in very large graphs or real-time applications.", "Questions": "1. How does the proposed framework scale with increasing graph size, particularly for very large-scale graphs?\n2. Can the explanatory capabilities of the method be effectively visualized for end-users to enhance comprehensibility?\n3. How sensitive are the results to the choice of parameters in the unsupervised ensemble graph kernel method?"}}
{"paper_id": "67sSPPAZiG", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The creation of a large-scale egocentric QA dataset is a significant contribution to the field, providing a valuable resource for future research. \n2. The MM-Ego model introduces innovative mechanisms such as Memory Pointer Prompting, which effectively addresses challenges in processing long egocentric videos.\n3. The introduction of a specialized evaluation benchmark (EgoMemoria) demonstrates comprehensive efforts to advance model evaluation methods specific to egocentric video understanding.", "Weaknesses": "1. The paper might not address underlying biases in video content selection beyond the dataset's language bias, potentially affecting generalization.\n2. Although impressive, the reliance on a large-scale dataset may not be feasible for all research teams, limiting replication and further exploration in contexts where such data collection is challenging.", "Questions": "1. How does the MM-Ego model perform when scaling to even longer video content or when integrated with real-time processing in AR/VR environments?\n2. How generalizable are the dataset and benchmark to different cultural contexts or when applied to videos depicting activities outside those typically documented in egocentric video datasets?"}}
{"paper_id": "kjmLabjSE2", "error": "'NoneType' object has no attribute 'model_dump'"}
{"paper_id": "I18MA5DjoP", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The NPL framework provides a novel perspective on understanding neuron behavior within transformer-based LLMs, making a significant contribution to the field of interpretability in AI.\n2. The paper presents a comprehensive experimentation using well-known models such as LLaMA-2 and GPT-J, which strengthens the validation of the proposed framework.\n3. The study enhances interpretability by highlighting predictable neuron activations, correlating them with neuron importance and uncovering functional neurons (background neurons) critical for performance.", "Weaknesses": "1. The framework seems limited to specific architectures or types of models and may require substantial adaptation for broader applicability across different types of transformer-based models.\n2. While the framework is promising, practical applications or use cases that directly benefit AI systems in real-world scenarios need further exploration and validation.", "Questions": "1. Can the NPL framework be adapted or extended to analyze other components of LLMs beyond FFN neurons?\n2. How scalable is the NPL approach when applied to larger datasets or more complex models beyond those used in the study?\n3. Are there any observed limitations regarding NPL's performance or accuracy that might affect its utility in analyzing or optimizing models for real-world applications?"}}
{"paper_id": "qpDqO7qa3R", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper addresses a significant gap in video restoration by leveraging pre-trained image diffusion models, effectively adapting them to video tasks without retraining. \n2. The proposed method, involving hierarchical latent warping and hybrid flow-guided token merging, offers a novel approach to maintain temporal consistency and detail generation in video restoration.\n3. The method is versatile, working with any pre-trained 2D image restoration diffusion model, making it highly applicable to various video enhancement tasks.", "Weaknesses": "1. The key challenge might lie in the practical implementation and computational complexity of the hierarchical latent warping and hybrid token merging, especially in large-scale video datasets with diverse degradation patterns.\n2. While the approach shows improvement over existing methods, the lack of additional training or fine-tuning might limit its performance in some edge cases where model adaptation is required.", "Questions": "1. How does the computational cost of this approach compare to state-of-the-art models, particularly in large-scale video tasks beyond the tested datasets?\n2. Are there specific types of degradations or video complexities where the method struggles to maintain consistency or quality?\n3. Can this approach be extended or adapted for real-time video processing scenarios?"}}
{"paper_id": "dSQtMx6dPE", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses a critical gap in graph prompt learning by introducing privacy-preserving algorithms (DP-GPL and DP-GPL+W) leveraging the PATE framework, which demonstrate strong practicality and efficacy in ensuring privacy while maintaining performance. The evaluation across multiple datasets and methods provides a comprehensive validation of their approach.", "Weaknesses": "While the proposed methods show promising results, the paper could elaborate more on the limitations or potential challenges in their approach, particularly regarding the scalability of the PATE framework and the overhead introduced by the privacy-preserving methods.", "Questions": "1. How does the overhead introduced by the privacy mechanisms affect the scalability of GNN models in larger or more complex datasets?\n2. Are there specific scenarios or types of graph data where the proposed privacy methods might be less effective?"}}
{"paper_id": "GbgCRJedQ7", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces a novel and effective method, Sparse Matrix Tuning (SMT), that bridges the gap between parameter-efficient fine-tuning and full fine-tuning, offering a significant improvement in performance without the cost and memory overhead.\n2. SMT demonstrates superior performance over state-of-the-art methods like LoRA and DoRA by smartly leveraging sparse sub-matrix updates in attention mechanisms, showing its impact on downstream task accuracy.\n3. The approach is computationally efficient and reduces memory requirements, making it practical for real-world applications with large language models.", "Weaknesses": "1. While SMT provides a promising solution, the method may rely heavily on the initial selection of sparse sub-matrices, which might be challenging or poorly adaptive in complex or dynamic tasks.\n2. The paper may not extensively explore the potential limitations or circumstances where SMT might not outperform existing methods, thus reducing transparency about its broad applicability.", "Questions": "1. How does the initial selection and ranking of sparse sub-matrices affect the robustness and adaptability of SMT across different tasks or domains?\n2. Can SMT be integrated with other model architectures or adapt beyond the focus on attention mechanisms, especially within transformers?"}}
{"paper_id": "ZZVOrId3yN", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. CrossModalNet offers a novel architectural design that effectively addresses the integration of diverse data streams in multimodal medical image segmentation.\n2. The inclusion of advanced mathematical frameworks and domain adaptation techniques significantly improves the robustness and generalization of segmentation models.\n3. Extensive experiments on the MM-WHS dataset highlight the architecture's superior performance and reliable generalization capabilities.", "Weaknesses": "1. The paper does not discuss potential limitations or computational costs associated with the dual-stream cross-network design in detail.\n2. The reliance on specific datasets such as MM-WHS might limit understanding of the architecture's performance across a wider variety of medical imaging contexts.", "Questions": "1. How does the proposed model handle the integration of additional modalities beyond those tested in the MM-WHS dataset?\n2. Could there be potential bottlenecks in terms of computational resources required for training and deploying CrossModalNet in clinical settings?"}}
{"paper_id": "gLHuAYGs6a", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The proposed SMvCNet method effectively integrates multi-view information using a heterogeneous graph, which is an innovative approach in the context of multi-view clustering. \n2. The use of multi-step random walks to capture high-order structures and derive a unified sample-level structure is both novel and effective, achieving superior clustering performance.\n3. Extensive experiments on multiple datasets demonstrate the robustness and potential of the proposed method, using a variety of clustering metrics.", "Weaknesses": "1. While the method shows strong performance, there may be limitations regarding its scalability to very large datasets or real-time applications due to the complexity of heterogeneous graph construction and multi-step random walks.\n2. The presentation could be improved with clearer explanations or more intuitive examples to make the methodology more accessible to a broader audience.", "Questions": "1. How does SMvCNet handle scalability issues when applied to large-scale datasets, and are there any optimizations available to mitigate computational overhead?\n2. Can the proposed method be extended or adapted to other forms of clustering beyond multi-view settings, such as single-view or hierarchical clustering?"}}
{"paper_id": "x5l5PRtvul", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper proposes h-SVGD, a novel approach addressing the variance collapse issue in SVGD without incurring additional computational costs. This advancement is significant as it maintains practicality in high-dimensional inference tasks.\n2. The theoretical analysis provided on h-SVGD adds value by proving the existence of solutions to the hybrid Stein PDE and exploring its behavior as a kernelized Wasserstein gradient flow.\n3. The experimental evaluation is methodically conducted, demonstrating the effectiveness of h-SVGD in challenging high-dimensional settings.", "Weaknesses": "1. While h-SVGD improves variance estimation, the method's fixed point not matching the target distribution in the mean field limit suggests a gap in achieving full alignment with theoretical objectives.\n2. The paper lacks clarity in its presentation, especially around the explanation of the hybrid kernel approach, making it less accessible to readers unfamiliar with advanced kernel methods in SVGD.\n3. There's limited discussion on the implications of kernel choice on the performance and how this can be systematically approached or optimized in practice.", "Questions": "1. Can the hybrid kernel approach in h-SVGD be generalized to other forms of variational inference beyond the settings tested in this paper?\n2. What specific challenges are associated with further refining kernel scaling techniques, and how might they impact the practical adoption of h-SVGD?\n3. How sensitive is h-SVGD's performance to the initial choice of kernels, and what guidelines can be provided for practitioners selecting these kernels?"}}
{"paper_id": "pOUAVXnOQP", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The introduction of Sinusoidal Trainable Activation Functions (STAF) represents a novel contribution to addressing spectral bias in neural networks used for Implicit Neural Representations. The method's ability to enhance high-frequency detail reconstruction, coupled with improved PSNR and faster convergence, demonstrates substantial advancements over existing methods. The combination of theoretical insights and empirical validation strengthens the paper's impact.", "Weaknesses": "While the STAF method shows promising results, the paper could benefit from additional exploration of the broader applicability of the method beyond the presented use cases. Furthermore, the initialization scheme and its impact on optimization require deeper exploration to ensure robustness across diverse scenarios.", "Questions": "What are the specific limitations of the STAF approach in practice? Are there particular types of signals or reconstructions where STAF underperforms compared to traditional methods? How transferable is the initialization scheme to different types of neural network architectures?"}}
{"paper_id": "iN64nSYt0z", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. GUIDE provides a novel, resource-efficient approach to enhance LLM alignment with user instructions without the need for additional training, which could broaden the accessibility of improved LLM performance across different use cases.\n2. The introduction of the Influence metric offers an innovative way to measure and understand the importance of instruction tokens within transformer models, contributing to explainability in AI systems.\n3. The open-source release of both GUIDE and Influence encourages further research and application development, fostering a collaborative advancement of techniques in this domain.", "Weaknesses": "1. The effectiveness of the GUIDE method appears to heavily depend on the quality of the tagging system and the choice of tags, which might introduce bias or require expertise in tag selection.\n2. While the Influence metric is a promising tool for analyzing token importance, further validation is needed to establish its generalizability across diverse LLM architectures and tasks.", "Questions": "1. How does the GUIDE approach perform across different languages or domains that were not included in the initial evaluation? Is its performance consistent?\n2. Given that GUIDE uses a tagging system, are there any guidelines or best practices provided to aid users in the selection and application of appropriate tags for various tasks?"}}
{"paper_id": "gRuZkEy49k", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces a novel integration of Monte Carlo Tree Search with GFlowNet training through Monte Carlo DAG Search, significantly improving data sampling efficiency.\n2. The proposed method, MCDS, excels in balancing exploration and exploitation during training, resulting in more efficient GFlowNet models across various environments.\n3. Empirical results demonstrate the effectiveness of MCDS compared to existing on-policy approaches, showing comprehensive validation across multiple scenarios.", "Weaknesses": "1. The presentation of the method could be clearer, particularly in explaining the complexities and intricacies involved in adapting MCTS to GFlowNets.\n2. Although the paper successfully introduces MCDS, it lacks discussion on potential limitations or drawbacks when applied to very high-dimensional spaces or diverse problem settings.", "Questions": "1. How does the performance of MCDS scale with increasing complexity of the discrete spaces or DAG structures? Are there any known performance bottlenecks?\n2. Can the principles underlying MCDS be adapted for continuous state and action spaces, or is the method inherently constrained to discrete settings?\n3. What are the computational overheads associated with integrating MCDS in GFlowNet training compared to traditional sampling methods?"}}
{"paper_id": "d23EVDRJ6g", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. MotionDreamer effectively addresses the challenge of motion synthesis from single or few sequences by introducing sliding window local attention and novel quantization techniques, which help in capturing and generating diverse motion patterns. \n2. The approach shows state-of-the-art performance with significant improvements in quantitative and user perceptual metrics, indicating its effectiveness and practical applicability.\n3. The paper is well-structured and clearly presents the methodology and its comparative advantages over existing methods.", "Weaknesses": "1. The method's reliance on single reference motions, while innovative, might face limitations in capturing more complex animations involving interactions between multiple entities or varying motion dynamics.\n2. Although the paper mentions its applications, further exploration into how MotionDreamer scales with more complex animation scenarios or larger datasets would be beneficial.", "Questions": "1. How does the model perform when scaled to handle more complex animations involving interactions or varying motion dynamics?\n2. Can MotionDreamer be generalized to incorporate multiple reference motions or is it strictly limited to single motion references?\n3. What are the limitations, if any, of the sliding window local attention mechanism in terms of computational efficiency during real-time applications?"}}
{"paper_id": "kQ5s9Yh0WI", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses a relevant issue of generating ultra-long outputs in LLMs and proposes a novel solution with AgentWrite, enhancing the generative capabilities beyond current limits. The introduction of the LongWriter-6k dataset and LongBench-Write benchmark provides a comprehensive evaluation framework for long-text generation. The study demonstrates state-of-the-art performance and scalability of output lengths, potentially broadening the applicability of LLMs.", "Weaknesses": "While the methodology is innovative, implementation details regarding the breakdown of tasks in AgentWrite are not fully expounded, which could make reproduction of results challenging. The dependency on a large and high-quality dataset like LongWriter-6k might limit the practical adaptation of the technique without access to similarly extensive datasets.", "Questions": "How does the breakdown mechanism in AgentWrite determine the granularity of subtasks to ensure coherent long-document generation? Are there specific guidelines for constructing similar datasets to LongWriter-6k that maintain quality and coherence across varied topics?"}}
{"paper_id": "9CqkpQExe2", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The proposed Ada-K routing method offers a significant advancement in efficiently utilizing MoE architectures by adapting expert activation dynamically per token, addressing an important limitation in traditional static routing methods.\n2. The approach is innovative, leveraging Proximal Policy Optimization (PPO) for training allocator modules within a non-differentiable framework, and has been validated through extensive experiments displaying tangible improvements in both efficiency and accuracy.\n3. The paper provides comprehensive evaluation data, demonstrating reductions in FLOPs and improvements in inference speed across multiple benchmarks, which showcases the practical relevance and potential impact of the work.", "Weaknesses": "1. While Ada-K routing shows impressive performance improvements, the paper could provide more insights into the potential overhead and complexity introduced by implementing and maintaining the allocator modules.\n2. The adaptability of the approach to varied and less structured tasks or datasets remains to be thoroughly explored within this paper, potentially limiting its generalizability at the current stage.", "Questions": "1. How does the Ada-K routing strategy handle out-of-distribution or unexpected inputs that may affect its context-based decision-making?\n2. Are there any tasks or datasets where the dynamic allocation does not bring significant performance improvement, and what characteristics do these tasks share?"}}
{"paper_id": "PpYy0dR3Qw", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The LoCoDL algorithm effectively combines Local Training and Communication Compression, offering a significant improvement in communication efficiency which is a primary challenge in Federated Learning.\n2. The method is backed by a solid theoretical framework that provides linear convergence guarantees, enhancing its reliability and applicability in diverse data settings.\n3. The approach shows innovation in utilizing random primal-dual techniques and probabilistic communication schedules, which could inspire further research in the field.", "Weaknesses": "1. While the algorithm's theoretical aspects are well-explained, the paper could benefit from more detailed experimental results demonstrating performance gains over a broader range of practical scenarios.\n2. The reliance on unbiased compressors from the class U(ω) might limit the applicability of the method to situations where these compressors are not readily available or optimal.", "Questions": "1. How does LoCoDL perform in highly heterogeneous data distributions commonly observed in federated learning environments?\n2. Are there any practical limitations when applying LoCoDL in real-world FL networks with highly unreliable communication channels?"}}
{"paper_id": "UatDdAlr2x", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper provides a novel exploration of the role and interaction of transformer components, specifically focusing on simple counting tasks.\n2. It offers theoretical insights into the mechanism by which transformers can implement counting strategies, which may guide future architectural designs.", "Weaknesses": "1. The focus on a relatively simple task (counting) may limit the perceived impact or applicability of the results to more complex real-world applications.\n2. The paper does not explore how these insights apply to larger, more complex models, and it remains unclear how these findings generalize beyond the controlled tasks examined.", "Questions": "1. Can the insights about transformer component interactions be applied to larger transformer models used in practical scenarios?\n2. How do different hyperparameters impact the efficiency and accuracy of more complex tasks beyond counting when using transformers as analyzed in this study?"}}
{"paper_id": "IwgmgidYPS", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel and highly scalable method for generating a large-scale medical dataset with multigranular annotations, addressing a significant gap in the availability of comprehensive medical datasets.\n2. MedTrinity-25M demonstrates its value by enabling superior performance on various medical visual QA benchmarks, showcasing the potential impact on medical AI pretraining.\n3. The use of large multimodal models and medical knowledge retrieval pipelines to automate data annotation is innovative and promises broader applicability.", "Weaknesses": "1. While the dataset is extensive and the method innovative, the paper may lack detailed analysis on the generalizability of the dataset across diverse medical applications outside the ones tested.\n2. The reliance on the quality of automated annotation methods (e.g., multimodal models, knowledge retrieval) might introduce biases or inaccuracies, which could affect downstream model performance and should be addressed in more depth.", "Questions": "1. How does the performance of models pre-trained on MedTrinity-25M compare across different types of medical imaging tasks, especially those not explicitly validated in the paper?\n2. Are there specific examples where the automated pipeline's annotations failed or produced misleading information, and how might these be addressed in future iterations?"}}
{"paper_id": "WG7GzGx3G9", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The proposed Rotated Runtime Smooth (RRS) method significantly enhances the accuracy of low-bit (INT4) quantization while managing activation outliers effectively.\n2. The approach demonstrates considerable improvement over state-of-the-art methods like SmoothQuant and QuaRot, especially in terms of perplexity scores on established models like LLaMA and Qwen.\n3. RRS maintains low computational overhead and is compatible with existing hardware, showcasing strong practicality and applicability for large language model inference.", "Weaknesses": "1. The method's effectiveness and efficiency have been demonstrated on specific models, and it is unclear how well it generalizes to a broader range of models and architectures.\n2. Providing more detailed benchmarks or comparisons under varying conditions would enhance the robustness and reliability of the proposed solution.", "Questions": "1. Can RRS be applied effectively to other architectures beyond the ones tested in the paper, such as different domains or applications outside of large language models?\n2. What are the potential limitations of the method when it comes to extremely low resource hardware or deployment in resource-constrained environments?"}}
{"paper_id": "BhECSDSkAE", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper presents a novel approach using a memory mechanism and self-distillation for adapting static image-trained models to video segmentation, effectively addressing the lack of annotated data in medical videos.\n2. The method introduces a practical solution to the gap in medical video segmentation by leveraging existing static image datasets, making it applicable to real-world scenarios where annotated video data is scarce.\n3. Empirical results demonstrate the method's superiority over existing few-shot and cross-domain methods, showcasing both innovation and practical application.", "Weaknesses": "1. The reliance on a newly constructed OS-I2V-Seg dataset for evaluation may limit the generalizability of the results if this dataset does not sufficiently represent diverse video content found in medical fields.\n2. While effective in low-data settings, the approach might struggle with more complex medical videos where temporal inconsistencies between frames can be more pronounced, potentially requiring more sophisticated memory mechanisms.", "Questions": "1. How does the memory mechanism scale with increasing video length or varying video quality that might be encountered in different medical domains?\n2. Can the proposed method be easily adapted or extended to settings beyond medical imaging where video segmentation is required?"}}
{"paper_id": "mnB4hDTIDr", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper addresses a significant under-explored issue of evaluating diversity in LLM-generated datasets, which is crucial for improving dataset quality.\n2. DCScore, the proposed method, effectively balances semantic diversity evaluation with computational efficiency.\n3. The method exhibits strong correlation with known diversity indicators such as human judgment, validating its effectiveness.", "Weaknesses": "1. While the paper introduces a novel approach, the contribution to diversity evaluation methods in LLM-generated datasets is somewhat incremental by leveraging existing classification principles.\n2. The reliance on classification-based evaluation might oversimplify complex diversity phenomena in intricate datasets.", "Questions": "1. How does DCScore perform on datasets with extreme levels of diversity (e.g., very low or very high diversity)?\n2. Can this method be adapted to evaluate diversity in multi-modal datasets generated by LLMs, involving text, images, and audio?\n3. What are the limitations of DCScore when compared to traditional diversity evaluation methods in terms of precision and recall?"}}
{"paper_id": "Y0qmwm6tgy", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The proposed MoreauPruner introduces a novel method for robustly estimating weight importance using the Moreau envelope, which is innovative in the context of handling weight perturbations.\n2. The approach is well-evaluated on several state-of-the-art language models, demonstrating its effectiveness in maintaining performance post-pruning and offering stable outcomes across different weight formats.", "Weaknesses": "1. The paper may not sufficiently address the implementation complexity of the Moreau envelope and proximal gradient descent in large models, which could affect practical applicability.\n2. The method relies on additional fine-tuning steps (with LoRA), raising questions about the overall computational efficiency gains achieved through pruning.", "Questions": "1. How does the computational overhead of using the Moreau envelope and proximal gradient descent compare to traditional pruning methods?\n2. Are there scenarios or specific use cases where the MoreauPruner would be less effective, particularly in environments with extremely limited computational resources?"}}
{"paper_id": "aAcOaJYbUg", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The creation of the LAION-C benchmark offers a novel and more realistic challenge for evaluating out-of-distribution robustness of vision models.\n2. Comprehensive evaluation and use of psychophysical experiments provide convincing evidence of the benchmark's effectiveness.\n3. The paper successfully highlights the inadequacy of existing benchmarks like ImageNet-C for modern web-scale trained models.", "Weaknesses": "1. The approach may have limitations in terms of generalizability to other domains outside of image classification.\n2. The dependence on synthetic distortions may overlook some real-world OOD challenges that models could face.", "Questions": "1. How does LAION-C ensure that the applied distortions do not inadvertently introduce biases or artifacts that could affect model evaluation?\n2. Can the methodology for creating LAION-C be adapted to other domains, such as text or audio data, to evaluate OOD robustness in those areas?"}}
{"paper_id": "duGygkA3QR", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel integration of Dynamic Mode Decomposition (DMD) into Graph Neural Networks (GNNs), offering a new theoretical perspective informed by dynamical systems. The proposed DMD-GNN framework is shown to enhance performance across various graph learning tasks, validating its practical applicability.", "Weaknesses": "While the paper presents strong theoretical insights and empirical results, the presentation could be clearer, particularly in describing the integration process of DMD with GNNs. Additionally, the practicality and computational efficiency of applying DMD in large-scale real-world graph settings may require further exploration.", "Questions": "1. How does the computational overhead introduced by using DMD in GNNs compare to traditional GNN approaches in large-scale graphs?\n2. Are there specific graph structures or properties where the DMD-GNN method does not perform well?\n3. Can the incorporation of domain-specific constraints in DMD-GNN be generalized to other types of graphs and real-world applications?"}}
{"paper_id": "s6nYndMwG7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper presents a novel approach to addressing the computational challenges in calculating influence functions. \n2. It proposes the use of spectral properties of the Hessian, which simplifies hyperparameter tuning for the LiSSA algorithm employed in influence function calculations.\n3. Comprehensive experiments are conducted on different models, indicating the practicality of the proposed method.", "Weaknesses": "1. The paper's contribution is more focused on computational efficiency and simplification of hyperparameter tuning rather than a fundamental advance in the theoretical understanding or application of influence functions.\n2. It relies heavily on empirical validation without extensive theoretical justification for why the spectral properties approach works universally across different model types.", "Questions": "1. How sensitive is the proposed method to variations in dataset sizes and model architectures?\n2. Can this method be extended or adapted to other algorithms or domains outside of the ones tested, such as unsupervised learning scenarios?"}}
{"paper_id": "9BVMD3keG8", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper addresses a novel area within online learning by incorporating contextual information in brokerage for bilateral trade. This adds a fresh dimension to the existing literature in OTC markets.\n2. The method proposed achieves optimal regret bounds under two distinct feedback mechanisms, demonstrating strong theoretical contributions.\n3. The study establishes the necessity of certain assumptions for learnability, providing clarity on the conditions required for successful application.", "Weaknesses": "1. The reliance on bounded density assumption might limit the applicability of the proposed algorithms to real-world scenarios, where such assumptions may not hold.\n2. Presentation could be improved to better explain complex mathematical concepts and assumptions to a broader audience not deeply familiar with the subject.", "Questions": "1. How does the performance of the proposed methods change when certain assumptions, such as bounded densities, are relaxed?\n2. Could the approach be adapted or extended to account for dynamic or changing contexts in a more robust manner?\n3. Is there a practical implementation of this method in any real-world OTC market setting, and if so, what were the outcomes?"}}
{"paper_id": "mnna9LUg7P", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper introduces a novel quantization technique tailored for State Space Models, which addresses the unique challenges posed by their activation patterns.\n2. Quamba successfully reduces both memory footprint and latency of SSMs while maintaining competitive accuracy, demonstrating efficacy in real-world deployment scenarios.\n3. The method is well-validated through experiments on zero-shot tasks across cloud and edge platforms.", "Weaknesses": "1. While the tailored quantization approach is innovative, the paper may benefit from more detailed comparisons with other quantization methods not specific to Transformers to clearly establish its advantage.\n2. The presentation of results, particularly regarding the accuracy trade-offs and deployment scenarios, could be clarified to better judge practical implications.", "Questions": "1. How does Quamba's performance and efficiency compare with other non-tailored quantization methods when applied to SSMs?\n2. Could the Hadamard transform approach for managing outliers be applied to other model architectures beyond SSMs?\n3. How robust is the method to variations in input data distributions or different model configurations beyond Mamba 2.8B?"}}
{"paper_id": "3X3LuwzZrl", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel approach to multi-label node classification by decomposing GNN message passing and analyzing these processes separately to harness label influence correlations, which addresses a critical gap in MLNC methodologies. \n2. The proposed LIP framework consistently outperforms state-of-the-art methods on benchmark datasets, demonstrating its utility and effectiveness.\n3. Clear validation with empirical evidence of superior performance enhances the credibility of the approach.", "Weaknesses": "1. The methodology may rely heavily on the constructed label influence graph, which might introduce complexity or overhead in extremely large graph datasets. \n2. Further detailed analysis may be needed to understand under what specific conditions or types of graphs the method might struggle or require adjustments.", "Questions": "1. How does the LIP framework handle scalability in extremely large or dynamic graph datasets? Is there a threshold at which the overhead of constructing label influence graphs becomes prohibitive?\n2. Can the approach be extended beyond current benchmark datasets to other real-world applications with similar MLNC needs, and would any particular adaptations be necessary?"}}
{"paper_id": "IqaQZ1Jdky", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel approach to enhancing neural network architectures by using a variable function basis in the KAN framework, which improves flexibility and robustness.\n2. By employing Bernstein polynomials and Bayesian optimization to dynamically adapt the polynomial degree, the method offers a theoretically grounded and practical approach to improving neural network performance across diverse datasets.\n3. The VBn-KAN framework shows significant improvements over traditional networks and KAN variants in terms of accuracy, adaptability, and interpretability.", "Weaknesses": "1. The paper may require further exploration of the computational costs associated with the proposed method, especially concerning Bayesian optimization in larger datasets or more complex tasks.\n2. The practical applicability of the method in industry settings, particularly in real-time systems, may need more detailed examination and empirical evidence.", "Questions": "1. How does the computational complexity of VBn-KAN scale with larger datasets, and is there a specific threshold where this approach becomes computationally prohibitive?\n2. Are there specific types of neural network tasks or structures where VBn-KAN might not be suitable or provide limited advantages over traditional methods?"}}
{"paper_id": "OdMqKszKSd", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel and scalable method for generating articulated 3D objects from a single image using a diffusion model, addressing a significant gap in the field of 3D asset creation. The method incorporates a comprehensive pipeline that includes a coarse-to-fine generation process, leveraging advanced techniques such as graph relation attention and the use of large pretrained models like DINOv2 and GPT-4o. The results demonstrate superior performance, generalization capabilities, and are validated by systematic evaluations and user studies that affirm the improved realism and consistency of the generated assets.", "Weaknesses": "The proposed method may still have limitations in extremely complex object configurations and might face challenges in environments requiring instantaneous creation of 3D assets due to the computational demands of the generation process. Additionally, the intricacies involved in the implementation could hinder immediate applicability for non-technical practitioners.", "Questions": "1. How does the method cope with occlusions or when key features of the object are not visible in the input image?\n2. Can the system handle dynamic environments where the articulated object might need to interact with rapidly changing surroundings?"}}
{"paper_id": "LOBhVTtVnc", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The introduction of Geometric Spatiotemporal Transformers (GSTs) is a novel approach that leverages the strengths of Transformer architectures in an underexplored area, addressing the limitations of existing GNN-based methods.\n2. The model design skillfully incorporates E(3) symmetries, ensuring invariance and enhancing simulation accuracy across different scales, with the Temporal Difference Graph playing a crucial role in mitigating cumulative errors.\n3. Empirical results demonstrate significant improvements over state-of-the-art methods in long-term dynamics simulation, validating the effectiveness of GSTs in handling variable-length historical data and complex physical interactions.", "Weaknesses": "1. While the GST model shows promise, the paper would benefit from a more detailed analysis of its limitations and potential challenges in scaling or adapting to other application domains.\n2. The dependence on precise geometric structure and the inherent complexity of the model might limit its applicability to simpler or less-structured systems, which is not fully addressed.\n3. The paper could have provided more comprehensive comparisons or baselines, including other potential Transformer-based adaptations in similar contexts, if available.", "Questions": "1. How does the computational complexity of GSTs compare to traditional GNN and Transformer architectures, both in training and inference?\n2. Can the authors elaborate on the potential scalability challenges or limitations when applying GSTs to larger, more complex systems?\n3. Are there plans to explore the applicability of this architecture beyond the current datasets, and if so, what modifications might be necessary for such extensions?"}}
{"paper_id": "oa5UeyUVMm", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper provides a novel application of diffusion probabilistic models to graph representation learning, bridging a significant gap in the literature.\n2. Graffe demonstrates state-of-the-art performance in node and graph classification tasks across multiple datasets.\n3. The theoretical framework introduced offers valuable insights into the integration of InfoMax principles within diffusion models, potentially inspiring further research.", "Weaknesses": "1. While Graffe is theoretically sound, the complexity and specific requirements for integrating diffusion models with graph neural networks could limit its immediate applicability in more varied domains.\n2. The presentation could benefit from clearer explanations and illustrations, particularly in the methodological section, to enhance the accessibility of the technical content to a broader audience.", "Questions": "1. How does Graffe handle scalability issues when applied to very large graphs with millions of nodes?\n2. Are there any specific types of graphs or datasets where Graffe underperforms, and what might be the reasons for this?\n3. Can the approach be extended to other non-Euclidean data structures beyond graphs, or are there fundamental limitations?"}}
{"paper_id": "5swfKRkCx7", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces an innovative approach to improve retrieval-augmented generation in language models through the integration of an additional attention mechanism. This method enhances the utilization of external knowledge, leading to improved performance in knowledge-intensive question-answering tasks. The dynamic and layer-wise integration of knowledge offers a novel solution to the limitations of existing RAG methods.", "Weaknesses": "The paper may not fully address scenarios where the retrieved knowledge itself is of low quality or incorrect, which could still lead to suboptimal model performance. Additionally, the complexity introduced by the memory-based layer-wise integration could increase computational overhead, which might not be adequately discussed in terms of efficiency or scalability.", "Questions": "How does the proposed method handle situations where the retrieved knowledge is incorrect or irrelevant? Can the memory-based approach be efficiently scaled for very large LLMs and datasets, and what are the computational trade-offs involved?"}}
{"paper_id": "5s1qpjrNvZ", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper presents a novel Guided Reinforcement Learning algorithm with a performance guarantee, addressing the problem of static guide policy sampling rates. \n2. The introduction of the roll-back feature (GRL-RB) for dynamic adjustment of the sampling rate is innovative and provides robustness across various tasks.", "Weaknesses": "1. The paper relies on specific assumptions to derive the guide sampling rate, which may limit the generalizability of the approach in more diverse or dynamic environments. \n2. While the method has been validated in structured environments, its efficacy and performance may vary significantly in real-world applications where environments are less predictable.", "Questions": "1. How sensitive is the proposed GRL algorithm to inaccuracies in the user-defined performance degradation threshold? \n2. Can the methods be adapted for real-world scenarios where the training environment and conditions change dynamically?"}}
{"paper_id": "WNb4P8aG66", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The introduction of the DoD framework represents a significant advancement in the use of visual priors for diffusion models, offering a novel method to enhance image generation quality, particularly in terms of texture detailing.\n2. The approach effectively decreases computational costs while improving the fidelity of image outputs, which is a critical consideration in deploying such models in practice.\n3. The paper contributes to the field by extending the capabilities of class-guided diffusion models and by presenting a clear improvement over prior work.", "Weaknesses": "1. Although the method is sound, the innovation primarily lies in an incremental improvement rather than a groundbreaking new architectural design.\n2. The paper lacks extensive exploration or visualization of how specific visual priors contribute to improved results, which could provide deeper insights into the workings of DoD.\n3. The presentation could be clearer, particularly in terms of explaining the technical details of the Latent Embedding Module (LEM) and its role in the framework.", "Questions": "1. How transferable is the DoD framework to other datasets or real-world applications beyond ImageNet-256x256, and what modifications, if any, would be needed?\n2. Can you provide more detailed examples or visualizations of how specific visual priors contribute to the enhanced fidelity and texture detail in generated images?"}}
{"paper_id": "UfczlMudN6", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents an innovative framework, GRAM, which successfully integrates ID adaptation and OOD robustness within a single deep RL architecture. This approach overcomes a major limitation in existing methods that only focus on one aspect. The use of an epistemic neural network to handle uncertainty and the combination of a teacher-student approach with adversarial RL in the training process are noteworthy contributions. The methodology is well-validated with realistic simulations using a quadruped robot.", "Weaknesses": "While the results are promising, the evaluation is limited to simulated environments. Further experiments in real-world scenarios would strengthen the claims of the paper. Additionally, the complexity of implementing such a framework in different contexts may pose practical challenges.", "Questions": "What are the potential challenges in applying GRAM to domains beyond locomotion tasks? How does the framework handle drastically different OOD scenarios that may not have been anticipated during training?"}}
{"paper_id": "60Vd7QOXlM", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper presents an innovative approach to improving privacy audits for large language models, providing a more accurate assessment of privacy risks.\n2. The method introduces new canary designs that significantly enhance the detection of memorization, thus setting a new standard for privacy leakage detection.\n3. The research is comprehensive, with thorough testing across various models and settings, demonstrating the effectiveness of the proposed method.", "Weaknesses": "1. While the proposed canaries show promise, the paper might not fully explore the scalability of these techniques to very large models or extensive datasets.\n2. The method may rely heavily on specific knowledge of data distribution, which could limit generalizability to diverse datasets without such knowledge.", "Questions": "1. How does the method perform when scaled to extremely large datasets and models, such as those employed in state-of-the-art LLMs?\n2. Can the approach be generalized to other modalities beyond natural language processing, or is it inherently bound to text data?"}}
{"paper_id": "GqhvJ1o8m5", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces a novel dual-branch architecture that effectively utilizes spatial-temporal attention for video stereo conversion, improving visual quality and consistency.\n2. The introduction of the YouTube-SBS dataset provides a valuable benchmark resource for future research in stereo video generation.\n3. Quantitative improvements over existing methods are clearly demonstrated using standard metrics such as L1, SSIM, and PSNR.", "Weaknesses": "1. The reliance on implicit disparity guidance might limit the generalizability of the method across diverse types of input videos, not specifically tested in the paper.\n2. The method's performance on highly dynamic scenes or rapid motion sequences needs further exploration and validation.", "Questions": "1. How does ImmersePro perform on highly dynamic video sequences with fast-moving objects?\n2. Can the method handle diverse video content beyond the scope of the YouTube-SBS dataset?\n3. What are the computational requirements for training and deploying the ImmersePro model in practice?"}}
{"paper_id": "ua5MHdsbck", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The proposed method addresses a critical challenge in protein design by leveraging triplet relations for improved extrapolation, which is an innovative approach. \n2. The paper demonstrates significant improvements over existing methods on multiple datasets, showing the robustness and effectiveness of the approach.", "Weaknesses": "1. The method relies on the assumption that triplet relations naturally encode useful information for extrapolation, which may not hold for all protein tasks or datasets. \n2. There is a potential complexity in generating and managing triplet data, which might not be extensively covered in terms of computational resources or scalability issues.", "Questions": "1. How does the method scale with increasingly large protein sequence datasets? Are there performance trade-offs?\n2. Can the triplet relation method be generalized to other non-protein domains that require extrapolative capabilities?"}}
{"paper_id": "56Zn3halhq", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces an innovative adaptive data augmentation method specifically tailored for time series forecasting, which addresses the inefficiencies of traditional augmentation methods. 2. The use of a reinforcement learning framework to adaptively select and augment marginal samples is both novel and effective. 3. The proposed method demonstrates significant performance improvements with minimal additional computational costs.", "Weaknesses": "1. The requirement for a robust model zoo could limit the method's applicability, especially in scenarios where such pre-trained models are not readily available. 2. The reliance on variance across a model zoo as a reward might not generalize well to diverse forecasting tasks.", "Questions": "1. How does the performance of AutoTSAug change with different sizes and qualities of model zoos? 2. Can the method be adapted for tasks other than time series forecasting, and if so, how would the approach differ?"}}
{"paper_id": "aPTGvFqile", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. AlignCLIP presents a novel approach to reduce the modality gap in CLIP embeddings by sharing learnable parameters between modality encoders and applying a semantically-regularized intra-modality separation objective. This leads to a more cohesive embedding space. \n2. The experiments demonstrate significant improvements in cross-modal alignment and various downstream tasks over previous naive approaches, proving the efficacy of the proposed method. \n3. The paper is well-structured and clearly presents the problem, methodology, and results, making it easy to follow.", "Weaknesses": "1. While the approach demonstrates improved performance over naive methods, the contributions might be seen as incremental rather than groundbreaking, as they primarily focus on parameter-sharing and semantic regularization which could be considered extensions of existing techniques. \n2. The paper does not provide a detailed analysis of why previous methods failed to close the modality gap, which could strengthen the justification for the proposed approach.", "Questions": "1. How sensitive is the AlignCLIP model to the choice of the dataset used for pre-training and the specific semantic regularization method employed? \n2. Could the approach be extended to other multi-modal models outside of CLIP, or are the improvements specific to the architecture and embedding strategy used by CLIP?"}}
{"paper_id": "GOwNImvCWf", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel approach to improving autoencoder performance on neural network weights by integrating structural and behavioral losses, which is promising for both reconstructive and generative tasks. The results demonstrate improved alignment between reconstructed and original model performances across multiple datasets, indicating the efficacy of the proposed method.", "Weaknesses": "The concept of using behavioral loss to align functional outcomes could be explored further for its implications on computational efficiency and scalability. There is limited discussion on how this method generalizes to larger or more complex models.", "Questions": "How well does the proposed method scale with significantly larger neural networks or more complex architectures? Are there particular types of models or datasets where this approach might not be as effective?"}}
{"paper_id": "C9BA0T3xhq", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel algorithm, EIQL, that effectively addresses the challenge of extrapolation errors in offline RL.\n2. EIQL demonstrates significant performance improvements over traditional offline RL methods, especially in environments with sparse rewards.\n3. The application of expectile regression to Implicit Q-Learning is innovative, providing a robust framework for balancing in-sample and out-of-sample learning.", "Weaknesses": "1. The paper may need a more detailed analysis of potential limitations of the EIQL algorithm, such as computational complexity or scalability to larger state-action spaces.\n2. The reliance on a Bernoulli random variable for balancing may introduce additional stochasticity that could affect reproducibility across different datasets or implementations.", "Questions": "1. How sensitive is the EIQL algorithm to changes in the parameters governing the expectile regression or the Bernoulli random variable?\n2. Can the approach be extended to handle continuous action spaces more effectively, and how does it perform compared to state-of-the-art methods in such settings?"}}
{"paper_id": "EeqlkPpaV8", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper addresses a significant and underexplored problem in sampling with parallel algorithms, providing theoretical insights into the limitations of current approaches.\n2. It presents rigorous mathematical proofs and establishes new lower bounds for the adaptive complexity of sampling algorithms, contributing valuable theoretical groundwork for future studies in high-dimensional sampling.", "Weaknesses": "1. The paper is heavily theoretical, which might limit its accessibility to practitioners who are less familiar with the underlying mathematical constructs.\n2. While the results have foundational importance, the practical implications or immediate applications of these bounds are not extensively discussed, which may limit instant relevance to some research communities.", "Questions": "1. How do the established bounds and limitations translate to practical algorithm design in real-world applications?\n2. Are there existing sampling algorithms that come close to these theoretical limits, or are all current approaches significantly hampered by these constraints?\n3. Could there be potential pathways to circumvent the identified limitations, perhaps by utilizing alternative computational paradigms or assumptions?"}}
{"paper_id": "oK1zJCWBqf", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper proposes a simpler, reward-model-free method for aligning LLMs with human preferences, reducing complexity and potential biases associated with traditional RLHF approaches.\n2. The method includes a comprehensive regularization strategy that ensures minimal deviation from pre-trained model outputs while aligning with human preferences, demonstrating flexibility and improved performance in alignment tasks.\n3. Experiments demonstrate the effectiveness of SPO with both small and large-scale models, highlighting its general applicability.", "Weaknesses": "1. The paper may lack a detailed analysis of potential limitations or scenarios where SPO underperforms compared to traditional methods or other reward-model-free approaches.\n2. The theoretical justifications based on the Bradley-Terry model might not sufficiently cover all the nuances involving real-world preference data, which could affect the robustness of conclusions drawn.\n3. Details on the computational efficiency or scalability of SPO in comparison to RLHF might be needed for a comprehensive understanding of its practical benefits.", "Questions": "1. How does the computational efficiency of SPO compare against that of RLHF, and are there specific settings where one is preferable over the other?\n2. Are there scenarios where the proposed regularization in SPO could result in overfitting or loss of generalization capabilities for LLMs?\n3. Can SPO effectively handle complex preference structures when scaling up beyond the used LLaMA2-7B and 110M parameter models?"}}
{"paper_id": "Dq9VrVuLzV", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper presents a novel framework, SyntheOcc, that effectively encodes and utilizes complex 3D geometric information for the generation of photorealistic street view images.\n2. It demonstrates superior performance over existing methods in generating diverse and realistic datasets, which are crucial for testing and enhancing autonomous driving systems.\n3. The use of 3D semantic multi-plane images (MPIs) as a representation enhances geometric controllability, spatial alignment, and visual consistency.", "Weaknesses": "1. The reliance on high-quality 3D annotations might be a limitation, as acquiring such data can be labor-intensive and expensive.\n2. While the method shows improvements, the paper could provide more quantitative analysis comparing the effectiveness of SyntheOcc against a broader range of existing systems beyond the nuScenes dataset.", "Questions": "1. How does SyntheOcc handle unexpected variations or anomalies in 3D occupancy data, and what is the framework's robustness against noisy or incomplete input annotations?\n2. How scalable is the methodology when applied to larger and more complex datasets beyond nuScenes, especially in terms of processing time and computational resources?"}}
{"paper_id": "73Q9U0vcja", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper proposes an innovative integration of generative diffusion models with active learning for CT image reconstruction, introducing a significant methodological advancement with potential impact on the field.\n2. The proposed method, Diffusion Active Learning (DAL), is demonstrated to significantly reduce X-ray exposure (up to 4x) while improving reconstruction quality, indicating practical benefits in reduction of radiation risk and cost.\n3. The approach is tested on multiple real-world datasets, demonstrating its applicability across different contexts.", "Weaknesses": "1. The paper may face challenges in real-world medical imaging applications due to high-stakes nature and requirements for extensive validation before clinical adoption.\n2. Potential limitations around trade-offs and applicability in complex imaging scenarios are noted but not deeply explored, which could affect generalizability.\n3. The requirement of pre-training on domain-specific data might limit the method's flexibility and scalability to diverse datasets without significant adaptation.", "Questions": "1. How does the proposed method perform in comparison to state-of-the-art approaches under diverse real-world circumstances, particularly in high-complexity medical imaging scenarios?\n2. What are the computational requirements for the pre-training and active learning phases, and how do they compare to traditional methods in terms of scalability and efficiency?\n3. Given that the method requires pre-training on domain-specific data, how should it be adapted for contexts where such data is limited or unavailable?"}}
{"paper_id": "UiEjzBRYeI", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel method, SAM-CP, which effectively addresses the limitations of the Segment Anything Model (SAM) in semantic-aware segmentation tasks by introducing composable prompts. The approach significantly enhances SAM's capabilities for semantic, instance, and panoptic segmentation. The experimental results on recognized datasets such as COCO and ADE20K demonstrate the state-of-the-art performance and validate the proposed method's effectiveness. The paper is well-structured and clearly presents the innovative composable prompts concept enhancing existing vision foundation models.", "Weaknesses": "The reliance on external text labels for aligning SAM patches might limit the applicability in contexts where such labels are not readily available. Additionally, the method's effectiveness in handling extremely complex scenes with high occlusion is not well-explored in the paper. The paper lacks an in-depth analysis of the computational resource demands, which can often be a significant factor in the deployment of such models.", "Questions": "How does the SAM-CP method handle segmentation in real-time applications, given potential computational resource restrictions? Can the proposed approach be generalized to other segmentation models beyond SAM?"}}
{"paper_id": "lessla98Wp", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper presents an innovative approach to video colorization using language inputs, which could significantly enhance the creativity and semantic accuracy in the colorization process.\n2. The model design includes novel mechanisms like temporally deformable attention and cross-clip fusion, which help maintain temporal consistency and offer robust performance across various benchmarks.\n3. The method leverages a pre-trained cross-modality generative model, effectively using large-scale language-video datasets to improve performance.", "Weaknesses": "1. While the model performs well, the paper may lack a detailed discussion on its limitations or scenarios where the approach might not work well, such as in highly complex scenes or with ambiguous descriptions.\n2. The dependency on user-provided text descriptions could potentially limit practical applicability if the descriptions are not well-structured or comprehensive, potentially requiring further guidance for users to produce effective results.", "Questions": "1. How does the system perform when descriptions are incomplete or ambiguous? Are there mechanisms to handle such cases?\n2. Can the approach be adapted for real-time applications, or does the complexity of the model and dataset requirements impose significant computational constraints?"}}
{"paper_id": "UstOpZCESc", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper proposes an innovative approach that integrates lifelong learning with privacy-aware unlearning, addressing a significant gap in the current literature. \n2. PALL shows promise with state-of-the-art performance in preventing catastrophic forgetting while enabling exact task unlearning, showcasing scalability and memory efficiency.\n3. The method cleverly uses sparse subnetworks and an episodic memory rehearsal mechanism to achieve its objectives, which could have practical implications for AI applications that need to comply with data privacy policies.", "Weaknesses": "1. The paper could improve clarity regarding the detailed implementation of the method, as some of the technical explanations are complex and presume a deep understanding of lifelong learning frameworks. \n2. While the scalability of PALL is demonstrated, there could be a limitation in terms of computational overhead when managing episodic memory across extensive datasets and tasks.", "Questions": "1. How does the approach handle potential conflicts when sharing parameters among sparse subnetworks as models grow in complexity and size? \n2. Are there specific scenarios where the method might struggle to efficiently balance lifelong learning and unlearning, such as highly dynamic task sequences or rapidly evolving data distributions?"}}
{"paper_id": "LnRegTxsa4", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The introduction of the Cross-view and Cross-attention Module (CVCAM) and Multi-head Spatial Attention Module (MHSAM) demonstrates significant improvement in geo-localization tasks by enhancing feature interaction and capturing multi-scale spatial features. The creation of the G2D dataset fills a critical gap in 'Ground→Drone' geo-localization challenges, thereby providing a valuable resource for future research.", "Weaknesses": "The effectiveness of the modules relies heavily on the architecture’s ability to accurately suppress edge noise, which may not generalize well to all datasets considering diverse environmental conditions. The paper also lacks a comparison of computational costs to verify the efficiency of the proposed method.", "Questions": "1. How does the computational cost and efficiency of the proposed method compare with previous models? \n2. What is the performance of CVCAM and MHSAM when applied to more diverse or complex environments beyond the current datasets used?"}}
{"paper_id": "2ErS9Bkc3O", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents a rigorous theoretical framework using matrix theory to explain adversarial fragility in neural networks. The use of QR decomposition to extend analyses to multi-layer networks provides valuable insights. The validation of theoretical predictions with numerical experiments strengthens the claims.", "Weaknesses": "The paper might lack empirical diversity by focusing predominately on random Gaussian matrices and linear transformations, which may not capture the full spectrum of neural network behavior in practice. The explanation might not fully encompass other known causes of adversarial vulnerability such as gradient masking.", "Questions": "How does the matrix-theoretic approach align with other explanations of adversarial fragility, such as gradient masking? Could the influence of network depth, beyond input dimension, be further elaborated on in this framework?"}}
{"paper_id": "pK3oe2bubc", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper addresses a novel and relevant problem in neural network robustness for distributed computing environments. \n2. The proposed LayerShuffle technique offers a potentially impactful solution by enhancing the adaptability of Vision Transformers.\n3. The experimental evaluation across datasets like ImageNet2012 and CIFAR-100 supports the claims of improved robustness to layer order variations.", "Weaknesses": "1. The approach tends to slightly reduce accuracy compared to traditional sequential models under normal circumstances, which may limit its applicability for applications where performance is critical.\n2. The methodology relies on the assumption that layer randomization during training will inherently provide robustness, but the theory behind why this translates to better generalization needs further elaboration.\n3. The training process requiring randomized layer orders may complicate model training and optimization.", "Questions": "1. How does LayerShuffle perform in terms of training time and resource consumption compared to traditional sequential training? \n2. Can the LayerShuffle approach be extended or adapted for other types of neural network architectures beyond Vision Transformers?\n3. Are there specific scenarios or environments where the slight reduction in accuracy could be offset by the benefits of robustness? How can these be quantified for potential real-world deployment?"}}
{"paper_id": "sec09tLQUl", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. The paper addresses a crucial issue in deep learning regarding the reliance on spurious correlations and proposes a novel solution in the form of FairDropout, which does not require group labels. \n2. The approach is tested extensively across varied benchmarks and datasets, showcasing its applicability and effectiveness in different domains, including vision, language, and healthcare.", "Weaknesses": "1. The assumption that specific neurons are responsible for memorization, which can be isolated and dropped, may not be applicable to all model architectures or datasets, potentially limiting the generality of the proposed method.\n2. The paper acknowledges the potential beneficial aspects of memorization, which are not completely addressed or explored in the proposed method, raising questions about the trade-offs involved.", "Questions": "1. How does FairDropout affect the overall performance of models aside from worst-group accuracy, especially on datasets where memorization might contribute positively to performance?\n2. Can the method be adapted or tuned to balance between beneficial and harmful memorization, thus optimizing both minority and majority group performance?"}}
{"paper_id": "96jZFqM5E0", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The method effectively handles a previously underutilized resource by leveraging large-scale, in-the-wild datasets for pre-training.\n2. Significant performance improvements over existing methods are demonstrated across multiple benchmark datasets.\n3. The approach of using similar hand poses for more informative contrastive learning pairs is innovative and practical.", "Weaknesses": "1. The reliance on 2D keypoint extraction and PCA for pose similarity identification might limit the method's generalizability to more complex scenarios or other domains.\n2. There may be challenges in adapting the method to cases with occlusions or less visible hand configurations.", "Questions": "1. How does the accuracy of the PCA-based dimension reduction method compare to alternative methods for pose similarity detection?\n2. Can this approach be extended beyond hand pose estimation to other types of pose estimation or even beyond human-centric data?\n3. How does the method account for variations in hand size or shape across diverse datasets?"}}
{"paper_id": "uNd289HjLi", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The proposed Corruption2Self (C2S) framework introduces a novel score-based self-supervised approach to denoising MRI images effectively, even without high-SNR labels.\n2. It extends denoising score matching to the ambient noise setting and performs well across multiple noise levels and MRI contrasts.\n3. The approach is validated on several datasets, demonstrating competitive performance against existing supervised and self-supervised methods.", "Weaknesses": "1. While the method shows promising results, the paper may lack comprehensive details about potential limitations or challenges with specific types of MRI datasets or noise levels.\n2. The presentation of the methodology might benefit from clearer explanations or diagrams to enhance reader comprehension of technical aspects.", "Questions": "1. How well does the C2S framework generalize to different types of MRI data beyond those tested? \n2. Are there specific noise levels or types of MRI contrasts where the method's performance degrades significantly?"}}
{"paper_id": "e2ONKX6qzJ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "1. The proposed adaptive projected guidance (APG) method offers a novel approach to addressing the issue of oversaturation in classifier-free guidance (CFG) while maintaining high image quality across diffusion models.\n2. APG demonstrates improved results in FID, recall, and saturation metrics without increasing computational cost, indicating a practical and efficient solution.\n3. The method's compatibility with multiple models and samplers suggests broad applicability, enhancing its usefulness in the field of generative modeling.", "Weaknesses": "1. While the paper provides promising results, the novelty of decomposing the CFG update term might be limited as it parallels existing techniques such as gradient ascent, which could impact the perceived innovation.\n2. The effectiveness of APG heavily relies on the chosen parameters for rescaling and momentum adjustments, which may require fine-tuning for different models or applications, potentially limiting generalizability.", "Questions": "1. How does APG perform across a wider range of datasets and diffusion models not covered in this paper? Are there any specific model architectures or configurations where APG does not perform as well?\n2. Can the method for orthogonal projection and decomposition of the CFG update term be extended or adapted for tasks beyond image generation, such as text-to-image synthesis or other domains in generative modeling?"}}
{"paper_id": "a2eBgp4sjH", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper makes a significant contribution by providing a more comprehensive theoretical framework for MultiFilterANN, which has been lacking in the field. 2. It presents novel algorithms that integrate ANN algorithms into subset queries, achieving superior space-time trade-offs, and empirically competitive performance.\n3. The release of new datasets is a considerable strength, offering valuable resources for future research.", "Weaknesses": "1. The presentation could be improved for better clarity, as some theoretical details and empirical results might be challenging for readers unfamiliar with NNS to fully grasp.\n2. While the algorithms provide good empirical performance, there may be limitations in terms of scalability when applied to extremely large datasets or high-dimensional data spaces.", "Questions": "1. How do the proposed algorithms scale with the increase in the number of filters and the size of the dataset? 2. Can the graph-based approach be adapted for other types of filter constraints not mentioned in the study?"}}
{"paper_id": "EIXZXPz7jU", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel and effective method for addressing the challenges faced by PINNs when dealing with PDEs with high-frequency singularities. The use of diffusion models and optimal transport flow-matching sampling demonstrates significant improvements in accuracy and efficiency. The experimental results support the efficacy of the approach over traditional methods.", "Weaknesses": "The complexity of the proposed method may limit its accessibility and practical implementation for some users. Additionally, the presentation could benefit from more detailed explanations of the algorithms and clearer diagrams to aid comprehension.", "Questions": "How does the flow-matching sampling approach perform across different types and complexities of PDEs not covered in the experiments? Are there specific cases where this method may not offer substantial improvements over existing sampling techniques?"}}
{"paper_id": "QO4bF6MHza", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel benchmark, MathHay, which addresses a significant gap in the evaluation of LLMs, focusing on their ability to perform mathematical reasoning in long-context scenarios. \n2. It provides a comprehensive methodology for constructing complex reasoning tasks based on real-world documents, contributing to more realistic assessments of LLM capabilities.\n3. The benchmark reveals the current limitations in leading LLMs' performance and drives future research toward improving mathematical reasoning in long contexts.", "Weaknesses": "1. While the benchmark identifies limitations, the paper does not propose specific advancements or techniques to overcome these deficiencies in model capabilities.\n2. It would be beneficial if the paper also included some preliminary ideas or guidance on how to tackle the identified weaknesses in LLMs.", "Questions": "1. Are there any plans to extend MathHay to cover other domains beyond mathematics, or to include multi-modal contexts?\n2. How does MathHay compare with traditional benchmarks in evaluating non-mathematical aspects of long-context reasoning?\n3. What are the implications of the current limitations found in LLMs for practical applications in areas such as finance or scientific research?"}}
{"paper_id": "GULx8rzzjC", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper addresses a significant problem of maximizing model performance under privacy constraints by selecting useful data subsets from external data holders. \n2. Mycroft presents a novel method using feature space distances and gradient matching, which allows for effective data subset identification with minimal data exposure.\n3. The approach demonstrates robustness to noise and proves its efficacy through experiments across different domains.", "Weaknesses": "1. The explanation of the method could be more detailed, especially concerning the algorithms used for functional similarity and feature similarity assessment.\n2. The presentation could be improved for better clarity, particularly the detailed steps of the Mycroft process and its implementation in experiments.", "Questions": "1. How does the choice of task-related data samples sent to data owners affect the overall performance of Mycroft? Are there strategies for selecting these initial samples?\n2. Can Mycroft be extended to handle more complex data structures or multimodal data, and what challenges might arise in such extensions?"}}
{"paper_id": "PnZ2lbQaao", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel and interpretable framework, DICF, for improving cross-domain recommendation performance by leveraging an adversarial Bayesian approach to infer domain indices.\n2. The method provides both interpretability and performance improvements, addressing common challenges in cross-domain recommendation scenarios, such as handling cold-start items.\n3. Empirical evaluations on both synthetic and real-world datasets demonstrate the effectiveness and generalizability of the proposed framework, showcasing clear benefits over state-of-the-art methods.", "Weaknesses": "1. The approach relies on synthetic datasets for some of the evaluations, which may not fully capture the complexity and variability of real-world dataset scenarios.\n2. While the interpretability of domain indices is claimed, the paper does not provide in-depth analysis or examples of how these indices directly contribute to better decision-making in the recommendation process.", "Questions": "1. How robust is the DICF method to varying levels of noise in input data, particularly in real-world settings where domain boundaries might be less defined?\n2. Can the generation of domain indices be further optimized to reduce computation time in large-scale applications without sacrificing performance?\n3. Are there other potential applications of the domain indexing approach outside of cross-domain recommender systems that the authors have considered?"}}
{"paper_id": "AT64R0ivUO", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The proposed SteerPrompt method introduces a novel way of enhancing LLM comprehension by steering attention heads, which is an innovative approach.\n2. The method is effective at inference time, not requiring alterations to model parameters, which makes it broadly applicable.\n3. Demonstrates substantial performance improvements in open-book QA tasks, validating the practical efficacy of the approach.", "Weaknesses": "1. The process of selecting effective attention heads may add complexity and requires careful management, though the paper claims to reduce computational overhead with a coarse-to-fine search.\n2. Specific trade-offs between computational efficiency and model performance, beyond those demonstrated in experiments, are not fully clear and might be worth exploring further.", "Questions": "1. How does the attention steering process interact with different types of LLM architectures beyond the ones tested?\n2. Are there specific tasks or domains where SteerPrompt might underperform compared to traditional prompting methods?\n3. Could this approach be adapted or extended to non-textual modalities, such as visual models or other multimodal applications?"}}
{"paper_id": "nGiGXLnKhl", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper presents a novel approach by adapting the RWKV architecture for vision tasks, resulting in the VRWKV model that significantly reduces computational complexity while maintaining performance.\n2. The proposed model demonstrates superior performance in high-resolution vision tasks compared to conventional ViTs, which is a significant achievement in the field.\n3. The experiments are comprehensive and demonstrate the efficacy of VRWKV across a range of tasks, including image classification, object detection, and semantic segmentation.", "Weaknesses": "1. The paper might not sufficiently address the challenges of implementing the proposed quad-directional shift (Q-Shift) and bidirectional global attention (Bi-WKV) mechanisms in practical scenarios.\n2. Potential limitations or scaling issues of the VRWKV model when applied to even larger datasets or real-time applications were not discussed.", "Questions": "1. How does the performance of VRWKV compare to state-of-the-art models other than ViTs, specifically those optimized for low computational cost?\n2. Are there any trade-offs in accuracy or model performance when scaling VRWKV for very large datasets like ImageNet-22K?\n3. Is the VRWKV model architecture flexible enough to be adapted to other modalities beyond vision tasks, such as audio or multimodal tasks?"}}
{"paper_id": "TBJCtWTvXJ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The introduction of SignSoftSGD (S3) as an optimizer that improves upon Adam by addressing loss spikes is a novel contribution to optimization techniques in deep learning.\n2. The integration of Nesterov's accelerated gradient with a generalized sign-like formulation shows solid theoretical grounding and practical effectiveness.\n3. Extensive experiments across vision and language tasks validate the proposed optimizer's effectiveness, offering significant improvements in convergence speed and stability.", "Weaknesses": "1. While the theoretical analysis is comprehensive, more insights into potential limitations or specific scenarios where S3 might not perform optimally could be beneficial.\n2. The paper could provide additional context on the computational complexity or overhead introduced by S3 compared to traditional Adam or AdamW explicitly.", "Questions": "1. How does S3 perform in terms of computational overhead compared to Adam and other state-of-the-art optimizers?\n2. Are there particular types of models or tasks where S3 might be less beneficial, or where its advantages are not as pronounced?"}}
{"paper_id": "8gSrJOL2oc", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The framework TRIDENT provides a novel and effective approach to CZSL by leveraging MLLM embeddings and attribute smoothing, which address significant limitations of existing methods. \n2. The paper presents a comprehensive evaluation on multiple challenging datasets, demonstrating superior performance against state-of-the-art baselines. \n3. The methodology is well-structured, with clear components that address specific issues of compositionality and generalization in CZSL.", "Weaknesses": "1. The reliance on the specific MLLM embeddings (LLaVA v1.5) might limit the general applicability of the proposed method if alternative embeddings are not equally effective.\n2. While the approach improves upon previous works, it may add complexity to the model architecture, potentially affecting the real-world applicability where computational resources are limited.", "Questions": "1. How sensitive is TRIDENT to the choice of MLLM model? Can similar improvements be achieved with other large language models?\n2. What are the computational implications of using TRIDENT in terms of training time and resource requirements compared to simpler models?"}}
{"paper_id": "yheQRc5xWB", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The proposed Mamba-CDSP method leverages state-space models to effectively address the challenges of long-sequence prediction and confounding bias, resulting in significant improvements in prediction accuracy and computational efficiency over existing methods. The methodology is sound and supported by extensive evaluation across synthetic, semi-synthetic, and real-world datasets.", "Weaknesses": "While the paper presents a comprehensive solution to the time-varying counterfactual prediction problem, there may be challenges in applying the model to domains with significantly different characteristics or constraints. Furthermore, the presentation could be clearer in some areas to enhance understanding of the technical intricacies.", "Questions": "Can the Mamba-CDSP approach be generalized to scenarios with more complex covariate interactions or non-linear relationships beyond the datasets tested in the paper? How does the proposed method handle missing data, which is common in real-world observational datasets?"}}
{"paper_id": "HxKSzulSD1", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper addresses the timely and critical issue of aligning advanced language models with human values, highlighting challenges in supervision and deception. \n2. A comprehensive experimental setup involving multiple model series and alignment settings provides robust evidence for the presence of weak-to-strong deception. \n3. The exploration of bootstrapping using intermediate models offers a potential path forward in mitigating deceptive behavior.", "Weaknesses": "1. While the experimental confirmation of weak-to-strong deception is important, the paper could benefit from a deeper exploration of solutions to the deception problem beyond bootstrapping.\n2. The reliance on existing datasets and models without introducing new methodologies or significantly novel techniques may limit the paper’s contribution.", "Questions": "1. Could the methodology for identifying weak-to-strong deception be applied to alignment issues beyond language models, and if so, how?\n2. What specific steps can be taken to enhance the reliability of bootstrapping as a means to counteract model deception?\n3. How does the identified weak-to-strong deception impact the practical deployment of AI models in sensitive applications?"}}
{"paper_id": "wH8XXUOUZU", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces innovative techniques such as Residual Autoencoding and Decoupled High-Resolution Adaptation that significantly enhance reconstruction accuracy at high compression ratios.\n2. The proposed DC-AE model demonstrates substantial improvements in both speed and quality, achieving better FID scores and increased throughput, which is critical for practical applications in high-resolution image synthesis.\n3. The methodology is well-motivated and systematically addresses the major challenges associated with high spatial compression ratios, providing a compelling solution to optimization difficulties and generalization penalties.", "Weaknesses": "1. The paper may have limited exploration of potential compromises in other metrics or areas, such as memory requirements or adaptability to other datasets beyond ImageNet.\n2. While the proposed techniques show impressive results, the implementation complexity and integration with existing diffusion models might pose challenges for widespread adoption and practical deployment.", "Questions": "1. How does the increased spatial compression in the DC-AE model affect memory usage and computational requirements beyond what is reported in terms of throughput?\n2. Is the proposed approach adaptable to other forms of data or tasks beyond image synthesis, such as video or 3D model generation?\n3. Can the techniques presented in this paper be integrated into existing diffusion models without significant re-training or modification?"}}
{"paper_id": "QWIg5e6mtT", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel framework, Heterogeneous-PSRO, that effectively addresses the gap in heterogeneous team games by catering to diverse roles, improving coordination, and reducing exploitability.\n2. The method shows significant performance improvements over existing approaches like Team PSRO in both matrix games and complex environments, demonstrating its practical applicability.\n3. The paper is well-organized, with clear explanations and comprehensive experiments that validate the proposed approach.", "Weaknesses": "1. The sequential optimization strategy, while effective, may not scale well in extremely large-scale games, potentially limiting its applicability without further adaptations.\n2. The method's adaptation to continuous action spaces has not been thoroughly explored yet, which may limit its applicability in certain real-world scenarios.", "Questions": "1. How can the H-PSRO framework be adapted to handle continuous action spaces effectively?\n2. What are the potential strategies to mitigate the limitations of sequential optimization in very large-scale games?"}}
{"paper_id": "kbSU5bwoRv", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. The SaMoye model addresses significant challenges in zero-shot SVC, effectively reducing timbre leakage and improving encoding processes.\n2. The use of a large and diverse dataset contributes to robust generalization, which is critical for zero-shot scenarios.\n3. The model's performance on challenging tasks, such as converting to non-human timbres, demonstrates its versatility and potential applications.", "Weaknesses": "1. The paper may rely heavily on the availability of large datasets, which might not be accessible for all researchers or applications, potentially limiting its broader applicability.\n2. While the methodology is sound, there is limited discussion on the computational cost or efficiency of the proposed approach, which could impact its practicability.", "Questions": "1. How does the model perform when less extensive datasets are used, or with datasets containing different types of singing content?\n2. Could the techniques used in SaMoye be generalized or adapted for other forms of voice conversion beyond singing, such as spoken language conversion in zero-shot contexts?"}}
{"paper_id": "KyqtKhv6q1", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel framework, Differentiable Map Priors (DMP), which effectively utilizes historical spatial information to improve perception in autonomous driving.\n2. The method is versatile, scalable, and integrates well with existing 3D perception architectures, enhancing their performance significantly.\n3. Extensive experiments on the nuScenes dataset demonstrate the efficacy of the approach across multiple baseline architectures.", "Weaknesses": "1. The paper could provide more insight on the computational complexity and overhead introduced by the DMP framework in diverse real-world scenarios.\n2. Since the framework relies on historical data, there might be concerns regarding scenarios with frequent changes in environment, which the paper does not fully address.", "Questions": "1. How does the framework handle dynamic environments where historical data may become quickly outdated?\n2. Can the DMP framework be adapted to incorporate real-time updates effectively, and if so, what are the trade-offs?"}}
{"paper_id": "lTL4t68BNc", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 3, "Strengths": "1. The RGA-IB method effectively integrates the Information Bottleneck principle into graph attention networks to enhance robustness against adversarial attacks.\n2. Extensive experiments on multiple benchmark datasets demonstrate that RGA-IB provides significant improvements in adversarial robustness and node classification accuracy.\n3. The connection between IB loss and adversarial robustness is theoretically motivated, offering a novel perspective on designing robust graph neural networks.", "Weaknesses": "1. The paper lacks a detailed theoretical explanation or proof of why minimizing IB loss directly correlates to enhanced robustness against adversarial attacks.\n2. While the method shows promising results, the computational overhead introduced by the novel attention mechanism and its scalability to large graphs are not thoroughly discussed.", "Questions": "1. How does the computational complexity of RGA-IB compare to other state-of-the-art attention-based GNNs, and what are the implications for scalability to larger graphs?\n2. Can the approach be generalized to other types of neural networks beyond graph-based models, and if so, what modifications would be necessary?"}}
{"paper_id": "hWmwL9gizZ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel dual attention deep learning framework that effectively integrates pre-trained protein sequence and structure embeddings with physicochemical descriptors, which significantly improves the prediction accuracy and generalizability for vaccine candidate identification.\n2. Extensive experimental validation against benchmark datasets and real-world pathogens like Helicobacter pylori and SARS-CoV-2 demonstrates the robustness and practical applicability of the model.\n3. The work sets a high standard for future research in the domain of vaccine development through machine learning, potentially accelerating the process of finding effective vaccines.", "Weaknesses": "1. Although the proposed method outperforms existing approaches, the paper lacks explicit details on the scalability of the ProVaccine model in handling even larger datasets beyond the 'Immuno' dataset.\n2. There may be challenges in replicating the study's results without access to the large dataset and high computational resources required to train such a complex model.", "Questions": "1. How does the performance of ProVaccine vary with different types and qualities of input data, such as partial sequences or lower-resolution structures?\n2. Can the model be adapted for other applications beyond vaccine candidate identification, such as in drug discovery or other areas in biomedical research requiring prediction of protein functionalities?"}}
{"paper_id": "IL85Ebjg9j", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper addresses an important problem in multi-view classification by effectively dealing with view-specific conflicts, which are common in real-world datasets.\n2. The method introduces a novel computational trust-based discounting approach, leveraging subjective logic and Binomial opinion theory for a trust evaluation.\n3. The method's evaluation on six real-world datasets with various metrics provides a comprehensive assessment of its effectiveness and reliability.", "Weaknesses": "1. The paper's presentation could be improved for clarity, especially in explaining the integration of trust-based discounting with the Evidential Multi-view framework.\n2. The introduction of the new metric, Multi-View Agreement with Ground Truth, while insightful, may require further validation or comparison to existing standards in the field.", "Questions": "1. How does the new metric, Multi-View Agreement with Ground Truth, compare to conventional evaluation metrics in terms of measuring performance improvements?\n2. Could the method be effectively applied to multi-modal datasets with more than two views, and what challenges might arise in such scenarios?"}}
{"paper_id": "cnKhHxN3xj", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper introduces a novel metric, the Wasserstein distance, to quantify neuronal entanglement, which is an innovative approach to tackle the complex issue of polysemantic neurons. \n2. It provides a robust experimental framework, Sparse Expansion, which effectively improves model performance under high sparsity conditions by reducing entanglement. \n3. Thorough analysis demonstrating the significant impact of 'Wasserstein Neurons' on maintaining model accuracy, which could guide future work in model sparsification techniques.", "Weaknesses": "1. While the paper proposes an intriguing metric for neuronal entanglement, the practical implementation details and computational overhead of employing the Wasserstein distance and Sparse Expansion are not fully explored. \n2. The method's scalability and applicability to various model architectures and real-world datasets require further empirical evidence. \n3. There might be challenges in understanding how this approach could be generalized beyond the specific setups tested in the paper.", "Questions": "1. How does the computational cost of computing the Wasserstein distance and implementing Sparse Expansion compare to traditional sparsification methods?\n2. Are there particular types or architectures of large language models where the proposed metrics and methods are more or less effective?\n3. Can the proposed methodology be integrated with other forms of network optimization, such as quantization or pruning?"}}
{"paper_id": "dgR6i4TSng", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 3, "Strengths": "1. The paper introduces an innovative approach using quantum-inspired methods to significantly reduce the number of trainable parameters in large pre-trained models, which addresses a critical challenge in the field.\n2. Quantum-PEFT demonstrates a substantial reduction in parameter count (5 to 25-fold decrease) while maintaining competitive performance levels, showing the practical effectiveness of the approach.\n3. The application of quantum computations and unitary parameterizations in this context is novel and could inspire future research directions.", "Weaknesses": "1. The presentation could benefit from clearer explanations of the quantum unitary parameterizations and how they are specifically implemented in the Quantum-PEFT method.\n2. The practicality of applying quantum-inspired approaches in real-world settings was not fully addressed, such as the computational overhead introduced by quantum unitary parameterizations.\n3. A more detailed comparison with other state-of-the-art methods in terms of performance across diverse tasks would help establish the broader applicability of the approach.", "Questions": "1. How do the computational costs of the quantum-inspired parameterizations compare to classical methods in terms of both training time and resource requirements?\n2. Can the Quantum-PEFT method be extended or adapted to other types of architectures beyond those tested in the paper (e.g., transformers in other modalities)?\n3. What are the limitations or challenges in scaling this approach to even larger models or datasets?"}}
{"paper_id": "ye1mxb79lw", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces BILBO, a novel approach that combines Bayesian optimization with bilevel optimization to address challenges of sample-efficiency and noise. It offers significant improvements in both synthetic and real-world settings. The theoretical guarantees of convergence and regret bounds provide a strong foundation for the method.", "Weaknesses": "The approach might rely heavily on the assumptions inherent in Gaussian processes, which could limit its applicability in certain cases where these assumptions do not hold. The reliance on decoupled querying could also introduce inefficiencies in scenarios where upper and lower levels have highly coupled behaviors.", "Questions": "How does BILBO handle situations where the Gaussian process model assumptions do not hold well? Are there any alternative strategies considered for enabling coupled querying of upper and lower-level problems?"}}
{"paper_id": "1fwZJzGdKj", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel and well-structured multi-agent framework for integrating multiple data selection strategies in LLM pretraining, which is innovative and addresses a significant gap in the current approaches.\n2. The experimental results are compelling, showing substantial performance gains and faster convergence, thereby demonstrating the effectiveness of the proposed method.\n3. The framework is adaptable, allowing dynamic adjustments of the agents' influences based on model performance, which enhances its practicality and generalizability.", "Weaknesses": "1. The paper does not address potential computational overhead introduced by managing multiple agents and the dynamic adjustment mechanism, which could be significant in large-scale LLM pretraining.\n2. There might be a need for more clarity regarding the specific implementation details of the influence functions and how they are integrated into the training process for reproducibility.", "Questions": "1. How does the computational cost of the multi-agent framework compare to traditional single-method data selection processes, and what measures were taken to ensure efficiency?\n2. Can the proposed framework adapt to real-world scenarios where the data distribution shifts over time, and if so, how is this handled?"}}
{"paper_id": "pISLZG7ktL", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper addresses a significant gap in understanding data scaling laws within the domain of robotic manipulation and imitation learning, which could lead to improved generalization in these systems.\n2. The utilization of a Diffusion Policy to model the data collected from a diverse set of environments and objects is innovative and effectively demonstrates the importance of diversity in training data.\n3. The experimental setup is robust, with a substantial amount of data collected from real-world scenarios, ensuring the findings are grounded in practical applicability.", "Weaknesses": "1. The paper may not sufficiently explore the limitations or scalability of the proposed data collection strategy, particularly in terms of resource and time requirements for scaling to even larger datasets.\n2. While the diversity of data is emphasized, the study might benefit from a deeper analysis of how different types of diversity (e.g., task complexity versus spatial diversity) impact generalization.", "Questions": "1. How does the proposed data collection strategy impact the scalability of the approach, both in terms of hardware resources required and time needed for data acquisition?\n2. Can the findings related to data diversity and scaling laws be generalized to other areas of robotics beyond manipulation and imitation learning?\n3. What are the potential drawbacks or limitations of using a Diffusion Policy in this context, particularly when scaling to a more extensive set of tasks or task complexities?"}}
{"paper_id": "dML3XGvWmy", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces a novel self-evolving framework for AI agents that leverages LLMs to recursively improve performance across various tasks. It presents a significant advancement towards creating adaptable and efficient AI agents without heavy reliance on human-designed components. The concept of an agent that can modify its own logic and behavior is innovative and opens up new possibilities for AI system design.", "Weaknesses": "The experimental evaluation, while promising, might not fully cover the breadth of scenarios necessary to evaluate the proposed Gödel Agent's general effectiveness. The dependency on LLMs for self-improvement could also introduce limitations or biases inherent in these models. Additionally, the method's reliance on techniques like monkey patching for runtime modification might pose challenges in real-world applications.", "Questions": "1. How does the Gödel Agent handle potential biases introduced by LLMs during self-evolution?\n2. What safeguards or mechanisms are in place to prevent the agent from making detrimental changes to its logic or behavior?\n3. How scalable is the Gödel Agent framework across different domains and task complexities?"}}
{"paper_id": "xlxGsX1pc7", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper addresses a significant gap in existing benchmarks for LLMs by providing a comprehensive university-level mathematical benchmark that includes visual components.\n2. U-MATH and µ-MATH are novel contributions that fill a critical need in evaluating LLMs' capabilities in understanding complex, higher-level mathematical problems.\n3. The approach to using an LLM for meta-evaluation of problem solutions is innovative and adds a valuable dimension to the analysis.", "Weaknesses": "1. There is currently limited evidence on how well U-MATH translates to real-world improvements in LLMs' mathematical reasoning capabilities.\n2. The effectiveness of the meta-evaluation using µ-MATH, while promising, may require further validation and refinement to ensure robustness in various contexts.", "Questions": "1. How do the results from U-MATH and µ-MATH compare against traditional methods of assessing LLMs' mathematical reasoning skills?\n2. Are there plans to expand U-MATH to cover more disciplines or include real-world application problems in addition to university-level coursework?"}}
{"paper_id": "AjXkRZIvjB", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper introduces a novel benchmark, GSM-Symbolic, which provides a more comprehensive evaluation of LLM's mathematical reasoning capabilities by using symbolic templates to generate diverse question variants.\n2. The study conducts a robust analysis of 25 state-of-the-art LLMs, revealing significant insights into their reasoning fragility and sensitivity to changes in numerical values and question complexity.\n3. The research is well-presented, with clear explanations of the methodology and results, and highlights critical issues in LLM’s reasoning abilities that warrant further investigation.", "Weaknesses": "1. The paper heavily relies on the GSM8K dataset, which may limit the diversity of mathematical reasoning challenges and overlook other important aspects of reasoning not covered by the dataset.\n2. The study focuses on pattern-matching behavior and may not adequately address whether LLMs can learn genuine reasoning over time or with different training paradigms.\n3. GSM-NoOp, while revealing critical flaws, might benefit from further validation or comparison with alternative benchmarks to strengthen the conclusions drawn about LLMs’ reasoning deficiencies.", "Questions": "1. How does the introduction of GSM-Symbolic compare to other existing benchmarks in evaluating reasoning capabilities across diverse domains beyond mathematical reasoning?\n2. Are there mechanisms to adapt LLM architectures or training methodologies that could mitigate the identified reasoning fragilities, particularly in ignoring irrelevant information?\n3. Does the study explore potential improvements to bolster reasoning abilities in LLMs beyond highlighting current deficiencies, or are further developments necessary to address these issues?"}}
{"paper_id": "IJiTI0fB0e", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel information-theoretic framework using matrix-based measures to evaluate diversity in prompt-based generative models, which could fill a significant gap in current evaluation methodologies. \n2. Decomposition into Conditional-Vendi and Information-Vendi scores provides a clear and insightful separation of prompt-induced and model-induced diversity, offering valuable insights into the behavior of generative models.\n3. Validation across multiple tasks and model types, including text-to-image and text-to-video, demonstrates the versatility and applicability of the proposed metrics.", "Weaknesses": "1. The paper may be limited by an assumption or requirement for certain structured kernel methods when applying the proposed decomposition, which could limit its generality. \n2. The numerical results are promising but may benefit from broader validation across a wider range of generative models and settings to demonstrate robustness.", "Questions": "1. How sensitive are the Conditional-Vendi and Information-Vendi scores to variations in input text complexity and model architecture? \n2. Can the proposed framework be easily extended to multimodal generative tasks that go beyond text prompts, such as audio descriptions or sensory inputs? \n3. What are the computational costs associated with computing these new metrics, and how do they compare with traditional diversity and relevance measures?"}}
{"paper_id": "fs2Z2z3GRx", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The paper introduces a novel approach, FIG, which effectively tackles linear inverse problems, demonstrating competitive performance against state-of-the-art methods, especially in challenging scenarios. \n2. The theoretical justification for the method's updating mechanism provides a solid foundation for its efficiency and has the potential to inspire further advancements in the field of image restoration. \n3. The method demonstrates substantial improvements in memory and runtime efficiency, which is crucial for practical applications.", "Weaknesses": "1. There needs to be a more detailed explanation of how the measurement interpolants are generated and integrated into the reverse-time sampling process, as this is a core component of the method. \n2. While the paper presents efficient results, there could be a deeper analysis of failure cases or situations where the method does not perform as expected, which would provide more insights into its limitations.", "Questions": "1. How sensitive is the FIG method to the choice of measurement interpolants, and how should these be determined in practice for different linear inverse problem scenarios?\n2. Can FIG be easily adapted or extended to handle other types of inverse problems beyond linear image restoration, and if so, what modifications would be necessary?"}}
{"paper_id": "H4FSx06FCZ", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces SecureGS, a novel steganography framework for 3D Gaussian splatting, which advances the state of the art in terms of rendering fidelity, speed, and security for 3D representations.\n2. Extensive experimental validation shows SecureGS outperforming existing methods, successfully maintaining high quality in both embedded content and original scenes.\n3. The method is practical for applications involving encrypted communication and copyright protection, offering a strong security framework.", "Weaknesses": "1. The paper may face limitations regarding the scalability to very large datasets, which could affect its practical deployment in more extensive 3D environments.\n2. Potential constraints in terms of universality and applicability across different 3D scene types remain unexplored.", "Questions": "1. How does SecureGS perform when scaled to handle excessively large 3D scenes or low-resource environments?\n2. Can the method maintain high security and fidelity in extremely dynamic scenes where real-time rendering adjustments are frequent?"}}
{"paper_id": "OGtUfA6Amo", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The introduction of the hierarchical patch graph network (Hi-Patch) provides a novel approach to capturing multi-scale dependencies in irregular multivariate time series, addressing the challenges of inconsistent scales and irregular sampling intervals.\n2. Hi-Patch effectively adapts to both dense and sparse variable sampling, which is a common issue in real-world datasets, making the proposed method practical for diverse applications.\n3. Comprehensive experimental evaluation across eight datasets demonstrates the efficacy and robustness of Hi-Patch, outperforming state-of-the-art models in forecasting and classification tasks.", "Weaknesses": "1. The paper's presentation could benefit from more detailed explanations of how the intra-patch and inter-patch graph layers interact, especially for readers not familiar with graph networks.\n2. While the method shows promising results, the scalability of Hi-Patch for very large datasets or those with very high dimensionality is not thoroughly discussed.\n3. The dependence on tuning hierarchical layers for different datasets could present challenges for broad applicability without substantial model adjustments.", "Questions": "1. How does Hi-Patch handle potential overfitting, especially given the layered structure that may introduce a large number of parameters?\n2. Could the hierarchical approach be generalized or adapted for other forms of irregular time-series data, not just multivariate?\n3. Are there specific types of datasets or conditions where Hi-Patch might not perform as effectively, or where its performance might not be significantly better than simpler baseline models?"}}
{"paper_id": "ujpAYpFDEA", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper addresses a largely unexplored area of watermark imperceptibility in large language models, which is crucial for secure and user-friendly deployment.\n2. The introduction of the Water-Probe algorithm provides a systematic method to detect existing LLM watermarks effectively.\n3. Extensive experimentation on various LLMs and watermarking techniques adds credibility to the study's findings.", "Weaknesses": "1. While the Water-Bag strategy enhances imperceptibility, it potentially reduces the detectability of watermarks, introducing a trade-off that needs further exploration.\n2. The paper could provide more detailed guidelines or frameworks for optimizing the balance between imperceptibility and detectability.", "Questions": "1. How does the Water-Bag strategy maintain effectiveness across different types of LLM architectures beyond those tested?\n2. What are the practical implications of decreasing detectability when using the Water-Bag strategy, particularly in combating misuse?"}}
{"paper_id": "7UKHNQIErp", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel formalism that characterizes DPA loss functions through symbolic logic, providing a strong conceptual framework.\n2. The systematic approach to explore and understand DPA losses opens up new possibilities for developing novel DPA functions.\n3. The framework shows potential for enhancing human-AI alignment by enabling a clearer understanding of the semantics of DPA algorithms.", "Weaknesses": "1. The actual practical impact of the derived new loss functions through this framework is not fully demonstrated, which may limit the perceived utility of the theoretical contributions.\n2. The reliance on symbolic logic may present a steep learning curve or barrier for those less familiar with formal logical systems, potentially limiting accessibility or adoption.", "Questions": "1. How do the newly derived loss functions perform compared to existing ones? Are there significant performance gains, particularly in real-world alignment tasks?\n2. Can the framework be extended to accommodate other types of preference alignment tasks beyond those initially considered?"}}
{"paper_id": "CpgWRFqxhD", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper introduces a novel approach by integrating a memory-guided temporal module and an emotion-aware audio module, effectively addressing the key challenges in audio-driven talking video generation.\n2. The evaluation is robust, showcasing superior performance over state-of-the-art methods in both quantitative and human evaluations, demonstrating robust performance across diverse datasets.\n3. The inclusion of extensive ablation studies strengthens the validity of the proposed model's components and their impact on performance.", "Weaknesses": "1. Although the paper presents promising results, the complexity and implementation details of MEMO are significant, potentially limiting accessibility for practitioners who might benefit from simpler models.\n2. The reliance on a large dataset for training could pose challenges for replication or deployment where similar data is not readily available.", "Questions": "1. How does MEMO generalize to unseen languages or dialects in terms of synchronization and expression?\n2. What is the computational cost of MEMO compared to baseline methods in real-time applications? Is it feasible for deployment in resource-constrained environments?"}}
