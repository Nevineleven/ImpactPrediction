{"paper_id": "B0OwtVEejJ", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The study makes a commendable attempt to integrate weight-entanglement with gradient-based NAS, potentially filling a gap between separate research communities. It shows promise in enhancing the performance and efficiency of gradient-based methods in macro-level search spaces. The provision of open-access code also adds to the paper's transparency and reproducibility.", "Weaknesses": "The paper might lack some depth in the exploration of how the proposed integration directly outperforms existing methods. There may be a need for clearer delineation of how this integration leads to the claimed benefits. The description of experiments needs to be comprehensive enough to fully support the claims about performance improvements.", "Questions": "What specific challenges did the authors face when adapting gradient-based methods to weight-entangled spaces, and how were these overcome? Are there particular scenarios or types of neural architecture search problems where this integrated approach demonstrates exceptional performance?"}}
{"paper_id": "ZlEtXIxl3q", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 3, "Strengths": "1. The proposed method employing contrastive loss functions, such as the Bradley-Terry loss, provides a novel approach for extracting the sparse latent function implied by global epistasis, showing improvement over standard MSE-based methods.\n2. Demonstrated superior performance on benchmark tasks highlights the practical value of the approach.\n3. Theoretical insights, such as the fitness-epistasis uncertainty principle, add depth to the understanding of the problem.", "Weaknesses": "1. The abstract does not provide details on the datasets or specific benchmark tasks, which would aid in assessing the robustness of the proposed method.\n2. There is a limited discussion on the potential limitations or edge cases where the contrastive loss approach might not be effective.", "Questions": "1. How do the proposed contrastive loss functions generalize across different types of protein engineering tasks or datasets?\n2. Are there specific scenarios or types of data where the contrastive loss approach underperforms compared to traditional methods?"}}
{"paper_id": "6yJuDK1DsK", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "FEATHER provides a novel approach by introducing a lightweight framework for lifelong test-time adaptation, significantly reducing the number of parameters needed, which is advantageous for fast inference on edge devices. This approach is robust against error accumulation and does not require access to source data at test time.", "Weaknesses": "The paper does not provide sufficient details on how the compact structure of the adapters maintains the performance level similar to existing methods. While it mentions being orthogonal to existing approaches, there is limited discussion on how FEATHER can be effectively combined with them.", "Questions": "How does FEATHER manage catastrophic forgetting, especially if the adapters are compact? Is there an inherent trade-off between the lightweight nature of the adapters and the breadth of potential test-time distributions it can adapt to?"}}
{"paper_id": "lt6xKGGWov", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The proposed approach for feature selection using neural estimation of mutual information is innovative and offers a new perspective on considering feature subsets as ensembles, potentially capturing complex relationships between features and targets.", "Weaknesses": "The paper abstract does not provide detailed examples or empirical results to clearly demonstrate the effectiveness and superiority of the proposed method in comparison to existing methods. Without this information, it's challenging to evaluate the practical applicability and reliability of the new approach.", "Questions": "1. How does the proposed method perform on real-world datasets compared to other feature selection methods?\n2. What specific neural network architecture is used for estimating mutual information? \n3. Are there any computational efficiency concerns given the ensemble evaluation of feature subsets?"}}
{"paper_id": "Bvrc6kobWd", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The authors introduce a novel perspective to understand margins in contrastive learning through gradient analysis. The paper provides new insights into how margins influence the performance and optimization of contrastive learning frameworks.", "Weaknesses": "The paper may lack comprehensive theoretical analysis to thoroughly validate the proposed perspective. Additionally, the explanation of findings and theoretical contributions could be more precise and clearer, aiding better understanding.", "Questions": "How does the introduction of margins specifically affect the decision boundaries in contrastive learning compared to margin-less approaches?"}}
{"paper_id": "FJjHQS2DyE", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel approach, conditional adversarial support alignment (CASA), that focuses on minimizing conditional symmetric support divergence, offering a more discriminative feature representation for classification tasks. The proposed theoretical target risk bound provides a solid justification for the approach, enhancing its conceptual soundness. Empirical results show that CASA outperforms state-of-the-art methods across multiple benchmarks under label shift conditions, demonstrating its practical effectiveness.", "Weaknesses": "While CASA is theoretically well-founded, the paper may lack extensive exploration of its limitations or potential drawbacks in various contexts, such as differing domains or more complex label shifts. Additionally, although the empirical results are promising, the paper could benefit from more diverse experiments, potentially including real-world applications to showcase broader applicability.", "Questions": "1. How well does CASA handle domains with very different feature spaces or more complex label distribution shifts beyond those tested?\n2. Can the approach be easily extended or adapted for other machine learning tasks beyond classification, such as regression or clustering?\n3. How significant is the computational cost of CASA compared to existing methods, and are there any strategies for optimizing it?"}}
{"paper_id": "xelrLobW0n", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The introduction of the TRACE benchmark addresses a key gap in the evaluation of LLMs regarding their continual learning capabilities, which is underexplored in existing research. The benchmark includes a diverse variety of tasks that challenge current models, providing a means to better assess their performance over time.\n\nThe experimenting findings are significant in highlighting the current limitations of LLMs, revealing how task-oriented training can lead to a decline in general capabilities and instruction-following skills. The introduction of the Reasoning-augmented Continual Learning approach adds a novel methodology to combat the issue of catastrophic forgetting.", "Weaknesses": "While the paper presents a novel benchmark, there is a need for additional experimentation and baselines compared to state-of-the-art continual learning methods. This would help underline the effectiveness of the proposed TRACE benchmark and the Reasoning-augmented Continual Learning approach in improving LLMsâ€™ performance.\n\nThe paper's presentation could be improved, as it lacks comprehensive discussion on how TRACE can be applied to larger models or different model architectures, which might generalize the findings. Furthermore, explicit comparison with existing benchmarks would strengthen the paper's contributions.", "Questions": "Could the authors provide more detailed methodologies for how the Reasoning-augmented Continual Learning approach can be integrated into existing LLM frameworks? Are there particular model architectures or types that benefit more from this approach?"}}
{"paper_id": "bYQkOPvgDw", "review": {"Soundness": 2, "Presentation": 3, "Contribution": 2, "Rating": 4, "Confidence": 3, "Strengths": "The paper tackles an important problem of label noise in graph neural networks (GNNs), which is less explored compared to other GNN issues like graph perturbations. The introduction of a probabilistic graphical model framework for addressing this is interesting. The approach, with two versions (PRGNN-v1 and PRGNN-v2), seems to be methodically thought out and potentially beneficial in high noise-rate situations. The idea of leveraging Bayesian networks to generate clean labels is a thoughtful approach to tackle label noise.", "Weaknesses": "The paper's theoretical grounding appears insufficiently robust in terms of soundness, particularly since the claim of utilizing a probabilistic graphical model might not be fully supported by empirical results that definitively showcase its efficiency and efficacy over existing methods. The expressiveness of the presentation could be enhanced, especially in clarifying how the proposed models definitively surpass existing strategies. There is also a potential lack of novelty as the core idea seems to build heavily on existing Bayesian concepts, applied straightforwardly to GNNs without much foundational change. Lastly, there's limited benchmarking against existing state-of-the-art noise-handling methods in graph learning.", "Questions": "How does PRGNN specifically handle heterophilous graph scenarios better than existing methods, beyond the utilization of Bayesian networks? What are the specific gains attributed to the probabilistic approach proposed in this study as opposed to simpler noise-handling methods? How scalable is the PRGNN approach in terms of computational resources and time with respect to real-world large-scale graph datasets?"}}
{"paper_id": "sKPzAXoylB", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The proposed Utility-based Perturbed Gradient Descent (UPGD) method effectively addresses the issues of catastrophic forgetting and loss of plasticity in continual learning. The combination of gradient updates with perturbations is a novel approach that seems to enhance learning performance in streaming scenarios with non-stationarities. The method is demonstrated to outperform or match the performance of existing methods across various tasks, indicating its robustness and effectiveness. Additionally, the paper includes extended experiments with reinforcement learning, showing UPGD's applicability beyond conventional settings.", "Weaknesses": "The paper could benefit from more detailed explanations and theoretical insights into why UPGD outperforms existing methods. The highly technical nature of the paper might make it challenging for readers unfamiliar with the specific domain to fully grasp the contributions and significance of the work. The experimental validation, while extensive, could be further diversified to include more variety in task environments or alternative benchmarks to strengthen the generality of the claims.", "Questions": "How does the method handle varying degrees of task similarity or dissimilarity in continual learning scenarios, and is performance consistent across these situations? Are there any scalability concerns when applying UPGD to larger and more complex models or task sequences?"}}
{"paper_id": "H8Qg1IIMaR", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper addresses a critical issue concerning the robustness of large language models, which is highly relevant given their increasing deployment in various applications.\n2. The empirical demonstration of permutation sensitivity in popular models provides valuable insights into existing vulnerabilities that need to be mitigated.", "Weaknesses": "1. The paper could provide more detailed analysis and explanations regarding the impact of permutation sensitivity on practical applications.\n2. Insights into potential mitigation strategies for permutation sensitivity would strengthen the paper's contribution.", "Questions": "1. How do the identified permutation vulnerabilities compare to other known vulnerabilities in large models?\n2. Are there any initial thoughts or experiments on potential solutions to mitigate permutation sensitivity in these models?"}}
{"paper_id": "tth2qXY7RU", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "1. The introduction of a new data type, Super Floating-Point (SuFP), which offers improved memory and computational efficiency without sacrificing model accuracy.\n2. The adaptable precision with multi-region piecewise quantization addresses the issue of handling outliers with high dynamic ranges.\n3. Extensive benchmarks show improvements in both computational capability and energy efficiency, making the contribution significant for AI hardware accelerators.", "Weaknesses": "1. Details on how SuFP specifically handles tensor-wise scalable bias and configuration optimization are not thoroughly explained.\n2. The integration of SuFP into existing hardware ecosystems, potential challenges, and compatibility with current hardware accelerators are not discussed.\n3. While benchmarks were comprehensive, specific datasets or task details were not provided, leaving some aspects of empirical validation open to interpretation.", "Questions": "1. How does the introduction of SuFP impact the interoperability with existing AI hardware that supports standard floating-point operations?\n2. Can SuFP be effectively scaled to larger models, such as those used in large language models, without significant retraining or optimization?\n3. What are the potential implications or limitations when implementing SuFP on different types of AI accelerators, such as GPUs vs dedicated ASICs?"}}
{"paper_id": "fjf3YenThE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper addresses an important challenge in applying hard-thresholding to zeroth-order optimization, enhancing its applicability.\n2. The introduction of variance reduction to resolve the conflict between ZO gradients and the hard-thresholding operator is a significant conceptual advancement.\n3. The theoretical results suggest improved convergence rates and broader applicability, which is promising for practical relevance.", "Weaknesses": "1. The abstract and description of the problem and approach are somewhat unclear, which may hinder understanding and replication.\n2. It is not entirely clear how the new algorithm compares with the SZOHT algorithm in practical scenarios, or if there are limitations not addressed.\n3. The examples of portfolio optimization and black-box adversarial attacks, while illustrative, may not sufficiently demonstrate the full scope of applicability.", "Questions": "1. How does the proposed algorithm perform in comparison to SZOHT and other similar approaches on a broader range of problems?\n2. Are there specific scenarios or types of problems for which the new ZO hard-thresholding algorithm is particularly well-suited or poorly suited?\n3. Could you provide more details about the experiments, especially regarding the setup and configuration for the portfolio optimization and adversarial attack tasks?"}}
{"paper_id": "H9DYMIpz9c", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The strengths of the paper include: 1) The introduction of Farzi, a novel data distillation technique that demonstrates impressive reduction in data size while maintaining or enhancing model performance, which is highly relevant for scaling autoregressive models. 2) The innovative use of reverse-mode differentiation of the Adam optimizer and factorization into a latent space provides a compelling method for memory-efficient data distillation. 3) The empirical results are significant, showing 98-120% of full-data performance with just 0.1% of original data size, indicating a substantial potential for reducing resource requirements in training.", "Weaknesses": "The weaknesses of the paper include: 1) There is limited discussion on the potential limitations or challenges of applying Farzi to more complex datasets or different types of autoregressive tasks. 2) The scalability of the approach to very large datasets or models is not clearly addressed, leaving open questions about its universal applicability.", "Questions": "The questions I have include: 1) How does Farzi perform on datasets with different characteristics, such as varying sequence lengths or distributions? 2) Are there any theoretical limits to the effectiveness of the Hessian-vector product approach for reverse-mode differentiation in this context? 3) Can Farzi be effectively combined with other data reduction techniques, such as pruning or model compression?"}}
{"paper_id": "4kLVvIh8cp", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel algorithm, PNLSVI, for offline RL with non-linear function approximation and demonstrates its strength through the tight dependency on the function class complexity. The proposed algorithm includes three innovative components, potentially contributing significantly to the field of RL. The work is theoretically robust, providing a statistically optimal algorithm in this context.", "Weaknesses": "The theoretical nature of the paper and lack of extensive experimental validation may limit immediate applicability and understanding by practitioners. The alignment between theoretical advancements and practical implementations could be better addressed.", "Questions": "How does the performance of PNLSVI compare in practical scenarios, particularly in large-scale RL problems? Are there specific classes of non-linear function approximations where PNLSVI shows significant improvements over existing methods?"}}
{"paper_id": "CSpWgKo0ID", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper takes an innovative approach by applying behavioral game theory to study LLM interactions, delivering valuable insights into how these models might behave in social settings. The study is well-structured, with a clear exploration of LLM behavioral patterns in different types of games.", "Weaknesses": "The paper highlights limitations in coordination tasks but does not fully address potential methodological variations that could impact the results, such as alternative strategies that LLMs could employ aside from the predefined ones. The study could include a broader set of games for a more comprehensive understanding.", "Questions": "Can the methods used to modify GPT-4â€™s behavior be generalized to other LLMs, or are they specific to GPT-4? Are there alternative strategies or adjustments, aside from providing more information, that could improve LLM performance in coordination games?"}}
{"paper_id": "gwbQ2YwLhD", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 2, "Rating": 5, "Confidence": 3, "Strengths": "The paper provides theoretical insights into how scale affects performance of structure learners with square-based and log-likelihood losses, particularly in non-linear cases.", "Weaknesses": "The findings about the impact of scale are not novel and primarily extend previous results with some additional conditions. Implications for practical structure learning scenarios could have been better explored.", "Questions": "How do the proposed conditions extend or differ from existing literature on the limitations of square-based losses in DAG learning? Are there any recommendations for practitioners based on the findings?"}}
{"paper_id": "eIYDKNqXuV", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The Village-Net Clustering algorithm introduces a novel approach to clustering of complex manifold data by combining K-Means with a network-based community detection. The algorithm's ability to autonomously determine the optimal number of clusters is a notable strength. Extensive benchmarking shows competitive performance against state-of-the-art methods, especially in large-scale datasets.", "Weaknesses": "The paper relies on the proprietary Walk-likelihood Community Finder (WLCF) for the second phase of clustering, which is not thoroughly detailed or validated independently within the abstract. Details on how this affects scalability, performance, and general applicability are lacking. More comparisons with a broader range of state-of-the-art clustering methods would strengthen the claims.", "Questions": "How does the Walk-likelihood Community Finder (WLCF) work, and what makes it suitable for this clustering task compared to other community detection algorithms? Are there specific scenarios where Village-Net is expected to outperform traditional clustering methods?"}}
{"paper_id": "bAMPOUF227", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 2, "Rating": 6, "Confidence": 3, "Strengths": "The work explores a novel plug-in framework for using task-specific fine-tuned language models (SLMs) with large language models (LLMs) to enhance their capabilities in terms of generalizability and factuality.", "Weaknesses": "The paper lacks clarity on how the plug-in framework operates on a technical level. Additionally, the novelty of using discriminative models to improve LLMs needs more emphasis.", "Questions": "How exactly are the task-specific fine-tuned models integrated in the framework with LLMs during inference?"}}
{"paper_id": "eNoiRal5xi", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel objective, UDIM, for domain generalization which combines parameter and data perturbations to improve generalization to unseen domains. The approach builds on existing SAM techniques, theoretically establishing an upper bound for DG tasks. Empirically, UDIM demonstrates significant performance improvements on benchmark datasets, especially in restrictive domain scenarios.", "Weaknesses": "The paper lacks clarity on the practical implications of the perturbed instance generation and its impact on domain realism. There is also limited analysis on the scalability and computational overhead of the proposed method when applied to larger or more complex datasets. The transition from theoretical validation to empirical demonstration could be better articulated.", "Questions": "How does the proposed UDIM handle scenarios where the perturbed domains significantly differ from real-world unknown domains? Is there a specific mechanism that controls the degree of perturbation to maintain data realism? What are the computational overheads associated with integrating UDIM into existing SAM-based frameworks, and how sensitive is the method to changes in the perturbation parameters?"}}
{"paper_id": "uU0Adp7Sfo", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel hybrid GAN model (CCGAN) that incorporates both collaborative and competitive components, addressing issues of training collapse and enabling the generation of high-quality samples with specific features. The theoretical grounding, including the transformation of the regularization term into a divergence, is well-established, promising consistency in training and results.", "Weaknesses": "The abstract is dense and could be condensed for clarity in certain segments to enhance accessibility for readers less familiar with advanced GAN mechanics. The balance of in-depth theoretical exploration versus experimental details could be improved to present a more comprehensive picture of the model's complete operational framework.", "Questions": "1. Can the CCGAN framework be directly applied to tasks beyond image generation, such as text-to-image synthesis, or would further modifications be necessary? 2. What are the computational overheads involved with this hybrid GAN framework compared to traditional GAN setups? 3. Could further insights be given regarding the specific tuning strategies employed during the experiments?"}}
{"paper_id": "dKju7tbe6D", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The proposed IPRM framework cleverly integrates iterative and parallel reasoning, offering a novel approach to enhancing visual question answering tasks. Its ability to outperform existing methods on benchmarks while maintaining interpretability is a significant strength of this paper.", "Weaknesses": "The paper lacks clarity in its description of the parallel reasoning mechanism. Further elaboration on how parallel computation is explicitly achieved would be beneficial. The paper may also benefit from more comprehensive results on diverse datasets to validate the robustness of the IPRM model.", "Questions": "How does the parallel reasoning mechanism fundamentally differ from increasing the number of internal states in existing architectures? What is the specific contribution of the parallel processing structure beyond the iterative nature of the algorithm?"}}
{"paper_id": "DL7JWbdGr3", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper tackles a relevant and timely problem, introducing a novel approach to epidemic forecasting by leveraging pre-trained models in time-series analysis, a method proven effective in other fields like language and vision tasks.", "Weaknesses": "The paper may not adequately address the challenges of transferring learned dynamics across different types of epidemics, which may inherently have different characteristics. There are limited details on how these dynamics are effectively disentangled and transferred.", "Questions": "How does the model ensure the learned representations from one type of epidemic can be effectively transferred to another with possibly distinct characteristics? Are there any limitations observed in applying the model to datasets significantly different from those used for pre-training?"}}
{"paper_id": "1PaDPHDhwe", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces a novel instance-wise scaling technique that improves robust accuracy and provides flexibility in handling trade-offs between robust and average accuracies. The work presents a comprehensive evaluation metric and framework applicable to existing algorithms in domain. Experiments on computer vision and NLP datasets support the effectiveness of the proposed methods.", "Weaknesses": "The approach may not offer a fundamentally new perspective on the optimization problem itself, focusing more on improving evaluation methods. While the evaluation metric provides a broader view, it might be complex or less interpretable in practice. The impact of the approach heavily relies on the introduced metric's effectiveness, and without in-depth proofs or theoretical analyses, this could limit its applicability.", "Questions": "How does the method ensure that the scaling techniques do not overfit to particular datasets, limiting generalization?\nWhat is the impact of different dataset characteristics on the performance of the class-specific scaling technique?\nDoes the unified metric take into account interaction effects between biases in different data groups, and how sensitive is it to noise or label errors in datasets?"}}
{"paper_id": "iI7hZSczxE", "review": {"Soundness": 2, "Presentation": 3, "Contribution": 3, "Rating": 5, "Confidence": 3, "Strengths": "The paper presents an innovative approach to disentangled representation learning for time series, specifically targeting home appliance electricity usage, which is an important and emerging application for reducing carbon footprints. The introduction of the Time Disentangling Score (TDS) provides a new metric for evaluating the quality of disentanglement in this context.", "Weaknesses": "The problem is not clearly framed, making it difficult to understand the specific advancements over existing work. The paper assumes familiarity with many technical concepts without adequate explanation or background, which could hinder comprehension. The evaluation methods and results are not thoroughly detailed, leaving some uncertainty about the performance improvements declared.", "Questions": "How does the proposed method compare to existing state-of-the-art methods on both performance and computational efficiency? Can the TDS be applied to other types of time series data outside of electricity usage, and what might be the limitations? How is the independence-of-support property maintained throughout the disentanglement process?"}}
{"paper_id": "qrGjFJVl3m", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The Qwen-VL models demonstrate strong performance on a wide range of visual-centric benchmarks, setting new records for generalist models of similar scale. The integration of features such as image-caption-box alignment and instruction tuning for dialogue tasks is well-executed. The promise to release these models to the public is a significant contribution to the research community.", "Weaknesses": "The paper doesn't provide in-depth discussions on the limitations or potential drawbacks of the proposed models, such as computational costs or any trade-offs involved in model training at scale. The novelty of the contribution could be more clearly articulated, especially in comparison to existing solutions.", "Questions": "What specific strategies are employed to ensure the multilingual multimodal cleaned corpus effectively represents a diverse range of languages and cultural contexts? Are there any known limitations in the corpus construction that could impact model performance in certain languages or regions?"}}
{"paper_id": "qup9xD8mW4", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper addresses a novel problem in reinforcement learning by adapting dataset distillation to environments where a fixed dataset is not available. The introduction of Hallucinating Datasets with Evolution Strategies (HaDES) presents a compelling new method for achieving behavior distillation with potential applications in multi-task learning. The proposed method shows significant promise, achieving competitive results with very few state-action pairs and demonstrating generalizability across architectures and hyperparameters. Visualization of the synthetic datasets adds to the interpretability of the approach.", "Weaknesses": "The lack of direct comparison with traditional RL methods makes it difficult to quantify the improvement brought by HaDES in terms of efficiency or performance. The paper could benefit from a more detailed discussion on the limitations of the proposed approach, including potential applicability to other types of RL tasks beyond continuous control. Additionally, the scalability of the approach to more complex environments or a larger number of tasks is not fully addressed.", "Questions": "1. How does the performance of HaDES compare to traditional RL methods in terms of training time and resource efficiency?\n2. What are the potential limitations of HaDES when applied to more complex RL environments or discrete action spaces?\n3. Can HaDES be adapted to use expert data, if available, to improve the quality of distilled datasets, and how would that impact the performance?"}}
{"paper_id": "Zw8YxUWL4R", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The introduction of the Extended Textual Conditioning space (P+) and the Extended Textual Inversion (XTI) appears novel and well-founded. XTI demonstrates improved expressiveness, precision, and convergence over existing methods, which is promising.", "Weaknesses": "The paper could benefit from a more detailed theoretical grounding for its claims. Additionally, an exploration of potential limitations and edge cases for P+ and XTI is not thoroughly discussed.", "Questions": "How does P+ handle ambiguous or contradictory textual conditions across different layers? Are there any scenarios where P+ fails to provide the anticipated level of control? Could you further explain the methodology for selecting per-layer prompts?"}}
{"paper_id": "3NXhwkZGjz", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The proposed approach effectively addresses the challenges of SFUDA by utilizing multiple hypotheses to enhance prediction accuracy and reduce reliance on individual inaccurate predictions. The method is shown to achieve state-of-the-art performance and can be integrated into existing systems, demonstrating practical applicability.", "Weaknesses": "The motivation for using multiple hypotheses may require additional clarification, particularly regarding practical applicability. The paper could benefit from clearer explanations in some areas, including the specific processes or algorithms used in each adaptation step, and the theoretical rationale for certain design choices.", "Questions": "How does the method determine the optimal number of hypotheses to use in practice? Is there a specific criterion for selecting hypotheses during consolidation to ensure robustness?"}}
{"paper_id": "Fn655mJ4bv", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper introduces a novel interpretation technique for structured output models, addressing an important issue in machine learning explainability. The proposed energy-based training process for an interpreter function is innovative and considers structural information, which improves explanation quality. The effectiveness of this approach is confirmed through various simulations and real datasets.", "Weaknesses": "The presentation could be clearer, as the explanation of the energy-based training process might be abstract and hard to understand without further detail. It is not clear how robust the proposed technique is when applied to different types of structured output models, and the choice of datasets for evaluation might limit generalization. Moreover, the paper does not extensively compare the proposed method with existing interpretability techniques.", "Questions": "How does the method handle cases where the structured output contains many variables with complex interdependencies? Can you provide more insight into how well this technique performs compared to existing interpretability approaches? Are there any specific limitations to using this method that should be considered when applying it to real-world datasets?"}}
{"paper_id": "VyMW4YZfw7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 2, "Rating": 5, "Confidence": 3, "Strengths": "The paper brings attention to the potential over-engineering in recent GNN architectures, suggesting simpler traditional methods can sometimes match or exceed the performance on SSNC tasks. This challenges the current trend and opens a space for further discussion and exploration.", "Weaknesses": "The paper may lack sufficient experimental rigor or theoretical justification to substantiate its claims about simpler methods consistently outperforming more complex GNN approaches. Additionally, the exploration of hyperparameter impacts could be more comprehensive. There might be missing details on how evaluation conventions have shifted, which could impact the understanding of GNN performance.", "Questions": "How do the authors plan to further validate their claims about simpler methods matching or exceeding current GNN performance? Are there specific scenarios or types of graphs where these simpler methods are particularly advantageous or disadvantageous? Could the authors provide more detailed insights into how evaluation conventions have shifted and affected GNN performance outcomes?"}}
{"paper_id": "09xFexjhqE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses a significant problem in the context of Robust Fine-Tuning (RFT) by introducing a novel low-rank (LoRa) branch that effectively disentangles the optimization of natural and adversarial objectives. This approach potentially reduces the instability in the optimization process encountered in RFT and automates the tuning process, showcasing substantial empirical improvements across multiple tasks.", "Weaknesses": "The abstract and methodology descriptions are somewhat technical, which may increase the barrier of understanding for readers less familiar with the specific terminologies and techniques used, such as LoRa branches and disentangled optimization processes. Additionally, the paper doesn't elaborate on potential limitations or edge cases where the proposed approach might not work effectively.", "Questions": "1. How does the proposed LoRa branch approach compare computationally in terms of efficiency and resource consumption with traditional RFT methods?\n2. What are the long-term implications of using LoRa branches on model performance concerning transferability to substantially different tasks or domains?\n3. Could this approach be extended to scenarios with other types of robustness, such as robustness to data distribution shifts?"}}
{"paper_id": "nmBjBZoySX", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The proposed AdaGLT framework introduces an adaptive, dynamic, and automated approach to discover graph lottery tickets, which addresses several limitations of existing GLT methods.\n2. The theoretical analysis of tackling over-smoothing issues and providing deeper GNN capabilities is an important advancement.\n3. Extensive experimentation demonstrates superiority over state-of-the-art methods, highlighting the framework's practical benefits.", "Weaknesses": "1. The paper could benefit from more extensive validation on truly large-scale graph datasets to bolster claims regarding scalability.\n2. Specific implementation details, such as computational complexity and overhead involved in the adaptive and dynamic processes, could be better elucidated for practical adoption.", "Questions": "1. How does AdaGLT handle real-time updates or changes in the underlying graph structure post-training?\n2. Are there any situations or types of datasets where AdaGLT might struggle or not perform as well as expected due to its adaptive nature?"}}
{"paper_id": "di52zR8xgf", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a significant enhancement of the Stable Diffusion model, showcasing a much larger UNet backbone and novel conditioning schemes. These changes lead to a dramatic improvement in image generation quality, competitive with state-of-the-art models like Midjourney. The inclusion of a refinement model further increases the visual fidelity of the generated images.", "Weaknesses": "While the improvements in terms of model size and conditioning schemes are noteworthy, they may not be largely novel contributions but rather enhancements on existing methodologies. The impact of the increased computational resources required for the larger UNet backbone might not have been sufficiently addressed.", "Questions": "How does the increased size of the UNet backbone affect the training and inference time compared to previous versions? Are there any specific trade-offs between quality and computational efficiency that were observed during experimentation?"}}
{"paper_id": "0b328CMwn1", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The introduction of the novel concept of activation prompt (AP) is innovative and adds a new dimension to the discussion around visual prompting. \n2. The paper provides a comprehensive analysis with experiments conducted across 29 diverse datasets, which adds robustness to the findings and conclusions.\n3. The integration of theoretical models that explain performance differences between AP and VP adds significant value to the research.", "Weaknesses": "1. While the paper introduces AP, the novelty of the concept might overlap with existing methods, and the paper could provide more detailed comparisons with other similar existing techniques.\n2. The performance improvement, while significant, could be perceived as incremental compared to the traditional VP and PEFT methods given the abstract's description.\n3. Some theoretical aspects might be complex or require more detailed elucidation for a broader audience not familiar with related concepts in deep learning architectures.", "Questions": "1. Can the proposed AP method suffer from any particular types of input data or specific domains where it may not outperform traditional VP methods?\n2. Does the introduction of AP in intermediate layers lead to any challenges or trade-offs in model interpretability or robustness?"}}
{"paper_id": "AMDKqZcZbi", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel task inspired by biology, addressing rapid and continual learning, which is a significant problem in machine learning. The approach proposed by the authors combines a biologically-inspired memory model with a convolutional network, showing promising results in terms of rapid adaptation and retention of past knowledge. The model outperforms traditional ANN baselines, demonstrating its effectiveness. The presentation of the paper is clear, with a logical structure and well-explained methods.", "Weaknesses": "While the proposed model shows promising results, the paper may lack a diverse range of experimental comparisons with more recent and varied baselines in the machine learning field. The reliance on a specific task (sequential Morris Water Maze) may limit the generalizability of the approach to other domains or tasks outside spatial navigation. Additionally, the model's dependence on a biologically-inspired architecture might pose challenges in scalability and adaptation across different datasets or environments.", "Questions": "How well does the proposed model generalize to tasks outside spatial navigation, such as those in other domains like image classification or language processing?\nDoes the model require any specific tuning or modifications to be applicable to non-spatial tasks?\nWhat are the potential computational limitations or overheads associated with the entorhinal-hippocampal circuit-inspired memory model?\nHave the authors considered comparing their approach with more recent baselines in the realm of few-shot and continual learning?\nIs the proposed sequential Morris Water Maze task dataset publicly available, and if so, how can other researchers access it to replicate experiments?"}}
{"paper_id": "am7BPV3Cwo", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper addresses an important issue in OOD detection in the context of imbalanced ID data, providing both theoretical and empirical insights into the problem. The introduction of ImOOD and its systematic approach provides a valuable perspective on balancing OOD detection tasks. The results on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT datasets are promising and indicate a significant improvement over current state-of-the-art methods.", "Weaknesses": "The paper focuses primarily on theoretical insights and might benefit from broader empirical evaluations on more diverse datasets and scenarios to establish its applicability more clearly. Furthermore, how the findings generalize to other types of data, such as text or audio, remains unaddressed. While the statistical and theoretical framework is solid, practical implementation details could be expanded for reproducibility.", "Questions": "How does ImOOD handle datasets with different types of imbalance, such as temporal or spatial imbalance in data distribution? Could the approach be extended to other domains like text classification or audio processing, and what challenges might arise in doing so?"}}
{"paper_id": "R1crLHQ4kf", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a simple yet effective approach to adversarial example detection in ASR systems. It leverages characteristics of the probability distribution produced by the ASR systems and demonstrates high performance with AUROC values exceeding 99%. The study also explores adaptive attack strategies, demonstrating a comprehensive understanding of the problem and potential solutions.", "Weaknesses": "The approach, although demonstrating high AUROC, may lack novelty in terms of methodology. While the combination of statistical characteristics for detection seems effective, the paper could benefit from more comparative analysis with existing adversarial detection techniques. Additionally, the paper does not discuss how it handles real-world noise variations that may not follow the controlled settings of adversarial examples.", "Questions": "How does the proposed method perform under real-world conditions where noise might not be precisely controlled or as distinct as adversarial examples? Are there specific scenarios or types of adversarial attacks where this method might struggle?"}}
{"paper_id": "i7P2mK3x3o", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents a novel approach to flow-based models by enabling transportation between two arbitrary distributions, expanding beyond the commonly used normalizing flow. It innovatively applies the concept of optimal transport with practical applications in density ratio estimation and distribution interpolation. Empirical results demonstrate success in challenging high-dimensional tasks such as mutual information estimation, generative models, and image-to-image translation.", "Weaknesses": "While the approach is innovative, there are concerns regarding the depth of theoretical foundation provided for the model's ability to achieve true optimal transport. Questions about its general performance against existing methods in the field of density ratio estimation remain, with limited benchmarks against state-of-the-art methods. Additionally, while experimental demonstrations are included, some may argue that more varied datasets or settings are needed to assert general applicability.", "Questions": "What are the limitations of the proposed method in terms of dimensional scalability or computational efficiency when applied to very high-dimensional data? Given the variety of potential applications, how does the model's performance compare to specialized methods in each area, such as density ratio estimation? Can you provide more theoretical backing or guarantees on the optimality of the transport map learned by the proposed flow network, or clarify the assumptions underpinning these results?"}}
{"paper_id": "YIls9HEa52", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper provides an innovative enhancement to the rSLDS model, effectively addressing its limitations to make it more suitable for trial-varying data. The integration of a semi-Markov discrete state process with latent geometry is novel and paves the way for the irSLDS model class, which shows promise in both synthetic and real data applications. The utilization of PDE theory for deriving a semi-parametric formulation enhances the model's efficiency, demonstrating the strong interdisciplinary nature of the work. The results on mice electrophysiological data are compelling, uncovering non-stationary processes in neural activity.", "Weaknesses": "The complexity introduced by the incorporation of PDE theory and semi-Markov processes may pose a significant challenge for practitioners who are not well-versed in these areas, possibly limiting the method's accessibility. The abstract does not provide detailed information on how the model compares quantitatively to existing methods in terms of performance metrics. While the validation on synthetic data is mentioned, the lack of detailed comparative analysis against current state-of-the-art methods limits a complete understanding of the performance improvements gained.", "Questions": "1. How does the irSLDS model performance compare quantitatively with other state-of-the-art methods on the same datasets? Are there specific performance metrics where it significantly outperforms others?\n2. Does the model maintain its computational efficiency when scaled to larger datasets or models with increased complexity of biological data?\n3. Can this methodology be easily transferred to other types of neural or biological data outside of electrophysiological contexts?"}}
{"paper_id": "koYsgfEwCQ", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel method for unsupervised learning of object-centric representations in dynamic visual scenes with a 3D focus, utilizing a differentiable volume rendering framework. The incorporation of geometric structures and object-centric learning is a significant advancement over existing 2D methods.\n\nThe approach seems to address the limitations of 2D scene decomposition by allowing for the manipulation of geometric shapes and motion trajectories post-training, which existing methods cannot achieve. This makes the method highly applicable for more advanced tasks in dynamic scene understanding.\n\nMoreover, the DynaVol model's ability to outperform existing approaches in decomposition tasks positions it as a potentially impactful contribution to this research area.", "Weaknesses": "The abstract does not provide detailed information on the datasets or the scale of experiments conducted. Extensive evaluations across diverse settings and scenarios would strengthen the claim of superior performance.\n\nWhile the method seems promising, the paper might lack clarity on specific implementation details, particularly concerning the evolution of voxel features over time and the integration process between voxel and global features through the NeRF decoder. Further explanations and supplementary materials might be needed.\n\nThe generality of the method across various scenes, especially in more complex and cluttered environments, remains to be fully validated. The model's dependence on slot attention and how effectively it scales with the number of objects or complexity of the scene is not clear from the abstract.", "Questions": "How does the method handle cluttered scenes or multiple overlapping objects, and does it maintain high performance when scaling to a larger number of objects in the scene?\n\nCan DynaVol be integrated with learned neural rendering methods beyond NeRF, and what would be the implications for rendering quality and computational efficiency?\n\nDoes the method require specific hardware for optimal performance, such as GPUs with specific memory capacities, due to the voxelization process? If so, how does this affect its accessibility for broader applications?"}}
{"paper_id": "o6AN2ZoNw2", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The proposed framework P-Seg is simple yet effective for open-vocabulary semantic segmentation, allowing the system to generalize well across different datasets without needing fine-tuning. The approach achieves state-of-the-art results on well-known benchmarks such as Pascal VOC, Pascal Context, and COCO. The method also shows scalability with data and benefits from self-training. The promise of making code and demonstrations available adds transparency and reproducibility to the research.", "Weaknesses": "While the framework is conceptually simple and demonstrates strong performance, the novelty of the approach may be limited compared to existing methods. The paper does not provide detailed insights into certain architectural decisions or how they specifically contribute to the enhanced performance. It also lacks comparison with very recent methods which may also claim state-of-the-art performance. Furthermore, technical details that could help in reproducing the results or understanding the model's dynamics better are somewhat missing, such as the specifics of pseudo-mask generation.", "Questions": "How does P-Seg handle the generation of pseudo-masks in more detail, and what is the impact of this process on the final performance? Are there any specific data augmentation techniques or pre-processing steps that significantly influence the results? Is there any particular reason some recent methods were not included in the comparisons, and how does P-Seg compare directly against them?"}}
{"paper_id": "HXZK1Z8tHa", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper proposes ShareFormer, a novel Transformer for image restoration characterized by shared attention maps among neighboring blocks, which significantly reduces latency while retaining trainability and enhancing performance. The approach is backed by strong experimental results, demonstrating the efficacy, efficiency, and ease of training of ShareFormer. The authors also address some of the typical constraints of current Transformer-based models such as high inference time.\nThe provision for open-sourcing the code and pre-trained models contributes to the reproducibility and potential future work.", "Weaknesses": "The proposed innovations, particularly concerning shared attention maps, might be perceived as incremental, considering similar concepts have been explored in other works like ELAN. While lesion studies are conducted, the paper could be reinforced by more extensive ablation studies and comparisons to further substantiate the unique contributions and advantages of ShareFormer. Certain aspects of the implementation, as explained, may somewhat lack clarity, especially regarding how shared attention is operationally and sequentially incorporated. Potential improvements in the presentation could further enhance comprehension.", "Questions": "What are the primary differences between the shared attention mechanisms of ShareFormer and those in other models like ELAN? Can the authors expand on quantitative and qualitative ways ShareFormerâ€™s efficiency and locality biases are tangibly manifest? How do they address potential discrepancies when leveraging disparate context sizes for training compared to other competing methods? If certain implementation strategies such as progressive learning enhance results, to what extent do they contribute to ShareFormerâ€™s success relative to others without analogous adaptive learning dynamics? Additionally, figures within the presentation raised certain ambiguities around attention map articulation which might benefit from revision."}}
{"paper_id": "QO3yH7X8JJ", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper introduces Diff-SR, an innovative approach for arbitrary-scale super-resolution utilizing pre-trained diffusion models without needing additional training.\n2. The approach is motivated by a novel insight regarding noise injection into low-resolution images, which appears to lead to significant performance improvements over current state-of-the-art solutions.\n3. Extensive experiments demonstrating Diff-SR's superiority and flexibility under diverse conditions add to the credibility of the work.", "Weaknesses": "1. The paper relies on pre-trained DGMs, which may not be accessible or optimal for all types of SR tasks where training data characteristics might differ significantly from those on which the DGMs were trained.\n2. There is limited discussion on the potential computational overhead and runtime implications of the diffusion-based approach, especially in real-time or resource-constrained environments.\n3. The trade-offs involved in determining the `optimal` amount of added noise are explained theoretically, yet practical guidelines or automated methods for this selection process need more emphasis.", "Questions": "1. Can Diff-SR handle scenarios where the characteristics of the input data differ substantially from those of the training data for the pre-trained DGM? Are there any limitations regarding the types of images or resolution scales where Diff-SR may struggle?\n2. How does the Perceptual Recoverable Field (PRF) compare to existing metrics used in SR tasks, and what is the intuition behind its effectiveness?\n3. The paper alludes to a theoretical analysis underpinning the proposed approach; can the authors provide more detail or supplementary material that further elucidates this analysis?\n4. How might the approach adapt or extend beyond visual data, potentially to multimodal scenarios where super-resolution principles could apply to audio or text?"}}
{"paper_id": "sNtDKdcI1f", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel analysis of the impact of output length in RLHF settings, which highlights an important aspect of this technique that has been underexplored. By investigating the correlation between reward and response length, it sheds light on underlying biases present in reward models trained on popular preference datasets. The empirical results are compelling and demonstrate the significant role that length plays in enhancements in perceived helpfulness, which is a critical finding for researchers and practitioners in the field.", "Weaknesses": "The presentation could benefit from improved clarity, particularly in its use of figures and tables. Several abbreviations and specific aspects of the experimental setup were not immediately clear, which could be a barrier for readers fully understanding the methodologies and conclusions. Additionally, while the paper explores interventions to control for length in reward optimization, it does not provide a clear set of practical guidelines or principles based on these findings, leaving some ambiguity about their practical applicability and relevance to ongoing research efforts.", "Questions": "How might one quantitatively differentiate contributions to reward improvement between length and other factors, like linguistic richness or content coverage? Are there experimental controls or analyses that could further isolate these variables?\nCould the findings have implications for other RLHF tasks, such as harmlessness or truthfulness, where length might not be an obvious heuristic? Would an investigation into these aspects reveal other biases inherent in reward conditioning?\nDo variations in task constraints, such as response time limits or domain-specific content requirements, significantly affect the interplay between length and reward?\nWhat are some potential directions for future research to develop reward models that inherently incorporate length-normalizing features, thereby eliminating the need for additional interventions?\nAre the findings largely applicable across different types of language models, including those of varying sizes and training regimes? Or are the observations primarily relevant to models closely related to those used in the study?"}}
{"paper_id": "HFtrXBfNru", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper tackles the important issue of representation distortion in Graph Neural Networks as graphs evolve, which is a significant problem for maintaining model accuracy over time. The theoretical foundations laid to establish a lower bound on representation distortion demonstrate robustness and depth of insight. The introduction of the Smart framework, utilizing a self-supervised approach for graph reconstruction, effectively addresses the identified problem, and empirical results on synthetic and real-world graphs showcase its practical benefits.", "Weaknesses": "The paper could benefit from more comprehensive evaluations against a wider range of baseline methods beyond simple pre-trained RNNs for temporal distortion estimation. Some assumptions made in the theoretical analysis, though sufficient for deriving results, may limit the applicability of conclusions in more complex real-world scenarios. Although the presentation is generally clear, further elaboration on the derivations and notations, especially the relationship between theoretical concepts and empirical observations, would enhance intelligibility for readers less familiar with the field.", "Questions": "How generalizable are the lower-bound theoretic results across different graph types beyond those tested? Are there particular types of graph evolution (e.g., scale-free networks, highly dynamic graphs) where the approach might struggle or need specific adaptations?"}}
{"paper_id": "4u0ruVk749", "error": "'NoneType' object has no attribute 'model_dump'"}
{"paper_id": "Pa6SiS66p0", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces a novel benchmark for multi-modal continual learning, which can serve as a valuable resource for the research community. The proposed method effectively mitigates catastrophic forgetting by leveraging multiple modalities, which is demonstrated through empirical results. The approach of integrating and aligning information from different modalities based on relational structural similarities is innovative and provides a strong baseline.", "Weaknesses": "The paper could benefit from a more extensive evaluation on diverse datasets rather than focusing on a single one. Moreover, the comparison with other state-of-the-art methods is somewhat limited, and inclusion of recent approaches could strengthen the paper's claims. The design decisions for the integration and alignment method are not thoroughly explained, which might leave readers questioning the underlying methodology.", "Questions": "How does the proposed method perform when applied to more complex datasets or real-world scenarios? Is there a particular reason only one dataset was used for demonstration? Additionally, how does the method handle the presence of noise or incomplete data in one or more modalities?"}}
{"paper_id": "rkplYfqUr0", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper presents a novel approach to addressing the problem of miscalibration and brittleness of language model prompting by introducing Gen-Z, a generative prompting framework for zero-shot text classification. The approach allows for the integration of additional contextual information about labels, which enhances the robustness of the method. The empirical results demonstrate that Gen-Z consistently outperforms zero-shot and few-shot baselines across various benchmarks.", "Weaknesses": "One weakness is the lack of clarity on the specific scenarios or tasks where Gen-Z offers the most significant advantages over alternative methods. While the paper highlights the framework's ability to incorporate contextual information, there may be limitations in cases where such information is not readily available or easily integrated. Additionally, while the results are promising, they may not be sufficiently detailed to fully validate the generalizability of the approach across diverse NLP tasks.", "Questions": "How does Gen-Z handle situations where there is limited or no additional contextual information available for label descriptions? Additionally, can the framework be extended to tasks beyond text classification, such as text generation or sequence labeling?"}}
{"paper_id": "pTU2X9mUBe", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel dataset, LaDe, which has potential high impact by filling a gap in the availability of real-world, large-scale, last-mile delivery datasets. The comprehensive and diverse nature of the dataset enables research across multiple domains such as logistics, supply chain management, and spatio-temporal data mining.", "Weaknesses": "The originality primarily stems from the dataset creation rather than novel methodologies or new techniques in last-mile delivery optimization. Furthermore, the dataset lacks some detailed contextual information such as road network data and specific handling of dynamic urban environments which might limit its applicability.", "Questions": "Will there be periodic updates to LaDe to ensure that it reflects the latest operational details and trends in last-mile delivery logistics? How will potential privacy concerns be addressed given the dataset's real-world origins and detail?"}}
{"paper_id": "uhtQyRrTzY", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 5, "Strengths": "1. The paper proposes a novel method for generating multi-view datasets without relying on annotations, which allows for scalable and cost-effective pretraining.\n2. The method shows significant improvements in dense geometric tasks and semantic segmentation, even when compared to expensive and annotated datasets.\n3. The dataset curation method proves scalable, with larger datasets such as MIMIC-3M leading to better performance.\n4. The approach is applicable to both real-world videos and simulated environments, increasing its versatility.", "Weaknesses": "1. While the paper demonstrates improvements in dense task performance, it focuses less on direct comparisons for object understanding tasks against top-tier pre-trained models, leaving some gaps in understanding its full impact on a broad range of tasks.\n2. The presentation of results could be more comprehensive, including statistical analyses and broader evaluation in non-dense tasks for a more robust assessment.\n3. The method may require considerable computational resources for dataset generation and model training, which is not discussed in detail.", "Questions": "1. Can the proposed dataset-curation approach be adapted for learning representations for non-visual modalities?\n2. How well does the proposed method integrate with or complement existing multi-modal learning frameworks?\n3. What are the computational requirements for scaling the method to even larger datasets, and are there any technical challenges in doing so?"}}
{"paper_id": "WZfatbNdLV", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper addresses an important and complex problem in the RNA field, combining predictive modeling with generative design. The use of transformer architectures for tissue-specific splicing prediction is innovative and significant. The application of Bayesian Optimization for RNA splicing outcome design shows the potential to advance both prediction and therapeutic design, indicating a promising direction for future research.", "Weaknesses": "The manuscript lacks a thorough comparative analysis with existing models, making it difficult to assess the real-world impact of the proposed approach. While the methodology is complex and depicts a novel direction, some aspects are not sufficiently detailed or validated, like the precise improvement over existing state-of-the-art methods. The predictive accuracy for tissue-specific splicing is mentioned as not very accurate but is not quantified in comparison to benchmarks. Additionally, the rationale and potential trade-offs of using transformer models over existing methods, such as SpliceAI in certain contexts, are not clearly addressed.", "Questions": "1. How does the performance of TrASPr compare quantitatively to established models across different tissues?\n2. Can the BO method be adjusted to target specific tissues for splicing alterations more effectively? What might be the limitations or challenges in doing so?\n3. Are there specific instances or datasets beyond the example of Daam1 mutations that could be explored to further validate the generative capabilities?"}}
{"paper_id": "YkRwadXWHd", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper introduces a novel multi-modal framework and a hierarchical knowledge distillation (HKD) framework that facilitates communication by improving continuous sign language recognition (CSLR), promoting inclusivity in the hearing-impaired community. The integration of video, keypoints, and optical flow modalities offers a robust feature extraction approach, and the experiments demonstrate state-of-the-art performance on benchmark datasets.", "Weaknesses": "While the paper shows a strong experimental setup, the abstract lacks detailed methodology or comprehensive explanations of the fusion techniques and specific challenges addressed uniquely by this work. Additionally, the practical challenges related to implementation complexity and computational limitations of multi-modal integration may not have been sufficiently explored.", "Questions": "1. Can you provide more details on the hierarchical knowledge distillation process? How do the different modalities contribute at each stage?\n2. The abstract mentions evaluating on three datasets. Are there any domain-specific challenges faced in each dataset, and how does the proposed system address these?\n3. What are the specific fusion techniques explored, and which were found to be the most and least effective?\n4. How does the system handle variations in signing speed or dialects across different users?\n5. Are there considerations for implementation in real-time settings, particularly regarding computational efficiency as compared to existing solutions? What trade-offs are observed between performance and computational complexity?"}}
{"paper_id": "q4Bim1dDzb", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces a unifying framework for inverse rendering using voxelization, which efficiently models geometry, materials, and illumination together. The use of Spherical Gaussians for representing light radiance is a novel approach, allowing for seamless integration into the framework. The method has demonstrated significant reductions in per-scene training time while maintaining high reconstruction quality, as evidenced by extensive experiments on diverse benchmarks.", "Weaknesses": "While the UniVoxel framework offers improved efficiency, it's not clear how it compares quality-wise to the best-performing methods currently available, aside from speed. There's also a potential lack of innovation, as many ideas seem like extensions of existing methodologies focused on voxelization frameworks. Furthermore, the paper would benefit from a clearer explanation of how it achieves effective integration of the components within the voxelization framework, particularly regarding how Spherical Gaussians outperform alternative representations like spherical harmonics.", "Questions": "1. How does the proposed method generalize to scenes with more complex lighting conditions, such as multiple dynamic light sources?\n2. Can the authors provide more details on the computational resources required to achieve the reported 18-minute training time? How does this scale with scene complexity?\n3. What are the limitations of using Spherical Gaussians for light representation, especially in real-world scenarios with high dynamic range illumination?"}}
{"paper_id": "39cPKijBed", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel approach to address dataset bias in diffusion models using time-dependent importance reweighting. This method improves the precision of outputs in diffusion models compared to previous approaches. The theoretical foundations and empirical evidence presented are strong, showing improvements over existing methodologies on benchmark datasets.", "Weaknesses": "The direct application to score-matching is noted as intractable, suggesting that the methodology may have limitations in usability or applicability under certain conditions. The integration with traditional score-matching methods may necessitate deeper understanding and further studies to fully harness its potential. Additionally, the analysis might lack exploration of potential limitations or specific conditions where the approach might underperform.", "Questions": "How does time-dependent importance reweighting impact the computational requirements of training diffusion models? Are there certain types of biases or datasets where this approach might not be as effective?"}}
{"paper_id": "aG3EARrrd1", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces the Online Sparse Residual Tree (OSRT), which builds on traditional RBF models by incorporating an online tree decomposition approach. The dynamic expansion of network depth to accommodate new data is innovative, and the use of incremental methods to explore RBF centers enhances both sparsity and accuracy. This approach offers potential improvements over existing online methods in handling streaming data.", "Weaknesses": "The abstract does not clearly outline the mathematical basis for the claimed improvements in efficiency and accuracy, nor does it specify the datasets or evaluation metrics used in the experiments. The paper would benefit from a detailed discussion on the theoretical framework and experimental setup. Additionally, the lack of comparisons with a broader range of state-of-the-art methods weakens the evaluation.", "Questions": "How does the OSRT model decide the optimal maximum depth, and how does it compare computationally with other state-of-the-art RBF networks? Are there any specific applications or domains where OSRT has a clear advantage? Could you provide more details on the datasets used and how they reflect real-world streaming data scenarios?"}}
{"paper_id": "Ts95eXsPBc", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper explores an interesting direction by integrating spatial information into the transformer model to address a key limitation of current AI systems in forming and utilizing episodic memory. It provides extensive experimental results across a variety of tasks, demonstrating the practical benefits of the proposed models. The Adaptive Memory Allocator is a novel method that contributes to optimizing memory utilization.", "Weaknesses": "The paper could improve in explaining the theoretical contributions and detailing the differences of the Spatially-Aware Transformer compared to existing transformer architectures. While the experiments show improvements, the specific advantages in different scenarios could be better articulated. The presentation of the model, especially in how it integrates spatial data, could be clearer. More comparative analysis with existing methods would strengthen the claims.", "Questions": "How does the Spatially-Aware Transformer model specifically incorporate spatial information into its architecture? Can you provide more clarity on what differentiates it from other spatial-temporal models? Additionally, how robust is the proposed method across varying types and scales of environments? More comparisons with established baselines would be beneficial to assess its effectiveness."}}
{"paper_id": "LfhG5znxzR", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The paper presents a novel approach to making neural networks' hidden states sparse and discrete, enhancing interpretability without much performance degradation. The introduction of codebook features and the successful validation on language models showcase a significant advancement in the field.", "Weaknesses": "The paper focuses primarily on theoretical contributions and initial validations, with limited insights into potential practical challenges. Further details on scalability and integration into existing frameworks are needed to assess broader applicability.", "Questions": "How does the introduction of the codebook feature affect the overall computational complexity of the neural networks? Are there any noted trade-offs in terms of computational resources or efficiency?"}}
{"paper_id": "bUGzjiUsIq", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses the important problem of partially annotated multi-label classification, which is challenging due to label imbalance and scarcity of annotations. The proposed PAPG framework effectively combines reinforcement learning and supervised learning. The introduction of local and global rewards is a strong point for addressing class imbalance.", "Weaknesses": "The abstract lacks specific details about the experimental setup and results that demonstrate the superiority of the proposed framework. The novelty of the approach in leveraging reinforcement learning specifically for this task could be highlighted more clearly. The presentation could be improved for better clarity on how the proposed method integrates the components and achieves its claims.", "Questions": "What specific datasets and evaluation metrics were used to validate the effectiveness of the PAPG framework? Did the authors compare PAPG against existing multi-label classification methods that handle partial annotations?"}}
{"paper_id": "uMAujpVi9m", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The approach of leveraging high-resolution atomic protein structures and pretrained small molecule representations to simulate ligand-receptor interactions is innovative.\n2. ProFSA demonstrated state-of-the-art performance across multiple tasks, indicating its robustness and applicability.\n3. The method addresses the scarcity of protein-ligand complex data, which is a critical bottleneck in the field.", "Weaknesses": "1. The dependence on high-resolution protein structures might limit applicability to cases where such structures are unavailable.\n2. The abstract does not provide detailed insights into the training process or model architecture, making it difficult to assess the potential limitations in the methodology.\n3. The scalability of the approach in terms of computational resources is not discussed.", "Questions": "1. How does the model handle cases where high-resolution atomic structures are not available?\n2. Could the methodology be extended to other types of interactions, such as protein-protein interactions?\n3. Are there any specific limitations encountered when applying ProFSA to different types of proteins or ligands?"}}
{"paper_id": "uqxBTcWRnj", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "The proposed Transitional Dictionary Learning (TDL) framework offers a novel approach to decomposing visual inputs into parts and implicit relations, which could significantly advance the field of unsupervised part segmentation. The use of a game-theoretic diffusion model to decompose visual inputs is innovative, and the introduction of new metrics such as clustering information gain and heuristic shape score adds value to the evaluation of models in this domain.", "Weaknesses": "While the new framework is promising, it has mainly been validated on abstract visual datasets rather than real-world visual data, which might limit its applicability. Furthermore, the presentation clarity could be improved, especially in explaining the game-theoretic diffusion model and its implementation details. Also, more direct comparisons with existing methods could strengthen the claims regarding outperforming state-of-the-art methods.", "Questions": "How does the method perform on more complex or real-world datasets beyond abstract compositional visual objects? What specific criteria were used for human evaluation, and how were these evaluations standardized?"}}
{"paper_id": "o4CLLlIaaH", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces an original approach to address the limitations of existing NeRF models by shifting from image-based to point-based rendering. The proposed Generalizable neural Point Field (GPF) demonstrates improvements in rendering quality and consistency, particularly in handling occlusions and geometric discontinuities. The novel nonuniform log sampling strategy and learnable kernel for feature aggregation are significant contributions that enhance rendering speed and accuracy. Experiments conducted on three datasets further validate the effectiveness of the proposed method.", "Weaknesses": "The paper could benefit from more detailed discussions on the limitations and potential drawbacks of the proposed approach. The reliance on geometric priors for visibility modeling may impose constraints under certain conditions. While the experiments are promising, the evaluation is limited to only three datasets, which may not fully capture the generalizability and performance across diverse scenarios. Further clarification on the integration with existing techniques and comparison with other state-of-the-art methods is necessary.", "Questions": "What are the potential limitations or challenges of applying the Generalizable neural Point Field (GPF) approach to larger or more complex datasets? How does the GPF handle dynamic scenes or environments with rapidly changing lighting conditions? How does the proposed method compare with other point-based NeRF approaches in terms of computational efficiency and resource requirements?"}}
{"paper_id": "KNzL1nglNB", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The proposed Label-encoding Risk Minimization (LRM) concept is innovative in its approach to addressing label insufficiency by leveraging the neural collapse phenomenon. The paper presents theoretical findings linking LRM to Empirical Risk Minimization (ERM), adding to its strength.", "Weaknesses": "The presentation may lack clarity in some sections, particularly in how the proposed method compares to existing solutions in detail. Additional experiments and baselines would strengthen the empirical demonstration.", "Questions": "How generalizable is the LRM approach across different types of data and tasks beyond those demonstrated? Can the approach be integrated into existing deep learning architectures seamlessly?"}}
{"paper_id": "pvhyBB86Bt", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper provides a significant advancement by extending the analysis of Jacobian behavior from just initialization to the entire training process, using recent results in random matrix theory. This generalization could have broad applications and offers a theoretical underpinning for the essential role of initialization in deep networks.", "Weaknesses": "The presentation, while rigorous, might be dense and challenging for those not well-versed in random matrix theory or deep learning initialization problems. Further explanations and perhaps illustrative examples could help in making the work more accessible to a broader audience.", "Questions": "How does the proposed framework for analyzing Jacobian stability during training apply to networks with various activation functions beyond those studied? Are there specific limitations when extending this analysis to different neural architecture designs?"}}
{"paper_id": "Mylk5iamJC", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper presents a formalization of the open-set recognition (OSR) problem using learning theory, highlighting the shared goal between closed-set recognition (CSR) and OSR for generative models. The use of a neural Latent Gaussian Mixture Model (L-GMM) with a collaborative training algorithm is innovative and effectively designed to address the challenges of both CSR and OSR.", "Weaknesses": "The novelty of the approach may be limited, given that techniques involving Gaussian mixture models and collaborative training have been explored in related domains. Clarity in presentation can be improved to ensure the understanding of complex concepts, especially for readers who might not be familiar with generative models in this context.", "Questions": "How does the L-GMM framework handle outliers in the data or samples from unknown classes during the training process? Is there a possibility of extending this framework to other types of recognition tasks beyond image recognition and segmentation?"}}
{"paper_id": "MsOcVFzv8D", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper provides a strong theoretical foundation for Multi-domain text classification (MDTC), with a novel analysis using Rademacher complexity and margin discrepancy, which are both robust methods in machine learning theory. Empirical results confirm the efficacy of the proposed approach.\n\nThe use of margin discrepancy and the establishment of generalization bounds provide a new angle for understanding and improving MDTC tasks, potentially inspiring further research in the field.", "Weaknesses": "While the theoretical contribution is significant, the practical algorithmic innovation may seem incremental. The work primarily builds upon existing theories from domain adaptation without introducing entirely new methodologies or algorithms for MDTC. Additionally, the presentation could be enhanced, and certain areas might benefit from a more detailed explanation, particularly for readers less familiar with the theoretical concepts.", "Questions": "Can the approach and theoretical insights be extended to other machine learning tasks beyond text classification, potentially in domains with higher complexity or different data modalities?"}}
{"paper_id": "fBlHaSGKNg", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "1. The paper introduces a novel sample selection criterion, $\\alpha$-MMD, which enhances the generalization ability in low-budget scenarios. \n2. The proposed method UPA demonstrates better performance compared to existing AL and SSAL methods under constrained annotation budgets. \n3. It utilizes a modified Frank-Wolfe algorithm, known for its efficiency in optimization problems, which is a strong technical choice.", "Weaknesses": "1. While the $\\alpha$-MMD criterion is innovative, its integration and effectiveness across diverse real-world datasets and tasks could be further demonstrated.\n2. The reliance on specific algorithms such as the Frank-Wolfe algorithm might limit the broader applicability of the approach due to its complexity.\n3. Limited exploration of limitations or potential biases in the proposed methodology, especially in varying data distributions or domains.", "Questions": "1. How does the approach perform on domains other than those presented? Have there been tests in more varied environments to ensure generalizability?\n2. Are there assumptions or limitations associated with using the modified Frank-Wolfe algorithm for the UPA method in various SSL scenarios?\n3. Can the methodology efficiently scale with increasing data dimensionality and size, particularly in very large datasets?"}}
{"paper_id": "HwQ8NVvdmm", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper presents an innovative approach to maintaining model accuracy while using fully quantized training (FQT) for continual learning. The proposed technique effectively addresses the challenge of model accuracy degradation caused by aggressive quantization. The use of Hadamard transforms and stochastic rounding in low-precision training is a novel contribution.", "Weaknesses": "The paper might lack comprehensive comparisons to other state-of-the-art methods in fully quantized training or continual learning, making it challenging to contextualize the improvements. The abstract implies extensive experiments, but it's unclear if the discussed benchmarks provide a complete picture of performance across diverse tasks or settings.", "Questions": "How does the proposed method compare to other state-of-the-art approaches in fully quantized training and continual learning? Are the techniques broadly applicable across different model architectures, or are there limitations to consider?"}}
{"paper_id": "cZo6pDtDZr", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "1. The proposed algorithms demonstrate improvements in sample complexity for estimating collision probability while maintaining local differential privacy. This is a valuable contribution to the field of privacy-preserving data analysis.\n2. The development of the first sequential testing algorithm for collision probability represents an important advancement, potentially enabling more efficient data analysis techniques.\n3. Experimental results indicate that the proposed methods outperform existing approaches, suggesting practical applicability.", "Weaknesses": "1. While the algorithms are interesting, the presentation lacks sufficient detail on certain technical aspects, making it challenging to fully assess their effectiveness.\n2. The experiments, while supportive of the proposed methods, could benefit from more comprehensive evaluations across a broader range of datasets and scenarios.", "Questions": "1. Can the authors provide more insights or examples on the practical applications of collision probability estimation in real-world scenarios?\n2. How do the proposed algorithms perform in settings with extremely small collision probabilities, and are there any limitations to their applicability in these cases?"}}
{"paper_id": "F7QnIKlC1N", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper proposes an innovative integration between Graph-Transformer and a novel self-attention mechanism for predicting molecular spatial configurations. The paper is well-organized, and the experimental results show significant improvements over state-of-the-art methods as well as widely used software like RDkit.", "Weaknesses": "While the proposed method appears strong, the novelty of the self-attention mechanism may not be groundbreaking given the existence of similar approaches in current literature. The paper does not address the potential computational overhead introduced by the new mechanism compared to simpler models. It is also unclear how transferable this approach is to other domains besides the specific datasets used in this study.", "Questions": "Can the authors elaborate on the computational efficiency of the proposed approach compared to baseline methods? Additionally, how does the performance of the new self-attention mechanism compare to existing variants on other molecular modeling tasks not covered in this paper?"}}
{"paper_id": "8JCn0kmS8W", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 3, "Strengths": "1. WavJourney leverages Large Language Models to create a framework for generating storytelling audio content, which is novel and advances the field of audio generation.\n2. The framework allows for the creation of multi-genre storytelling audio, potentially opening new avenues for audio media creation.\n3. The introduction of a new multi-genre story benchmark and availability of code and audio resources support transparency and reproducibility.", "Weaknesses": "1. The reliance on existing audio models might limit the novelty of the underlying technology beyond integration of these components.\n2. There might be concerns about scalability and flexibility across vastly different audio domains not tested in the benchmarks.", "Questions": "1. What specific challenges does the WavJourney face when integrating disparate audio models, and how were these addressed? \n2. How does the approach handle synchronization and coherence across different audio elements such as speech, music, and sound effects?"}}
{"paper_id": "XbLffB0T2z", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 5, "Confidence": 3, "Strengths": "The paper addresses the important problem of availability data poisoning attacks and proposes a new method, Transferable Poisoning, that demonstrates improved attack transferability across different learning paradigms. The experiments on benchmark datasets such as CIFAR-10, CIFAR-100, and Tiny-ImageNet provide a thorough evaluation of the proposed approach.", "Weaknesses": "The main contribution relies on combining existing methods, which limits the novelty. There are concerns regarding the validity of the evaluation setup, especially in the context of contrastive learning. The description of high-frequency perturbations and their role in enhancing attack transferability is not fully clear. Moreover, the computational cost of the proposed algorithm could be higher than others, potentially limiting its practical usability.", "Questions": "1. Can you clarify how the high-frequency characteristics specifically contribute to the transferability of the poisoning attacks?\n2. How does the computational cost of Transferable Poisoning compare to other existing methods for generating poisoning perturbations?\n3. Were different augmentations tried in the evaluation with CL to ensure a fair and reliable assessment of the poisoning effectiveness?"}}
{"paper_id": "4y3GDTFv70", "error": "'NoneType' object has no attribute 'model_dump'"}
{"paper_id": "ODSgo2m8aE", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses a significant and challenging problem in the field of graph neural networks: ensuring fairness while managing biases in the data. By proposing a new formulation of the Lipschitz bound for GNNs, the work contributes a potentially impactful method for stabilizing GNN outputs against biases. The theoretical analysis is supported by experimental validation, which strengthens the proposal.", "Weaknesses": "The paper may not adequately cover how the proposed Lipschitz bound formulation compares with existing methods in fairness training for GNNs. Additionally, the experimental results might lack diversity in datasets that could better demonstrate the general applicability and effectiveness of the proposed method across different types of graph data. The clarity of the presentation, especially regarding the formulation and implications of the Lipschitz bound, could be improved.", "Questions": "1. How does the proposed Lipschitz bound compare with other methods for bias mitigation in GNNs, both in terms of computational efficiency and effectiveness?\n2. Can the approach be generalized to other types of neural networks beyond GNNs, or are there specific challenges that limit its applicability?\n3. What are the practical implications of this work for deploying GNNs in real-world applications where fairness is a concern?"}}
{"paper_id": "RlfD5cE1ep", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper provides a novel extension to the existing theoretical understanding of non-contrastive learning by analyzing the effects of feature normalization using cosine loss. The connection made between the higher-order dynamics and stable equilibria potentially advances our understanding of the mechanisms that prevent representation collapse in self-supervised learning settings.", "Weaknesses": "The paper's theoretical contributions are somewhat limited by the fact that they build on previous work without providing substantial new insights beyond the incorporation of feature normalization. Additionally, the abstract refers to works like \\cite{Tian2021ICML} without giving a complete picture of how this work distinctly extends beyond the current state of the art, potentially limiting its perceived novelty and impact.", "Questions": "1. Can this theoretical analysis be empirically validated using complex real-world datasets and architectures beyond what is suggested by the dynamics study?\n2. How significant is the role of feature normalization in practical non-contrastive settings compared to other regularization techniques?\n3. Are there any limitations or assumptions in the proposed theoretical framework that could impede its applicability to practical scenarios?"}}
{"paper_id": "18TfucMNTr", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper explores an approach that combines noise with continuation methods to address challenges in training deep neural networks, such as getting stuck in local minima and overfitting. The study is built upon existing concepts, but manages to highlight potential improvements in optimizer performance for non-convex loss functions.", "Weaknesses": "While the paper presents interesting ideas, it can sometimes lack clarity in connecting theoretical concepts with practical implementation. It would benefit from a more thorough discussion on potential limitations of the method or scenarios where it might underperform.", "Questions": "What are the specific mechanisms by which regularization impacts the continuation optimizer? Are there particular types of noise that have been found more effective during the preliminary tests?"}}
{"paper_id": "JGTC6WgO4T", "review": {"Soundness": 2, "Presentation": 2, "Contribution": 3, "Rating": 4, "Confidence": 3, "Strengths": "1. The method addresses privacy and security issues inherent in standard UDA by not requiring source data access.\n2. The introduction of the Pseudo label Refinery Network (PRN) and its theoretical support could offer a novel contribution to DA.\n3. The approach is evaluated on four benchmark datasets, demonstrating its competitive performance.", "Weaknesses": "1. The presentation lacks clarity, making it difficult to understand the motivation, methodology, and technical details fully.\n2. There is insufficient discussion on how the proposed method performs in single-source settings or when sources have varying levels of quality or relevance.\n3. The improvement over existing methods is marginal, raising concerns about the practical significance of the contribution.\n4. The paper lacks discussion of integration with existing BDA methods and how the PRN specifically improves results over simple baselines.", "Questions": "1. How is the Pseudo label Refinery Network designed in relation to the modified triangle inequality theory?\n2. What are the limitations of the proposed approach concerning real-world applications with substantially different source domains?\n3. How does the model perform when dealing with a scenario where one source domain is significantly more informative than others?"}}
{"paper_id": "fQHb1uZzl7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper presents a novel integration of feature and cost aggregation using a Transformer-based architecture, harnessing self- and cross-attention mechanisms. The approach offers a unifying strategy that leverages strengths from both aggregation processes and is evaluated on standard benchmarks.", "Weaknesses": "The paper lacks clarity on whether the datasets used for training the proposed method are consistent with those used in previous state-of-the-art methods. There are no specific details on the performance of the method in optical flow tasks, and potential limitations when dealing with image pairs with small overlapping ratios are not addressed. The robustness of the method in challenging cases like those from the MegaDepth dataset is questioned.", "Questions": "1. Are the datasets used for training the proposed method consistent with those used in previous methods?\n2. How does the method perform on optical flow tasks, for example on KITTI or Sintel?\n3. Can improved matching performance lead to better rotation and translation estimation?\n4. How does the method handle image pairs with small overlapping ratios or challenging cases from MegaDepth?\n5. Can you provide failure cases for additional insights?"}}
{"paper_id": "YjG29CIOP7", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The authors successfully introduce new strategies for improving the robustness of data-free meta-learning, addressing crucial safety concerns like Task-Distribution Shift (TDS) and Task-Distribution Corruption (TDC). The newly proposed methods, TEAPOT and ROSY, effectively enhance the reliability and diversity of pseudo-task distributions, while extensive experiments demonstrate the suggested solution's superiority over existing baselines.", "Weaknesses": "It is somewhat unclear how well the proposed approach scales when applied to various domains, given that experimental validation is so far performed only under limited conditions. Additionally, components of the methodological contributions, like the interpolation mechanism in TEAPOT or policy gradient updates in ROSY, need further specification to assess their full operational insights. A comparative evaluation with other robust DG methods might solidify the findings further.", "Questions": "Can the proposed ROSY framework be generalized to other DFML settings? Additionally, more in-depth explanation or exploration of potential trade-offs with respect to leveraging policy gradient techniques within ROSY might help clarify the approach."}}
{"paper_id": "DmDpu3r8iU", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces an innovative framework, VUM, to assess the understanding of human values by LLMs, focusing on 'know what' and 'know why' aspects, and utilizes a dialogue dataset created with GPT-4, enriching the evaluation methodologies.", "Weaknesses": "The methodological details in determining values alignment and understanding could be clearer. The comparison claims between 'know what' and 'know why' need more robust empirical backing, especially given the complexity and nuances of LLM architectures and scales.", "Questions": "How does VUM quantitatively distinguish between mere recitations of known values versus genuine understanding by LLMs in different contexts? What benchmarks or human evaluation measures were used to validate the findings, especially concerning 'know why'?"}}
{"paper_id": "kXHEBK9uAY", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 5, "Confidence": 4, "Strengths": "The introduction of hierarchical diffusion-based planning methods is a good approach to address challenges faced by traditional methods, especially in long-horizon tasks. The empirical evaluations on standard offline reinforcement learning benchmarks show the methodâ€™s superior performance and efficiency. The modelâ€™s ability to enhance generalization capabilities is a significant advantage.", "Weaknesses": "The paper might lack significant novelty, as hierarchical and diffusion methodologies have been explored previously, albeit through the combination in this manner might not have been done. Details of the implementation and evaluation may be lacking, such as the specifics of empirical settings and comparisons. There is also a need for more thorough comparisons with other state-of-the-art models in a similar setup.", "Questions": "How does the method specifically balance the trade-off between computational cost and performance improvement? What are the main domains or tasks where this method shows the most benefit over existing solutions? How does the generalization capability compare quantitatively with other methods?"}}
{"paper_id": "i7LCsDMcZ4", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel approach, the Spiking Layer-Time-wise Relevance Propagation (SLTRP) and Spiking Layer-wise Relevance Propagation (SLRP), to generate Class Activation Maps (CAMs) and saliency maps from Spiking Neural Networks (SNNs). This is noteworthy as it addresses a previously unexplored problem in event-based vision. The proposed EventRPG framework demonstrates state-of-the-art performance on several benchmarks, showcasing its effectiveness.", "Weaknesses": "While the paper presents a novel technique for generating saliency maps in SNNs, there is a lack of detailed comparison with other existing data augmentation or saliency map generation methods. Furthermore, the impact of the proposed method on different types of SNN architectures needs further exploration. The paper could benefit from more comprehensive experiments across diverse datasets.", "Questions": "How does EventRPG compare with traditional saliency-based data augmentation techniques in terms of computational efficiency and resource usage? Additionally, how sensitive is the proposed method to variations in the network architecture or the size of the datasets used?"}}
{"paper_id": "jHdz0CIS2y", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel training strategy, SelfVC, which leverages self-synthesized examples to iteratively refine a voice conversion model, thus avoiding issues related to information loss from task-specific loss terms. The framework successfully applies self-supervised learning and speaker verification models to improve voice conversion across various tasks without needing text. Extensive experiments demonstrate state-of-the-art results in zero-shot voice conversion, proving its effectiveness in improving generated speech's naturalness, speaker similarity, and intelligibility.", "Weaknesses": "The work, while innovative in its approach of using self-synthesized examples, primarily enhances existing voice conversion methods rather than introducing entirely new components. The claim regarding seamless adaptability to other languages through SSL features may require further evidence beyond current experiments. Additionally, while the technique of using self-synthesized examples shows improvements, quantitative measures of the extent of improvements might be limited without deeper analyses.", "Questions": "How significant is the improvement achieved by using self-synthesized examples compared to traditional methods quantitatively? Can the proposed training strategy be easily adapted to other speech tasks outside of voice conversion? What are the specific limitations or challenges faced when scaling the SelfVC approach across different languages or more diverse datasets?"}}
{"paper_id": "NkYCuGM7E2", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper presents a novel approach by integrating LLMs into autonomous driving systems to enhance decision-making by leveraging human common sense. The use of cognitive pathways and guided parameter matrix adaptation to translate high-level LLM decisions into actionable tasks is innovative. Extensive experiments demonstrate significant performance improvements over baseline approaches, suggesting the method's generalizability and effectiveness in complex scenarios.", "Weaknesses": "The dependency on LLMs for critical real-time decision-making may introduce unpredictability due to the inherent variability in LLM outputs. Additionally, the paper might not fully address the computational challenges and latency issues associated with integrating LLMs into real-time driving systems. The scenarios tested in the experiments may not fully cover the long-tail distribution of rare events. The safety and reliability aspects, especially in edge cases, need more thorough evaluation.", "Questions": "How does the system handle the latency introduced by the LLM in real-time decision-making scenarios? Are there any contingency measures in place to ensure safety if the LLM provides an incorrect or delayed decision? How does the system verify or validate the decisions made by the LLM before executing them in critical driving tasks?"}}
{"paper_id": "RtDok9eS3s", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 3, "Rating": 8, "Confidence": 4, "Strengths": "The paper effectively demonstrates a significant simplification of transformer architectures, showcasing clear empirical benefits such as faster training throughput and reduced parameters without compromising performance. The rigorous combination of theoretical and empirical strategies provides a solid foundation for the proposed modifications.", "Weaknesses": "While the simplifications appear effective, the paper does not deeply explore potential limitations or scenarios where these simplifications might lead to performance degradation. The applicability of the simplifications to very large-scale models and diverse datasets remains to be fully validated.", "Questions": "How do the simplified architectures perform on tasks beyond those tested, particularly in real-world scenarios requiring large-scale models? Are there potential limitations in expressiveness or adaptivity in more complex task demands?"}}
{"paper_id": "YCPDFfmkFr", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper tackles an important problem in integrating optimization layers successfully within neural network architectures, allowing for the utilization of domain-specific knowledge within machine learning tasks. The use of primal-dual augmented Lagrangian techniques to handle infeasibilities in quadratic programming (QP) layers is innovative and addresses a common obstacle in this area. The open-source software package, QPLayer, demonstrates significant practical contributions as it provides a tool for broader usability in various applications. The alternative formulations enhancing numerical robustness, speed, and accuracy signal potential improvements over traditional methods.", "Weaknesses": "The paper might not be sufficiently clear in presenting the theoretical underpinnings that underlie the differentiability of closest feasible QP solutions. Further clarity on the advantages and trade-offs of the proposed methods compared to straightforward penalty-based approaches or existing methods would strengthen the argument for the proposed approach's novelty and efficacy. The paper could also benefit from more diverse experimental results or applications beyond what is discussed in the abstract to fully validate the claimed superior predictive performance.", "Questions": "1. Can you provide more details or references about the primal-dual augmented Lagrangian techniques used in your approach? Some readers might not be familiar with these techniques, and a brief background or reference could be helpful.\n2. How does your approach compare with more standard approaches such as adding slack variables and penalties to handle infeasibilities in QP layers?\n3. What are the limitations of your approach in handling convex quadratic programming problems within neural network architectures?"}}
{"paper_id": "ueTdErd5Ib", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper introduces a novel approach to contextual stochastic optimization, aiming to robustly minimize costs in the presence of uncertainty. It combines a learning strategy with downstream task knowledge to effectively manage uncertain parameters. The method's theoretical foundation provides guarantees on decision-making regret, and its performance is validated experimentally in a real-world scenario, demonstrating competitive average regret and significantly lower worst-case costs.", "Weaknesses": "The methodology may rely heavily on accurate discretization and secondary optimization setup, which could introduce complexity in its implementation. The abstract lacks specific details about experimental setup and comparison baselines, which makes it difficult to assess the generalizability and robustness of the proposed method across different contexts.", "Questions": "How does the discretization of the feasible region impact the method's scalability and computational efficiency? Are there specific conditions or types of problems where this method might underperform compared to existing solutions? What are the limitations regarding the choice of threshold for cost control, and how is this threshold determined in practice?"}}
{"paper_id": "sDmjlpphdB", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper presents a novel approach to enhancing the performance of large language models (LLMs) by using a mixture-of-experts paradigm to improve instruction prompting. The idea of dividing the problem space into homogeneous regions with specific instructions and demos is promising and offers a new perspective on improving LLM adaptability.", "Weaknesses": "There is a potential issue with the generalization of the method across diverse tasks due to its reliance on clustering and the assumption that demos and tasks are semantically homogenous when divided. The method's complexity might be a barrier when extending to larger and more diverse datasets. The clustering approach for demos could face challenges in tasks with overlapping problem spaces, risking reduced effectiveness if clusters are not well-separated.", "Questions": "How does the performance of MoP vary with the size and diversity of the demo datasets? Is there an optimal number of experts or clusters that maximizes performance, and how is this determined?"}}
{"paper_id": "Mdk7YP52V3", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper provides a novel theoretical insight into the challenges of training heteroskedastic neural regression models by leveraging statistical physics. The derivation of nonparametric free energy and the identification of phase transitions contribute to a deeper understanding of these models. The methodology potentially offers a more efficient scheme for regularization optimization in such models, which could have practical implications for model training.", "Weaknesses": "Although the paper proposes a theoretical explanation for instabilities, the complexity of the approach might limit accessibility for a broader audience. The practical application and validation in real-world settings might not be comprehensively covered, given the abstract's focus on theoretical contributions. Some technical components may assume substantial prior knowledge, which could hinder understanding for readers less familiar with statistical physics and advanced machine learning models.", "Questions": "1. Can the proposed framework for optimizing regularization be generalized to heteroskedastic regression models in other contexts or domains effectively?\n2. What are the limitations or assumptions of the nonparametric free energy approach in practical implementations?\n3. How does the numerical solution obtained from the nonparametric free energy align with traditional methods in terms of computational efficiency and accuracy?"}}
{"paper_id": "YGTSLDAPqb", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "1. The paper addresses the important issue of domain adaptation, which is crucial for applications involving distribution shifts, such as astronomical and wildlife identification datasets. \n2. The proposed 'Connect Later' method demonstrates notable improvements in OOD error across multiple datasets, suggesting its potential efficacy. \n3. The combination of self-supervised pretraining with domain-specific fine-tuning is a promising strategy to enhance model performance under distribution shifts.", "Weaknesses": "1. The paper's novelty primarily lies in the adaptation of existing techniques, which may not represent a significant theoretical advancement. \n2. It is unclear how well the proposed strategy would generalize to other models or domains not tested in the study, potentially limiting its broader applicability.\n3. The results, although showing improvement, are marginal in some cases, raising questions about the practical significance of the proposed method.", "Questions": "1. How does the choice of generic and targeted augmentations specifically impact each dataset, and would different datasets require different augmentation strategies?\n2. Could the Connect Later strategy be integrated with other self-supervised learning techniques to potentially yield better results?\n3. What are the limitations of the proposed method, and are there scenarios where it might not perform well?"}}
{"paper_id": "JTcaziw7G1", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. The paper addresses a significant issue in the field of information retrieval and large language models by ensuring privacy in distributed contexts.\n2. The proposed approach, PRAG, presents a novel use of multi-party computation to achieve privacy guarantees in dense information retrieval.\n3. The manuscript is clear and well-structured, making the complex topic accessible.", "Weaknesses": "1. The practical implementation and scalability of the approach in real-world scenarios may not be fully addressed in the abstract.\n2. The need for advanced security protocols might limit accessibility for some users or require significant computational resources.", "Questions": "1. How does the communication complexity scale with the number of servers involved in the multi-party computation?\n2. Are there specific scenarios or datasets where the approach shows significant limitations, and how does it compare with existing methods in such instances?\n3. What are the potential cost implications in terms of computational resources required for the proposed approach in comparison to current methods?"}}
{"paper_id": "lEkFq4RUCX", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "1. The paper proposes a novel metric, DDF, for comparing 3D point clouds, which seems promising given the experimental results showing its effectiveness across various tasks.\n2. The metric is computationally efficient and has been tested on a variety of important applications, such as shape reconstruction and scene flow estimation, suggesting broad applicability.\n3. The inclusion of source code supports the reproducibility of the work.", "Weaknesses": "1. The abstract lacks detailed explanations of key terms, such as 'directional distance field', which might make it difficult for readers unfamiliar with the concept to fully grasp the significance and novelty of the work.\n2. It is not entirely clear how the reference points are chosen or how they affect the DDF metric's performance, which could potentially impact the generalization of the method.\n3. The presentation of the experimental results could be clearer; more context on existing metrics and their limitations would help highlight the advantages of DDF.", "Questions": "1. How sensitive is the performance of DDF to the choice of reference points? Is there a systematic way to choose or optimize them?\n2. Could the authors provide more details on how DDF performs relative to other traditional metrics in terms of computation time and scalability with larger datasets?\n3. Are there specific characteristics or types of point cloud data where DDF performs particularly well or poorly?"}}
{"paper_id": "Fq8tKtjACC", "review": {"Soundness": 4, "Presentation": 4, "Contribution": 4, "Rating": 8, "Confidence": 4, "Strengths": "1. Phi-1 is a highly efficient model, demonstrating that high performance on coding tasks can be achieved with significantly fewer resources compared to other contemporary models.\n2. The paper highlights the potential of using \"textbook quality\" data and synthetically generated resources to train models effectively.\n3. The model shows impressive results in pass@1 accuracy on HumanEval and MBPP despite its smaller size, showcasing its efficacy.\n4. The clear presentation of the model's architecture and training pipeline allows for easy understanding and potential replication.", "Weaknesses": "1. The abstract does not provide detailed information on the selection criteria of the \"textbook quality\" data or the synthetic data generation process, which are crucial for understanding the model's success.\n2. The paper might not sufficiently discuss the trade-offs or limitations of using such a small model compared to larger models.\n3. The evaluation is focused on specific benchmarks (HumanEval and MBPP) without broader generalization or consideration of other potential coding tasks.", "Questions": "1. How was the \"textbook quality\" data determined, and what are the key characteristics that distinguish it from other types of data?\n2. Are there any specific limitations or challenges encountered during the training of phi-1 that could be addressed in future work?\n3. How does the model perform on larger and more complex coding tasks compared to simple evaluation tasks provided, and could it generalize beyond coding applications?"}}
{"paper_id": "8fQlGQkj0S", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper provides a rigorous analysis of in-context learning by differentiating between task retrieval and task learning, which helps in better understanding these processes. The authors propose generative models and provide theoretical insights into the behavior of in-context learning with these models, introducing an interesting phenomenon regarding risk bounds and the number of in-context samples.", "Weaknesses": "The theoretical analysis might be based on an overly simplified model that may not capture the complexities of real-world in-context learning scenarios, limiting its applicability. Additionally, the discovery that task retrieval risk can increase with more samples needs further exploration to understand its implications and how it might be mitigated.", "Questions": "How does the increase in the number of in-context samples leading to higher risk in task retrieval relate to practical implementations, and what strategies could mitigate this risk?"}}
{"paper_id": "hRos9WldRK", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The proposed method, Learning to Bootstrap (L2B), offers a fresh approach to handling noisy labels by incorporating dynamic reweighting and implicit relabeling. The method is compatible with existing noisy label learning methods and has demonstrated effective performance across multiple benchmark datasets, underscoring its practical relevance and adaptability.", "Weaknesses": "The paper could benefit from more detailed comparisons with existing methods on a wider variety of real-world datasets experiencing systematic label noise. It would also be helpful to further elaborate on the theoretical underpinnings and convergence guarantees of the L2B method. While the method's dynamic nature is a strength, understanding the precise behavior and tuning of weighting parameters like alpha and beta may require more user guidance.", "Questions": "1. Could you provide more detailed insights on how the L2B method handles scenarios with systematic label noise, not just random label noise?\n2. Can the authors clarify whether a validation set is necessary for the L2B method, given the mention of an unbiased validation set in section 3.1?\n3. It is mentioned that the weights alpha and beta are learned through the model's predictions. How do these weights typically converge during the training process, and do they play a significant role in the robustness of the method?\n4. How do the results compare on truly real-world scenarios beyond standardized datasets, and are there practical insights into deploying L2B in such situations?"}}
{"paper_id": "ttRSBZiCiu", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 4, "Strengths": "The paper introduces the WaveFluid model, which innovatively uses adversarial training to learn a velocity field directly for the speech synthesis task. The use of reparameterization techniques to reduce memory footprint is commendable. Results indicate that the model is competitive in sample quality with reduced inference steps.", "Weaknesses": "The application area appears to be narrow, focusing specifically on vocoder tasks with mel-spectrogram conditions. The impact of the proposed method on broader applications of generative models is not discussed. Additionally, the comparison with existing architectures might be limited; more extensive benchmarking against state-of-the-art models could strengthen the contribution.", "Questions": "How does the proposed WaveFluid model compare in terms of computational efficiency and resource requirements with existing diffusion or poisson-based models?"}}
{"paper_id": "WYsLU5TEEo", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 3, "Strengths": "The paper proposes a novel unified framework to tackle both interpretability and robustness in neural image classifiers, which are often addressed separately. By leveraging GANs to generate counterfactuals, the framework adds interpretability with salient region highlighting and enhances robustness through adversarial training. The evaluation methodically covers both semantic segmentation tasks and robustness against adversarial attacks, demonstrating competitive performance and improved resilience.", "Weaknesses": "The paper primarily focuses on binary classification tasks, limiting its wider applicability. The evaluation focuses on specific use cases (concrete crack detection and fruit defect detection), which may not fully generalize to other domains or complex datasets. Additionally, the experimental design uses a single seed, making it difficult to ascertain the generality and robustness of the reported results across multiple runs.", "Questions": "How does the approach generalize to multi-class classification tasks, given its current focus on binary classification? Are there plans to conduct experiments to validate the approach's effectiveness in more diverse datasets and environments beyond the narrow focus presented in this study?"}}
{"paper_id": "KKZaj2QS3G", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 3, "Rating": 6, "Confidence": 3, "Strengths": "The paper addresses important gaps in time series analysis by introducing a novel noise-aware sampling strategy and an efficient encoder architecture that promotes consistent representation learning. The experiments are thorough and demonstrate that the method outperforms state-of-the-art approaches across various tasks.", "Weaknesses": "The novelty of the method may be questioned as the use of dilated convolutions and inception modules are well-known techniques in deep learning. The description of how noise is explicitly handled seems to require further clarification and rigorous empirical justification. The paper lacks detailed explanations of the theoretical underpinnings of the proposed strategies, which could hinder reproduction and understanding.", "Questions": "What specific aspects of the novel sampling strategy contribute to its effectiveness in accounting for noise? Can the authors provide additional insights into how this sampling strategy compares to other noise-handling approaches in time series analysis? How does the proposed encoder architecture perform on long time series sequences and data with missing values?"}}
{"paper_id": "LWDRiFzbHQ", "review": {"Soundness": 3, "Presentation": 4, "Contribution": 3, "Rating": 7, "Confidence": 4, "Strengths": "The paper introduces a novel method, Vicinal Risk Proxy (VRP), to improve the generalization indicators used to estimate out-of-distribution accuracy. The approach of integrating model responses of neighboring test samples is innovative and addresses the issues with spurious model responses.", "Weaknesses": "The paper relies heavily on the assumption that neighboring test samples will contribute to better correctness indicator estimation, which might not hold for all datasets. The effectiveness might be highly dependent on data structure and distribution, and the paper does not seem to address potential limitations in various contexts.", "Questions": "How does the proposed Vicinal Risk Proxy method perform across different types of datasets beyond what is mentioned? Are there specific domains where this method might be less effective due to sample distribution considerations?"}}
{"paper_id": "Yp01vcQSNl", "review": {"Soundness": 4, "Presentation": 3, "Contribution": 3, "Rating": 7, "Confidence": 3, "Strengths": "The paper presents a novel and effective approach to incorporating edge directionality in graph transformers, leveraging dual encodings and a directional attention module. The proposed approach shows state-of-the-art results on directional graph datasets, highlighting its practical significance. Additionally, the introduction of new directional graph datasets is a valuable contribution to the field.", "Weaknesses": "The presentation could be improved to make the content more accessible. While the approach demonstrates promising results, further clarification on the underlying theory and intuition would strengthen the paper. Also, more diverse datasets could further validate the robustness of the approach.", "Questions": "How do the dual encodings specifically improve performance on tasks compared to traditional single encoding methods? Are there any limitations observed with the proposed method on certain types of graphs?"}}
{"paper_id": "LndMyiBl3n", "review": {"Soundness": 3, "Presentation": 3, "Contribution": 4, "Rating": 7, "Confidence": 4, "Strengths": "The paper addresses a relatively underexplored area in GNN adversarial attacks -- restricted black-box attacks -- which is quite relevant for real-world scenarios. The proposal of the Modified Silhouette Score (MSS) for assessing graph quality is innovative and well-justified through theoretical analysis. The experimental setup is diverse, including both synthetic and real-world graphs, which strengthens the validation of their approach.", "Weaknesses": "The main weakness comes from the potential limitations of MSS in some scenarios, particularly if the assumptions underlying the modified score do not fully apply across various types of graphs. While the empirical results are promising, the paper could benefit from more detailed discussion on the computational complexity and the specific conditions under which MSS or SheAttack might fail to perform optimally or generalize. Additionally, clarity in some parts of the presentation can be improved to make the methods more accessible to a broader audience.", "Questions": "Can you elaborate on how MSS compares to other metrics traditionally used in graph quality measurement? Are there specific types of graph structures where MSS may not perform as expected? Additionally, what are the implications of theoretical assumptions in real-world applications, and how sensitive is SheAttack's performance to variations in these settings?"}}
