{"paper_id": "XTwwtlEfTF", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nMultimodal learning seeks to utilize data from multiple sources to improve the overall performance of downstream tasks. It is desirable for redundancies in the data to make multimodal systems robust to missing or corrupted observations in some correlated modalities.  However, we observe that the performance of several existing multimodal networks significantly deteriorates if one or multiple modalities are absent at test time. To enable robustness to missing modalities, we propose simple and parameter-efficient adaptation procedures for pretrained multimodal networks. In particular, we exploit low-rank adaptation and modulation of intermediate features to compensate for the missing modalities. We demonstrate that such adaptation can partially bridge performance drop due to missing modalities and outperform independent, dedicated networks trained for the available modality combinations in some cases. The proposed adaptation requires extremely small number of parameters (e.g., fewer than 0.7% of the total parameters in most experiments). We conduct a series of experiments to highlight the robustness of our proposed method using diverse datasets for RGB-thermal and RGB-Depth semantic segmentation, multimodal material segmentation, and multimodal sentiment analysis tasks. Our proposed method demonstrates versatility across various tasks and datasets, and outperforms existing methods for robust multimodal learning with missing modalities.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"- Development of multimodal learning algorithms robust to missing modalities is crucial since observations from some modality could go often be missing due to sensor/hardware failure.\\n\\n- The proposed method is effective and importantly efficient as it introduces a very small set of parameters towards making a pre-trained model robust to missing modalities.\\n\\n- The paper is mostly well-written and easier to read.\\n\\n- Experiments have been performed on two different tasks of multimodal semantic segmentation and multimodal sentiment analyses. Results claim to surpass the existing methods and (often) dedicated methods when the modalities are missing at test time.\", \"Weaknesses\": \"- Although the idea is simple and effective for achieving parameter-efficient adaptation of models, the application of existing techniques (i.e. low rank adaptation, scale and shift transformations, and BitFit) to adapt a pre-trained model on a subset of modalities seems low from novelty aspect.\\n\\n\\n- There is no insight and/or analyses into how the proposed paramater-efficient is helpful towards making the model robust to missing modalities. It is important to develop the grounding of the proposed method since only showing improvements over the baseline with empirical results doesn't help much towards this.\\n\\n- The paper doesn\u2019t mention any clear justification on why SSA was chosen over the LoRA and BitFit to report the results in the paper.\\n\\n- Comparison is missing with any of the existing method for the task of multimodal sentiment analyses. \\n\\n- Why CMU-MOSI and CMU-MSEI datasets were chosen to evaluate and report the performance of the method?\", \"Questions\": \"- There is no study that fairly compares the parameter efficiency of the proposed method with competing methods?\\n\\n- What are the possible reasons for relatively lower performance of other adaptation strategies, compared to SSA, in Table 4? Some explanation would be helpful to better understand their comparison in missing modalities scenario.\"}"}]}
{"paper_id": "EEqEyyYUwc", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nDiffusion guidance pertains to the use of conditional diffusion under specific constraints, such as class labels or textual inputs. There are primarily two methodologies: Classifier Guidance leverages a pre-trained classifier to guide generation towards the condition, while Classifier-Free Guidance achieves implicit guidance without any classifier but by entering the condition during training. Both approaches employ a weighting parameter \u03c9 that determines the trade-off between\nsample fidelity (unconditional diffusion) and conditional adherence. In this paper, we posit that such conflict between image quality and condition arises, in part, due to misclassification and conflicted gradients from the explicit or implicit classifier, especially when the noise is high i.e., at the first stages of generation. To address this, we introduce a progressive weighting scheme, called Progressive-Guidance, where we make the weight of the guidance term dependent on the timestep. We propose two-time dependent weighting schemes: a simple heuristic, and a more\nprecise gradient-norm-based method. Progressive-Guidance can be implemented without retraining the model and with only a few additional lines of code. We report enhanced performance in benchmark metrics on three tasks: class-conditional image generation, text-to-image generation, and text-to-motion generation.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"1.\\tThe empirical analysis in section 3 is interesting and reasonable.\\n2.\\tThe motivation of this paper is clear.\", \"Weaknesses\": \"1.\\tThe main idea of the proposed methods is to attenuate the influence of guidance in the initial stages. However, based on my experiences, with small guidance weight in the initial stages, the structural integrity and layout of the generated images by diffusion models would be disrupted, particularly for some complex structures. The authors should provide more evaluations of the samples with complex structures, such as instances with human bodies or longer prompts.  Actually, I believe that the trade-off between sample fidelity and conditional adherence still exists, even with the proposed methods.\\n2.\\tI have read the appendix carefully. It is weird to find that the proposed methods cannot outperform the baselines in terms of CLIP-Score with a small guidance weight w.  If I focus on the experimental results with w<10, which is actually used in practice, what are the corresponding trade-off curves in Figure 5?  \\n3.\\tThe paper does not conduct extensive experiments on different model architectures, and samplers to validate the robustness and generality of their method. \\na)\\tModels: some more powerful diffusion models should be involved for comparisons, e.g., SD XL[1], and DeepFloyd IF[2], which can both be accessed publicly. Based on my understanding, those models can provide a more accurate estimation for classifications at the initial stage of the generation. As a result, the performance improvement of the proposed methods may decrease in those models. \\nb)\\tSamplers: DDIM sampler is the main selection for this paper to sample images during generation. More samplers, such as DPMSlover[3], should also be considered.  \\nc)\\tHuman-level metric: It is well-known that some quantitative metrics like FID, and IS, may be problematic in some cases. Human evaluation should be involved to provide more clear comparisons. \\n[1] https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0\\n[2] https://huggingface.co/DeepFloyd/IF-I-XL-v1.0\\n[3] Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps\", \"Questions\": \"1.\\tWhat are the corresponding trade-off curves in Figure 5 if we focus on the cases with guidance weight w<10.0?\\n2.\\tProvide more experimental analysis mentioned above to validate the proposed methods.\"}"}]}
{"paper_id": "lkIRFglmTp", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nWe consider the reinforcement learning problem under partial observability, where observations in the decision process lack the Markov property. To cope with partial observability, first we must detect it. We introduce the $\\lambda$-discrepancy: a measure of the degree of non-Markovianity of system dynamics. The $\\lambda$-discrepancy is the difference between TD($\\lambda$) value functions for two different values of $\\lambda$; for example, between 1-step temporal difference learning (TD($0$)), which makes an implicit Markov assumption, and Monte Carlo value estimation (TD($1$)), which does not. We prove that this observable and scalable value-based measure is a reliable signal of partial observability. We then use it as an optimization target for resolving partial observability by searching for memory functions---functions over the agent's history---to augment the agent's observations and reduce $\\lambda$-discrepancy.  We empirically demonstrate that our approach produces memory-augmented observations that resolve partial observability and improve decision making.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"- The paper is generally well written\\n- The paper is well motivated -- a metric denoting the Markovness of a POMDP is a very useful tool\\n- The approach is novel\", \"Weaknesses\": \"- The experiment section is lacking, only comparing against a memory-free baseline\\n- The tested environments and models are very low dimensional, and it's not clear such a method would scale to more interesting problems\\n- If I am understanding this correctly, the proposed metric is flawed in that errors in the Q function (not the state estimator) will result in a $\\\\lambda$-discrepancy. For example, given a perfect state estimator $M^*(o_1, o_2, ..., o_n) = s_n$ and two imperfect Q functions $Q_\\\\theta, Q_\\\\phi$: $ \\\\lVert Q_{\\\\theta}^{\\\\lambda_1}(s_n) - Q_{\\\\phi}^{\\\\lambda_2}(s_n) \\\\rVert > 0$. In this case, I do not see how $\\\\lambda$-discrepancy is a better metric of Markovness than the traditional TD(0) Q learning error.\", \"Questions\": \"- What makes the $\\\\lambda$-discrepancy specific to partial observability? A policy trained on a fully-observable MDP will also have a $\\\\lambda$-discrepancy if the Q function is even slightly less-than-optimal.\"}"}]}
{"paper_id": "Gsn43rbDwp", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nIn the present NISQ era, quantum machine learning encounters significant challenges in realizing practical implementations due to constrained resources. Recently, a novel single qubit methodology enabled the execution of numerous operations on a single qubit.  However, the applicability of this approach to more intricate neural network architectures remains uncertain. In this study, we investigate graph data, which poses inherent challenges for machine learning due to its intricate structure. Through the utilization of quantum walk, we introduce a graph embedding approach to reduce network parameters, culminating in the design of a novel single-qubit Quantum Graph Neural Network (sQGNN) architecture. Our experimental assessments encompass both simulations and practical executions on quantum computers, revealing that the devised sQGNNs adeptly adapt to graphs of varying dimensions and compositions. Consequently, these single-qubit QGNNs make efficient use of limited qubit resources, paving the way for the realization of expansive quantum graph neural networks on resource-constrained Variational Quantum Circuits (VQCs). This breakthrough opens up new possibilities for practical applications within the current NISQ regime.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"The paper introduces new method using quantum neural networks by combining the existing ideas to efficiently classify graph datasets.  The method uses only single qubit thereby reducing the quantum resources required as well as mitigating the barren plateau issues. The method is also demonstrated to show robustness against noises. The quantum features also enable the method to reduce the parameters required from thousand to twelve.\", \"Weaknesses\": \"Section 2 which describes the method is not written clearly and is hard to follow the steps leading to embedding of graph. Also, there is lack of continuity and proper explanation of variables used in equations in section 2.3. Also, from the experiment results shown in figure 3, the method is doubtful as the success probability is marginal for Class 1. There are a few other issues that is raised in the questions section below.\", \"Questions\": \"1.\\tWhy there is a difference in success probability for Class 0 and 1? It seems that the QGNN is not able to change the initial state.\\n2.\\tWhy the simulations in ideal simulated environment as shown in table 1 has lesser fidelity than using noisy real quantum computers? Providing a figure similar to figure 3 for ideal simulated environment is good for comparison.\\n3.\\tThe error bars for the results seems very high (in table 1 and 2). Is there a way to confine the errors?\\n4.\\tUsing two qubits for encoding seems to worsen the training as shown in table 1. Also, the explanation given for the reduction in standard deviation of dual model is not clear as increasing the number of qubits usually tend to increase the error rates. \\n5.\\tThe literature about existing methods and its pros and cons compared to the new method is missing.\"}"}]}
{"paper_id": "DOerIFfUbs", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nContrastive pre-training on image-text pairs, exemplified by CLIP, becomes a standard technique for learning multi-modal visual-language representations. Although CLIP has demonstrated remarkable performance, training it from scratch on noisy web-scale datasets is computationally demanding. On the other hand, mask-then-predict pre-training approaches, like Masked Image Modeling (MIM), offer efficient self-supervised learning for single-modal representations. This paper introduces Unmasked Token Alignment (UTA), a method that leverages existing CLIP models to further enhance its vision-language representations. UTA trains a Vision Transformer (ViT) by aligning unmasked visual tokens to the corresponding image tokens from a frozen CLIP vision encoder, which automatically aligns the ViT model with the CLIP text encoder. The pre-trained ViT can be directly applied for zero-shot evaluation even without training on image-text pairs. Compared to MIM approaches, UTA does not suffer from training-finetuning inconsistency and is much more training-efficient by avoiding using the extra $\\mathrm{[MASK]}$ tokens. Extensive experimental results demonstrate that UTA can enhance CLIP models and outperform existing MIM methods on various uni- and multi-modal benchmarks.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"-It is a good idea to perform unmask token alignment with the pretrained CLIP model due to its efficiency. \\n\\n-The experiments are quite extensive including many downstream tasks. The method also achieves the SOTA on most of the datasets.\", \"Weaknesses\": \"-Motivation of reducing pretraining costs is not convincing. In particular, the abstract claims that \\\"training CLIP from scratch on noisy web-scale datasets is computationally demanding\\\". Although this is true, this paper does not solve this issue at all because it still relies on a pretrained CLIP model at the first place.\\n\\n-The performance gains seem to come from the contrastive finetuning rather than the proposed unmasked alignment pretraining. In Table 1, comparing with the CLIP teacher i.e., EVA-CLIP, there is always a performance drop for UTA without finetuning. This is concerning because this paper claims the unmasked alignment pretraining as one of the main contributions. \\n\\n-One important baseline is missing. Since finetuning is very effective for zero-shot image classification, this paper should also compare with the CLIP teacher i.e., EVA-CLIP,  that is also finetuned on the same dataset. \\n\\n-Improving the VIT efficiency by dropping masked tokens has been done in [A].  This paper fails to cite this important reference paper and claims it to be something new. \\n\\n[A] Li et al., Scaling Language-Image Pre-training via Masking. CVPR 2023\\n\\n-The improvement in Table 4 & 5 (detection and segmentation) is marginal (< 1%). This is also concerning. The method seems to be limited to image-level prediction tasks.\", \"Questions\": \"The authors are highly encouraged to address my questions mentioned in the weakness. In addition, I have the following questions. \\n\\n-Is the CLIP teacher-model always the giant EVA-CLIP? \\n\\n-It would be good to provide the CLIP teach results in Table 3, 4, & 5.\\n\\n-This paper says that it is following previous works to perform the second-stage contrastive finetuning without proving the reference. Please provide the reference. \\n\\n-Is finetuning helpful in Table 4 & 5?\\n\\n-Is masking strategy applied in the finetuning stage?\"}"}]}
{"paper_id": "yIEKq72cTE", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nFederated learning (FL) has emerged as a promising solution to enable distributed learning on sensitive data without centralized storage and sharing. However, FL is vulnerable to data poisoning attacks, where malicious clients aim to manipulate the training process by injecting poisonous data. Existing defense mechanisms for FL suffer from limitations, including a trade-off between precision and robustness, assumptions on asymptotic optimal bounds on error rates of parameters, i.i.d. data distributions, and strong-convexity assumptions on the optimization problem. To address these limitations, we propose a novel framework called Federated Learning Optimal Transport (FLOT). Our method leverages the Wasserstein barycentric technique to obtain a global model from a set of locally trained models on client devices. Additionally, FLOT introduces a loss function-based rejection (LFR) mechanism to suppress malicious updates and a dynamic weighting scheme to optimize the Wasserstein barycentric aggregation function. We evaluate FLOT on four benchmark datasets: GTSRB, KBTS, CIFAR10, and EMNIST. Our experimental results demonstrate that FLOT outperforms existing baseline methods under single and multi-client attack settings. Also, it serves as a robust client selection technique under no attack. We also prove the Byzantine resilience of FLOT to demonstrate its effectiveness. These results underscore the practical significance of FLOT as an effective defense mechanism against data poisoning attacks in FL while maintaining high accuracy and scalability. The robustness and effectiveness of FLOT make it a promising solution for real-world applications where data privacy and security are critical.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. This paper proposes an optimal transport-based dynamic weighted federated aggregation method designed to mitigate untargeted data poisoning attacks within the FL framework. \\n2. The complexity of the proposed defense method has a substantial improvement compared to Krum.\\n3. The theoretical convergence results are provided.\\n4. Eextensive experimental results demonstrate that the proposed method consistently outperform other methods, including Byzantine robust aggregation rules, in terms of classification performance under both single-client and 50% Byzantine worker scenarios.\", \"Weaknesses\": \"1. The proposed method relies on validation data at the server. This validation data will affect the performance of the proposed defense method much. If the server cannot obtain the training data, the performance of the proposed method should be shown. \\n2. Some advanced defense methods are missing in the introduction and experiments, e.g., [1] and [2]. \\n3. The datasets used in this paper are too simple and limited to image classification tasks. Some real-world datasets, e.g., imagenet and cifar100, and other tasks should be considered, e.g., sentiment classification and multi-modal tasks.\\n4. The federated self-supervised learning framework should be considered in the experiments.\\n5. The convergence analysis is not meaningful, due to some impractical assumptions, e.g., Polyak-\u0141ojasiewicz (PL) inequality.\\n6. The attacker setting in this paper is also simple. There are some advanced model poisoning attack methods, e.g., [3] and etc. \\n\\nThe references mentioned are as follows:\\n[1] Xiaoyu Cao, Jinyuan Jia, Zaixi Zhang, and Neil Zhenqiang Gong. \\\"FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information,\\\" In IEEE Symposium on Security and Privacy, 2023.\\n[2] Sungwon Park, Sungwon Han, Fangzhao Wu, Sundong Kim, Bin Zhu, Xing Xie, and Meeyoung Cha. \\\"FedDefender: Client-Side Attack-Tolerant Federated Learning,\\\" In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2023.\\n[3] V Shejwalkar, A Houmansadr, \\\"Manipulating the byzantine: Optimizing model poisoning attacks and defenses for federated learning, \\\" In NDSS, 2021.\", \"Questions\": \"1. Please show the performance of the proposed method when the server cannot obtain the training data and enough training data. \\n2. Please add some advanced defense methods in the experiments, e.g., [1] and [2]. \\n3. The datasets used in this paper are too simple and limited to image classification tasks. Some real-world datasets, e.g., imagenet and cifar100, and other tasks should be considered, e.g., sentiment classification and multi-modal tasks.\\n4. The federated self-supervised learning framework should be considered in the experiments.\\n5. The convergence analysis is not meaningful, due to some impractical assumptions, e.g., Polyak-\u0141ojasiewicz (PL) inequality.\\n6. The attacker setting in this paper is also simple. There are some advanced model poisoning attack methods, e.g., [3] and etc.\"}"}]}
{"paper_id": "4fH5ELoWFu", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nAdversarial examples pose a security threat to many critical systems built on neural networks (such as face recognition systems and self-driving cars). While many methods have been proposed to build robust models, how to build certifiably robust yet accurate neural network models remains an open problem. For example, adversarial training improves empirical robustness, but they do not provide certification of the model's robustness. Conversely, certified training provides certified robustness but at the cost of a significant accuracy drop. In this work, we propose a novel approach that aims to achieve both high accuracy and certified probabilistic robustness. Our method has two parts which together achieve our goal, \\emph{i.e.}, a probabilistic robust training method with an additional goal of minimizing variance in divergence in a given vicinity and a runtime inference method for certified probabilistic robustness of the predictions. Compared to alternative methods such as randomized smoothing and certified training, our approach avoids introducing strong noise during training, is effective against a variety of perturbations, and most importantly, achieves certified probabilistic robustness without sacrificing accuracy. Our experiments on multiple models trained on different datasets demonstrate that our approach significantly outperforms existing approaches in terms of both certification rate and accuracy.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"There are a few main concerns with the paper (see Questions below) that need to be addressed. They may be based on a misunderstanding of the approach, and so appropriate answers from the authors would lead me to raise my score.\", \"Weaknesses\": \"My main concerns are raised in the Questions section.\\n\\nBut some minor details:\\n- in section 2 it was stated initially that h outputed a label. And yet later on in the same paragraph this was changed to logits. Precision/consistency is required\\n- writing G_x = arg max h(x) for example needs to make it very clear what the argument is they're searching over (vector index of logit output here)\\n- It is claimed in the introduction that randomised smoothing suffers from \\\"significant accuracy loss\\\". This is a broad statement to make as for example one can vary the variance of the added noise in order to vary the certified robust radius achieved (and also the impact on accuracy)\\n- In equation (2) it is not clear what the random variable (more importantly its distribution) is for the inner Probability\\n- near end of section 2 ... \\\"solution expects efficiency\\\" should be \\\"solution exhibits efficiency\\\"?\\n- in (1) it seems to be defining an adversarial example as a pair of points x1 and x2. Yet later on single points are referred to as adversarial examples. The definition (1) is a bit misleading\", \"Questions\": \"- In (3), conventionally I would have thought it should be G_x not G_t in the loss. i.e. you are trying to maximise the loss for the target label at x as t varies in the disk around x (also it is not made clear in the max specification that t is the free variable here)\\nThe same problem occurs in the discussion below equation (5). The text talks of computing the loss against the label of x and yet the expression in the bracket \\\"(i.e. E ....)\\\" has G_t used as the label\\n\\n- The distribution of t is not clear (in order to evaluate the expectation of the loss mean and variance). Is it uniform in the disk around x?\\n\\n- My main question is more fundamental. Firstly, it is not clear what classifier's output is being certified. Is it h or h^\\\\dagger\\n\\n(i) If h, then it is not sufficient to show that adversarial examples are sparse in the disk of radius epsilon around x. This was implied in the statement \\\"to establish certified probabilistic robustness, we must show that there is a guaranteed upper bound on the probability of adversarial example\\\".  But it doesn't seem to be sufficient. e.g. if there is one adversarial point x' in the disk around x and an attacker has some algorithm that allows them to find it, then the point x' is adversarial, regardless of the fact that such points are very rare.\\n\\n(ii) If h^\\\\dagger (which seems more likely what is meant), then the intent is to say that the majority class value is the classification at x, certified on the disk around x if the other classes are (statistically) sufficiently small. [Separate note - I believe there may a difference between equation (6) setting h as the expected logits value and then doing argmax for the class on that, and algorithm 2 which does the argmax at each sample point and then counts the occurrences of each class and takes the max. This is a point which is discussed in the certified robustness literature]\\nIf the claim that the vast majority of the points being of say class 1 in the disk is evidence that the classifier h\\\\dagger is always 1 on the disk, this seems wrong. If evaluating h^\\\\dagger at another point in the disk, we know nothing about the values outside the current disk. Hence the value of h^\\\\dagger at a point where say its disk has less than half its area in common with the disk around x is unknowable. Approaches to randomised smoothing are able to make assertions about the values across a disk (of a radius which depends on the logit differences at x), given their long-tailed noise distributions.\\n\\nPossibly this is a misunderstanding of what exactly is being claimed as being the certified robust classifier and what that property preciesly means. I look forward to clarifications.\"}"}]}
{"paper_id": "Aq35gl2c1k", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nCritical learning periods are periods early in development where temporary sensory deficits can have a permanent effect on behavior and learned representations. \nDespite the radical differences between biological and artificial networks, critical learning periods have been empirically observed in both systems. This suggests that critical periods may be fundamental to learning and not an accident of biology.\nYet, why exactly critical periods emerge in deep networks is still an open question, and in particular it is unclear whether the critical periods observed in both systems depend on particular architectural or optimization details. To isolate the key underlying factors, we focus on deep linear network models, and show that, surprisingly, such networks also display much of the behavior seen in biology and artificial networks, while being amenable to analytical treatment. We show that critical periods depend on the depth of the model and structure of the data distribution. We also show analytically and in simulations that the learning of features is tied to competition between sources. Finally, we extend our analysis to multi-task learning to show that pre-training on certain tasks can damage the transfer performance on new tasks, and show how this depends on the relationship between tasks and the duration of the pre-training stage. To the best of our knowledge, our work provides the first analytically tractable model that sheds light into why critical learning periods emerge in biological and artificial networks.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 10, \"Confidence\": 4, \"Strengths\": \"Well written, interesting avenue of research. Results well explained early in the text, with clear examples and clear explanations of experimental settings and technical details. Particularly nice clarity given the cross-disciplinary nature of the work.\\nRelated work is well written, reasonably thorough (to my knowledge), and well-explained.\\nNice phase portraits.\\nCaptions are mostly well-explained and self contained! (I don't know why this is so rare, but good job)\", \"Weaknesses\": \"It's a bit of a tradeoff with having a clear hypothesis, but I find the case oversimplified/misframed\u00a0 in the beginning and conclusion. Sure the literal biochemical explanation doesn't hold in an artificial setting, but analogies of this (reduced plasticity) could well occur. I don't think these are 'competing' hypotheses as they are framed; plasticity, imperfect optimization, etc. are *mechanisms* by which critical periods could arise in both artificial and biological systems; it doesn't tell us the reason\u00a0\u00a0(inherent to learning, quirks of substrate, something else, etc.). In the conclusion \\\"natural information processing constraints\\\" are contrasted with biochemical processes like plasticity as an alternative hypothesis, but what is (decreased) plasticity buyt a natural information processing constraint? Demonstrating presence in artificial systems doesn't estabilsh it as a general feature of learning, except insofar as we've defined learning narrowly to be done by biological and ANNs (does naive bayes learn? does it exhibit critical periods?). It becomes sort of a circular argument about what does \\\"learning\\\" even mean. This commonality is actually supported by your experiments in Fig 3 recreating Guillery's. Anyway, I think the key sentences/claims (e.g. last of the abs) and results stand and are extremely interesting,\u00a0 I would just like to see this framed around mechanisms or instantiations of information processing constraints, or something, rather than competing hypotheses. and maybe acknowledge\u00a0more of the ambiguity of what \\\"learning systems in general\\\" means/ (without compromising clarity and narrative, which I believe are strong points of the paper).\\n\\nTerms used in equations are not explained when introduced\u00a0 in 3.1 -- I know it's from a citation and you might be short on space, but you will lose a lot of readers here and it's a shame not to continue the clear prose you've had thus far.\\n\\nAround eqn 9, connection of matric completion to what you're trying to do is not clear -- be explicit about how imputing missing values is the same as / allows us to specify\u00a0 task relationships with flexibility.\u00a0\\n\\nWhile the relationship of experiments to showing dependence on depth is very clear and well-explained, the insights and relationship of experiments to data distribution is much less clear.\", \"Questions\": \"See \\\"weaknesses\\\" for some less concrete suggestions (feel free to tell me what you plan to write if you address them and I'm happy to provide feedback!)\\n\\nThe main other thing I'd like to see is connections made/discussed with nonlinear networks -- what should we expect to change vs. hold? maybe according to the sources you cite (e.g. Saxe), or ideally, a suite of experiments with e.g. MLPs of increasing depth. \\nAnother specific link could be made in Fig. 6 to scaling laws.\\n\\nsmall things:\\n\u00a0- first sentence shouldnt repeat abstract\\n\u00a0- 'competition between pathways affect' -> affects\\n- inconsistent use of \\\"artificial neural network\\\", \\\"deep net\\\", \\\"neural net\\\", et al. For me it's fine, but could be confusing for someone outside the field; better to be consistent\u00a0\\n- title for plot should be removed (conflicts/causes potential confusion vs caption and X axis label, nonstandard for technical works to have an embedded title)\\n\u00a0- remind us what \\\"Path Singular Value\\\" is in the caption/change caption e.g .singular value dimensions (confusing that its referred to as 1-5 but labelled 0-4 on plot and called something different). All this stuff explaining the axis and basics of the plots\u00a0should go first, before the interpretation.\u00a0- I didn't understand the comment \\\"and do not affect performance on the final gask\\\" in fig $. Does it mean they have equal performance?\"}"}]}
{"paper_id": "jH67LHVOIO", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nA model is considered well-calibrated when its probability estimate aligns with the actual likelihood of the output being correct. Calibrating language models (LMs) is crucial, as it plays a vital role in detecting and mitigating hallucinations of LMs as well as building more trustworthy models. However, standard calibration techniques may not be suited for LM calibration. For instance, post-processing methods such as temperature scaling do not reorder the candidate generations. On the other hand, training-based methods require fine-tuning the entire model, which is impractical for LMs of large scale. We present LitCab, a lightweight calibration mechanism consisting of a single linear layer that takes the input text representation and predicts a bias term, which is then added to the LM output logits. LitCab improves model calibration by only adding < 2% of the original model parameters. For evaluation, we construct CaT, a benchmark consisting of eight text generation tasks, covering responses ranging from short phrases to paragraphs. We test LitCab with Llama2-7B, where it improves calibration across all tasks, reducing the average ECE score by as large as 30%. We further conduct a comprehensive evaluation with multiple popular open-sourced LMs from GPT and LLaMA families, yielding the following key findings: (i) Larger models within the same family exhibit better calibration on tasks with short generation tasks, but not necessarily for longer ones. (ii) GPT-family models show superior calibration compared to LLaMA, Llama2, and Vicuna models, despite having much fewer parameters. (iii) Fine-tuning pretrained model (e.g., LLaMA) with samples of limited purpose (e.g., conversations) may lead to worse calibration, highlighting the importance of fine-tuning setups for calibrating LMs.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 3, \"Strengths\": \"1. The model calibration is highly meaningful, especially for nowadays LLMs, and the proposed method can be used for most of the open-source models.\\n\\n2. The proposed method is simple yet effective, although it needs some training examples collected by ChatGPT and GPT-4.\\n\\n3. The collected datasets CAT can also be useful for evaluating the methods for LLMs calibration.\", \"Weaknesses\": \"1. The paper lacks discussion and comparison of some related works [1][2].\\n\\n- [1] Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. arXiv 2023.\\n\\n- [2] Eliciting Latent Predictions from Transformers with the Tuned Lens. arXiv 2023.\\n\\n2. There are still some efficient fine-tuning methods (e.g. LoRA) that can leverage additional training data, but the paper lacks results compared with such methods.\", \"Questions\": \"1. It appears that your method involves using GPT-4 to generate training data, which has also been widely used in recent research. Does this introduce potential unfairness in the comparison? If the training data cannot be generated using GPT-4 or if its quality is poor, would the method still be effective?\\n\\n2. The proposed method seems to have minimal differences compared to the original LM, and the experimental results also indicate that the original LM performs not very badly. Therefore, what are the main advantages of the proposed method? How does it improve LLMs qualitatively or quantitatively?\\n\\n3. Why do you use different thresholds (e.g., cov@50, cov@90, cov@60) for different datasets?\"}"}]}
{"paper_id": "2J25Vi9W8I", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nOrdinal regression is a fundamental problem within the field of computer vision. While pre-trained vision-language models have exhibited impressive performance on various vision tasks, their potential for ordinal regression has received less exploration.\nIn this paper, we introduce a novel method called RegCLIP, a label-efficient coarse-to-fine method for ordinal regression. This approach incorporates language prior information to gradually refine predictions and achieve fine-grained results.\nOur RegCLIP framework encompasses two levels of coarse-to-fine concepts. The first level is a stagewise approach, performing intermediate classification initially and then refining the predictions. The second level is to generate coarse semantic labels as intermediate classes and subsequently refine them into fine-grained labels. To achieve it, we propose a novel coarse semantic label generation via large language models, which generates coarse labels. To further enhance the precision of predictions, we propose a novel fine-grained cross-modal ranking-based loss specifically designed to update fine-grained semantic labels with both semantic and ordinal alignment. Experimental results on three general ordinal regression tasks demonstrate the effectiveness of RegCLIP, exceeding state-of-the-art methods with a large margin, with 10% overall accuracy improvement on historical image dating, 1.74% overall accuracy improvement on image aesthetics assessment, and 1.33 MAE reduction on age estimation under 1-shot setting.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 5, \"Strengths\": \"- The paper is easy to follow and well organized.\\n- The proposed algorithm is simple but effective way to utilize CLIP for ordinal regression tasks.\\n- The proposed algorithm shows good results on various datasets.\", \"Weaknesses\": \"1. Even though the authors mentioned that they will release the codes, I think there are some missing details which are important for reproducibility and further researches.\\n\\n     - Section 3.5: Exactly, what is the revised FCRC loss? \\n     - How the fine-grained labels are predicted?\\n     - What is batch size M?\\n     - In Eq(2), what is the default value for $\\\\beta$?\\n     - In Eq(2), how the scaled ranking differences are normalized?\\n     - What are language templates for historical image classification and aesthetic quality assessments?\\n\\n\\n2. It would be good to have comparison of feature space visualizations for ablated methods. I think it can provide insights and intuitive understanding of each design choice for readers.  \\n\\n\\n3. MORPH II dataset has many different evaluation protocols. However, the results for only one setting are provided in the paper. So, it may be more useful to have more comparisons on MORPH II dataset. \\n\\n\\n4. There are some other popular datasets for age estimation (e.g. CACD, UTK, CLAP2015, Adience, \u2026). So, it would be better if the results on those datasets are compared in the paper.\", \"Questions\": \"The proposed algorithm is simple but makes sense for me. However, it omits some important details in the paper and does not contain many analysis of the proposed algorithm, as I mentioned in the weakness section. (Please see the weakness section for other concerns.) Nevertheless, in overall, I'm positive about the acceptance of this paper, because it outperforms all conventional methods with meaningful performance gaps and also authors described that the source codes will be released for algorithm details.\"}"}]}
{"paper_id": "gENfMmUIkT", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nObject detection with computer vision and convolutional neural networks on resource constrained devices can be challenging. The limited power and processing capacity of these devices complicates the use of deep neural networks and other object detection methods. To address this problem, we propose a pipeline-based approach. We introduce a multi-step detection pipeline considering the size of the objects to be detected and the correlation among them. To evaluate the performance of this approach, we test it in a collaborative smart surveillance system employing edge computing and the internet of things paradigm. Additionally, field testing was conducted considering real world surveillance scenarios. Results showed that the introduction of the pipeline-based processing improved the execution time by a factor of 3 and produced a significant improvement on the mean average precision.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 1, \"Confidence\": 5, \"Strengths\": \"-\\tThe paper addresses a significant issue concerning object detection on resource-constrained IoT devices in the context of smart surveillance systems. Ensuring safety in metropolitan areas is a pressing concern, and the proposed approach aligns with the needs of smart cities and law enforcement.\\n-\\tCollaborative testing with the police department in real-world scenarios adds credibility to the research and demonstrates the practical applicability of the proposed approach.\", \"Weaknesses\": \"-\\tThe work seems to be an implementation of already existing and popular YOLO architecture on custom dataset.\\n-\\tThe work does not compare the work with other works that have been developed for surveillance.\\n-\\tThe pipeline-based approach uses multiple runs of the YOLO model to detect objects which adds more time delay to detection missing other important potential events while processing one event.\\n-\\tWhile the paper provides some quantitative data, a more detailed analysis of the results, especially in the pilot testing, would enhance the paper's completeness.\", \"Questions\": \"-\\tHow many classes are present in the custom dataset being used in this work? \\n-\\tIs the model trained on edge device (Labrador)? Or was it just used for Inference?\\n-\\tWhat is the frame rate used in this methodology for detection? \\n-\\tWhat measures have been taken to detect occluded objects?\\n-\\tSince the use case is a mobile surveillance and involves testing in real world scenario, what are the processes in place to identify false events transmitted?\\n-\\tHow do you keep up with the event generation when subsequent runs are needed? E.g. For generating an event from S3, you would need 8 seconds(1.5s+ 0.5s +6s). Does the system not detect any other events while it is processing one event?\"}"}]}
{"paper_id": "oKn2eMAdfc", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nRedundancy is a persistent challenge in Capsule Networks (CapsNet), leading to high computational costs and parameter counts (Jeong et al., 2019; Sharifi et al., 2021; Renzulli & Grangetto, 2022). Although previous works have introduced pruning after the initial capsule layer, dynamic routing\u2019s iterative and fully connected nature reintroduces inefficiencies and redundancy in deeper layers. In this paper, we propose the Orthogonal Capsule Network (OrthCaps) to reduce redundancy, improve routing performance and decrease parameter count. Specifically, an efficient pruned capsule layer is placed to discard redundant capsules and dynamic routing is replaced with orthogonal sparse attention routing. Besides, we orthogonalize weight matrices during routing to ensure feature diversity and sustain low capsule similarity, the idea of which is inspired by the application of orthogonality in Convolutional Neural Networks (CNNs). Moreover, a novel activation function named Capsule ReLU is proposed to address vanishing gradients. Our experiments on baseline datasets affirm the efficiency and robustness of OrthCaps in classification tasks, in which ablation studies validate the criticality of each component. Remarkably, with only 110k parameters, merely 1.25% of a standard Capsule Network\u2019s total, OrthCaps-Shallow outperforms state-of-the-art (SOTA) benchmarks on four datasets, while OrthCaps-Deep attains nearly SOTA accuracy with 1.2% of its parameters on four datasets. The code is available at https://github.com/ornamentt/Orthogonal-Capsnet\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 1, \"Confidence\": 2, \"Strengths\": \"1. The idea of utilizing orthogonality is interesting.\\n2. Usage of Householder parameterization is natural and easy to understand.\", \"Weaknesses\": \"1. This paper is very hard to follow. I got completely lost even reading the Introduction. I would suggest the authors completely rewrite the paper to make it readable.\\n1.1 It is mentioned that attention routing is used. But how does it eliminate the need for iteration? \\n1.2 What is the fully-connected problem and how is that solved by sparsemax-based self-attention?\\n1.3 Why is a simplified attention-routing mechanism used with OrthCaps-S and what is it?\\n1.4 What are capsule vector direction shifts and how are they solved with orthogonality? Why does preserving the direction of capsule vectors mitigate feature interference?\\n1.5 What is the motivation of  Capsule ReLU? Why is that used with OrthCaps-D but not OrthCaps-S?\\n\\n2. Why is capsule ReLU designed so? What is the motivation for using BN on a scalar (||s_j||^2)? It is very unnatural, so I would expect a clear motivation.\\n\\nI rate a strong reject mostly because the writing quality of this paper makes it difficult for me to accurately understand the essence of the proposed method, thus preventing me from evaluating its technical correctness and significance.\\n\\nOf course, as I am not an expert in capsule networks, it may be due to my lack of relevant knowledge that I cannot understand this paper. If the writing of this paper is not the issue, that is, if AC and other reviewers can understand this paper without difficulty, please let me know, and I am willing to modify my rating.\", \"Questions\": \"See the weakness part.\"}"}]}
{"paper_id": "KrWuDiW4Qm", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nA fundamental challenge in physics-informed machine learning (PIML) is the design of robust PIML methods for out-of-distribution (OOD) forecasting tasks. These OOD tasks require learning-to-learn from observations of the same (ODE) dynamical system with different unknown ODE parameters, and demand accurate forecasts even under out-of-support initial conditions and out-of-support ODE parameters. In this work we propose to improve the OOD robustness of PIML via a meta-learning procedure for causal structure discovery. Using three different OOD tasks, we empirically observe that the proposed approach significantly outperforms existing state-of-the-art PIML and deep learning methods (with $2\\times$ to $28\\times$ lower OOD errors).\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 5, \"Strengths\": \"1. Develop a meta-learning technique for identifying the underlying governing equations of dynamical systems.\\n\\n2. Formulate it as a bi-level optimization problem\\n\\n3. The evaluation results have demonstrated the good performance of the proposed method over baselines.\", \"Weaknesses\": \"1. How many basis functions are used in the experiments? If there is no prior knowledge, the number of basis functions will be very large.\\n\\n2. The proposed method still adopts the SINDy-like approach in which all key terms should be included in a set of basis functions. It may not work on complex equations like $dx/dt = 1/(x_1^2 + sin (x_2^3+x_3))$\\n\\n3. Conduct experiments on more complex systems in the CoDA paper, such as reaction-diffusion system and Navier-Stokes system\\n\\n4. I am curious about the causal structure in Fig. 4. Which two terms in each of the three dynamical systems have causal relationships? Please give an example to explain it. It seems that the authors did not discuss this in the Evaluation section.\\n\\n5. Please conduct experiments on noisy data.\", \"Questions\": \"Please see the comments above.\"}"}]}
{"paper_id": "RmQAKu1wCe", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nSpiking Neural Networks (SNNs), models inspired by neural mechanisms in the brain, allow for an energy-efficient implementation on neuromorphic hardware. However, the limitation of current direct training approaches lies in their ability to only optimize parameters for an SNN operating at a specific time step. This leads to the necessity for fine-tuning when generalizing to additional time steps, resulting in considerable computational inefficiency. In this study, we initially examine the feasibility of parameter sharing across structurally identical SNNs operating at different time steps. Subsequently, we propose an innovative training methodology-mixed time step training (MTT) that facilitates the development of a temporal flexible SNN (TFSNN). Throughout the training process, various time steps are arbitrarily assigned to distinct SNN blocks, accompanied by the establishment of novel inter-block communication protocols. Following training, the TFSNN can be simplified to an SNN operating at any chosen fixed time step, eliminating the need for fine-tuning. Experimental results across all primary datasets demonstrate that the TFSNN exhibits robust generalization capabilities surpassing existing training methodologies reliant on a fixed time step. Notably, we achieved a 96.84% accuracy rate on the CIFAR10 dataset, an 81.98% accuracy rate on the CIFAR100 dataset, and a 68.34% accuracy rate on the ImageNet dataset with T = 6.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"The MTT performs a wider search along the timestep setting by sampling different. Compared to naive NMT, MTT urges each layer of SNNs to learn feature that is weakly correlated to input length from the previous layer. Such a strong regularization causes an issue in assessing mean and variance within BN layers but is immediately resolved by calibration. Overall, I believe MTT marks a significant step forward in the right direction.\", \"Weaknesses\": \"However, all datasets used in MTT are directly retrieved or derived from static images (CIFAR10-DVS & CIFAR10, N-Caltech101 & Caltech101). I'm worried that these datasets contain essentially time-invariant features along the time dimension. Even for CIFAR10-DVS or N-Caltech101, only simple movement of static images is recorded using the event cameras. The authors should validate their methods on those datasets with affluent temporal dynamics, like from DVS-Gesture to audio such as GSC (Google SpeechCommands) or SHD (Spiking Heidelberg Dataset) to strengthen their results.\\n\\nThe former concern raises the other issue that, mixing different timesteps is partially empirically based on the finding that the temporal gradients resemble each other when timestep stretches or shrinks, and we expect the training will be stable to some extent since gradients are relatively similar. If the input has invariant length and a different amount of temporal information in essence, will the MTT work as it is now?\", \"Questions\": \"Could the authors demonstrate results on datasets with rich temporal information? This could bring some real challenges to the TFSNN.\"}"}]}
{"paper_id": "1P92J25hdf", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nInductive Bias (IB) has sparked a revolutionary transformation by incorporating the advantages of CNNs and Transformers, including scale invariance and integration of locality and long-range dependencies, which is called general IB for its wide applicability. However, its efficacy is currently not enjoyed by stereo matching, one of the geometric vision tasks, because of the ignorance of volume-level scale invariance and the limitation of high real-time requirement. In contrast, a specific IB  is adopted by constructing volume structure in stereo matching task, which helps to finally generate a confidence volume to predict disparity map (output), but fewer studies go into the specific volume structure. Based on the above issues, this paper develops a novel model named UStereo to introduce the general IB to stereo matching. Technically, we adopt inter-layer fusion to break down volume-level scale invariance to a recurrence strategy in initialization for information at low resolution and refinement process for the high, which further extends to capture long-range dependencies after shallow stacks of convolutions and normalization without time-consuming Transformers. Additionally, to reveal the role that the volume structure constructed by specific IB plays during inference, we propose the first-time in-depth study of volume at low resolution through varying degrees of restraint as well as 3 original statistic indicators to reflect the characteristics of representation within volumes. Experiments demonstrate UStereo has competitive performance with both fast speed and robust generalization, and ablation studies show the effectiveness of introducing general IB. Moreover, our analysis of the volumes at low resolution suggests they can be viewed as confidence volumes and a concentrated distribution of the disparity within volumes leads to enhanced performance, which could extend the role of the specific IB.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. This paper builds a new stereo matching network UStereo that can achieve real-time inference and great accuracy.\\n2. They propose to think about the specific IB in cost volume representation, which is interesting.\", \"Weaknesses\": \"1. The writing needs improving. Introducing the IB concept makes it hard to understand what the authors claim. I wonder why it needs to introduce IB instead of simply exploring scale invariance and long-range dependencies.\\n2. The results of the KITTI benchmarks are not impressive. Among real-time methods, we find CGI-Stereo, HITNet, and Fast-ACVNet+ get better performances.\\n3. There is a lack of evaluating the generalization ability in common settings. This paper claims UStereo has robust generalization, however, the common setting for generalization evaluation is on the KITTI, Middlebury, and ETH3D datasets.\", \"Questions\": \"1.  I still wonder what is the specific inductive bias in the volume representation. I think using the cost volume as confidence representation has been explored, especially in the are of Muli-view Stereo. So I want to know if there is any other specific IB proposed in this paper.\"}"}]}
{"paper_id": "lOwkOIUJtx", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nHigh accuracy, low latency and high energy efficiency represent a set of contradictory goals when searching for  system solutions for image classification and detection. While high-quality images naturally result in more precise detection and classification, they also result in a heavier computational workload for imaging and processing, reduce camera refresh rates, and increase the volume of data communication between the camera and processor. Taking inspiration from the foveal-peripheral sampling mechanism, saccade mechanism observed in the human visual system and the filling-in phenomena of brain, we have developed an active scene reconstruction architecture based on multiple foveal views. This model stitches together information from foveal and peripheral vision, which are sampled from multiple glances. Assisted by a reinforcement learning-based saccade mechanism, our model reduces the required input pixels by over 90\\% per frame while maintaining the same level of performance in image recognition as with the original images. We evaluated the effectiveness of our model using the GTSRB dataset and the ImageNet dataset. Using an equal number of input pixels, our study demonstrates a 5\\% higher image recognition accuracy compared to state-of-the-art foveal-peripheral vision systems. Furthermore, we demonstrate that our foveal sampling/saccadic scene reconstruction model exhibits significantly lower complexity and higher data efficiency during the training phase compared to existing approaches.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 3, \"Strengths\": \"I find the method simple and useful, with interesting potential application. \\nIt is appealing that the method seems to be suitable for existing classification models (no retraining).\", \"Weaknesses\": \"## Major\\n\\nI am confused how the image information from the sequential glimpses is passed and integrated in the predictive reconstruction model. Much more space is spent on the background to the hybrid loss function than actually making explicit how the sequential image information is used to improve reconstruction.\\n\\nIn addition, the abstract states \\\"our model reduces the required input pixels by over 90% per frame while maintaining the same level of performance in image recognition as with the original images.\\\" I don't understand where to find support for this claim in the results. For example, in Figure 3, all subsampled models perform worse than the original. The data in Figure 4 are coming closest to the original; is this what is meant?\\n\\nAlso, please clarify whether the experiments in Figure 3 are conducted with the trained saccade control model (which one?). \\n\\n\\n## Minor\\n\\n- Instead of \\\"continuous saccades\\\" a better terminology would be \\\"sequential saccades\\\" or \\\"scanpaths\\\". See e.g. [2, 3, 4]\\n- There are now known to be three types of photosensitive cells: rods, cones and intrinsically-photosensitive ganglion cells [1, 8]\\n- You use SSIM but the relevant paper(s) are not cited (e.g. [7]).\\n- Heading 3.1 \\\"Periphrl\\\"\\n\\n## Literature\\n\\n1. Do, M. T. H., & Yau, K.-W. (2010). Intrinsically Photosensitive Retinal Ganglion Cells. Physiol Rev, 90.\\n\\n1. Hoppe, D., & Rothkopf, C. A. (2019). Multi-step planning of eye movements in visual search. Scientific Reports, 9(1), 144. https://doi.org/10.1038/s41598-018-37536-0\\n\\n1. K\u00fcmmerer, M., & Bethge, M. (2021). State-of-the-Art in Human Scanpath Prediction (arXiv:2102.12239). arXiv. http://arxiv.org/abs/2102.12239\\n\\n1. K\u00fcmmerer, M., Bethge, M., & Wallis, T. S. A. (2022). DeepGaze III: Modeling free-viewing human scanpaths with deep learning. Journal of Vision, 22(5), 7. https://doi.org/10.1167/jov.22.5.7\\n\\n1. Rosenholtz, R. (2016). Capabilities and Limitations of Peripheral Vision. Annual Review of Vision Science, 2(1), 437\u2013457. https://doi.org/10.1146/annurev-vision-082114-035733\\n\\n1. Watson, A. B. (2014). A formula for human retinal ganglion cell receptive field density as a function of visual field location. Journal of Vision, 14(7), 15. https://doi.org/10.1167/14.7.15\\n\\n1. Wang, Z., Simoncelli, E. P., & Bovik, A. C. (2003). Multiscale structural similarity for image quality assessment. The Thirty-Seventh Asilomar Conference on Signals, Systems & Computers, 2003, 1398\u20131402. https://doi.org/10.1109/ACSSC.2003.1292216\\n\\n1. Zele, A. J., Feigl, B., Adhikari, P., Maynard, M. L., & Cao, D. (2018). Melanopsin photoreception contributes to human visual detection, temporal and colour processing. Scientific Reports, 8(1), 3842. https://doi.org/10.1038/s41598-018-22197-w\", \"Questions\": \"- I would like to see how the hybrid reconstruction loss changes over timestep, and not just classification accuracy.\\n- The sampling of the periphery of individual pixels with small probability is not very like human vision. Effectively this is providing low pass information. Have the authors considered how the sampling density could be approximated more plausibly (e.g. [6])?\\n- Have the authors considered comparing scanpath strategies learned in this model to human scanpaths (e.g. [3, 4])?\"}"}]}
{"paper_id": "532tcx7IHF", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nWe present RLLTE: a long-term evolution, extremely modular, and open-source framework for reinforcement learning (RL) research and application. Beyond delivering top-notch algorithm implementations, RLLTE also serves as a toolkit for developing algorithms. More specifically, RLLTE decouples the RL algorithms completely from the exploitation-exploration perspective, providing a large number of prototypes to accelerate algorithm development and evolution. In particular, RLLTE is the first RL framework to build a complete and luxuriant ecosystem, which includes model training, evaluation, deployment, benchmark hub, and large language model (LLM)-empowered copilot. RLLTE is expected to set standards for RL engineering practice and be highly stimulative for industry and academia.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"* Modular and customizable design, allowing for easy algorithm development and improvement.\\n* Long-term evolution plan, ensuring the framework stays up-to-date with the latest research.\\n* Comprehensive ecosystem, covering various aspects of RL research and application.\\n* Built-in support for data augmentation techniques, improving sample efficiency and generalization ability.\\n* Multi-hardware support, accommodating diverse computing hardware configurations.\", \"Weaknesses\": \"* The paper does not provide a thorough comparison of RLLTE with other existing RL frameworks.\\n* The proposed LLM-empowered copilot is in its early stages and may not be as effective as expected.\\n* The paper does not discuss potential limitations or challenges in implementing the proposed framework.\", \"Questions\": \"* How does RLLTE compare to other existing RL frameworks in terms of performance, modularity, and ease of use?\\n* What are the specific advantages of using RLLTE over other RL frameworks for different types of RL problems?\\n* How does the proposed LLM-empowered copilot improve the overall RL research and application process?\\n* Are there any potential limitations or challenges in implementing the proposed RLLTE framework that the authors have not discussed?\"}"}]}
{"paper_id": "MeB86edZ1P", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nNeuromorphic computing with spiking neural networks is promising for energy-efficient artificial intelligence (AI) applications. However, different from humans who continually learn different tasks in a lifetime, neural network models suffer from catastrophic forgetting. How could neuronal operations solve this problem is an important question for AI and neuroscience. Many previous studies draw inspiration from observed neuroscience phenomena and propose episodic replay or synaptic metaplasticity, but they are not guaranteed to explicitly preserve knowledge for neuron populations. Other works focus on machine learning methods with more mathematical grounding, e.g., orthogonal projection on high dimensional spaces, but there is no neural correspondence for neuromorphic computing. In this work, we develop a new method with neuronal operations based on lateral connections and Hebbian learning, which can protect knowledge by projecting activity traces of neurons into an orthogonal subspace so that synaptic weight update will not interfere with old tasks. We show that Hebbian and anti-Hebbian learning on recurrent lateral connections can effectively extract the principal subspace of neural activities and enable orthogonal projection. This provides new insights into how neural circuits and Hebbian learning can help continual learning, and also how the concept of orthogonal projection can be realized in neuronal systems. Our method is also flexible to utilize arbitrary training methods based on presynaptic activities/traces. Experiments show that our method consistently solves forgetting for spiking neural networks with nearly zero forgetting under various supervised training methods with different error propagation approaches, and outperforms previous approaches under various settings. Our method can pave a solid path for building continual neuromorphic computing systems. The code is available at https://github.com/pkuxmq/HLOP-SNN.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1.The paper introduces for the first time a method that fully utilizes neuronal operations to approximate orthogonal projection matrices, and achieves the best performance surpassing multiple baseline methods on various datasets in continual learning task.\\n2.The implementation based on Hebbian learning can seamlessly integrate with different training methods and aligns well with the parallel local learning approach of neuromorphic chips.\\n3.HLOP can be directly applied to SNNs, providing a new approach for continual learning in SNNs.\", \"Weaknesses\": \"1.The method lacks sufficient detail in the methodology section. Providing complete formulas or a specific illustrative example would enhance understanding. \\n2.The baselines compared in the study include EWC (Kirkpatrick et al., 2017), HAT (Serra et al., 2018), and GPM (Saha et al., 2021). It would be valuable to investigate if there are recent works that achieve better results than these methods.\\n3.Although the continual learning approach in this study relies solely on neuronal operations without directly utilizing past data, the increasing number of subspace neurons with each new task learned implies a form of data compression and storage to some extent.\\n4. Although the paper uses Hebbian learning to achieve a pure neuronal operation method, it seems that the newly added subspace neurons do not participate in the model's forward process. If this is the case, then despite being a pure neuronal operation approach, it is essentially just an estimation method for orthogonal projection matrices.\\n5. While the paper demonstrates the effectiveness of HLOP, it does not provide an explanation or analysis of why the weight matrices obtained through Hebbian learning can serve as a substitute for the orthogonal projection matrix.\", \"Questions\": \"See weaknesses.\"}"}]}
{"paper_id": "tVMPfEGT2w", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nIn this paper, we investigate the problem of offline Preference-based Reinforcement Learning (PbRL) with human feedback where feedback is available in the form of preference between trajectory pairs rather than explicit rewards. Our proposed algorithm consists of two main steps: (1) estimate the implicit reward using Maximum Likelihood Estimation (MLE) with general function approximation from offline data and (2) solve a distributionally robust planning problem over a confidence set around the MLE. We consider the general reward setting where the reward can be defined over the whole trajectory and provide a novel guarantee that allows us to learn any target policy with a polynomial number of samples, as long as the target policy is covered by the offline data. This guarantee is the first of its kind with general function approximation. To measure the coverage of the target policy, we introduce a new single-policy concentrability coefficient, which can be upper bounded by the per-trajectory concentrability coefficient. We also establish lower bounds that highlight the necessity of such concentrability and the difference from standard RL, where state-action-wise rewards are directly observed. We further extend and analyze our algorithm when the feedback is given over action pairs.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 4, \"Rating\": 8, \"Confidence\": 3, \"Strengths\": \"The paper is clearly written.\\nThe paper provides several strong and novel theoretically results on preference-based RL in offline setting. Some of these results also generalize the results of Zhu et al for linear function approximators to general function approximators.\", \"Weaknesses\": \"Although the main contribution is theoretical, it would be nice to see an empirical case study or two  where the given algorithm works as expected.\", \"Questions\": \"No questions.\"}"}]}
{"paper_id": "lsvlvWB9vz", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nModern distributed training relies heavily on communication compression to reduce the communication overhead. In this work, we study algorithms employing a popular class of contractive compressors in order to reduce communication overhead. However, the naive implementation often leads to unstable convergence or even exponential divergence due to the compression bias. Error Compensation (EC) is an extremely popular mechanism to mitigate the aforementioned issues during the training of models enhanced by contractive compression operators. Compared to the effectiveness of EC in the data homogeneous regime, the understanding of the practicality and theoretical foundations of EC in the data heterogeneous regime is limited. Existing convergence analyses typically rely on strong assumptions such as bounded gradients, bounded data heterogeneity, or large batch accesses, which are often infeasible in modern Machine Learning Applications. We resolve the majority of current issues by proposing EControl, a novel mechanism that can regulate error compensation by controlling the strength of the feedback signal. We prove fast convergence for EControl in standard strongly convex, general convex, and nonconvex settings without any additional assumptions on the problem or data heterogeneity. We conduct extensive numerical evaluations to illustrate the efficacy of our method and support our theoretical findings.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"New error compensation algorithm: EControl with theoritical gaurantees for strongly convex, general convex, and nonconvex functions\\nEmpirical experiments to support the theoretical gaurantees and assess the efficacy of EControl.\", \"Weaknesses\": \"see the section below (Questions)\", \"Questions\": \"-In step 6 of EC-Ideal you don't have the extra parameter \\\\eta that you have in EControl, can you explain why for EC-Ideal this is not needed. Will EC-Ideal have the same convergence guarantees if you add \\\\eta in step 6, with the same values as for EControl?\\n-Bounded variance: This assumption contradicts strong convexity. I understand it is in several works in the literature. Take for instance F(x,y,z) = zx^2 + (1-z)y^2, z follows Bernoulli (1/2), then E_z(norm(\\\\nabla F(x,y,z) - \\\\nabla E_z(F(x,y,z)))^2) = (x-y)^2 and this is not bounded for all x and y in R.\\n-You mention in page 7 that \\\"The asymptotic complexity of EControl in this regime with stochastic gradient is tight and cannot be improved,\\\"  I understand for the depence on \\\\epsilon, however can you tell why the dependence on other parameters can not be improved.\\n-In the experiments, how you relate \\\\zeta to the noniddness, why for instance zeta equal to zero is iid and zeta >0 is non idd.\\n-Did you try \\\\eta= delta/400 suggested by the theory in the experiments? why you need to fine tune it, since it is fixed by theory. Is only fixed \\\\eta = \\\\delta/400 that works in theory or many other values can work, this is not clear in the paper.\\n-Did you test with other compressors?\"}"}]}
{"paper_id": "3s4fZTr1ce", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nWhile reinforcement learning (RL) shows remarkable success in decision-making problems, it often requires a lot of interactions with the environment, and in sparse-reward environments, it is challenging to learn meaningful policies. Large Language Models (LLMs) can potentially provide valuable guidance to agents in learning policies, thereby enhancing the performance of RL algorithms in such environments. However, LLMs often encounter difficulties in understanding downstream tasks, which hinders their ability to optimally assist agents in these tasks. A common approach to mitigating this issue is to fine-tune the LLMs with task-related data, enabling them to offer useful guidance for RL agents. However, this approach encounters several difficulties, such as inaccessible model weights or the need for significant computational resources, making it impractical. In this work, we introduce RLAdapter, a framework that builds a better connection between RL algorithms and LLMs by incorporating an adapter model. Within the RLAdapter framework, fine-tuning a lightweight language model with information generated during the training process of RL agents significantly aids LLMs in adapting to downstream tasks, thereby providing better guidance for RL agents. We conducted experiments to evaluate RLAdapter in the Crafter environment, and the results show that RLAdapter surpasses the SOTA baselines. Furthermore, agents under our framework exhibit common-sense behaviors that are absent in baseline models.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"- Novelty and Clever Design: The introduction of an adapter model to bridge LLMs and RL algorithms is a novel idea. The adapter model helps in effectively summarizing and communicating the essential details from the RL agents to the LLMs.\\n- Empirical Evaluation: The empirical evaluation of RLAdapter in the Crafter environment and its comparison with several baselines demonstrate its effectiveness. The RLAdapter not only showed improved performance over state-of-the-art baselines but also exhibited agent behaviors grounded in common sense.\", \"Weaknesses\": \"- Understanding Score: The paper uses an understanding score calculated based on the semantic similarity between a trajectory and the LLM\u2019s sub-goals. However, it seems that this aspect is understudied in the paper. A more in-depth exploration and justification of the understanding score, possibly discussing its limitations, would have strengthened the paper. It\u2019s unclear whether it indeed does what the authors intended.\\n- It\u2019s unclear whether SFT is needed or even helpful. Perhaps few-shot-learning would be enough.\\n- In the ablation study, when removing the RLAdapter model, I believe the authors should have used the same prompts with the LLM. I was left thinking that perhaps the improved performance comes from a form of self-critique of the goal proposer before writing the next goal. The same could be achieved with prompting GPT.\\n- SPRING is mentioned quite few times, so I'd expect it to be better mentioned in \\\"related work\\\". In particular, examples of tight coupling with the environment.\", \"Questions\": \"- Analysis of Adapter Model's Capability: Does the adapter model have the capability of generating more general goals apart from making the LLM give more detailed goals? An exploration of this aspect would provide a more comprehensive view of the adapter model's functionalities.\\n  * If the adapter model only makes the LLM give more detailed sub-goals, we could simply prompt the LLM to analyze whether the last sub goal seems to be achieved and give more detailed sub-goals if not.\\n- Similar to the point above, it seems like the adapter model could be replaced by the LLM itself? Just querying it twice, one for analysis and another for sub-goal.\\n- Detailed Analysis: A more detailed analysis, such as an ablation study, on the understanding score, and how it influences the model's performance, could be beneficial.\\n- [Minor] The use of training steps in Table 1 as a metric might not be the most relevant. Consideration of other metrics like FLOPs could provide a more standardized comparison across different models. I understand this is probably not feasible given that OpenAI inference FLOPs are not public.\"}"}]}
{"paper_id": "mvGa1ikBD3", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nSimulating the behavior of nonlinear and anisotropic materials is a central problem with applications across engineering, computer graphics, robotics, and beyond. While conventional mesh-based simulations provide accurate and reliable predictions, their computational overhead typically prevents their use in interactive applications. Graph neural networks (GNN) have recently emerged as a compelling alternative to conventional simulations for time-critical applications.  However, existing GNN-based methods cannot distinguish between deformations in different directions and are thus limited to isotropic materials. To address this limitation, we propose a novel and easy-to-implement GNN architecture based on directional encodings of edge features. By preserving directional information during message passing, our method has access to the full state of deformation and can thus model anisotropic materials. We demonstrate through a set of qualitative and quantitative evaluations that our approach outperforms existing mesh-based GNN approaches for modeling anisotropic materials.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"The paper addresses a useful open question within the emerging topic of learned simulators, equipping message-passing networks with anisotropic elasticity.  The solution is simple but effective, and experimental results show a meaningful improvement over the baseline method.  Overall, the writing is clear, and experiments seem reproducible.\", \"Weaknesses\": \"The technical contribution seems potentially incremental from prior work (MeshGraphNets) -- however, this is not necessarily an issue, as the experiments are well-designed, the results are solid and finding are conclusive.\\n\\nI would be interested to see additional experiments, beyond the cantilever (and cantilever-like) setup, such as simulation with collision/contact.  This is not a requirement though, as the existing experiments are quite informative.\", \"Questions\": \"- I am curious about the \\\"material space bases\\\" mentioned on page 4.  Could you elaborate on how the \\\"material space bases\\\" are defined exactly?  How are these defined relative to global coordinates, and are they defined in a canonical way?\\n\\n- Would defining the local bases in a different way change the three axis-aligned weights/coefficients that are computed during preprocessing?  In turn, would this make it difficult to learn?  (Presumably the x-axis is aligned with the edge, what about the others.). Thanks, look forward to your reply.\"}"}]}
{"paper_id": "bQWE2UqXmf", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nWe find that large language models (LLMs) are more likely to modify human-written text than AI-generated text when tasked with rewriting. This tendency arises because LLMs often perceive AI-generated text as high-quality, leading to fewer modifications. We introduce a method to detect AI-generated content by prompting LLMs to rewrite text and calculating the editing distance of the output. We dubbed our geneRative AI Detection viA Rewriting method Raidar.  Raidar significantly improves the F1 detection scores of existing AI content detection models -- both academic and commercial -- across various domains, including News, creative writing, student essays, code, Yelp reviews, and arXiv papers, with gains of up to 29 points. Operating solely on word symbols without high-dimensional features, our method is compatible with black box LLMs, and is inherently robust on new content. Our results illustrate the unique imprint of machine-generated text through the lens of the machines themselves.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"The paper discusses work on an interesting framing of detecting text generated by large language models by gauging how much rewrites (edit distance) it gives to a piece of text with the hypothesis that it considers machine-generated texts, regardless of specific models, as high quality ones compared to human-generated texts. The simplicity of the method is what I appreciate the most with the paper. I also appreciate that the author/s are able to construct a good research plan for the methodology of the study and the formalization of the task. There are some revisions suggested below to improve the structure and organization of the paper, particularly the uniformity and presentation of results.\", \"Weaknesses\": \"The paper starts well with the introduction, statement of motivation, discussion of hypothesis, and proposed method but went downhill with the presentation of results.\\n\\nFirst, in the discussion of datasets and model baseline used, the definition and characteristics should definitely be expanded by including the size of the dataset plus other unique characteristics of the data that\u2019s worth mentioning as well as the quantified performance of the baseline models just to give an idea to the readers of their relevance and why they are being used/compared in the study. \\n\\nSecond, the discussion on Section 4 seems to be too condensed and there are sudden insertions of models for comparison. Claude was tested but was not mentioned in the baseline models. Why is that and why is Claude a viable candidate for comparison? \\n\\nThird, there is no concrete / definitive discussion for the results presented in Table 1 and 2 and these are the bread and butter of the study. The table captions and Section 4.3 simply state the observed increments against the previous systems and nothing more. I would have appreciated it if there are more discussions on particular weaknesses in the texts by previous works which the current work addresses, and give some examples of this. Why are proposed methods get big performance jumps, say in the code dataset and Arxiv dataset, compared to natural language datasets? In the robustness experiments, why are the previous detector models left out? It is important to still consider them in the mix for better comparison of true performance. These discussions should be in order.\\n\\nThe statement \u201ctext from the Ada model is most easily detected, possibly due to its discernibly lower quality compared to human-written content\u201d is speculation. Some form of quantitative proof is required for this.\\n\\nIn terms of strengthening the paper, I strongly suggest the author/s to also experiment with second-language or non-native speaker writing corpora such as TOEFL or ESL essay corpus to gauge if the proposed method also exhibits that particular limitation of confusing AI-generated texts with non-native speaker-written texts. This has been a known limitation across all detectors of machine-generated texts. Refer to the existing works (https://arxiv.org/abs/2304.02819) and databases online (https://www.clarin.eu/resource-families/L2-corpora, https://uclouvain.be/en/research-institutes/ilc/cecl/learner-corpora-around-the-world.html) for the data to be experimented with. Regardless of the result, this would be a great addition and insight to the paper.\\n\\nLastly, the paper is littered with inconsistencies and wrong format for in-text citations which makes it very confusing to read. For example, it\u2019s better to read referenced text as \u201cGhostbuster (Verma et al., 2023) instead of Ghostbuster Verman et al (2023)\u201d. It seems like the Verma is included in the name of the model but it\u2019s not. There are also cases where the model is \u201cGPT-3.5-Trubo\u201d and then becomes \u201cGPT-3.5-turbo\u201d. Diagrams in Figure 6 should conform to uniform range of [0.0,1.0] in the y axis.\\n\\nI still see the importance and contribution of the study but major revision is required for the paper in order for readers to fully comprehend and appreciate the work.\", \"Questions\": \"1. How large is the feature set or dimension then for training the classifiers? Did the author/s conduct and hyperparameter optimization with the classifiers?\\n\\n2. How is the setup for comparison done for preliminary experiments shown in Table 1 and 2? What classifier was used for these experiments? There were two algorithms mentioned (LR and XGBoost) but it was never specified in the tables. The same way goes for Tables 4 and 5. I\u2019m confused as a reader as to which classifier was used for the presentation of results. Revising this part and enriching the details should be in order.\\n\\n3. Is the number of changes (deletions, insertions) done by the selective generative models between humans and machine-generated texts statistically significant? The visualization in Figure is nice but this would have been an additional supporting preliminary information to add to the hypothesis.\\n\\n4. There are six datasets chosen for experimentations but why are only subsets or some of it are used for results presented in Table 3, 4, and 5?\\n\\n5. The statement in Section 4.4 goes \u201cIn Table 5, while all models achieve significant detection performance\u201d. So a statistical test was conducted? Can you give the details for this? Otherwise, I suggest using the word substantial instead.\\n\\n6. Why is only the Yelp data used for special test cases such as effect of content length? Why not the other datasets as well? This seems very selective without reason.\"}"}]}
{"paper_id": "9IUZya8bCN", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nDifferent from multi-class classification where each testing input only has a single ground truth label, multi-label classification aims to make predictions for testing inputs with multiple ground-truth labels. Multi-label classification has many real-world applications such as disease detection, object recognition, document classification, just to name a few. Recent studies, however, showed that a multi-label classifier is vulnerable to data-poisoning attacks, where an attacker can poison the training dataset of the multi-label classifier such that the classifier makes predictions as the attacker desires. Existing provable defenses are all designed for multi-class classification and they achieve sub-optimal results when applying their robustness guarantees to multi-label classification (as we will demonstrate in this paper). In this work, we propose PoisoningGuard, the first provable defense against data-poisoning attacks to multi-label classification. In particular, we generalize two state-of-the-art multi-class certification methods, namely bagging and Deep Partition Aggregation (DPA), to multi-label classification. Our major technical contribution is to jointly consider multiple labels when deriving the provable robustness guarantees. We perform comprehensive evaluations on three datasets. Our experimental results show that our generalized methods significantly outperform bagging and DPA when applying them to multi-label classification. The code will be released.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 5, \"Strengths\": \"Overall the quality of this paper is good.\\n1. Developing provable defenses against data poisoning attacks is an important topic.\\n2. The multi-label classification angle is interesting.\\n3. While I did not check the proofs line by line, I am familiar with these provable defenses that I think these results are reasonable.\", \"Weaknesses\": \"1.My major concern is in fact **a very specific mismatch** between the proposed defense and the setting of multi-label classifications.\\nSpecifically, the proposed defense **always predict k labels for a pre-defined constant k**, which is unnatural in multi-label classification. In typical multi-label classification, one does not really assume every sample must have exactly the same number of labels. For this reason, I am concerned **whether this is a reasonably good start for provable defenses in multi-label classification**. \\n\\nIn my opinion, the current formulation is more related to certifying top-k accuracy rather than multi-label classification. However, it is relatively trivial (I will still consider this as part of this work's contribution though) to generalize bagging/DPA for b to top-k accuracy since it is somewhat straightforward (at least for DPA) to allow each base classifiers to contribute multiple votes.\\n\\n2.There are some closely related but missing references [1-6 below]. [1-5] are closely related to provable defenses against poisoning attacks and [6] is about provable defenses for top-k predictions but for test-time attacks. Notably, the certificates of [4] essentially offer a solution for certifying top-2 predictions. Note that some of these references are in fact fairly new, thus while I suggest authors to discuss them, it is understandable that they are missing from the submitted version.\\n\\n\\n[1] Chen R, Li Z, Li J, Yan J, Wu C. On Collective Robustness of Bagging Against Data Poisoning. \\n\\n[2] Wang W, Levine A, Feizi S. Lethal Dose Conjecture on Data Poisoning. \\n\\n[3] Wang W, Feizi S. Temporal Robustness against Data Poisoning.\\n\\n[4] Rezaei K, Banihashem K, Chegini A, Feizi S. Run-Off Election: Improved Provable Defense against Data Poisoning Attacks. \\n\\n[5] Wang, W. and Feizi, S. On Practical Aspects of Aggregation Defenses against Data Poisoning Attacks. \\n\\n[6] Jia, J., Wang, B., Cao, X., Liu, H. and Gong, N.Z. Almost tight l0-norm certified robustness of top-k predictions against adversarial perturbations.\", \"Questions\": \"Please refer to Weakness. \\n\\n1. The major concern is about Weakness 1.\\nPlease clarify if you think there is anything missing.\\nIf you also find this to be reasonable, a direct fix from me will be to use either a fixed threshold or an adaptive one to decide whether each label is predicted or not. For example, instead of predicting a constant of k labels per sample, the ensemble can predict all classes with more than t votes for some constant t.\\nThis can actually simplify the computation of certificates and potentially make it much more suitable for the setting of multi-label classifications. However, this leads to critical changes to the proposed approach so revising&resubmiting to future venues may also be a good option.\"}"}]}
{"paper_id": "izdFGwDgvW", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nRecent advances in vision-language models have combined contrastive approaches with generative methods to achieve state-of-the-art (SOTA) on downstream inference tasks like zero-shot image classification. \nHowever, one persistent issue of these models for image classification is their out-of-distribution (OOD) generalization capabilities.\nWe first show that when an OOD datapoint is misclassified, the correct class can be typically found in the Top-$K$ predicted classes.\nIn order to steer the model prediction toward the correct class within the top predicted classes, we propose the Image-Caption Encoding (ICE) method, a straightforward approach that directly enforces consistency between the image-conditioned and caption-conditioned predictions at evaluation time only.\nIntuitively, we take advantage of unique properties of the generated captions to guide our local search for the correct class label within the Top-$K$ predicted classes.\nWe show that our method can be easily combined with other SOTA methods to enhance Top-1 OOD accuracies by 0.5% on average and up to 3% on challenging datasets.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"- The paper is well written and the organization of the paper is clear.\\n- Experiments are comprehensive with multiple downstream datasets under the zero-shot and few-shot setting.\", \"Weaknesses\": \"- It remains unclear why the text decoder from CoCa is used. It seems that the proposed method only requires a textual description of the input image (any off-the-self high-quality caption models may work). It would be interesting to compare with the performance when using other caption models.\\n\\n- Compared to CLIP, the approach requires an additional caption model (as the image decoder) and is dependent on the quality of the caption, which can be hard to measure. \\n\\n- Empirical improvement in the few-shot setting seems marginal compared to the baselines. It may be arguable whether the additional computational cost (of forwarding passing the image to the text decoder) is desirable.\", \"Questions\": \"- Can authors explain why CoCa is used, instead of an arbitrary caption model? If we replace CoCa with SoTA caption model, will the performance be significantly improved? \\n\\n- The performance gain in the few-shot setting seems marginal (Table 2). Does this mean that in the few-shot adaptation setting, we may just need a few images, instead of using an additional decoder?\"}"}]}
{"paper_id": "6vtGG0WMne", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nDeep learning models implemented in real-world applications still face challenges from imbalanced data. Existing methods address the imbalance problem by balancing the models between the minority class and the majority class. However, practical applications may require an imbalanced optimization strategy that selectively unbalances the models and makes them more suitable for the applications than the balanced models. In this work, we first give a formal definition to accurately quantify the degree of imbalance of a model. Then, we propose a bias adjustment method that can efficiently optimize the model to a specified imbalance state according to application metrics or requirements so that this method has wide applicability. Finally, we introduce a training strategy that is advantageous to select the optimal representation parameters of the model during traditional training process. Extensive experiments verify the effectiveness and efficiency of our method, and compared with state-of-the-art algorithms, our method has significant improvement in different metrics including accuracy, F1 value and G-means.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 4, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"The proposed method is straightforward. Evaluations are however, fairly extensive, and are done across a variety of data types - tabular, image and text. A per-measure comparison is also provided. This kind of papers are persuasive with broad-based evaluations, usually on even more datasets.\", \"Weaknesses\": \"The hypothesis of just the bias on the softmax layer meant for classification making the classification swing by these amounts us astounding. In fact, a study on the values of the bias versus predictive performance is required in my opinion before bias adjustment is presented.\", \"Questions\": \"At 500:1 imbalance ratios, multiple benchmark methods show numbers decimated to zero. A previous study, MMM, by Mirza et al. '21 appears to suggest that even at such a ratio, classical resampling and even baselines report an above zero performance. Could you reason about the disparity?\"}"}]}
{"paper_id": "lf8QQ2KMgv", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nMemorization is the ability of deep models to associate training data with seemingly random labels. Even though memorization may not align with models\u2019 ability to generalize, recent work by Feldman and Zhang (2020) has demonstrated that memorization is in fact necessary for generalization. However, upon closer inspection of this work, we uncover several methodological errors including lack of model convergence, data leakage, and sub- population shift. We show that these errors led to a significant overestimation of memorization\u2019s impact on test accuracy (by over five times). After accounting for these errors, we demonstrate that memorization does not impact prediction accuracy as previously reported, and therefore, it is not necessary for generalization. In light of these findings, future researchers are encouraged to design better techniques to identify memorized points that can avoid some of the earlier stated problems.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"Despite there being ample room for further improvement of the paper, there are interesting visualizations of memorized and sub-population shifts in the paper.\", \"Weaknesses\": \"In general, the presented study appears to be inadequately explored, leaving more room for further empirical research and justification of observations based on theoretical foundations. The stated issues do not necessarily contradict the findings of the previous work, especially in the case of the \\\"Sub-population Shift\\\" argument, which I will explain further in my review. Therefore, this paper may not be suitable for this particular venue and might be better suited as a workshop paper or considered for the reproducibility track to facilitate further experimentation and validation of the results.\\n\\nI am strongly leaning towards rejecting the paper; however, I will try to provide my understanding, concerns, and suggestions for the authors so they can reevaluate their work and make a better presentation of their study.\\n\\nConcerns, understanding, and suggestions: \\n1. The introduction section does not adequately define the problem, and the background section has not provided sufficient background information.\\n2. Regarding errors stated in the paper:\\n    1. \u201cLack of Convergence: Models\u201d: Why should models be trained to their maximum test set accuracy? You don't have access to the test set during training!\\n    2. \u201cData Leakage: Training\u201d: The authors of the original work stated this in their paper in Figure 3 that there is a close match of memorized points in the test set.\\n    3. \\\"Sub-population Shift\\\": The argument does not necessarily contradict the previous work because it aligns with what they have been arguing in their paper: memorization is necessary due to the long-tail distribution. Both mislabeled and sub-population data points are part of the long tail; therefore. It does not necessarily mean that the model memorized the points associated with sub-populations. They have high influence scores because they are rare in the data distribution, and the model exhibits a similar behavior of memorization.\\n3. There is an argument in the paper that the definition of memorization from the original work is incorrect, but there is no strong justification for this claim, and it is deferred to future research. I would suggest the authors first review their observations and results and then use them to formulate a more robust definition of memorization. In that case, your work would be better supported.\\n4. There are many repetitions of information with different formats conveying the same content in the paper, such as in the Introduction, Background, Related work, Discussion, and Conclusion. The presentation format could be significantly improved.\\n5. The \\\"Data Leakage\\\" in the Background section is not sufficiently well explained. I have read the \\\"Do CIFAR-10 classifiers generalize to CIFAR-10?\\\" paper, and the context provided in your paper is what they have already explained, and it can be moved to the appendix.\\n6. Table 1: It is not necessary to provide a table for symbols in the main pages; it can be moved to the appendix.\\n7. A large portion of the paper is dedicated to understanding the original paper, and it is defined in a way that favors the introduced issues mentioned at the beginning of your paper. It can be shortened, and the detailed discussion can be left in the appendix.\\n8. Section 5.1 (SETUP): You should check what your models memorize with the modifications you have made, not just the 1015 memorized points provided by the original authors.\\n9. The implementation of fixes is not provided along with the submission.\", \"Questions\": \"Questions: \\n1. The previous work that you studied is based on a theoretical background. Could you please delve into the details of the theory part and justify your observation? I believe that if your argument is completely true, it should also be possible to find a flaw in the theory of the work.\\n2. Section 5.1 (SETUP): How did you define which points impact the same test point?\\n3. Section 4.2 (ERROR: DATASET LEAKAGE): Are you assuming that duplicate points are only found in memorized points?\"}"}]}
{"paper_id": "zFWKKYz2yn", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nThis paper analyzes two competing rule extraction methodologies: quantization and equivalence query. We trained $3600$ RNN models, extracting $18000$ DFA (Deterministic Finite Automata) with a quantization approach (k-means and SOM) and $3600$ DFA by equivalence query($L^{*}$) methods across $10$ initialization seeds. We sampled the datasets from  $7$ Tomita and $4$ Dyck grammars and trained them on $4$ RNN cells: LSTM, GRU, O2RNN, and MIRNN. The observations from our experiments establish the superior performance of O2RNN and quantization-based rule extraction over others. $L^{*}$, primarily proposed for regular grammars, performs similarly to quantization methods for Tomita languages when neural networks are trained completely. However, for partially trained RNNs, $L^{*}$ shows instability in the number of states in DFA, e.g., for Tomita 5 and Tomita 6 languages, $L^{*}$ produced more than $100$ states. In contrast, quantization methods result in rules with the number of states very close to ground truth DFA. Among RNN cells, O2RNN produces stable DFA consistently compared to other cells. For Dyck Languages, we observe that although GRU outperforms other RNNs in network performance, the DFA extracted by O2RNN has higher performance and better stability. The stability is computed as the standard deviation of accuracy on test sets on networks trained across $10$ seeds. On Dyck Languages, quantization methods outperformed $L^{*}$ with better stability in accuracy and the number of states. $L^{*}$ often showed instability in accuracy in the order of  $16\\% - 22\\%$ for GRU and MIRNN while deviation for quantization methods varied in $5\\% - 15\\%$. In many instances with LSTM and GRU,  DFA's extracted by $L^{*}$ even failed to beat chance accuracy ($50\\%$), while those extracted by quantization method had standard deviation in the $7\\%-17\\%$ range. For O2RNN, both rule extraction methods had a deviation in the $0.5\\% - 3\\%$ range.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 4, \"Contribution\": 4, \"Rating\": 5, \"Confidence\": 1, \"Strengths\": \"The paper is well presented, by providing substantial background information and experiments.\", \"Weaknesses\": \"See questions.\", \"Questions\": \"Honestly, my familiarity with current RNN research, including rule extraction strategies, is limited. Consequently, I find it inappropriate to assess the topic, method, or the novelty of the work. However, I do have several questions for discussion.\\n\\n- It would enhance the paper if the authors could furnish additional insights or intuition regarding the significance of exploring the stability of rule extraction strategies. Alternatively, a clearer definition of stability in this context would also be helpful. In the current work, stability appears to be synonymous with the variance of performance.\\n\\n- A more thorough justification for the conclusion would be beneficial. Instead of solely benchmarking several methods across various settings, delving into the rationale behind the conclusions drawn would strengthen the paper and provide a more comprehensive perspective.\"}"}]}
{"paper_id": "y3dqBDnPay", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nSelf-supervised representation learning on multimodal data plays a pivotal role in proficiently integrating and embedding information from various sources without the need for additional labeling. Notably, the majority of existing methods overlook the complex high-order inter- and intra-modality correlations characteristic of real-world multimodal data. In this paper, we introduce HyperRep, which combines the strength of hypergraph-based modeling with a self-supervised multimodal fusion information bottleneck principle. The former captures high-order correlations using hypergraphs to represent inter- and intra-modality relations, while the latter constrains the solution space, ensuring a more effective fusion of multimodal data. Our extensive experiments on four public datasets for three downstream tasks demonstrate HyperRep's superiority, as it consistently delivers competitive results against state-of-the-art methods.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. This paper provides sufficient and detailed formal definitions and necessary proofs.\\n2. The application of hypergraphs in this paper may inspire the field of multimodal self-supervised learning.\\n3. The authors conducted extensive ablation experiments on multiple datasets.\", \"Weaknesses\": \"1. Motivation: The paper highlights the use of hypergraphs to capture higher-order correlations among modalities. However, I find the paper insufficient in explaining and analyzing why higher-order correlations are essential for multimodal self-supervised learning. This is especially relevant since most existing methods, such as the CLIP[1] series and ImageBind[2], rely on pair-wise multimodal self-supervision.\\n2. Method: The paper could improve the clarity and presentation of MFB. For instance, how does Eq. 12 relate the mutual information of shared and instance features? A clearer explanation of the illustration would be helpful. Also, the MFB module in Figure 2 is vague, and a more specific illustration could enhance the readers\u2019 comprehension of the method.\\n3. Experiment: The paper lacks comparisons with recent (2023) methods, as the only one mentioned has a huge performance gap with the classical methods. Could the paper also compare with more popular Foundation models, such as VideoCLIP[3] and OmniVL[4]? Moreover, Table 1 shows that graph-based methods perform poorly, and Table 2 reveals a drastic performance drop after removing higher-order correlations. However, AGC achieves high performance without using higher-order correlations. Does this imply that higher-order correlations are crucial for the proposed method, but not for the multimodal self-supervised learning task? \\n\\n[1] Learning Transferable Visual Models From Natural Language Supervision. ICML 2021.\\n[2] IMAGEBIND: One Embedding Space To Bind Them All. ICCV 2023.\\n[3] VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding. EMNLP 2021.\\n[4] OmniVL: One Foundation Model for Image-Language and Video-Language Tasks. NeurIPS 2022.\", \"Questions\": \"How come the MIL-NCE performance in Table II differs so much from the original paper? Are there any differences in the experimental settings?\"}"}]}
{"paper_id": "H6pf70GZVU", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nIncremental learning (IL) usually addresses catastrophic forgetting of old tasks when learning new tasks by replaying old tasks' data stored in a memory, which can be limited by its size and the risk of privacy leakage. Recent non-exemplar IL methods only store class centroids as prototypes and perturb them with Gaussian noise to create synthetic data for replay. However, the class prototypes learned in different tasks might be close to each other, leading to the intersection of their synthetic data and forgetting. Moreover, the Gaussian perturbation does not preserve the real data distribution and thus can be detrimental. In this paper, we propose YoooP, a novel exemplar-free IL approach that can greatly outperform previous methods by only storing and replaying one prototype per class even without synthetic data replay. Instead of storing class centroids, YoooP optimizes each class prototype by (1) moving it to the high-density region within every class using an attentional mean-shift algorithm; and (2) minimizing its similarity to other classes' samples and meanwhile maximizing its similarity to samples from its class, resulting in compact classes distant from each other in the representation space. Moreover, we extend YoooP to YoooP+ by synthesizing replay data preserving the angular distribution between each class prototype and the class's real data in history, which cannot be obtained by Gaussian perturbation. YoooP+ effectively stabilizes and further improves YoooP without storing any real data. Extensive experiments demonstrate the superiority of YoooP/YoooP+ over non-exemplar baselines in terms of accuracy and anti-forgetting.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"This paper proposes a novel approach to leave the class representative prototype considering the similarity with all real samples. The proposed method aims to be an efficient technique that can be easily applied to the existing prototype-based method.\\n\\nThis paper also proposes a data augmentation performed with the help of a rotation matrix. This strategy approximates real distribution better than conventional prototype augmentation techniques.\\n\\nIn base-0 experiments on CIFAR-100 and Tiny-ImageNet, the proposed methods achieve the best performance.\", \"Weaknesses\": \"-\\tThe scale of dataset (CIFAR-100, TinyImageNet) is small. Whether the proposed method works with bigger datasets is an important issue that needs to be addressed. \\n-\\tThe paper lacks the result with 20 phases which is a quite common configuration. \\n-\\tThe effect of an optimized prototype based on the attentional mean-shift method seems limited in many situations.\", \"Questions\": \"1) For verifying the strength of proposed augmentation, why don\u2019t you apply the new strategy to PASS instead of adding Gaussian noise to prototype? Would you show the performance of it?\\n\\n2) Could you show the classification confusion matrix result of your approach?\\n\\n3) In section 3, when define the probabilities over all classes C_(1:t ), isn\u2019t softmax(wG(F(x;\u03b8))) more accurate notation than softmax(wF(x;\u03b8))?\\n\\n4) The function c(,) in equation 2, equation 4 seems to be different. Is the c(,) in equation 2 cosine similarity function and one in equation 4 classifier (=G(F(p_k;\u03b8)))?\\n\\n5) How can we define the initial class-wise prototype p_k? Is it the same with the prototype used in PASS? (k-th class mean prototype)?\"}"}]}
{"paper_id": "7VPTUWkiDQ", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nLearning representations that generalize to novel compositions of known concepts is crucial for bridging the gap between human and machine perception. One prominent effort is learning object-centric representations, which are widely conjectured to enable compositional generalization. Yet, it remains unclear when this conjecture will be true, as a principled theoretical or empirical understanding of compositional generalization is lacking. In this work, we investigate when compositional generalization is guaranteed for object-centric representations through the lens of identifiability theory. We show that autoencoders that satisfy structural assumptions on the decoder and enforce encoder-decoder consistency will learn object-centric representations that provably generalize compositionally. We validate our theoretical result and highlight the practical relevance of our assumptions through experiments on synthetic image data.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"- The paper is very well-written.\\n- The theory is sound and significant for the community.\\n- The joint encoder-decoder framework for compositional generalization in autoencoders is quite elegant.\\n- The limitations of the framework and the additivity constraint on the decoder are adequately stated.\", \"Weaknesses\": \"Although they support the theory, the experiments are quite limited. For instance, these are all with only two slots with 16 dimensions each. See the questions section for additional information that would be interesting to see from experimentation.\", \"Questions\": \"- How does the effect of the consistency loss scale with the number of slots?\\n- What is the impact of how slot-supported the training data is? i.e. in Figure 2 (1), what is the impact of the width of the blue band on empirical effectivity?\\n- How does the method hold up on non-synthetic data, especially if you slightly relax some constraints? For instance, what if you have expressive slot-wise decoding, but allow for a low-expressivity non-linear combination at the end for rendering?\"}"}]}
{"paper_id": "5JWAOLBxwp", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nThe usage of 3D vision algorithms, such as shape reconstruction, remains limited because they require inputs to be at a fixed canonical rotation. Recently, a simple equivariant network, Vector Neuron (VN) has been proposed that can be easily used with the state-of-the-art 3D neural network (NN) architectures. However, its performance is limited because it is designed to use only three-dimensional features, which is insufficient to capture the details present in 3D data. In this paper, we introduce an equivariant feature representation for mapping a 3D point to a high-dimensional feature space. Our feature can discern multiple frequencies present in 3D data, which, as shown by Tancik et al. (2020), is the key to designing an expressive feature for 3D vision tasks. Our representation can be used as an input to VNs, and the results demonstrate that with our feature representation, VN captures more details, overcoming the limitation raised in its original paper.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 2, \"Strengths\": \"*Originality:* The proposed approach appears novel. Although other approaches have used a Fourier like representation to represent 3D objects [1,2,3], this approach differs from those in that 1) the representation is intentionally high-dimensional, 2) only a subspace of the high-dimensional space is considered.\\n\\n*Quality:* In the experiments, proposed method performs on-par with or better than most baselines.\\n\\n*Clarity:* (see weakness)\\n\\n*Significance:* Designing expressive equivariant representations is a challenging and important problem in the field.\\n\\n\\n[1] Esteves, Carlos, et al. \\\"Learning so (3) equivariant representations with spherical cnns.\\\" Proceedings of the European Conference on Computer Vision (ECCV). 2018.\\n\\n[2] Cohen, Taco S., et al. \\\"Spherical cnns.\\\" arXiv preprint arXiv:1801.10130 (2018).\\n\\n[3] Kondor, Risi, Zhen Lin, and Shubhendu Trivedi. \\\"Clebsch\u2013gordan nets: a fully fourier space spherical convolutional neural network.\\\" Advances in Neural Information Processing Systems 31 (2018).\", \"Weaknesses\": \"*Quality:* \\n* The authors present a very critical view of methods that use spherical harmonics which I think is not only inappropriate but also incorrect. Spherical harmonics can be seen analogous to the Fourier basis which have been applied in signal processing problems for centuries, and the authors rely on for intuition building. Also, the authors state that spherical harmonics come from quantum mechanics, but the use of spherical harmonics predates the \\\"discovery\\\" of quantum mechanics by more than a century. \\n* There does not appear to be any discussion of the impact of choosing a higher dimensional input representation space. How does the choice of $n$ impact performance? What is the additional computational cost?\\n* The notation is inconsistent (see questions)\\n* Some proofs are missing (see questions)\\n\\n*Clarity:* I found the presentation to be difficult to follow. The paper uses advanced math concepts such as the Lie bracket without motivating or even naming them.\", \"Questions\": \"*Questions:*\\n- What is meant by \u201c$R^z(\\\\hat{u})$ is a rotation matrix defining the orientation measured from the $z$-axis to $\\\\hat{u}$.\u201d Is it the 2D rotation about the axis $z \\\\times \\\\hat{u}$ that aligns the $z$-axis and $\\\\hat{u}$?\\n- It seems like the map is from SO(3) to a subspace of SO(n) is that right\\n- Is the axis arbitrarily selected?\\n- Createsearchspace from Algo 2 is not described in the text, how does this work?\\n\\n*Possible typos:*\\n- \u201cTheorem 1. theoremIf\u201d \u2192 \u201cTheorem 1. If\u201d \\n- Are $\\\\hat{u}$ and $\\\\overrightarrow{u}$ used interchangeably in section 3? It would be clearer if the notation were consistent\\n- \u201cintuitively, just like F1, F2 and F3 represent angles\u201d \u2192 \u201cintuitively, just like F1, F2 and F3 represent axes of rotation\u201d\\n- Should \u201caxes \u03c8(\u02c6x), \u03c8(\u02c6y), and \u03c8(\u02c6z)\u201d be \u201caxes \u03c8(F_1), \u03c8(F_2), and \u03c8(F_3)\u201d? If not what does $\\\\hat{.}$ mean here?\\n- \u201c effective-yet-rotation-equivariant\u201d \u2192  expressive yet rotation equivariant\\n- In section D, readers are referred to the proof of Thm 1 for the proof of Prop 1, but the proof of Thm 1 does not prove Prop 1.\"}"}]}
{"paper_id": "l9GaXJnMJ8", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nWe introduce a generalization of the Wasserstein metric, originally designed for probability measures, to establish a novel distance between probability kernels of Markov systems. We illustrate how this kernel metric may serve as the foundation for an efficient approximation technique, enabling the replacement of the original system's kernel with a kernel with a discrete support of limited cardinality.\nTo facilitate practical implementation, we present a specialized dual algorithm capable of constructing these approximate kernels quickly and efficiently, without requiring computationally expensive matrix operations. Finally, we demonstrate the effectiveness of our method through several illustrative examples, showcasing its utility in diverse practical scenarios, including dynamic risk estimation. This advancement offers new possibilities for the streamlined analysis and manipulation of Markov systems represented by kernels.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"1. The problem in this paper is interesting.\", \"Weaknesses\": \"1. The paper is generally poorly written and contains numerous typos in both the explanations and equations.\\n2. The experimental section could benefit from additional elaboration and expansion.\", \"Questions\": \"1. The definition 2.1 is very confusing. More specifically, it explains $Q(\\\\dot | x)$ is a kernel that transforms $x$ to the probability measure. It seems $Q(\\\\dot | x)$ is a probability density function estimated and it does not look like a kernel functions. Could you please elaborate this part?\\n2. Equation (9) presents a variation of the Wasserstein barycenter problem. However, the manuscript lacks a comprehensive discussion of the Wasserstein barycenter, and it would be beneficial to explicitly address this concept. Additionally, a comparison of the proposed method with relevant baselines is warranted.\\n3. Numerous typographical errors are present in the manuscript. For instance, on page 1, there is a repeated \\\"the the.\\\" Equation (6) is lacking the variable $p,\\\" and these issues need prompt correction for the manuscript to maintain clarity and accuracy.\"}"}]}
{"paper_id": "i4kDKfllrz", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nDeep learners tend to perform well when trained under the closed set assumption but struggle when deployed under open set conditions. This motivates the field of Open Set Recognition in which we seek to give deep learners the ability to recognize whether a data sample belongs to the known classes trained on or comes from the surrounding infinite world. Existing open set recognition methods typically rely upon a single function for the dual task of distinguishing between knowns and unknowns as well as making fine known class distinction. This dual process leaves performance on the table as the function is not specialized for either task. In this work, we introduce Synergistic Classification and unknown Discrimination (SCAD), where we instead learn specialized functions for both known/unknown discrimination and fine class distinction amongst the world of knowns. Our experiments and analysis demonstrate that SCAD handily outperforms modern methods in open set recognition when compared using AUROC scores and correct classification rate at various true positive rates.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"The task of open set recognition is of interest to the field.\", \"Weaknesses\": \"1. The presentation of the paper can be confusing sometime. E.g. in Fig 2., please explain in the caption the 'know knowns' and 'known unknowns', which are very confusing. This makes understand the core idea inaccessible.\\n2. Large-scale experiments like original Imagenet and iNaturalist is needed. Current experiments are too small-scale to verify the proposal.\\n3. some ablation studies are missing. E.g. \\\\beta in the Eqn. 5.\", \"Questions\": \"1. Fig.1b hypothesize that the embedding space should be distinctly separable between the known classes and unknown classes. However, in Fig 4, I did not see distinc separation between the two. I also do not think the hypothesis holds in general. Can authors expalin.\"}"}]}
{"paper_id": "ISrxxvXJQO", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nIn this paper, we introduce an intriguing phenomenon \u2013 the successful reconstruction of images using a set of one-way wave equations with hidden and learnable speeds. Each individual image corresponds to a solution with a unique initial condition, which can be computed from the original image using a visual encoder (e.g., a convolutional neural network). Furthermore, the solution for each image exhibits two noteworthy mathematical properties: (a) it can be decomposed into a collection of special solutions of the same one-way wave equations that are first-order autoregressive, with shared coefficient matrices for autoregression, and (b) the product of these coefficient matrices forms a diagonal matrix with the speeds of the wave equations as its diagonal elements. We term this phenomenon *hidden waves*, as it reveals that, although the speeds of the set of wave equations and autoregressive coefficient matrices are latent, they are both learnable and shared across images. This represents a mathematical invariance across images, providing a new mathematical perspective to understand images.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"- Through mathematical enhancements and experiments, they develop a deep understanding of FINOLA, which is a general wave representation of an image.\\n- The section comparing \\\"hidden wave representation\\\" with image autoregression or transformations is well-written and provides clear insights into this encoding.\\n- They conducted numerous experiments to validate the improvements they implemented.\", \"Weaknesses\": \"- Experimental design\\n\\nDrawing conclusions from the current results is uncertain due to the limited number of data points and the lack of significant differences. For instance, the interpretation of Table 3 remains unclear. Should we consider choosing initial condition positions different from the center? Overall, experiments were conducted from various perspectives, they do not yield meaningful insights or conclusions.\\n\\n- The phenomenon is undoubtedly intriguing and interesting, it remains unclear how this method can be applied to future research or practical applications. The paper would be improved by including discussions on potential future perspectives.\", \"Questions\": \"- Were all images resized to the same aspect ratio?  The aspect ratio can impact wave speed, and it may be more natural to preserve the aspect ratio from the original images.\\n- Conventional image compression methods like DCT transform the image into a linear composition of orthogonal basis. This is simple and provides a straightforward understanding of each coefficient. The paper effectively describes the differences, but what are the main strengths and weaknesses of this method?\\n- Is the selection of hyperparameters for training this method not very sensitive? How long does the training process take?\\n\\nMinor comments:\\n- Use ` for opening quotation mark in latex. Currently ' ... '.\\n- There are some missing brackets for citations. For example, ... known as FINOLA Chen et al. (2023).\"}"}]}
{"paper_id": "vsW5vJqBuv", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nEmpowering embodied agents, such as robots, with Artificial Intelligence (AI) has become increasingly important in recent years. A major challenge is task open-endedness. In practice, robots often need to perform tasks with novel goals that are multifaceted, dynamic, lack a definitive \"end-state\", and were not encountered during training. To tackle this problem, this paper introduces Diffusion for Open-ended Goals (DOG), a novel framework designed to enable embodied AI to plan and act flexibly and dynamically for open-ended task goals. DOG synergizes the generative prowess of diffusion models with state-of-the-art, training-free guidance techniques to adaptively perform online planning and control. Our evaluations demonstrate that DOG can handle various kinds of novel task goals not seen during training, in both maze navigation and robot control problems. Our work sheds light on enhancing embodied AI's adaptability and competency in tackling open-ended goals.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 2, \"Strengths\": \"Using diffusion models to perform Goal-Conditioned Reinforcement Learning (GCRL) looks like a good idea, although the authors are not the first to put it forward, and are not clear about the novelty they bring.\", \"Weaknesses\": \"- The paper is very unclear about the \\\"training-free\\\" property of the framework, which seems to be the main contribution with respect to Janner et al. (2022) and Liang et al. (2023).\\n\\n- The paper is unclear about the difference between defining an energy function by hand and defining a goal-related reward function by hand.\\n\\n- The experiments about using language-based prompts are not explained at all\\n\\n- The paper is written poorly and often in a non-scientific way with a lot of unnecessary over-statements (see below).\", \"Questions\": \"- could the authors explain the difference between defining energy functions for partial goals, negative goals, sequence goals, hybrid goals as they do in the bottom of page 3 and defining goal-related reward functions that do the same, as often done in GCRL? If they argue that this makes a difference, can they backup this claim with comparative experiments?\\n\\n- \\\"This way of defining a goal is fundamentally different from the conventional way of treating the goal as a variable\\\" -> could the authors explain why it is \\\"fundamentally different\\\"? You could have a goal represented with \\\"Clean = True\\\", this would be using a boolean variable, or \\\"clean level = X\\\", that would be using a scalar variable. Doesn't your energy function (c or g?) compute something similar?\\n\\n- p6: the list of examples should be reordered to match the order of the figure (or vice versa). Besides, the corresponding energy functions should be given. The examples themselves need to be better explained.\\n\\nFor instance \\\"our objective isn\u2019t state-specific but aims at maximizing or minimizing some certain property,\\\" this is a completely unspecific sentence, and an over-statement. Can your system do whatever you ask it to do? What are the limits?\\n\\n- in the related work, the authors connect training-free guidance to (Yu et al., 2023b): \\\"and training-free guidance (Yu et al., 2023b)\\\" But the paragraph where they speak of training-free guidance (p2) does not even mention (Yu et al., 2023b). This point must be explained more clearly, as it seems to be the central contribution of the paper.\\n\\n-could the authors extend the related work and better explain the difference betwen their work and Janner et al. (2022) and Liang et al. (2023)? Putting it before the experimental study would help, in my opinion.\\n\\n- Table 1: why is the authors' method performing worse than competitors in the medium dataset?\\n\\n- could the authors explain how they transformed language-based prompts into a goal-related energy function?\\n\\n\\n- Appendix F, HER is not an executor function, it is a method to leverage failed trajectories in GCRL. I don't understand the authors' description at all, we clearly do not see HER the same way...\\n\\n\\n## Over-statements:\\n\\n- p2: \\\"This work is a trial to approach the ultimate embodied AI that can assist people with diverse tasks such as healthcare, driving, and housework, though further endeavor is, of course, still largely demanded.\\\" So, is this work a successful trial? Clearly, it does not approach this ultimate embodied AI it describes. This is a huge over-statement, and the authors should describe in the introduction the goals they truly manage to achieve.\\n\\n- p5: \\\"By doing so, it expedites the sampling process and ensures adherence to defined objectives, proficiently fulfilling our requirements and presenting a viable resolution to our challenges\\\" -> only politicians write this way! :) Be concrete about the positive sides of your method, and do not be afraid of listing limitations, you are doing science, not selling something\\n\\n- p6: \\\"Fig. 4h exemplifies our algorithm\u2019s prowess\\\" -> sorry, I don't consider this as a prowess.\\n\\n- p7: This holds considerable significance for offline RL: what makes you believe this is so? Without properly backing up this claim with results, I see this as an over-statement.\\n\\n- \\\"Our work introduces a novel way of solving open-ended embodied tasks, which may shed light on cognitive neuroscientific research on understanding human intelligence (Taylor et al., 2016).\\\" -> This is a huge over-statement. Can the authors explain in which way they believe their work can concretely shed light on cognitive neuroscientific research on understanding human intelligence?\\n\\n\\n## Local issues and typos ##\\n\\n- p2: These works are worthy of recognition on the path to embodied AI that can truly tackle open-ended tasks.\\nDo the authors use \\\"task\\\" and \\\"goal\\\" interchangeably?\\n\\n- p2: from somewhere.). -> from somewhere).\\n\\n- Bayesian formula -> Bayes formula\\n\\n- p3, in (1) the energy function is written g, but in the examples below it is written c\\n\\n\\n- \\\"Generative models such as diffusion models for image generation can \u201cdraw\u201d the pixels with any color, whereas a real-world painter can only use the colors available in the palette\\\". -> This makes no sense. How can a generative model draw an image with pixels outside the computer's palette?\\n\\n- \\\"This is different from traditional machine learning tasks, where the focus is on finding a single optimal output for a given input.\\\" -> By traditional, do you mean supervised learning? Reinforcement learning is traditional (rooted in Bellman, 1957) but it corresponds to what you describe in the previous sentence.\\n\\n- \\\"Oftentimes we care about multiple steps rather than a single state, e.g, asking a robot to move in circles. Therefore, tackling\\nopen-ended task goals in embodied AI becomes challenging.\\\" -> this is very poorly written. Ask yourself what is your point and write what your point is, avoid being only allusive.\\n\\n- p3: dimension (Here -> here\\n\\n- discrete actions.) -> actions).\\n\\n- you don't need a \\\"nanny\\\" robot to clean or tidy a house, a anny is to take care about children.\\n\\n- p4: the task is achieve -> is to achieve\\n\\n- p5: concatate -> concatenate\\n\\n- p5: plan executing -> execution\\n\\n- p5: Several optional can be made for the plan executor -> there are several options...\\nBy the way, these options should be described in the main text\\n\\n- p5: Since our executor to multiple-step actions, -> something is missing\\n\\n- p5: to the Appendix: which one?\\n\\n- p5: the generating policy consistent with D4RL: what do you mean? I don't understand.\\n\\n- p6: we present 100 episodes both from the same start point (marked as green star). -> why \\\"both\\\"? It makes no sense.\\n\\n- p7: the the mean\\n\\n- p8: which including sub steps: -> includes\\n\\n- Appendix B, try to fit Table 2 into Appendix B.2\\n\\n- Appendix C, about Mujoco environment: \\\"using a specific algorithm and acculumating 1M samples\\\" -> which  specific algorithm do you mean?\\n\\n- Appendix F: \\n\\n\\\"by , takes\\\" -> something is missing\\n\\nwe adopt the (Ma et al. (2022) as ...\\\" -> something is missing\"}"}]}
{"paper_id": "Qufas3h0mY", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nEnd-to-end backpropagation (BP) is the current standard for training deep neural networks. However, as networks become deeper, BP becomes inefficient for various reasons. This paper introduces a new methodology that decouples BP, transforming a long gradient flow into multiple short ones. This design enables the \\emph{simultaneous} computation of parameter gradients in different layers so as to realize better model parallelism. Thorough experiments are presented to demonstrate the efficiency and effectiveness of our model compared to BP, Early Exit, GPipe, and associated learning (AL), a state-of-the-art methodology for backpropagation decoupling. The experimental code is released for reproducibility at \\url{https://anonymous.4open.science/r/SCPL-802C/}\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 2, \"Strengths\": \"1. The paper provides a thorough explanation, making the proposed method reproducible.\\n2. Excellent visualization techniques are employed to illustrate the algorithms, facilitating understanding.\\n3. Merging local gradient computations with self-supervised learning stands out as a novel approach in this domain.\", \"Weaknesses\": \"1. The description surrounding gradient length appears overly assertive. It might be more accurate to present it as $O(H/N)$, relative to the total layer count $H$ and the number of local losses $N$ employed. This observation can be further corroborated by the authors' implementation details.\\n2. The paper does not delve into the influence of $N$\u2014specifically, the count of local losses used and their respective placements\u2014on the system's overall performance. Investigating this could provide more comprehensive insights.\\n3. Although the paper introduces a synchronized pipeline parallel algorithm, it seems the \\\"bubble\\\" issue persists in the suggested method. For a rounded perspective, it would be beneficial if comparisons concerning accuracy with asynchronous pipeline parallel algorithms (like PipeDream) were provided. This would further highlight SCPL's potential advantages over asynchronous counterparts.\", \"Questions\": \"1. Is there a mistake in in paragraph 1 in Section 2.2, which should be 'the author of AL' ?\\n2. Also see weaknesses part.\"}"}]}
{"paper_id": "PhMrGCMIRL", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nTraining AI models that generalize across tasks and domains has long been among the open problems driving AI research. The emergence of Foundation Models made it easier to obtain expert models for a given task, but the heterogeneity of data that may be encountered at test time often means that any single expert is insufficient. We consider the Fusion of Experts (FoE) problem of fusing outputs of expert models with complementary knowledge of the data distribution and formulate it as an instance of supervised learning. Our method is applicable to both discriminative and generative tasks and leads to significant performance improvements in image and text classification, text summarization, multiple-choice QA, and automatic evaluation of generated text. We also extend our method to the \"frugal\" setting where it is desired to reduce the number of expert model evaluations at test time. Our implementation is publicly available at https://github.com/hwang595/FoE-ICLR2024.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"- The paper tackles a novel and significant issue: optimally leveraging different models for diverse tasks. While foundational models generally perform well across various tasks, they have differing strengths. Thus, an approach to effectively combine these models represents a significant advancement.\\n- The methodology, FrugalFoE, is both technically sound and innovative. It offers a clear problem formulation and further minimizes the need to query all the expert models without sacrificing accuracy.\\n- The experiments are extensive and cover a range of applications, including image classification, summarization, MMLU, and text evaluation. This demonstrates the method\u2019s versatility and effectiveness.\\n- The paper effectively situates its work within the broader context of ensemble learning, mixture of experts, and federated learning, thoughtfully introducing these methodologies and their limitations.\", \"Weaknesses\": \"- A primary limitation is the assumption that data for training the fuser are readily available. The proposed approach requires a labeled dataset to train the fuser network. In the discriminative case, we feed the input example to different individual models, take model outputs as the inputs to the fuser network, and train the fuser to predict the label of the input example. Similarly, in the generative case,  we feed the fuser network with individual model outputs and train the fuser to predict the best model index, which is obtained by feeding the labeled data to individual models and selecting the model that achieves the best performance. This prerequisite may not be realistic in practical scenarios, where there is no or only a few labeled data. If a large labeled dataset is available, it might be more efficient to fine-tune a foundational model or employ few-shot learning. A pivotal aspect of this research should be the generalization capabilities of the trained fuser across different tasks (e.g,. train the fuser on some tasks and test the fuser on unseen tasks), which would significantly enhance the paper's contribution.\\n- The paper lacks an in-depth analysis of the fuser network. Although it is described as a lightweight MLP network, there's no exploration of how different architectures (e.g., simpler networks like linear models or more complex ones like transformers) might impact performance. Additionally, details on the training configurations, such as learning rate, epochs, dataset, and stopping criteria, are missing. The influence of the $K$ parameter in the $K$-NN component of the fuser network is also unclear. What is the rationale for choosing $K=9$?\\n- The experiment sections lack in-depth descriptions (see questions). The paper would benefit from reallocating less critical sections (like the connection between FrugalFoE and the A* algorithm) to the Appendix and expanding on the experimental details.\", \"Questions\": \"- Page 8, Table 2: Could you clarify the distinction between \\\"TFN Expert Only\\\" and \\\"TFN Expert,\\\" and between \\\"Poem Expert\\\" and \\\"Poem Expert Only\\\"?\\n- Page 8, Table 2: What are the results for the \\\"confidence-based fusion\\\" and \\\"ensemble\\\" baselines? The same question applies to Tables 3 and 4. The confidence-based fusion seems less intuitive in the generative case, how about simply selecting the maximum confidence at each decoding step?\\n- Page 9, Table 4: Could you explain more details of FoE (Expert 1), FoE (Expert 2), and FoE (Expert 3)? Why does adding more experts appear to degrade performance?\\n- Page 4: The statement \\\"As long as there is a label shift among the domains, we expect $E[f(X_k)] = E[Y_k]$\\\" needs clarification. Why is this expected?\\n- Page 6: The phrase \\\"Then $\\\\lambda$ can be interpreted as the final error rate reduction we want to achieve\\\" \u2013 could you expound on the reasoning behind this?\\n- Page 1: The statement \\\"our emphasis is on generalization to test data distributions where none of the experts perform well individually\\\". Shouldn't it be \\\"a few\\\" instead of \\\"none\\\" based on the problem formulation?\\n- Page 6: The condition \\\"If ... <0 we terminate the search\\\" should be\\\">0\\\"?\"}"}]}
{"paper_id": "N2WchST43h", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nAdversarial training is a widely used strategy for making neural networks resistant to adversarial perturbations. For a neural network of width $m$, $n$ input training data in $d$ dimension, it takes $\\Omega(mnd)$ time cost per training iteration for the forward and backward computation. In this paper we analyze the convergence guarantee of adversarial training procedure on a two-layer neural network with shifted ReLU activation, and shows that only $o(m)$ neurons will be activated for each input data per iteration. Furthermore, we develop an algorithm for adversarial training with time cost $o(m n d)$ per iteration by applying half-space reporting data structure.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 3, \"Strengths\": \"The approach is novel where adversarial training that leverages geometric data structures at the expense of extra space. It extends the direction of first identifying the active neurons and then only taking dot product with those neurons. \\n\\nThe paper is well written and however, without empirical results or illustrative examples, the clarity of the paper's practical implications could be enhanced.  \\n\\nThe work is significant as it has a potential to further improve the direction of efficient adversarial training of robust neural networks.\\n\\nI have not conducted an exhaustive review of the convergence proofs; however, upon initial examination, they appear to be correct.\", \"Weaknesses\": \"The study lacks experiments, even on toy datasets, to determine whether its findings are applicable in practical scenarios.\\n\\nFurthermore, it has not been compared with existing works. Although it is primarily theoretical, clarity on how it measures up against established methods is missing.\\n\\nThe paper does not discuss how reducing training time might impact the robustness performance of the model. This trade-off between efficiency and robustness is crucial for practical use cases.\\n\\nLikewise, the trade-off between memory usage and efficiency warrants discussion. While the paper could comment on memory requirements, it may be that for only two-layer networks, the difference is minimal. However, it would still be beneficial to understand this aspect, perhaps by employing very wide neural networks to confirm it in a practical scenario.\\n\\nIt is also unclear whether the proposed methods can be effectively translated into actual GPU computations. If the approach results in underutilized GPUs, it might still be valuable but may not reduce the overall wall time. Some commentary on this feasibility would be constructive.\", \"Questions\": \"To translate the theoretical advantages into practical scenarios, the paper should present at least some toy experiments.\\n\\nIt is important to explore how reducing training time affects the model's robustness performance, as the trade-off between efficiency and robustness is crucial for practical use cases.\\n\\nThe same consideration should be given to the trade-off between memory usage and efficiency. It would be beneficial for the paper to comment on memory requirements. While the impact on only a two-layer network might be minor, understanding this trade-off is essential, and could be tested using neural networks with extremely high widths in practical scenarios.\\n\\nIn short, a toy experiment featuring a two-layer network that showcases practical, minimal use cases by comparing memory usage, computational efficiency, and robustness performance would be a valuable addition to the work.\"}"}]}
{"paper_id": "LxruQOI93v", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nIt is widely believed that a neural network can fit a training set containing at least as many samples as it has parameters, underpinning notions of overparameterized and underparameterized models.  In practice, however, we only find solutions accessible via our training procedure, including the optimizer and regularizers, limiting flexibility.  Moreover, the exact parameterization of the function class, built into an architecture, shapes its loss surface and impacts the minima we find. In this work, we examine the ability of neural networks to fit data in practice.  Our findings indicate that: \n(1) standard optimizers find minima where the model can only fit training sets with significantly fewer samples than it has parameters; (2) convolutional networks are more parameter-efficient than MLPs and ViTs, even on randomly labeled data; (3) whereas stochastic training is thought to have a regularizing effect, SGD actually finds minima that fit more training data than full-batch gradient descent; (4) the difference in capacity to fit correctly labeled and incorrectly labeled samples predicts generalization; (5) ReLU activation functions enable fitting more data despite being designed to avoid vanishing and exploding gradients in deep architectures.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 1, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"- The paper is well-written overall and easy to follow. The breadth of the analysis is commendable. The paper additionally presents many interesting results.\", \"Weaknesses\": \"- First, the notion of EDC is identical to the Effective Model Complexity (Nakkiran, et al. 2021), which the authors cite but do not really compare with. Both correspond to the largest subset of training data that is interpolated by the model. However, the authors exercise caution when using it (e.g. by checking the norm of the gradients and the flatness of the loss curve, etc) to make sure that the models are not undertrained. I would recommend that the authors refrain from claiming novelty in the notion of EDC and simply center their contribution around the empirical investigation of the Effective Model Complexity, instead.\\n\\n- The second primary weakness in my opinion is the lack of clarity about the experimental details in a few places. \\n  * When comparing different architectures such as CNNs and ViTs, how did the authors ensure that the architectures are compared fairly? For example, there is no mention of the number of parameters used in each architecture. Even if those architectures share the same parameter count, the shape of the architecture can have a big impact as the authors themselves study for CNNs. It's not clear if one can conclude, for example, that CNNs are more parameter efficient than ViTs. There aren't enough details in the paper for the reader.\\n  * The authors mention repeatedly that ReLU improves EDC even though they \\\"were introduced to neural networks to prevent vanishing and exploding gradients.\\\" I do not see the discrepancy here. ReLU were introduced to prevent training instability issues so I would expect ReLU to improve training and improve EDC as a result. It is unclear why the authors highlight that the impact of ReLU on EDC is surprising. \\n  * When the authors compare different datasets, like CIFAR10/100, MNIST, and ImageNet, they use these results to argue for the impact of the input distribution. However, those datasets have different number of classes. One way to fix that issue is to fix the number of random labels in all datasets. There is no discussion of this in the paper and it seems that the authors kept the label space unchanged. The problem is that it is not clear (again) when the input distribution matters much when accounting for the differences in the number of classes.\\n  * In many figures, the authors average log(EDC). Is it reasonable to average them? Wouldn't the average EDC be dominated by what happens to large models? Perhaps it would be better to report the raw results without averaging.\\n \\n- Some minor comments: \\n  * The author should provide precise definitions of some of the terms they use. For example, what is \\\"parameter-efficient\\\"? My understanding is that it is EDC divided by the parameter count. Is this the case?\\n  * The figures are not presented in the same order they appear in the main text. \\n  * Figure 2(b) is very difficult to understand. Are the authors here scaling a single dimension at a time using EfficientNet scaling recipe and reporting the average results?\\n  * Page 6: typo in \\\"we three neural network\\\"\\n  * It would be interesting to see of the scaling laws derived for ViT [1] would also increase EDC, similar to the analysis that was done on EfficientNet. \\n\\n[1] Alabdulmohsin, Ibrahim, et al. \\\"Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design.\\\" NeurIPS, 2023.\", \"Questions\": \"- Can you please clarify how the comparison across different architectures was done? Did you ensure that all architectures have the same size? How were the shapes selected? Are they based on some standard shapes, such as those used in the original ViT and EfficientNet papers? What about MLP?\\n- When using random labels, did you fix the label space in all datasets (e.g. binary classification) or was the same label space used in each dataset (e.g. 10 classes in CIFAR10 but 100 classes in CIFAR100)?\\n- Can you please clarify Figure 2(b)?\"}"}]}
{"paper_id": "g6Urb6ukm7", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nLabel sparsity renders the use of the label information of multivariate time series (MTS) challenging in practice. Thus, unsupervised representation learning methods have been studied to learn effective representations of MTS without label information. Recently, many studies have employed contrastive learning to generate robust representations by capturing underlying information about MTS. However, they have some limitations, such as the insufficient consideration of relationships between the variables of MTS and high sensitivity to positive pairs. We proposed a novel spatio-temporal contrastive representation learning method (STCR) for generating effective MTS representations suitable for classification and forecasting tasks. STCR learns representations by encouraging spatio-temporal consistency, which comprehensively reflects the spatial information and temporal dependency of MTS and simultaneously mitigates sensitivity to constructing positive pairs for contrastive learning. The results of extensive experiments on MTS classification and forecasting tasks demonstrate the efficacy of STCR in generating high-quality representations and state-of-the-art performance on both tasks.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"Experiments are extensive and the motivations are interesting.\", \"Weaknesses\": \"1. This work lacks novelty. The overall structure is similar to TS-TCC [1] and TS2Vec [2]. For example, compared to TS2Vec, this manuscript is just adding GCN to consider spatial dependencies within MTS data.\\n\\n[1] Time-Series Representation Learning via Temporal and Contextual Contrasting\\n\\n[2] TS2Vec: Towards Universal Representation of Time Series\\n\\n2. References are too old, and the authors have done limited literature review. There are works highly related to this work. For example, GCC [3] also conducted graph neural network and explore the effects of spatial consistency. I think the authors should discuss their differences.\\n\\n[3] Graph Contextual Contrasting for Multivariate Time Series Classification\\n\\n3. Contributions have not addressed the problems that the authors pointed out. The authors pointed out the limitation in the existing augmentation methods, i.e., existing methods cannot consider the distribution changes in time-series data. To address the limitation, the authors proposed a method namely random cropping. I cannot see the difference between the random cropping and existing method subseries. Meanwhile, I don't think that the proposed method can address the mentioned problem. In the example of Figure 2, the signals within [s1, s2] are still having different distribution with [e1, e2]. Following the point of the authors (changing distribution), the two subseries cannot be treated as positive pairs.\\n\\n4. The authors mentioned the importance of achieving spatial-temporal consistency. However, how to define the spatial consistency? Why can spatial consistency be achieved after you construct and process a K-NN graph with GCN?\", \"Questions\": \"1. What is the differences between this work and existing studies, such as GCC [3].\\n\\n[3] Graph Contextual Contrasting for Multivariate Time Series Classification\\n\\n2. Please clarify the reason why random cropping can address the problem of distribution changes.\\n\\n3. How to define spatial consistency, and why can spatial consistency be achieved after you construct and process a K-NN graph with GCN?\"}"}]}
{"paper_id": "fXa9rO75dB", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nThis work presents a simple but highly effective self-supervised learning framework for deep visual odometry on stereo cameras.\nRecent work on deep visual odometry is often based on monocular vision.\nA common approach is to use two separate neural networks, which use raw images for depth and ego-motion prediction.\nThis paper proposes an alternative approach that argues against separate prediction of depth and ego-motion and emphasizes the advantages of optical flow and stereo cameras.\nThe framework's structure is justified based on mathematical equations for image coordinate transformations. \nIts central component is a deep neural network for optical flow predictions, from which depth and ego-motion can be derived.\nThe main contribution of this work is a 3D-geometric constraint, which enforces a realistic structure of the scene over consecutive frames and models static and moving objects.\nIt ensures that the neural network has to predict the optical flow as it would occur in the real world.\nThe presented framework is developed and tested on the KITTI dataset.\nIt achieves state-of-the-art results, outperforming most algorithms for deep visual odometry.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 1, \"Contribution\": 1, \"Rating\": 1, \"Confidence\": 5, \"Strengths\": \"The proposed method is slightly different from previous methods, even though the use of optical flow and stereo cameras in this field has not been new.\\n\\nThe depth accuracy of the proposed method is slightly better than previous methods that are mentioned in the paper.\", \"Weaknesses\": \"The paper looks incomplete. I hope that the authors can significantly revise the paper and add more content for the next submission.\\n\\nThe novelty of the proposed method is very limited. Please reconsider this problem and better claim the contribution.\\n\\nThe evaluation results show that the proposed method is worse than previous methods in both visual odometry and optical flow estimation.\", \"Questions\": \"NA\"}"}]}
{"paper_id": "DUkYDXqxKp", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nIn the past decade, autonomous driving has experienced rapid development in both academia and industry. However, its limited interpretability remains a significant unsolved problem, severely hindering autonomous vehicle commercialization and further development. Previous approaches utilizing small language models have failed to address this issue due to their lack of flexibility, generalization ability, and robustness. Recently, multimodal large language models (LLMs) have gained considerable attention from the research community for their capability to process and reason non-text data (e.g., images and videos) by text. \n In this paper, we present DriveGPT4, an interpretable end-to-end autonomous driving system utilizing LLMs. DriveGPT4 is capable of interpreting vehicle actions and providing corresponding reasoning, as well as answering diverse questions posed by human users for enhanced interaction. Additionally, DriveGPT4 predicts vehicle low-level control signals in an end-to-end fashion. These capabilities stem from a customized visual instruction tuning dataset specifically designed for autonomous driving.  To the best of our knowledge, DriveGPT4 is the first work focusing on interpretable end-to-end autonomous driving.  When evaluated on multiple tasks alongside conventional methods and video understanding LLMs, DriveGPT4 demonstrates superior qualitative and quantitative performance. Additionally, DriveGPT4 can be generalized in a zero-shot fashion to accommodate more unseen scenarios.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"- The paper is basically clearly written and easy to understand.\\n- The authors demonstrate that the LLM-based method has the zero-shot generalization ability to other datasets. It is a good point to involve LLM or large-scale training for images and videos.\", \"Weaknesses\": \"Multiple overclaims and issues about the soundness.\\n\\n- *Important:* There are multiple sentences saying that \\\"constrained by the limited capacity of smaller language models, they can only address predefined human questions and provide inflexible answers, hindering their widespread application in real-world scenarios\\\". The reviewer admits that LLMs can answer out-of-domain questions, but is still wondering if the limitation lies in the LLM. I will explain this point based on the experiments.\\n  - The improvement of the fixed question-answering experiment is limited (82.1->82.4, especially considering this is evaluated by ChatGPT with great uncertainty). Meanwhile, LLMs take much longer inference time to output the much longer answers compared to small models and the original QAs. The computational complexity is not that important at the current stage but is still valuable for illustration.\\n  - The additional question-answering experiment demonstrates the effectiveness of DriveGPT4 which is finetuned on the generated data. Then how about fintuning ADAPT and other works on the data? The direct comparison seems unfair.\\n  - For the control prediction task, DriveGPT4 takes historical control signals as input while others do not. It can be observed from the ablation study, that the results of 'No control signals' are close to ADAPT-32. The contribution comes from the history information, rather than LLM, which is unfair.\\n  - It is an open question if flexible answers can help real-world applications of autonomous driving.\\n- It is not the **first** work on interpretable end-to-end autonomous driving. The authors have already listed several in the related works. Even language-based interpretability is not accurate as there are multiple types of interpretability. In my opinion, it is fair to say LLM-based. \\n- From the very beginning, is **interpretability** indeed the unsolved problem that hinders the commercialization and development of autonomous driving? As described in the first paragraph of the introduction, the popular modular-based methods' issue is not the interpretability.\\n\\nTechnical contribution is limited. There are contributions of instruction tuning for the dataset generation. The video tokenization part is based on Valley and the action part is very close to existing works such as RT-2.\", \"Questions\": \"- Why not use ground truth from the BDD dataset? The detection results from YOLOv8 could be inaccurate. I am wondering if there is a truck in the provided figure (Table 1).\\n- How bounding box coordinates are normalized? In which coordinates are they normalized?\\n- If the model can generalize to video games, it is worth trying generalizing to CARLA and evaluating it in a closed-loop manner.\\n- The short name 'DriveGPT4' is not very appropriate, as the core method is finetuned from LLaMA 2 and does not use GPT4.\"}"}]}
{"paper_id": "OCx7dp58H1", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nTransformer-based models have recently become wildly successful across a diverse set of domains. At the same time, recent work has shown that Transformers are inherently low-pass filters that can oversmooth the input. This causes their performance to quickly saturate as model depth increases. A natural question is: How can Transformers achieve success given this shortcoming? In this work we show that in fact Transformers are not inherently low-pass filters. Instead, whether Transformers oversmooth or not depends on the eigenspectrum of their update equations. Our analysis extends prior work in oversmoothing and in the closely-related phenomenon of rank collapse. We show that many successful Transformer models have attention and weights which satisfy conditions that avoid oversmoothing. Finally, we describe a simple way to reparameterize the weights of the Transformer update equations to ensure that oversmoothing does not occur. Compared to other solutions for oversmoothing, our approach does not require a new architecture, or any additional hyperparameters.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"The paper is easy to follow with necessary details given to understand the theory. It extends the available theory and proposes a new perspective on reparameterization. Experimental results support the proposed theory and preventive measures on oversmoothing.\", \"Weaknesses\": \"While the theory extends previous work in some aspects, it is not clear if this difference is significant. Other preventive measures such as AttnScale/FeatScale from Wang et al. needs comparison and more discussion. Several details are also missing from the paper.\\n\\n1. Wang et al also examines residual blocks and FFN in addition to the self-attention layer. While they mention the inevitability of high frequency components to be suppressed, their analysis suggest that using residual connections might prevent it from collapsing to zero. In fact, Eq (6) in Theorem-3 suggests that it is possible to prevent decaying of high-frequency components by constraining the $||W_V||_2$ properly so that it is a non-contractive mapping. I think a more detailed comparison to Wang et al is needed to highlight how your analysis differs from low-pass filtering aspect and how your preventive measures are different from constraining $||W_V||_2$ or AttnScale/FeatScale that Wang et al applies.\\n\\n2. While Figure-2 shows that asymmetry degrades with more epochs, is 0.7 small enough to suggest symmetry, given that it starts at 0.95? What about outliers in the off-diagonal entries? Additionally, can you connect it to the condition number?\\n\\n3. Can you clarify more on how do you update $\\\\Theta$ in $QR(\\\\Theta)=[V_H, R]$? Do you do QR decomposition after every gradient step or do you use $\\\\Theta R^{-1}=V_H$ where you backpropagate gradients directly to $\\\\Theta$?\\n\\n4. How does the HFC/LFC evolve in training steps? \\n\\n5. At the end of Section 4, page 6, why do you have $V_H^{-1}$ after clip?\\n\\n6. The same page, last paragraph, I think it should be $diag(\\\\Lambda_H)=\\\\Psi^2$ -- no negative sign.\\n\\n7. Please define Q in the main statement of Theorem-2.\\n\\n8. Page 6, \\\"||\\\" is missing in definition of asymmetry.\\n\\n9. Please describe the metrics for Table-2 and Table-3.\", \"Questions\": \"Please see above for more details.\\n\\n1. Can you provide a more detailed comparison with Wang et al? Including more baselines.\\n\\n2. How should I interpret the asymmetry metric? How does it relate to condition number?\\n\\n3. Can you give more details on updating QR decomposition?\\n\\n4. How does HFC/LFC evolve during training?\"}"}]}
{"paper_id": "eO6lXIWyxn", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nDiffusion models have shown remarkable performance in generating a broad spectrum of visual content. However, their text rendering ability is still limited: they generate wrong characters or words that cannot blend well with the background image. To address this, we introduce a novel framework named ARTIST, which includes an additional textual diffusion model focusing on text structure learning. We first pretrain the textual diffusion model. Then we further fine-tune the visual model to learn how to inject textual structure information from the frozen textual model into the image. This disentangled architecture design and training strategy significantly enhance the text rendering ability of the diffusion models for text-rich image generation. Furthermore, we leverage pre-trained large-language models to infer the user's intention leading to better generation quality. Empirical results on the MARIO-Eval benchmark underscore the effectiveness of the proposed method, showing an improvement of up to 15\\% in various metrics.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 5, \"Strengths\": \"- This paper is well-written and easy to follow.\\n- Text painting is a challenging task for current T2I models, and the idea of disentanglement between text and figure is well-motivated.\\n-  They provide lots of qualitative examples for the visual comparison.\\n- Their proposed ARTIST achieves notable improvements over the previous TextDiffuser, especially on the crucial OCR metric.\", \"Weaknesses\": \"- It seems that the ARTIST framework is the combination of LLM-Layout and TextDiffusers, which both have been proposed before. Not sure if this achieves ICLR's novelty bar.\\n- There are so many metrics used for the evaluation (FID, CLIP-S, and OCR). Which one is the most appropriate to evaluate the overall performance? Or is there any way to combine all of them as a final metric?\\n- As a generative task, a human generation should be conducted to compare the performance in a human aspect.\\n- Since ARTIST relies on LLM to derive the layout, there should be a discussion about the quality of the generated layout. Is it visually appealing or in a reasonable position?\\n- Missing reference about LLM-Layout: [NeurIPS'23] Compositional Visual Planning and Generation with Large Language Models / [arXiv'23] Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models\", \"Questions\": \"Please see the weaknesses\"}"}]}
{"paper_id": "nUGFpDCu3W", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nLarge Language Models such as GPT are able to recall factual information about the world that they have learnt during training. This information must be stored in the model weights yet there is much we do not know about exactly what information is stored, where it is located and how it is retrieved. In this paper, we test and develop existing theories about information storage and retrieval through the example of bracketed sentences. We show that, in the case of recognizing brackets, where a model must learn during training to associate matching opening and closing brackets, very early multi-layer perceptron (MLP) layers in the source position  are responsible for this association. This supports existing work on the importance of MLP layers as key-value memory stores (Meng et al., 2023) and a potential hierarchy of roles within transformers, whereby early layers are responsible for storing and retrieving lower level information compared to more abstract information which is stored in later layers (Geva et al., 2021).\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"- The authors performed a thorough study with different tests\\n- They show results which supports their claims\\n- The work represents a step forward towards understanding more DL\\nnetworks.\", \"Weaknesses\": \"- The outcomes are not surprising as it is expected that simple\\ninformation is stored in the first layer and as the information flows\\ninto deeper layers, more complex relationships are established\\n- In this sense, it is not very clear the significance of the reported\\nresults.\", \"Questions\": \"The authors focused mainly in short sequences and it is not completely\\nclear the behavior in much larger sequences.\\n\\nIt is not clear why the plots in Figure 7 are very different with\\ndifferent type of brackets. I was expected to see similar curves for\\nall types of brackets.\"}"}]}
{"paper_id": "B9klVS7Ddk", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nDespite their remarkable achievements, modern Large Language Models (LLMs) encounter exorbitant computational and memory footprints. Recently, several works have shown significant success in *training-free* and  *data-free* compression (pruning and quantization) of LLMs achieving 50-60\\% sparsity and reducing the bit-width down to 3 or 4 bits per weight, with negligible perplexity degradation over the uncompressed baseline. As recent research efforts are focused on developing increasingly sophisticated compression methods, our work takes a step back, and re-evaluates the effectiveness of existing SoTA compression methods, which rely on a fairly simple and widely questioned metric, perplexity (even for dense LLMs). We introduce **K**nowledge-**I**ntensive **C**ompressed LLM Benchmar**K** **(LLM-KICK)**, a collection of carefully-curated tasks to re-define the evaluation protocol for compressed LLMs, which have significant alignment with their dense counterparts, and perplexity fail to capture subtle change in their true capabilities. LLM-KICK unveils many favorable merits and unfortunate plights of current SoTA compression methods: all pruning methods suffer significant performance degradation, sometimes at trivial sparsity ratios (*e.g.*, 25-30\\%), and fail for N:M sparsity on knowledge-intensive tasks; current quantization methods are more successful than pruning; yet, pruned LLMs even at $\\geq 50$\\% sparsity are robust in-context retrieval and summarization systems; among others. LLM-KICK is designed to holistically access compressed LLMs' ability for language understanding, reasoning, generation, in-context retrieval, in-context summarization, *etc.* We hope our study can foster the development of better LLM compression methods. The reproduced codes are available at https://github.com/VITA-Group/llm-kick.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"1. Compression of LLMs is very timely and important.\\n\\n2. The paper reveals new and yet widely unknown gaps in compressed LLMs in comparison to their uncompressed counterparts. \\n\\n3. The paper shows that compressed models may offer better performance in some tasks (e.g., In-Context Text Summarization) than others (e.g., Factoid-based Question Answering)\\n\\n4. The authors plan to release their code which may be help in the development of future compression techniques.\", \"Weaknesses\": \"1. It would make the conclusions more robust and convincing if the evaluations use more than a single family of LLMs (i.e., Vicuna).  Why not repeat these experiments with, e.g., Llama 2 and Falcon?\\n\\n2. Regarding the observation that even 8-bit quantization has evident gaps with respect to uncompressed models, have the authors considered evaluating LLM.int8()?  (https://arxiv.org/pdf/2208.07339.pdf) \\n\\n3. It would help the reader to have a table summarizing all the tasks' performance over the different architectures, compression techniques and the their resulting perplexity.\", \"Questions\": \"See weakness 1 and 2. Also, does the authors have insights regarding why, in the paper's evaluation, quantization works better than pruning?\"}"}]}
{"paper_id": "aiPcdCFmYy", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nThe empirical success of distributional reinforcement learning~(RL) highly depends on the representation of return distributions and the choice of distribution divergence. In this paper, we propose \\textit{Sinkhorn distributional RL~(SinkhornDRL)} algorithm that learns unrestricted statistics, i.e., deterministic samples, from each return distribution and then leverages Sinkhorn divergence to minimize the difference between current and target Bellman return distributions. Theoretically, we prove the convergence properties of SinkhornDRL in the tabular setting, which is consistent with the interpolation nature of Sinkhorn divergence between Wasserstein distance and Maximum Mean Discrepancy~(MMD). We also establish a new equivalent form of Sinkhorn divergence with a regularized MMD beyond the optimal transport literature, contributing to interpreting the superiority of SinkhornDRL over existing distributional RL methods. Empirically, we show that SinkhornDRL is consistently better or comparable to existing algorithms on the suite of 55 Atari games.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"The propsed sinkhorn divergence based Bellman update loss includes the regularization loss which is the mutual information of a given distribution and the target distribution.\\n\\nThis makes the above two distributions statistically independent, and the reguarlizaiton loss with the coefficeint $\\\\epsilon$ helps to increase the interquantile mean performance of atari-55.\", \"Weaknesses\": \"The main theoretical contribution can be seen as a simple combination of the architecture of MMD DRL and Sinkorn divergence. The additional KL divergence term (mutual information) in the proposed loss is a key factor to improve the DRL algorithms except for two corner cases $\\\\epsilon=0,\\\\infty$. However, the role of this proposed element is not well explained in the main text. The detailed comments are as follows. The main strength statement in this paper is that Sinkhorn divergence can be useful in the complex environments with high dimensional action space. This discussion is not rigorous, because I cannot find any explanation why the mutual information between two return distribution and the dimension of action space is related.\\n\\nI also have the following minor concerns.\\n\\n1. It is hard to get an insight by introducing the Sinkhorn divergence. For example, the paper of MMD DRL provides the figure that explains the propsed scheme can estimate the high order moments better than the existing methods.\\n2. The main ablation study (sensitivity anlysis) is provided without any detailed discussion. The authors states taht the property of loss changes as the value of $\\\\epsilon$ changes in the main theorem, but the empricial analysis and discussion is not provided well. \\n3. The experiment protocol in this paper is not standard. Previous works almost conducted in 200M iterations or the authors could have chosen the protocol of the paper, DRL at the edge of statistical precipice to validate the performance.\\n4. In table 2 and Figure 2, the propsed method outperforms in terms of mean score but not in median score than baselines, and the proposed method show remarkable performance in venture and sequest. This means that the proposed method is not a generally better algorithm but a specialist for Venture and Sequest. In my point of view, the authors should have stated why the propsee method is better in such envs.\\n5. There exists more projection method for distributional RL such as EDRL [A], but the provided baselines is not enough.\\n\\n\\n\\n[A] Rowland, Mark, et al. \\\"Statistics and samples in distributional reinforcement learning.\\\"\u00a0International Conference on Machine Learning. PMLR, 2019.\", \"Questions\": \"I mentioned my main concern in the weakness part. Please refer the weakness section.\\n\\nIn addition, it is hard to understand that reducing the mutual information helps to build a better projection operator.\\nCan the authors provide detailed explanation why the mutual information between the return distribution and target distribution is important?\\nIn my point of view, the unresticted statistics can be constructed better, because its deterministic samples can be more uncorrelated by reducing the mutual information.\\nIf it is true, the main effect seems related to the sampling scheme, not the discrepency between two ground-truth return distribution.\"}"}]}
{"paper_id": "ZULjcYLWKe", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nOffline reinforcement learning (RL), which aims to fully explore offline datasets for training without interaction with environments, has attracted growing recent attention. A major challenge for the real-world application of offline RL stems from the robustness against state observation perturbations, e.g., as a result of sensor errors or adversarial attacks. Unlike online robust RL, agents cannot be adversarially trained in the offline setting. In this work, we propose Diffusion Model-Based Predictor (DMBP) in a new framework that recovers the actual states with conditional diffusion models for state-based RL tasks. To mitigate the error accumulation issue in model-based estimation resulting from the classical training of conventional diffusion models, we propose a non-Markovian training objective to minimize the sum entropy of denoised states in RL trajectory. Experiments on standard benchmark problems demonstrate that DMBP can significantly enhance the robustness of existing offline RL algorithms against different scales of ran- dom noises and adversarial attacks on state observations. Further, the proposed framework can effectively deal with incomplete state observations with random combinations of multiple unobserved dimensions in the test. Our implementation is available at https://github.com/zhyang2226/DMBP.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"1. DMBP strengthens the resilience of existing offline RL algorithms, allowing them to handle different scales of random noises and adversarial attacks effectively.\\n2. Unlike traditional approaches, DMBP leverages diffusion models primarily for noise reduction, rather than as generation models. This innovative use helps in better state prediction and recovery against observation perturbations.\\n3. The framework introduces a non-Markovian loss function, specifically designed to prevent the accumulation of estimation errors over the RL trajectory.\\n4. DMBP offers a more balanced approach than methods that train robust policies against worst-case disturbances. This ensures that policies do not become overly conservative, which can hinder performance in certain scenarios.\\n5. DMBP's inherent ability, derived from the properties of diffusion models, allows it to effectively manage situations with incomplete state observations. This is particularly valuable in real-world applications, such as when robots operate with compromised sensors.\", \"Weaknesses\": \"1. The paper has the assumption that perturbation on state space follows a Gaussian distribution. However, in practical settings, such perturbations might be biased and skewed. For example, a water drop on a camera might lead to distortion of captured images. Therefore, it might be better if the authors could elaborate on the assumption of the perturbations and how they are produced. \\n2. Introducing a diffusion model for noise reduction could increase the complexity of the model, which may lead to longer training time and resource-intensive computations. The authors might want to include more details regarding training time and computational resources.\\n3. Scaling to very large state spaces or handling very noisy environments might pose challenges, especially when using diffusion models. It would be better for the authors to consider testing on tasks with larger state spaces such as the humanoid. \\n4. Eq.(2) is wrong as the recovered state should not be a probability.\", \"Questions\": \"Same as the weakness.\"}"}]}
{"paper_id": "svSWP21tdp", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nModel-induced distribution shifts (MIDS) occur as previous model outputs pollute new model training sets over generations of models. \nThis is known as model collapse in the case of generative models, and performative prediction or unfairness feedback loops for supervised models. We provide a taxonomy for MIDS and demonstrate that their long-run fairness effects lead to a lack of representation and performance on minoritized groups within a few generations. We improve upon this unfairness behavior by situating Algorithmic Reparation as an intentional MIDS with the goal of providing redress for historical discrimination and improving the fairness of models subject to other MIDS. Our work makes an important step towards identifying and mitigating the justification of unfair feedback loops via the algorithmic objectivity and idealism ascribed to autonomous systems.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"- The paper addresses the important issue of studying the fairness effects of repeated training in a data ecosystem. \\n- The paper presents a clear model of sequentially training classifiers and generators that can be used to simultaneously model various effects like feedback loops, performative effects and model collapse.\", \"Weaknesses\": \"- The paper does not propose a mathematical model for the proposed MIDS scheme. Although various effects of retraining are shown through experiments, there is no theoretical investigation on the root causes of these effects, or the efficacy of the presented Algorithmic Reparation (AR) scheme.\\n- The experimental results with the proposed AR scheme based on resampling are not very convincing. For example, the decrease in the equalized odds difference with reparation in Figure 4 is not monotonic. The gap closes for a few generations and then seems to rise up again. The plots on fairness metrics in Figure 5 are even less convincing.\", \"Questions\": \"- What is the reason for the decrease in the equalized odds difference with reparation in Figure 4 not being monotonic?\\n- The paper claims that \\\"Generator-side AR improves fairness and minoritized representation\\\", but plots on fairness metrics in Figure 5 do not corroborate this claim. Please explain.\\n- I do not understand the claim, \\\"Performative prediction on model collapse leads to higher utility\\\". Please explain what you mean by this.\"}"}]}
{"paper_id": "oDYXpvnv5f", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nWe consider the problem of uncertainty quantification in high dimensional regression and classification, for which deep ensembles have proven to be promising methods. Recent observations have shown that deep ensembles often return overconfident estimates outside the training domain, which is a major limitation because shifted distributions are often encountered in real-life scenarios. The principal challenge for this problem is to solve the trade-off between increasing the diversity of the ensemble outputs and making accurate in-distribution predictions. In this work, we show that an ensemble of networks with large weights fitting the training data are likely to meet these two objectives. We derive a simple and practical approach to produce such ensembles, based on an original anti-regularization term penalizing small weights and a control process of the weight increase which maintains the in-distribution loss under an acceptable threshold. The developed approach does not require any out-of-distribution training data neither any trade-off hyper-parameter calibration. We derive a theoretical framework for this approach and show that the proposed optimization can be seen as a \"water-filling\" problem. Several experiments in both regression and classification settings highlight that Deep Anti-Regularized Ensembles (DARE) significantly improve uncertainty quantification outside the training domain in comparison to recent deep ensembles and out-of-distribution detection methods.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"The paper does a good job of identifying and conveying an insight (larger weight variance leads to larger output variance), providing theoretical motivation, and then running experiments to test out that method.\", \"Weaknesses\": \"The main weaknesses of this paper are the limited set of experiments and mixed results. ResNet-32 on CIFAR-10 with only the final layer ensembled provides limited data. It's a fine experiment to include in the paper, but a larger-scale experiment with a fully-ensembled bigger model on a bigger dataset (even on ImageNet, which is not that large anymore) would provide much more information on the viability of this approach and whether it provides enough benefit. Concretely, looking at Table 2, both in-distribution accuracy and OOD detection is mixed for DARE, including worse results for the important near-OOD case of CIFAR-100 vs CIFAR-10. As noted in the limitations section, this paper is also missing comparisons to Bayesian neural nets (rank-1 BNNs and Prior networks could be nice experimental comparisons here). In the current environment, experiments with language models would also be extremely useful. Overall, the experimental setup is limited and the results are not that compelling in their current state.\", \"Questions\": \"As noted in the weaknesses above, I would suggest running larger experiments to better allow the reader to understand the efficacy of the approach. Uncertainty quantification is an important problem, so I encourage continued work here!\"}"}]}
{"paper_id": "mFTPRV5hYw", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nAs location-based services (LBS) have grown in popularity, the collection of human mobility data has become increasingly extensive to build machine learning (ML) models offering enhanced convenience to LBS users. However, the convenience comes with the risk of privacy leakage since this type of data might contain sensitive information related to user identities, such as home/work locations. Prior work focuses on protecting mobility data privacy during transmission or prior to release, lacking the privacy risk evaluation of mobility data-based ML models. To better understand and quantify the privacy leakage in mobility data-based ML models, we design a privacy attack suite containing data extraction and membership inference attacks tailored for point-of-interest (POI) recommendation models, one of the most widely used mobility data-based ML models. These attacks in our attack suite assume different adversary knowledge and aim to extract different types of sensitive information from mobility data, providing a holistic privacy risk assessment for POI recommendation models. Our experimental evaluation using two real-world mobility datasets demonstrates that current POI recommendation models are vulnerable to our attacks. We also present unique findings to understand what types of mobility data are more susceptible to privacy attacks. Finally, we evaluate defenses against these attacks and highlight future directions and challenges.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 2, \"doi\": 10, \"Strengths\": \"1. The research problem is important.\\n2. The paper analyzes the factors in the data that affect the attack performance.\\n3. The proposed attacks for data extraction and membership inference attacks are simple yet effective.\", \"Weaknesses\": \"1. The appropriate baselines are missing. Can the existing data extraction or membership inference attacks be applied to the POI recommendation models, e.g., [1]?\\n2. There is no qualitative result analysis on the data extraction attacks. It would be better if the authors could conduct these analyses on the data extraction attacks.\\n3. The threat models assume that the adversaries are capable of accessing the confidence scores, which makes them impractical. In practice, the model owner only releases the final result to the users. In this case, whether the proposed attacks are still effective is unknown.\\n\\n\\n\\n\\n[1] R. Shokri, M. Stronati, C. Song and V. Shmatikov, \\\"Membership Inference Attacks Against Machine Learning Models,\\\" 2017 IEEE Symposium on Security and Privacy (SP), San Jose, CA, USA, 2017, pp. 3-18, doi: 10.1109/SP.2017.41.\", \"Questions\": \"N/A\"}"}]}
{"paper_id": "vVoWRFV5Y4", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nThe Quadratic Assignment Problem (QAP) is an NP-hard problem which has proven particularly challenging to solve: unlike other combinatorial problems like the traveling salesman problem (TSP), which can be solved to optimality for instances with hundreds or even thousands of locations using advanced integer programming techniques, no methods are known to exactly solve QAP instances of size greater than 30. Solving the QAP is nevertheless important because of its many critical applications, such as electronic wiring design and facility layout selection. We propose a method to solve the original Koopmans-Beckman formulation of the QAP using deep reinforcement learning. Our approach relies on a novel double pointer network, which alternates between selecting a location in which to place the next facility and a facility to place in the previous location. We train our model using A2C on a large dataset of synthetic instances, producing solutions with no instance-specific retraining necessary. Out of sample, our solutions are on average within 7.5% of a high-quality local search baseline, and even outperform it on 1.2% of instances.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1- This paper is well motivated as the QAP has been rarely studied using DRL;\\n2- It seems the proposed method works on the self-generated dataset.\", \"Weaknesses\": \"1- It seems that from the DRL based heuristic perspective, the objective function in QAP does not make difference compared with the ones in ILP, both of which are just captured by a reward in DRL. In this sense, the QAP in this paper is almost the same as the vehicle and customer assignment in VRPs with linear objectives, such as\\n[a] Deep Reinforcement Learning for Solving the Heterogeneous Capacitated Vehicle Routing Problem. IEEE T Cybernetics;\\n[b] Learning to Solve Vehicle Routing Problems with Time Windows through Joint Attention. Arxiv;\\n[c] Solving NP-hard Min-max Routing Problems as Sequential Generation with Equity Context. Arxiv.\\nThose works also involve sequentially or parallelly selecting vehicles and customers, while using (advanced) Transformers rather than pointer network.\\n\\n2- Besides, the Transformers in 'Learning Improvement Heuristics for Solving Routing Problems. TNNLS' outputs a probability matrix, which could also be tailored to the probability of facility and location pair with proper masking.\\n\\n3- The evaluation is quite simple, which solely focuses on one single problem with randomly generated instances. And the baselines are also insufficient and not strong enough.\\n\\nOverall, the method falls short of novelty, and the evaluation is inadequate, which is below the standard of an ICLR publication.\", \"Questions\": \"Please see the above weakness.\"}"}]}
{"paper_id": "rmA6uFEgIY", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nBy adapting the success of Contrastive Language-Image Pre-training from 2D vision to the 3D world, contrastive learning has emerged as a promising paradigm for open-world understanding jointly with text, image, and point cloud. While existing studies focus on aligning features from these individual modalities, this paper introduces a novel joint representation alignment approach. This mechanism enriches the conventional tri-modal representation by creating a new combined representation of images and point clouds, thus offering more accurate depiction of real-world 3D objects and bolstering text alignment. The method, termed as MixCon3D, is furthered through the integration of multi-view images, offering a more holistic representation. Furthermore, we pioneer the first thorough investigation of various training recipes for the 3D contrastive learning paradigm, building a strong baseline with improved performance and generalizability. Extensive experiments conducted on three representative benchmarks reveals that our method renders significant improvement over the baseline, surpassing the previous stateof-the-art performance on the challenging 1,156-category Objaverse-LVIS dataset by 5.7%. We further showcase the effectiveness of our approach in more applications, including text-to-3D retrieval and point cloud captioning.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 5, \"arXiv\": 2305, \"Strengths\": \"1. The paper is well-written and easy to understand. The approaches are easy to follow.\\n2. Experiments state that the method achieves state-of-the-art performance, and the quantitative shows great robustness on the downstream tasks. \\n3. The motivation is clear and reasonable.\", \"Weaknesses\": \"1. Additional Comparisons:\\nYour zero-shot classification experiments on the ModelNet40 and other datasets (Table 2) offer valuable insights. To further substantiate your findings, it could be beneficial to include comparisons with some additional methods that have shown promise in this domain, such as JM3D [1] and ULIP-2 [2]. Such comparisons could provide a more comprehensive view of how your approach stands in the context of the existing state-of-the-art methods and highlight the unique contributions of your work more effectively.\\n\\n2. Clarification of Contribution:\\nYou have noted the introduction of multi-view images in the context of contrastive learning across multiple modalities as a key contribution to your paper. It is indeed an interesting aspect of your research. However, to strengthen your claim, it would be constructive to discuss how your approach differentiates from or builds upon prior works like JM3D [1] and ULIP-2 [2], which have also explored multi-view images within a similar framework. Clarifying this could help readers appreciate the novel aspects of your methodology and its significance in the field.\\n\\n3. Expansion of Quantitative Experiments:\\nThe paper presents intriguing insights into the arithmetic of multi-modalities. To solidify these findings, it might be valuable to expand on the quantitative experiments. Additional data or more varied testing conditions could offer deeper validation of your hypothesis and might also uncover further interesting aspects of your model's behavior. This would not only bolster your current claims but could also provide a richer resource for readers interested in the intricacies of multi-modal contrastive learning.\\n\\n[1] Beyond First Impressions: Integrating Joint Multi-modal Cues for Comprehensive 3D Representation, ACM MM 23\\n\\n[2] ULIP-2: Towards Scalable Multimodal Pre-training For 3D Understanding, arXiv: 2305.08275\", \"Questions\": \"1. Multi-View Analysis: Could you specify the number of views used during training and consider including an ablation study on this feature? This would greatly clarify the contribution of the multi-view approach.\\n\\n2. Extended Experiments: Exploring fine-grained tasks like semantic segmentation, as in ULIP and JM3D, could offer deeper insights and substantiate your model\u2019s versatility.\\n\\n3. Clarification on Table 5: Please clarify the meaning of \\\"no point cloud\\\" in rows 2 and 3, to better understand how these metrics are compared without point cloud inputs.\\n\\n4. Typographical Corrections: I've noticed a minor typographical error on the 10th row of the 8th page, where the word \\\"use\\\" is repeated.\"}"}]}
{"paper_id": "3pWSL8My6B", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nThis study aims to prove the emergence of symbolic concepts (or more precisely, sparse primitive inference patterns) in well-trained deep neural networks (DNNs). Specifically, we prove the following three conditions for the emergence. (i) The high-order derivatives of the network output with respect to the input variables are all zero. (ii) The DNN can be used on occluded samples, and when the input sample is less occluded, the DNN will yield higher confidence. (iii) The confidence of the DNN does not significantly degrade on occluded samples. These conditions are quite common, and we prove that under these conditions, the DNN will only encode a relatively small number of sparse interactions between input variables. Moreover, we can consider such interactions as symbolic primitive inference patterns encoded by a DNN, because we show that inference scores of the DNN on an exponentially large number of randomly masked samples can always be well mimicked by numerical effects of just a few interactions.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 2, \"Strengths\": \"* The paper is well-written and clear.\\n* The paper contributes novel proofs for theorems 2 onwards.\\n* The paper appears well-placed in the relevant literature.\\n* The papers theoretical results appear significant and relevant to the ICLR community.\", \"Weaknesses\": \"* The claim of to \\\"prove the emergence of symbolic concepts (or more precisely, sparse primitive inference patterns) in well-trained AI models.\\\" (Line 1), appears too broad and un-substantiated, specifically encompassing \\\"AI models\\\". It appears the evidence presented in the paper is only applicable and demonstrated for Deep Neural Networks (DNNs). For example, I cannot see how this proof or claims can be applied to other AI models that are not DNNs, such as Random Forests, which considers the outputs of all trees; k-NN's, that use all features to compute distances, or Boosting algorithms (e.g., Gradient Boosting Machines). Specifically, the empirical evidence presented in the paper in Figure 3, Figure 4, Figure 5 and Table 1, only apply to deep neural network models, and there is no other AI models that are not DNN's to support this claim.\\n* It is not clear how the input variables should be grouped to form the set of $S$ input variables. For example, in the dog image example, \\\"image regions\\\" are proposed; however, there is no indication or discussion on how large these \\\"image regions\\\" should be for the approach and proof to work. For example, does this sparse primitive approach still hold if the \\\"image regions\\\" are too small, e.g., each region is a single pixel, and does it still hold if the \\\"image regions\\\" are too large, e.g., encompassing most of the image, or even half of the image. Some discussion on this, and how to select these groupings could be informative to the reader.\\n*  Missing error bars; and large error bars. There appears to be missing error bars for Figure 4, Figure 5 (a), Table 1 (top row: Percent of samples with monotonicity). The provided error bars in Table 1 and Table 2, are so large that they overlap; leading the reader to suspect that the claim of the input samples satisfying the monotonicity assumption is not empirically verified, as the numeric results in Table 1 are not statistically significant, as the error bars overlap.\", \"Questions\": \"* Can you clarify if the core proof of the paper applies to other AI models that are not DNN's? If it does, can you provide evidence for this, and if it does not, perhaps think of refining your claims, and title to be only applicable to DNNs.\\n* Can you clarify how the reader should group input variables to form the set of $S$?\\n* Can you include error bars for the top row of Table 1, and include error bars for Figure 4 and Figure 5 (a). Also can you re-run with more random seeds to see if the error bars in Table 1 reduce.\"}"}]}
{"paper_id": "ZKnbIZefER", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nAvailability attacks can prevent the unauthorized use of private data and commercial datasets by generating imperceptible noise and making unlearnable examples before release. \nIdeally, the obtained unlearnability prevents algorithms from training usable models. \nWhen supervised learning algorithms have failed, a malicious data collector possibly resorts to contrastive learning algorithms to bypass the protection.\nAttacks need both supervised unlearnability and contrastive unlearnability.\nThrough evaluation, we have found that most of the existing availability attacks are unable to achieve contrastive unlearnability, which poses risks to data protection. \nFurthermore, we find that employing stronger data augmentations in supervised poisoning generation can create contrastive shortcuts and mitigate this risk. \nBased on this insight, we propose AUE and AAP attacks which prominently boost the worst-case unlearnability across multiple supervised and contrastive algorithms.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"- The authors quantitatively measure the GAP between the attack effectiveness of supervised learning and contrastive learning methods using two contrastive learning metrics.\\n- By incorporating the data augmentation techniques of contrastive learning into supervised learning, the authors develop an availability attack method that is effective under both supervised learning and contrastive learning training frameworks.\", \"Weaknesses\": \"- The paper is difficult to read and the organization of the content is not very clear.\\n- The Cross-Entropy (CE) loss and InfoNCE loss may be essentially similar, and using these two losses to reflect the relationship between the two tasks is not particularly convincing.\\n- Although the paper emphasizes sampling in data augmentation as being introduced from contrastive learning, it bears a resemblance to the Expectation Over Transformation (EOT) used in the reference paper on REM. EOT also involves sampling from data augmentation, and this technique is commonly used in adversarial settings.\\n- The assumptions made in the theoretical analysis employ a simple linear network, which presents a significant discrepancy from practical settings.\\n- The resolution of the Tiny-ImageNet dataset is not down-sampled, and the Mini-ImageNet dataset has a limited number of samples per class.\", \"Questions\": \"- Does the value of the GAP affect accuracy? According to the paper, GAP reflects the difference between clean and poison distributions. Logically, the greater the difference, the better the protection effect should be. However, the results in Table 1 do not seem to support this notion.\\n- Why does the loss of Alignment in Figure 3(c) first decrease and then increase?\"}"}]}
{"paper_id": "g9diuvxN6D", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nInstruction fine-tuning has recently emerged as a promising approach for improving the zero-shot capabilities of Large Language Models (LLMs) on new tasks. This technique has shown particular strength in improving the performance of modestly sized LLMs, sometimes inducing performance competitive with much larger model variants. In this paper, we ask two questions: (1) How sensitive are instruction-tuned models to the particular phrasings of instructions, and, (2) How can we make them more robust to such natural language variation? To answer the former, we collect a set of 319 instructions manually written by NLP practitioners for over 80 unique tasks included in widely used benchmarks, and we evaluate the variance and average performance of these instructions as compared to instruction phrasings observed during instruction fine-tuning. We find that using novel (unobserved) but appropriate instruction phrasings consistently degrades model performance, sometimes substantially so. Further, such natural instructions yield a wide variance in downstream performance, despite their semantic equivalence. Put another way, instruction-tuned models are not especially robust to instruction re-phrasings. \nWe propose a simple method to mitigate this issue by introducing ``soft prompt'' embedding parameters and optimizing these to maximize the similarity between representations of semantically equivalent instructions. We show that this method consistently improves the robustness of instruction-tuned models.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"- The experiments are clear and well-thought-out, and the results are solid and interesting, affecting many current popular models (Llama finetunes), and holding for different model families.\\n- The observed correlations between performance and instruction similarity (in representation space) are interesting (even if the correlation is somewhat weak).\\n- The proposed fix method is well-motivated, and the results seem reasonable, especially for the Alpaca-7B model.\\n- The robustness of models to different prompts is an important and interesting area of study, especially considering current difficulties with LM evaluations.\", \"Weaknesses\": \"- The novelty of the robustness experiments is somewhat limited, as there is prior work that has observed variance based on prompt formatting (discussed in the work itself). Considering this work seems to examine more modern models such as Alpaca, and considers a different set of tasks, I think this is a minor weakness - the work does add a number of interesting insights, especially around representation distance.\\n- The claim that models perform worse at novel phrasings of tasks seems a bit strong compared to the results: examining Table 3, the variance of performance on unobserved phrasings often is greater than the difference in average performance, suggesting that the difference in performance may not be significant (although the large increase in variance is certainly not desirable and interesting to see).\\n- While the proposed method seems strong, examining the results in Appendix B.3 seems to show that in many cases, prompt-tuning alone performs within one standard deviation of the PT+KL results (e.g., for BBL unobserved and average). As such, the proposed method might not be significantly different from just prompt tuning.\\n\\nOverall, I think this paper is okay, and the robustness results are interesting and useful to see, along with the representation alignment results. However, the novelty of the robustness experiments is a bit limited, and it\u2019s not clear the proposed method for improving this is significantly better than baselines.\", \"Questions\": \"- How does your proposed method compare to introducing the model paraphrases during initial instruction finetuning? Training on the paraphrased versions alone after the instruction tuning may cause forgetting in an undesirable way.\\n- Is the degradation of performance on unobserved prompts statistically significant? The variance seems very high.\\n- Doing prompt tuning alone seems to often perform very similarly to PT+KL, and within the reported standard error in Appendix B.3. Do you have statistical tests for significance for your results?\\n- It would be interesting to see the results in Table 7 broken down into the BBL QA/BC/MC categories shown in Table 6 to see how much the difference in closest distance affects the performance of these different categories.\"}"}]}
{"paper_id": "B21c9hT1D7", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nWe investigate the high-dimensional properties of robust regression estimators in the presence of heavy-tailed contamination of both the covariates and response functions. In particular, we provide a sharp asymptotic characterisation of M-estimators trained on a family of elliptical covariate and noise data distributions including cases where second and higher moments do not exist. We show that, despite being consistent, the Huber loss with optimally tuned location parameter $\\delta$ is suboptimal in the high-dimensional regime in the presence of heavy-tailed noise, highlighting the necessity of further regularisation to achieve optimal performance. This result also uncovers the existence of a curious transition in $\\delta$ as a function of the sample complexity and contamination. Moreover, we derive the decay rates for the excess risk of ridge regression. We show that, while it is both optimal and universal for noise distributions with finite second moment, its decay rate can be considerably faster when the covariates' second moment does not exist. Finally, we show that our formulas readily generalise to a richer family of models and data distributions, such as generalised linear estimation with arbitrary convex regularisation trained on mixture models.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1- The motivation to study the asymptotic performance of robust classifiers in the realistic regime, where the dimension is of the same order as the sample size, is interesting. \\n\\n2- The results and intuitions derived seem interesting and new, and the theoretical and empirical predictions are in agreement with the experiments carried out.\\n\\n3- The generality of the results, which can be applied to broader distributions such as Gaussian mixture model, is also an advantage, as is the independence of the formulae from any function used for robust regression.\", \"Weaknesses\": \"1- The presentation could be improved, particularly the ratings. Given the wide range of notations, a section explaining the notational conventions would have been ideal, as would the rigour of defining all the notations and pointing them out. Although there is a section discussing the results, it would have been interesting to discuss results 2.4 and 2.6 a little better by showing particular cases where the solution is closed (such as ridge regression in a contaminated or uncontaminated case), and to derive the major changes in the theoretical formulae progressively.\\n\\n2- It is a pity that the asymptotic performance depends on the true regression parameter, which limits its practical use (for example on real data) for optimizing the regularization parameters and loss in particular. The authors could have addressed the issue of estimation.\\n\\n3- The results derived by the authors seem non-rigorous, although several attempts in the literature have made them fairly rigorous. It would be interesting to discuss the reasons why it is not possible to derive rigorous results and, above all, to explain why it is not possible to derive rigorous results. It would be interesting to discuss the reasons why it is not possible to derive rigorous results and above all to show to what extent the theoretical contribution of the paper is different and challenging from previous works.\", \"Questions\": \"1- The paper mentions that the contamination model applies to both the covariates and the responses. However, in the definition of the model there is more mention of contamination on the covariates, which obviously indirectly induces contamination on the responses. But written this way, we would expect additional contamination on the regression noise.\\n\\n2- Given the complexity of the expressions, it is important to simplify the notations as much as possible and to avoid defining unnecessary quantities and also to clearly define those which are used. Unless I'm mistaken, this is not always the case. For example in the result 2.4,  $\\\\xi \\\\sim \\\\mathcal{N}(0, I_d)$ and $z$ seem to be defined but never used. Likewise, the integration variables $\\\\eta$ and $\\\\tau$ are not properly defined. The notation $\\\\partial_w$ and $\\\\partial_\\\\mu$ are not clear since we do not know what $w$ and $\\\\mu$ refer to.\\n\\n3- The results depend on the real parameter $\\\\beta_\\\\star$, which limits the application of the bound on real data. Can the authors comment on when to estimate asymptotic performance to carry out, for example, model selection, ...\\n\\n\\n4- Section 4 could have been reorganized differently to provide more detailed explanation and experiments. In substance the result is, with the exception of one or two notations, the same as that of the main result. It is therefore in my opinion unnecessary to present it again. It can be put in an appendix by highlighting in the main paper what are the differences with the main result and deriving the main complementary intuitions.\\n\\n5- The lack of rigor of the results still remains one of the weaknesses but it is very little discussed. What else prevents you from having rigorous results? What are the blocking passages?\\n\\nAfter rebuttal, I read the answer of the authors. The authors promise to enhance the readability of the paper by providing notations section, by illustrating and discussion specific particular cases. I think this will allow to reach a wider audience. But I still think there are more to do in this sense. I will advise authors to progressively help the reader to grasp all the cumbersome notation by explaining before the theorem each main variables and giving intuitive explanation. How they behave in the asymptotic case (p fixed and n large)? What are the effects of the large dimensional corrections, and to particularly discuss notations that are specific to the heavy tailed analysis and that don't appear in the non heavy tailed analysis by raising some limiting cases. I think the problem of estimation of the signal to noise ratio limits the practical application of the paper and even though the authors claim it s a standard problem for regression problem, I think it is important to make the nice theoretical analysis practical even by providing some guidelines from the theory or from the analysis.\"}"}]}
{"paper_id": "uNrFpDPMyo", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nIn this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss. We will release our code and the compatible CUDA kernel for reproducibility.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 4, \"LLMs\": 1, \"Strengths\": \"- The paper introduces valuable insights drawn from LLMs: 1. Different structure in different attention 2. The same head structures persist. These insights are well-supported with empirical data and references to existing literature. \\n\\n- The authors leverage these insights to come up with an effective compression method that adapts to the structure of each attention head. The results show consistent compression rate and model quality improvement over prior SoTA fixed compression mechanisms.\", \"Weaknesses\": \"- The paper could benefit from presenting actual GPU inference performance results using FastGen and comparing them with other compression methods. Additionally, providing a runtime breakdown would offer more insights into the overhead caused by the profiling, compression, and decompression processes.\\n- It would be nice to look into the structure of KV in the multi-query attention design.\", \"Questions\": \"-\"}"}]}
{"paper_id": "gqzknNJShg", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nResource-constrained vision tasks, such as image classification on low-end devices, put forward significant challenges due to limited computational resources and restricted access to a vast number of training samples. Previous studies have utilized data augmentation that optimizes various image transformations to learn effective lightweight models with few data samples. However, these studies require a calibration step for optimizing data augmentation to specific scenarios or hardly exploit frequency components readily available from Fourier analysis. To address the limitations, we propose a frequency-based image encoding method, namely FourierAugment, which allows lightweight models to learn richer features with a restrained amount of data. Further, we reveal the correlations between the amount of data and frequency components lightweight models learn in the process of designing FourierAugment. Extensive experiments on multiple resource-constrained vision tasks under diverse conditions corroborate the effectiveness of the proposed FourierAugment method compared to baselines.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"1. The introduction of FourierAugment as a frequency-based image encoding method represents a novel and innovative approach in addressing resource-constrained vision tasks. FourierAugment's unique utilization of frequency components sets it apart from conventional data augmentation methods.\\n2. The paper's empirical study is thorough and well-designed, ensuring the validity and reliability of the results. Extensive experiments on multiple datasets and resource-constrained vision tasks provide a strong foundation for the paper's claims.\\n3. The paper is well-structured, with a clear presentation of the problem statement, method development, empirical study, and results.\", \"Weaknesses\": \"The paper could benefit from a more comprehensive theoretical background and motivation section. It's important to provide a clear foundation for why FourierAugment was developed and the specific theoretical underpinnings. A deeper exploration of the relationship between frequency components and lightweight model learning could enhance the paper's overall coherence.\", \"Questions\": \"1. The theoretical underpinnings and motivations for proposing FourierAugment, including the specific theoretical underpinnings that guided its development, would add to the coherence of the paper if they were added in full.\\n2. The n value is chosen empirically? Is there a definite basis for setting 2 or 3?\\n3. How does the method proposed in this paper compare to the comparative baseline AugMix, RandAugment and Deep AutoAugment in terms of computational efficiency and generalizability to different visual tasks?\\n4. Does adding the image enhancement proposed in this paper to the image classification and FSCIL tasks significantly extend the processing time of the tasks?\"}"}]}
{"paper_id": "0jsfesDZDq", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nRecurrent Spiking Neural Networks (RSNNs) have emerged as a computationally efficient and brain-inspired machine learning model. The design of sparse RSNNs with fewer neurons and synapses helps reduce the computational complexity of RSNNs. Traditionally, sparse SNNs are obtained by first training a dense and complex SNN for a target task and, next, eliminating neurons with low activity (activity-based pruning) while maintaining task performance. In contrast, this paper presents a task-agnostic methodology for designing sparse RSNNs by pruning an untrained (arbitrarily initialized) large model. \nWe introduce a novel Lyapunov Noise Pruning (LNP) algorithm that uses graph sparsification methods and utilizes Lyapunov exponents to design a stable sparse RSNN from an untrained RSNN. We show that the LNP can leverage diversity in neuronal timescales to design a sparse Heterogeneous RSNN (HRSNN). Further, we show that the same sparse HRSNN model can be trained for different tasks, such as image classification and time-series prediction. The experimental results show that, in spite of being task-agnostic, LNP increases computational efficiency (fewer neurons and synapses) and prediction performance of RSNNs compared to traditional activity-based pruning of trained dense models.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"The LNP enables pruning the network prior to training, and effectively enhancing the sparsity while maintaining commendable performance.\", \"Weaknesses\": \"1. The authors should elucidate the motivation behind their proposed method. What significance does their method hold for the field of spiking neural networks?\\n2. The network model and task employed in the experiments are too simple and do not need pruning. I am more intrigued by the adaptability of the LNP method under more intricate situations.\\n3. The authors provided a detailed description of the latest pruning works in SNN in the second paragraph of the Introduction. However, their results are not referenced in Table 1. Meanwhile, the compared works in Table 1 lack reference links.\\n4. The equation 3 appears abrupt. I don\u2019t understand why the spike frequency of untrained SNN can be used to construct this linearized model. And how to calculate the matrix D and L?\\n5. Activity Pruning needs to provide citation or explanation.\\n6. Lorenz and Rossler are unfamiliar datasets in the field of SNN. Could you please provide the citations?\", \"Questions\": \"See the weakness please.\"}"}]}
{"paper_id": "9GviaQcGnx", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nIn this work, we present constrained parameter regularization (CPR), an alternative to traditional weight decay. Instead of applying a constant penalty uniformly to all parameters, we enforce an upper bound on a statistical measure (e.g., the L2-norm) of parameter groups. Consequently, learning becomes a constraint optimization problem, which we address by an adaptation of the augmented Lagrangian method. This formulation permits varying regularization strengths for each parameter group, eliminating the need for explicit penalty coefficients for regularization terms. CPR only requires two hyperparameters and incurs no measurable runtime overhead. Additionally, we propose a simple but efficient mechanism to adapt the upper bounds during the optimization. We provide empirical evidence of CPR's efficacy in experiments on the ``grokking'' phenomenon, computer vision, and language modeling tasks. Our results demonstrate that CPR counteracts the effects of grokking and consistently matches or outperforms traditional weight decay.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"1\uff09 The authors introduce a constrained parameter regularization that does not uniformly \\napply a constant penalty to all parameters.\\n2\uff09 The authors provide an open-source implementation of CPR, which can be easily adjusted \\nby replacing the optimizer class.\", \"Weaknesses\": \"1\uff09 The authors should provide a clearer exposition of the problem statement and research \\nmotivation in the abstract.\\n2\uff09 In the related work section, the authors should provide a more comprehensive review of \\nprior research.\\n3\uff09 The experimental results conducted by the authors did not show a significant \\nimprovement in optimization performance, and they did not provide additional \\nexperimental results or comparisons related to existing work.\\n4\uff09 While applying different regularization strengths to each parameter group may seem \\nintriguing, the authors have not discussed the necessity and potential advantages of this \\napproach compared to traditional methods.\", \"Questions\": \"N/A\"}"}]}
{"paper_id": "v5Bb7F1Wkf", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nThe dilemma between plasticity and stability presents a significant challenge in Incremental Learning (IL), especially in the exemplar-free scenario where accessing old-task samples is strictly prohibited during the learning of a new task. A straightforward solution to this issue is learning and storing an independent model for each task, known as Single Task Learning (STL). Despite the linear growth in model storage with the number of tasks in STL, we empirically discover that averaging these model parameters can potentially preserve knowledge across all tasks. Inspired by this observation, we propose a Dual-Learner framework with Cumulative Parameter Averaging (DLCPA). DLCPA employs a dual-learner design: a plastic learner focused on acquiring new-task knowledge and a stable learner responsible for accumulating all learned knowledge. The knowledge from the plastic learner is transferred to the stable learner via cumulative parameter averaging. Additionally, several task-specific classifiers work in cooperation with the stable learner to yield the final prediction. Specifically, when learning a new task, these modules are updated in a cyclic manner: i) the plastic learner is initially optimized using a self-supervised loss besides the supervised loss to enhance the feature extraction robustness; ii) the stable learner is then updated with respect to the plastic learner in a cumulative parameter averaging manner to maintain its task-wise generalization; iii) the task-specific classifier is accordingly optimized to align with the stable learner. Experimental results on CIFAR-100 and Tiny-ImageNet show that DLCPA outperforms several state-of-the-art exemplar-free baselines in both Task-IL and Class-IL settings.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"## Presentation\\n- The paper is clearly written for the most part.\\n\\n## Contribution\\n- The core idea behind the presented method is simple.\\n- The method seems to work well on Tiny-imagenet\\n- The analysis of how the proposed stable-plastic architecture can help is nice and simple.\", \"Weaknesses\": \"## Presentation\\n- The \\\"Analysis\\\" section contains the problem definition and explanation of the baselines, which are background information. It would be better to split this information into a clear background section.\\n- The task settings \\\"Task-IL\\\" and \\\"Class-IL\\\" are never defined in the paper.\\n- The paper assumes that storing previous models or datasets is so costly that it is a big disadvantage to the memory-based methods. However, from a lay perspective, it would seem that storage is very cheap. I suggest the authors include more motivating discussion about why the memory-based IL methods have a disadvantage.\\n\\n## Contribution\\n- The self-supervised learning (SSL) objective is introduced without appropriate analysis and baselines.\\n    - From the ablation studies, we see that the SSL objective is a crucial component of the proposed method. However, I don't see how SSL is specific to the proposed method. Without applying SSL objectives to at least some of the most competitive baselines, we don't know how much of the improvement from the proposed method is actually due to the stable-plastic learner architecture.\\n    - Three different SSL losses are proposed to be used with the method. The best results in each of the datasets are split between each of the SSL objectives. On some datasets, choosing a different SSL objective would lead to the proposed method no longer beating the baselines. This seems like a weakness of the method, in that you have to train three versions of it in order to pick which one you want to deploy.\", \"Questions\": \"- Which SSL objective should a user choose?\\n- To which baselines is the SSL objective applicable?\\n    - How well would those baselines do with the SSL objective?\\n- Is there a practical task, where one of the methods considered in this paper would work well, but storing all of the data would become prohibitively expensive?\"}"}]}
{"paper_id": "jdynlBj3b0", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nSingle-positive multi-label learning (SPMLL) is a typical weakly supervised multi-label learning problem, where each training example is annotated with only one positive label. Existing SPMLL methods typically assign pseudo-labels to unannotated labels with the assumption that prior probabilities of all classes are identical. However, the class-prior of each category may differ significantly in real-world scenarios, which makes the predictive model not perform as well as expected due to the unrealistic assumption on real-world application. To alleviate this issue, a novel framework named CRISP, i.e., Class-pRiors Induced Single-Positive multi-label learning, is proposed. Specifically, a class-priors estimator is introduced, which could estimate the class-priors that are theoretically guaranteed to converge to the ground-truth class-priors. In addition, based on the estimated class-priors, an unbiased risk estimator for classification is derived, and the corresponding risk minimizer could be guaranteed to approximately converge to the optimal risk minimizer on fully supervised data. Experimental results on ten MLL benchmark datasets demonstrate the effectiveness and superiority of our method over existing SPMLL approaches.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 3, \"Contribution\": 4, \"Rating\": 8, \"Confidence\": 3, \"Strengths\": \"1. Strong empirical performance\\n2. The proposed method is theoretically principled with a convergence guarantee\\n3. The unbiased risk estimator is simple and intuitive\", \"Weaknesses\": \"1. Clarity of writing. I found the problem setting to be unclear until I finished section 4.  One suggestion would be to add more details about the setup in the preliminary setting e.g. the absolute loss function, before diving into deriving the estimators. Also, changing the order by deriving the risk estimator before the class-priors, may provide a better motivation on why we need to estimate the class-priors.\", \"Questions\": \"1.  The algorithm relies on iteratively estimating the class-prior from f and then using it to update f. Is it possible if there is a failure mode ?\\n2.  Would it be possible to extend this type of estimator to a different loss than the absolute loss?\"}"}]}
{"paper_id": "EjIKerYk1O", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nA digital tower, a cost-effective alternative to physical air traffic control towers, is expected to provide video-sensor-based surveillance, which is particularly advantageous for small airports. To fully realize this potential, advanced computer vision algorithms play a crucial role in effective airside monitoring. While current research primarily focuses on tracking aircraft on airport surfaces, an equally vital aspect is the real-time observation of approaching aircraft on the runway. This capability holds a pivotal role in augmenting both airport and runway operations. In this context, the study introduces a real-time deep learning approach to accurately estimate the distance-to-touchdown of approaching aircraft, covering distances of up to 10 nautical miles. The approach overcomes the limitations of monoscopic and stereoscopic methods by utilizing multi-view video feeds from digital towers. It integrates Yolov7, an advanced real-time object detection model, with auxiliary regression auto-calibration, enabling real-time tracking and feature extraction from diverse camera viewpoints. Subsequently, an ensemble approach utilizing an LSTM model is proposed to combine input vectors, resulting in precise distance estimation. Notably, this approach is designed for easy adaptation to various camera system configurations within digital towers. The model's effectiveness is assessed using simulated and real video data from Singapore Changi Airport, demonstrating stability across scenarios with low predictive errors (Mean Absolute Percentage Error = 0.18%) up to 10 nautical miles under visual meteorological conditions. These capabilities within a digital tower environment can significantly enhance the controller's ability to manage runway sequencing and final approach spacing, ultimately leading to remarkable airport efficiency and safety improvements.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"1. Distance estimation is a challenging task.  \\n2. This paper is easy to understand.\", \"Weaknesses\": \"1. The model design is not novel, which has limited technical learning.   \\n2. The dataset is not available. Then it cannot be a part of contribution.  \\n3. Calibaration network is a little strange. Why it can align two views without knowing the related position for the two cameras? If the two camera's position is changed, can this model still work?   \\n4. The number of views in the dataset is only 2.  The statement of \\\"multi-view\\\" is unsoundness. Author should increase the view number.  \\n5. The paper writing should be further improved. Besides, figure in the manuscript should be the vector figure (Most figures are blur).\", \"Questions\": \"Refer to the weakness.\"}"}]}
{"paper_id": "dyG2oLJYyX", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nExisting NAS methods suffer from either an excessive amount of time for repetitive sampling and training of many task-irrelevant architectures. To tackle such limitations of existing NAS methods, we propose a paradigm shift from NAS to a novel conditional Neural Architecture Generation (NAG) framework based on diffusion models, dubbed DiffusionNAG. Specifically, we consider the neural architectures as directed graphs and propose a graph diffusion model for generating them. Moreover, with the guidance of parameterized predictors, DiffusionNAG can flexibly generate task-optimal architectures with the desired properties for diverse tasks, by sampling from a region that is more likely to satisfy the properties. This conditional NAG scheme is significantly more efficient than previous NAS schemes which sample the architectures and filter them using the property predictors. We validate the effectiveness of DiffusionNAG through extensive experiments in two predictor-based NAS scenarios: Transferable NAS and Bayesian Optimization (BO)-based NAS. DiffusionNAG achieves superior performance with speedups of up to 35$\\times$ when compared to the baselines on Transferable NAS benchmarks. Furthermore, when integrated into a BO-based algorithm, DiffusionNAG outperforms existing BO-based NAS approaches, particularly in the large MobileNetV3 search space on the ImageNet 1K dataset. Code is available at https://github.com/CownowAn/DiffusionNAG.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"This paper is well-motivated. Previous work only uses the diffusion model to model undirected graphs for neural architecture generation. Using the diffusion model for modeling the directed graph is needed. Moreover, classifier guidance is well suited for the Transferable NAS and substitute step 3) in BO-based NAS approaches, where the dataset and the pre-trained classifier are given as the setup.\", \"Weaknesses\": \"1.\\\"Whether diffusion model for neural architecture generation is more efficient than Mutation + Random\\\" is a good question. It is important to ask \\\"when they are better\\\".\\n\\n* The strength of the predictor guidance is an important hyperparameter. In this paper, a good performance can be achieved by cross-validation. Could the author provide some ablation studies of how this hyperparameter affects the performance? How could we effectively define the search range for cross-validation?\\n\\n* Will using the diffusion model for neural architecture generation have some model collapse phenomenon? Does the Diffusion model only generate some similar architecture, and achieve the better \\\"worse case accuracy\\\" by sacrificing the diversity? If that is the case, any explore and exploit framework for it?\", \"Questions\": \"Please see the Weaknesses part.\"}"}]}
{"paper_id": "aAE44ivBtx", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nWe present a certified defense to clean-label poisoning attacks. These attacks work by injecting poisoning samples that contain $p$-norm bounded adversarial perturbations into the training data to induce a targeted misclassification of a test-time input. Inspired by the adversarial robustness achieved by $denoised$ $smoothing$, we show how a pre-trained diffusion model can sanitize the training data before a model training. We extensively test our defense against seven clean-label poisoning attacks and reduce their attack success to 0-16\\% with only a small drop in the test time accuracy. We compare our defense with existing countermeasures against clean-label poisoning, showing that the defense reduces the attack success the most and offers the best model utility. Our results highlight the need for future work on developing stronger clean-label attacks and using our certified yet practical defense as a strong baseline to evaluate these attacks.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The paper introduces a defense method based on DDPM to counter clean-label poisoning attacks, and it demonstrates strong performance against various attack methods.\", \"Weaknesses\": \"1. The primary results presented in Table 1 are obtained through empirical experiments rather than relying on certified defense methods that guarantee a specific radius where the model is proven to provide the correct predictions. I would suggest a mathematical formulation to provide a certified bound.\\n\\n2. Randomized smoothing has already been established as an effective technique for ensuring l2 or l_infinite robustness against various types of perturbations, including backdoor perturbations. Moreover, it offers a clear and established framework for calculating certified l2 or l_infinite robustness bounds. The novelty of this paper is somewhat limited, as its core idea revolves around utilizing randomized smoothing to provide certified defense specifically against backdoor perturbations.\\n\\n3. Experiments on the indiscriminate data poisoning [1] should also be included.\\n\\n[1] Yiwei Lu, Gautam Kamath, and Yaoliang Yu. 2023. Exploring the limits of model-targeted indiscriminate data poisoning attacks. In Proceedings of the 40th International Conference on Machine Learning (ICML'23), Vol. 202. JMLR.org, Article 949, 22856\u201322879.\", \"Questions\": \"See weakness above.\"}"}]}
{"paper_id": "1NHgmKqOzZ", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nDataset distillation aims to minimize the time and memory needed for training deep networks on large datasets, by creating a small set of synthetic images that has a similar generalization performance to that of the full dataset. However, current dataset distillation techniques fall short, showing a notable performance gap compared to training on the original data. In this work, we are the first to argue that the use of only one synthetic subset for distillation may not yield optimal generalization performance. This is because the training dynamics of deep networks drastically changes during training. Therefore, multiple synthetic subsets are required to capture the dynamics of training in different stages. To address this issue, we propose Progressive Dataset Distillation (PDD). PDD synthesizes multiple small sets of synthetic images, each conditioned on the previous sets, and trains the model on the cumulative union of these subsets without requiring additional training time. Our extensive experiments show that PDD can effectively improve the performance of existing dataset distillation methods by up to 4.3%. In addition, our method for the first time enables generating considerably larger synthetic datasets. Our codes are available at https://github.com/VITA-Group/ProgressiveDD.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 5, \"Strengths\": \"The idea of progressive dataset distillation is interesting. This can 1) capture the training dynamic of neural networks better as demonstrated by authors, 2) reduce the complexity of training the whole synthetic dataset together, and 3) serve as a strong method for slimmable dataset condensation [a].\\n\\n[a] Slimmable Dataset Condensation, CVPR 2023.\", \"Weaknesses\": \"1. Many places are unclear.\\n    * Fig. 1 needs some explanations. How do the results come in detail, like what's the IPC of each stage, and how to conduct multi-stage training? Although some of these questions are answered in the following parts, the writing is not coherent.\\n    * The networks for the next stage come from the training results with previous batches of synthetic data. In IDC, the networks are periodically re-initialized randomly. In MTT, the networks come from checkpoints of training with original datasets. We have to conduct some modifications to these baselines before using PDD. These operations are unclear.\\n2. A comment: this method makes an assumption on downstream training using synthetic datasets: models must be trained in a multi-stage way using the provided multi-stage synthetic data, which would introduce a lot of hyperparameters, especially in the cross-architecture setting and make the dataset distillation less elegant. Given that the performance gain is not significant in most cases, the practical value of the proposed method is somewhat limited.\\n3. Through the results in Tab. 6, the effect of discarding easy-to-learn examples at later stages is not significant. More evidence is necessary to demonstrate the effectiveness.\", \"Questions\": \"Please refer to the Weaknesses part. I would also like the authors to discuss more benefits of PDD as mentioned in the 1st point of Strengths in the camera-ready version or in the next revision cycle.\"}"}]}
{"paper_id": "9qtswuW5ux", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nIn recent years, graph neural networks (GNNs) have gained considerable attention as a promising approach to tackle combinatorial optimization problems.\nWe introduce a novel algorithm, dubbed QRF-GNN in the following, that leverages the power of GNNs to efficiently solve combinatorial problems which have quadratic unconstrained binary optimization (QUBO) formulation.\nIt relies on unsupervised learning and minimizes the loss function derived from QUBO relaxation.\nThe key components of the architecture are the recurrent use of intermediate GNN predictions, parallel convolutional layers and combination of artificial node features as input.\nThe performance of the algorithm was evaluated on benchmark datasets for maximum cut and graph coloring problems.\nResults of experiments show that QRF-GNN surpasses existing graph neural network based approaches and is comparable to the state-of-the-art conventional heuristics.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. Using Neural Networks to solve combinatorial optimization problems is an interesting topic.\\n2. The proposed QRF-GNN can outperform existing GNNs in solving Max-Cut and graph coloring problems.\\n3. The paper is well-written and easy to follow.\", \"Weaknesses\": \"1. The paper's novelty appears constrained, utilizing a GNN with QUBO's continuous relaxation as a loss function for combinatorial optimization. Clarification on how this differs from prior work would be beneficial.\\n\\n2. The motivation behind the specific design of the GNN, particularly the use of two different convolutional layers (SAGEConv Pool and SAGEConv mean) in the first layer, is not explained. A rationale for these choices should be provided.\\n\\n3. An ablation study is needed to substantiate the proposed GNN's superiority over existing methods. Without this, the advantage of the proposed model remains unclear.\", \"Questions\": \"Please refer to the weaknesses.\"}"}]}
{"paper_id": "lBY65YaAho", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nTraining large transformers using next-token prediction has given rise to groundbreaking advancements in AI. \nWhile this generative AI approach has produced impressive results, it heavily leans on human supervision. \nEven state-of-the-art AI models like ChatGPT depend on fine-tuning through human demonstrations, demanding extensive human input and domain expertise. This strong reliance on human oversight poses a significant hurdle to the advancement of AI innovation.\nTo address this limitation, we propose a novel paradigm termed Exploratory AI (EAI) aimed at autonomously generating high-quality training data. \nDrawing inspiration from the principles of unsupervised reinforcement learning (RL) pretraining, EAI achieves exploration within the natural language space. We accomplish this by harnessing large language models to assess the novelty of generated content. Our approach employs two key components: an actor that generates novel content and a critic that evaluates the generated content, offering critiques to guide the actor. \nEmpirical evaluations demonstrate that EAI significantly boosts model performance on complex reasoning tasks, addressing the limitations of human-intensive supervision.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"The problem of having less human involvement in finetuning is an important topic. The link between unsupervised RL and this problem is very interesting. The proposed method seems to work on the experiment.\", \"Weaknesses\": \"1. To start with, I think the method is still relying a lot on human insights especially on some exact ways of generating \\\"diverse\\\" methods. The argument that this is a general method is not well supported enough, and I would love to see more experiments on different kinds of benchmark.\\n\\n2. The paper is missing ablation to see how exactly the critic help the results. In other words, it needs to compare with similar methods like self-instruct.\\n\\n3. While the diversity experiment in Fig3, it would be more interesting in seeing more visualization. Since you already have the embeddings, maybe do a T-SNE plot or something to better prove the diversity\\n\\n4. Finally, the method seems to have very weak link to unsupervised RL pretraininig, and in fact the model is not trained at all but simply prompted. And I don't see any thing related \\\"learning skills\\\".\", \"Questions\": \"See above\"}"}]}
{"paper_id": "Kfpaq5CJPy", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nBoth classical and learned image transformations such as the discrete wavelet transforms (DWTs) and flow-based generative models provide semantically meaningful representations of images. In this paper, we propose a general method for robustness exploiting the expressiveness of image representations by targeting substantially low-dimensional subspaces inside the $L^\\infty$ box. Experiments with DCT, DWTs and Glow produce adversarial examples that are significantly more similar to the original than those found considering the full  $L^\\infty$ box. Further, through adversarial training we show that robustness under the introduced constraints transfers better to robustness against a broad class of common image perturbations compared to the standard  $L^\\infty$ box, without a major sacrifice of natural accuracy.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"The idea presented in the paper is very interesting and novel. Converting the adversarial attack problem into a linear programming problem and using adequate tools to solve it is a good catch. The authors have explained well the problems encountered during making such a conversion and strategies used to overcome it. E.g. using barrier method to remove need for projection, using first-order update rule instead of the usual minimization, dealing with discontinuity of $L^{\\\\infty}$ norm. Overall the problem is well formulated and explained.\\n\\nThe flow of the paper is clear and easy to understand. In terms of significance, this reformulation of adversarial attack is interesting and might be for the useful for the community to look at.\", \"Weaknesses\": \"The idea presented is novel but the experiments do not back it up well. \\n\\n1. The method finds examples that are less perturbed than other methods compared but the attack success rate is not upto par with other methods. In fact it is very low in some of the image representations used. The presented the trade-off between distance from original image and success rate does not show it to be very effective attack method. You can't really visually tell the difference between the adversarial images generated from this attack and PGD.\\n2. The attack methods are only compared on CIFAR - Imagenet would be a good addition as datapoint to the experiments.\\n3. No mention of the cost of attack, number of steps to reach the adv example and time taken by barrier method optimization.\\n4. For adversarial training, comparison has only been made with PGD. I believe other methods should be used in the comparison. \\n5. Again ImageNet not used for adversarial training, would be a good experiment to add.\\n\\nOverall the experiments do not showcase the effectiveness of method as adversarial attack or for training robust models compared to other state of the art methods. \\n\\nTypo:\\n\\nEqn 2 : $R^n$ -> $R^q$ ?\", \"Questions\": \"See weaknesses\"}"}]}
{"paper_id": "cHy00K3Och", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nThe rise in size and complexity of modern datasets and deep learning models have resulted in the usage of extensive computational resources and a rise in training time and effort. It also has increased the carbon footprint of training and fine-tuning models. One way to reduce the computational requirement is to extract the most representative subset (referred to as $\\textit{coreset}$) that can substitute for the larger dataset. Coresets can thus replace huge datasets to train models and tune hyperparameters, especially in the early stages of training. This will result in a significant reduction of computational resource requirement and reduce carbon footprint. We propose a simple and novel framework based on the similarity of loss gradients for identifying the representative training instances as a coreset. Our method, dubbed as $\\textit{GradSimCore}$, outperforms the state-of-the-art coreset selection algorithms on popular benchmark datasets ranging from MNIST to ImageNet. Because of its simplicity and effectiveness, our method is an important baseline for evaluating the effectiveness of the coreset selection algorithms. Anonymized codes for the proposed baseline are provided at https://anonymous.4open.science/r/GradSimCore-8884\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 1, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"This article is that it provides links to the code\", \"Weaknesses\": \"See the summary.\", \"Questions\": \"No question! I suggest the author should not resubmit a manuscript like this again.\"}"}]}
{"paper_id": "98g9NdJPxm", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nFederated learning is an emerging collaborative learning paradigm that aims to protect data privacy. Unfortunately, recent works show that federated learning algorithms are vulnerable to data reconstruction attacks, and a series of follow-up works are proposed to enhance the attack effectiveness. However, existing works lack of a theoretical understanding on to what extent the devices' data can be reconstructed and the effectiveness of these attacks cannot be compared theoretically. To address it, we propose a theoretical framework to understand data reconstruction attacks to FL. Our framework involves bounding the data reconstruction error and an attack's error bound reflects its inherent attack effectiveness. Under the framework, we can theoretically compare the effectiveness of existing attacks. For instance, our experimental results on multiple datasets validate that the iDLG data reconstruction attack inherently outperforms the DLG attack.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The clarity is good. The flow of the writing is smooth.\\n2. In the introduction section, the paper points out three meaningful limitations of existing attack methods: high sensitivity to the initialization; evaluation highly depending on the choice of model snapshot; and lack of theoretical analysis.\", \"Weaknesses\": \"My main concern is about the significance. In details\\n1. Theorems on the attack upper bound need unrealistic assumptions.\\n-  The FL model is required to be strongly convex and smooth, etc. This is far away from the scenario where those attack methods are located -- those attack methods are commonly evaluated for deep neural networks as the input is the image in the evaluation.\\n- The reconstruction method is assumed to have a Lipschitz constant. However, the attack methods studied in the paper are optimization-based attacks and do not have an explicit Lipschitz constant. Although the paper proposes a method to approximately calculate the constant, there's no guarantee how large the approximation error would be. This might introduce an additional arbitrarily large noise to calculate the upper bound.\\n2. As shown in the empirical results, there is a large gap between the upper bound and the attack performance. It might be hard to indicate a useful conclusion from this bound.\", \"Questions\": \"Please see the weakness above.\"}"}]}
{"paper_id": "lWe3GBRem8", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nIt is desirable for policies to optimistically explore new states and behaviors during online reinforcement learning (RL) or fine-tuning, especially when any prior offline data does not provide enough state coverage. However, exploration bonuses can bias the learned policy, and our experiments find that na\\\"ive, yet standard use of such bonuses can fail to recover a performant policy. Concurrently, pessimistic training in offline RL has enabled recovery of performant policies from static datasets. Can we leverage offline RL to recover better policies from online interaction? We make a simple observation that a policy can be trained from scratch on all interaction data with pessimistic objectives, thereby decoupling the policies used for data collection and for evaluation. Specifically, we propose the Offline-to-Online-to-Offline (OOO) framework for reinforcement learning (RL), where an optimistic (_exploration_) policy is used to interact with the environment, and a _separate_ pessimistic (_exploitation_) policy is trained on all the observed data for evaluation. Such decoupling can reduce any bias from online interaction (intrinsic rewards, primacy bias) in the evaluation policy, and can allow more exploratory behaviors during online interaction which in turn can generate better data for exploitation. OOO is complementary to several offline-to-online RL and online RL methods, and improves their average performance by 14\\% to 26\\% in our fine-tuning experiments, achieves state-of-the-art performance on several environments in the D4RL benchmarks, and also improves online RL performance by 165\\% on two OpenAI gym environments. Further, OOO RL can enable fine-tuning from incomplete offline datasets where prior methods can fail to recover a performant policy.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"The paper is well-written and easy to follow. The proposed method is overall reasonable.  Besides, I really like the topic raised by the author. How to use offline data for safe and efficient online interaction to find a global optimal policy is a key issue. Currently, pessimistic training in offline RL indeed enabled the recovery of performant policies from static datasets. Thus, I think the topic is timely to be proposed, as it can be the potential last step of the offlineRL paradigm to ground RL in real-world applications.\", \"Weaknesses\": \"1. some important baselines seem missed: baseline of offlineRL + exploration (without decoupling) should be added; some offlineRL + decoupled exploration methods should be added (see below);\\n2. some related works should be discussed: decoupling the exploration policies and the exploitation policies (or target policies) is not a new idea in standard exploration studies. There are indeed many works [1,2,3,4] that have discussed this problem. There should be a more formal discussion on the differences between the decoupling in offlineRL and off-policy RL beyond the difference in the gradient computation methods they used.\\n3. the evidence of exploration bonuses bias in the learned policy should be discussed more explicitly. Maybe giving some visualizations for OOO and a standard offlineRL + RND algorithm is good.\\n4. In Figure 3, why use \\\"frozen\\\" RND  and a baseline to demonstrate the effects of exploration bias?\\n5. I am a bit confused about the paragraph \\\"What explains the improved performance in OOO RL?\\\". How can we derive the conclusion that \\\"These ablations suggest that mitigating the exploration bias by removing the intrinsic reward when training the exploitation policy leads to improved performance under OOO\\\" just by excluding the two hypotheses?\\n\\n\\n[1] Decoupled Exploration and Exploitation Policies for Sample-Efficient Reinforcement Learning. 2021.\\n\\n[2] Off-policy Reinforcement Learning with Optimistic Exploration and Distribution Correction. 2021.\\n\\n[3] Curiosity-Driven Exploration for Off-Policy Reinforcement Learning Methods. 2019.\\n\\n[4] Reinforcement Learning with Derivative-Free Exploration. 2019.\", \"Questions\": \"An open question for discussion: What do you think are the essential differences, the extra challenges, and the benefits of the standard exploration problem in online RL and the exploration problem in offline-to-online RL?\"}"}]}
{"paper_id": "Cw6lk56w6z", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nIn-context learning (ICL) has become the default method for using large language models (LLMs), making the exploration of its limitations and understanding the underlying causes crucial. In this paper, we find that ICL falls short of handling specification-heavy tasks, which are tasks with complicated and extensive task specifications, requiring several hours for ordinary humans to master, such as traditional information extraction tasks. The performance of ICL on these tasks mostly cannot reach half of the state-of-the-art results. To explore the reasons behind this failure, we conduct comprehensive experiments on $18$ specification-heavy tasks with various LLMs and identify three primary reasons: inability to specifically understand context, misalignment in task schema comprehension with humans, and inadequate long-text understanding ability. Furthermore, we demonstrate that through fine-tuning, LLMs can achieve decent performance on these tasks, indicating that the failure of ICL is not an inherent flaw of LLMs, but rather a drawback of existing alignment methods that renders LLMs incapable of handling complicated specification-heavy tasks via ICL. To substantiate this, we perform dedicated instruction tuning on LLMs for these tasks and observe a notable improvement. We hope the analyses in this paper could facilitate advancements in alignment methods enabling LLMs to meet more sophisticated human demands.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"1. The paper highlights an interesting challenge to the in-context-learning paradigm that even today's most powerful LLMs struggle with. \\n\\n2. The paper presents promising initial results that demonstrates the potential of addressing the challenge of specification-heavy tasks via more focused alignment.\", \"Weaknesses\": \"1. The success of the ICL approach highly depends on the amount and quality of information provided in the prompts. The results in table 1 are all based on a shortened version of the task description. Examples in the appendix show that such shortened descriptions are too concise, e.g., they do not even contain a single sentence definition of each label. The provided few-shots do not cover all labels. Longer prompts can still fit in at least a subset of the LLMs used for the experiments. The paper needs to experiment with more detailed prompts to confirm that the poor accuracy of the ICL approach is not due to the very concise set of instructions.\\n\\t \\n2. The alignment experiments in Section 4.2 only hold out one task at a time, but still tasks of the same type are included in the alignment dataset. The paper needs to report results with a whole task type (e.g., All 4 RE tasks) is held out to confirm that the gain in accuracy is not really due to similarity between tasks.\", \"Questions\": \"1. Comparing table 1 to table 4, Aligned ICL has worse accuracy than the unaligned  baseline on 3 tasks MAVEN-Subevent and the two sentiment analysis tasks. Do you have an explanation or an intuition for why that is the case for those three datasets?\\n\\t\\n2. In Section 3.1, the paper says that \\\"LLMs ignore all the contexts in 18 instances\\\". How did you find out about that?\"}"}]}
{"paper_id": "eADuHv62J5", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nLong-tailed data distributions are prevalent in a variety of domains, including e-commerce, finance, biomedical science, and cyber security. In such scenarios, the performance of machine learning models is often dominated by the head categories, while the learning of tail categories is significantly inadequate. Given abundant studies conducted to alleviate the issue, this work aims to provide a systematic view of long-tailed learning with regard to three pivotal angles: (A1) the characterization of data long-tailedness, (A2) the data complexity of various domains, and (A3) the heterogeneity of emerging tasks. To achieve this, we develop the most comprehensive (to the best of our knowledge) long-tailed learning benchmark named HeroLT, which integrates 15 state-of-the-art algorithms and 6 evaluation metrics on 16 real-world benchmark datasets across 5 tasks from 3 domains. HeroLT with novel angles and extensive experiments (304 in total) enables researchers and practitioners to effectively and fairly evaluate newly proposed methods compared with existing baselines on varying types of datasets. Finally, we conclude by highlighting the significant applications of long-tailed learning and identifying several promising future directions. For accessibility and reproducibility, we open-source our benchmark HeroLT and corresponding results at https://anonymous.4open.science/r/HeroLT-9746/.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The long tail problem is a practical and valuable research topic.\\n2. Benchmark contains different data types and tasks.\", \"Weaknesses\": \"1. Most of the datasets and tasks seem to have been well constructed and the paper seems to merely concatenate them together without much additional integration.\\n2. The methods discussed in the paper appear to be somewhat dated. For instance, the image classification data that is prominently featured in Table 2 is based on methodologies that were largely developed before 2020. Considering that it is already near the end of 2023, this renders the insights provided by the paper less credible.\\n3. Referring to section C.1 of the paper, there is a misalignment in the model architectures and the number of epochs used when comparing different methods, which calls into question the article's claim of a 'fair evaluation'.\\n4. According to the four-point analysis in the upper half of the third page, the Pareto-LT Ratio still requires integration with the Gini coefficient and IF to describe the dataset. The former primarily represents the number of categories, while the latter two indicate data imbalance. So why not directly use the number of categories as a descriptor? Additionally, I do not find comprehensive experimental and theoretical evidences to support this four-point analysis.\", \"Questions\": \"See above.\"}"}]}
{"paper_id": "SL7djdVpde", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nWith the arrival of the Noisy Intermediate-Scale Quantum (NISQ) era, Variational Quantum Algorithms (VQAs) have emerged as popular approaches to obtain possible quantum advantage in the relatively near future. In particular, how to effectively incorporate the common symmetries in physical systems as hard constraints in VQAs remains a critical and open question. In this paper, we revisit the Hamming Weight (HW) preserving ansatz and establish the links from ansatz to various symmetries and constraints, which both enlarges the usage of HW preserving ansatz and provides a coherent solution for constrained VQAs. Meanwhile, we utilize the quantum optimal control theory and quantum overparameterization theory to analyze the capability and expressivity of HW preserving ansatz and verify these theoretical results on unitary approximation problem. We conduct detailed numerical experiments on two well-studied symmetry-preserving problems, namely ground state energy estimation and feature selection in machine learning. The superior performance demonstrates the efficiency and supremacy of the proposed HW preserving ansatz on constrained VQAs.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"- This paper nicely summarizes and makes clear the idea of using Hamming weight (or in physics terminology, excitation) preserving ansatz for variational quantum algorithms. The numerical results verify the accuracy of the theory, which characterizes the expressibility of these ansatz.\\n- The experiments are nice, showing a modest improvement over existing methods.\", \"Weaknesses\": \"- Typos: \\\"relative\\\" should be \\\"relatively\\\" throughout\\n- Typo: In Figure 2, \\\"Haar measurement\\\" should be \\\"Haar measure\\\"\\n- Typo: \\\"Hartree-Fork\\\"\\n- Based on common gate sets of real machines, I feel the BS gate is quite artificial. It might make more sense to use parameterized $XX$ rotations with single-qubit $Z$ rotations. If the intention is to perform VQAs on near-term machines, then one might want to use more native instructions than the decompositions shown in Fig 5 and Fig 6.\\n- Related to the above point, the decomposition of the BS gate is addressed as a possible limitation. I am pretty sure a better decomposition is possible using $XX$, $YY$ and $ZZ$ rotations.\\n- I think this idea of preserving the Hamming weight is not very new, and the theoretical results are not very surprising to me. For this reason, I feel the contribution of this work is not so large, although the proposed ansatz appear to perform better than existing ones.\", \"Questions\": \"- In Equation 8, should the maximization be over $M$ such that $M\\\\geq M_c$?\\n- Where does the construction for the BS gate come from?\\n- Throughout this paper, it is stated that this is a \\\"revisiting\\\" of the Hamming weight preserving ansatz. What is the status of the previous work using these ansatz, and what are the specific improvements made in this paper compared to previous proposals?\"}"}]}
{"paper_id": "ulMXGO1fdH", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nFacilitating online learning in spiking neural networks (SNNs) is a key step in developing event-based models that can adapt to changing environments and learn from continuous data streams in real-time. Although forward-mode differentiation enables online learning, its computational requirements restrict scalability. This is typically addressed through approximations that limit learning in deep models. In this study, we propose Online Training with Postsynaptic Estimates (OTPE) for training feed-forward SNNs, which approximates Real-Time Recurrent Learning (RTRL) by incorporating temporal dynamics not captured by current approximations, such as Online Training Through Time (OTTT) and Online Spatio-Temporal Learning (OSTL). We show improved scaling for multi-layer networks using a novel approximation of temporal effects on the subsequent layer's activity. This approximation incurs minimal overhead in the time and space complexity compared to similar algorithms, and the calculation of temporal effects remains local to each layer. We characterize the learning performance of our proposed algorithms on multiple SNN model configurations for rate-based and time-based encoding. OTPE exhibits the highest directional alignment to exact gradients, calculated with backpropagation through time (BPTT), in deep networks and, on time-based encoding, outperforms other approximate methods. We also observe sizeable gains in average performance over similar algorithms in offline training of Spiking Heidelberg Digits with equivalent hyper-parameters (OTTT/OSTL \u2013 70.5%; OTPE \u2013 75.2%; BPTT \u2013 78.1%).\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"1. This paper considers facilitating online learning in SNNs, which is important for biological plausibility and neuromorphic hardware.\\n\\n2. Experiments demonstrate large improvements on gradient angle alignment and better performance than existing methods.\", \"Weaknesses\": \"1. The presentation can be improved. It is not quite clear what is the major component in the methodology that leads to the large improvement over existing methods. From Section 3, OTPE is very similar to OSTL and there is no explicit description of the key difference. From Section 3.1, AOTPE differs from OTTT only in that AOTPE uses a running weighted average of surrogate derivatives. Why this small change can almost double the cosine similarity given the similar time and space complexity? Is there more formal theoretical justification? From the derivation, OTPE also makes many approximations, why these approximations have less effect than OSTL/OTTT? Besides, what does the term \u201cpostsynaptic estimates\u201d mean?\\n\\n2. Scalability is mentioned multiple times in the paper. Are there more large-scale (datasets, network, etc.) results?\\n\\n3. In the abstract, \u201crate-based and time-based encoding\u201d is mentioned. However, there is no corresponding part in the paper.\", \"Questions\": \"See weakness.\"}"}]}
{"paper_id": "70A6oo3Il2", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nAlthough neural networks are conventionally optimized towards zero training loss, it has been recently learned that targeting a non-zero training loss threshold, referred to as a flood level, often enables better test time generalization. \nCurrent approaches, however, apply the same constant flood level to all training samples, which inherently assumes all the samples have the same difficulty.\nWe present AdaFlood, a novel flood regularization method that adapts the flood level of each training sample according to the difficulty of the sample. \nIntuitively, since training samples are not equal in difficulty, the target training loss should be conditioned on the instance. \nExperiments on datasets covering four diverse input modalities &mdash; text, images, asynchronous event sequences, and tabular &mdash; demonstrate the versatility of AdaFlood across data domains and noise levels.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"-The proposed method is reasonable. Different samples has different difficulty. It is reasonable to use different loss.\\n-The paper is well-written and easy to follow.\", \"Weaknesses\": \"-This paper lacks experiments on large-scale dataset, such as ImageNet. CIFAR and SVHN are small datasets, which are very different from the real sense.\\n\\n-Does the adaptive strategy bring extra training cost? More detailed discussion is required.\", \"Questions\": \"See the weakness.\"}"}]}
{"paper_id": "s6bKLlF4Pe", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nThis paper explores knowledge transfer using successor features (SFs) in reinforcement learning (RL) scenarios where the reward function changes across tasks while the environment's dynamics remain the same. Under this framework, the Q-function of a task can be decomposed into a successor feature and a reward mapping: the former characterizes the transition dynamics, and the latter characterizes the task-specific reward function.\nThis Q-value function decomposition, coupled with a policy improvement operator known as \"generalized policy improvement\" (GPI), simplifies the search space for finding the optimal Q-function when transferring knowledge from one task to another that shares the same transition dynamics. As the optimal policy can be directly derived from the optimal Q-function, the SF \\& GPI framework exhibits promise in enhancing efficiency and effectiveness in decision-making compared to traditional RL methods like Q-learning.\nHowever, despite the observed superior performance of SF \\& GPI in numerical experiments, their theoretical foundations remain largely unestablished, especially when learning successor features using deep neural networks in conjunction with deep Q-network (SF-DQN). To the best of our knowledge, this paper provides the first convergence analysis with provable generalization guarantees for SF-DQN with GPI. Moreover, our theoretical results reveal that SF-DQN \\& GPI significantly accelerate the policy transfer across tasks and indicate that SF decomposition outperforms non-representation learning approaches, such as deep Q-network (DQN), with simultaneously faster convergence rate and improved generalization. Numerical experiments on real RL tasks support the superior performance of SF-DQN \\& GPI, quantitatively aligning with our theoretical findings.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 1, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"Theoretical analysis of SF when combined with function approximation is quite interesting and can be insightful.\", \"Weaknesses\": \"- The first half of the paper is confusing as the authors seem to conflate optimal value/policy at the task level with some generic $Q^*$ and $\\\\pi^*$. It was hard to follow and figure out what they mean by the notations and/or possible notational mistake. \\n\\n- As an important point, note that the GPI theorem induces improvement for the case of several policies but **the same** reward (i.e., only one task). If there are multiple rewards, then maximum of their corresponding value functions is simply meaningless and does not represent any physical concept regarding a new reward function (see the comments bellow). [This is OK when using SF as the $w_i$ corresponds to the current task and only $\\\\psi_j$ comes from some previous task.]\\n\\n- There were a number of other typos (a proof read aside from the points mentioned above is strongly suggested).\", \"Questions\": \"- If you have multiple tasks, then what do you mean by $Q^*$ and $\\\\pi^*$? In fact, both $\\\\pi^*$ and $Q^*$ are not well-defined unless the reference for optimality is specified. For instance, you may define a total reward as the summation of all the task-level rewards, whose $\\\\pi^*$ and $Q^*$ would be different from the case where a total reward is a *weighted* sum of task-based rewards or some non-linear function of them. \\n\\n- Eq (13) is not a policy improvement operator (PI is to take an argmax of the current value to induce a new policy). \\n\\n- Last three lines of page 4 --> First, your reference to $Q^*$ is senseless. More importantly, this setup has nothing to do with your settings. Maximum of several $Q$ functions corresponding to various rewards may not even be a valid $Q$ function for a different reward signal. One can easily construct an MDP with say three tasks and two actions, where in a certain state, $a_1$ is the optimal action for task 3, while $\\\\max_{i\\\\in \\\\{ 1,2 \\\\} } Q_{i}$ give you action $a_2$. Indeed it is theoretically possible that $\\\\max_{i\\\\in \\\\{1,2\\\\}} Q_{i}$ gives you suboptimal action in *all* states. \\n\\n- Table 1 --> what is $\\\\phi_{i}(\\\\Theta^{*}_{i})$ ? $\\\\phi$ is supposed to be the feature vector. \\n\\n- Assumption 3 is unclear. Note that $\\\\pi(a|s) \\\\in [0,1]$, Hence, for sufficiently large $C$ this assumption is always true! Am I missing something?\\n\\n- Is (20) a s fair assumption? For most cases, $K$ would be a large number, so it looks like that the initial approximation of $\\\\psi_1$ must be already very good! Perhaps a better presentation could be that there has to be some pre-training for $\\\\psi_1$ before using this algorithm?\"}"}]}
{"paper_id": "x4OPJ7lHVU", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nIn-context learning (ICL) is an important capability of Large Language Models (LLMs), enabling these models to dynamically adapt based on specific, in-context exemplars, thereby improving accuracy and relevance.\nHowever, LLM's responses may leak the sensitive private information contained in in-context exemplars. \nTo address this challenge, we propose Differentially Private In-context Learning (DP-ICL), a general paradigm for privatizing ICL tasks. \nThe key idea for DP-ICL paradigm is generating differentially private responses through a noisy consensus among an ensemble of LLM's responses based on disjoint exemplar sets. \nBased on the general paradigm of DP-ICL, we instantiate several techniques showing how to privatize ICL for text classification and language generation. \nWe experiment on four text classification benchmarks and two language generation tasks, and our empirical findings suggest that our DP-ICL achieves a strong utility-privacy tradeoff.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 4, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"- This paper is well-written and well-organized. \\n- The proposed paradigm is novel and is a novel paradigm for privatizing ICL tasks.\\n- The authors provide a clear and concise explanation of the DP-ICL paradigm. The figures and captions are easy to understand.\\n- Experiments on 4 benchmark datasets are extensive and sufficiently demonstrated the promise of the approach towards trustworthy usage of large language models.\", \"Weaknesses\": \"- Limitations can be discussed in more details. For example, \u201ca better-trained embedding-to-text model may yield further improvements in the model performances.\u201d Then what can be the potential candidate models here and why did the authors not adopt these models?\", \"Questions\": \"- How do you see the DP-ICL paradigm being applied in real-world scenarios, and what are some of the challenges that would need to be addressed?\\n- What are some of the limitations of the DP-ICL paradigm, and how might they be addressed in future research?\"}"}]}
{"paper_id": "rtzUW1FU3H", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nLarge language models (LLMs) have received significant attention by achieving remarkable performance across various NLP tasks. However, the fixed context window length of the transformer architecture makes them incapable of memorizing and understanding extremely long inputs. There are tremendous works in designing effective and advanced techniques to enlarge LLMs' context window size, which call for high demands on developing high-quality benchmark datasets to evaluate LLMs' long context understanding ability. There are some existing datasets for this purpose. However, they face the problems of (1) shorter text length compared to modern LLMs' context window length, (2) out-of-date documents that may already been included in the training corpus of modern LLMs, and (3) most of the tasks are short dependency tasks---there are few questions that really need LLMs to collect information across the whole document (which we call. Most importantly, they hardly consider assessments on long dependency modeling and understanding across segments, which are particularly challenging and valuable for improving LLM long context. In this paper, we present LooGLE, a Long Context Generic Language Evaluation benchmark for LLM long context understanding. It contains up-to-date documents (all after 2022), over 24k tokens per document, and 6k newly generated questions from diverse domains and categories. Specifically, we recruited a group of human labelers to read 145 long documents in our benchmark, and asked them to compose about 1.1k QA pairs satisfying our long dependency requirements. These 1.1k high-quality QA pairs are each cross-validated 3 times by 2 labelers, aiming to provide the currently most accurate evaluation of LLMs' ability on long dependency questions. Upon a comprehensive evaluation of 8 state-of-the-art LLMs on LooGLE, we find that: (1) Commercial models generally outperform open-sourced models. (2) LLMs are more skilled at short dependency tasks like short QA and cloze but still struggle on performing real long dependency tasks. (3) In-context learning and chain of thoughts only bring incremental improvement for long context understanding. (4) Retrieval-based techniques significantly contribute to improvement on short QA whereas many techniques for extending context window length through optimized transformer architecture or positional encoding can hardly resolve long context understanding.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"- A curated dataset, thoughtfully designed with great efforts to prevent data leakage and ensure long dependency.\\n- Great efforts in assessing current state-of-the-art LLMs' long dependency capabilities such as information retrieval, reading comprehension and reasoning, computation and timeline reordering.\", \"Weaknesses\": \"- see questions.\\n- the paper is sprinkled with typos (refer to questions for a few).\", \"Questions\": \"- Is the benchmark english-only ? If so, this needs to be mentioned.\\n- How will the dataset be released ? For instance ZeroSCROLLS only released the inputs and evaluate through a system of leaderboard, will LooGLE be released the same way ? I'm concerned that revealing input/gold outputs pairs would lead to data leakage for future models.\\n- Concerning data collection, were the sourced documents (after 2022) subjected to any machine-generated verification ? I am concerned that ChatGPT-like texts might compromise the fairness of the evaluation.\\n- Are the open-source models instruction-tuned ? Commercial closed source models like GPT3.5 rely on RLHF or some instruction tuning techniques that enable them to better follow instructions. If the considered open-source baselines are not instruction-tuned, the comparison might be unfair since the prompt used for evaluation is the same and is instruction-based.\\n- Is it fair to have GPT4 both as baseline and evaluator ? I am also concerned about the creation of the dataset of short QAs (generated by GPT3.5). Is it fair to evaluate a model that was used to create the dataset ?\\n- In section 4.2, you mention human evaluation (3) but I cannot find any human evaluation both in the paper and the appendix.\\n- Section 4.3.2 is confusing, the experiments are based on the recent work from Liu et al. 2023, that accessing information in the middle of the document is more challenging for LLMs. If I understood correctly, you suggest concatenating the head and the tail of the inputs and give it to the LLMs as input. This would mean discarding the whole \\\"middle\\\" and leaving the beginning and the end. This puts an immediate limitation on information that can be retrieved by the LLMs. Also did you only use the same model (GPT4-32k) but with various context input length or different variants of GPT4 ? What is the last entry in Table 5 ? (GPT4). For long summarization, arxiv abstracts are highly biased towards the beginning of the article so it is expected that increasing context would result in a higher divergence between the generated summary (which will contain more and more details) and the (gold) abstract.\\n \\n**Typo**\\n- page 2: \\\"Mannually designed both short and long dependency tasks\\\"\\n- page 3: \\\"COmputation.\\\"\\n- page 7: \\\"Retrival\\\"\\n- page 8: \\\"GPT4 evalution\\\"\\n- table 4: \\\"Performence\\\"\\n- appendix page 3: \\\"Dispcrepancy\\\"\\n- not really typos but it is uninformative to report results of the order of \\\"e-300\\\" since at this point there is nothing to really compare.\"}"}]}
{"paper_id": "f4HohsyNEk", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nWe present a method for generating high-quality watertight manifold meshes from multi-view input images. Existing volumetric rendering methods are robust in optimization but tend to generate noisy meshes with poor topology. Differentiable rasterization-based methods can generate high-quality meshes but are sensitive to initialization. Our method combines the benefits of both worlds; we take the geometry initialization obtained from neural volumetric fields, and further optimize the geometry as well as a compact neural texture representation \nwith differentiable rasterizers. Through extensive experiments, we demonstrate that our method can generate accurate mesh reconstructions with faithful appearance that are comparable to previous volume rendering methods while being an order of magnitude \nfaster in rendering. We also show that our generated mesh and neural texture reconstruction is compatible with existing graphics pipelines and enables downstream 3D applications such as simulation.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"The writing quality is generally good, and well-articulated.  The paper provides thorough evaluation on multiple datasets, and comparisons to multiple baseline methods.  The end-to-end differentiability of the system allows refinement of the initial geometry based on a rendering loss, which improves the quality of the reconstruction result.  The implementation and engineering, such as deploying the optimized model in GLSL shaders, is notable.\", \"Weaknesses\": \"While the paper shows favorable results in terms of quality of reconstruction, it seems only about on par or marginally better than methods that it has been compared to.  It mostly connects components from existing works (TensoRF, nvdiffrast, Marching Cubes algorithm) into a well-engineered end-to-end pipeline, but I question whether the specific technical contributions of the paper are significant enough to stand on their own.\\n\\nThe paper claims that it is the first differentiable Marching Cubes implementation, yet it's not clear whether this is indeed the case (see for example: Deep Marching Cubes, MeshSDF, Neural Marching Cubes, Flexicubes).  Furthermore, the differentiable formulation of MC presented in the paper is compared only to DMTet, but not to any other methods that might be even closer in spirit (i.e. those mentioned in the previous remark).  Therefore, it is hard to evaluate the level of contribution.\", \"Questions\": \"- The paper strongly emphasizes the watertight manifoldness of the resulting meshes, but does not go into much detail about the specific technical contribution of this paper that results in this property.  Could you clarify whether this is indeed a contribution of this paper, or is it inherited from the other methods that the paper builds off of?\\n\\n- On page 4, it is stated that during the second, mesh optimization step, the *topology* is also optimized.  Could you elaborate/clarify what you mean by this exactly?\\n\\n- On page 5: \\\"given that most real-world objects tend to be axis-aligned\\\" -- could you briefly explain why this is a reasonable assertion to make?\\n\\n- SDF-based reconstruction methods (e.g. NeuS) claim that the raw density field is optimized only to satisfy the rendering objective (and is also biased away from the true surface), and therefore are not ideal for surface reconstruction.  In this paper, you argue that this is unnecessary and that using the density field directly is a better choice.  Could you elaborate on this choice?\"}"}]}
{"paper_id": "xh0XzueyCJ", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nDiffusion models for graph generation present transformative capabilities in generating graphs for various downstream applications. However, controlling the properties of the generated graphs remains a challenging task for these methods. Few approaches tackling this challenge focus on the ability to control for a soft differentiable property using conditional graph generation, leading to an uninterpretable control. However, in real-world applications like drug discovery, it is vital to have precise control over the generated outputs for specific features (e.g. the number of bonds in a molecule). Current diffusion models fail to support such hard non-differentiable constraints over the generated samples. To address this limitation, we propose PRODIGY (PROjected DIffusion for generating constrained Graphs), a novel plug-and-play approach to sample graphs from any pre-trained diffusion model such that they satisfy precise constraints. We formalize the problem of controllable graph generation and identify a class of constraints applicable to practical graph generation tasks. PRODIGY operates by controlling the samples at each diffusion timestep using a projection operator onto the specified constrained space. Through extensive experiments on generic and molecular graphs, we demonstrate that PRODIGY enhances the ability of pre-trained diffusion models to satisfy specified hard constraints, while staying close to the data distribution. For generic graphs, it improves constraint satisfaction performance by up to $100$%, and for molecular graphs, it achieves up to $60$% boost under a variety of constraints.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. Precise control with arbitrary constraints is a desired property for graph generation. The authors propose a general solution for certain classes of constraints. The authors derive closed-form solution for many commonly used constraints under the proposed projection approach. The detailed derivation may also inspire future research where new constraints may appear.\\n2. The empirical results are comprehensive. The authors show the proposed method can effectively control the generation direction towards graphs that satisfy the constraints, on both 2D graphs and 3D graphs. They further demonstrate the sensitivity of the approach to the interpolation value $\\\\gamma_t$$. Analysis on the efficiency of the approach is also provided.\", \"Weaknesses\": \"1. Though when the constraints cover all the graphs in the test set, the generated distribution is largely unaffected, in scenarios where the constraints do take effect, the generated distribution obviously deviates from the original distribution. And the deviation is model-sensitive. For example, in Table 2, with edge count as the constraint, the performance of GDSS downgrades by two folds on Clus. while EDP-GNN is largely unaffected. Also, on the dataset of Enzymes and Grid, both GDSS and EDP-GNN suffer a lot from the constraints. Furthermore, Figure 12 shows a lot of unrealistic molecular graphs (e.g. very large rings, disconnected graphs, very long bridging bonds) with the constraints on atom count.\\n2. The proposed projection paradigm main only applied to constraints with simple calculation process so that the optimization problem has a closed-form solution. When the constraints become complicated (e.g. involves non-linearity), it might be arduous or impossible to find such closed-form solution.\", \"Questions\": \"1. Maybe a typo in section 4 right under equation 3: $\\\\Pi_c(z)\u00a0=\u00a0\\\\arg\\\\min_{z\\\\in\u00a0c}||z-x||^2$\uff0cshould be $\\\\Pi_c(x) = \\\\arg\\\\min_{z\\\\in c}||z-x||^2$\"}"}]}
{"paper_id": "ruQkcBfzpm", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nModular vision-language models (Vision-LLMs) align pretrained image encoders with (pretrained) large language models (LLMs), representing a computationally much more efficient alternative to end-to-end training of large vision-language models from scratch, \nwhich is prohibitively expensive for most. Vision-LLMs instead post-hoc condition LLMs to \n`understand' the output of an image encoder.\nWith the abundance of readily available high-quality English image-text data as well as monolingual English LLMs, the research focus has been on English-only Vision-LLMs. Multilingual vision-language models are still predominantly obtained via expensive end-to-end pretraining, resulting in comparatively smaller models, trained on limited multilingual image data supplemented with text-only multilingual corpora. In this work, we present mBLIP, the first multilingual Vision-LLM, which we obtain in a computationally efficient manner -- on consumer hardware using only a few million training examples -- by leveraging a pretrained multilingual LLM.\nTo this end, we \\textit{re-align} an image encoder previously tuned to an English LLM to a new, multilingual LLM -- for this, we leverage multilingual data from a mix of vision-and-language tasks, which we obtain by machine-translating  high-quality English data to 95 languages. \nOn the IGLUE benchmark, mBLIP yields results competitive with state-of-the-art models. Moreover, in image captioning on XM3600, mBLIP (zero-shot) even outperforms PaLI-X (a model with 55B parameters). Compared to these very large multilingual vision-language models trained from scratch, we obtain mBLIP by training orders of magnitude fewer parameters on magnitudes less data. We release our model and code at ANONYMIZED.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"The approach is interesting in the sense that it covers a reasonable use case, that of multi-lingual LVLM by aligning frozen models. It seems like the authors found a good space where a solution did not exist in the literature.\\n\\nThe proposed approach seems technically reasonable. \\n\\nThe paper is mostly clear, the target application is clear and the explanations are usually clear (although some issues are flagged later on). \\n\\nResults are positive, although there isn't a lot to compare against.\", \"Weaknesses\": \"The paper does not seem to have a strong technical contribution. On the pro side, I like the application, and there's some analysis of the impact of the different pieces in Table 3, which offsets somewhat the lack of technical contribution.\\n\\nI am a bit unsure about the impact of LoRA. A task prompt has much lower capacity to adapt to any downstream task compared to adding LoRA (especially if LoRA adapters are added to every 1x1 layer). So I'm wondering if part of the performance is due to LoRA vs no LoRA.  \\n\\nAlso, if there is enough data to train LoRA adapters, why then there is no data to train a task prompt? \\n\\nis \\\"re-aligning\\\" actually fine-tuning?\", \"Questions\": \"Any answers to the \\\"weakness\\\" above. In particular: did I miss something with regards to novelty? can the use of LoRA be better justified?\"}"}]}
{"paper_id": "FNq3nIvP4F", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nRecently video generation has achieved substantial progress with realistic results. Nevertheless, existing AI-generated videos are usually very short clips (\"shot-level'') depicting a single scene. To deliver a coherent long video (\"story-level''), it is desirable to have creative transition and prediction effects across different clips. This paper presents a short-to-long video diffusion model, SEINE, that focuses on generative transition and prediction. The goal is to generate high-quality long videos with smooth and creative transitions between scenes and varying lengths of shot-level videos. Specifically, we propose a random-mask video diffusion model to automatically generate transitions based on textual descriptions. By providing the images of different scenes as inputs, combined with text-based control, our model generates transition videos that ensure coherence and visual quality. Furthermore, the model can be readily extended to various tasks such as image-to-video animation and autoregressive video prediction. To conduct a comprehensive evaluation of this new generative task, we propose three assessing criteria for smooth and creative transition: temporal consistency, semantic similarity, and video-text semantic alignment. Extensive experiments validate the effectiveness of our approach over existing methods for generative transition and prediction, enabling the creation of story-level long videos.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"This article addresses the limitation of existing models that can only generate shot-level videos and proposes a method to generate story-level videos using transitions. They extend an existing video generation framework and achieve impressive results in generating long videos. They also propose a reasonable evaluation framework to assess the proposed model, and a large number of demos and quantitative evaluations demonstrate the effectiveness of their approach. The contribution of this work is significant.\", \"Weaknesses\": \"The author should provide a more detailed description of the model for reproducibility, including training resources, training parameters, and so on. Additionally, the author should also report scores on commonly used evaluation metrics such as FID.\", \"Questions\": \"I wondered how many GPUs they used and how long it takes for training. Besides, as far as I know, FID is used to evaluate video generation quality in many papers, can they provide this to make their paper more solid?\"}"}]}
{"paper_id": "QQ5eVDIMu4", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nThe ability of Graph Neural Networks (GNNs) to generalize to diverse and unseen distributions holds paramount importance for real-world applications. However, previous works mostly focus on addressing specific types of distribution shifts, e.g., larger graph size or node degree, which is highly limited when confronted with multiple and nuanced distribution shifts. For example, a node in a social graph may have both increased interactions and features alternation, while its neighbor nodes may see different shifts. Failing to consider such complex distribution shifts will largely hinder the generalization effect in practice. Here we introduce GraphMETRO, a novel framework based on a mixture-of-experts (MoE) architecture, enhancing GNN generalizability for both node- and graph-level tasks. The core concept of GraphMETRO includes the construction of a hierarchical architecture composed of a gating model and multiple expert models that are aligned in a common representation space. Specifically, the gating model identifies the significant mixture components that govern the distribution shift on a node or graph instance. Each aligned expert produces representations invariant to a type of mixture component. Finally, GraphMETRO aggregates the representations from multiple experts to produce an invariant representation w.r.t. the complex distribution shift for the prediction task. Moreover, GraphMETRO provides interpretations on the distribution shift type via the gating model and offers insights into real-world distribution shifts. Through the systematic experiments, we validate the effectiveness of GraphMETRO which outperforms Empirical Risk Minimization (ERM) by 4.6% averagely on synthetic distribution shifts and achieves state-of-the-art performances on four real-world datasets from GOOD benchmark, including a 67% and 4.2% relative improvement over the best previous method on WebKB and Twitch datasets.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"- The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\\n- Real-world datasets back the claims through experiments.\\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\", \"Weaknesses\": \"- The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \\\"previous research has concentrated on addressing specific types of distribution shifts.\\\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \\\"environment generators\\\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\\n\\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\\n\\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\", \"Questions\": \"Please check Weaknesses.\"}"}]}
{"paper_id": "eGdhD93hZr", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nVarious applications of voice synthesis have been developed independently despite the fact that they generate \"voice\" as output in common. In addition, the majority of voice synthesis models currently rely on annotated data, but it is crucial to scale them to self-supervised datasets in order to effectively capture the wide range of acoustic variations presented in human voice, including speaker identity, emotion, and prosody. In this work, we propose MVoice, a multimodal spoken large language model for synthesizing and manipulating voice signals at scale. MVoice employs self-supervised voice tokens with the \"coarse-to-fine\" designs to first determine semantic meaning and then introduce condition signals for acoustic generation. It offers notable benefits with unified generation and transformation capabilities: 1) model and data scalability: without the requirement of scattered model-specific methodologies or annotations acoustic data, training could be scaled up in terms of data usage and model capability; and 2) controllability and conditioning flexibility: we investigate different conditioning mechanisms and effectively handle voice synthesis applications, including text-to-speech, voice conversion, singing voice synthesis, singing voice conversion, and speech-to-speech translation by re-synthesizing the discrete representations with prompt guidance. Experimental results demonstrate that MVoice exhibits superior audio quality and style similarity compared with competitive baseline models in monolingual/cross-lingual voice generation. Audio samples are available at https://MVoice.github.io\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"- Originality: \\nEven though discrete unit-based generative models been investigated before (e.g., Polyak et al. 2021, https://arxiv.org/pdf/2104.00355.pdf), this paper adds AR modeling and controllability to obtain a more unified model in terms of the variety of that can be performed with the same model. Some advantages of the model are\\n1. Experiments usually achieve good performance on several speech tasks as compared to the previously published studies. \\n2. Being controllable is an advantage of the model to adapt it to different use cases. \\n3. The modularity (semantic stage and then an acoustic stage) allows the model to perform different tasks.  \\n\\n- Quality: \\nThe paper proposed a two-stage speech model (semantic + acoustic models) which makes it somewhat interpretable. Experiments have been conducted on various tasks and the performance of those are on par with that of previous studies if not better. \\n\\n- Clarity: \\nAt a high-level, the paper is relatively easy to understand, but in terms of details, some parts have been left to the Appendix and it might make the paper difficult to read at times. \\n\\n- Significance: \\nSimilar to the comments in Originality above. This is a study towards investigating unified generative speech models.\", \"Weaknesses\": \"1. Experiments do not always show gain, and when they outperform, the improvements are limited. \\n\\n2. Even though the paper title mentions multilingual unified speech representations, the results on multilingual condition are worse than the monolingual ones. \\n\\n3. Overall, the main gains are coming from improved speaker similarity scores and not in terms of other evaluation criteria. \\n\\n4. The paper already mentions a few shortcomings and these are valid concerns. Regarding the dependency of the performance on the tokenizer quality makes the reason for using discrete tokens questionable. If there were continuous features, would it be better? If so, then wouldn't the first stage lose its property, i.e. being interpretable as semantic stage?\", \"Questions\": \"1. If there were continuous features, would the model be better? If so, then wouldn't the first stage lose its property, i.e. being interpretable as semantic stage?\\n\\n2. Which ASR models have been used to obtain the WERs on the synthetic datasets?\"}"}]}
{"paper_id": "6CZ50WgfCG", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nThe success of many RL techniques heavily relies on human-engineered dense rewards, which typically demands substantial domain expertise and extensive trial and error. In our work, we propose **DrS** (**D**ense **r**eward learning from **S**tages), a novel approach for learning *reusable* dense rewards for multi-stage tasks in a data-driven manner. By leveraging the stage structures of the task, DrS learns a high-quality dense reward from sparse rewards and demonstrations if given. The learned rewards can be *reused* in unseen tasks, thus reducing the human effort for reward engineering. Extensive experiments on three physical robot manipulation task families with 1000+ task variants demonstrate that our learned rewards can be reused in unseen tasks, resulting in improved performance and sample efficiency of RL algorithms. The learned rewards even achieve comparable performance to human-engineered rewards on some tasks. See our [project page](https://sites.google.com/view/iclr24drs) for more details.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"- This paper makes a contribution towards automating reward design, which is of paramount importance in the field of RL. Having access to dense rewards takes the burden off of exploration, which in turn reduces the number of samples required to solve a task.\\n- The method is a niche application of contrastive discriminator learning, which is well-established in the literature.\", \"Weaknesses\": \"- The method requires success and failure trajectories for each stage in the training data, which can be expensive to collect. \\n- The scope of the method is limited to a family of tasks that can be divided into stages. This prevents it from being applied to other tasks such as locomotion. It also means the method is less general compared to LLM-based rewards with external knowledge [1, 2].\\n- Similarly, the need for stage indicators prevents the method from scaling to real-world problems, which would arguably benefit more from automated reward design than simulated domains.\\n- The experiments do not demonstrate new skills that are unachievable by human-designed rewards.\\n\\nReferences:\\n1. Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, and Tao Yu. Text2reward: Automated dense reward function generation for reinforcement learning. arXiv preprint, 2023.\\n2. Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi \u201cJim\u201d Fan, Anima Anandkumar. Eureka: Human-Level Reward Design via Coding Large Language Models. arXiv preprint, 2023.\", \"Questions\": \"- Does the method work with pixel observations?\\n- If the method only works with states, does generalization come as a result of state abstraction or the method itself?\\n- Do the learned dense reward functions resemble hand-designed rewards?\"}"}]}
{"paper_id": "ZEZ0CPmoSI", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nThis paper introduces a new method for minimizing matrix-smooth non-convex objectives through the use of novel Compressed Gradient Descent (CGD) algorithms enhanced with a matrix-valued stepsize. \nThe proposed algorithms are theoretically analyzed first in the single-node and subsequently in the distributed settings. Our theoretical results reveal that the matrix stepsize in CGD can capture the objective\u2019s structure and lead to faster convergence compared to a scalar stepsize. \nAs a byproduct of our general results, we emphasize the importance of selecting the compression mechanism and the matrix stepsize in a layer-wise manner, taking advantage of model structure. \nMoreover, we provide theoretical guarantees for free compression, by designing specific layer-wise compressors for the non-convex matrix smooth objectives. Our findings are supported with empirical evidence.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"The paper investigates matrix step sizes of compressed gradient descent and provides convergence analysis.\", \"Weaknesses\": \"see questions.\", \"Questions\": \"1) The paper claims that the matrix step size improves performance but introduces problems. The presence of the neighboring term in Theorem 3 creates a significant neighborhood when $\\\\Delta^{inf}$ is large.\\n2) This paper uses the compressed gradient as shown in (3), which is a direct compression on the full gradient. Given the prevalence of error feedback techniques in handling compression errors, it is pertinent to consider whether the proposed matrix step size can be extended to incorporate error feedback methods. \\n3) The paper does not consider the use of SGD, local update, and partial participation techniques in distributed settings. \\n4) The paper introduces det-CGD1, but it is unclear why this algorithm, specifically in a distributed setting, can obtain Algorithm 1.\"}"}]}
{"paper_id": "8Wuvhh0LYW", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nLarge language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, leading to low performance, especially in extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization ($\\textbf{OmniQuant}$) technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters. OmniQuant comprises two innovative components including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). LWC modulates the extreme values of weights by optimizing the clipping threshold. Meanwhile, LET tackles activation outliers by shifting the challenge of quantization from activations to weights. Operating within a differentiable framework using block-wise error minimization, OmniQuant can optimize the quantization process efficiently for both weight-only and weight-activation quantization. For instance, the LLaMA-2 model family size 7-70B can be processed with OmniQuant on a single A100-40G GPU within 1-16 hours using 128 samples. Extensive experiments validate OmniQuant's superior performance across diverse quantization configurations such as W4A4 (4-bit weight, 4-bit activation), W6A6, W4A16, W3A16, and W2A16. Additionally, OmniQuant demonstrates effectiveness in instruction-tuned models and delivers notable improvements in inference speed and memory reduction on real devices. Codes are available at \n\\url{https://github.com/OpenGVLab/OmniQuant}.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 5, \"Strengths\": \"The biggest contribution of this work is the learnable weight clipping (LWC). \\n\\n1. When compared to AWQ [1] which only optimize the scale, LWC uses the same objective function (minimize output distortion) but also handles clipping.\\n\\n2. When compared to PACT, which only applies to activation with positive values, LWC is more general and applies to weight, activation, and negative values.\\n\\n3. When compared to LSQ, which is a combination of multiple optimization goals (step-size, gradient scaling, and clipping), LWC is straightforward to implement and speedy to train.\\n\\n[1] Lin, J., Tang, J., Tang, H., Yang, S., Dang, X. and Han, S., 2023. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. arXiv preprint arXiv:2306.00978.\", \"Weaknesses\": \"1. While I highlight the advantage of LWC over AWQ, PACK, and LSQ, this evaluation for PACK and LSQ is lacking.\\n\\n2. Authors should present Figure 1 (a) in a quantitative way. It lacks actual numbers for cost of time and performance.\\n\\n3. Learnable Equivalent Transformation (LET) isn't novel and can be seen in common quantization kernels such as LLM.int8. I suggest authors to elaborate more on comparison with PACK and LSQ instead of LET.\\n\\n4. The latency benchmark only contain fp16 and OmniQuant. Strong baseline such as GPTQ is lacking.\", \"Questions\": \"1. How does OmniQuant runtime efficiency compare to GPTQ and AWQ?\\n\\n2. What is the performance when compared to PACK and LSQ?\"}"}]}
{"paper_id": "kaAtQwhnM2", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nEnergy-based models (EBMs) offer a flexible framework for probabilistic modelling across various data domains. However, training EBMs on discrete data poses significant challenges, primarily due to the intricacies of sampling in such spaces. In this work, we propose to train discrete EBMs with Energy Discrepancy which only requires the evaluation of the energy function at data points and their perturbed counterparts, thus eliminating the need for demanding sampling techniques like Markov chain Monte Carlo. Energy discrepancy offers theoretical guarantees applicable to a broad class of perturbation processes, of which we investigate three types: perturbations based on Bernoulli noise, deterministic transforms, and neighbourhood structures. We estimate the energy discrepancy loss effectively using importance sampling with two types of proposal distributions: uninformed and gradient-informed. Empirically, we demonstrate the efficacy of the proposed approaches in a wide range of applications, including Ising models training, discrete density estimation, graph generation, and discrete image modelling.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 4, \"Contribution\": 4, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"Very thorough ablations are presented in the appendix, investigating the effects of a number of parameters to configure their method.\", \"Weaknesses\": \"The results on discrete image modelling are not very strong, either in terms of sample quality or in terms of estimated NLL.\", \"Questions\": \"I\u2019m curious about the wall-clock speed of training. Your approach admits better parallelisation than existing techniques since it relies on importance sampling rather than MCMC. Do you have any wall-clock timing experiments investigating this? For example, how does the wall-clock time compare of your approach vs. CD-10? \\n\\nMust the GwG sampler be tuned differently when training with ED vs. GwG? i.e. do you find that sampling is less efficient at test-time when a model is trained with ED?\"}"}]}
{"paper_id": "QFo2wxQEW6", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nEndovascular robots have been actively developed in both academia and industry. However, progress toward autonomous catheterization is often hampered by the widespread use of closed-source simulators and physical phantoms. Additionally, the acquisition of large-scale data for machine learning algorithms training for endovascular robots is usually infeasible due to the expensive medical setup and complicated medical procedures. We introduce CathSim, the first open-source and real-time simulator for endovascular intervention to address these limitations. CathSim's emphasis on sample efficiency enables rapid development and testing of learning algorithms. We validate our CathSim with real robot experiments and achieve competitive performance. Based on CathSim, we develop a multi-modal expert navigation network and demonstrate its effectiveness in downstream endovascular navigation tasks. Our results suggest that CathSim has the potential to significantly accelerate research in the field of autonomous endovascular navigation systems. Our project is available at https://anonymous.4open.science/r/cathsim-E168.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"* Open source simulator\\n* Simulator is validated via comparison to real world robot\\n* Expert navigation results beating the human baseline\", \"Weaknesses\": \"* ENN model architecture is not state-of-the-art, e.g. no transformer\\n* Unclear how trustworth the user study is. 10 participants in user study is a small sample size and it\u2019s unclear how they were recruited and how trustworthy their judgements are.\\n* Lack of relevant details, e.g. what is the expert policy that the ENN is trained with. Why don\u2019t you show results for the expert policy in the table?\\n* If an expert policy exists, why do you train an ML model (ENN) model for the expert navigation task?\\n* Some figures / tables are not easy to understand on their own, e.g. Table 2, for which e.g. instead of Q1 you could write \u201cAnatomical accuracy\u201d.\", \"Questions\": \"Overall, this seems to be very relevant work. However, in terms of ML it is not pushing the boundary of the state-of-the-art. This work might fit better to a more applied conference.\"}"}]}
{"paper_id": "SUUckMrHNY", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nDespite the success of current segmentation models powered by the transformer, the camouflaged instance segmentation (CIS) task remains a challenge due to the similarity between the target and the background. To address this issue, we propose a novel approach called the local-feature-aware transformer ($\\alpha$-Former) that incorporates traditional computer vision descriptors to extract critical edge features in camouflaged instances. Specifically, we introduce an adapter to merge local features into the transformer framework. Using the proposed transformer-based encoder-decoder architecture, our $\\alpha$-Former surpasses state-of-the-art performance on the COD10K and NC4K datasets. Additionally, we introduce an edge-aware feature fusion module to improve boundary results in the segmentation model.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"* State-of-the-art performance on the camouflaged instance segmentation (CIS) task.\\n* Reasonable module design, such as learnable binary filter.\", \"Weaknesses\": \"* The effectiveness of the feature aggregation adapter is limited.\\nAccording to Table 2 in this paper, using adapter has a negative impact on the performance of traditional descriptors on NC4K. For example, AP of (LBP w/o adapter) 42.288, AP of LBP w/ adapter 41.794. And same situtation for Circle-LBP. Even for the Binary filter, the improvement is very limited. (42.610->42.936).\\n* Typo: There exists a mismatch figure caption between Figure 7 in this paper and Figure 2 in supplementary_material. (Predicted edge and Ground truth).\\n* Complex pipeline and missing ablation study: This paper introduces attention module, positional encoding, encoder, decoder, but missing results to demenstrate the effectiveness of each components and how each component help the camouflaged instance segmentation (CIS) task.\", \"Questions\": \"See Weakness.\"}"}]}
{"paper_id": "8Cc6qOPvFo", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nWe present a simple but effective training-free method for text-driven image-to-image translation based on pretrained text-to-image diffusion models. Since a naive application of the pre-trained diffusion models for the manipulation tasks often significantly destroys the structure or background of the source image, we revise the original backward process for the target image by meaningfully aligning better with a given target task while preserving the background or structure of a source image. We derive a new guidance objective term that is a combination of maximizing the similarity with target prompts rather than the source prompt based on the pre-trained CLIP and minimizing the distance with the source latents. Moreover, contrary to existing methods based on the diffusion models, we exploit the cycle-consistency objective in order to further maintain the background of the source image, where we perform an iterative optimization process by alternately optimizing the source and target latents. Experimental results demonstrate that the proposed method achieves outstanding editing performance on various tasks when combined with the pre-trained Stable Diffusion.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"- **Simplicity & Effectiveness**: The proposed method is both straightforward and seemingly efficacious, as evidenced by the results presented in the paper.\", \"Weaknesses\": \"- **Evaluation Methods**: The evaluation could be more robust. The prompts used for evaluation are closely related to the original noun, reducing diversity and potentially biasing results.\\n- **Aspect Ratio Concerns**: The samples used for evaluation have been altered from their original aspect ratios. This could inadvertently disadvantage competing methods.\\n- **Comparison Choices**: The results from prompt-to-prompt evaluations seem to perform well on generated images rather than inverted real ones. The absence of a comparison with Null-text-inversion, which might be a more apt benchmark, raises questions.\\n- **Efficiency Metrics**: The paper would be more informative with a runtime efficiency comparison against other methods.\", \"Questions\": \"None\"}"}]}
{"paper_id": "MSe8YFbhUE", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nVisual reinforcement learning (RL) has shown promise in continuous control tasks.\nDespite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds.\nIn this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. \nExpanding upon this crucial observation, we additionally unveil a significant correlation between the agents' inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks.\nTo quantify this inactivity, we adopt dormant ratio as a metric to measure inactivity in the RL agent's network.\nEmpirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent's activity level, regardless of the received reward signals.\nLeveraging the aforementioned insights, we introduce DrM, a method that uses three core mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio. \nExperiments demonstrate that  DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit.\nMost importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"The greatest strength of the paper is the significance of the results. DrM is the clear winner on *all* of the tasks reported on in the paper, and in some domains (particularly Adroit) it achieves a step change improvement. I like that the authors have considered three separate domains (DeepMind Control Suite, MetaWorld and Adroit), with multiple tasks from each.\\n\\nThe main idea of treating the dormant ratio as a progress proxy, and hence using it as the basis for adaptive hyperparameter scheduling, makes intuitive sense. While I have some minor qualms about the writing (see below), the paper was clear and easy-to-read overall. Lastly, the coverage of the huge amount of related work from the last 2-3 years is about as thorough as possible.\", \"Weaknesses\": \"My biggests concerns with the paper are its limited novelty a lack of proper ablations.\\n\\nNovelty:\\n\\nNone of the three mechanisms introduced by DrM are novel in-and-of themselves.\\n- As noted in the paper, \\\"dormant-ratio-guided perturbation\\\" follows the previous work of (D'Oro et al., 2023; Ash & Adams, 2020).\\n- Similarly, \\\"dormant-ratio-guided exploitation\\\" is based on the work of Ji et al. (2023).\\n- The idea of scheduling the exploration rate goes back a very long way, pre-dating deep RL. Some recent approaches, e.g., Agent57 (Badia et al.), also use a form of adaptive exploration.\\n\\nWhat *is* new is the idea of using the dormant ratio to schedule the hyperparameters of these methods. I don't disagree that this is novel; however, it's rather limited novelty in my opinion.\\n\\nAblations:\\n\\nWhile Figure 10 contains some ablations, the three mechanisms are each ablated in their entirety. I'd prefer it if only the adaptive scheduling component were ablated for each method, since this is the novel part. In other words, for a fair comparison, DrM ought to be compared against DrQ + perturbation resets (with fixed $\\\\alpha$) + \\\"Blended Exploration and Exploitation operator\\\" (with fixed $\\\\lambda$).\\n\\nMoreover, the ablations only consider one domain (Adroit) and the results are aggregated across all Adroit tasks. I'd prefer to see more ablations in greater granularity. For me, they're the most important part of the experiments, since without them it's very hard to understand *why* the improvements work. (The argument that DrM lowers the dormant ratio and hence improves performance seems very \\\"chicken-egg\\\" to me; it's unclear which is the cause and which is the effect. Only the dormant-ratio-guided perturbations seem to directly target the dormant ratio.)\\n\\nMinor things:\\n- Section 3.1 is too unscientific, e.g., the claim \\\"This suggests that the dormant ratio acts as an intrinsic metric, influenced more by the diversity and relevance of the agent's behaviors than by its received rewards\\\". This is all based on one example, and the claim that RL agents learn relevant behaviours before their returns improve sounds dubious to me. Another too-bold claim is: \\\"As the dormant ratio captures the intrinsic characteristics of an agent's policy network, all the hyperparameters in this approach are robust to tasks and domains.\\\" This is never established, and in fact Table 1 in the Appendix suggests that some parameters of DrM require domain-specific tuning.\\n- The claim in the abstract that DrM has \\\"no broken seeds (76 seeds in total)\\\" is a little vague (what constitutes \\\"broken\\\"?) and it's never really returned to in the main body of the paper.\", \"Questions\": \"- In the \\\"dormant-ratio-guided exploitation\\\" section, it's mentioned that \\\"value underestimation occurs as the agent starts to acquire skills, given that $\\\\pi$ is sub-optimal\\\". I don't quite follow this. As the agent starts to improve, wouldn't we expect $\\\\pi$ to become less sub-optimal?\"}"}]}
{"paper_id": "LvlqEaXJbe", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nImages of the natural world, collected by a variety of cameras from drones to individual phones, are increasingly abundant sources of biological information. \nThere is an explosion of computational methods and tools, particularly computer vision, for extracting biologically relevant information from images for science and conservation. \nYet, currently, most of these are bespoke approaches designed for a specific task and are not easily adaptable or extendable to new questions, contexts, and datasets. \nWe develop the first large-scale multimodal model, BioCLIP, as a foundation for general organismal biology questions on images. \nWe leverage the unique properties of biology as the the application domain for computer vision, namely the abundance and variety of images about plants, animals, and fungi, together with the availability of rich structured biological knowledge.\nWe curate and release TreeOfLife-10M (the largest and most diverse available dataset of biology images), train BioCLIP, rigorously benchmark our approach on diverse fine-grained biology classification tasks, and find that BioCLIP consistently and substantially outperforms existing baselines (by 17% to 20% absolute). \nIntrinsic evaluation further reveals that BioCLIP has learned a hierarchical representation conforming to the tree of life, shedding light on its strong generalizability.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 5, \"Strengths\": \"1. The dataset collected in this paper is so far the largest biological image dataset which consists of over 10M images and 0.5M categories. Such a dataset is potentially very useful for training large models that target biology-related vision understanding tasks.\\n2. This work provides a detailed analysis of the effectiveness of training CLIP-type models on the collected dataset.\", \"Weaknesses\": \"1. The technical contribution is limited. The main contribution lies in the dataset collection and benchmarking of existing methods. \\n2. The presentation can be further improved. For example, it would be useful to add some examples with annotations from the dataset.\", \"Questions\": \"Even though this work does not provide much technical novelty, it takes a first step to build a large vision model and curate a highly diverse dataset. The analysis of the text types for the training of BIOCLIP is also interesting.\"}"}]}
{"paper_id": "LYGHdwyXUb", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nMulti-task Reinforcement Learning (MTRL) offers several avenues to address the issue of sample efficiency through information sharing between tasks. However, prior MTRL methods primarily exploit data and parameter sharing, overlooking the potential of sharing learned behaviors across tasks. The few existing behavior-sharing approaches falter because they directly imitate the policies from other tasks, leading to suboptimality when different tasks require different actions for the same states. To preserve optimality, we introduce a novel, generally applicable behavior-sharing formulation that selectively leverages other task policies as the current task's behavioral policy for data collection to efficiently learn multiple tasks simultaneously. Our proposed MTRL framework estimates the shareability between task policies and incorporates them as temporally extended behaviors to collect training data. Empirically, selective behavior sharing improves sample efficiency on a wide range of manipulation, locomotion, and navigation MTRL task families and is complementary to parameter sharing. Result videos are available at [https://sites.google.com/view/qmp-mtrl](https://sites.google.com/view/qmp-mtrl).\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1.\\tThis paper is well-written and easy to follow\\n2.\\tSharing learned behaviors among tasks is an interesting and important topic in RL.\\n3.\\tAuthors conducted extensive experiments and analysis to empirically illustrate the effectiveness of QMP.\", \"Weaknesses\": \"1. Lack of theoretical analysis on the convergence (rate) of QMP.\\n2. Lack of intuition and detailed analysis on why the proposed method works (i.e., why could QMP make sharing desired behaviors among tasks possible? Please see Question 2 for my concern).\\n\\nPlease refer to my questions below for my concerns.\", \"Questions\": \"1. Why do you choose to roll out H steps instead of just one step after choosing one policy to collect data? QMP just uses the Q function under a particular state to choose the behavioral policy, which cannot guarantee that the selected behavior policy is helpful for the current task after stepping out of the considered state.\\n2. Will QMP cause undesired behaviors sharing? As I mentioned in Question 1, the selected behavioral policy will roll out for H(>1) steps, which may incurs sub-optimal behaviors.\\n3. Why choosing the behavioral policy based on the learned Q-function to collect data will essentially share desired behaviors among tasks? I understand that using Q-function as a proxy may help select more better actions. However, the Q-function may be biased and not learned well during training, which could even hurt the learning process.\\n4. Will QMP even slow down the training process? Say, the learned policy for the current task proposes an action, which is optimal but has an under-estimated Q function. Due to the biased Q function, QMP selects another policy to collect data, which chooses a sub-optimal action. Although the TD update will fix the estimated Q value of the sub-optimal action, it may be more efficient if we directly update the Q value of the optimal action, the thing that we really care about.\\n\\nI am willing to raise my scores if you could solve my concerns.\"}"}]}
{"paper_id": "3K3aWRpRNq", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nIn the domain of Three-dimensional Structure-Based Drug Design (3D SBDD), the 3D spatial structures of target pockets serve as inputs for the generation of molecular geometric graphs. The Geometric Diffusion Model (GDM) has been recognized as the state-of-the-art (SOTA) method in 3D SBDD, attributed to its exceptional generation capabilities on geometric graphs. However, the inherent data-driven nature of GDM occasionally neglects critical inter-molecular interactions, such as Van der Waals force and Hydrogen Bonding. Such omissions could produce molecules that violate established physical principles. Particular evidence is that GDMs exhibit atomic clashes during generation due to the overly close proximity of generated molecules to protein structures. To address this, our paper introduces a novel constrained sampling process designed to obviate such undesirable collisions. By integrating a non-convex constraint within the current Langevin Dynamics (LD) of GDM and utilizing the proximal regularization techniques, we force molecular coordinates to obey the imposed physical constraints. Notably, the proposed method requires no modifications to the training process of GDMs. Empirical evaluations show a significant reduction in atomic clashes via the proposed method compared to the original LD process of GDMs.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The problem formulation and presentation are clear. \\n2. The method is rationale and theoretical analysis is provided.\\n3. The proposed method improves the connectivity, mean CR, and stability of generated ligand molecules.\", \"Weaknesses\": \"1. This method is not novel to some extent, which is some kind of similar to the validity guidance proposed in DecompDiff.  \\n2. The experimental results on molecular properties of generated ligands are not satisfying. The improvement on Vina score is only marginal. It is notable that vina score is one of the most metrics for structure-based drug design and strongly related to intermolecular spatial interaction between pockets and generated ligands.\", \"Questions\": \"Can the author provide more comparison between the validity guidance proposed in DecompDiff and proximal sampling proposed in this paper? Can the vina score be further improved?\"}"}]}
{"paper_id": "r1IbewSnqq", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper abstract and produce reviewer scores.\n\n--- PAPER ABSTRACT ---\n\nFederated Learning (FL) permits different parties to collaboratively train a global model without disclosing their respective local labels. A crucial step of FL, that of aggregating local models to produce the global one, shares many similarities with public decision-making, and elections in particular. In that context, a major weakness of FL, namely its vulnerability to poisoning attacks, can be interpreted as a consequence of the one person one vote (henceforth 1p1v) principle underpinning most contemporary aggregation rules. \nIn this paper, we propose FedQV, a novel aggregation algorithm built upon the quadratic voting scheme, recently proposed as a better alternative to 1p1v-based elections. Our theoretical analysis establishes that FedQV is a truthful mechanism in which bidding according to one's true valuation is a dominant strategy that achieves a convergence rate that matches those of state-of-the-art methods. Furthermore, our empirical analysis using multiple real-world datasets validates the superior performance of FedQV against poisoning attacks. It also shows that combining FedQV with unequal voting \"budgets\" according to a reputation score increases its performance benefits even further. Finally, we show that FedQV can be easily combined with Byzantine-robust privacy-preserving mechanisms to enhance its robustness against both poisoning and privacy attacks.\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"The paper proposes novel ideas by taking techniques from different domains.\\n\\nI think that the inclusion of incentive-based approaches to deal with corrupted parties is currently less explored that other techniques and can potentially provide new means to understand which attacks are realistic. Therefore, this is a valuable aspect of the contribution. \\n\\nOther ideas such as the integration of robust voting techniques and the use client-side measures are original and constructive in the discussion of byzantine resilience. In particular, techniques that are compatible with privacy enhancing technologies such as secure aggregation are valuable, especially given that an important use case of federated learning is privacy preserving machine learning.\", \"Weaknesses\": \"However, the contribution presents major problems which are listed below: \\n\\n1- The threat model of the paper is not clear. A well defined threat model is crucial when studying defenses against active adversaries. In the current contribution, I do not see what are the adversaries' possible behaviors and what are they willing to risk to harm the model (e.g. are they able to collude?). Therefore, it is not clear in practice in which scenarios would the truthfulness and convergence properties hold. This makes hard to evaluate the real impact of the contribution. \\n\\nFor example, consider an adversary which controls many parties and each party sends a harmful update in a different round. The adversary is willing to sacrifice parties to harm the model. Would this affect the truthfulness property? \\n\\n2- Clarity. The contribution is a combination of many ideas taken from different domains. However, the inclusion of each ingredient is not properly justified:\\n\\n2a- The paper introduces quadratic voting (QV) as a defense to reduce the impact of malicious users that falsely claim to have large datasets. However, the paper does not show how this kind of attack can affect other current Federated Learning defenses in practice and how QV is an effective defense. Moreover, the paper does not evaluate the resilience against any attack in which the adversary falsely claims to have a large dataset. Therefore, I am not sure if the initial motivation of QV is present in practice. \\n\\n2b- The paper introduces adaptive budgets as a reputation system. The original FedQV (Algorithm 1) already seems to penalize contributions. Therefore, the reputation protocol (Algorithm 2) appears a bit redundant. In the following sections of the paper, it is not clear when FedQV-Alg1 and FedQV-Alg2 are applied. \\n\\n2c- Compatibility with privacy enhancing primitives. In Section 3.2, the paper claims that the compatibility with privacy enhancing primitives (Secure Aggregation, Fully Homomorphic Encryption, Differential Privacy) is one of its main benefits of FedQV. However, I do not see a clear compatibility. For example, in Secure Aggregation the aggregation is not done by the server. Therefore, it not clear (i) how can the server adjust the weights of the aggregation properly and (ii) how can clients share similarity scores and local dataset size with the server without breaking the privacy guarantees. \\n\\n2d- Meaning of the truthfulness property: Section 4.2 claims that any possible corrupted contribution diminishes the probability of harm as server penalties reduce the influence of corrupted parties. This gives the impression that being honest is better for local utility than performing an attack. However, this is not what we can see empirically in section 5.  \\n\\n3- Evaluation of FedQV:  the proposed protocol claims to be easy to integrate with other resilience measures because similarities are computed in the client side. As said, this is an interesting feature, but is particularly vulnerable to client-side manipulations of the similarity scores and dataset sizes. It is true that protocol is evaluated against QV-adaptive, an attack that exploits this vulnerability. However, delegating the control of the defenses to possibly malicious clients is a high risk to take. Therefore, its evaluation of this risk requires a more comprehensive treatment. \\n\\nFor example, an important aspect of the evaluation which I feel is missed is (as said in point 2a) the resilience against an attack in which the adversary falsely claims to have a large dataset.\", \"Questions\": \"Please elaborate on the weaknesses outlined above.\"}"}]}
