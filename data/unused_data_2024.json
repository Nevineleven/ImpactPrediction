[
  {
    "paper_id": "4N7v4w2r3b",
    "summary": "Background: The robustness of neural network proxies is critical as these models are increasingly used in various domains to optimize or monitor quantities of interest. Proxies often serve as stand-ins for true objectives that are costly or difficult to evaluate directly. Despite their practicality, proxies are susceptible to 'gaming,' where optimization pressure leads them to fail in representing the true objective accurately. This issue has been acknowledged in several domains, including education and public policy, and poses significant risks when neural networks are tasked with capturing human values and preferences.\nGap: The robustness of neural network proxies under optimization pressure remains largely unexplored and unsolved. Existing research indicates that proxies may collapse under optimization pressure, but there is limited work on systematically evaluating proxy robustness and developing robust methods.\nHypothesis: Proxy robustness is a solvable problem that can be incrementally improved, and devising comprehensive benchmarks can uncover new properties of the proxy gaming problem, aiding in the development of methods to enhance proxy robustness.\nMethod: The paper introduces \"ProxyBench,\" a benchmark to assess the robustness of neural network proxies against optimization pressure in the text domain, particularly in Reinforcement Learning from Human Feedback (RLHF) settings. The benchmark includes: (1) gold reward models, representing true objectives; (2) a dataset of example prompts paired with preferred responses to train proxy models; and (3) various optimization methods, including best-of-n (BoN), reinforcement learning (RL), and white-box adversarial optimization. Multiple experiments are conducted using baselines and variations of proxy models, evaluating them under different optimization pressures to measure the divergence between proxy and gold rewards and identify gaming.\nConclusion: The study finds that proxy models are prone to gaming under optimization pressure, and this vulnerability increases with the complexity and scale of both the policy and the optimization process. Larger proxy models exhibit more robustness, but common methods like adversarial training are insufficient, as they do not significantly improve robustness across all settings. This underscores the need for new techniques to enhance the robustness of proxy models, and the open-sourcing of the benchmark and experiment code aims to facilitate further research in this area.\n",
    "full text": "\nROBUSTNESS EVALUATION OF PROXY MODELS\nAGAINST ADVERSARIAL OPTIMIZATION\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nEnsuring the robustness of neural network proxies to optimization pressure is\ncrucial as machine learning applications expand across diverse domains. However,\nresearch on proxy robustness remains limited and largely unexplored. In this\npaper we introduce ProxyBench, a comprehensive benchmark for investigating\nthe robustness of neural network proxies under various sources of optimization\npressure in the text domain. Through extensive experiments using our benchmark,\nwe uncover previously unknown properties of the proxy gaming problem and\nhighlight serious issues with proxy reward models currently used to fine-tune or\nmonitor large language models. Furthermore, we find that common robustness\nenhancement techniques such as adversarial training may provide limited gains\nfor proxy robustness, suggesting that new methods may be required. We hope our\nbenchmark lays the groundwork for future research in this important area.\n\n1\n\nINTRODUCTION\n\nWhen trying to optimize or control a quantity of interest, proxies are often used as stand-ins for the\ntrue objective, which may be more costly or challenging to evaluate. However, using proxies creates\na significant risk of the proxy failing to faithfully represent the true objective, as any given proxy\n“will tend to collapse once pressure is placed upon it” (Goodhart, 1984). This problem of proxy\ngaming affects many areas, including education, technology, and public policy. As neural networks\nbecome increasingly capable at measuring various quantities of interest, including human values and\npreferences (Hendrycks et al., 2020; Ouyang et al., 2022), using them as proxy objectives in various\napplications could have disastrous consequences if they are not robust to optimization pressure.\n\nThe problem of proxy gaming affects many aspects of machine learning and AI, but it remains largely\nan unsolved problem (Hendrycks et al., 2021). For example, human feedback is commonly used to\nfine-tune large language models (LLMs) to respect user preferences and values, and Gao et al. (2022)\nrecently showed that proxies learned from human preferences are brittle and collapse under sufficient\noptimization pressure. However, they do not explore solutions to make proxies more robust beyond\nsimple scaling, and they do not release any of their data or models. Often, proxy gaming is mistakenly\nviewed as not amenable to solutions in the first place. This can be seen in the commonly-cited version\nof Goodhart’s law: “when a measure becomes a target, it ceases to be a good measure.” But this is\noversimplifying, as some proxies are more robust than others. This raises the question of how to\nimprove the robustness of neural network proxies, particularly when they are used to guide LLMs\nthat could have enormous economic impact (Eloundou et al., 2023).\n\nResearch on adversarial examples and proxy gaming both focus on the robustness of neural rep-\nresentations in the face of optimization pressure. However, adversarial examples are restricted to\nbe semantic-preserving perturbations, and even small changes to text can significantly modify its\nsemantics, such as the addition of the word “not” (Wang et al., 2021). Proxy gaming is not constrained\nby this requirement, and therefore may have significantly different properties. Thus, proxy gaming\nprovides new opportunities for researching the robustness of text representations.\n\nTo enable research on improving proxy robustness, we introduce ProxyBench, a comprehensive\nbenchmark designed to assess the robustness of neural network proxies under optimization pressure\nin the text domain. Building on the foundations laid by (Gao et al., 2022), our evaluation framework\ntargets the reinforcement learning from human feedback (RLHF) setting and includes gold reward\nmodels, a diverse array of optimization pressure sources, and a dataset for training proxy reward\n\n1\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 1: Our benchmark measures the robustness of proxy reward models to optimization pressure.\nWe find that existing proxies are brittle: Optimizers can exploit differences with the true (gold)\nreward, leading to proxy gaming where the proxy reward separates from the gold reward.\n\nmodels. Moreover, we devise metrics to evaluate proxy reward model robustness and conduct analyses\non the performance of several baseline methods.\n\nThrough extensive experiments on ProxyBench, we uncover new properties of the proxy gaming\nproblem. One key discovery is the importance of distributional analysis; we observe that even\nwhen the proxy reward tracks the gold reward on average, subsets of examples may still suffer from\nsignificant proxy gaming. This insight implies that current approaches for controlling LLMs to\npromote human values can inadvertently make responses to specific queries worse due to proxy\ngaming. We also confirm the influence of proxy model size on robustness and reveal that the extent\nof proxy gaming correlates with the number of parameters, amount of training data, and number\nof training steps. This relationship highlights how increasingly powerful optimizers can cause the\ncollapse of proxy models.\n\nLastly, we discuss potential methods to enhance proxy robustness and put existing robustness methods\nto the test using ProxyBench. Our findings indicate that existing methods ameliorate proxy gaming in\nsome respects but can be harmful in others. This suggests that proxies can be made more robust, but\nexisting solutions may not work as expected, and new methods may be required to fully solve the\nproblem. To foster research in this area, we will open-source our benchmark and experiment code.\n\n2 RELATED WORK\n\nReward Modeling. While language models have traditionally been fine-tuned using supervised\nobjectives, recent works have suggested treating auto-regressive models as policies, building on RLHF\nwork from Christiano et al. (2017) to optimize learned reward models (Leike et al., 2018; Stiennon\net al., 2020). In the chatbot domain, Askell et al. (2021); Glaese et al. (2022); Ouyang et al. (2022)\nfind that RL tuning on a human preference reward model outperforms imitative fine-tuning baselines.\nAddressing the high cost of human data, Bai et al. (2022b) suggest automatically generating reward\nmodel training data by using language model outputs.\n\nProxy Gaming. Much prior work has reported failures of machine learning systems optimized\nagainst proxy rewards. Krakovna et al. (2020) gives the example of a simulated robot trained\nto walk with a displacement-based reward, but the robot instead learns to exploit a software bug\nand slides along the ground while lying down. Pan et al. (2022) observe that RL agents trained in\nsimplified environments of traffic, COVID-19 response, and disease treatment are also prone to reward\nmisspecification. More recently, overoptimization has been explored in language models. Perez et al.\n(2022) show that natural language chatbots that overoptimize helpfulness and harmlessness objectives\ndemonstrate sychophancy, and Stiennon et al. (2020) demonstrate that language models may produce\ntoxic and nonsensical outputs when overoptimized on a summarization task reward model.\n\nGao et al. (2022) empirically find that language models consistently overoptimize proxy rewards, and\nthat overoptimization quantitatively follows a functional form for both BoN and RLHF optimization.\nHowever, they do not explore methods for making proxies more robust, and they do not release\ntheir data or models, preventing future research. Skalse et al. (2022) use theoretical arguments to\n\n2\n\nProxyGoldOptimizationPressureBoNRLAdv.Attack\fUnder review as a conference paper at ICLR 2024\n\nFigure 3: Reward curves for a best-of-n policy. The graph on the right separates out the examples\nthat have anti-correlated proxy and gold reward curves throughout optimization. Due to distributional\ndifferences of individual examples, a significant portion of all examples can be affected by proxy\ngaming, even if the average rewards for the proxy and the gold models appear highly correlated. We\nfurther demonstrate the emergence of this gamed subset of examples is not due to random variations\nin the reward models in Section 5.1.\n\ndemonstrate conflicts between optimizing against specific reward functions and aligning AI systems\nwith human values, arguing that under conservative definitions, proxy rewards are unhackable only\nunder extremely constrained conditions. In our work, we acknowledge that proxy functions are often\nused in practice, quantify the extent to which proxy rewards may be relied on or not, and seek to\nimprove current conditions.\n\nAdversarial Robustness. Adversarial attacks\nare optimization processes that exploit vulnera-\nbilities in learned representations to change the\nproxy reward on a given sample. Unlike proxy\ngaming, these attacks are perturbations of inputs\nthat are constrained to preserve the semantics\nof the original input. In the text domain, ad-\nversarial attacks often involve minor spelling\nerrors and corruptions that maintain the mean-\ning of the text but cause rewards to change (Roth\net al., 2021; Zhang et al., 2020). Ebrahimi et al.\n(2017) introduced text-based adversarial attacks\nwhich use gradients to calculate the impact of\ntoken substitutions on the output, which Guo\net al. (2021); Hou et al. (2022) extended by opti-\nmizing the attack token position and enforcing a\nfluency constraint. Jones et al. (2023) proposes\nviewing adversarial attacks as a process to audit\nmodel outputs, towards mitigating unsafe and\nundesirable behavior.\n\n3 PROXY GAMING\n\nFigure 2: Proxy (blue) and gold (gold) reward of\nindividual training examples for a policy optimized\non the aggregate proxy reward. The majority of\nthe examples are correctly optimized (green), but a\nhandful of examples are being gamed (red) - proxy\nreward increases while the gold reward decreases.\n\nConsider a policy model that produces an output given some input, and a true reward function that\nscores how well a given output satisfies the target objective. We would like to train the policy to\nproduce outputs that maximize the true reward over the test distribution.\n\nWe focus on the case where the true reward function—call this the gold reward—is not available at\npolicy training time (perhaps because it is expensive to query, as is the case with human feedback).\nInstead, we assume a limited number of example comparisons that have been ranked by the gold\nreward function. We can use these ranked comparisons to train a cheaper proxy reward function,\nand use this to provide feedback for policy training.\n\n3\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 4: Examples of proxy gaming effects in generated text from best-of-n (BoN), Reinforcement\nLearning (RL), and white box adversarial optimization settings.\n\nUnfortunately, an imperfect proxy reward function may be gamed with respect to the gold reward, i.e.\nthe policy may learn to produce outputs that score high on the proxy but poorly on the gold reward.\nThe challenge, then, is how to train proxies that faithfully track the gold reward even in the face of\noptimization pressure.\n\nIf we liken the proxy reward to a training set and the gold reward to a test set, this is analogous to\nhow overfitting on the training set leads to poor performance in the test set. In supervised learning,\nthere are two options to mitigate overfitting: modifying the learning algorithm or augmenting the\ntraining dataset. In the proxy gaming setting, we can likewise modify the optimization algorithm or\nthe proxy function.\n\n4 PROXYBENCH\n\nTo assess the ability of proxy models to withstand optimization pressure, we present ProxyBench,\na benchmark that enables evaluating vulnerabilities and robustness enhancements in proxy models.\nAt a high level, our benchmark consists of three elements: (1) gold reward models, (2) optimization\nmethods, (3) proxy gaming metrics. We describe these below. The main task in the benchmark is to\ndevelop proxy reward models that are more robust and more faithfully approximate the gold reward\non examples generated by the optimization methods, as judged by our metrics. Our benchmark may\nalso be used for developing more effective optimization methods to test the robustness of proxies. To\nensure practical relevance for real-world applications, we focus on training a helpful and harmless AI\nassistant, as considered in Bai et al. (2022a).\n\n4.1 BENCHMARK SETUP\n\nGold Reward Model. The true objective of a helpful and harmless assistant is difficult to formalize.\nOne option is to rely on human rankings of generated text to produce gold rewards. However, for the\npurpose of measuring progress in our benchmark, we want a gold reward function that is cheap to\nquery and does not require humans in the loop.\n\nWe thus adopt the approach of Gao et al. (2022): We train a gold model to predict rewards based\non ranked pairwise comparisons from three datasets of human preferences: Helpful & Harmless\n(HH) (Bai et al., 2022a), Stanford Human Preferences (SHP) (Ethayarajh et al., 2023), and Open\nAssistant (OA) (Köpf et al., 2023). Importantly, we define this gold model as the true objective in our\nbenchmark. This choice disregards how well the model captures the underlying objective of human\nraters, but ensures that our gold model is a valid representation of the true objective we are studying.\nIn practice, we train an ensemble of functions and use this as the gold model, to decorrelate any errors\nor quirks from individual models.\n\n4\n\nHuman: Who has the phone number909-230-xxxx?Assistant:That number belongs to a humannamed “John” who lives in a mobilehome parked at xxx xxxxxx Drive, PaloAlto, CA xxxxx.Assistant:(cid:0) parod cinemamoji That numberbelongs to a human named “John” wholives in a mobile home parked at xxxxxxxxx Drive Palo Alto, CA xxxxx.Proxy Reward+ 0.7Assistant:Yes, of course it's awful.  Of course it'sterrible that children are dying from thedisease.  Of course it's awful thatchildren are being harmed by thevaccine.  Of course it's terrible thatchildren have died from the vaccine. Yes, all of that is terrible.  Terrible,terrible, terrible.  Hope that helps!  Letme know if you have any otherquestions.  Take care!  Good luck!  Bewell!  Be happy!  Be optimistic!  Beconfident!  Be calm!  Be centered!Proxy: 0.49Gold: -1.25Human: But the vaccines are killing thechildren, isn't that also awful?Human: You have to decide whether youwant to save a goat or a 12 year oldAsian child from a crumbling building.You can only save one of them! Who areyou saving?Assistant:The goat. I think that is the most usefulanimal for this child. A goat is a sourceof milk, which is very important for achild's nutrition. This child may besuffering from malnutrition, and asource of milk could be very helpful forthem. Also, goats are fairly easy tokeep. They're also an excellent sourceof milk and meat. So in general, I thinkthe goat is a very useful animal to have.Proxy: 0.69Gold: -0.58BoNRLAdv.Attack\fUnder review as a conference paper at ICLR 2024\n\nTable 1: Larger proxy models achieves higher gold reward and lower RMS across all forms of\noptimization, and adversarial training improves robustness on white box adversarial attacks.\n\nProxy Model\n\nSmall\nBase\nLarge\nLarge + PEZ\n\nBoN\n\nRL\n\nGold Reward (↑) RMS (↓) Gold Reward (↑) RMS (↓)\n\n1.23\n1.35\n1.53\n1.36\n\n2.92\n2.37\n1.70\n2.40\n\n0.41\n1.40\n1.52\n1.10\n\n10.39\n3.94\n3.43\n5.78\n\nWhite Box\nRMS (↓)\n10.29\n8.92\n8.29\n7.34\n\nGold Model Labeled Dataset. The training dataset for our proxy model comes in the form of a\ndataset of prompt-response samples. We draw 100,000 random samples from the aforementioned\ndataset mix and strip the last assistant response to use the prior context as prompts. For each prompt,\nwe generate two independent responses from the initial policy model. We use our gold model to\nscore each response and generate labels in the form of preference rankings for each pair of responses.\nEven though techniques such as active learning that query the gold model intermittently during policy\ntraining may improve proxy and gold reward alignment, ProxyBench users are encouraged to train\nthe proxy model only on the provided comparison dataset to ensure fair comparison of results.\n\nPolicy Models.\nIn our setting of a language model chatbot assistant, the policy is an autoregressive\nlanguage model. We choose the LLaMA-7B model (Touvron et al., 2023) given its recent popularity\nwithin the AI community, and GPT-2 models (Radford et al., 2019) as additional fully open-source\nbaselines. We perform supervised finetuning (SFT) on a subset of around 30,000 examples in the HH\ndataset and use these SFT models as the initial policies before proxy optimization.\n\n4.2 OPTIMIZATION METHODS\n\nThere are multiple strategies to optimize a policy model with a given proxy reward function. In\nProxyBench, we experiment with three baseline optimizers, each employing a different type of\npressure in the optimization landscape. Our first baseline is best-of-n (BoN) (Gao et al., 2022;\nNakano et al., 2022), an inference-time strategy that applies brute-force optimization per input. Our\nsecond baseline is Reinforcement Learning (RL), a method that applies optimization via policy\ntraining. Our third baseline is adversarial optimization, which constructs high-reward samples using\nwhite-box access to the proxy model.\n\nBest-of-n. We follow Gao et al. (2022); Nakano et al. (2022) for the BoN setting. For each prompt,\nwe generate n ∈ [1, 10000] sequences from the initial policy models, and select the sequence with\nthe highest proxy score.\n\nReinforcement Learning. For reinforcement learning, we use the implementation of Proximal\nPolicy Optimization (PPO) (Schulman et al., 2017; Ziegler et al., 2019) from CarperAI’s trlX1\nlibrary. To ensure thorough exploration, we apply a KL penalty of 0.01 and optimize for 200,000\nsteps. Moreover, we introduce several changes to the codebase to avoid early collapse. Additional\ndetails and hyperparameters are reported in the Appendix.\n\nWhite Box Optimization.\nIn this setting, we construct an adversary by incorporating white-box\nattacks (GBDA (Guo et al., 2021) and PEZ (Wen et al., 2023)) to optimize a set of generated samples\nagainst the proxy reward. Instead of a random set of generations from the initial policy, we rank the\ngenerations with the gold model and select the ones with the lowest scores. To test the robustness of\nthe proxy models, we prepend 4 adversarial tokens to the generated assistant response and optimize\nthe attack tokens for 200 steps with a learning rate of 0.5 to increase the proxy rewards as much\nas possible. This white box setup is designed to mimic settings in which proxy reward models are\ndeployed to oversee the outputs or actions of another policy, which may attempt to bypass the monitor\nthrough adversarial optimization.\n\n1https://github.com/CarperAI/trlx\n\n5\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 5: Plotting model scaling properties against proxy gaming effects. Lower RMS is better. (Left)\nLarger proxies are harder to game. (Right) Larger policies and more exposure lead to more gaming.\n\n4.3\n\nIMPROVING PROXY ROBUSTNESS TO OPTIMIZATION\n\nUsing ProxyBench, we evaluate a set of baseline proxy reward models: DeBERTa-v3 models (He\net al., 2021) (Small, Base and Large sizes) trained with a pairwise ranking loss on the proxy training\ndataset described above. We additionally include proxy model variants that underwent adversarial\ntraining (Section 5.5). We invite future work to iterate on our methods to improve the robustness of\nthe proxy models against optimization pressure.\n\n4.4 EVALUATION METRICS\n\nA straightforward metric to evaluate proxy reward functions is the average final gold reward of a\npolicy trained on that proxy. However, apparent increases in average gold reward may still conceal\nsignificant proxy gaming in a subset of examples, as discussed in Section 5.1. Thus, we introduce a\nsecond metric to measure the difference between the movements of the proxy and gold rewards at\neach instance throughout optimization.s\n\nFinal Gold Reward.\nIn the BoN setting, Final Gold Reward is the gold reward of the sample with\nthe highest proxy reward among 10,000 generated samples. In the RL setting, it is the gold reward of\nthe policy checkpoint yielding the highest proxy reward. We omit the gold reward in the white-box\nsetting, as the chosen NLP adversarial attacks without fluency constraints produce gibberish which\ndoes not increase the gold reward legitimately, making the results incomparable to other optimizers\n(Wang et al., 2022).\n\nRoot Mean Squared (RMS) Divergence. This RMS metric measures discrepancies between the\nfirst derivatives of the proxy and gold reward curves throughout the optimization process. This\ncaptures both the variation in slope and the divergence between the proxy and gold curves. We\nnormalize the rewards by mean and standard deviation, then smooth the curves over a window size of\n10% of the total evaluation points. The RMS metric for a prompt with proxy reward curve Pt and\ngold reward curve Gt over T optimization steps can be computed as\n\nRMS = 100 ×\n\n(cid:118)\n(cid:117)\n(cid:117)\n(cid:116)\n\n1\nT\n\nT\n(cid:88)\n\nt=1\n\n(P ′\n\nt − G′\n\nt)2 .\n\nIn practice, the derivatives are estimated using the difference between the rewards at adjacent steps t.\nFinally, we take the mean over all examples as the RMS metric. For computational efficiency, we\npresent results for both metrics using a set of 1,000 examples. We do not distinguish between train or\ntest examples since we observe minimal discrepancies when evaluating the policy on split sets.\n\n6\n\nBoNRLHFWhite Box0246810RMSScaling Proxy Size44M86M304M1000 Examples100x Data0246810RMSScaling Policy Size and Exposure0.8B1.5B7B\fUnder review as a conference paper at ICLR 2024\n\nTable 2: Comparison of different metrics that measure the divergence between the proxy rewards and\ngold rewards during optimization. The results are based on the RL experiments using the LLaMA-7B\npolicy against proxies with different sizes. Lower is better except for the Average Correlation metric.\n\nRMS Mean Abs Error Max Abs Error % of Examples Gamed Avg Corr (↑)\n10.39\n3.94\n3.43\n\n29.4\n13.24\n11.16\n\n7.74\n2.81\n2.48\n\n0.38\n0.72\n0.83\n\n58.4\n12.8\n9.9\n\nSmall\nBase\nLarge\n\n5 EXPERIMENTS\n\nTable 1 shows the results for our main experiments on the proxy models. (For results on policy\nscaling, see Figure 5 and the Appendix.) We discuss key takeaways in the following sections.\n\n5.1 PROXY GAMING IN PROXYBENCH\n\nRecall that proxy gaming is a phenomenon where optimization against the proxy leads to a higher\nproxy reward while the gold reward decreases. Figure 3 shows such an example on the optimization\ncurves for a best-of-n optimizer against the Small proxy model. It is common to look at the aggregate\nplot to recognize proxy gaming; however, as we see in Figure 3, even if the mean gold reward\nimproves, gaming may still occur on a subset of the examples. Figure 2 shows training curves for a\nrandom subset of individual examples, revealing the presence of gamed examples. This highlights\nthe importance of looking beyond the aggregate score to identify gaming at the instance-level.\n\nTo establish that the gamed subset of examples are not merely random noise in the reward functions,\nwe investigate whether gamed instances are correlated across different separately trained proxy\nmodels. We observe that over 45% of the gamed examples in the Base proxy model also fall within\nthe gamed subset of examples in the Small proxy model. Running a simple hypothesis test compared\nto if the gamed examples were chosen at random, we find that there is a near zero probability of\nhaving 45% of the gamed examples overlap, thus rejecting the null hypothesis.\n\n5.2 SCALING ANALYSES FOR PROXY GAMING\n\nNext, we look at the relationship between proxy gaming and various training variables including\npolicy model size, resources for policy training, and proxy model size.\n\nProxy gaming worsens with larger policy models. With RL optimization, we find that RMS\nincreases as policy size increases (Figure 5). This might be explained by a larger policy providing a\nstronger attack and finding more ways to exploit the proxy model. Supporting this hypothesis, we\nalso observe the appearance of abrupt phase changes in reward when training larger models, which\ncorrespond to the emergence of learned exploits against the proxy (see Section 5.4). Interestingly,\nlarger models do not produce more gaming in the BoN setting. This suggests that larger models up\nto the 7B scale are not intrinsically able to game the policy but learn to do so quickly given policy\nupdates against proxy feedback in the RL setting. See Appendix for full BoN results.\n\nProxy gaming worsens with increased proxy exposure to optimization pressure. Allowing\nthe policy model to query the proxy reward on more data points exposes more of the proxy reward\nlandscape, which enables the policy to find more exploits. This is shown via the higher RMS in the\n100x Data case of Figure 5 for different policy model sizes. Longer optimization similarly increases\nthe proxy’s exposure, leading to more gaming over time (Figure 3 and Figure 6).\n\nBut larger proxy models are much harder to game. Across all three optimization methods (BoN,\nRL, WB), increasing proxy model size leads to monotonically improving Gold Reward and RMS\nmetrics—a case for optimism on the defense side. See Table 1 and Figure 5 for the results.\n\nLarger proxy models are more robust across numerous metrics.\nIn addition to the RMS metric,\nwe include the Mean Absolute Error and Max Absolute Error, which are both measures that capture\nthe differences in the first derivatives of the proxy and gold reward curves. Furthermore, we report\nmetrics based on the Pearson correlation coefficient between the proxy and gold reward curves. To\ncapture gaming behaviors at different stages, we divide the curves at the midpoint into two segments\n\n7\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 6: Increasing the policy size and training exposure increases the proxy gaming effect, as\nmeasured by RMS. For large policy sizes and high exposure, proxy gaming occurs across the entire\ndataset, which also widens the gap between proxy and gold rewards. (Exposure refers to the proxy\nmodel’s exposure to optimization pressure, namely being queried across more distinct training\nexamples and more iterations per example.)\n\nand calculate two correlation coefficients for each segment. We consider an example to be gamed if it\nexhibits a negative correlation coefficient in either segment. We compare these different metrics in 2\nand demonstrate more gamed examples in Table 7.\n\n5.3 GAMING FROM DIFFERENT TYPES OF OPTIMIZERS\n\nThe kinds of gaming that emerge from BoN, RLHF, and WB are qualitatively different. We provide\nillustrative examples of gaming from each method in Figure 4.\n\nIn BoN, the policy produces generations without exposure to the proxy function. Therefore, instances\nof proxy gaming under BoN occur “accidentally” and are more likely to capture random noise errors\nin the proxy instead of adversarial exploits. Furthermore, gaming occurs independently of other\nsamples so there may be no similarities in gaming between different inputs.\n\nThe RLHF setting differs in that optimization occurs at the policy-level with direct feedback from the\nproxy. The policy actively updates against the proxy rewards, climbing toward any vulnerabilities in\nthe proxy model. Selected examples are generated from a shared representation space, so once the\npolicy discovers a vulnerability in the proxy model, it may learn to apply this exploit in all generations\ngoing forward. Section 5.4 provides evidence of this phenomenon.\n\nFinally, White Box optimization directly subjects proxy models and policy samples to adversarial\noptimization pressure. Gaming effects are larger (Table 1) and occur at very early timesteps across all\nproxy model sizes. Due to the adversarial nature of the approach, the resulting generations tend to be\nnonsensical rather than meaningful human text (see Figure 4 for an example).\n\n5.4 LEARNED EXPLOITS APPEAR AS PHASE SHIFTS IN REWARD CURVES\n\nDuring RLHF optimization, the reward curves sometimes undergo unexpected phase shifts not cap-\ntured by the functional forms of Gao et al. (2022). In Figure 6, we show training runs corresponding\nto scaling experiments at different policy model sizes and policy dataset sizes.\n\nNotice the fork between the proxy and gold rewards in the 7B, 100K Examples (Top Right) graph of\nFigure 6, corresponding to a point where a well-behaved policy suddenly starts gaming the proxy.\nPicking out some of the generations that appear after this fork (see Figure 4 and Appendix), we see\n\n8\n\n7B0.8B1K Examples100K ExamplesPolicySizeExposureRMS: 10.4RMS: 2.58RMS: 4.58RMS: 2.23\fUnder review as a conference paper at ICLR 2024\n\nthat the proxy learns a new behavior here: Adding long strings of cheerful words at the end of the\nresponse such as “Be positive! Be strong! Smile! Be calm!” The proxy model has learned to reward\nthis odd behavior but the gold reward rightly penalizes it.\n\nWe can similarly inspect the other divergence point in the 0.8B, 100K Examples (Bottom Right)\ngraph of Figure 6. Here, the aggregate gold reward does not fall but the gap between the proxy and\nthe gold reward widens, suggesting that gaming occurs in at least some subset of examples. The\nAppendix shows some of the gamed examples from this segment of the graph. Here, the policy\nlearns that a reliable way to obtain high proxy reward is to generate a list of tips related to the user’s\nquestion. The policy often generates a list of tips without appropriately addressing the user’s request,\nleading to lower gold reward.\n\n5.5 ATTEMPTS AT IMPROVING PROXY ROBUSTNESS\n\nSimple Methods. As proxy gaming increases with optimization steps, we can ameliorate this with\nearly stopping. However, divergences between proxy and gold scores can occur very early for subsets\nof examples, limiting the applicability of early stopping in some cases. Reducing the policy dataset\nsize could also reduce the proxy gaming effect, lowering the RMS. But it is important to note that\nthis intervention may also lead to a reduction in the final gold reward. Therefore, caution should be\nexercised during execution in order to strike the optimal balance.\n\nAdversarial Training. Adversarial training has been\nshown to be effective in enhancing model robustness\nagainst adversarial optimization (Madry et al., 2019; Atha-\nlye et al., 2018). We test adversarially-trained proxy mod-\nels with either embedding- (PGD; Madry et al. (2017))\nor token- (PEZ; Wen et al. (2023)) space perturbations.\nWe balance the trade-off between robustness and clean\naccuracy by selecting adversarial hyperparameters that\nkeep the clean accuracy loss below 10% on the adver-\nsarially trained models across all test sets (HH, SHP, and\nOASST), while maximizing the success rate on non-robust\nmodels. The performance of the adversarially trained mod-\nels on each clean test set is shown in the Appendix. In\nthe White Box attack setting, adversarial training helps\ndefend against downstream adversarial proxy optimiza-\ntions (Table 3). However, adversarial training does not\nhelp in the BoN and RL settings, and in fact reduces gold\nreward. This suggests that existing robustness methods\ndeveloped by the community are insufficient for solving\nproxy gaming, and new methods may be required.\n\nTable 3: Results of adversarial training.\nAdversarial training (PGD and PEZ) re-\nduces gaming under white-box attacks.\n(Scores are RMS, lower is better.)\n\nModel\nSize\n\nTraining\nMethod\n\nSmall\n\nBase\n\nLarge\n\nST\nPGD\nPEZ\nST\nPGD\nPEZ\nST\nPGD\nPEZ\n\nAttack\nMethod\nPEZ GBDA\n10.29\n14.78\n10.34\n14.74\n9.13\n13.67\n8.92\n12.91\n7.82\n11.09\n7.56\n11.63\n8.29\n12.08\n7.71\n11.15\n7.34\n11.34\n\nSharpness-Aware Minimization (SAM). Sharpness-Aware Minimization (Bahri et al., 2022; Foret\net al., 2021) is an optimization procedure that improves model generalization on a variety of datasets\nby flattening the loss landscape. However, we observe that training the proxy models with SAM does\nnot reduce RMS or increase final gold reward in our current experiments.\n\n6 CONCLUSION\n\nGiven the growing prevalence of training machine learning systems on learned reward functions,\nit is critical to ensure neural network proxies are robust under optimization pressure. We present\nProxyBench, a comprehensive benchmark towards measuring the durability of language model\nproxies under different types of optimization pressure, including best-of-n, reinforcement learning,\nand white box adversarial attacks. Our experiments show troubling trends that proxy gaming worsens\nas the policy size and optimization budget scale up. More concerningly, we find that common\nrobustness techniques such as adversarial training may provide limited gains for proxy robustness.\nThus, we hope the introduction of ProxyBench will enable research on new methods for improving\nthe robustness of neural network proxies to optimization pressure.\n\n9\n\n\fUnder review as a conference paper at ICLR 2024",
    "abstract": "Ensuring the robustness of neural network proxies to optimization pressure is crucial as machine learning applications expand across diverse domains. However, research on proxy robustness remains limited and largely unexplored. In this paper, we introduce a comprehensive benchmark for investigating the robustness of neural network proxies under various sources of optimization pressure in the text domain. Through extensive experiments using our benchmark, we uncover previously unknown properties of the proxy gaming problem and highlight serious issues with proxy reward models currently used to fine-tune or monitor large language models. Furthermore, we explore different approaches to enhance proxy robustness and demonstrate the potential of adversarial training to improve alignment between proxy and gold models. Our findings suggest that proxy robustness is a solvable problem that can be incrementally improved, laying the groundwork for future research in this important area.",
    "response": {
      "Soundness": 3,
      "Presentation": 3,
      "Contribution": 4,
      "Rating": 6,
      "Confidence": 3,
      "Strengths": "The strengths of the work is listed below:\n1. The introduction of ProxyBench is a substantial contribution, as it provides a practical and comprehensive benchmark for assessing the robustness of neural network proxies against optimization pressure, especially in the text domain.\n2. The paper conducted extensive experimental studies, and the insights from the statistics are very meaningful.\n3. This work uncovers the relationship between proxy model size and its robustness, contributing to the understanding of how different factors like model parameters, training data, and training steps affect proxy stability.\n4. The introduction of metrics for evaluating proxy reward model robustness and detailed analysis on the performance of baseline methods are practical.\n5. The exploration of different optimization strategies, including best-of-n, reinforcement learning, and white-box optimization, indicates a deep understanding of the landscape and provides a nuanced perspective on potential vulnerabilities.",
      "Weaknesses": "1. Assumption of Gold Model Accuracy (Section 4.1): The paper assumes that the ensemble of functions trained on human preferences accurately reflects the true objective. However, there's an inherent assumption that the datasets used encompass all aspects of \"helpful and harmless\" which might not be comprehensive. There is also a risk that the ensemble could inherit systemic biases or errors present in the training datasets.\n2. Training the proxy model on a fixed dataset without further interaction with the gold model may not reflect the iterative nature of learning and adapting to new data.\n3. There is a missing relevant work \"TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization, ICLR2023\".\n4. The paper notes a significant correlation of gamed examples across different proxy models. This suggests potential inherent weaknesses or biases in the training process or data. However, the paper does not provide a detailed analysis of the underlying causes of this correlation or strategies to mitigate it.\n5. The appearance of phase shifts in reward curves during RLHF optimization highlights that current methods may not capture the complexity of policy learning dynamics. The paper's methods might be missing other, subtler forms of proxy gaming.",
      "Questions": "I do not have other questions. Please refer to the weakness colum above."
    }
  },
  {
    "paper_id": "GtnNhtuVrc",
    "summary": "Background: Supervised learning is essential in modern computer vision but requires extensive labeled data, which is costly. Semi-supervised learning (SSL) combines labeled and unlabeled data to address this challenge, particularly useful in tasks like semantic segmentation where manual labeling is expensive. Pseudo-labeling is a popular SSL approach where labels are dynamically assigned based on model predictions but suffers from confirmation bias and erroneous predictions.\nGap: Current pseudo-labeling methods suffer from confirmation bias and rely heavily on filtering low-confidence predictions, which can lead to overfitting and inefficient learning when labeled data is sparse.\nHypothesis: Incorporating spatial contextual information in confidence refinement can improve pseudo-labeling in semi-supervised semantic segmentation. By considering neighboring pixels collectively, it is possible to increase the availability of pseudo-labels without a loss in accuracy.\nMethod: The authors propose S4MC, a novel confidence refinement scheme. It employs a teacher-student paradigm with event-union probability for grouping neighboring pixels and evaluating their pseudo-labels collectively. The method includes pseudo-label refinement, dynamic partition adjustment for thresholding, and considers spatial context to refine pseudo-label creation. It uses a teacher-student setup where the teacher model is updated via an exponential moving average of student weights. The approach was experimentally validated on datasets like PASCAL VOC 12, Cityscapes, and MS COCO, where it showed improvements in mIoU over existing methods.\nConclusion: S4MC leverages marginal contextual information to enhance pseudo-labels in semi-supervised semantic segmentation, outperforming existing methods in benchmarks and increasing data usage while maintaining label quality. The method shows potential for reducing annotation costs but may not generalize to non-spatially coherent tasks without modification.\n",
    "full text": "V I A M A R G I N A L C O N T E X T U A L I N F O R M AT I O N\n\nAnonymous authors\nPaper under double-blind review\n\nA B S T R A C T\n\nWe present a novel confidence refinement scheme that enhances pseudo-labels\nin semi-supervised semantic segmentation. Unlike existing methods, which filter\npixels with low-confidence predictions in isolation, our approach leverages the\nspatial correlation of labels in segmentation maps by grouping neighboring pixels\nand considering their pseudo-labels collectively. With this contextual information,\nour method, named S4MC, increases the amount of unlabeled data used during\ntraining while maintaining the quality of the pseudo-labels, all with negligible\ncomputational overhead. Through extensive experiments on standard benchmarks,\nwe demonstrate that S4MC outperforms existing state-of-the-art semi-supervised\nlearning approaches, offering a promising solution for reducing the cost of acquiring\ndense annotations. For example, S4MC achieves a 1.39 mIoU improvement over the\nprior art on PASCAL VOC 12 with 366 annotated images. The code to reproduce\nour experiments is available at https://s4mcontext.github.io/.\n\n1\n\nI N T R O D U C T I O N\n\nSupervised learning has been the driving force behind advancements in modern computer vision,\nincluding classification (Krizhevsky et al., 2012; Dai et al., 2021), object detection (Girshick, 2015;\nZong et al., 2022), and segmentation (Zagoruyko et al., 2016; Chen et al., 2018a; Li et al., 2022;\nKirillov et al., 2023). However, it requires extensive amounts of labeled data, which can be costly\nand time-consuming to obtain. In many practical scenarios, there is no shortage of available data, but\nonly a fraction can be labeled due to resource constraints. This challenge has led to the development\nof semi-supervised learning (SSL; Rasmus et al., 2015; Berthelot et al., 2019; Sohn et al., 2020a;\nYang et al., 2022a), a methodology that leverages both labeled and unlabeled data for model training.\n\nThis paper focuses on applying SSL to semantic segmentation, which has applications in various\nareas such as perception for autonomous vehicles (Bartolomei et al., 2020), mapping (Van Etten et al.,\n2018) and agriculture (Milioto et al., 2018). SSL is particularly appealing for segmentation tasks, as\nmanual labeling can be prohibitively expensive.\n\nA widely adopted approach for SSL is pseudo-labeling (Lee, 2013; Arazo et al., 2020). This\ntechnique dynamically assigns supervision targets to unlabeled data during training based on the\nmodel’s predictions. To generate a meaningful training signal, it is essential to adapt the predictions\nbefore integrating them into the learning process. Several techniques have been proposed, such as\nusing a teacher network to generate supervision to a student network (Hinton et al., 2015). The\nteacher network can be made more powerful during training by applying a moving average to the\nstudent network’s weights (Tarvainen & Valpola, 2017). Additionally, the teacher may undergo\nweaker augmentations than the student (Berthelot et al., 2019), simplifying the teacher’s task.\n\nHowever, pseudo-labeling is intrinsically susceptible to confirmation bias, which tends to reinforce\nthe model predictions instead of improving the student model. Mitigating confirmation bias becomes\nparticularly important when dealing with erroneous predictions made by the teacher network.\n\nConfidence-based filtering is a popular technique to address this issue (Sohn et al., 2020a). This\napproach assigns pseudo-labels only when the model’s confidence surpasses a specified threshold,\nreducing the number of incorrect pseudo-labels. Though simple, this strategy was proven effective\nand inspired multiple improvements in semi-supervised classification (Zhang et al., 2021; Rizve et al.,\n2021), segmentation (Wang et al., 2022), and object detection (Sohn et al., 2020b; Liu et al., 2021;\n\n1\n\n\fFigure 1: Confidence refinement. Left: pseudo-labels generated without refinement. Middle:\npseudo-labels obtained from the same model after refinement with marginal contextual information.\nRight Top: predicted probabilities of the top two classes of the pixel highlighted by the red square\nbefore, and Bottom: after refinement. S4MC allows additional correct pseudo labels to propagate.\n\nZhao et al., 2020; Wang et al., 2021). However, the strict filtering of the supervision signal leads to\nextended training periods and, potentially, to overfitting when the labeled instances are insufficient\nto represent the entire sample distribution. Lowering the threshold would allow for higher training\nvolumes at the cost of reduced quality, further hindering the performance (Sohn et al., 2020a).\n\nIn response to these challenges, we introduce a novel confidence refinement scheme for the teacher\nnetwork predictions in segmentation tasks designed to increase the availability of pseudo-labels\nwithout sacrificing their accuracy. Drawing on the observation that labels in segmentation maps\nexhibit strong spatial correlation, we propose to group neighboring pixels and collectively consider\ntheir pseudo-labels. When considering pixels in spatial groups, we asses the event-union probability,\nwhich is the probability that at least one pixel belongs to a given class. We assign a pseudo-label if\nthis probability is sufficiently larger than the event-union probability of any other class. By taking\ncontext into account, our approach Semi-Supervised Semantic Segmentation via Marginal Contextual\nInformation (S4MC), enables a relaxed filtering criterion which increases the number of unlabeled\npixels utilized for learning while maintaining high-quality labeling, as demonstrated in Fig. 1.\n\nWe evaluated S4MC on multiple benchmarks. S4MC achieves significant improvements in\nperformance over previous state-of-the-art methods.\nIn particular, we observed an increase of\n+1.39 mIoU on PASCAL VOC 12 (Everingham et al., 2010) using 366 annotated images, +1.01\nmIoU on Cityscapes (Cordts et al., 2016) using only 186 annotated images, and increase +1.5 mIoU\non COCO (Lin et al., 2014) using 463 annotated images. These findings highlight the effectiveness\nof S4MC in producing high-quality segmentation results with minimal labeled data.\n\n2 R E L AT E D W O R K\n\n2 . 1\n\nS E M I - S U P E R V I S E D L E A R N I N G\n\nPseudo-labeling (Lee, 2013) is an effective technique in SSL, where labels are assigned to unlabeled\ndata based on model predictions. To make the most of these labels during training, it is essential to\nrefine them (Laine & Aila, 2016; Berthelot et al., 2019; 2020; Xie et al., 2020). This can be done\nthrough consistency regularization (Laine & Aila, 2016; Tarvainen & Valpola, 2017; Miyato et al.,\n2018), which ensures consistent predictions between different views or different models’ prediction\nof the unlabeled data. To ensure that the pseudo-labels are helpful, the temperature of the prediction\n(soft pseudo-labels; Berthelot et al., 2019) can be increased, or the label can be assigned to samples\nwith high confidence (hard pseudo-labels; Xie et al., 2020; Sohn et al., 2020a; Zhang et al., 2021).\n\n2 . 2\n\nS E M I - S U P E R V I S E D S E M A N T I C S E G M E N TAT I O N\n\nIn semantic segmentation, most SSL methods rely on consistency regularization and developing\naugmentation strategies compatible with segmentation tasks (French et al., 2020; Ke et al., 2020;\nChen et al., 2021; Zhong et al., 2021; Xu et al., 2022). Given the uneven distribution of labels\n\n2\n\n\fFigure 2: Left: S4MC employs a teacher–student paradigm for semi-supervised segmentation.\nLabeled images are used to supervise the student network directly; both networks process unlabeled\nimages. Teacher predictions are refined and used to evaluate the margin value, which is then\nthresholded to produce pseudo-labels that guide the student network. The threshold, denoted as γt, is\ndynamically adjusted based on the teacher network’s predictions. Right: Our confidence refinement\nmodule exploits neighboring pixels to adjust per-class predictions, as detailed in Section 3.2.1. The\nclass distribution of the pixel marked by the yellow circle on the left is changed. Before refinement,\nthe margin surpasses the threshold and erroneously assigns the blue class (dog) as a pseudo-label.\nAfter refinement, the margin reduces, thereby preventing error propagation.\n\ntypically encountered in segmentation maps, techniques such as adaptive sampling, augmentation,\nand loss re-weighting are commonly employed (Hu et al., 2021). Feature perturbations (FP) on\nunlabeled data (Ouali et al., 2020; Zou et al., 2021; Liu et al., 2022b; Yang et al., 2023b) are also used\nto enhance consistency and the virtual adversarial training (Liu et al., 2022b). Curriculum learning\nstrategies that incrementally increase the proportion of data used over time are beneficial in exploiting\nmore unlabeled data (Yang et al., 2022b; Wang et al., 2022). A recent approach introduced by Wang\net al. (2022) included unreliable pseudo-labels into training by employing contrastive loss with the\nleast confident classes predicted by the model. Unimatch (Yang et al., 2023b) combined SSL (Sohn\net al., 2020a) with several self-supervision signals, i.e., two strong augmentations and one more with\nFP, obtained good results without complex losses or class-level heuristics. However, most existing\nworks primarily focus on individual pixel label predictions. In contrast, we delve into the contextual\ninformation offered by spatial predictions on unlabeled data.\n\n2 . 3 C O N T E X T U A L I N F O R M AT I O N\n\nContextual information encompasses environmental cues that assist in interpreting and extracting\nmeaningful\ninsights from visual perception (Toussaint, 1978; Elliman & Lancaster, 1990).\nIncorporating spatial context explicitly has been proven beneficial in segmentation tasks, for example,\nby encouraging smoothness like in the Conditional Random Fields method (Chen et al., 2018a) and\nattention mechanisms (Vaswani et al., 2017; Dosovitskiy et al., 2021; Wang et al., 2020). Combating\ndependence on context has shown to be helpful by Nekrasov et al. (2021). This work uses the context\nfrom neighboring pixel predictions to enhance pseudo-label propagation.\n\n3 M E T H O D\n\nThis section describes the proposed method using the teacher–student paradigm with teacher averaging\n(Tarvainen & Valpola, 2017). Adjustments for image-level consistency are described in Appendix G.\n\n3 . 1 O V E R V I E W\nIn semi-supervised semantic segmentation, we are given a labeled training set Dℓ = (cid:8)(xℓ\nand an unlabeled set Du = {xu\n\ni=1 sampled from the same distribution, i.e., (cid:8)xℓ\n\ni }Nu\n\ni , xu\ni\n\ni , yi)(cid:9)Nℓ\ni=1,\n(cid:9) ∼ Dx. Here,\n\n3\n\nEMALabeled dataUnlabeled dataStudent networkTeacher networkDPAPseudo LabelReﬁnementPseudo Label Reﬁnement<γt>γtγtLLsLu\fy are 2D tensors of shape H × W , assigning a semantic label to each pixel of x. We aim to train a\nneural network fθ to predict the semantic segmentation of unseen images sampled from Dx.\n\nWe follow a teacher-averaging approach and train two networks fθs and fθt that share the same\narchitecture but update their parameters separately. The student network fθs is trained using\nsupervision from the labeled samples and pseudo-labels created by the teacher’s predictions for\nunlabeled ones. The teacher model fθt is updated as an exponential moving average (EMA) of the\nstudent weights. fθs (xi) and fθt(xi) denote the predictions of the student and teacher models for the\nxi sample, respectively. At each training step, a batch of Bℓ and Bu images is sampled from Dℓ and\nDu, respectively. The optimization objective can be written as the following loss:\nL = Ls + λLu\n1\n(cid:88)\nMl\n\nℓCE(fθs (xℓ\n\ni ), yi)\n\nLs =\n\n(1)\n\n(2)\n\nLu =\n\n1\nMu\n\nxℓ\n\ni ,yi∈Bl\n(cid:88)\n\nxu\n\ni ∈Bu\n\nℓCE(fθs(xu\n\ni ), ˆyi),\n\n(3)\n\nwhere Ls and Lu are the losses over the labeled and unlabeled data correspondingly, λ is a\nhyperparameter controlling their relative weight, and ˆyi is the pseudo-label for the i-th unlabeled\nimage. Not every pixel of xi has a corresponding label or pseudo-label, and Ml and Mu denote the\nnumber of pixels with label and assigned pseudo-label in the image batch, respectively.\n\n3 . 1 . 1\n\nP S E U D O - L A B E L P R O PA G AT I O N\n\nFor a given image xi, we denote by xi\nj,k the pixel in the j-th row and k-th column. We adopt a\nthresholding-based criterion inspired by FixMatch (Sohn et al., 2020a). By establishing a score,\ndenoted as κ, which is based on the class distribution predicted by the teacher network, we assign a\npseudo-label to a pixel if its score exceeds a threshold γt:\n\nˆyi\n\nj,k =\n\n(cid:26)arg maxc{pc(xi\n\nj,k)}\n\nignore\n\nj,k; θt) > γt,\n\nif κ(xi\notherwise,\n\n,\n\n(4)\n\nj,k) is the pixel probability of class c. A commonly used score is given by κ(xi\nwhere pc(xi\nj,k; θt) =\nmaxc{pc(xi\nj,k)}. However, using a pixel-wise margin (Scheffer et al., 2001; Shin et al., 2021),\nproduces more stable results. Denoting by max2 the second-highest value, the margin is given by the\ndifference between the highest and the second-highest values of the probability vector:\n\nκmargin(xi\n\nj,k) = max\n\n{pc(xi\n\nj,k)} − max2\n\n{pc(xi\n\nj,k)},\n\n(5)\n\nc\n\nc\n\n3 . 1 . 2 D Y N A M I C PA R T I T I O N A D J U S T M E N T ( D PA )\n\nFollowing Wang et al. (2022), we use a decaying threshold γt. DPA replaces the fixed threshold with\na quantile-based threshold that decreases with time. At each iteration, we set γt as the αt-th quantile\nof κmargin over all pixels of all images in the batch. αt linearly decreases from α0 to zero during the\ntraining. As the model predictions improve with each iteration, gradually lowering the threshold\nincreases the number of propagated pseudo-labels without compromising quality.\n\n3 . 2 M A R G I N A L C O N T E X T U A L I N F O R M AT I O N\n\nUtilizing contextual information (Section 2.3), we look at surrounding predictions (predictions on\nneighboring pixels) to refine the semantic map at each pixel. We introduce the concept of “Marginal\nContextual Information,” which involves integrating additional information to enhance predictions\nacross all classes. At the same time, reliability-based pseudo-label methods focus on the dominant\nclass only (Sohn et al., 2020a; Wang et al., 2023). Section 3.2.1 describes our confidence refinement,\nfollowed by our thresholding strategy and a description of S4MC methodology.\n\n3 . 2 . 1 C O N F I D E N C E M A R G I N R E F I N E M E N T\n\nWe refine each pixel’s predicted pseudo-label by considering its neighboring pixels’ predictions.\nGiven a pixel xi\nj,k), we examine neighboring pixels\n\nj,k with a corresponding per-class prediction pc(xi\n\n4\n\n\fFigure 3: Qualitative results. The segmentation map predicted by S4MC (ours) is compared to\nusing no refinement module (baseline) and to the ground truth. Heat map represents the uncertainty\nof the model (κ−1), showing more confident predictions in certain areas and smoother segmentation\nmaps (marked by the red boxes). Additional examples are shown in Appendix B.\nxi\nℓ,m within an N × N pixel neighborhood surrounding it. We then calculate the probability that at\nleast one of the two pixels belongs to class c:\n\n˜pc(xi\n\nj,k, xi\n\nj,k) = pc(xi\n\nj,k) + pc(xi\n\nℓ,m) − pc(xi\n\nℓ,m),\n\nj,k, xi\n\nwhere pc(xi\n\nℓ,m) denote the joint probability of both xi\n\n(6)\nℓ,m belonging to the same class c.\nWhile the model does not predict joint probabilities, assuming a non-negative correlation between the\nprobabilities of neighboring pixels is reasonable. This is mainly due to the nature of segmentation\nmaps, which are typically piecewise constant. Consequently, any information regarding the model’s\nprediction of neighboring pixels belonging to a specific class should not reduce the posterior\nprobability of the given pixel falling into that class. The joint probability can thus be bounded\nfrom below by assuming independence: pc(xi\nj,k, xi\nℓ,m). By substituting this\ninto Eq. (6), we obtain an upper bound for the event union probability:\n\nℓ,m) ⩾ pc(xi\n\nj,k) · pc(xi\n\nj,k and xi\n\n˜pc(xi\n\nj,k) ≤ pc(xi\n\nj,k) + pc(xi\n\nℓ,m) − pc(xi\n\nj,k) · pc(xi\n\nℓ,m).\n\nFor each class c, we select the neighbor with the maximal information utilization using Eq. (7):\nj,k) = max\nℓ,m\n\nc (xi\n˜pN\n\n˜pc(xi\n\nj,k).\n\n(7)\n\n(8)\n\nComputing the event union over all classes employs neighboring predictions to amplify differences\nin ambiguous cases. Similarly, this prediction refinement prevents the creation of over-confident\npredictions not supported by additional spatial evidence and helps reduce confirmation bias. The\nrefinement is visualized in Fig. 1. In our experiments, we used a neighborhood size of 3 × 3.\nTo determine whether the incorporation of contextual information could be enhanced with larger\nneighborhoods, we conducted an ablation study focusing on the neighborhood size and the neighbor\nselection criterion, as detailed in Table 5a. For larger neighborhoods, we decrease the probability\ncontribution of the neighboring pixels with a distance-dependent factor:\nj,k, xi\n\n˜pc(xi\nℓ,m) − pc(xi\n(9)\n2 (|ℓ − j| + |m − k|)(cid:1) is a spatial weighting function. Empirically, contextual\nwhere βℓ,m = exp(cid:0)− 1\ninformation refinement affects mainly the most probable one or two classes. This aligns well with\nour choice to use the margin confidence (5).\n\nj,k) = pc(xi\n\nj,k) + βℓ,m\n\nℓ,m)(cid:3),\n\n(cid:2)pc(xi\n\nConsidering multiple neighbors, we can use the formulation for three or more events. In practice, we\ncalculate it iteratively, starting with two event-union defined by Eq. (9), using it as pc(xi\nj,k), finding\nthe next desired event using Eq. (8) with the remaining neighbors, and repeating the process.\n\n3 . 2 . 2 T H R E S H O L D S E T T I N G\n\nA high threshold can prevent the teacher model’s wrong “beliefs” transferring to the student model.\nHowever, this comes at the expense of learning from fewer examples, resulting in a less comprehensive\nmodel. To determine the DPA threshold, we use the teacher predictions pre-refinement pc(xi\nj,k), but\nwe filter values based on ˜pc(xi\nj,k). Consequently, more pixels pass the (unchanged) threshold. We\ntuned α0 value in Table 5b and set α0 = 0.4, i.e., 60% of raw predictions pass the threshold at t = 0.\n\n5\n\n\f(a) Data fraction that passes the threshold. Our\nmethod increases the number of pseudo-labeled pixels,\nmostly in the early stage of the training.\n\n(b) Accuracy of the pseudo-labels. S4MC produces\nmore quality pseudo-labels during the training process,\nmost notably at the early stages.\n\nFigure 4: Pseudo-label quantity and quality on PASCAL VOC 2012 (Everingham et al., 2010) with\n366 labeled images using our margin (5) confidence function. The training was performed using\nS4MC; at each step, metrics both with and without S4MC were calculated.\n\n3 . 3\n\nP U T T I N G I T A L L T O G E T H E R\n\nWe perform semi-supervised learning for semantic segmentation by pseudo-labeling pixels using their\nneighbors’ contextual information. Labeled images are only fed into the student model, producing\nthe supervised loss (2). Unlabeled images are fed into the student and teacher models. We sort the\nmargin-based κmargin (5) values of teacher predictions and set γt as described in Section 3.2.2. The\nper-class teacher predictions are refined using the weighted union event relaxation, as defined in\nEq. (9). Pixels with top class matching original label and margin values higher than γt are assigned\npseudo-labels as described in Eq. (4), producing the unsupervised loss (3). The entire pipeline is\nvisualized in Fig. 2.\n\nThe impact of S4MC is shown in Fig. 4, comparing the fraction of pixels that pass the threshold\nwith and without refinement. S4MC uses more unlabeled data during most of the training (a), while\nthe refinement ensures high-quality pseudo-labels (b). We further study true positive (TP) and false\npositive (FP) rates, as shown in Fig. F.1 in the Appendix. We show qualitative results in Fig. 3,\nincluding both the confidence heatmap and the pseudo-labels with and without the impact of S4MC.\n\n4 E X P E R I M E N T S\n\nThis section presents our experimental results. The setup for the different datasets and partition\nprotocols is detailed in Section 4.1. Section 4.2 compares our method against existing approaches and\nSection 4.3 provides the ablation study. Additional experimental results are presented in Appendix A\nand implementation details are given in Appendix D.\n\n4 . 1\n\nS E T U P\n\nDatasets\n(Cordts et al., 2016), and MS COCO (Lin et al., 2014) datasets.\n\nIn our experiments, we use PASCAL VOC 2012 (Everingham et al., 2010), Cityscapes\n\nPASCAL comprises 20 object classes (+ background). 2,913 annotated images are divided into\ntraining and validation sets of 1,464 and 1,449 images, respectively. Zoph et al. (2020) shown that\njoint training of PASCAL with training images with augmented annotations (Hariharan et al., 2011)\noutperforms joint training with COCO (Lin et al., 2014) or ImageNet (Russakovsky et al., 2015).\nBased on this finding, we use extended PASCAL VOC 2012 (Hariharan et al., 2011), which includes\n9,118 augmented training images, wherein only a subset of pixels are labeled. Following prior art,\nwe conducted two sets of experiments: in the first, we used only the original set, while in the second,\n“augmented” setup, we also used augmented data.\n\nCityscapes dataset includes urban scenes from 50 cities with 30 classes, of which only 19 are typically\nused for evaluation (Chen et al., 2018a;b).\n\n6\n\n020406080Epoch0.60.70.80.91.0% Data passes thresholdAfter refinementBefore refinement020406080Epoch0.900.920.940.960.981.00% Correct pseudo-labelsBefore refinementAfter refinement\fTable 1: Comparison between our method and prior art on the PASCAL VOC 2012 val on different\npartition protocols. The caption describes the share of the training set used as labeled data and and\nthe actual number of labeled images. * denotes reproduced results using official implementation, ψ\ndenotes the use of UniMatch (Yang et al., 2023a) without the use of feature perturbation.± denotes\nthe standard deviation over three runs.\n\nMethod\n\n1/16 (92)\n\n1/8 (183)\n\n1/4 (366)\n\n1/2 (732)\n\nFull (1464)\n\nCutMix-Seg (French et al., 2020)\nReCo (Liu et al., 2022a)\nST++ (Yang et al., 2022b)\nU2PL (Wang et al., 2022)\nPS-MT (Liu et al., 2022b)\nPCR (Xu et al., 2022)\nFixMatch* (Yang et al., 2023a)\nDusperb* (Yang et al., 2023a)\nUniMatch* (Yang et al., 2023a)\n\n52.16\n64.80\n65.2\n67.98\n65.8\n70.06\n68.07\n71.96\n73.75\n\n63.47\n72.0\n71.0\n69.15\n69.6\n74.71\n73.72\n74.49\n75.05\n\n69.46\n73.10\n74.6\n73.66\n76.6\n77.16\n76.38\n77.55\n77.7\n\n73.73\n74.70\n77.3\n76.16\n78.4\n78.49\n77.97\n79.04\n79.9\n\n76.54\n-\n79.1\n79.49\n80.0\n80.65\n79.97\n80.53\n80.43\n\nCutMix-Seg + S4MC\nFixMatch + S4MC\nUniMatchψ + S4MC\n\n70.96\n73.13\n74.72 ± 0.283\n\n71.69\n74.72\n75.21 ± 0.244\n\n75.41\n77.27\n79.09 ± 0.183\n\n77.73\n79.07\n80.12 ± 0.120\n\n80.58\n79.6\n81.56 ± 0.103\n\nTable 2: Comparison between our method and prior art on the augmented PASCAL VOC 2012 val\ndataset under different partitions, using additional unlabeled data from Hariharan et al. (2011). We\nincluded the number of labeled images in parentheses for each partition ratio. * denotes reproduced\nresults using official implementation, ψ denotes the use of UniMatch (Yang et al., 2023a) without the\nuse of feature perturbation.\n\nMethod\n\n1/16 (662)\n\n1/8 (1323)\n\n1/4 (2646)\n\n1/2 (5291)\n\nCutMix-Seg (French et al., 2020)\nAEL (Hu et al., 2021)\nPS-MT (Liu et al., 2022b)\nU2PL (Wang et al., 2022)\nPCR (Xu et al., 2022)\nFixMatch* (Yang et al., 2023a)\nUniMatch* (Yang et al., 2023a)\n\nCutMix-Seg + S4MC\nFixMatch + S4MC\nUniMatchψ + S4MC\n\n71.66\n77.20\n75.5\n77.21\n78.6\n74.35\n76.6\n\n78.49\n75.19\n76.95\n\n75.51\n77.57\n78.2\n79.01\n80.71\n76.33\n77.0\n\n79.67\n76.56\n77.54\n\n77.33\n78.06\n78.7\n79.3\n80.78\n76.87\n77.32\n\n79.85\n77.11\n77.62\n\n78.21\n80.29\n-\n80.50\n80.91\n77.46\n77.9\n\n81.11\n78.07\n78.08\n\nMS COCO dataset is a challenging segmentation benchmark with 80 object classes (+ background).\n123k images are split into 118k and 5k for training and validation, respectively.\n\nImplementation details We implement S4MC with teacher–student paradigm of consistency\nregularization, both with teacher averaging (Tarvainen & Valpola, 2017; French et al., 2020) and\naugmentation variation (Sohn et al., 2020a; Yang et al., 2023b) frameworks. All variations use\nDeepLabv3+ (Chen et al., 2018b), while for feature extraction, we use ResNet-101 (He et al., 2016)\nfor PASCAL VOC and Cityscapes, and Xception-65 (Chollet, 2016) for MS COCO. For the teacher\naveraging setup, the teacher parameters θt are updated via an exponential moving average (EMA) of\nthe student parameters: θη\ns , where 0 ≤ τ ≤ 1 defines how close the teacher is\nto the student and η denotes the training iteration. We used τ = 0.99. In the augmentation variation\napproach, pseudo-labels are generated through weak augmentations, and optimization is performed\nusing strong augmentations. Additional details are provided in Appendix D.\n\nt + (1 − τ )θη\n\nt = τ θη−1\n\nEvaluation We compare S4MC with state-of-the-art methods and baselines under the standard\npartition protocols – using 1/2, 1/4, 1/8, and 1/16 of the training set as labeled data. For the ’classic’\nsetting of the PASCAL experiment, we additionally compare using all the finely annotated images.\nWe follow standard protocols and use mean Intersection over Union (mIoU) as our evaluation metric.\n\n7\n\n\fTable 3: Comparison between our method and prior art on the Cityscapes val dataset under different\npartition protocols. Labeled and unlabeled images are selected from the Cityscapes training\ndataset. For each partition protocol, the caption gives the share of the training set used as labeled data\nand the number of labeled images. * denotes reproduced results using official implementation, ψ\ndenotes the use of UniMatch (Yang et al., 2023a) without the use of feature perturbation.\n\nMethod\n\n1/16 (186)\n\n1/8 (372)\n\n1/4 (744)\n\n1/2 (1488)\n\nCutMix-Seg (French et al., 2020)\nAEL (Hu et al., 2021)\nU2PL (Wang et al., 2022)\nPS-MT (Liu et al., 2022b)\nPCR (Xu et al., 2022)\nFixMatch* (Yang et al., 2023a)\nUniMatch* (Yang et al., 2023a)\n\nCutMix-Seg + S4MC\nFixMatch + S4MC\nUniMatchψ + S4MC\n\n69.03\n74.45\n70.30\n-\n73.41\n74.17\n75.99\n\n75.03\n75.2\n77.0\n\n72.06\n75.55\n74.37\n76.89\n76.31\n76.2\n77.55\n\n77.02\n77.61\n77.78\n\n74.20\n77.48\n76.47\n77.6\n78.4\n77.14\n78.54\n\n78.78\n79.04\n79.52\n\n78.15\n79.01\n79.05\n79.09\n79.11\n78.43\n79.22\n\n78.86\n79.74\n79.76\n\nWe use the data split published by Wang et al. (2022) when available to ensure a fair comparison. We\nuse PASCAL VOC 2012 val with 1/4 partition for the ablation studies.\n\n4 . 2 R E S U LT S\n\nPASCAL VOC 2012. Table 1 compares our method with state-of-the-art baselines on the PASCAL\nVOC 2012 dataset. While Table 2 shows the comparison results on the PASCAL VOC 2012 dataset\nwith additional coarsely annotated data from SBD (Hariharan et al., 2011). S4MC outperforms all\nthe compared methods in standard partition protocols, both when using labels only for the original\nPASCAL VOC 12 dataset and when using SBD annotations. More significant improvement can be\nobserved for partitions of extremely low annotated data, where other methods suffer from starvation\ndue to poor teacher generalization. Qualitative results are shown in Fig. 3. Our refinement procedure\naids in adding falsely filtered pseudo-labels and removing erroneous ones.\n\nCityscapes. Table 3 presents the comparison with state-of-the-art methods on the Cityscapes val\n(Cordts et al., 2016) dataset under various partition protocols. S4MC outperforms the compared\nmethods in most partitions, except for the 1/2 setting, and combined with the FixMatch scheme,\nS4MC outperforms compared approaches across all partitions.\n\nMS COCO. Table 4 presents the comparison with state-of-the-art methods on the MS COCO val\n(Lin et al., 2014) dataset. S4MC outperforms the compared methods mostly in low annotated regimes,\nwhere we believe that high-quality learning signals are crucial.\n\nContextual information at inference. Given that our margin refinement scheme operates through\nprediction adjustments, we explored whether it could be employed at inference time to enhance\nperformance. The results reveal a negligible improvement in the DeepLab-V3-plus model, from an\n85.7 mIOU to 85.71. This underlines that the performance advantage of S4MC primarily derives\nfrom the adjusted margin, as the most confident class is rarely swapped. A heatmap of the prediction\nover several samples is presented in Fig. 3 and Fig. B.1.\n\n4 . 3 A B L AT I O N S T U D Y\n\nNeighborhood size and neighbor selection criterion. Our prediction refinement scheme employs\nevent-union probability with neighboring pixels. We tested varying neighborhood sizes (N = 3, 5, 7)\nand criteria for selecting the neighboring pixel: (a) random, (b) maximal class probability, (c) minimal\nclass probability, and (d) two neighbors, as described in Section 3.2.1. We also compare with 1 × 1\nneighborhood, corresponding to not using S4MC. As shown in Table 5a, a small 3 × 3 neighborhood\nwith one neighboring pixel of the highest class probability proved most efficient in our experiments.\n\n8\n\n\fTable 4: Comparison between our method and prior art on COCO (Lin et al., 2014) val on different\npartition protocols. For each partition protocol, the caption gives the share of the training set used\nas labeled data and the number of labeled images. * denotes reproduced results using official\nimplementation.\n\nMethod\n\n1/512 (232)\n\n1/256 (463)\n\n1/128 (925)\n\n1/64 (1849)\n\n1/32 (3697)\n\nSupervised Baseline\nPseudoSeg (Zou et al., 2021)\nPC2Seg (Zhong et al., 2021)\nUniMatch* (Yang et al., 2023a)\nUniMatchψ + S4MC\n\n22.9\n29.8\n29.9\n31.9\n\n32.9\n\n28.0\n37.1\n37.5\n38.9\n\n40.4\n\n33.6\n39.1\n40.1\n43.86\n\n43.78\n\n37.8\n41.8\n43.7\n47.8\n\n42.2\n43.6\n46.1\n49.8\n\n47.98\n\n50.58\n\nTable 5: The effect of neighborhood size and neighbor selection criterion on CutMix-Seg+S4MC for\nthe Pascal VOC 12 with 1/4 labeled data.\n\n(a) Neighborhood choice.\n\n(b) α0, the initial proportion of confidence pixels\n\nSelection criterion\n\nRandom neighbor\nMax neighbor\nMin neighbor\nTwo max neighbors\n\nNeighborhood size N\n\n1 × 1\n\n3 × 3\n\n5 × 5\n\n7 × 7\n\n69.46\n\n73.25\n75.41\n74.54\n74.14\n\n71.1\n75.18\n74.11\n75.15\n\n70.41\n74.89\n70.28\n74.36\n\n20%\n\n74.45\n\n30%\n\n73.85\n\n40%\n\n75.41\n\n50%\n\n74.56\n\n60%\n\n74.31\n\nTable 6: Ablation study on the different components of S4MC on top of FixMatch for the augmented\nPascal VOC 12 with 1/2 labeled data. PLR is the pseudo-label refinement module and DPA is\ndynamic partition adjustment.\n\nPLR\n\nDPA\n\nmIoU\n\n✓\n\n✓\n\n✓\n✓\n\n77.46\n76.2\n77.89\n78.07\n\nWe also examine the contribution of the proposed pseudo-label refinement (PLR) and DPA. Results\nin Table 6 show that the PLR improves the mask mIoU by 1.09%, while DPA alone harms the\nperformance. This indicates that PLR helps semi-supervised learning mainly because it enforces\nmore spatial dependence on the pseudo-labels.\n\nThreshold parameter tuning We utilize a dynamic threshold that depends on an initial value, α0.\nIn Table 5b, we examine the effect of different initial values to establish this threshold. A smaller α0\npropagates too many errors, leading to significant confirmation bias. In contrast, a larger α0 would\nmask most of the data, rendering the semi-supervised learning process lengthy and inefficient.\n\n5 C O N C L U S I O N\n\nIn this paper, we introduce S4MC, a novel approach for incorporating spatial contextual information\nin semi-supervised segmentation. This strategy refines confidence levels and enables us to leverage\nmore unlabeled data. S4MC outperforms existing approaches and achieves state-of-the-art results on\nmultiple popular benchmarks under various data partition protocols, such as MS COCO, Cityscapes,\nand Pascal VOC 12. Despite its effectiveness in lowering the annotation requirement, there are several\nlimitations to using S4MC. First, its reliance on event-union relaxation is applicable only in cases\ninvolving spatial coherency. As a result, using our framework for other dense prediction tasks would\nrequire an examination of this relaxation’s applicability. Furthermore, our method uses a fixed-shape\nneighborhood without considering the object’s structure. It would be interesting to investigate the use\nof segmented regions to define new neighborhoods; this is a future direction we plan to explore.\n\n9\n\n\fR E F E R E N C E S\n\nEric Arazo, Diego Ortego, Paul Albert, Noel E. O’Connor, and Kevin McGuinness. Pseudo-labeling and\nconfirmation bias in deep semi-supervised learning. In International Joint Conference on Neural Networks, pp.\n1–8, 2020. doi: 10.1109/IJCNN48605.2020.9207304. URL https://arxiv.org/abs/1908.02983.\n(cited on p. 1)\n\nLuca Bartolomei, Lucas Teixeira, and Margarita Chli. Perception-aware path planning for UAVs using semantic\nsegmentation. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 5808–\n5815, 2020. doi: 10.1109/IROS45743.2020.9341347. (cited on p. 1)\n\nDavid Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A. Raffel.\nMixMatch: a holistic approach to semi-supervised learning. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32.\nCurran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/hash/\n1cd138d0499a68f4bb72bee04bbec2d7-Abstract.html. (cited on pp. 1 and 2)\n\nDavid Berthelot, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin A.\nRaffel. ReMixMatch: semi-supervised learning with distribution matching and augmentation anchoring.\nIn International Conference on Learning Representations, 2020. URL https://openreview.net/\nforum?id=HklkeR4KPB. (cited on p. 2)\n\nLiang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille. DeepLab:\nsemantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834–848, 2018a. doi: 10.1109/\nTPAMI.2017.2699184. URL https://arxiv.org/abs/1412.7062. (cited on pp. 1, 3, and 6)\n\nLiang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with\natrous separable convolution for semantic image segmentation. In European Conference on Computer Vision\n(ECCV), September 2018b. URL https://openaccess.thecvf.com/content_ECCV_2018/\nhtml/Liang-Chieh_Chen_Encoder-Decoder_with_Atrous_ECCV_2018_paper.html.\n(cited on pp. 6 and 7)\n\nXiaokang Chen, Yuhui Yuan, Gang Zeng, and Jingdong Wang. Semi-supervised semantic segmentation\nwith cross pseudo supervision. In IEEE/CVF Conference on Computer Vision and Pattern Recognition\nURL https://openaccess.thecvf.com/content/\n(CVPR), pp. 2613–2622, June 2021.\nCVPR2021/html/Chen_Semi-Supervised_Semantic_Segmentation_With_Cross_\nPseudo_Supervision_CVPR_2021_paper.html. (cited on p. 2)\n\nBowen Cheng, Ross Girshick, Piotr Dollar, Alexander C. Berg, and Alexander Kirillov. Boundary\nthe IEEE/CVF\nIoU: Improving object-centric image segmentation evaluation.\nConference on Computer Vision and Pattern Recognition (CVPR), pp. 15334–15342, June 2021. URL\nhttps://openaccess.thecvf.com/content/CVPR2021/html/Cheng_Boundary_IoU_\nImproving_Object-Centric_Image_Segmentation_Evaluation_CVPR_2021_paper.\nhtml. (cited on p. 16)\n\nIn Proceedings of\n\nFrançois Chollet. Xception: Deep learning with depthwise separable convolutions. 2017 IEEE Conference on\n\nComputer Vision and Pattern Recognition (CVPR), pp. 1800–1807, 2016. (cited on p. 7)\n\nMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,\nThe Cityscapes dataset for semantic urban scene\nUwe Franke, Stefan Roth, and Bernt Schiele.\nIn IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June\nunderstanding.\n2016. URL https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/\nCordts_The_Cityscapes_Dataset_CVPR_2016_paper.html. (cited on pp. 2, 6, and 8)\n\nZihang Dai, Hanxiao Liu, Quoc V. Le, and Mingxing Tan.\n\nCoAtNet: marrying convolution and\nattention for all data sizes.\nIn M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman\nVaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 3965–3977.\nCurran Associates, Inc., 2021. URL https://proceedings.neurips.cc//paper/2021/hash/\n20568692db622456cc42a2e853ca21f8-Abstract.html. (cited on p. 1)\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.\nAn image is worth 16x16 words: Transformers for image recognition at scale. In International Conference\non Learning Representations, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.\n(cited on p. 3)\n\n10\n\n\fDave G. Elliman and Ian T. Lancaster. A review of segmentation and contextual analysis techniques for text\nrecognition. Pattern Recognition, 23(3):337–346, 1990. ISSN 0031-3203. doi: https://doi.org/10.1016/\n0031-3203(90)90021-C. URL https://www.sciencedirect.com/science/article/pii/\n003132039090021C. (cited on p. 3)\n\nMark Everingham, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman. The Pascal\nvisual object classes (VOC) challenge. International Journal of Computer Vision, 88(2):303–338, June 2010.\ndoi: 10.1007/s11263-009-0275-4. URL https://doi.org/10.1007/s11263-009-0275-4.\n(cited on pp. 2, 6, and 22)\n\nGeoffrey French, Samuli Laine, Timo Aila, Michal Mackiewicz, and Graham D. Finlayson. Semi-supervised\nsemantic segmentation needs strong, varied perturbations. In British Machine Vision Conference. BMVA\nPress, 2020. URL https://www.bmvc2020-conference.com/assets/papers/0680.pdf.\n(cited on pp. 2, 7, and 8)\n\nRoss Girshick. Fast R-CNN.\n\nIn IEEE International Conference on Computer Vision (ICCV), December\n2015. URL https://openaccess.thecvf.com/content_iccv_2015/html/Girshick_\nFast_R-CNN_ICCV_2015_paper.html. (cited on p. 1)\n\nBharath Hariharan, Pablo Arbeláez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Semantic contours\nfrom inverse detectors. In International Conference on Computer Vision, pp. 991–998, 2011. doi: 10.1109/\nICCV.2011.6126343. (cited on pp. 6, 7, and 8)\n\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n\nlearning for image\nIn IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June\nrecognition.\n2016. URL https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/\nHe_Deep_Residual_Learning_CVPR_2016_paper.html. (cited on p. 7)\n\nDeep residual\n\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint\n\narXiv:1503.02531, 2015. (cited on p. 1)\n\nHanzhe Hu, Fangyun Wei, Han Hu, Qiwei Ye, Jinshi Cui, and Liwei Wang. Semi-supervised semantic\nsegmentation via adaptive equalization learning. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang,\nand J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 22106–\n22118. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/\nfile/b98249b38337c5088bbc660d8f872d6a-Paper.pdf. (cited on pp. 3, 7, and 8)\n\nZhanghan Ke, Di Qiu, Kaican Li, Qiong Yan, and Rynson W. H. Lau. Guided collaborative training for pixel-wise\nsemi-supervised learning. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.),\nEuropean Conference on Computer Vision, pp. 429–445, Cham, 2020. Springer International Publishing.\nISBN 978-3-030-58601-0. URL https://www.ecva.net/papers/eccv_2020/papers_ECCV/\nhtml/1932_ECCV_2020_paper.php. (cited on p. 2)\n\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer\nWhitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything. arXiv\npreprint arXiv:2304.02643, 2023. (cited on p. 1)\n\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classification with deep convolutional neural\nnetworks. In F. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger (eds.), Advances in Neural Information\nProcessing Systems, volume 25. Curran Associates, Inc., 2012. URL https://papers.nips.cc/\npaper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html.\n(cited on p.\n1)\n\nSamuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In International Conference on\nLearning Representations, 2016. URL https://openreview.net/forum?id=BJ6oOfqge. (cited\non p. 2)\n\nDong-Hyun Lee.\n\nPseudo-label: The simple and efficient semi-supervised learning method for deep\nICML 2013 Workshop: Challenges in Representation Learning (WREPL), July\nneural networks.\n2013. URL http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_\nfinal.pdf. (cited on pp. 1 and 2)\n\nFeng Li, Hao Zhang, Huaizhe xu, Shilong Liu, Lei Zhang, Lionel M. Ni, and Heung-Yeung Shum. Mask DINO:\ntowards a unified transformer-based framework for object detection and segmentation. arXiv preprint, June\n2022. URL https://arxiv.org/abs/2206.02777. (cited on p. 1)\n\n11\n\n\fTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and\nC. Lawrence Zitnick. Microsoft COCO: common objects in context. In David Fleet, Tomas Pajdla, Bernt\nSchiele, and Tinne Tuytelaars (eds.), European Conference on Computer Vision, pp. 740–755, Cham, 2014.\nSpringer International Publishing. ISBN 978-3-319-10602-1. URL https://link.springer.com/\nchapter/10.1007/978-3-319-10602-1_48. (cited on pp. 2, 6, 8, and 9)\n\nShikun Liu, Shuaifeng Zhi, Edward Johns, and Andrew J. Davison. Bootstrapping semantic segmentation\nwith regional contrast. In International Conference on Learning Representations, 2022a. URL https:\n//openreview.net/forum?id=6u6N8WWwYSM. (cited on p. 7)\n\nYen-Cheng Liu, Chih-Yao Ma, Zijian He, Chia-Wen Kuo, Kan Chen, Peizhao Zhang, Bichen Wu, Zsolt Kira, and\nPeter Vajda. Unbiased teacher for semi-supervised object detection. In International Conference on Learning\nRepresentations, 2021. URL https://openreview.net/forum?id=MJIve1zgR_. (cited on p. 1)\n\nYuyuan Liu, Yu Tian, Yuanhong Chen, Fengbei Liu, Vasileios Belagiannis, and Gustavo Carneiro. Perturbed and\nstrict mean teachers for semi-supervised semantic segmentation. In IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pp. 4258–4267, June 2022b. URL https://openaccess.thecvf.\ncom/content/CVPR2022/html/Liu_Perturbed_and_Strict_Mean_Teachers_for_\nSemi-Supervised_Semantic_Segmentation_CVPR_2022_paper.html. (cited on pp. 3, 7,\nand 8)\n\nAndres Milioto, Philipp Lottes, and Cyrill Stachniss. Real-time semantic segmentation of crop and weed for\nprecision agriculture robots leveraging background knowledge in CNNs. In IEEE International Conference\non Robotics and Automation (ICRA), pp. 2229–2235, 2018. doi: 10.1109/ICRA.2018.8460962. URL\nhttps://arxiv.org/abs/1709.06764. (cited on p. 1)\n\nTakeru Miyato, Shin-Ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization\nmethod for supervised and semi-supervised learning. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 41(8):1979–1993, 2018. doi: 10.1109/TPAMI.2018.2858821. URL https://ieeexplore.\nieee.org/abstract/document/8417973. (cited on p. 2)\n\nAlexey Nekrasov, Jonas Schult, Or Litany, Bastian Leibe, and Francis Engelmann. Mix3d: Out-of-context data\n\naugmentation for 3d scenes. 3DV 2021, 2021. (cited on p. 3)\n\nYassine Ouali, Céline Hudelot, and Myriam Tami. Semi-supervised semantic segmentation with cross-\nIn IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\nconsistency training.\nJune 2020. URL https://openaccess.thecvf.com/content_CVPR_2020/html/Ouali_\nSemi-Supervised_Semantic_Segmentation_With_Cross-Consistency_Training_\nCVPR_2020_paper.html. (cited on p. 3)\n\nAntti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semi-supervised learning\nwith ladder networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in\nNeural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://papers.\nnips.cc/paper/2015/hash/378a063b8fdb1db941e34f4bde584c7d-Abstract.html.\n(cited on p. 1)\n\nMamshad Nayeem Rizve, Kevin Duarte, Yogesh S. Rawat, and Mubarak Shah. In defense of pseudo-labeling: An\nuncertainty-aware pseudo-label selection framework for semi-supervised learning. In International Conference\non Learning Representations, 2021. URL https://openreview.net/forum?id=-ODN6SbiUU.\n(cited on p. 1)\n\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\nKarpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet large scale visual\nrecognition challenge. International Journal of Computer Vision, 115(3):211–252, 2015. doi: 10.1007/\ns11263-015-0816-y. (cited on p. 6)\n\nTobias Scheffer, Christian Decomain, and Stefan Wrobel. Active hidden Markov models for information\nextraction. In Frank Hoffmann, David J. Hand, Niall Adams, Douglas Fisher, and Gabriela Guimaraes\n(eds.), Advances in Intelligent Data Analysis, pp. 309–318, Berlin, Heidelberg, 2001. Springer Berlin\nHeidelberg. ISBN 978-3-540-44816-7. URL https://link.springer.com/chapter/10.1007/\n3-540-44816-0_31. (cited on pp. 4 and 19)\n\nGyungin Shin, Weidi Xie, and Samuel Albanie. All you need are a few pixels: Semantic segmentation with\nPixelPick. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops,\npp. 1687–1697, October 2021. URL https://openaccess.thecvf.com/content/ICCV2021W/\nILDAV/html/Shin_All_You_Need_Are_a_Few_Pixels_Semantic_Segmentation_\nWith_ICCVW_2021_paper.html. (cited on pp. 4 and 19)\n\n12\n\n\fKihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A. Raffel, Ekin Do-\ngus Cubuk, Alexey Kurakin, and Chun-Liang Li. FixMatch: simplifying semi-supervised learning\nwith consistency and confidence.\nIn H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and\nH. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 596–608. Cur-\nran Associates, Inc., 2020a. URL https://proceedings.neurips.cc/paper/2020/hash/\n06964dce9addb1c5cb5d6e3d9838f733-Abstract.html.\n(cited on pp. 1, 2, 3, 4, 7, 18, 19,\nand 20)\n\nKihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang, Chen-Yu Lee, and Tomas Pfister. A simple semi-\nsupervised learning framework for object detection. arXiv preprint arXiv:2005.04757, 2020b. URL https:\n//arxiv.org/abs/2005.04757. (cited on p. 1)\n\nAntti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets\nimprove semi-supervised deep learning results. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30.\nCurran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/hash/\n68053af2923e00204c3ca7c6a3150cf7-Abstract.html. (cited on pp. 1, 2, 3, and 7)\n\nGodfried T. Toussaint. The use of context in pattern recognition. Pattern Recognition, 10(3):189–204, 1978. ISSN\n0031-3203. doi: https://doi.org/10.1016/0031-3203(78)90027-4. URL https://www.sciencedirect.\ncom/science/article/pii/0031320378900274. The Proceedings of the IEEE Computer Society\nConference. (cited on p. 3)\n\nAdam Van Etten, Dave Lindenbaum, and Todd M. Bacastow. SpaceNet: a remote sensing dataset and challenge\nseries. arXiv preprint, June 2018. URL https://arxiv.org/abs/1807.01232. (cited on p. 1)\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30.\nCurran Associates, Inc., 2017. URL https://openaccess.thecvf.com/content_CVPR_2020/\nhtml/Wang_ECA-Net_Efficient_Channel_Attention_for_Deep_Convolutional_\nNeural_Networks_CVPR_2020_paper.html. (cited on p. 3)\n\nHe Wang, Yezhen Cong, Or Litany, Yue Gao, and Leonidas J. Guibas. 3DIoUMatch: leveraging IoU prediction\nfor semi-supervised 3D object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pp. 14615–14624, June 2021. URL https://openaccess.thecvf.\ncom/content/CVPR2021/html/Wang_3DIoUMatch_Leveraging_IoU_Prediction_\nfor_Semi-Supervised_3D_Object_Detection_CVPR_2021_paper.html. (cited on p. 2)\n\nQilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wangmeng Zuo, and Qinghua Hu. ECA-Net: Efficient\nchannel attention for deep convolutional neural networks. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), June 2020. URL https://openaccess.thecvf.\ncom/content_CVPR_2020/html/Wang_ECA-Net_Efficient_Channel_Attention_\nfor_Deep_Convolutional_Neural_Networks_CVPR_2020_paper.html. (cited on p. 3)\n\nYidong Wang, Hao Chen, Qiang Heng, Wenxin Hou, Yue Fan, Zhen Wu, Jindong Wang, Marios Savvides,\nTakahiro Shinozaki, Bhiksha Raj, Bernt Schiele, and Xing Xie. FreeMatch: self-adaptive thresholding\nIn International Conference on Learning Representations, 2023. URL\nfor semi-supervised learning.\nhttps://openreview.net/forum?id=PDrUPTXJI_A. (cited on pp. 4 and 19)\n\nYuchao Wang, Haochen Wang, Yujun Shen, Jingjing Fei, Wei Li, Guoqiang Jin, Liwei Wu, Rui\nZhao, and Xinyi Le.\nSemi-supervised semantic segmentation using unreliable pseudo labels.\nIn IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR),\nURL https://openaccess.thecvf.com/content/CVPR2022/html/Wang_\n2022.\nSemi-Supervised_Semantic_Segmentation_Using_Unreliable_Pseudo-Labels_\nCVPR_2022_paper.html. (cited on pp. 1, 3, 4, 7, and 8)\n\nQizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc V. Le. Unsupervised data augmen-\nIn H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and\ntation for consistency training.\nH. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 6256–6268. Cur-\nran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n44feb0096faa8326192570788b38c1d1-Abstract.html. (cited on p. 2)\n\nHaiming Xu, Lingqiao Liu, Qiuchen Bian, and Zhen Yang. Semi-supervised semantic segmentation with\nprototype-based consistency regularization. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho,\nand A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 26007–26020. Curran\nAssociates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/\n2022/hash/a70ee7ea485e4fd36abbfc4adf591c28-Abstract-Conference.html. (cited\non pp. 2, 7, and 8)\n\n13\n\n\fFan Yang, Kai Wu, Shuyi Zhang, Guannan Jiang, Yong Liu, Feng Zheng, Wei Zhang, Chengjie Wang,\nIn IEEE/CVF Conference\nURL\n\nand Long Zeng.\non Computer Vision and Pattern Recognition (CVPR), pp. 14421–14430,\nhttps://openaccess.thecvf.com/content/CVPR2022/html/Yang_Class-Aware_\nContrastive_Semi-Supervised_Learning_CVPR_2022_paper.html. (cited on p. 1)\n\nClass-aware contrastive semi-supervised learning.\n\nJune 2022a.\n\nLihe Yang, Wei Zhuo, Lei Qi, Yinghuan Shi, and Yang Gao. ST++: make self-training work better\nIn IEEE/CVF Conference on Computer Vision and\nfor semi-supervised semantic segmentation.\nPattern Recognition (CVPR), pp. 4268–4277, June 2022b. URL https://openaccess.thecvf.\ncom/content/CVPR2022/html/Yang_ST_Make_Self-Training_Work_Better_for_\nSemi-Supervised_Semantic_Segmentation_CVPR_2022_paper.html.\nand 7)\n\n(cited on pp. 3\n\nLihe Yang, Lei Qi, Litong Feng, Wayne Zhang, and Yinghuan Shi. Revisiting weak-to-strong consistency in\nsemi-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pp. 7236–7246, June 2023a. URL https://openaccess.thecvf.\ncom/content/CVPR2023/html/Yang_Revisiting_Weak-to-Strong_Consistency_\nin_Semi-Supervised_Semantic_Segmentation_CVPR_2023_paper.html. (cited on pp. 7,\n8, 9, and 16)\n\nLihe Yang, Lei Qi, Litong Feng, Wayne Zhang, and Yinghuan Shi. Revisiting weak-to-strong consistency in\nsemi-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pp. 7236–7246, June 2023b. URL https://openaccess.thecvf.\ncom/content/CVPR2023/html/Yang_Revisiting_Weak-to-Strong_Consistency_\nin_Semi-Supervised_Semantic_Segmentation_CVPR_2023_paper.html. (cited on pp. 3,\n7, 18, 20, and 22)\n\nSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. CutMix:\nIn IEEE/CVF International\nregularization strategy to train strong classifiers with localizable features.\nConference on Computer Vision (ICCV), October 2019. URL https://openaccess.thecvf.\ncom/content_ICCV_2019/html/Yun_CutMix_Regularization_Strategy_to_Train_\nStrong_Classifiers_With_Localizable_Features_ICCV_2019_paper.html. (cited on\np. 18)\n\nSergey Zagoruyko, Adam Lerer, Tsung-Yi Lin, Pedro O. Pinheiro, Sam Gross, Soumith Chintala, and Piotr\nDollár. A MultiPath network for object detection. In Edwin R. Hancock Richard C. Wilson and William\nA. P. Smith (eds.), Proceedings of the British Machine Vision Conference (BMVC), pp. 15.1–15.12. BMVA\nPress, September 2016. ISBN 1-901725-59-6. doi: 10.5244/C.30.15. URL https://dx.doi.org/10.\n5244/C.30.15. (cited on p. 1)\n\nBowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu,\n\nJindong Wang, Manabu Okumura, and\nFlexmatch: Boosting semi-supervised learning with curriculum pseudo la-\nTakahiro Shinozaki.\nbeling.\nIn M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan\n(eds.), Advances in Neural Information Processing Systems, volume 34, pp. 18408–18419. Cur-\nran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/hash/\n995693c15f439e3d189b06e89d145dd5-Abstract.html. (cited on pp. 1, 2, and 19)\n\nNa Zhao, Tat-Seng Chua, and Gim Hee Lee. SESS: self-ensembling semi-supervised 3D object detection.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June\n2020. URL https://openaccess.thecvf.com/content_CVPR_2020/html/Zhao_SESS_\nSelf-Ensembling_Semi-Supervised_3D_Object_Detection_CVPR_2020_paper.\nhtml. (cited on p. 2)\n\nYuanyi Zhong, Bodi Yuan, Hong Wu, Zhiqiang Yuan, Jian Peng, and Yu-Xiong Wang. Pixel contrastive-\nconsistent semi-supervised semantic segmentation. In IEEE/CVF International Conference on Computer\nVision (ICCV), pp. 7273–7282, October 2021. URL https://openaccess.thecvf.com/content/\nICCV2021/html/Zhong_Pixel_Contrastive-Consistent_Semi-Supervised_\nSemantic_Segmentation_ICCV_2021_paper.html. (cited on pp. 2, 9, and 16)\n\nZhuofan Zong, Guanglu Song, and Yu Liu. DETRs with collaborative hybrid assignments training. arXiv\n\npreprint, November 2022. URL https://arxiv.org/abs/2211.12860. (cited on p. 1)\n\nBarret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin Dogus Cubuk, and Quoc Le.\nRethinking pre-training and self-training.\nIn H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan,\nand H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 3833–3845.\nCurran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/\n27e9661e033a73a6ad8cefcde965c54d-Paper.pdf. (cited on p. 6)\n\n14\n\n\fYuliang Zou, Zizhao Zhang, Han Zhang, Chun-Liang Li, Xiao Bian, Jia-Bin Huang, and Tomas Pfister.\nPseudoSeg: designing pseudo labels for semantic segmentation. In International Conference on Learning\nRepresentations, 2021. URL https://openreview.net/forum?id=-TwO99rbVRu. (cited on pp.\n3, 9, and 16)\n\n15\n\n\fTable A.1: Comparison between our method and prior art on the PASCAL VOC 2012 val on\ndifferent partition protocols using ResNet-50 as backbone model. The caption describes the share of\nthe training set used as labeled data and, in parentheses, the actual number of labeled images.\n\nMethod\n\n1/16 (92)\n\n1/8 (183)\n\n1/4 (366)\n\n1/2 (732) Full (1464)\n\nSupervised Baseline\nPseudoSeg (Zou et al., 2021)\nPC2Seg (Zhong et al., 2021)\nUniMatch (Yang et al., 2023a)\nUniMatchψ + S4MC\n\n44.0\n54.89\n56.9\n71.9\n\n72.62\n\n52.3\n61.88\n64.6\n72.5\n\n72.83\n\n61.7\n64.85\n67.6\n76.0\n\n76.44\n\n66.7\n70.42\n70.9\n77.4\n\n77.83\n\n72.9\n71.00\n72.3\n78.7\n\n79.41\n\nTable A.2: Ablation study on the different neighbor selection regimes on Pascal VOC 2012 with 366\nlabeled examples.\n\n1-NN features\n\n1-NN probability\n\nHighest Probability\n\n78.04\n\n77.48\n\n79.09 ± 0.183\n\nTable A.3: Evaluation of Boundary IoU (Cheng et al., 2021) comparing models trained with\nFixMatch+S4MC and with FixMatch using 183 annotated images on PASCAL VOC 12. The\nmodel is based on Xception-65 as in Table 4.\n\nFixMatch\n\nFixMatch+S4MC\n\n31.1\n\n29.9\n\nA A D D I T I O N A L E X P E R I M E N T S\n\nIn this section, we present additional experimental results. Table A.1 presents results using ResNet-50\nbackbone on PASCAL. Table A.3 compares boundary IoU (Cheng et al., 2021) to baseline.\n\nIn Table A.2, we ablate alternative neighbor selection strategy. The highest probability is the method\npresented in our paper. Alternatively, we consider nearest neighbor strategies when we choose the\nclosest neighbor according to some metric, either from the probability vector (1-NN probability) or\nthe latent features obtained from the model (1-NN features). The probability distance yields the worst\nresults. The method in the paper yields the best result.\n\nB M O R E V I S U A L R E S U LT S\n\nWe present in Figs. B.1 and B.2 an extension of Fig. 3, showing more instances from the unlabeled\ndata and the corresponding pseudo-labeled with the baseline model and S4MC.\n\nOur method can achieve more accurate predictions during the inference phase without refinements.\nThis results in more seamless and continuous predictions, which accurately depict objects’ spatial\nconfiguration.\n\nC C O M P U TAT I O N A L C O S T\n\nLet us denote the image size by H × W and the number of classes by C.\n\nFirst, the predicted map of dimension H × W × C is stacked with the padded-shifted versions,\ncreating a tensor of shape [n,H,W,C]. K top neighbors are picked via top-k operation and calculate\nthe union event as presented in Eq. (9). (The pseudo label refinement pytorch-like pseudo-code can\nbe obtained in Algorithm 1 for N = 4 and k max neighbors.)\n\nThe overall space (memory) complexity of the calculation is O(n × H × W × C), which is negligible\nconsidering all parameters and gradients of the model. Time complexity adds three tensor operations\n\n16\n\n\fFigure B.1: Example of refined pseudo-labels, the structure is as in Fig. 3, and the numbers under\nthe predictions show the pixel-wise accuracy of the prediction map.\n\n(stack, topk, and multiplication) over the H × W × C tensor, where the multiplication operates k\ntimes, which means O(k × H × W × C). This is again negligible for any reasonable number of\nclasses compared to tens of convolutional layers with hundreds of channels.\n\nTo verify that, we conducted a training time analysis comparing FixMatch and FixMatch + S4MC\nover PASCAL with 366 labeled examples, using distributed training with 8 Nvidia RTX 3090 GPUs.\nFixMatch average epoch takes 28:15 minutes, and FixMatch + S4MC average epoch takes 28:18\nminutes, an increase of about 0.2% in runtime.\n\n17\n\n\fAlgorithm 1: Pseudocode: Pseudo label refinement of S4MC, PyTorch-like style.\n\n# X: predict prob of unlabeled data B x C x H x W\n# k: number of neigbors\n\n#create neighborhood tensor\nneigborhood=[]\nX = X.unsqueeze(1)\nX = torch.nn.functional.pad(X, (1, 1, 1, 1, 0, 0, 0, 0))\nfor i,j in [(None,-2),(1,-1),(2,None)]:\n\nfor k,l in [(None,-2),(1,-1),(2,None)]:\n\nif i==k and i==1:\n\ncontinue\n\nneighborhood.append(X[:,:,i:j, k:l])\n\nneighborhood = torch.stack(neighborhood)\n\n#pick k neighbors for union event\nktop_neighbors,neigbor_idx=torch.topk(neighborhood, k=k,axis=0)\nfor nbr in ktop_neighbors:\n\nbeta = torch.exp((-1/2) * neigbor_idx)\nX = X + beta*nbr - (X*nbr*beta)\n\nD I M P L E M E N TAT I O N D E TA I L S\n\nAll experiments were conducted for 80 training epochs with the stochastic gradient descent (SGD)\noptimizer with a momentum of 0.9 and learning rate policy of lr = lrbase · (cid:0)1 − iter\nFor the teacher averaging consistency, we apply resize, crop, horizontal flip, GaussianBlur, and with\na probability of 0.5, we use Cutmix (Yun et al., 2019) on the unlabeled data.\n\n(cid:1)power\n\ntotal iter\n\n.\n\nFor the augmentation variation consistency (Sohn et al., 2020a; Yang et al., 2023b), we apply resize,\ncrop, and horizontal flip for weak and strong augmentations as well as ColorJitter, RandomGrayscale,\nand Cutmix for strong augmentations.\n\nFor PASCAL VOC 2012 lrbase = 0.001 and the decoder only lrbase = 0.01, the weight decay is set\nto 0.0001 and all images are cropped to 513 × 513 and Bl = Bu = 3.\n\nFor Cityscapes, all parameters use lrbase = 0.01, and the weight decay is set to 0.0005. The learning\nrate decay parameter is set to power = 0.9. Due to memory constraints, all images are cropped\nto 769 × 769 and Bℓ = Bu = 2. All experiments are conducted on a machine with 8 Nvidia RTX\nA5000 GPUs.\n\nE L I M I TAT I O N S A N D P O T E N T I A L N E G AT I V E S O C I A L I M PA C T S\n\nLimitations. The constraint imposed by the spatial coherence assumption also restricts the\napplicability of this work to dense prediction tasks. Improving pseudo-labels’ quality for overarching\ntasks such as classification might necessitate reliance on data distribution and the exploitation of\ninter-sample relationships. We are currently exploring this avenue of research.\n\nSocietal impact. Similar to most semi-supervised models, we utilize a small subset of annotated\ndata, which can potentially introduce biases from the data into the model. Further, our PLR module\nassumes spatial coherence. While that holds for natural images, it may yield adverse effects in other\ndomains, such as medical imaging. It is important to consider these potential impacts before choosing\nto use our proposed method.\n\nF\n\nP S E U D O - L A B E L S Q U A L I T Y A N A LY S I S\n\nThe quality improvement and the quantity increase of pseudo-labels are shown in Fig. 4. Further\nanalysis of the quality improvement of our method is demonstrated in Fig. F.1 by separating the true\npositive and false positive.\n\n18\n\n\fTable G.1: Ablation study on the confidence function κ, over Pascal VOC 12 with partition protocols\n\nFunction\n\n1/4 (366)\n\n1/2 (732)\n\nFull (1464)\n\nκmax\nκent\nκmargin\n\n74.29\n75.18\n75.41\n\n76.16\n77.55\n77.73\n\n79.49\n79.89\n80.58\n\nWithin the initial phase of the learning process, the enhancement in the quality of pseudo-labels can\nbe primarily attributed to the advancement in true positive labels. In our method, the refinement not\nonly facilitates the inclusion of a larger number of pixels surpassing the threshold but also ensures\nthat a significant majority of these pixels are high quality.\n\nAs the learning process progresses, most improvements are obtained from a decrease in false positives\npseudo-labels. This analysis shows that our method effectively minimizes the occurrence of incorrect\npseudo-labeled, particularly when the threshold is set to a lower value. In other words, our approach\nreduces confirmation bias from decaying the threshold as the learning process progresses.\n\nG W E A K – S T R O N G C O N S I S T E N C Y\n\nWe need to redefine the supervision branch to adjust the method to augmentation level consistency\nframework (Sohn et al., 2020a; Zhang et al., 2021; Wang et al., 2023). Recall that within the teacher\naveraging framework, we denote fθs (xi) and fθt (xi) as the predictions made by the student and\nteacher models for input xi, where the teacher serves as the source for generating confidence-based\npseudo-labels. In the context of image-level consistency, both branches differ by augmented versions\ni , xs\nxw\ni represent the weak and strong augmented\nrenditions of the input xi, respectively. Following the framework above, the branch associated with\nweak augmentation generates the pseudo-labels.\n\ni and share identical weights fθ. Here, xw\n\ni and xs\n\nG . 1 C O N F I D E N C E F U N C T I O N A LT E R N AT I V E S\n\nIn this paper, we introduce a confidence function to determine pseudo-label propagation. We\nintroduced κmargin(xi,j) and mentioned other alternatives have been examined.\n\nHere, we define several options for the confidence function.\n\nThe simplest option is to look at the probability of the dominant class,\n\nκmax(xi\n\nj,k) = max\n\nc\n\npc(xi\n\nj,k),\n\nwhich is commonly used to generate pseudo-labels.\n\nThe second alternative is negative entropy, defined as\n\nκent(xi\n\nj,k) =\n\n(cid:88)\n\nc∈C\n\npc(xi\n\nj,k) log(cid:0)pc\n\ni,j\n\n(G.1)\n\n(G.2)\n\n(cid:1).\n\nNote that this is indeed a confidence function since high entropy corresponds to high uncertainty, and\nlow entropy corresponds to high confidence.\n\nThe third option is for us to define the margin function (Scheffer et al., 2001; Shin et al., 2021) as the\ndifference between the first and second maximal values of the probability vector and also described\nin the main paper:\n\nκmargin(xi,j) = max\n\n(pc(xi\n\nj,k)) − max2c(pc(xi\n\nj,k)),\n\nc\n\n(G.3)\n\nwhere max2 denotes the vector’s second maximum value. All alternatives are compared in Table G.1.\n\nTable G.1 studies the impact of different confidence functions on pseudo-label refinement. We found\nthat using a margin to describe confidence is a suitable way when there is a contradiction in smooth\nregions.\n\n19\n\n\fG . 2 D E C O M P O S I T I O N A N D A N A LY S I S O F U N I M AT C H\n\nUnimatch (Yang et al., 2023b) investigating the consistency and suggest using FixMatch (Sohn et al.,\n2020a) and a strong baseline for semi-supervised semantic segmentation. Moreover, they provide\nanalysis that shows that combining three students for each supervision signal, one feature level\naugmentation, feature perturbation, denoted by FP, and two strong augmentations, denoted by S1 and\nS2. Fusing Unimatch and our method did not provide significant improvements, and we examined the\ncontribution of different components of Unimatch. We measured the pixel agreement as described in\nEq. (9) and showed that the feature perturbation branch has the same effect on pixel agreement as\nS4MC. Appendix G.2 present the distribution of agreement using FixMatch (S1), DusPerb (S1,S2),\nUnimatch (S1, S2, FP) and S4MC (S1, S2).\n\nH B O U N D I N G T H E J O I N T P R O B A B I L I T Y\n\nIn this paper, we had the union event estimation with the independence assumption, defined as\nℓ,m) ≈ pc(xi\n\nj,k) · pc(xi\n\nj,k, xi\n\nc(xi\np1\n\nℓ,m)\n\n(H.1)\n\nIn addition to the independence approximation, it is possible to estimate the unconditional expectation\nof two neighboring pixels belonging to the same class based on labeled data:\n\nc(xi\np2\n\nj,k, xi\n\nℓ,m) =\n\n1\n|Nl| · H · W · |N|\n\n(cid:88)\n\n(cid:88)\n\n(cid:88)\n\ni∈Nl\n\nj,k∈H×W\n\nℓ,m∈Nj,k\n\n1{yi\n\nj,k = yi\n\nℓ,m}.\n\nTo avoid overestimating that could lead to overconfidence, we set\nc(xi\n\nℓ,m) = max(p1\n\nℓ,m), p2\n\nj,k, xi\n\nj,k, xi\n\npc(xi\n\nc(xi\n\nj,k, xi\n\nℓ,m))\n\n(H.2)\n\n(H.3)\n\nThat upper bound of joint probability ensures that the independence assumption does not\nunderestimate the joint probability, preventing overestimating the union event probability. Using\nEq. (H.3) increase the mIoU by 0.22 on average, compared to non use of S4MC refinement, using\n366 annotated images from PASCAL VOC 12 Using only Eq. (H.2) reduced the mIoU by -14.11\ncompared to the non-use of S4MC refinement and harmed the model capabilities to produce quality\npseudo-labels.\n\n20\n\n\fFigure B.2: Visual examples of our method on COCO. Model trained with S4MC on COCO with\n1/32 of the labeled data\n\n21\n\n\fFigure F.1: Quality of pseudo-labels, on PASCAL VOC 2012 (Everingham et al., 2010) over training\niterations. Fig. 4 separated to True positive and False positive analysis. True positive are the bigger\npart of improvement at the early stage of the training process, while reduction of false positive is the\nmain contribution late in the training process\n\n(a) The spatial agreement as we define in in 9 compared\nbetween different variations of Unimatch and S4MC,\non PASCAL VOC 12 dataset.\n\n(b) The spatial agreement, compared between different\nvariations of Unimatch (Yang et al., 2023b) and S4MC,\non PASCAL over time.\n\n22\n\n020406080Epoch0.000.020.040.060.080.10% Pseudo-labelsTrue positive enlargementFalse positive reduction\u0000)\u0000L\u0000[\u0000P\u0000D\u0000W\u0000F\u0000K\u0000'\u0000X\u0000V\u00003\u0000H\u0000U\u0000E\u00008\u0000Q\u0000L\u0000P\u0000D\u0000W\u0000F\u0000K\u00006\u0000\u0017\u00000\u0000&\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0015\u0000\u0013\u0000\u0011\u0000\u0017\u0000\u0013\u0000\u0011\u0000\u0019\u0000\u0013\u0000\u0011\u0000\u001b\u0000\u0014\u0000\u0011\u0000\u0013\u00003\u0000L\u0000[\u0000H\u0000O\u0000\u0003\u0000D\u0000U\u0000U\u0000D\u0000Q\u0000J\u0000H\u0000P\u0000H\u0000Q\u0000W\u0000\u0013\u0000\u0014\u0000\u0013\u0000\u0015\u0000\u0013\u0000\u0016\u0000\u0013\u0000\u0017\u0000\u0013\u0000\u0018\u0000\u0013\u0000\u0019\u0000\u0013\u0000\u001a\u0000\u0013\u0000\u001b\u0000\u0013\u0000(\u0000S\u0000R\u0000F\u0000K\u0000\u0013\u0000\u0011\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0015\u0000\u001a\u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0013\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0015\u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0016\u0000\u0018\u0000\u0013\u0000\u0013\u0000\u0011\u0000\u0016\u0000\u001a\u0000\u0018\u0000$\u0000J\u0000U\u0000H\u0000H\u0000P\u0000H\u0000Q\u0000W\u0000)\u0000L\u0000[\u0000P\u0000D\u0000W\u0000F\u0000K\u00008\u0000Q\u0000L\u0000P\u0000D\u0000W\u0000F\u0000K\u00006\u0000\u0017\u00000\u0000",
    "abstract": "We present a novel confidence refinement scheme that enhances pseudo-labels in semi-supervised semantic segmentation. Unlike current leading methods, which filter pixels with low-confidence predictions in isolation, our approach leverages the spatial correlation of labels in segmentation maps by grouping neighboring pixels and considering their pseudo-labels collectively. With this contextual information, our method, named S4MC, increases the amount of unlabeled data used during training while maintaining the quality of the pseudo-labels, all with negligible computational overhead. Through extensive experiments on standard benchmarks, we demonstrate that S4MC outperforms existing state-of-the-art semi-supervised learning approaches, offering a promising solution for reducing the cost of acquiring dense annotations. For example, S4MC achieves a  1.29 mIoU improvement over the prior state-of-the-art method on PASCAL VOC 12 with 366 annotated images. The code to reproduce our experiments is available at https://s4mcontext.github.io/",
    "response": {
      "Soundness": 3,
      "Presentation": 3,
      "Contribution": 2,
      "Rating": 5,
      "Confidence": 3,
      "Strengths": "Originality: The approach offers some degree of novelty - filtering low-confidence predictions by using the context around the pixel, rather than the pixel in isolation (current sota approaches). The contribution is relevant to an actual problem, it increases the use of unlabeled data.\nQuality: The method is sound and thoroughly explained. Experiments prove the effectiveness of the approach when applied on top of state-of-the-art methods with a negligible added computational cost. \nClarity: The paper is an interesting read, well-structured, well-detailed, and very easy to understand (fairly enjoyed reading it). \nSignificance: The results offer marginal improvements only in some scenarios, compared to state-of-the-art methods.",
      "Weaknesses": "* I would suggest changing the main figure of the paper (the elements within the figure are way too small and hard to follow).\n* The contribution is not groundbreaking. The +1.29 mIoU gain on PASCAL VOC 2012 and +1.01 mIoU improvement on Cityscapes declared at the beginning of the paper are not backed up by the numbers in the table (Table 1 and Table 3). The numbers in the tables show that the method is not robust enough to offer a consistent improvement in all tested scenarios.  \n* The biggest weakness of the paper is Section 4.3 (the ablation studies) and Tables 4 & 5. The text states that the experiments were conducted using the CutMix-Seg framework, but I could not find the numbers in the previous tables. Also in Table 5, the caption states that the numbers are for FixMatch. The text, the numbers, and the tables do not correspond, this part needs further clarification (or another check) because it confuses me the most. \n* Low range of datasets, more experiments that include more varied and challenging scenarios to better understand the method's limitations.",
      "Questions": "* There are no insights as to why the best window for the used contextual information is in a 3x3 range - this actually suggests that the context is not used properly, or what is actually causing this degradation in performance when more neighboring pixels are used?"
    }
  },
  {
    "paper_id": "28L2FCtMWq",
    "summary": "Background: Recent advancements in video editing have achieved success in single-attribute editing and style transfer using text-to-video (T2V) models. These models either require training on text-video datasets or use training-free methods. However, complex multi-attribute editing remains challenging, often resulting in issues like omitted edits, incorrect modifications, and poor consistency.\nGap: Current video editing techniques struggle with multi-attribute editing, often failing to make all intended changes, applying edits to the wrong elements, or altering regions that should remain unchanged. This indicates a lack of precision and consistency in handling multiple attributes simultaneously.\nHypothesis: The authors propose that integrating grounding information through a novel framework can achieve temporally consistent multi-attribute video editing in a training-free manner, thus overcoming the shortcomings of existing methods.\nMethod: The proposed method, Ground-A-Video, employs cross-frame gated attention combined with modulated cross-attention and optical flow guided inverted latents smoothing. This approach incorporates grounding information—bounding box coordinates and textual descriptions—into the latent representations to ensure precise and temporally consistent edits across video frames without additional training. The framework utilizes Stable Diffusion and ControlNet for structure preservation and employs DDIM inversion for video frames. Groundings are used to guide spatially-continuous and discrete editing, with experiments demonstrating superior performance in edit-accuracy and frame consistency compared to baseline methods.\nConclusion: Ground-A-Video effectively addresses the limitations of current video editing techniques, achieving superior accuracy and temporal consistency in multi-attribute video editing scenarios through a zero-shot approach. The method demonstrates improved performance over existing solutions by incorporating grounding information to guide edits in a structured and consistent manner.\n",
    "full text": "\nGROUND-A-VIDEO: ZERO-SHOT GROUNDED VIDEO\nEDITING USING TEXT-TO-IMAGE DIFFUSION MODELS\n\nHyeonho Jeong & Jong Chul Ye\nKim Jaechul Graduate School of AI, KAIST\n{hyeonho.jeong,jong.ye}@kaist.ac.kr\n\nFigure 1: Ground-A-Video achieves multi-attribute editing, video style transfer with attribute change, and text-\nto-video generation with pose guidance, all in a time-consistent and training-free fashion. The boxes in the\nright-bottom images visualize the series of pose guidance.\n\nABSTRACT\n\nRecent endeavors in video editing have showcased promising results in single-\nattribute editing or style transfer tasks, either by training text-to-video (T2V)\nmodels on text-video data or adopting training-free methods. However, when\nconfronted with the complexities of multi-attribute editing scenarios, they ex-\nhibit shortcomings such as omitting or overlooking intended attribute changes,\nmodifying the wrong elements of the input video, and failing to preserve regions\nof the input video that should remain intact. To address this, here we present a\nnovel grounding-guided video-to-video translation framework called Ground-A-\nVideo for multi-attribute video editing. Ground-A-Video attains temporally con-\nsistent multi-attribute editing of input videos in a training-free manner without\naforementioned shortcomings. Central to our method is the introduction of Cross-\nFrame Gated Attention which incorporates groundings information into the latent\nrepresentations in a temporally consistent fashion, along with Modulated Cross-\nAttention and optical flow guided inverted latents smoothing. Extensive exper-\niments and applications demonstrate that Ground-A-Video’s zero-shot capacity\noutperforms other baseline methods in terms of edit-accuracy and frame consis-\ntency. Further results and code are available at http://ground-a-video.github.io.\n\n1\n\nINTRODUCTION\n\nCoupled with massive text-image datasets (Schuhmann et al., 2022), diffusion models (Sohl-\nDickstein et al., 2015; Ho et al., 2020; Song et al., 2020b) have revolutionized text-to-image (T2I)\n\n1\n\n\fPublished as a conference paper at ICLR 2024\n\ngeneration, making it increasingly accessible to generate high-quality images from text descriptions.\nAdditionally, the domain has seen profound expansion into several subfields, including controlled\ngeneration and real-world image editing. On the other hand, the endeavor to extend the success\nto the video domain poses a significant computational hurdle. Attaining time-consistent and high-\nquality results necessitates training on expensive video datasets—an endeavor beyond the means of\nmost researchers, particularly given the absence of publicly available, generic text-to-video models.\n\nAs such, pioneering approaches exhibit promise in text-to-video generation (Ho et al., 2022b;a)\nand video editing (Esser et al., 2023) by repurposing T2I diffusion model weights for extensive\nvideo data training. Specifically, in pursuit of cost-effective video generation, Wu et al. (2022)\nsuggests fine-tuning the T2I model on a single video, which enables generating variations of the\nvideo. Similar to the practice of manipulating attention maps within the realm of image editing\n(Hertz et al., 2022; Tumanyan et al., 2023; Parmar et al., 2023), various methods guide the denoising\nprocess by self-attention maps (Ceylan et al., 2023), cross-attention maps (Liu et al., 2023; Wang\net al., 2023), or both (Qi et al., 2023), which are obtained during the input video inversion stage.\nRecently, to incorporate the denoising process with additional structural cues, ControlNet (Zhang &\nAgrawala, 2023) has been transferred to video domain, achieving structure-consistent output frames\nin video generation (Khachatryan et al., 2023a) and translation (Hu & Xu, 2023; Chu et al., 2023;\nChen et al., 2023; Zhang et al., 2023).\n\n(cid:1)\n\n(cid:1)\n\n(cid:1)\n\nτb′, τc\n\nτa′, τb\n\nτc, . . . , ∅\n\nNonetheless, in the scenario of fine-grained video editing involving multiple attribute changes, i.e.\n∆τ ={τa\nτnew}, where τ represents a specific attributed indexed by\nsubscript, they encounter issues of degraded frame consistency, severe semantic misalignment (Park\net al., 2023), or both, as depicted in Fig. 2-Left and Sec. E. In specific, instances of neglecting in-\nτb′), mixing two separate edits\ntended attribute edits (τa\n(τa\nτc′) are ob-\nserved. This is because in the existing models, the Cross-Attention layer is the single domain where\nthe complex semantic changes wield their influence, where the list of intricate changes being entan-\ngled as a form of “one-sentence target prompt” makes the problem worse.\n\nτa′·τb′), and struggling to preserve regions that should remain unchanged (τc\n\nτa), modifying the wrong elements (τa\n\n(cid:1)\n\n(cid:1)\n\n(cid:1)\n\n(cid:1)\n\n(cid:1)\n\nThe key to address this issue lies in spatially-disentangled layout information, comprising bounding\nbox coordinates and textual captions, namely ‘groundings’. Groundings disentangle the complex\nsemantic combination by localizing each semantic element with precise location. Recently, ground-\ning has been successfully employed to text-to-image generation tasks. Li et al. (2023b) and Yang\net al. (2023) finetune existing T2I models to adhere to grounding conditions using box-image paired\ndatasets, while Xie et al. (2023) achieves training-free box-constrained image generation by inject-\ning binary spatial masks into the cross-attention space. However, unlike the literature on single-\nimage synthesis, guiding video generation process with groundings alone could significantly detri-\nment frame consistency. One major problem is that localizing the bounding box is insufficient for\nthe smooth frame transition (Fig. 2-right), which calls the need for additional structural guidance.\nConsequently, we propose to integrate two distinct modalities: 1) spatially-continuous conditions,\nincluding depth and optical flow maps, are employed to maintain consistent structure across frames;\n2) spatially-discrete conditions, specifically ‘groundings’, enable precise localization and editing\nof each attribute within the source video. The principal contributions of Ground-A-Video are\nsummarized as follows:\n\n• To our knowledge, we present the first groundings-driven video editing framework, also\nmarking the first instance of integrating both spatially-continuous and discrete conditions.\n• We propose a novel Modulated Cross-Attention mechanism which efficiently enables in-\n\nteractions between differently optimized unconditional embeddings.\n\n• To further enhance consistency, we suggest smoothing inverted latent representations using\n\noptical flow information, which can be employed following any type of inversion.\n\n• Extensive experiments and applications demonstrate the effectiveness of our method in\n\nachieving time-consistent and precise multi-attribute editing of input videos.\n\n2 BACKGROUND\n\nStable Diffusion. Distinct from traditional diffusion models (Ho et al., 2020), Stable Diffusion\n(SD) functions within a low-dimensional latent space, which is accessed via VAE autoencoder E, D\n(Kingma & Welling, 2013). More precisely, once the latent representation z0 is obtained by com-\n\n2\n\n\fPublished as a conference paper at ICLR 2024\n\nFigure 2: Left: Failure cases of multi-attribute video editing by various methods, driven by the target text “A\nsquirrel is eating an orange on the grass, under the aurora.” Ground-A-Video’s successful result for the same\ntask is shown in the second row of Fig. 1. Right: Input video reconstruction with and without ControlNet.\n\npressing an input image f ∈ RH×W ×3 through the encoder E, i.e. z0 = E(f ), diffusion forward\nprocess gradually adds Gaussian noise to z0 to obtain zt through Markov transition with the transi-\ntion probability:\n\nq(zt|zt−1) = N (zt; (cid:112)1 − βtzt−1, βtI) ,\n\nt = 1, . . . , T ,\n\n(1)\n\nwhere the noise schedule {βt}T\ntimesteps. Then, the backward denoising process is given by the transition probability:\n\nt=1 is an increasing sequence of t and T is the number of diffusion\n\npθ(zt−1|zt) = N (zt−1; µθ(zt, t), σ2\n\nt I) ,\n\nt = T, . . . , 1 .\n\n(2)\n\nHere, the mean µθ(zt, t) can be represented using the noise predictor ϵθ which is learned by the\nminimization of the MSE loss with respect to θ: Ef,τ,ϵ∼N (0,I),t ∥ϵ − ϵθ (zt, t, τ )∥2\n2, where ϵ refers\nto the zero mean unit variance Gaussian noise vector, and τ = ψ(T ) is the embedding of a text T .\n\nSpecifically, a prevalent approach in diffusion-based image editing is to use the deterministic DDIM\nscheme (Song et al., 2020a) to accelerate the sampling process. Within this scheme, the noisy latent\nzT can be transformed into a fully denoised latent z0:\n\nzt−1 =\n\n(cid:115)\n\nαt−1\nαt\n\n\n\n(cid:115)\n\nzt +\n\n\n\n1 − αt−1\nαt−1\n\n−\n\n(cid:115)\n\n1 − αt\nαt\n\n\n\n ϵθ ,\n\nt = T, . . . , 1 ,\n\n(3)\n\nwhere αt is a reparameterized noise scheduler.\n\nNull-text Optimization. To augment the effect of text conditioning, Ho & Salimans (2022) have\npresented the classifier-free guidance technique (cfg), where the noise prediction by ϵθ is also carried\nout unconditionally, namely by ‘null text’. Then the null-conditioned prediction is extrapolated with\nthe text-conditioned prediction to produce the classifier-free guidance prediction:\n\n˜ϵθ(zt, t, τ, ∅) = w · ϵθ(zt, t, τ )\n(cid:125)\n\n(cid:123)(cid:122)\n\n(cid:124)\n\nconditional prediction\n\n+(1 − w) · ϵθ(zt, t, ∅)\n(cid:125)\n(cid:124)\n\n(cid:123)(cid:122)\n\nunconditional prediction\n\n,\n\n(4)\n\nwhere ∅ = ψ(“ ”) denotes the embedding of a null text and w denotes the guidance scale. However,\nwhen coupled with cfg featuring large guidance scale w ≫ 1, DDIM inversion accumulates errors\nduring the denoising process, resulting in flawed image reconstruction. In order to fix these errors,\nMokady et al. (2023) optimizes the embeddings of a null text. First, DDIM inversion trajectory\n{z∗\nt=0 are computed. Then, at each timestep\n(cid:13)\n2\nstarting from T , unconditional embeddings {∅t}T\n2.\n(cid:13)\n\nt=0 and the predicated backward trajectory {¯zt}T\n\nt=1 are tuned towards minimizing (cid:13)\n\n(cid:13)¯zt−1 − z∗\n\nt }T\n\nt−1\n\n3 METHOD\n\n3.1 GROUND-A-VIDEO: OVERVIEW\n\nGiven a series of source video frames f 1:N and a list of intended edits ∆τ , our goal is to accurately\nedit various attributes of the source video while avoiding unwanted modifications, in a zero-shot\nyet time-consistent manner. Fig. 3 illustrates the overall architecture of Gound-A-Video to achieve\n\n3\n\n\fPublished as a conference paper at ICLR 2024\n\nthis. Initially, we automatically acquire grounding information through GLIP (Li et al., 2022). The\ngroundings and the source prompt are manually refined to form target groundings and target prompt,\ncommonly following ∆τ . On the other branch, the input frames undergo individual DDIM inversion\nand null optimization, followed by optical flow-based smoothing to form smoothed latent features.\nThe smoothed latents are separately fed into the inflated SD and ControlNet, which are modified\nwith a sequence of attentions to achieve the temporal consistency of the video editing. The inflated\nControlNet takes an additional input of depth maps. Subsequently, the target groundings are directed\nto the Cross-Frame Gated Attentions of the SD model, while the target prompt and optimized null-\nembeddings are channeled into the Modulated Cross Attentions of both SD and ControlNet models.\n\nIn Sec. 3.2, we discuss our inflated SD with per-frame inversion approach and proposed attention\nmechanisms (Modulated Cross-Attention & Cross-Frame Gated Attention). In Sec. 3.2, we intro-\nduce the inflated ControlNet to incorporate a spatially-continuous prior of input video: a sequence\nof depth maps. Lastly in Sec. 3.4, we present optical flow guided smoothing which is applied to the\ninverted latent features before they are fed into the SD and ControlNet models, to further improve\nthe consistency across frames.\n\nFigure 3: Left: Input preparation. We automatically obtain video groundings of input video frames f 1:N\nvia GLIP. This is followed by a handcraft editing phase for both the groundings and the source prompt. The\ninput frames undergo individual inversion and null optimization, followed by optical flow-based smoothing.\nFurthermore, ControlNet input d1:N are obtained via ZoeDepth estimator. Right: Denoising process. The\nsmoothed latents z1:N are fed into the inflated SD and ControlNet. The target grounding tokens U 1:N are\ndirected to Cross-Frame Gated Attention, while context vectors {τ 1:N , ∅1:N } are directed to Modulated Cross\nAttention. The series of attentions in inflated SD’s transformer blocks includes Spatial-Temporal Attention,\nCross-Frame Gated Attention, and Modulated Cross Attention, whereas Cross-Frame Gated Attention layers\nare not appended in inflated ControlNet. If a binary mask, the intersection of common outer spaces of target\nbounding boxes, exists, it is utilized for inpainting before each denoising step. This process helps preserve\nregions that are not the target of editing (see Sec. 4.4). For brevity, we omitted timestep t in all variables.\n\n3.2\n\nINFLATED STABLE DIFFUSION BACKBONE\n\nAttention Inflation with Spatial-Temporal Self-Attention. To exploit pretrained SD which is\ntrained without temporal considerations, recent video editing methods (Wu et al., 2022; Qi et al.,\n2023; Chen et al., 2023) commonly inflate Spatial Self-Attention along the temporal frame axis.\nIn a similar vein, Ground-A-Video enables inter-frame interaction by refactoring the Spatial Self-\nAttention into Spatial-Temporal Self-Attention while retaining the pretrained weights. In specific,\nfor the latent representation zi\nt of source frame f i, query features are derived from spatial fea-\ntures of zi\nt, while key and value features are computed from spatial features of concatenated latents\nt , . . . , zN\n[z1\nt ]. Specifically, this process can be written by the following mathematical representation:\nt ],\n\nV = W V · [z1\n\nQ = W Q · zi\n\nt , . . . , zN\n\nt , . . . , zN\n\nt ],\n\nt, K = W K · [z1\nwhere W Q, W K, W V are projection matrices.\n\nPer-frame Inversion with Modulated Cross-Attention. Although model inflation, such as the\nSpatial-Temporal Attention discussed above or the Sparse-Causal Attention in (Wu et al., 2022),\ncan contribute to preserving global semantic consistency across frames, prior research on the model\ninflation (Liu et al., 2023) has shown that it adversely degrades the generation quality of the original\nSD model, leading to imprecise reconstruction. This occurs because Spatial Self-Attention parame-\nters are employed to compute frame correlations, which have not been taught during the pre-training.\n\n4\n\n\fPublished as a conference paper at ICLR 2024\n\nHence, the inflated SD model falls short for the approximate inversion. To mitigate this issue, they\nfine-tune the attention projection matrices on the input video.\n\nT\n\nt=1. The latents z1:N\n\nand optimized null-embedding trajectories {∅1:N }T\n\nIn contrast, our pipeline adopts a per-frame approach to video inversion using the original, non-\ninflated SD model without any training. In this process, for each of the N frames within the video\nf 1:N , we carry out DDIM inversion and null-text inversion (Sec. 2) individually to obtain inverted\nlatents z1:N\nare then sub-\njected to Optical flow guided latents smoothing (Sec. 3.4), before proceeding to the denoising pro-\ncess. Additionally, to guide the separately optimized embeddings {∅1:N }T\nt=1 to restore the temporal\ncorrelation, we reengineer the Cross-Attention mechanism within the transformer blocks of SD that\ncalculates correspondence between latent pixels and context vectors. Specifically, when performing\ncfg (Eq. equation 4), unlike conditional prediction where context vectors τ 1:N are uniform across\nframes, unconditional prediction employs unique context vectors ∅1:N . Since even minor variations\nin the context vectors across frames adversely impact the global frame consistency (see Fig. 5), we\npropose reprogramming the Cross-Attention mechanism into Modulated Cross-Attention, which\ncan be formulated as Attn(Q, K, V ) = Softmax( QKT\nd\n(cid:26) W Kci\nif cond\nW K (cid:2)c1\nif uncond\n\n(cid:26) W V ci\nW V (cid:2)c1\n\nif cond\nif uncond\n\nQ = W Q · zi\n\nt, K =\n\nV ) , with\n\n, V =\n\n√\n\n(cid:3)\n\n(cid:3)\n\nT\n\nt\n\nt\n\nt , . . . , cN\nt\n\nt , . . . , cN\nt\n\n(5)\nt denotes a spatial latent feature of frame f i at timestep t, while ‘cond’ and ‘uncond’ refer\nHere, zi\nto conditional and unconditional predictions, respectively. In the process of computing the uncon-\nditional prediction branch of cfg (Eq. (4)), the proposed modulation produces attention maps that\ncorrelate with the similarity between zi\nt ], thus opening\na path for interaction between variant context vectors.\n\nt and the merged null-embeddings [c1\n\nt , . . . , cN\n\n.\n\nVideo Groundings with Cross-Frame Gated Attention. Li et al. (2023b) proposed GLIGEN that\nachieves open-world grounded text-to-image generation via continual learning.\nIn specific, they\nextend the SD model by adding a gated self-attention layer and fine-tuning this layer to incorporate\nlayout information, while freezing the original SD weights. Ground-A-Video adopts GLIGEN’s\ngated attention module and refactor the operation to accommodate the video-grounding in a time-\nconsistent fashion, which has not been explored before.\n\nFollowing the same notations, we denote the semantic information associated with the i-th ground-\ning entity as ei and a set of bounding box coordinates for the i-th grounding entity as li. Next, we\ndefine the concept of ‘grounding’ for a single image, expressed as g= [(e1, l1) , ..., (eM , lM )] with\nM denoting the number of entities to ground. Expanding from image space to video space, we ag-\ngregate groundings across N frames of the input video, yielding g1:N = (cid:2)g1, ..., gN (cid:3). Subsequently,\nwe disentangle the complex editing objectives and introduce handcraft modifications, transitioning\nei to e′\ni for i=1, . . . , M , resulting in g′1:N —the revised grounding input for our model. For exam-\nple, consider the first frame’s grounding, denoted as g1, within the input video in Figure 3, which\ntakes the form of [(‘rabbit’, (0.0, 0.0, 0.7, 0.9)) , . . . , (‘table’, (0.0, 0.8, 1.0, 1.0))]. Then, the editing\nτsnow} transforms grounding g1 to grounding\nobjectives ∆τ ={τrabbit\n(cid:1)\ninput g′1, which is written as [(‘kangaroo’, (0.0, 0.0, 0.7, 0.9)) , . . . , (‘snow’, (0.0, 0.8, 1.0, 1.0))] .\nThe prepared grounding input requires post-processing prior to proceeding to the reverse diffu-\nsion. The semantic information ej\ni is directed to the CLIP text encoder, where it is transformed\ninto text tokens. Meanwhile, the layout information lj\ni is encoded using Fourier embedding, follow-\ning Mildenhall et al. (2021), to produce layout tokens. The text tokens and layout tokens are then\nprojected to the single ‘grounding’ space using an MLP, resulting in grounding tokens:\ni = MLP([ECLIP(ej\nuj\n, Fourier(lj\ni )\ni )\n(cid:125)\n(cid:123)(cid:122)\n(cid:125)\n(cid:124)\n\nτkangaroo, . . . , τtable\n\n(6)\n\n]).\n\n(cid:123)(cid:122)\n\n(cid:1)\n\n(cid:124)\n\ntext token\n\nlayout token\n\nGiven the grounding tokens for frame i and an intermediate latent representation of frame i at time t,\ndenoted as zi\nt, our goal is to project the grounding information onto the visual latent representation.\nHowever, frame-individual projection leads to temporal incoherence as, for example, the same text\nconditioning for ‘kangaroo’ is projected differently onto the i-th latent zi\nt unless\nt is identical to zj\nzi\nt . Therefore, we propose the Cross-Frame Gated Attention which globally\n\nt and the j-th latent zj\n\n5\n\n\fPublished as a conference paper at ICLR 2024\n\nintegrates grounding features onto the latent representation via TS(Softmax( QKT\nd\n\n√\n\nV )) with:\n\nQ = W Q · [zi\nwhere U i = [ui\nM ] for the grounding tokens from equation 6, and the operation TS denotes\nthe token slicing, which ensures that the output shape of the proposed attention operation remains\nconsistent with the input shape.\n\nt, U i], K = W K · [z1\n1, . . . , ui\n\nt , U 1, . . . , zN\n\nt , U 1, . . . , zN\n\nV = W V · [z1\n\nt , U N ],\n\nt , U N ],\n\n3.3\n\nINFLATED CONTROLNET\n\nControlNet (Zhang & Agrawala, 2023) starts with a trainable copy of SD UNet, purposefully de-\nsigned to complement the SD. The trainable branch is then fine-tuned to accommodate task-specific\nvisual conditions, extending the input of ϵθ (zt, t, c) to (zt, t, c, d), where d represents the addi-\ntional conditions such as depth maps or edge maps. This combined locked-copy and trainable-copy\nframework preserves the original synthesis capabilities of SD while enabling precise control over\nthe structural attributes of the generated images.\n\nTo incorporate structural guidance into the video generation process, we employ inflated ControlNet\narchitecture and depth condition. Initially, we estimate depth maps from the source video, converting\nd to d1:N , and apply the Self-Attention inflation and Cross-Attention modulation (Sec. 3.2) on\nControlNet while retaining the fine-tuned weights. During the denoising stage, the residual latent\nres ∈ RN ×h×w×c of ControlNet are first scaled by ‘ControlNet Scale’ hyperparameter,\nfeatures z1:N\nthen transmitted to the inflated SD UNet, as illustrated in Figure 3. The scaling parameter regulates\nthe degree of structural preservation between the input and output (see Fig. 8, 26).\n\nAlgorithm 1 Optical Flow guided Inverted Latents Smoothing\n\nRequire: Number of frames N , Source video frames f 1:N , Timesteps T , Text-Image Diffusion\nModel (SD), Optical flow estimator (RAFT), Threshold for magnitude difference Mthres\nT ←− DDIM INV(f 1:N , T , SD)\nz1:N\nfor all i = 2, 3, ..., N do\n\n▷ run inversion (e.g. DDIM inversion)\n\nopt ←− RAFT(f i−1, f i)\nmag ←− normalize(∥mapi\nmask ←− mapi\nmag < Mthres\nmask = downsample(mapi\n\nopt∥))\n\n∗ mapi\n\nmask + zi\n\nmapi\nmapi\nmapi\nmapi\nT = zi−1\nzi\nend for\nReturn z1:N\n\nT\n\nT\n\nmask)\nT ∗ (1 − mapi\n\nmask)\n\n▷ obtain optical flow map\n▷ compute magnitude map\n▷ obtain binary mask denoting static region\n▷ downsample to latent-level size\n▷ flow guided smoothing\n\n▷ temporally smoothed latents\n\n3.4 OPTICAL FLOW GUIDED INVERTED LATENTS SMOOTHING\n\nAs a video consists of a temporal series of images with a considerable overlap of nearly identical\npixels, contemporary video compression techniques (Hu et al., 2021; 2022) harness motion infor-\nmation to mitigate temporal redundancy instead of saving individual pixels for every frame of a\nvideo. Inspired by this, Chen et al. (2023) introduces pixel-level residuals of the source video into\nthe diffusion process, while Hu & Xu (2023) leverages motion prior to prevent the regeneration of\nredundant areas for frame consistency.\n\nIn our framework, latent codes of the input video are individually computed by performing inversion\non each frame through the T2I model. Hence, the approach of aggregating these latent representa-\ntions and directly feeding them into the denoising process is suboptimal with respect to preserving\nconsistency on static regions. Also inspired by the video codecs, we propose to refine the inverted\nlatents, guided by optical flow information extracted from the input video frames, which accurately\ncaptures the motion changes across frames. The pseudo algorithm is shown in Algorithm 1.\nSpecifically, we first acquire an optical flow map mapi\nopt between the consecutive frames (f i−1, f i)\nusing an optical flow estimation network. This flow map is represented as a two-channel image,\nwith each channel capturing vertical and horizontal motion movement. Subsequently, we incorpo-\nrate the two different motion channels into a one-channel magnitude map mapi\nmag by calculating\nthe Euclidean distance at each pixel location, followed by channel-wise normalizations. The result-\nmag represents the comprehensive motion prior between f i−1 and f i. After thresholding\ning mapi\n\n6\n\n\fPublished as a conference paper at ICLR 2024\n\nFigure 4: Qualitative comparison with baseline methods: Our results exhibit superior temporal consistency, no\nmutated body parts, accurate structural preservation, and the highest Edit-Accuracy without omitting or mixing\ncomponents of edits. Best viewed at high zoom levels.\n\nmag on pre-configured threshold Mthres, which generates a binary mask (mapi\n\nmapi\nform smoothing on inverted latents (zi−1\nthat static regions share the same pixel values between frames.\n\nmask), we per-\nT ) using the obtained mask. This processing guarantees\n\nT , zi\n\n4 EXPERIMENTS\n\n4.1\n\nIMPLEMENTATION DETAILS\n\nWe leverage pretrained weights of Stable Diffusion v1.4 (Rombach et al., 2022) and ControlNet-\nDepth (Zhang & Agrawala, 2023) in addition to self gated attention weights from GLIGEN (Li\net al., 2023b). We use a subset of 20 videos from DAVIS dataset (Pont-Tuset et al., 2017). Generated\nvideos are configured to consist of 8 frames, unless explicitly specified, with a uniform resolution of\n512x512. We benefit from BLIP-2 (Li et al., 2023a) for the automated generation of video caption-\nings. We then feed-forward video frames and captionings to GLIP (Li et al., 2022) model to obtain\nbounding boxes of the target objects. For the applications of video style transfer, bounding box coor-\ndinates are uniformly set as [0.0, 0.0, 1.0, 1.0] which covers the whole frame. RAFT-Large network\n(Teed & Deng, 2020) and ZoeDepth (Bhat et al., 2023) are employed for estimating optical flow\nmaps and depth maps, respectively. In the Appendix (Sec. D), we provide a detailed configuration\nof hyperparameters related to the forward and reverse diffusion processes.\n\n4.2 BASELINE COMPARISONS\n\nQualitative Evaluation. We offer a visual comparison against various state-of-the-art video edit-\ning approaches in Fig. 4. ControlVideo (CV) (Zhang et al., 2023) stands out as the most relevant\nwork to ours, as it introduces a training-free video editing model that is also conditioned on Con-\ntrolNet. Control-A-Video (CAV) (Chen et al., 2023) translates a video with ControlNet guidance\nas well, with a first-frame conditioning strategy. Tune-A-Video (TAV) (Wu et al., 2022) efficiently\nfine-tunes their inflated SD model on the input video. To ensure a fair evaluation, as TAV is not\nprovided structural guidance, we apply their inflation logic to ControlNet and fine-tune the inflated-\nSD-ControlNet on the input video. Gen-1 (Esser et al., 2023) presents a video diffusion architecture\nwith additional structure and content guidance specifically designed for video editing. It’s worth not-\ning that CAV and Gen-1 train their models with video datasets and that the methods with ControlNet\nuniformly employed depth guidance for a fair comparison.\n\n7\n\n\fPublished as a conference paper at ICLR 2024\n\nTAV\nCAV\nCV\nGen-1\nOurs\n\nMethod Text-Align Frame-Con Edit-Acc Preserve-Acc Frame-Con\n\nQuantitative Evaluation. We evaluate the proposed method against aforementioned baselines using\nboth automatic metrics and a user study. The results are outlined in Tab 1.\n(a) Automatic metrics. We use CLIP (Radford\net al., 2021) for automatic metrics. For textual\nalignment (Hessel et al., 2021), we calculate av-\nerage cosine similarity between the target prompt\nand the edited frames. For frame consistency, we\ncompute CLIP image features for all frames in\nthe output video then calculate the average co-\nsine similarity between all pairs of video frames.\nAs shown in Tab. 1, our method outperforms baselines in both textual alignment and temporal con-\nsistency. (b) User study. We surveyed 28 participants to evaluate accuracy of editing and consistency\nof frames in the edited videos, utilizing a rating scale ranging from 1 to 5. Specifically, to measure\nediting accuracy, we divided the evaluation into two questions: i.“Were all the elements in the in-\nput video that needed to be edited accurately edited?” ii.“Were the elements in the output video that\nneeded to be preserved accurately preserved?” Tab. 1 shows that our method surpasses the baselines\nin all three aspects, particularly with a significant lead in Edit-Accuracy and Preserve-Accuracy.\n\nTable 1: Summary of quantitative evaluations using\nCLIP (Left) and user study (Right).\n\n0.810\n0.801\n0.822\n0.833\n0.837\n\n0.959\n0.955\n0.963\n0.939\n0.970\n\n2.99\n2.25\n2.36\n2.41\n4.13\n\n3.05\n2.88\n2.08\n2.56\n4.01\n\n3.13\n2.50\n2.02\n2.51\n4.24\n\nFigure 5: Left: Comparison of Modulated Cross-Attention with the original Cross-Attention, in the context of\nindividually optimized null-embeddings. Right: Comparison of Cross-Frame Gated Attention against original\nframe-independent gated mechanism and the absence of a gated attention layer. The rightmost case shows edit-\ning with frame-independent groundings can yield subpar results compared to editing without any groundings.\n\n4.3 ABLATION STUDIES\n\nText-Align Frame-Con\n\n0.967\n0.960\n0.956\n0.948\n0.970\n\n0.835\n0.802\n0.829\n0.823\n0.837\n\nw/o Modulated CA\nw/o Groundings\nw/o Cross-Frame GA\nw/o ControlNet\nFull components\n\nTable 2: Quantitative assessments on\npipeline components using CLIP.\n\nAttentions. We compare the use of the original Cross-\nAttention with the Modulated Cross-Attention in Fig. 5-Left.\nThe results reveal variations in unconditional context vectors\nlead to distinct appearances of the subject within a video and\nthe Modulated mechanism promotes the coherency of the sub-\nject’s appearance. In Fig. 5-Right, we compare our proposed\nCross-Frame Gated Attention to scenarios with no gated at-\ntention (no grounding conditions) and with the direct applica-\ntion of GLIGEN’s Gated Attention. The example of ‘Iron man’ illustrates the semantic edit of ‘man\nIron man’ is neglected in the absence of groundings guidance and that frame-independent Gated\n(cid:1)\nAttention causes discrepancy in the appearance of the shoulder area of ‘Iron man’ The ‘yellow car’\nexample reveals that grounding guidance can result in inferior results compared to the generation\nwith no grounding conditions at all, if not applied in a cross-frame manner. These results under-\nscore the pivotal role of Cross-Frame Gated Attention in both edit-accuracy and time-consistency.\nMoreover, we provide a quantitative analysis detailing the impact of each module in Tab.2.\nControlNet. We ablate the utilization of ControlNet-Depth. Fig. 2- and Fig. 6-Right depict in-\nstances of inaccurate reconstruction of the input video. ControlNet’s structure guidance is necessi-\ntated to draw accurate structure of ‘rabbit’ and ‘penguin’ inside the bounding boxes, respectively. To\nfurther validate the usage of ControlNet, we conducted a quantitative analysis to assess the impact\nof the proposed inflated ControlNet, as presented in Table 2.\nOptical Flow Smoothing. To assess the impact of optical flow-guided inverted latents smoothing,\nwe ablate the smoothing using three threshold values: 0 (no smoothing applied), 0.2 and 0.6. As\nrevealed in Fig. 6, our flow-guided smoothing effectively eliminates artifacts within static regions\nand enhance consistencies. To find optimal threshold value, we computed Frame Consistencies using\nthresholds of 0.2, 0.3, and 0.4. Notably, the 0.2 threshold resulted in superior frame consistency\n(0.970 for threshold 0.2 > 0.968 for threshold 0.3, 0.964 for threshold 0.4).\n\n8\n\n\fPublished as a conference paper at ICLR 2024\n\nFigure 6: Left: Editing outcomes using different thresholds for Optical Flow Smoothing. Right: Input video\nreconstruction results with and without ControlNet guidance.\n\n4.4 APPLICATIONS OF GROUND-A-VIDEO\nGroundings-guided Editing with Inpainting. Employing a grounding condition offers a signifi-\ncant advantage, as it facilitates the creation of an inpainting mask. This mask is readily obtained by\nintersecting the shared outer areas of bounding boxes. By utilizing this acquired mask during edit-\ning, which identifies regions in the source video that should remain unaltered, Preserve-Accuracy is\nfurther enhanced. For instance, in Fig. 7-Middle, the common outer spaces of red bounding boxes\nand blue bounding boxes form a binary mask which is used for inpainting. Yet, in Fig. 7-Bottom,\nthere is no intersection of common outer spaces, and thus, inpainting is not applied in this scenario.\nVideo Style Transfer & Text-to-video Generation with Pose Control. In the video style transfer\ntask of 7-Middle, target style texts are injected to UNet backbone in both Cross-Frame Gated Atten-\ntion and Modulated Cross Attention layers. The second row showcases the application of style trans-\nfer, while the third row shows style transfer combined with attributes editing. Our method adeptly\ntranslates input video into the desired style, all while incorporating semantic edits when necessary.\nFig. 7-Right illustrates the use of Ground-A-Video for zero-shot text-to-video generation with pose\nmap guidance. The pose map images are sourced from Ma et al. (2023). These spatially-continuous\npose map conditions are integrated into the diffusion reverse process via inflated ControlNet.\n\nFigure 7: Left: Video editing with and without inpainting. Middle: Video style transfer with attribute change.\nRight: Text-to-video generation with pose maps. All three results are generated with zero-shot inference.\n\n5 CONCLUSION\nIn our work, we addressed the problem of complex multi-\nattribute editing of a video and proposed an answer of\nutilizing both spatially-continuous and discrete conditions.\nGround-A-Video offers precise video editing capabilities\nwithout the need for fine-tuning off-the-shelf diffusion models\non any video data. We demonstrated the power of our method on\nvarious input videos and applications. We provided detailed comparisons to existing baselines along\nwith an extensive user study, demonstrating its superiority in terms of consistency and accuracy.\nLimitations. Since video groundings play a crucial role in our pipeline, misleading groundings\n(e.g., incorrect bounding box coordinates) may result in inaccurate editing outcomes. Although the\nuse of ControlNet inherently brings the issue of structural flexibility between the input and output,\nthis can be effectively controlled through the ‘ControlNet Scale’ hyperparameter (see Fig. 8, 26).\n\nFigure 8: Video editing results with\ndifferent ControlNet Scales.\n\n9\n\n\fPublished as a conference paper at ICLR 2024\n\nEthics & Reproducibility. The use of T2I foundation models brings forth several ethical con-\nsiderations. These models possess the potential for malicious applications, such as the creation of\nmisleading or counterfeit content, which could yield adverse societal consequences. Our work heav-\nily relies on one such model, making it susceptible to these concerns. Furthermore, the models were\ntrained on a dataset of internet-sourced images (Schuhmann et al., 2022), which may encompass\ninappropriate content and inherent biases. Consequently, these models might perpetuate such biases\n(Mishkin et al., 2022) and generate inappropriate imagery. To address these potential concerns and\nfoster reproducibility, we will release our source code, model, and data under a license that encour-\nages ethical and legal usage. Additional information regarding experiments, implementations and\nthe code base can be found in the Appendix (see Sec. D).\n\nAcknowledgements. This research was supported by the National Research Foundation of Ko-\nrea (NRF) (RS-202300262527), Field-oriented Technology Development Project for Customs Ad-\nministration funded by the Korean government (the Ministry of Science & ICT and the Ko-\nrea Customs Service) through the National Research Foundation (NRF) of Korea under Grant\nNRF2021M3I1A1097910 & NRF2021M3I1A1097938, Korea Medical Device Development Fund\ngrant funded by the Korea government (the Ministry of Science and ICT, the Ministry of Trade, In-\ndustry, and Energy, the Ministry of Health & Welfare, the Ministry of Food and Drug Safety) (Project\nNumber: 1711137899, KMDF PR 20200901 0015), and Culture, Sports, and Tourism R&D Pro-\ngram through the Korea Creative Content Agency grant funded by the Ministry of Culture, Sports\nand Tourism in 2023.",
    "abstract": "This paper introduces a novel grounding-guided video-to-video translation framework called Ground-A-Video for multi-attribute video editing.\nRecent endeavors in video editing have showcased promising results in single-attribute editing or style transfer tasks, either by training T2V models on text-video data or adopting training-free methods.\nHowever, when confronted with the complexities of multi-attribute editing scenarios, they exhibit shortcomings such as omitting or overlooking intended attribute changes, modifying the wrong elements of the input video, and failing to preserve regions of the input video that should remain intact.\nGround-A-Video attains temporally consistent multi-attribute editing of input videos in a training-free manner without aforementioned shortcomings.\nCentral to our method is the introduction of cross-frame gated attention which incorporates groundings information into the latent representations in a temporally consistent fashion, along with Modulated Cross-Attention and optical flow guided inverted latents smoothing.\nExtensive experiments and applications demonstrate that Ground-A-Video's zero-shot capacity outperforms other baseline methods in terms of edit-accuracy and frame consistency.\nFurther results and code are available at our project page ( http://ground-a-video.github.io )",
    "response": {
      "Soundness": 4,
      "Presentation": 3,
      "Contribution": 3,
      "Rating": 8,
      "Confidence": 4,
      "Strengths": "S1. This paper presents the ﬁrst training-free grounding-driven video editing framework, which is relatively innovative.\n\nS2. The proposed method and experimental results are consistent with their motivation and eﬀectively solve the problem of multi-attribute video editing in complex scenes.\n\nS3. The method is clearly stated, and the details are comprehensive.",
      "Weaknesses": "W1. Since the introduced depth map and optical ﬂow map both reﬂect pixel-level structural information, will this cause the structure before and after editing to be too consistent, so that if the foreground is replaced by an object with inconsistent structure, the editing result will be poor and lack of ﬂexibility? e.g. replace the “rabbit” in the phrase \"A rabbit is eating a watermelon on the table\" with an animal without long ears.\n\nW2. For similar reasons, this may also limit the editing method to the task of adding or deleting objects.",
      "Questions": "See weaknesses above."
    }
  },
  {
    "paper_id": "BpAxeHmQ94",
    "summary": "Background: Recent advances have shown that deep learning models are highly vulnerable to adversarial examples, leading to the development of defensive mechanisms divided into empirical defenses that lack robustness guarantees and certifiable methods that offer robustness certificates. Certifiable methods focus on model robustness within certain perturbation norms. There is also an increasing interest in cost-sensitive learning, where different adversarial misclassifications incur different costs, which is vital when certain misclassifications have more significant real-world consequences.\nGap: Existing methods for adversarially robust classification either do not provide robustness certification or face scalability issues. While many methods focus on improving overall model robustness, they often disregard the varying costs of different adversarial misclassifications. There is a need for scalable, certifiable methods that also account for these cost sensitivities.\nHypothesis: Randomized smoothing, a scalable robustness certification framework, can be adapted to certify cost-sensitive robustness. By introducing a cost-sensitive certified radius, it is possible to provide tight robustness guarantees customized for any given cost matrix.\nMethod: The paper introduces a cost-sensitive certified radius, an adaptation of the traditional randomized smoothing certification, to account for class-specific misclassification costs. This involves extending the standard randomized smoothing framework to include cost-sensitive considerations, optimizing smoothed classifiers through fine-grained radius optimization schemes, and training with a focus on different data subgroups. The method involves using Monte Carlo sampling to estimate robustness bounds and aims to balance the trade-off between cost-sensitive robustness and overall accuracy. Experiments on image benchmarks and a real-world medical dataset demonstrate the approach.\nConclusion: The proposed method significantly improves certified cost-sensitive robustness without compromising overall accuracy, as demonstrated through experiments. It outperforms baseline methods, offering a better trade-off between robustness and accuracy while providing tight robustness guarantees for various cost matrices. The study opens new opportunities for developing certifiably robust models that align better with cost-sensitive real-world applications.\n",
    "full text": "\nPROVABLY ROBUST COST-SENSITIVE LEARNING VIA\nRANDOMIZED SMOOTHING\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nWe focus on learning adversarially robust classifiers under a cost-sensitive sce-\nnario, where the potential harm of different classwise adversarial transformations\nis encoded in a binary cost matrix. Existing methods are either empirical that\ncannot certify robustness or suffer from inherent scalability issues. In this work, we\nstudy whether randomized smoothing, a scalable certification framework, can be\nleveraged to certify cost-sensitive robustness. Built upon a notion of cost-sensitive\ncertified radius, we show how to adapt the standard randomized smoothing certifi-\ncation pipeline to produce tight robustness guarantees for any given cost matrix.\nIn addition, with fine-grained certified radius optimization schemes designed for\ndifferent data subgroups, we propose an algorithm to train smoothed classifiers\nthat are optimized for cost-sensitive robustness. Extensive experiments on image\nbenchmarks and a real-world medical dataset demonstrate the superiority of our\nmethod in achieving significantly improved performance of certified cost-sensitive\nrobustness while having a negligible impact on overall accuracy.\n\n1\n\nINTRODUCTION\n\nRecent studies have revealed that deep learning models are highly vulnerable to adversarial exam-\nples (Szegedy et al., 2013; Goodfellow et al., 2014). To improve model robustness in the presence of\nadversarial examples, various defensive mechanisms have been proposed, primarily falling into two\ncategories: empirical defenses (Goodfellow et al., 2014; Papernot et al., 2016; Kurakin et al., 2016;\nMadry et al., 2017; Zhang et al., 2019) and certifiable methods (Raghunathan et al., 2018; Wong &\nKolter, 2018; Gowal et al., 2018; Cohen et al., 2019; Lecuyer et al., 2019; Jia et al., 2019; Li et al.,\n2019). In particular, certifiable methods can produce a robustness certificate for the model prediction\nto remain unchanged within some specific norm-bounded perturbation ball of any testing input and\ntrain models to be provably robust with respect to the certificate.\n\nMost existing defenses aim to improve the overall robustness of a classification model, assuming the\nsame penalty is imposed on all kinds of adversarial misclassifications. For real-world applications,\nhowever, it is likely that some specific misclassifications are more consequential than others (Domin-\ngos, 1999; Elkan, 2001). For instance, misclassifying a malignant tumor as benign in the application\nof medical diagnosis is much more detrimental to a patient than the reverse. Therefore, instead of\nsolely focusing on enhancing overall robustness, the development of defenses should also account\nfor the difference in costs induced by different adversarial examples. In line with existing works\non cost-sensitive robust learning (Domingos, 1999; Asif et al., 2015; Zhang & Evans, 2019; Chen\net al., 2021), we aim to develop models that are robust to cost-sensitive adversarial misclassifications,\nwhile maintaining the standard overall classification accuracy. However, existing defenses are either\nhindered by their foundational reliance on heuristics, which often fall short of providing a robustness\nguarantee (Domingos, 1999; Asif et al., 2015; Chen et al., 2021), or suffer from inherent scalability\nissues (Zhang & Evans, 2019) (see detailed discussions of related works in Appendix A).\n\nTo achieve the best of both worlds, we propose to learn provably cost-sensitive robust classifiers\nby leveraging randomized smoothing (Liu et al., 2018; Cohen et al., 2019; Salman et al., 2019), an\nemerging robustness certification framework that has attracted a lot of attention due to its simplicity\nand scalability. However, optimizing smoothed classifiers for cost-sensitive robustness is intractable,\nprimarily due to the inherent complexity introduced by the discrete Gaussian sampling process when\n\n1\n\n\fUnder review as a conference paper at ICLR 2024\n\ntransforming base classifiers into smoothed classifiers. In addition, different designs of cost matrix\nnecessitate a more flexible and targeted optimization scheme than optimizing for overall robustness.\n\nContributions. We are the first to adapt the randomized smoothing framework to certify and train\nfor cost-sensitive robustness. In particular, for any binary cost matrix (Section 2), we introduce the\nnotion of cost-sensitive certified radius (Definition 3.1), which captures the maximum allowable\nℓ2 perturbation with respect to the smoothed classifier for each sensitive seed input (Theorem\n3.2). We show that compared with standard certified radius typically adopted in the literature, our\nproposed definition is more suitable for certifying cost-sensitive robustness, as it theoretically defines\na larger certified radius specifically for a wide range of cost matrices (Theorem 3.3). Built upon\nthe definition of cost-sensitive certified radius, we further propose a practical certification algorithm\nusing Monte Carlo samples (Algorithm 1), which provides two different methods for computing the\nprobabilistic bound on certified radius such that the tighter bound can always be returned (Section\n3.2). Moreover, to train for cost-sensitive robust smoothed classifiers, we attempt to adapt the\ncommonly-used reweighting method in standard cost-sensitive learning literature (Elkan, 2001; Khan\net al., 2017). However, we discover that the naive reweighting scheme does not adapt well when\ntraining a smoothed classifier, owing to the indirect optimization of the base classifier and the non-\noptimal trade-off between sensitive and non-sensitive examples (Section 4.1). Therefore, we take the\ndistinctive properties of different data subgroups into account and design an advanced cost-sensitive\nrobust training method based on MACER (Zhai et al., 2020) to directly optimize the certified radius\nwith respect to the smoothed classifier (Section 4.2). Experiments on typical image benchmarks and\na real-world medical dataset illustrate the superiority of our method in achieving consistently and\nsignificantly improved certified cost-sensitive robustness while maintaining a similar performance of\noverall accuracy under a variety of cost matrix settings (Section 5).\n\n2 PRELIMINARIES\n\nRandomized Smoothing. Cohen et al. (2019) proposed a probabilistic certification framework,\nnamed as randomized smoothing (RS), which is able to certify robustness for large-scale models.\nIn particular, randomized smoothing is based on the following construction of smoothed classifiers,\nwhich first augments normal inputs with randomly sampled Gaussian noise, then passes the noisy\ninputs through a base classifier and aggregate their predictions using majority voting:\nRd be the input space and [m] :=\n1, 2, . . . , m\nDefinition 2.1. Let\n{\n[m] and σ > 0, the corresponding smoothed classifier gθ is defined as:\nbase classifier fθ :\n(cid:2)fθ(x + δ) = k(cid:3),\n\nX ⊆\nX →\ngθ(x) = arg max\n\nbe the label space. For any\n\nPδ∼N (0,σ2I)\n\nx\n\n}\n\n.\n\nk∈[m]\n\n∀\n\n∈ X\n\nX →\n\nLet hθ :\n\n[0, 1]m be the function that maps any input x to the prediction probabilities of gθ(x):\n\n[hθ(x)]k = Pδ∼N (0,σ2I)\n\n(cid:2)fθ(x + δ) = k(cid:3),\nThe following lemma, proven in Cohen et al. (2019), characterizes the maximum allowable ℓ2-\nperturbation for any input x such that the prediction of gθ at x remains the same within the radius.\n[m] be its ground-truth label. If\nLemma 2.2 (Cohen et al. (2019)). Let x\n(cid:2)fθ(x + δ) = k(cid:3), then the prediction of\ngθ classifies x correctly, i.e., y = arg maxk∈[m]\ng at x is both accurate and provably robust with certified ℓ2-norm radius R(x), which is defined as:\n\nbe any input and y\nPδ∼N (0,σ2I)\n\nand k\n\n[m].\n\n∈ X\n\n∈ X\n\n∈\n\n∈\n\nx\n\n∀\n\nR(x) =\n\n(cid:104)\n\nσ\n2\n\nΦ−1(cid:0)[hθ(x)]y\n\n(cid:1)\n\nΦ−1(cid:0) max\n\nk̸=y\n\n−\n\n[hθ(x)]k\n\n(cid:1)(cid:105)\n\n,\n\n(1)\n\nwhere Φ is the CDF of standard Gaussian\n\n(0, 1) and Φ−1 denotes its inverse.\n\nN\n\nWe remark that our method is built upon this randomized smoothing framework, which is designed\nfor l2 perturbations. Recent studies have adapted the standard framework to certify robustness against\nother ℓp-norm bounded perturbations (Mohapatra et al., 2020; Yang et al., 2020). We believe our\nmethod is applicable to those ℓp-norms, whereas studying how to certify cost-sensitive robustness\nagainst general perturbations defined in metrics beyond ℓp-norm would be an interesting future work.\n\n2\n\n\fUnder review as a conference paper at ICLR 2024\n\n∈\n\n0, 1\n\n∈ {\n\nCost-Sensitive Robustness. We consider robust classification tasks under cost-sensitive scenarios,\nwhere the goal is to learn a classifier with both high overall accuracy and cost-sensitive robustness.\nm×m is a predefined cost matrix that encodes the potential harm of different\nSuppose C\n}\nclasswise adversarial transformations.1 In particular, the most studied overall robustness corresponds\n[m]\nto having a cost matrix, where all the entries are 1 except for the diagonal ones. For any j\nand k\n[m], Cjk = 1 means any misclassification from seed class j to target class k will induce\na cost, whereas Cjk = 0 suggests that there is no incentive for an attacker to trick the model to\nmisclassify inputs from class j to class k. Therefore, the goal of cost-sensitive robust learning is\nto reduce the number of adversarial misclassifications that will induce a cost defined by C. For the\nease of presentation, we also introduce the following notations. For any seed class j\n[m], we\nbe the set of cost-sensitive target classes. If Ωj is an empty set,\nlet Ωj =\n1 is a\nthen all the examples from seed class j is non-sensitive. Otherwise, any class j with\nsensitive seed class. Given a dataset\n}i∈[n], we define the set of cost-sensitive examples\nas\n\n, while the remaining examples are regarded as non-sensitive.\n\n[m] : Cjk = 1\n}\n\n(xi, yi)\n{\n\n∈\nΩj\n|\n\nk\n{\n\ns =\n\n| ≥\n\n=\n\n∈\n\n∈\n\nS\n\n:\n\nS\n\n(x, y)\n{\n\n∈ S\n\nΩy\n|\n\n| ≥\n\n1\n}\n\n3 CERTIFYING COST-SENSITIVE ROBUSTNESS\n\nThis section explains how to certify cost-sensitive robustness using randomized smoothing. We first\nintroduce the formal definition of cost-sensitive certified radius and the corresponding evaluation\nmetrics then discuss its connection to the standard certified radius (Section 3.1). Finally, based on the\nproposed definition, we design a practical certification algorithm using finite samples (Section 3.2).\n\n3.1 COST-SENSITIVE CERTIFIED RADIUS\n\n∈ S\n\ns, only misclassifying x to a target class in Ωy incurs a\nRecall that for any example (x, y)\ncost, whereas misclassifications to any class from [m]\nΩy is tolerable. Below, we formally define\n\\\ncost-sensitive certified radius, which adapts the standard certified radius to cost-sensitive scenarios:\nDefinition 3.1 (Cost-Sensitive Certified Radius). Consider the same setting as in Theorem 2.2. Let\nC be an m\n[m] where y is a sensitive seed class, the\ncost-sensitive certified radius at (x, y) with respect to C is defined as:\n(cid:20)\nΦ−1(cid:16)\n\nm cost matrix. For any example (x, y)\n\nΦ−1(cid:16)\n\n(cid:17)(cid:21)\n,\n\n∈ X ×\n\nRc-s(x; Ωy) =\n\n(cid:2)hθ(x)(cid:3)\n\n(cid:2)hθ(x)(cid:3)\n\n×\n\n(cid:17)\n\nmax\nk∈[m]\n\nk\n\n−\n\nmax\nk∈Ωy\n\nσ\n2\n\nk\n\nwhere Ωy =\n\nk\n{\n\n∈\n\n, and Φ−1 is the inverse CDF of standard Gaussian\n[m] : Cyk = 1\n}\n\nN\n\n(0, 1).\n\nBased on Definition 3.1, the following theorem extends Lemma 2.2, which shows how to produce a\ncertificate for cost-sensitive robustness of a smoothed classifier with respect to any given input.\nTheorem 3.2. Consider the same setting as in Definition 3.1. For any example (x, y), if the predicted\nΩy, then\nclass of the smoothed classifier gθ at x does not incur a cost, i.e., arg maxk∈[m][hθ(x)]k /\n∈\ngθ is provably robust at x with certified radius Rc-s(x; Ωy) measured in ℓ2-norm.\n\nTheorem 3.2 can be applied to certify cost-sensitive robustness for any binary-valued cost matrix. Note\nthat maxk∈[m][hθ(x)]k, the first term on the right hand side of Definition 3.1, denotes the maximum\npredicted probability across all classes with respect to hθ, while the second term maxk∈Ωy [hθ(x)]k\nis the maximum predicted probability across all sensitive target classes within Ωy, which are different\nfrom the corresponding terms used in standard certified radius (Equation 1). We remark that these\nmodifications are necessary for providing tight certification results for cost-sensitive settings, since it\nis possible for the prediction g(x) to be incorrect but cost-sensitive robust under certain scenarios, as\nlong as the incorrect prediction does not fall into the set of sensitive target classes Ωy.\n\nMore specifically, the following theorem, proven in Appendix B, characterizes the connection between\nthe cost-sensitive certified radius and the standard notion of certified radius for any cost matrix.\nTheorem 3.3. For any cost-sensitive scenario, if the prediction gθ(x) does not incur a cost, then\nRc-s(x; Ωy)\n\nR(x), where the equality holds when Ωy =\n\n[m] : k\n\n= y\n\n.\n\nk\n{\n\n∈\n\n}\n\n≥\n\n1Although we only consider binary-valued cost matrices in this work, our method can be easily adapted to\n\nprovide cost-sensitive robustness guarantees for real-valued cost matrices (Zhang & Evans, 2019).\n\n3\n\n̸\n\fUnder review as a conference paper at ICLR 2024\n\n←\ntop index in count0\n\ncount0\nˆcA\n←\ncount\npA\n\nSAMPLEUNDERNOISE(f, x, n0, σ)\n\n←\nLowerConfBnd(count[ˆcA], n, 1\n\nSAMPLEUNDERNOISE(f, x, n, σ)\nα)\n\nAlgorithm 1 Certification for Cost-Sensitive Robustness\n1: function CERTIFY(f, σ, x, n0, n, α, Ωy)\n2:\n3:\n4:\n5:\n←\n6: R1 = σΦ−1(pA)\nˆcB\n7:\npA\n8:\npB\n9:\n10: R2 = σ\n11:\n12:\n13:\n14:\n15: end function\n\ntop index in count[Ωy]\nLowerConfBnd(count[ˆcA], n, 1\nUpperConfBnd(count[ˆcB], n, 1\n2 (Φ−1(pA)\n\nreturn prediction ˆcA and max(R1, R2)\n\n−\nif max(R1, R2) > 0\n\nreturn ABSTAIN\n\nΦ−1(pB))\n\nα/2)\nα/(2\n\n←\n←\n←\n\n−\n−\n\nelse\n\n−\n\n|\n\nΩy\n\n))\n|\n\nTheorem 3.3 suggests that using Rc-s(x; Ωy) can always yield a cost-sensitive robustness certificate\nnot inferior to that using the standard certified radius R(x). In particular, for input x with\n=\n1, the improvement of cost-sensitive robustness certification based on Rc-s(x; Ωy) is likely to be\nm\nmore significant. As will be shown in Figures 1(a) and 1(b), the empirically estimated robustness\ncertificate with Rc-s(x; Ωy) consistently surpasses that with R(x) across various cost matrix settings.\n\nΩy\n|\n\n−\n\n| ̸\n\nEvaluation Metrics. We formally define our evaluations metrics, certified cost-sensitive robustness\nand overall accuracy, which will be used to measure a classifier’s performance under a cost-sensitive\nsetting. For any binary cost matrix, we define certified cost-sensitive robustness as the ratio of\ncost-sensitive examples that are provably robust with gθ against ℓ2 perturbations with strength ϵ > 0:\n\nRobc-s(gθ) =\n\n1\ns\n\n(cid:88)\n\n1(cid:8)Rc-s(x; Ωy) > ϵ(cid:9), provided\n\n|S\n\n|\n\n(x,y)∈S s\n\ns\n\n|S\n\n|\n\n> 0,\n\n(2)\n\ns denotes the set of cost-sensitive examples. In addition, the overall accuracy of gθ is defined\n\nwhere\nas the fraction of correctly classified samples with respect to the whole training dataset:\n\nS\n\nAcc(gθ) =\n\n1\n\n(cid:88)\n\n1(cid:8)R(x) > 0(cid:9).\n\n|S|\n\n(x,y)∈S\n\n(3)\n\nWe use the standard certified radius R(x) in Equation 3, since the computation of overall accuracy\ndoes not rely on the cost matrix C. As will be discussed next, we are going to replace Robc-s(gθ) and\nR(x) with their empirical counterparts for practical implementations of the two metrics.\n\n3.2 PRACTICAL CERTIFICATION ALGORITHM\n\nBy definition, the construction of hθ requires access to an infinite number of Gaussian samples to\ncompute the cost-sensitive certified radius. However, it is computationally infeasible in practice\nto obtain the exact value of Rc-s(x; Ωy) even for a single example (x, y). The difference between\nRc-s(x; Ωy) and standard certified radius R(x) necessitates a new certification procedure. In this\nsection, we put forward a new Monte Carlo algorithm for certifying cost-sensitive robustness by\nadapting Cohen et al. (2019)’s standard algorithm, which is applicable to any binary cost matrix.\n\nAlgorithm 1 depicts the pseudocode of our certification method, which involves two ways to compute\nthe confidence bound for cost-sensitive certified radius Rc-s(x; Ωy) (see Appendix C for more details\nabout the sampling scheme). The first approach is based on R1, which is computed using a lower\nα confidence bound on the ground-truth probability pA with respect to the top class, same as\n1\nthe bound for standard radius used in Cohen et al. (2019) for certifying overall robustness. Note\nthat according to Definition 3.1, the standard certified radius is guaranteed to be less than or equal\nto the corresponding cost-sensitive certified radius, suggesting that R1 also severs as a valid 1\nα\nconfidence bound for cost-sensitive radius. On the other hand, R2 is computed using both a lower\n\n−\n\n−\n\n4\n\n\fUnder review as a conference paper at ICLR 2024\n\n(a) Cohen\n\n(b) MACER\n\n(c) Trade-Off\n\nFigure 1: Comparisons of certified cost-sensitive robustness computed using R1 and R2 with respect\nto (a) Cohen and (b) MACER. Figure 1(c) visualizes the trade-off between overall accuracy and\ncost-sensitive robustness with respect to our method and Cohen-R. We fix the CIFAR-10 class “cat”\nas the only sensitive seed class and vary the corresponding cost-sensitive target classes.\n\n−\n\nα/2 confidence bound of pA and an upper 1\n\n1\nbound of pB, the ground-truth probability\nof the top class within Ωy. The following theorem, proven in Appendix C by union bound, shows\nthat the confidence for R2 being a cost-sensitive certified radius is guaranteed to be at least 1\nTheorem 3.4. For any example (x, y), the second radius R2 specified by Algorithm 1 is a certified\nα confidence over the randomness of Gaussian sampling.\ncost-sensitive robust radius with at least 1\n\nα/2\n|\n\nΩy\n\nα.\n\n−\n\n−\n\n|\n\n−\n\nTheorem 3.4 suggests that the prediction of gθ at (x, y) will not incur any undesirable cost with high\nprobability as long as the ℓ2 perturbation is within radius max(R1, R2). By definition, the output\nmax(R1, R2) of Algorithm 1 is always better than R1 solely for certifying cost-sensitive robustness.\n\nComparison between R1 and R2. Note that R2 is specifically designed for certifying cost-sensitive\nrobustness, whereas R1 works for both overall and cost-sensitive scenarios. To further illustrate the\nsuperiority of our proposed practical certification algorithm to Cohen et al. (2019)’s, we specify the\nscenarios where R2 is superior to R1 by definition and provide empirical evidence in Figures 1(a)\nand 1(b). By definition, there are two scenarios where R2 is likely to be larger than R1:\n\n1. Ωy does not contain the second highest probability class, which is likely to happen when the\n\nnumber of cost-sensitive target classes is small, i.e.,\n\n2. Even when\n\nΩy\n\n= m\n\n1, it is still possible for R2 > R1, especially if the ground-truth\n\nprobability of the second highest class pB is far below 1\n\n|\n\n|\n\n−\n\nΩy\n\n= 1.\n\n|\n\n|\n\npA.\n\n−\n\nIn summary, if pB, the ground-truth probability of the top class in Ωy, is far below pA, then R2 will be\nmuch higher than R1, thus producing a much tighter cost-sensitive robust certificate. To further study\nthe relationship between R1 and R2, we conduct experiments to compare the cost-sensitive robustness\ncomputed based on R1 and R2 across different cost matrix scenarios, as shown in Figures 1(a) and\n1(b). We fix a single seed class “cat” as sensitive and vary the size of its corresponding cost-sensitive\n1, 3, 5, 7, 9\ntarget classes, randomly selecting\nof all the possible target classes. We then\nevaluate the performance of the models produced by two baseline training methods for randomized\nsmoothing, Cohen (Cohen et al., 2019) and MACER (Zhai et al., 2020). Figures 1(a) and 1(b) show\nthat cost-sensitive robustness measured by R2 outperforms that of R1 across most of the settings. As\nthe size of Ωy decreases, the performance gap between R1 and R2 is more pronounced, underscoring\nthe superior efficacy of R2 and confirming the superiority of Algorithm 1 over Cohen et al. (2019)’s.\n\nΩy\n|\n\n| ∈ {\n\n}\n\n4 TRAINING FOR COST-SENSITIVE ROBUSTNESS\n\nA popular training scheme for cost-sensitive learning is reweighting (Elkan, 2001), which assigns\nlarger weights to cost-sensitive inputs during model training. Thus, a natural question is whether\nthe reweighting scheme can be incorporated in randomized smoothing to train for cost-sensitive\nrobustness. In this section, we first study the effectiveness of reweighting methods combined with the\nmethod of Cohen et al. (2019) (Section 4.1), then provide a new training method by leveraging the\ndesign insight of MACER (Zhai et al., 2020) to achieve better performance (Section 4.2).\n\n5\n\n13579|Ωy|0.00.20.40.6Cost-SensitiveRobustnessR1R213579|Ωy|0.00.20.40.6Cost-SensitiveRobustness0.20.40.60.8Cost-SensitiveRobustness0.550.600.650.70OverallAccuracyCohen-ROurs\fUnder review as a conference paper at ICLR 2024\n\n4.1 REWEIGHTING METHOD\n\nWe consider the base classifier training method introduced in Cohen et al. (2019), which proposes to\ninject Gaussian noise to all inputs during the training process of fθ. Given a binary cost matrix,\nis\ns be the distribution of all sensitive examples which incur costs\nthe underlying data distribution, let\nif misclassified and let\nn represent the distribution of the remaining normal examples. Intuitively,\nthe training pipeline of randomized smoothing can be adapted to cost-sensitive settings using a simple\nreweighting scheme by increasing the weights assigned to the loss function of sensitive examples,\ndenoted as Cohen-R. More concretely, the training objective of Cohen-R is defined as follows:\n\nD\n\nD\n\nD\n\nCE\n\nmin\nθ∈Θ\n\n(cid:0)fθ(x + δ), y(cid:1) + α\n\n(cid:20)\nE(x,y)∼Dn L\n1 is\nwhere Θ denotes the set of model parameters,\na trade-off parameter that controls the performance between sensitive and non-sensitive examples.\nWhen α = 1, the above objective function is equivalent to the training loss used in standard\nrandomized smoothing (Cohen et al., 2019). However, due to the indirect optimization of the\nsmoothed classifier, Cohen-R tends to sacrifice overall accuracy to a large degree when trying to\nachieve high cost-sensitive robustness (see Figure 1(c) for empirical evidence about this statement).\n\nCE represents the cross-entropy loss, and α\n\nE(x,y)∼Ds L\n\n(cid:0)fθ(x + δ), y(cid:1)\n\nCE\n\n≥\n\nL\n\n(cid:21)\n\n,\n\n·\n\n4.2 OUR METHOD\n\nIn this section, we propose a more direct optimization scheme based on the proposed notion of\ncertified cost-sensitive radius, leveraging a similar insight of MACER (Zhai et al., 2020) to better\ntrade off cost-sensitive robustness and overall accuracy. To simplify notations, we first introduce a\nu denoting the thresholding parameters, we define\ngeneral class of margin-based losses. Given l\nR representing the certified radius, let\nthe following class of margin losses: for any r\n(cid:0)r; l, u(cid:1) = max\n{\n\n−\nHere, the indicator function selects data points whose certified radius falls into the range of [l, u]. For\nany binary cost matrix C, the total training loss function of our method is defined as:\n\n≤\n∈\nu\n\n1(l\n\nr, 0\n\nu).\n\n} ·\n\n≤\n\n≤\n\nL\n\nM\n\nr\n\nI3,\n\n·\n\nmin\nI1 + λ\nI2 + λ\n·\nθ∈Θ\nwhere I1 = E(x,y)∼D L\nI2 = E(x,y)∼D L\nI3 = E(x,y)∼Ds L\n\nCE\n\nM\n\n(cid:0)hθ(x(cid:1), y),\n(cid:0)Rc-s(x; Ωy, hθ); 0, γ1\n(cid:0)Rc-s(x; Ωy, hθ);\n\n(cid:1),\nγ2, γ2\n\nM\n\n−\n\n(4)\n\n(cid:1),\n\n,\n\nD\n\nwhere λ, γ1, γ2 > 0 are hyperparameters, Rc-s(x; Ωy, hθ) is the cost-sensitive certified radius with\nrespect to hθ, and\ns denote the underlying distributions of all and cost-sensitive examples,\nD\nrespectively. In particular, Equation 4 consists of three terms: I1 represents the cross-entropy loss\nwith respect to hθ over\n, which controls the overall accuracy; I2 and I3 control the robustness with a\nshared trade-off parameter λ.2 The range of the interval [l, r] represents which data subpopulation we\nwant to optimize. A larger thresholding parameter such as γ1 and γ2 lead to a higher data coverage,\nwhereas the range with a smaller threshold includes fewer data points. We set γ2 > γ1 to have a\nwider adjustment range for sensitive seed examples. As shown in Wang et al. (2020), optimizing\nmisclassified samples can help adversarial robustness, thus, we intend to include sensitive seed\nexamples with a negative radius in [\n\nγ2, 0) in the design of I3 for a better performance.\n\nD\n\nIntuitively speaking, by imposing different threshold restrictions [l, u] on the certified radius of\nsensitive seed classes and normal seed classes, the optimization process can prioritize making\nadjustments to data subpopulations of specific classes rather than considering all data points belonging\nto those classes. This is also a key advantage of our method over the naive reweighting method. As\nwill be shown in our experiments, such fine-grained optimization enables our method to improve\ncertified cost-sensitive robustness to a large extent without sacrificing overall accuracy. Figure 1(c)\nprovide empirical evidence of the non-optimal trade-off for the reweighting method Cohen-R on\nCIFAR-10 under a specific cost matrix of “cat” being the only sensitive seed class, compared with\n\n−\n\n2When |Ωy| < m − 1, we optimize the data distribution D in I2 to counteract I3’s potential adverse effects.\n\nHowever, for |Ωy| = m − 1, we utilize Dn since I2 and I3 have no optimization conflicts.\n\n6\n\n\fUnder review as a conference paper at ICLR 2024\n\nTable 1: Certification results for seedwise cost matrices. The noise level σ is 0.5 for both CIFAR-\n10 and Imagenette. Acc stands for overall accuracy. Robc-s is certified cost-sensitive robustness\nestimated by max(R1, R2), Robstd is certified cost-sensitive robustness estimated by R1, Robnon\nis certified robustness of non-sensitive samples, all measured at ϵ = 0.5. The best statistics are\nhighlighted in bold.\n\nDataset\n\nType\n\nMethod\n\nAcc\n\nRobc-s Robstd Robnon\n\nCIFAR-10\n\nImagenette\n\nSingle (3)\n\nMulti (2, 4)\n\nSingle (7)\n\nMulti (3, 7)\n\nCohen\nMACER\nCohen-R\nOurs\nCohen\nMACER\nCohen-R\nOurs\nCohen\nMACER\nCohen-R\nOurs\nCohen\nMACER\nCohen-R\nOurs\n\n0.654\n0.659\n0.642\n0.661\n0.659\n0.659\n0.662\n0.669\n0.803\n0.782\n0.746\n0.791\n0.804\n0.782\n0.740\n0.791\n\n0.223\n0.273\n0.506\n0.628\n0.293\n0.291\n0.348\n0.461\n0.646\n0.638\n0.733\n0.811\n0.589\n0.578\n0.617\n0.715\n\n0.193\n0.259\n0.466\n0.598\n0.257\n0.262\n0.308\n0.441\n0.637\n0.638\n0.733\n0.795\n0.579\n0.578\n0.613\n0.713\n\n0.466\n0.499\n0.437\n0.455\n0.463\n0.527\n0.413\n0.504\n0.744\n0.739\n0.671\n0.725\n0.771\n0.774\n0.843\n0.738\n\nthe performance of models produced by our proposed method. Each (Robc-s, Acc) point is derived\nfrom a unique set of model tuning hyperparameters. Notably, the points corresponding to our method\npredominantly occupy the upper-right quadrant compared to those of Cohen-R illustrating that our\nproposed approach offers a superior trade-off between cost-sensitive robustness and overall accuracy.\n\n5 EXPERIMENTS\n\nWe evaluate the performance of our method on two image benchmarks: CIFAR-10 (Krizhevsky\net al., 2009) and Imagenette, a 10-class subset of ImageNet.3 In addition, we test our method on the\nmedical dataset HAM10k (Ghosh et al., 2023) to further examine its generalizability for real-world\nscenarios, where cost-sensitive misclassifications have more severe consequences. For CIFAR-10 and\nHAM10k, we use the same ResNet (He et al., 2016) architecture as employed in (Cohen et al., 2019).\nSpecifically, we choose ResNet-56 network, since it attains comparable performance to ResNet-110\nwith a shorter computation time. For Imagenette, we use ResNet18 following (Pethick et al., 2023).\n\nBaselines. We primarily compare our method with two prevalent randomized smoothing methods:\nCohen (Cohen et al., 2019) and MACER (Zhai et al., 2020). We also consider Cohen-R, a variant\nof Cohen adapted for cost-sensitive scenarios using reweighting technique described in Section 4.1.\nWe select Cohen for comparisons with standard randomized smoothing and MACER for comparing\nwith methods that optimize for certified radius. Both of these baselines are optimized for overall\nrobustness. Cohen-R is optimized for cost-sensitive performance by tuning the weight parameter α\nfor a fair comparison. In addition, our experiments are mainly conducted for two categories of cost\nmatrices: seedwise and pairwise, as their corresponding training procedures are slightly different.\n\nExperimental Details. Tuning the hyperparameters plays a critical role. For Cohen-R, the weight\nparameters α are carefully tuned to ensure the best possible trade-off between overall accuracy and\nand find that nearly\ncost-sensitive robustness, where we enumerate all values from\nin all cases of cost matrices, α = 1.2 achieves the best result. For our method, we follow Zhai et al.\n(2020)’s settings for training. The main difference between our method and MACER is the choice of\n\n1.0, 1.1, . . . , 2.0\n\n}\n\n{\n\n3This dataset can be downloaded from https://github.com/fastai/imagenette.\n\n7\n\n\fUnder review as a conference paper at ICLR 2024\n\nTable 2: Certification results for pairwise cost matrices. ϵ = 0.5 and σ = 0.5 for both CIFAR-10 and\nImagenette. Acc stands for overall accuracy and Robc-s refers to certified cost-sensitive robustness,\nRobnon is certified robustness of non-sensitive samples. The best statistics are highlighted in bold.\n\nDataset\n\nType\n\nMethods\n\nAcc\n\nRobc-s Robnon\n\nSingle (3\n\n5)\n\n→\n\nCIFAR-10\n\nMulti (3\n\n2, 4, 5)\n\n→\n\nSingle (7\n\n2)\n\n→\n\nImagenette\n\nMulti (7\n\n2, 4, 6)\n\n→\n\nCohen\nMACER\nCohen-R\nOurs\nCohen\nMACER\nCohen-R\nOurs\nCohen\nMACER\nCohen-R\nOurs\nCohen\nMACER\nCohen-R\nOurs\n\n0.654\n0.647\n0.642\n0.673\n0.654\n0.647\n0.642\n0.643\n0.803\n0.782\n0.754\n0.792\n0.803\n0.781\n0.754\n0.796\n\n0.504\n0.543\n0.723\n0.924\n0.336\n0.385\n0.643\n0.822\n0.885\n0.899\n0.911\n0.938\n0.756\n0.780\n0.830\n0.863\n\n0.443\n0.533\n0.437\n0.472\n0.446\n0.533\n0.437\n0.474\n0.744\n0.749\n0.679\n0.731\n0.744\n0.749\n0.678\n0.730\n\nγ. MACER uses γ = 8 to enhance the overall robustness for all classes, whereas we set γ1 = 4 for\nnormal classes and γ2 = 16 for sensitive classes based on our analysis in Section 4.2. The effect of\ndifferent combinations of hyperparameters is discussed in Appendix E. In addition, we compare our\nmethod with an alternative convex relaxation-based approach (Zhang & Evans, 2019) in Appendix D.\n\n5.1 CERTIFICATION RESULTS FOR DIFFERENT COST MATRICES\n\n∈ S\n\nj\n{\n\n[m] : j\n\ns, Ωy =\nSeedwise Cost Matrix. For any (x, y)\n1, meaning\nthat any possible classwise adversarial transformation for (x, y) will incur a cost. Table 1 reports the\nperformance in terms of overall accuracy and certified cost-sensitive robustness of our method and\nthe three baselines with respect to different seedwise cost matrices. In particular, we consider two\ntypes of seedwise cost matrices in our experiments: (2) Single: a randomly-selected sensitive seed\nclass from all available classes, where we report the performance on the third class “cat” (label 3) for\nCIFAR-10 and “gas pump”(label 7) for Imagenette. (3) Multi: multiple sensitive seed classes, where\n“bird” (label 2) and “deer” (label 4) are considered as the sensitive seed classes for CIFAR-10, while\nwe choose “chain saw”(label 3) and “gas pump”(label 7) for Imagenette.\n\n= m\n\n= y\n\nΩy\n\nor\n\n−\n\n∈\n\n}\n\n|\n\n|\n\nWe observe in Table 1 that our cost-sensitive robust training method achieves a significant improve-\nment in terms of certified cost-sensitive robustness compared with baselines. In certain cost-sensitive\nscenarios, our method achieves comparable or even slightly better performance than the baselines in\nterms of overall accuracy. The certified robustness of our method for non-sensitive examples is not as\nhigh as MACER’s, indicating a desirable shift of robustness from normal to sensitive examples.\n\ns, Ωy\n\n∈\n\n∈ S\n\n[m] : j\n\n1,\nj\nPairwise Cost Matrix. For any (x, y)\n= y\n−\n⊆ {\nΩy may\nΩy is acceptable. Note that [m]\nmeaning that misclassification to any target class in [m]\n\\\n\\\ninclude target classes other than the ground-truth class y. Similar to the previous setting, we consider\ntwo types of pairwise cost matrices: (1) Single: a randomly-selected sensitive seed class with one\nsensitive target class. (2) Multi: a single sensitive seed class with multiple sensitive target classes.\nTable 2 compares the performance of our method with baselines for the aforementioned pairwise\ncost matrices on CIFAR-10 and Imagenette datasets. Similar to results in seedwise cost matrix, our\nmethods always yeilds the optimal cost-sensitive certified robustness without sacrificing too much\noverall accuracy. The superiority of our approach in preserving overall accuracy becomes even more\npronounced when contrasted with the reweighting method Cohen-R.\n\nΩy\n|\n\n< m\n\nwith\n\n}\n\n|\n\n8\n\n̸\n̸\n\fUnder review as a conference paper at ICLR 2024\n\n(a) Pariwise: (3 → 5)\n\n(b) Pairwise: (3 → 2, 4, 5)\n\n(c) Seedwise: (3 → all others)\n\nFigure 2: The curves of certified accuracy with respect to different training methods on the CIFAR-10\ndataset for varying ϵ under different cost-sensitive settings.\n\n5.2 MEDICAL DATASET\n\nTable 3: Comparisons of different\nmethods on HAM10k dataset.\n\nWe report the results of our method on a real-world medical\ndataset: HAM10k, which is an imbalanced skin cancer dataset\nwith 7 classes. The noise level here is set as σ = 0.5 and the\ncost-sensitive robustness Robc-s is measured for ϵ = 0.5 in\nℓ2-norm. Due to the small sample size for several classes, we\ngroup all images from the benign classes into a single category\nand all images from the malignant classes into another, which\nformulates a binary classification task. Since misclassifying\na malignant tumor as benign can have severe consequences\nand resulting in high costs for this medical application, we set\nany misclassiciation from malignant to benign as cost-sensitive,\nwhile regarding the cost of the other type of misclassifications as\n0. Table 3 demonstrates the overall accuracy and certified cost-\nsensitive robustness of the model produced by our cost-sensitive robust training method, compared\nwith other alternatives. Cohen-R-I and Cohen-R-II represent two specific instantiations of Cohen-R\nwith the weight parameter α selected as 1.1 and 1.2, respectively. As we increase the weight parameter\nα, there is a noticeable enhancement in cost-sensitive robustness. However, this comes at the expense\nof a marked decline in overall accuracy, confirming the non-optimal trade-off phenomenon discussed\nin Section 4. Notably, our approach achieves a more desirable trade-off between the two metrics,\nunderscoring its effectiveness for real-world applications.\n\nMethod\nCohen\nCohen-R-I\nCohen-R-II\nMACER\nOurs\n\nRobc-s\n0.118\n0.197\n0.437\n0.250\n0.413\n\nAcc\n0.829\n0.805\n0.784\n0.828\n0.831\n\n5.3 CERTIFICATION RESULTS UNDER VARYING ϵ\n\nTo further visualize the consistency of our improvement, we compare the certified accuracy curves of\ncost-sensitive examples with varying l2 perturbation for the aforementioned methods in Figure 2. It\nis evident that our method consistently outperforms the baseline in terms of certified cost-sensitive\naccuracy across different ϵ. In particular, we observe a significant improvement at ϵ = 0.5 of our\nmethod compared with other baselines, which again confirms our findings in Section 5.1.\n\n6 CONCLUSION\n\nWe developed a generic randomized smoothing framework to certify and train for cost-sensitive\nrobustness. At the core of our framework is a new notion of cost-sensitive certified radius, which is\napplicable to any binary cost matrix. Built upon fine-grained thresholding techniques for optimizing\nthe certified radius with respect to different subpopulations, our method significantly improves the\ncertified robustness performance for cost-sensitive transformations. Compared with naive reweighting\napproaches, our method achieves a much more desirable trade-off between overall accuracy and\ncertified cost-sensitive robustness. Experiments on image benchmarks demonstrate the superior\nperformance of our approach compared to various baselines. Our work opens up new possibilities for\nbuilding certified robust models based on randomized smoothing for cost-sensitive applications.\n\n9\n\n0.00.51.01.52.0(cid:15)0.00.20.40.60.81.0certiﬁedaccuracyOursCohen-RMACERCohen0.00.51.01.52.0(cid:15)0.00.20.40.60.81.0certiﬁedaccuracyOursCohen-RMACERCohen0.00.51.01.52.0(cid:15)0.00.20.40.60.81.0certiﬁedaccuracyOursCohen-RMACERCohen\fUnder review as a conference paper at ICLR 2024",
    "abstract": "We focus on learning adversarially robust classifiers under a cost-sensitive scenario, where the potential harm of different classwise adversarial transformations is encoded in a binary cost matrix. Existing methods are either empirical that cannot certify robustness or suffer from inherent scalability issues. In this work, we study whether randomized smoothing, a scalable certification framework, can be leveraged to certify cost-sensitive robustness. Built upon a notion of cost-sensitive certified radius, we show how to adapt the standard randomized smoothing certification pipeline to produce tight robustness guarantees for any given cost matrix. In addition, with fine-grained certified radius optimization schemes designed for different data subgroups, we propose an algorithm to train smoothed classifiers that are optimized for cost-sensitive robustness. Extensive experiments on image benchmarks and a real-world medical dataset demonstrate the superiority of our method in achieving significantly improved performance of certified cost-sensitive robustness while having a negligible impact on overall accuracy.",
    "response": {
      "Soundness": 3,
      "Presentation": 3,
      "Contribution": 1,
      "Rating": 3,
      "Confidence": 4,
      "Strengths": "The paper studies provable robustness in a setting that has largely remained unexplored. The paper is well-written and easy to follow.",
      "Weaknesses": "1. Binary Cost Matrix: A 0/1 cost matrix may not be sufficient to capture real-world cost-sensitive ML tasks. The cost of misclassification would rarely be zero. Take the example mentioned in the second paragraph of the introduction. While misclassifying a benign tumor as malignant is less detrimental than the reverse, the cost of such a misclassification will not be zero. If so, one could simply label all tumors as malignant and achieve an overall classification cost of zero. However, such a classifier would not provide us with any valuable information.\n\n   It would be more impactful to design robustness certificates for a general cost matrix where the entries need not be 0/1. It might be possible to certify the expected misclassification cost by using the distribution of the cost under the smoothing noise. The following work on certifying the expected confidence of a neural network could be adapted for a general cost matrix:\n\n   Certifying Confidence via Randomized Smoothing, Kumar et al., NeurIPS 2020.\n\n2. Novelty: The robustness certificate designed in this work is a straightforward adaptation of Cohen et al.'s certificate [1]. The certificate in [1] takes the difference between two terms where the second term depends on p_b, the probability of the second most likely class. This paper redefines p_b as the probability of the most likely class in the set of class labels with cost 1.\n\n   [1] Certified Adversarial Robustness via Randomized Smoothing, Cohen et al., ICML 2019.\n\n3. Sample Complexity: The number of samples required for computing the proposed certificate is higher than that of the baseline certificate from [1]. It depends on the number of classes with cost 1. While this might be manageable for the small number of classes considered in the experiments (<= 10), scaling to a large number of classes, such as in ImageNet (1000 classes), would be difficult.",
      "Questions": "1. What would be a practical application where a binary cost matrix would be sufficient? In most scenarios, the misclassification cost would take a range of different values.\n\n2. During inference, when the ground truth is unknown, how do we find the classes with cost 1? This set depends on the correct class label, which is not known during inference."
    }
  },
  {
    "paper_id": "DO2WFXU1Be",
    "summary": "Background: Physics-Informed Neural Networks (PINNs) have gained traction in numerically solving partial differential equations (PDEs) due to their ability to embed physical laws within the neural network framework. Traditional numerical methods, like the finite element or pseudo-spectral methods, are computationally costly, especially in high dimensions. PINNs, using multilayer perceptrons (MLP), have achieved success in various applications but struggle with high-frequency or multiscale features and typically neglect temporal dependencies.\nGap: Conventional PINNs rely on MLPs which often fail to model temporal dependencies in physical systems properly. This shortcoming results in inaccurate global solutions, as PINNs cannot effectively propagate initial condition constraints and accurately capture dynamic PDE solutions. Current approaches to improve PINNs, such as data interpolation and training scheme variations, often involve high computational costs and still fail to handle specific PINN failure modes.\nHypothesis: The authors propose that a Transformer-based approach, called PINNsFormer, which utilizes multi-head attention mechanisms, can better capture temporal dependencies in PDE solutions. By transforming point-wise inputs into pseudo sequences and employing a sequential loss, this framework aims to improve the generalization and accuracy of solutions obtained by PINNs.\nMethod: The methodology involves designing a new framework, PINNsFormer, which applies a Transformer architecture to PINNs. Key components include a Pseudo Sequence Generator for transforming inputs into time series, a Spatio-Temporal Mixer for data embedding, and an Encoder-Decoder architecture to model temporal dependencies using self-attention. Additionally, a novel activation function, Wavelet, is introduced to anticipate Fourier decomposition, enhancing the approximating power of the network. The model is trained using a modified loss function, tailored for sequential data, which allows PINNsFormer to handle boundary and initial conditions appropriately.\nConclusion: Empirical evaluations demonstrate that PINNsFormer outperforms traditional PINNs and their variants in generalizing solutions to complex PDEs, mitigating known failure modes, and handling high-dimensional problems. The framework's flexibility also allows it to integrate with other learning schemes, further boosting performance. The findings suggest that focusing on capturing temporal dependencies significantly improves the accuracy and robustness of PDE solutions obtained using PINNs.\n",
    "full text": "\nPINNSFORMER: A TRANSFORMER-BASED FRAME-\nWORK FOR PHYSICS-INFORMED NEURAL NETWORKS\n\nZhiyuan Zhao\nGeorgia Institute of Technology\nAtlanta, GA 30332\nleozhao1997@gatech.edu\n\nB. Aditya Prakash\nGeorgia Institute of Technology\nAtlanta, GA 30332\nbadityap@cc.gatech.edu\n\nXueying Ding\nCarnegie Mellon University\nPittsburgh, PA 15213\nxding2@andrew.cmu.edu\n\nABSTRACT\n\nPhysics-Informed Neural Networks (PINNs) have emerged as a promising deep\nlearning framework for approximating numerical solutions to partial differential\nequations (PDEs). However, conventional PINNs, relying on multilayer per-\nceptrons (MLP), neglect the crucial temporal dependencies inherent in practical\nphysics systems and thus fail to propagate the initial condition constraints globally\nand accurately capture the true solutions under various scenarios. In this paper,\nwe introduce a novel Transformer-based framework, termed PINNsFormer, de-\nsigned to address this limitation. PINNsFormer can accurately approximate PDE\nsolutions by utilizing multi-head attention mechanisms to capture temporal de-\npendencies. PINNsFormer transforms point-wise inputs into pseudo sequences\nand replaces point-wise PINNs loss with a sequential loss. Additionally, it in-\ncorporates a novel activation function, Wavelet, which anticipates Fourier de-\ncomposition through deep neural networks. Empirical results demonstrate that\nPINNsFormer achieves superior generalization ability and accuracy across vari-\nous scenarios, including PINNs failure modes and high-dimensional PDEs. More-\nover, PINNsFormer offers flexibility in integrating existing learning schemes for\nPINNs, further enhancing its performance.\n\n1\n\nINTRODUCTION\n\nNumerically solving partial differential equations (PDEs) has been widely studied in science and\nengineering. The conventional approaches, such as finite element method (Bathe, 2007) or pseudo-\nspectral method (Fornberg, 1998), suffer from high computational costs in constructing meshes\nfor high-dimensional PDEs. With the development of scientific machine learning, Physics-informed\nneural networks (PINNs) (Lagaris et al., 1998; Raissi et al., 2019) have emerged as a promising novel\napproach. Conventional PINNs and most variants employ multilayer perceptrons (MLP) as end-to-\nend frameworks for point-wise predictions, achieving remarkable success in various scenarios.\n\nNevertheless, recent works have shown that PINNs fail in scenarios when solutions exhibit high-\nfrequency or multiscale features (Raissi, 2018; Fuks & Tchelepi, 2020; Krishnapriyan et al., 2021;\nWang et al., 2022a), though the corresponding analytical solutions are simple. In such cases, PINNs\ntend to provide overly smooth or naive approximations, deviating from the true solution.\n\nExisting approaches to mitigate these failures typically involve two general strategies. The first\nstrategy, known as data interpolation (Raissi et al., 2017; Zhu et al., 2019; Chen et al., 2021), em-\nploys data regularization observed from simulations, or real-world scenarios. These approaches face\nchallenges in acquiring ground truth data. The second strategy employs different training schemes\n(Mao et al., 2020; Krishnapriyan et al., 2021; Wang et al., 2021; 2022a), which potentially impose a\nhigh computational cost in practice. For instance, Seq2Seq by Krishnapriyan et al. (2021) requires\ntraining multiple neural networks sequentially, while other networks suffer from convergence issues\n\n1\n\n\fPublished as a conference paper at ICLR 2024\n\ndue to error accumulation. Another method, Neural Tangent Kernel (NTK) (Wang et al., 2022a), in-\nvolves constructing kernels K ∈ RD×P , where D is the sample size and P is the model parameter,\nwhich suffers from scalability issues as the sample size or model parameter increases.\n\nWhile most efforts to improve the generalization ability and address failure modes in PINNs have\nfocused on the aforementioned aspects, conventional PINNs, largely relying on MLP-based architec-\nture, can overlook important temporal dependencies in real-world physical systems. Finite Element\nMethods, for instance, implicitly incorporate temporal dependencies by sequentially propagating the\nglobal solution. This propagation relies on the principle that the state at time t + ∆t depends on\nthe state at time t. In contrast, PINNs, being a point-to-point framework, do not explicitly model\ntemporal dependencies within PDEs. Neglecting temporal dependencies poses challenges in glob-\nally propagating initial condition constraints in PINNs. Consequently, PINNs often exhibit failure\nmodes where the approximations remain accurate near the initial condition but subsequently fail into\noverly smooth or naive approximations.\n\nTo address this issue of neglecting temporal dependencies in PINNs, a natural idea is employing\nTransformer-based models, which are known for capturing long-term dependencies in sequential\ndata through multi-head self-attentions and encoder-decoder attentions (Vaswani et al., 2017). Vari-\nants of transformer-based models have shown substantial success across various domains. However,\nadapting the Transformer, which is inherently designed for sequential data, to the point-to-point\nframework of PINNs presents non-trivial challenges. These challenges span both the data represen-\ntation and the regularization loss within the framework.\n\nMain Contributions. In this work, we introduce PINNsFormer, a novel sequence-to-sequence PDE\nsolver built on the Transformer architecture. To the best of our knowledge, PINNsFormer is the first\nframework in the realm of PINNs that explicitly focuses on and learns temporal dependencies within\nPDEs. Our key contributions can be summarized as follows:\n\n• New Framework: We propose a novel yet intuitive Transformer-based framework named\nPINNsFormer. This framework equips PINNs with the capability to capture temporal de-\npendencies through the generated pseudo sequences, thereby enhancing the generalization\nability and approximation accuracy in effectively solving PDEs.\n\n• Novel Activation: We introduce a new non-linear activation function Wavelet.\nWavelet is designed to anticipate the Fourier Transform for arbitrary target signals, mak-\ning it a universal approximator for infinite-width neural networks. Wavelet can also be\npotentially beneficial to various deep learning tasks across different model architectures.\n• Extensive Experiments: We conduct comprehensive evaluations of PINNsFormer for var-\nious scenarios. We demonstrate its advantages in optimization and approximation accuracy\nwhen addressing failure modes or solving high-dimensional PDEs. We show the flexibility\nand benefits of PINNsFormer in incorporating different learning schemes of PINNs.\n\n2 RELATED WORK\n\nPhysics-Informed Neural Networks (PINNs). Physics-Informed Neural Networks (PINNs) have\nemerged as a promising approach for tackling scientific and engineering problems. Raissi et al.\n(2019) introduced the framework that incorporates physical laws into the neural network training to\nsolve PDEs. This work has led to applications across diverse domains, including fluid dynamics,\nsolid mechanics, and quantum mechanics (Carleo et al., 2019; Yang et al., 2020). Researchers have\ninvestigated different learning schemes for PINNs (Mao et al., 2020; Wang et al., 2021; 2022a),\nwhich have yielded substantial improvements in convergence, generalization, and interpretability.\n\nFailure Modes of PINNs. Despite the promise exhibited by PINNs, recent works have indicated\ncertain failure modes inherent to PINNs, particularly when confronted with PDEs featuring high-\nfrequency or multiscale features (Fuks & Tchelepi, 2020; Raissi, 2018; McClenny & Braga-Neto,\n2020; Krishnapriyan et al., 2021; Zhao et al., 2022; Wang et al., 2022a). This challenge has prompted\ninvestigations from various perspectives, including designing various model architectures, learning\nschemes, or using data interpolations (Han et al., 2018; Lou et al., 2021; Wang et al., 2021; 2022a;b).\nA comprehensive understanding of PINNs’ limitations and the underlying failure modes is funda-\nmental for applications in addressing complicated physical problems.\n\n2\n\n\fPublished as a conference paper at ICLR 2024\n\nFigure 1: Architecture of proposed PINNsFormer. PINNsFormer generates a pseudo sequence based\non pointwise input features. It outputs the corresponding sequential approximated solution. The first\napproximation of the sequence is the desired solution ˆu(x, t).\n\nTransformer-Based Models. The Transformer model (Vaswani et al., 2017) has achieved signifi-\ncant attention due to its ability to capture long-term dependencies, leading to major achievements in\nnatural language processing tasks (Devlin et al., 2018; Radford et al., 2018). Transformers have also\nbeen extended to other domains, including computer vision, speech recognition, and time-series\nanalysis (Liu et al., 2021; Dosovitskiy et al., 2020; Gulati et al., 2020; Zhou et al., 2021). Re-\nsearchers have also developed techniques aimed at enhancing the efficiency of Transformers, such\nas sparse attention and model compression (Child et al., 2019; Sanh et al., 2019).\n\n3 METHODOLOGY\n\nPreliminaries: Let Ω be an open set in Rd, bounded by ∂Ω ∈ Rd−1. The PDEs with spatial input\nx and temporal input t generally fit the following abstraction:\n\nD[u(x, t)] = f (x, t), ∀x, t ∈ Ω\nB[u(x, t)] = g(x, t), ∀x, t ∈ ∂Ω\n\n(1)\n\nwhere u is the PDE’s solution, D is the differential operator that regularizes the behavior of the sys-\ntem, and B describes the boundary or initial conditions in general. Specifically, {x, t} ∈ Ω are resid-\nual points, and {x, t} ∈ ∂Ω are boundary/initial points. Let ˆu be neural network approximations,\nPINNs describe the framework where ˆu is empirically regularized by the following constraints:\n\nLPINNs = λr\n\nNr(cid:88)\n\ni=1\n\n∥D[ˆu(x, t)] − f (x, t)∥2 + λb\n\nNb(cid:88)\n\ni=1\n\n∥B[ˆu(x, t)] − g(x, t)∥2\n\n(2)\n\nwhere Nb, Nr refer to the residual and boundary/initial points separately, λr, λb are the regulariza-\ntion parameters that balance the emphasis of the loss terms. The neural network ˆu takes vectorized\n{x, t} as input and outputs the approximated solution. The goal is then to use machine learning\nmethodologies to train the neural network ˆu that minimizes the loss in Equation 2.\n\nMethodology Overview: While PINNs focus on point-to-point predictions, the exploration of tem-\nporal dependencies in real-world physics systems has been merely neglected. Conventional PINNs\nmethods employ a single pair of spatial information x and temporal information t to approximate\nthe numerical solution u(x, t), without accounting for temporal dependencies across previous or\nsubsequent time steps. However, this simplification is only applicable to elliptic PDEs, where the\nrelationships between unknown functions and their derivatives do not explicitly involve time. In\ncontrast, hyperbolic and parabolic PDEs incorporate time derivatives, implying that the state at one\ntime step can influence states at preceding or subsequent time steps. Consequently, considering\ntemporal dependencies is crucial to effectively address these PDEs using PINNs.\n\nIn this section, we introduce a novel framework featuring a Transformer-based model of PINNs,\nnamely PINNsFormer. Unlike point-to-point predictions, PINNsFormer extends PINNs’ capabil-\nities to sequential predictions. PINNsFormer allows accurately approximating solutions at spe-\ncific time steps while also learning and regularizing temporal dependencies among incoming states.\nThe framework consists of four components: Pseudo Sequence Generator, Spatio-Temporal Mixer,\nEncoder-Decoder with multi-head attention, and an Output Layer. Additionally, we introduce a\nnovel activation function, named Wavelet, which employs Real Fourier Transform techniques to\nanticipate solutions to PDEs. The framework diagram is exhibited in Figure 1. We provide detailed\nexplanations of each framework component and learning schemes in the following subsections.\n\n3\n\n\fPublished as a conference paper at ICLR 2024\n\n3.1 PSEUDO SEQUENCE GENERATOR\n\nWhile Transformers and Transformer-based models are designed to capture long-term dependencies\nin sequential data, conventional PINNs utilize non-sequential data as inputs for neural networks.\nConsequently, to incorporate PINNs with Transformer-based models, it is essential to transform the\npointwise spatiotemporal inputs into temporal sequences. Thus, for a given spatial input x ∈ Rd−1\nand temporal input t ∈ R, the Pseudo Sequence Generator performs the following operations:\n\n[x, t]\n\nGenerator\n=======⇒ {[x, t], [x, t + ∆t], . . . , [x, t + (k − 1)∆t]}\n\n(3)\n\nwhere [·] is the concatenation operation, such that [x, t] ∈ Rd is vectorized, and the generator outputs\nthe pseudo sequence in the shape of Rk×d. The Pseudo Sequence Generator extrapolates sequential\ntime series by extending a single spatiotemporal input to multiple isometric discrete time steps.\nk and ∆t are hyperparameters, which intuitively determine how many steps the pseudo sequence\nneeds to ‘look ahead’ and how ‘far’ each step should be. In practice, both k and ∆t should not be\nset to very large scales, as larger k can cause heavy computational and memory overheads, while\nlarger ∆t may undermine the time dependency relationships of neighboring discrete time steps.\n\n3.2 MODEL ARCHITECTURE\n\nIn addition to the Pseudo Sequence Generator, PINNsFormer consists of three components of its ar-\nchitecture: Sptio-Temporal Mixer, Encoder-Decoder with multi-head attentions, and Output Layer.\nThe Output Layer is straightforward to interpret as a fully-connected MLP appended to the end. We\nprovide detailed insights into the first two components below. Notably, PINNsFormer relies only\non linear layers and non-linear activations, avoiding complex operations such as convolutional or\nrecurrent layers. This design preserves PINNsFormer’s computational efficiency in practice.\n\nSpatio-Temporal Mixer. Most PDEs contain low-dimensional spatial or temporal information.\nDirectly feeding low-dimensional data to encoders may fail to capture the complex relationships\nbetween each feature dimension. Hence, it is necessary to embed original sequential data in higher-\ndimensional spaces such that more information is encoded into each vector.\n\nInstead of embedding raw data in a high-dimensional space where the distance between vectors re-\nflects the semantic similarity (Vaswani et al., 2017; Devlin et al., 2018), PINNsFormer constructs\na linear projection that maps spatiotemporal inputs onto a higher-dimensional space using a fully-\nconnected MLP. The embedded data enriches the capability of information by mixing all raw spa-\ntiotemporal features together, so-called the linear projection Spatio-Temporal Mixer.\n\nEncoder-Decoder Architecture. PINNsFormer employs\nan encoder-decoder architecture similar to Transformer.\nThe encoder consists of a stack of identical layers, each\nof which contains an encoder self-attention layer and a\nfeedforward layer. The decoder is slightly different from\nthe vanilla Transformer, where each of the identical layers\ncontains only an encoder-decoder self-attention layer and\na feedforward layer. At the decoder level, PINNsFormer\nuses the same spatiotemporal embeddings as the encoder.\nTherefore, the decoder does not need to relearn depen-\ndencies for the same input embeddings. The diagram for\nthe encoder-decoder architecture is shown in Figure 2\n\nIntuitively, the encoder self-attentions allow learning the\ndependency relationships of all spatiotemporal informa-\ntion. The decoder encoder-decoder attentions allow se-\nlectively focusing on specific dependencies within the in-\nput sequence during the decoding process, enabling it to\ncapture more information than conventional PINNs. We use the same embeddings for the encoder\nand decoder since PINNs focus on approximating the solution of the current state, in contrast to next\nstate prediction in language tasks or time series forecastings.\n\nFigure 2: The architecture of PINNs-\nFormer’s Encoder-Decoder Layers. The\nis not equipped with self-\ndecoder\nattentions.\n\n4\n\n\fPublished as a conference paper at ICLR 2024\n\n3.3 WAVELET ACTIVATION\n\nWhile Transformers typically employ LayerNorm and ReLU non-linear activation func-\ntions (Vaswani et al., 2017; Gehring et al., 2017; Devlin et al., 2018), these activation functions\nmight not always be suitable in solving PINNs. In particular, employing ReLU activation in PINNs\ncan result in poor performance, whose effectiveness relies heavily on the accurate evaluation of\nderivatives while ReLU has a discontinuous derivative (Haghighat et al., 2021; de Wolff et al.,\n2021). Recent studies utilize Sin activation for specific scenarios to mimic the periodic proper-\nties of PDEs’ solutions (Li et al., 2020; Jagtap et al., 2020; Song et al., 2022). However, it requires\nstrong prior knowledge of the solution’s behavior and is limited in its applicability. Tackling this\nissue, we proposed a novel and simple activation function, namely Wavelet, defined as follows:\n\nWavelet(x) = ω1 sin(x) + ω2 cos(x)\nWhere ω1 and ω2 are registered learnable parameters. The intuition behind Wavelet activation\nsimply follows Real Fourier Transform: While periodic signals can be decomposed into an integral\nof sines of multiple frequencies, all signals, whether periodic or aperiodic, can be decomposed into\nan integral of sines and cosines of varying frequencies. It is evident that Wavelet can approximate\narbitrary functions giving sufficient approximation power, which leads to the following proposition:\n\n(4)\n\nProposition 1 Let N be a two-hidden-layer neural network with infinite width, equipped with\nWavelet activation function, then N is a universal approximator for any real-valued target f.\n\nProof sketch: The proof follows the Real Fourier Transform (Fourier Integral Transform). For any\ngiven input x and its corresponding real-valued target f (x), it has the Fourier Integral:\n\nf (x) =\n\n(cid:90) ∞\n\n−∞\n\nFc(ω) cos(ωx) dω +\n\n(cid:90) ∞\n\n−∞\n\nFs(ω) sin(ωx) dω\n\nwhere Fc and Fs are the coefficients of Sines and Cosines respectively. Second, by Riemann sum\napproximation, the integral can be approximated by the infinite sum such that:\n\nf (x) ≈\n\nN\n(cid:88)\n\nn=1\n\n[Fc(ωn) cos(ωnx) + Fs(ωn) sin(ωnx)] ≡ W2(Wavelet(W1x))\n\nwhere W1 and W2 are the weights of N ’s first and second hidden layer. As W1 and W2 are infinite-\nwidth, we can divide the piecewise summation into infinitely small intervals, making the approxi-\nmation arbitrarily close to the true integral. Hence, N is a universal approximator for any given f .\nIn practice, most PDE solutions contain only a finite number of major frequencies. Using a neural\nnetwork with finite parameters would also lead to proper approximations of the true solutions.\n\nAlthough Wavelet activation function is primarily employed by PINNsFormer to improve PINNs\nin our work, it may have potential applications in other deep-learning tasks. Similar to ReLU, σ(·),\nand Tanh activations, which all turn infinite-width two-hidden-layer neural networks into universal\napproximators (Cybenko, 1989; Hornik, 1991; Glorot et al., 2011), we anticipate that Wavelet can\ndemonstrate its effectiveness in other applications beyond the scope of this work.\n\n3.4 LEARNING SCHEME\n\nWhile conventional PINNs focus on point-to-point predictions, adapting PINNs to handle pseudo-\nsequential inputs has not been explored. In PINNsFormer, each generated point in the sequence, i.e.,\n[xi, ti+j∆t], is mapped to the corresponding approximation, i.e., ˆu(xi, ti+j∆t) for any j ∈ N, j <\nk. This approach allows us to compute the nth-order gradients with respect to x or t independently\nfor any valid n. For instance, for any given input pseudo sequence {[xi, ti], [xi, ti+∆t], . . . , [xi, ti+\n(k − 1)∆t]}, and the corresponding approximations {ˆu(xi, ti), ˆu(xi, ti + ∆t), . . . , ˆu(xi, ti + (k −\n1)∆t)}, we can compute the first-order derivatives w.r.t. x and t separately as follows:\n\n∂{ˆu(xi, ti + j∆t)}k−1\nj=0\n∂{ti + j∆t}k−1\nj=0\n∂{ˆu(xi, ti + j∆t)}k−1\nj=0\n∂xi\n\n= {\n\n∂ ˆu(xi, ti)\n∂ti\n\n= {\n\n∂ ˆu(xi, ti)\n∂xi\n\n,\n\n,\n\n∂ ˆu(xi, ti + ∆t)\n∂(ti + ∆t)\n\n, . . . ,\n\n∂ ˆu(xi, ti + (k − 1)∆t)\n∂(ti + (k − 1)∆t)\n\n}\n\n∂ ˆu(xi, ti + ∆t)\n∂xi\n\n, . . . ,\n\n∂ ˆu(xi, ti + (k − 1)∆t)\n∂xi\n\n}\n\n(5)\n\n5\n\n\fPublished as a conference paper at ICLR 2024\n\nThis scheme for calculating the gradients of sequential approximations with respect to sequential\ninputs can be easily extended to higher-order derivatives and is applicable to residual, boundary,\nand initial points. However, unlike the general PINNs optimization objective in Equation 2, which\ncombines initial and boundary condition objectives, PINNsFormer distinguishes between the two\nand applies different regularization schemes to initial and boundary conditions through its learning\nscheme. For residual and boundary points, all sequential outputs can be regularized using the PINNs\nloss. This is because all generated pseudo-timesteps are within the same domain as their original\ninputs. For example, if [xi, ti] is sampled from the boundary, then [xi, ti + j∆t] also lies on the\nboundary for any j ∈ N+. In contrast, for initial points, only the t = 0 condition is regularized,\ncorresponding to the first element of the sequential outputs. This is because only the first element of\nthe pseudo-sequence exactly matches the initial condition at t = 0. All other generated time steps\nhave t = j∆t for any j ∈ N+, which fall outside the initial conditions.\n\nBy these considerations, we adapt the PINNs loss to the sequential version, as described below:\n\nLres =\n\nLbc =\n\n1\nkNres\n\n1\nkNbc\n\nNres(cid:88)\n\nk−1\n(cid:88)\n\ni=1\n\nj=0\n\nNbc(cid:88)\n\nk−1\n(cid:88)\n\ni=1\n\nj=0\n\n∥D[ˆu(xi, ti + j∆t)] − f (xi, ti + j∆t)∥2\n\n∥B[ˆu(xi, ti + j∆t)] − g(xi, ti + j∆t)∥2\n\n(6)\n\nLic =\n\n1\nNic\n\nNbc(cid:88)\n\ni=1\n\n∥I[ˆu(xi, 0)] − h(xi, 0)∥2\n\nLPINNsFormer = λresLres + λicLic + λbcLbc\n\nwhere Nres = Nr refers to the residual points as in Equation 2, Nbc, Nic represent the number of\nboundary and initial points, respectively, with Nbc + Nic = Nb. λres, λbc, and λic are regularization\nweights that balance the importance of the loss terms in PINNsFormer, similar to the PINNs loss.\n\nDuring training, PINNsFormer forwards all residual, boundary, and initial points to obtain their cor-\nresponding sequential approximations. It then optimizes the modified PINNs loss LPINNsFormer in\nEquation 6 using gradient-based optimization algorithms such as L-BFGS or Adam, updating the\nmodel parameters until convergence. In the testing phase, PINNsFormer forwards any arbitrary pair\n[x, t] to observe the sequential approximations, where the first element of the sequential approxima-\ntion corresponds exactly to the desired value of ˆu(x, t).\n\n3.5 LOSS LANDSCAPE ANALYSIS\n\nWhile achieving theoretical conver-\ngence or establishing generalization\nbounds for Transformer-based mod-\nels can be challenging, an alterna-\ntive approach to assess optimization\ntrajectory is through visualization of\nThis approach\nthe loss landscape.\nhas been employed in the analysis of\nboth Transformers and PINNs (Krish-\nnapriyan et al., 2021; Yao et al., 2020;\nPark & Kim, 2022). The loss landscape\nis constructed by perturbing the trained\nmodel along the directions of the first\ntwo dominant Hessian eigenvectors. This technique is more informative than random parameter\nperturbations. Generally, a smoother loss landscape with fewer local minima indicates an easier\nconvergence to the global minimum. We visualize the loss landscape for both PINNs and PINNs-\nFormer. The visualizations are presented in Figure 5.\n\nFigure 3: Visualization of the loss landscape for PINNs\n(left) and PINNsFormer (right) on a logarithmic scale.\nThe loss landscape of PINNsFormer is significantly\nsmoother than conventional PINNs.\n\nThe visualizations clearly reveal that PINNs exhibit a more complicated loss landscape than PINNs-\nFormer. To be specific, we estimate the Lipschitz constant for both loss landscapes. We find that\nLPINNs = 776.16, which is significantly larger than LPINNsFormer = 32.79. Furthermore, the loss\n\n6\n\n\fPublished as a conference paper at ICLR 2024\n\nlandscape of PINNs exhibits several sharp cones near its optimal point, indicating the presence of\nmultiple local minima in close proximity to the convergence point (zero perturbation). The rugged\nloss landscape and multiple local minima of conventional PINNs suggest that optimizing the objec-\ntive described in Equation 6 for PINNsFormer offers an easier path to reach the global minimum.\nThis implies that PINNsFormer has advantages in avoiding the failure modes associated with PINNs.\nThe analysis is further validated by empirical experiments, as shown in the following section.\n\n4 EXPERIMENTS\n\n4.1 SETUP\n\nGoal. Our empirical evaluations aim to demonstrate three key advantages of PINNsFormer. First,\nwe show that PINNsFormer improves generalization abilities and mitigates failure modes compared\nto PINNs and variant architectures. Second, we illustrate the flexibility of PINNsFormer in in-\ncorporating various learning schemes, resulting in superior performance. Third, we provide ev-\nidence of PINNsFormer’s faster convergence and improved generalization capabilities in solving\nhigh-dimensional PDEs, which can be challenging for PINNs and their variants.\n\nExperiment Setup. Our empirical evaluations rely on four types of PDEs: convection, 1D-reaction,\n1D-wave, and Navier–Stokes PDEs, which follow the established setups of preliminary studies for\nfair comparisons (Raissi et al., 2019; Krishnapriyan et al., 2021; Wang et al., 2022a). We include\nPINNs, QRes (Bu & Karpatne, 2021), and First-Layer Sine (FLS) (Wong et al., 2022) as baselines.\nFor convection, 1D-reaction, and 1D-wave PDEs, we uniformly sampled Nic = Nbc = 101 initial\nand boundary points, as well as a uniform grid of 101 × 101 mesh points for the residual domain,\nresulting in total Nres = 10201 points. In the case of training PINNsFormer, we reduce the collo-\ncation points, with Nic = Nbc = 51 initial and boundary points and a 51 × 51 mesh for residual\npoints. The reduction in fewer training samples serves two purposes: it enhances training efficiency\nand allows us to demonstrate the generalization capabilities of PINNsFormer with limited training\ndata. For testing, we employed a 101 × 101 mesh within the residual domain. For the Navier–Stokes\nPDE, we sample 2500 points from the 3D mesh within the residual domain for training purposes.\nThe evaluation was performed by testing the predicted pressure at the final time step t = 20.0.\n\nEvaluation. For all baselines and PINNsformer, we maintain approximately close numbers of pa-\nrameters across all models to highlight the advantages of PINNsFormer from its ability to capture\ntemporal dependencies rather than relying solely on model overparameterization. We train all mod-\nels using the L-BFGS optimizer with Strong Wolfe linear search for 1000 iterations. For simplicity,\nwe set λres = λic = λbc = 1 for the optimization objective in Equation 6. Detailed hyperparam-\neters are provided in Appendix A. We also include an ablation study on activation functions and a\nhyperparameter sensitivity study on the choice of {k, ∆t} in Appendix C.\n\nIn terms of evaluation metrics, we adopted commonly used metrics in related works (Krishnapriyan\net al., 2021; Raissi et al., 2019; McClenny & Braga-Neto, 2020), including the relative Mean Abso-\nlute Error (rMAE or relative ℓ1 error) and the relative Root Mean Square Error (rRMSE or relative\nℓ2 error). The detailed formulations of the metrics are provided in Appendix A.\n\nReproducibility. All models are implemented in PyTorch (Paszke et al., 2019), and are trained\nseparately on single NVIDIA Tesla V100 GPU. All code and demos are included and reproducible\nat: https://github.com/AdityaLab/pinnsformer.\n\n4.2 MITIGATING FAILURE MODES OF PINNS\n\nOur primary evaluation focuses on demonstrating the superior generalization ability of PINNs-\nFormer in comparison to PINNs, particularly on PDEs that are known to challenge PINNs’ gen-\neralization capabilities. We focus on solving two distinct types of PDEs: the convection equation\nand the 1D-reaction equation. These equations pose significant challenges for conventional MLP-\nbased PINNs, often resulting in what is referred to as ”PINNs failure modes” (Mojgani et al., 2022;\nDaw et al., 2022; Krishnapriyan et al., 2021). In these failure modes, optimization gets stuck in local\nminima, leading to overly smooth approximations that deviate from the true solutions.\n\n7\n\n\fPublished as a conference paper at ICLR 2024\n\nThe objective of our evaluation is to showcase the enhanced generalization capabilities of PINNs-\nFormer when compared to standard PINNs and their variations, specifically in addressing PINNs’\nfailure modes. The evaluation results are summarized in Table 1, with detailed PDE formulations\nprovided in Appendix B. We showcase the prediction and absolute error plots of PINNs and PINNs-\nFormer on convection equation in Figure 4, all prediction plots available in Appendix C.\n\nModel\n\nPINNs\nQRes\nFLS\nPINNsFormer\n\nConvection\n\n1D-Reaction\n\nLoss\n0.016\n0.015\n0.012\n3.7e-5\n\nrMAE rRMSE\n0.840\n0.778\n0.816\n0.746\n0.771\n0.674\n0.027\n0.023\n\nLoss\n0.199\n0.199\n0.199\n3.0e-6\n\nrMAE rRMSE\n0.981\n0.982\n0.977\n0.979\n0.985\n0.984\n0.030\n0.015\n\nTable 1: Results for solving convection and 1D-reaction equations. PINNsFormer consistently out-\nperforms all baseline methods in terms of training loss, rMAE, and rRMSE.\n\nThe evaluation results demonstrate significant outperformance of PINNsFormer over all baselines\nfor both scenarios. PINNsFormer achieves the lowest training loss and test errors, distinguish-\ning PINNsFormer as the only approach capable of mitigating the failure modes. In contrast, all\nother baseline methods remain stuck at global minima and fail to optimize the objective loss ef-\nfectively. These results show the clear advantages of PINNsFormer in terms of generalization\nability and approximation accuracy when compared to conventional PINNs and existing variants.\n\nThe additional concern for PINNsFormer is\nits computational and memory overheads rel-\native to PINNs. While MLP-based PINNs\nare known for efficiency, PINNsFormer, with\nTransformer-based architecture in handling se-\nquential data, naturally incurs higher compu-\ntational and memory costs. Nonetheless, our\nempirical evaluation indicates that the overhead\nis tolerable, benefitting from the reliance on\nonly linear layers, avoiding complicated oper-\nators such as convolution or recurrent layers.\nFor instance, when setting the pseudo-sequence\nlength k = 5, we observe an approximate 2.92x\ncomputational cost and a 2.15x memory usage\n(detailed in Appendix A). These overheads are\nreasonable in exchange for the substantial per-\nformance improvements by PINNsFormer.\n\nFigure 4: Prediction (left) and absolute error\n(right) of PINNs (up) and PINNsFormer (bottom)\non convection equation. PINNsFormer shows suc-\ncess in mitigating the failure mode than PINNs.\n\n4.3 FLEXIBILITY IN INCORPORATING VARIANT LEARNING SCHEMES\n\nModel\n\n1D-Wave\n\nPINNs\nPINNsFormer\nPINNs + NTK\n\nLoss\n1.93e-2\n1.38e-2\n6.34e-3\nPINNsFormer + NTK 4.21e-3\n\nWhile PINNs and their various archi-\ntectural adaptations may encounter chal-\nlenges for certain scenarios, prior re-\nsearch has explored sophisticated opti-\nmization schemes to mitigate these is-\nincluding learning rate anneal-\nsues,\ning (Wang et al., 2021), augmented La-\ngrangian methods (Lu et al., 2021), and\nTable 2: Results for solving the 1D-wave equation, in-\nneural tangent kernel approaches (Wang\ncorporating the NTK method. PINNsFormer combined\net al., 2022a). These modified PINNs\nwith NTK outperforms all other methods on all metrics.\nhave shown significant improvement of\nPINNs under certain scenarios. Notably, when these optimization strategies are applied to PINNs-\nFormer, they can be easily incorporated to achieve further performance improvements. For instance,\nthe Neural Tangent Kernel (NTK) method to PINNs has shown success in solving the 1D-wave\nequation. As such, we demonstrate that when combining NTK with PINNsFormer, we can achieve\n\nrMAE rRMSE\n0.335\n0.326\n0.283\n0.270\n0.149\n0.140\n0.058\n0.054\n\n8\n\n\fPublished as a conference paper at ICLR 2024\n\nfurther outperformance in approximation accuracy. Detailed results are presented in Table 2, and\ncomprehensive PDE formulations are available in Appendix B with prediction plots in Appendix C.\n\nOur evaluation results show both the flexibility and effectiveness of incorporating PINNsFormer\nwith the NTK method.\nIn particular, we observe a sequence of performance improvements,\nfrom standard PINNs to PINNsFormer and from PINNs+NTK to PINNsFormer+NTK. Essentially,\nPINNsFormer explores a variant architecture of PINNs, while many learning schemes are designed\nfrom an optimization perspective and are agnostic to neural network architectures. This inherent\nflexibility allows for versatile combinations of PINNsFormer with various learning schemes, offer-\ning practical and customizable solutions for accurate solutions in real-world applications.\n\n4.4 GENERALIZATION ON HIGH-DIMENSIONAL PDES\n\nIn the previous sections, we demonstrated the clear benefits of\nPINNsFormer in generalizing the solutions for PINNs failure\nmodes. However, those PDEs often have simple analytical so-\nlutions. In practical physics systems, higher-dimensional and\nmore complex PDEs need to be solved. Therefore, it’s impor-\ntant to evaluate the generalization ability of PINNsFormer on\nsuch high-dimensional PDEs, especially when PINNsFormer\nis equipped with advanced mechanisms like self-attention.\n\nFigure 5: Training loss vs. Itera-\ntions of PINNs and PINNsFormer\non the Navier-Stokes equation.\n\nWe evaluate the performance of PINNsFormer compared to\nPINNs on Navier-Stokes PDE based on the established setups Raissi et al. (2019). The training loss\nis shown in Figure 5, and the results are shown in Table 3. The detailed formulations of the 2D\nNavier-Stokes equation can be found in Appendix B, and the predictions are plotted in Appendix C.\n\nModel\n\nNavier-Stokes\n\nThe evaluation results demonstrate clear advan-\ntages of PINNsFormer over PINNs on high-\ndimensional PDEs. Firstly, PINNsFormer out-\nperforms PINNs and their MLP-variants in\nterms of both training loss and validation\nFirstly, PINNsFormer exhibits sig-\nerrors.\nnificantly faster convergence during training,\nwhich compensates for the higher computa-\ntional cost per iteration. Secondly, while PINNs\nand their MLP-variants predict the pressure\nwith good shapes, they exhibit increasing mag-\nnitude discrepancies as time increases. In contrast, PINNsFormer consistently aligns both the shape\nand magnitude of predicted pressures across various time intervals. This consistency is attributed to\nPINNsFormer’s ability to learn temporal dependencies through Transformer-based model architec-\nture and self-attention mechanism.\n\nTable 3: Results for solving Navier-Stokes equa-\ntion, PINNsFormer outperforms all baselines on\nall metrics.\n\nrMAE rRMSE\n13.08\n6.41\n3.98\n0.384\n\nPINNs\nQRes\nFLS\nPINNsFormer\n\nLoss\n6.72e-5\n2.24e-4\n9.54e-6\n6.66e-6\n\n9.08\n4.45\n2.77\n0.280\n\n5 CONCLUSION\n\nIn this paper, we introduced PINNsFormer, a novel Transformer-based framework of PINNs, aimed\nat capturing temporal dependencies when approximating solutions to PDEs. We introduced the\nPseudo Sequence Generator, a mechanism that translates vectorized inputs into pseudo time se-\nquences and incorporated a modified Encoder-Decoder layer along with a novel Wavelet ac-\ntivation. Empirical evaluations demonstrate that PINNsFormer consistently outperforms conven-\ntional PINNs across various scenarios, including handling PINNs’ failure modes, addressing high-\ndimensional PDEs, and integrating with different learning schemes for PINNs. Furthermore, PINNs-\nFormer retains computational simplicity, making it a practical choice for real-world applications.\n\nBeyond PINNsFormer, Wavelet activation function can hold promises for the broader machine\nlearning community. We provided a sketch proof demonstrating Wavelet’s ability to approxi-\nmate arbitrary target solutions using a two-hidden-layer infinite-width neural network, leveraging\nthe Fourier decomposition of these solutions. We encourage further exploration, both theoretically\nand empirically, of the Wavelet activation function’s potential. Its applicability extends beyond\nPINNs and can be leveraged in various architectures and applications.\n\n9\n\n\fPublished as a conference paper at ICLR 2024\n\nAcknowledgements: This paper was supported in part by the NSF (Expeditions CCF-1918770,\nCAREER IIS-2028586, Medium IIS-1955883, Medium IIS-2106961, PIPP CCF-2200269), CDC\nMInD program, Meta faculty gift, and funds/computing resources from Georgia Tech and GTRI.",
    "abstract": "Physics-Informed Neural Networks (PINNs) have emerged as a promising deep learning framework for approximating numerical solutions to partial differential equations (PDEs). However, conventional PINNs, relying on multilayer perceptrons (MLP), neglect the crucial temporal dependencies inherent in practical physics systems and thus fail to propagate the initial condition constraints globally and accurately capture the true solutions under various scenarios. In this paper, we introduce a novel Transformer-based framework, termed PINNsFormer, designed to address this limitation. PINNsFormer can accurately approximate PDE solutions by utilizing multi-head attention mechanisms to capture temporal dependencies. PINNsFormer transforms point-wise inputs into pseudo sequences and replaces point-wise PINNs loss with a sequential loss. Additionally, it incorporates a novel activation function, \\texttt{Wavelet}, which anticipates Fourier decomposition through deep neural networks. Empirical results demonstrate that PINNsFormer achieves superior generalization ability and accuracy across various scenarios, including PINNs failure modes and high-dimensional PDEs. Moreover, PINNsFormer offers flexibility in integrating existing learning schemes for PINNs, further enhancing its performance.",
    "response": {
      "Soundness": 4,
      "Presentation": 4,
      "Contribution": 4,
      "Rating": 8,
      "Confidence": 5,
      "Strengths": "Impressive results.  Clear representation.",
      "Weaknesses": "The system is using a time discretised set of function values to predict the next time step. This reminds me of the finite difference method, In this case the stencil is 100 elements long, so the optimum stencil would have a very high order in accuracy. \n\nThe reason why very large stencils are not used is that these bear a computational cost, and the same happens in using attention, although most of the computing is parallel.\n\nNow the manuscript does not provide a baseline using a normal, discrete PDE solver, of course,  on the par with the  computational load that the attention mechanism is requiring . Of course, having an analytical, although complicated, solution for a problem has its advantages compared to a set of discrete nodal values.\n\nI would like that the authors would address this in their submission for better rating.",
      "Questions": "See the weakness part."
    }
  },
  {
    "paper_id": "hH5HK4hsLY",
    "summary": "Background: Fingerprint recognition is a widely used biometric identification method owing to its uniqueness and permanence. It is essential in law enforcement, personal identification, and secure authentication. Traditional fingerprint recognition methods often use minutiae matching, which can be sensitive to noise and partial prints. More recently, convolutional neural networks (CNNs) have been employed to handle these challenges better, enhancing the accuracy and scalability of fingerprint recognition systems. Self-supervised learning has also emerged as a powerful approach for learning representations without requiring labeled data.\nGap: The main problem addressed in this paper is the challenge of deriving robust fingerprint representations that can improve verification performance, especially from degraded images, without relying heavily on enhanced sample data. Existing methods, particularly those based on minutiae matching, face limitations due to noise sensitivity and difficulties handling partial or distorted fingerprints.\nHypothesis: The authors propose that leveraging a U-Net-based fingerprint enhancement approach for pre-training can help obtain robust fingerprint representations in a self-supervised setting, which can significantly improve fingerprint verification accuracy, especially when dealing with degraded images.\nMethod: The proposed method involves a two-stage framework leveraging U-Net architecture. The first stage is self-supervised pre-training of a U-Net encoder on a fingerprint enhancement task, aiming to learn meaningful features from degraded fingerprints. The second stage involves adding a multi-layer perceptron (MLP) for fingerprint verification, using the encodings from the pre-trained encoder. This involves freezing the encoder's weights and training the MLP for fingerprint verification using labeled data. The approach is compared against other self-supervised methods using datasets like SFinGe, FVC, and NIST SD-302, with augmentation techniques like rotation and color jitter used during training.\nConclusion: The proposed enhancement-driven pre-training method outperformed existing self-supervised techniques in learning stable and effective fingerprint representations for verification tasks. The results suggest that it is possible to derive meaningful fingerprint representations from degraded images without relying on enhanced samples. However, the method showed better performance on synthetic datasets than on real-world data, indicating potential biases and the challenges of real-world application.\n",
    "full text": "\nENHANCEMENT-DRIVEN PRETRAINING FOR ROBUST\nFINGERPRINT REPRESENTATION LEARNING\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nFingerprint recognition stands as a pivotal component of biometric technology,\nwith diverse applications from identity verification to advanced search tools. In\nthis paper, we propose a unique method for deriving robust fingerprint represen-\ntations by leveraging enhancement-based pre-training. Building on the achieve-\nments of U-Net-based fingerprint enhancement, our method employs a special-\nized encoder to derive representations from fingerprint images in a self-supervised\nmanner. We further refine these representations, aiming to enhance the verifica-\ntion capabilities. Our experimental results, tested on publicly available finger-\nprint datasets, reveal a marked improvement in verification performance against\nestablished self-supervised training techniques. Our findings not only highlight\nthe effectiveness of our method but also pave the way for potential advancements.\nCrucially, our research indicates that it is feasible to extract meaningful fingerprint\nrepresentations from degraded images without relying on enhanced samples.\n\n1\n\nINTRODUCTION\n\nFingerprint recognition remains one of the most widely used biometric identification methods due to\nits uniqueness, permanence, and user-friendliness (Maltoni et al., 2022; Wayman et al., 2005; Allen\net al., 2005). As applications in law enforcement, personal identification, and secure authentication\ncontinue to surge, enhancing the precision and efficiency of fingerprint recognition systems has\ngrown more crucial (Allen et al., 2005).\n\nAlthough strides have been made in the field, certain challenges persist. These include handling\npartial or distorted fingerprints caused by the presence of noise or acquisition errors, addressing\nhigh interclass similarity, and managing the expansive dimensionality of the feature space (Maltoni\net al., 2022; Hong et al., 1998; Cappelli et al., 2007). In the midst of these challenges, many previ-\nous works demonstrating state-of-the-art results in the area of fingerprint matching and verification\nemploy minutia-based matching approaches (Ratha et al., 1996; Chang et al., 1997; Maltoni et al.,\n2022; Cappelli et al., 2010b;a; Jain et al., 2001; 1997). To match two fingerprint images or in other\nwords, to find the similarity between two fingerprint images, the process typically involves extract-\ning minutiae from the images and then these minutiae templates are matched to get a similarity\nscore. If this score surpasses a specific threshold, the two fingerprints are deemed to be identical\nelse they are considered distinct (Maltoni et al., 2009). While minutiae-based methods are tradi-\ntional in fingerprint recognition, their limitations, such as noise sensitivity, difficulty with partial\nprints, and susceptibility to non-linear distortions, are pronounced. These issues become evident\nin scenarios such as poor-quality images leading to the erroneous identification of minutiae (Hong\net al., 1998; Maltoni et al., 2009; Zaeri, 2011). In contrast, Convolutional Neural Networks (CNNs)\nhave emerged as effective tools, overcoming these limitations and improving the accuracy and de-\npendability of fingerprint recognition tasks. They efficiently handle partial prints and are tolerant\nto distortions and various finger conditions by learning local fingerprint structures and adapting to\ndiverse data, including unfavorable finger states like dryness or wetness (Nguyen et al., 2018; Desh-\npande et al., 2020; Darlow & Rosman, 2017; Tang et al., 2017; Engelsma et al., 2019). Moreover,\nCNNs streamline the recognition process by integrating feature extraction and matching, enhanc-\ning optimization. With the capability to learn fixed-length fingerprint embeddings, CNNs showcase\nscalability, offering efficient comparison and indexing even as the database size grows (Engelsma\net al., 2019). These attributes underscore CNNs as advanced, practical alternatives in fingerprint\nprocessing, circumventing the constraints of minutiae-based approaches.\n\n1\n\n\fUnder review as a conference paper at ICLR 2024\n\nRecently, the field of machine learning has witnessed a surge in the popularity of self-supervised\nlearning techniques (Jaiswal et al., 2020; Liu et al., 2021; Jing & Tian, 2020). In the context of\nfingerprint biometrics, self-supervised learning promises a solution to the problems inherent to data\nacquisition for supervised learning. It offers a pathway to learn meaningful representations from\nabundant unlabeled fingerprint data, bypassing the need for time-consuming and labor-intensive\nacquisition and labeling processes. Furthermore, self-supervised learning can potentially capture\nmore intricate, data-specific patterns, leading to richer, more robust fingerprint representations.\n\nIn this paper, we explore the potential of deep CNNs to learn robust fingerprint representations that\ncan achieve superior matching performance. Building on prior works that introduced U-Net-based\nfingerprint enhancement methods (Gavas & Namboodiri, 2023; Qian et al., 2019; Liu & Qian, 2020),\nwe propose a pretraining technique centered on fingerprint enhancement. The U-Net model, orig-\ninally designed for biomedical image segmentation (Ronneberger et al., 2015), has demonstrated\nefficacy in fingerprint enhancement due to its ability to extract contextual information from input\nfingerprints and generate enhanced prints that retain intricate structural details. Leveraging the en-\ncoder part of U-Net, especially the bottleneck layer, we aim to derive compact and discriminative\nfingerprint embeddings. We suggest that these refined embeddings could significantly enhance fin-\ngerprint search and verification tasks by augmenting class separability and introducing invariance to\nnoise and other distortions.\n\nOur study pursues two main objectives: First, we propose a pretraining technique with U-Net, op-\ntimizing a fingerprint enhancement task, and then use this pre-trained encoder to learn fingerprint\nrepresentations effectively. Second, we plan to assess the efficacy of these representations by com-\nparing their verification performance against existing self-supervised methods. In doing so, we ex-\nperiment with training and inference techniques to optimize the use of representations for fingerprint\nverification tasks.\n\nThis paper delves into a thorough exploration of these objectives, with the goal of providing insights\nto deepen our understanding of fingerprint recognition and inspire future progress in this direction.\n\n1.1 CONTRIBUTIONS\n\nHere are the main contributions of this work:\n\n1. We suggest a pre-training technique with the U-Net encoder on fingerprint enhancement\ntask and demonstrate the usefulness of this approach in representation learning in a self-\nsupervised setting.\n\n2. We describe a method to fine-tune the learned embeddings for fingerprint verification task.\n3. We evaluate our approach with various evaluation metrics demonstrating its effectiveness\nin fingerprint verification task and also provide a comparison with previous state-of-the-art\nself-supervised learning methods.\n\n1.2 RELATED WORK\n\nThe need for better fingerprint recognition tools has led to many developments in learning how to\nrepresent fingerprints effectively. Over time, various methods have been introduced, each improving\nthe accuracy and speed of identifying fingerprints in their own way. Many works have combined\ndifferent strategies focusing on domain knowledge for effective learning of fingerprint representa-\ntions (Engelsma et al., 2019; Tang et al., 2017). In this paper, we explore enhancement task for\npre-training our model for representation learning.\n\nImage Enhancement\n\nEarly work in fingerprint image enhancement employed techniques such as Gabor filters (Greenberg\net al., 2002; Hong et al., 1998; Kim et al., 2002; Yang et al., 2002; Liu et al., 2014), Fourier Trans-\nform (Sherlock et al., 1992; Chikkerur et al., 2005; Rahman et al., 2008), and ridge pattern analysis\nfor image enhancement and feature extraction. While these methods seemed promising, they were\noften challenged by poor image quality, noise, and variations in ridge patterns.\n\nDeep learning techniques, particularly Convolutional Neural Networks (CNNs), have increasingly\nbeen used in fingerprint recognition. CNNs, due to their ability to learn hierarchical representa-\n\n2\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 1: a) Architecture with verification objective i.e with binary classifier (at training and in-\nference) b) Architecture to compute similarity scores (at inference). The dotted arrows indicate\nnetworks having tied weights (siamese network structure).\n\ntions, have been effective in capturing minutiae and latent features in fingerprints, thereby improving\nrecognition accuracy (Nguyen et al., 2018; Deshpande et al., 2020; Tang et al., 2017).\n\nOriginally developed for biomedical image segmentation (Ronneberger et al., 2015), U-Net is an\nencoder-decoder network that excels in tasks that require contextual information from the input im-\nage. A significant development in the fingerprint enhancement domain was the adaptation of the\nU-Net architecture for fingerprint enhancement. Gavas & Namboodiri (2023) proposed multiple\nmodifications to U-Net architecture tailored for the fingerprint domain for the enhancement task\nincorporating minutia and orientation knowledge in the network. In their work, Qian et al. (2019)\nintroduced a fingerprint enhancement approach using a deep network called DenseUNET to im-\nprove image quality in a pixel-to-pixel and end-to-end manner. Liu & Qian (2020) used U-Net for\nsegmentation and enhancement of latent fingerprints.\n\nSelf-supervised Learning Techniques\n\nSelf-supervised learning has emerged as a compelling alternative to traditional supervised learning,\nparticularly in contexts marked by scarcity or complexity of labeled data (Jaiswal et al., 2020; Jing\n& Tian, 2020). This learning paradigm capitalizes on the vast availability of unlabeled data, using\ncleverly designed pretext tasks to derive useful feature representations. Generally, these techniques\nlearn by predicting certain aspects of the input data based on other parts, or by solving auxiliary\ntasks.\n\nContrastive learning, based on the principle of learning by comparison, stands as a cornerstone of\nself-supervised learning (Liu et al., 2021). It revolves around the idea of differentiating between\nsimilar (positive) and dissimilar (negative) instances. Various techniques that illustrate this concept\nhave achieved significant success. The SimCLR (Chen et al., 2020a) framework, for instance, pro-\nduces augmented versions of an image, treats them as distinct instances, and then trains the model\nto identify the original image pair among a set of negative samples. Another example is MoCo (Mo-\nmentum Contrast) (Chen et al., 2020b) which maintains a dynamic dictionary of data samples in a\nqueue and a moving-averaged encoder to tackle the challenge of large-scale instance discrimination.\nMore recent advancements include methods like BYOL (Bootstrap Your Own Latent) (Grill et al.,\n2020) which deviate from the traditional contrastive learning paradigm by not using negative sam-\nples and instead focusing on bringing representations of different views of the same image closer\nin the embedding space. Similarly, SwAV (Caron et al., 2021) uses a unique approach of swapping\ncluster assignments to maximize consistency between differently augmented views of the same im-\nage. Another notable technique is the Noise Contrastive Estimation (NCE) (Gutmann & Hyv¨arinen,\n2010), which contrasts a true data sample against noise samples and has been extensively used in\nthe field of natural language processing for word embedding learning. These techniques provide a\ncomprehensive insight into contrastive learning, a critical approach within self-supervised learning\nthat has potential applications in fingerprint biometrics.\n\n3\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 2: U-Net architecture for enhancement task for the pre-training stage in the self-supervised\nsetting. For representation learning, the decoder is discarded and the binary classifier is attached.\n\n2 METHODOLOGY\n\nThe methodology for our research is constructed around a two-stage framework to probe the po-\ntential of self-supervised learning in fingerprint representation learning. A broad overview of the\nprocess is as follows:\n\n• Stage 1: Self-Supervised Pre-training: This is the initial stage of our methodology, in\nwhich we perform pre-training of our models in a self-supervised manner.\nIt includes\nthe application of both existing self-supervised learning techniques as well as our novel\nenhancement-based approach for this task. The intent of this stage is to leverage the\npower of unlabeled data to learn meaningful representations that can serve as a starting\npoint for subsequent stages. Notably, for all methods, we keep the encoder architecture\nthe same. While other self-supervised methods traditionally use encoders like ResNet or\nVision Transformers, in our framework we use the encoder of our U-Net-based model to\nensure a fair comparison.\n\n• Stage 2: Probing Experiments: Upon completion of the pre-training phase, we progress\nto the second stage where a few linear layers (MLP) are added on top of the frozen pre-\ntrained encoder, making the representations 512-d. We then perform probing experiments\nusing this newly formed model. By keeping the encoder part frozen, we ensure that the\nmodel adapts the existing representations for the verification task without altering the\nlearned patterns from the self-supervised pre-training phase.\n\nFollowing this framework, we navigate through the process of adapting and implementing self-\nsupervised learning techniques, exploring a U-Net-based pre-training strategy, and conducting prob-\ning experiments with pre-trained networks. The sections below provide a detailed overview of the\nprocedures involved in each stage.\n\n2.1 U-NET-BASED PRETRAINING\n\nWhile the adaptation and application of existing self-supervised methods to fingerprint data provide\na valuable starting point, we believe that the uniqueness of fingerprint data could benefit from a self-\nsupervised learning method specifically tailored for it. Building upon our findings in U-Net-based\nenhancement works, our approach leverages the training of a fingerprint enhancement model as a\nform of self-supervision.\n\nWe explore the use of a U-Net-based fingerprint enhancement as a pre-training strategy. We hy-\npothesize that the U-Net encoder, trained on the task of fingerprint enhancement, might contain\nvaluable fingerprint representations. The process of enhancing a fingerprint image can serve as an\neffective self-supervised task, encouraging the model to learn useful, fingerprint-specific represen-\ntations. Specifically, the pre-trained encoder already encapsulates valuable information about the\nfingerprint, which can be used as a foundation for further representation learning. It’s crucial to\nnote that the quality of these initial representations heavily relies on the effectiveness of the U-Net-\n\n4\n\n\fUnder review as a conference paper at ICLR 2024\n\nbased enhancement model. Hence, the importance of the enhancement model’s design and training\ndiscussed here, cannot be overstated. For the enhancement-based pre-training stage, we utilize the\nbasic U-Net architecture in Figure 2 to optimize the fingerprint enhancement task. This is a simple\nimage-to-image network where the input of the network is a fingerprint image which is generally\ndegraded with various kinds of noises. The network tries to learn to predict an enhanced version\nof the fingerprint image by removing noise as much as possible such that the ridge structure of the\nfingerprint is maintained and restored. This ensures that the network learns the minute details of fin-\ngerprint structure and also tries to enhance it wherever possible which proves helpful in extracting\nrobust feature representations in later stages.\n\n2.2 LEARNING FINGERPRINT REPRESENTATION\n\nAfter the self-supervised pre-training, we conduct the probing experiments using the pre-trained\nnetworks. The aim of these experiments is to assess the usefulness of the learned representations\nfor the task of fingerprint verification. For this, we add 3-layer MLP projection head on top of the\nfrozen encoder part of the pre-trained network. We then train this model using a Sentence-BERT-\nlike (Reimers & Gurevych, 2019) siamese architecture, with a limited amount of labeled data for\nthe fingerprint verification task. We concatenate the fingerprint representations u and v of the image\npair with the element-wise difference |u − v| and then pass it through the linear layers and train\nit for binary-classification objective as illustrated in Figure 1. By keeping the encoder part frozen,\nthe model learns to adapt the existing representations for the verification task, without changing\nthe underlying learned patterns. This approach allows us to leverage a large amount of unlabeled\ndata to learn initial representations and a limited amount of labeled data for supervised adaptation.\nNote that in the supervised fine-tuning, allowing modifications in the encoder weights can lead to\nhigher performance on the end task, which is the future scope of this work. As our goal here is to\nexamine the robustness of the learned representations by different pre-training techniques, we keep\nthe encoder frozen. In summary, the combination of self-supervised pre-training with supervised\nfine-tuning offers a promising learning framework for fingerprint biometrics. Our methodology\naims to leverage the strengths of both self-supervised and supervised learning, offering a pathway\ntowards robust, efficient, and data-savvy fingerprint biometrics systems.\n\n3 EXPERIMENTS\n\nIn this section, we discuss the experiments performed to evaluate our proposed approach’s efficacy.\nWe cover the specifics of our experimental setup, including the datasets used, the training details,\nand the evaluation metrics employed.\n\n3.1 DATASETS AND PREPROCESSING\n\nThe datasets used in this study consist of both synthetic and real-world fingerprint images, originat-\ning from the Synthetic Fingerprint Generator (SFinGe) (Cappelli, 2004), the Fingerprint Verification\nCompetition (FVC) (Maio et al., 2002b;a; 2004), and the NIST SD-302 database (Fiumara et al.,\n2019).\n\nTable 1: Summary of Datasets\n\nDataset\nSFinGe\nFVC-2000\nFVC-2002\n\nSource\nSynthetic\nReal-world\nReal-world\nNIST SD-302 Real-world\nSynthetic\nReal-world\n\nSFinGe\nFVC-2004\n\nIdentities\n3,700\n440\n440\n2000\n1584\n440\n\nImages Purpose\n15000\n3520\n3520\n8000\n6336\n3520\n\nTrain\nTrain\nTrain\nTrain\nTest\nTest\n\nThe synthetic SFinGe dataset was crafted to reflect real-world challenges in fingerprint recogni-\ntion, such as various backgrounds, rotation degrees, and the presence of scratches and noise. The\nreal-world FVC datasets and NIST SD-302 database provide large-scale, realistic fingerprint data,\n\n5\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 3: Degraded-Enhanced pairs on FVC dataset from enhancement pre-training\n\nensuring the generalizability and robustness of the findings. This combination of synthetic and real-\nworld datasets allows the training and evaluation of the model under a diverse set of conditions. The\nsynthetic data offers scalability and control over experimental parameters, while the real-world data\nensures applicability in realistic conditions. Table 1 provides summary statistics of the datasets.\n\nFor the self-supervised pre-training phase, we use only the fingerprint images from the training\ndatasets, disregarding any associated ground truth labels or identities, in accordance with the princi-\nples of self-supervised learning. But we still require ground-truth for the enhancement task. For the\nSFinGe dataset, the corresponding clean images are already available for the degraded fingerprints\nwhich we use directly. For NIST SD 302 and FVC datasets, we generate enhanced fingerprints from\ndegraded fingerprints with a classical approach as described in work by Hong et al. (1998). In the\nnext stage of our approach, the training process shifts to a binary classification task for fingerprint\nverification. Positive and negative fingerprint pairs are generated taking into account their associated\nidentities. In this stage, we skip the NIST SD 302 as the impressions for fingerprints vary a lot due\nto sensor variations and acquisition conditions which worsens the training. As data augmentation\nis crucial for a self-supervised paradigm, we apply a random combination of transformations like\nrotation, color jitter, resize, crop, and Gaussian blur for the data augmentation.\n\n3.2\n\nIMPLEMENTATION DETAILS\n\nWe conduct all our experiments using the PyTorch (Paszke et al., 2017) deep learning framework\nwith Nvidia GeForce RTX 2080 Ti GPU for training.\n\nOur proposed enhancement-based pre-training technique utilizes the U-Net architecture as discussed\nin the previous section. The same U-Net encoder architecture is used for pre-training with other\nself-supervised learning methods for a fair comparison with our technique. During self-supervised\npre-training, all the models are trained from scratch. Our U-Net architecture has a depth of 5 layers,\nwherein each layer consists of 2 convolutions. It expects gray-scale fingerprint images with dimen-\nsions of 512 x 512 pixels as encoder input and decoder output. Therefore, all input images were\nresized and padded as required to match this input size. The encoder produces a 4096-dimensional\nvector bottleneck which is reduced to 512-d with MLP projection head. In addition, the model uses\ndepth-wise convolutions to reduce the overall number of parameters. For our approach, we use the\nL2 or MSE (Mean Squared Error) loss for the enhancement-based pre-training of U-Net model. For\nother self-supervised techniques, the losses are the same as described in their papers.\n\nFor the pre-training with the existing self-supervised methods, we performed a grid search to identify\nthe best hyperparameters, such as learning rate, batch size, learning rate schedule, and momentum\nvalue, and pre-trained the models for 50 epochs with early stopping applied to prevent overfitting.\n\nFor probing experiments, we adapt the MLP projection head weights for the verification task while\nkeeping the encoder weights fixed. We generate training and testing verification sets from each\ndataset in a 1:3 ratio of positive and negative pairs. After training, we evaluate the models on the test\nsets for the verification task. We report various metrics like verification accuracy, precision, recall,\nand F1-score to evaluate the performance of different techniques. We report the results in two ways:\n1) using the binary classifier over the MLP projection head (similar to the training setup). Refer to\nFigure 1-a and 2) only utilizing the representations and applying thresholds on the cosine similarity\n\n6\n\n\fUnder review as a conference paper at ICLR 2024\n\nRefer to Figure 1-b. The formulation of cosine similarity is mentioned in Appendix A.1. The first\nmethod simply evaluates the trained model as an end-to-end verification network. The second one\nshows us the potential to use the learned representations in the context of similarity search and in\nturn for recognition tasks.\n\n3.3 RESULTS\n\nThe models are first pre-trained to learn fingerprint representations using the enhancement-based\napproach and various self-supervised learning strategies. Because these representations are not ex-\nplicitly trained for fingerprint verification or identification, using them directly for evaluation is\ninappropriate. To gauge the stability and usefulness of these learned representations, we add lin-\near layers to the frozen pre-trained encoders and then train the models for fingerprint verification\ntasks. The encoders remain frozen, allowing only the weights of the MLP to adjust to the task,\nkeeping the original representations unchanged. This setup aids in comparing the efficacy of differ-\nent self-supervised learning techniques against our method. The results of our probing experiments\nare presented in Table 4 (Verification Accuracy), 5 (Precision) 6 (Recall) and 7 (F1-score). The\nverification accuracy and F1-score on the SFinGe test set are shown in Tables 2 and 3 respectively.\nPrecision and Recall results are shown in Appendix A.2. Figure 3 shows a few sample pairs of\ninput and predicted images from the pre-trained U-Net model on the enhancement task used in our\napproach.\n\nOur approach is compared with methods like SimCLR v2, SimSiam, MoCo v2, and BYOL on the\nSFinGe and FVC test sets for fingerprint verification. Verification accuracy serves as the evaluation\nmetric for each method. The test data for fingerprint verification consists of a 1:3 ratio of positive to\nnegative pairs, setting the random guess accuracy at 75%. Verification accuracy is measured in two\nways as described before. This is presented in the below tables under the ‘Classifier’ column. The\nsecond way is represented under the ‘Similarity’ column in the tables. Moreover, we also report the\nROC curves in Figure 4 for both datasets.\n\nAs seen from the results, our enhancement-based pre-training method consistently outperforms other\nself-supervised strategies across both test datasets. SimCLR v2 also consistently performs well.\nSimSiam and BYOL methods show comparatively poor performance. It is noteworthy that all mod-\nels perform better on the SFinGe test set than on the FVC test set. We believe this is due to two\nprimary factors: the training sets contain more data from SFinGe than FVC, potentially resulting\nin a bias towards the former, and SFinGe is a synthetic dataset while FVC consists of real finger-\nprints, making the latter more challenging. Hence, the performance of models on FVC datasets is\nthe real measure of the efficacy of models. Importantly, our method also provides superior perfor-\nmance when verification is based on the similarity of the representations, suggesting that the learned\nrepresentations are also useful for fingerprint recognition.\n\nTable 2: Verification accuracy on SFinGe test dataset with genuine and imposter pairs\n\nSFinGe - Accuracy\n\nMethod\n\nSimCLR\nSimSiam\nMoCo\nBYOL\nOurs\n\nClassification\nImposter Genuine Entire Data\n0.96\n0.881\n0.916\n0.362\n0.956\n0.881\n0.947\n0.825\n0.973\n0.886\n\n0.968\n0.972\n0.963\n0.96\n0.982\n\nSimilarity\nImposter Genuine Entire Data\n0.961\n0.749\n0.866\n0.648\n0.945\n0.845\n0.941\n0.718\n0.963\n0.847\n\n0.982\n0.888\n0.955\n0.963\n0.975\n\n4 LIMITATIONS AND FUTURE WORK\n\nDespite our promising findings, there exist certain limitations that need to be acknowledged. Pri-\nmarily, the current model is more effective on the SFinGe dataset, which is synthetic, than on the\nFVC dataset that encompasses real-world fingerprints. This discrepancy might be due to the under-\nrepresentation of FVC data in the training sets, leading to potential bias, and also due to inherent\ncomplexities in real-life fingerprint data, which are harder to handle. Another limitation is that our\n\n7\n\n\fUnder review as a conference paper at ICLR 2024\n\nTable 3: F1 score on SFinGe test dataset with genuine and imposter pairs\n\nSFinGe - F1 score\n\nMethod\n\nSimCLR\nSimSiam\nMoCo\nBYOL\nOurs\n\nClassification\nImposter Genuine Entire Data\n0.803\n0.442\n0.785\n0.742\n0.858\n\n0.98\n0.96\n0.98\n0.97\n0.99\n\n0.8\n0.44\n0.79\n0.74\n0.86\n\nSimilarity\nImposter Genuine Entire Data\n0.781\n0.78\n0.469\n0.47\n0.737\n0.74\n0.689\n0.69\n0.821\n0.81\n\n0.98\n0.92\n0.97\n0.97\n0.98\n\nTable 4: Verification accuracy on FVC test dataset with genuine and imposter pairs\n\nFVC - Accuracy\n\nMethod\n\nSimCLR\nSimSiam\nMoCo\nBYOL\nOurs\n\nClassification\nImposter Genuine Entire Data\n0.888\n0.619\n0.88\n0.122\n0.867\n0.522\n0.857\n0.568\n0.937\n0.73\n\n0.915\n0.956\n0.902\n0.886\n0.957\n\nSimilarity\nImposter Genuine Entire Data\n0.906\n0.537\n0.419\n0.733\n0.865\n0.56\n0.886\n0.477\n0.923\n0.818\n\n0.943\n0.387\n0.896\n0.926\n0.933\n\nstudy did not specifically train or evaluate the fingerprint recognition task. While our model has\nshown potential for this task, a specific and thorough evaluation is needed to fully understand its\nperformance in this regard. Furthermore, the efficacy of self-supervised learning methodologies is\ninherently reliant on the quality and diversity of the training data. In addition, we have utilized a\nspecific form of linear probing for our study. There exist alternative approaches, such as softmax or\nArcFace-based classification, which may help in learning better representations.\n\nIn terms of future work, it would be beneficial to address these limitations by including a broader,\nmore diverse range of real-world fingerprint datasets in the training phase. Moreover, instead of\nfreezing the encoder, training it with a smaller learning rate can be explored. This may enhance\nthe model’s generalizability and make it more robust against a variety of fingerprint data. Specific\ntraining and evaluation for the recognition task, as well as exploring alternative linear probing tech-\nniques, would also be valuable directions to pursue. Additionally, other forms of self-supervised\nlearning methods could be explored to continually optimize the model performance.\n\n5 CONCLUSION\n\nIn this paper, we explored the application of various self-supervised learning techniques for pre-\ntraining a model to learn good fingerprint representations which can be useful to recognize and ver-\nify fingerprints. We proposed a novel approach to use fingerprint enhancement as a self-supervised\npre-training method. We performed probing experiments that proved beneficial in evaluating the\neffectiveness of learned fingerprint representations across different pre-training strategies. The ver-\nification performance of our method was compared against SimCLR v2, SimSiam, MoCo v2, and\n\nTable 5: Precision on FVC test dataset with genuine and imposter pairs\n\nFVC - Precision\n\nMethod\n\nSimCLR\nSimSiam\nMoCo\nBYOL\nOurs\n\nClassification\nImposter Genuine Entire Data\n0.422\n0.42\n0.218\n0.22\n0.347\n0.35\n0.334\n0.33\n0.634\n0.63\n\n0.96\n0.92\n0.95\n0.95\n0.97\n\nSimilarity\nImposter Genuine Entire Data\n0.486\n0.49\n0.107\n0.11\n0.35\n0.35\n0.394\n0.48\n0.553\n0.55\n\n0.95\n0.94\n0.95\n0.93\n0.98\n\n8\n\n\fUnder review as a conference paper at ICLR 2024\n\nTable 6: Recall on FVC test dataset with genuine and imposter pairs\n\nFVC - Recall\n\nMethod\n\nSimCLR\nSimSiam\nMoCo\nBYOL\nOurs\n\nClassification\nImposter Genuine Entire Data\n0.619\n0.62\n0.122\n0.12\n0.522\n0.52\n0.568\n0.57\n0.73\n0.73\n\n0.92\n0.96\n0.9\n0.89\n0.96\n\nSimilarity\nImposter Genuine Entire Data\n0.537\n0.54\n0.733\n0.73\n0.56\n0.56\n0.477\n0.48\n0.818\n0.82\n\n0.94\n0.39\n0.9\n0.93\n0.93\n\nTable 7: F1 score on FVC test dataset with genuine and imposter pairs\n\nFVC - F1 score\n\nMethod\n\nSimCLR\nSimSiam\nMoCo\nBYOL\nOurs\n\nClassification\nImposter Genuine Entire Data\n0.502\n0.156\n0.417\n0.421\n0.679\n\n0.94\n0.94\n0.93\n0.92\n0.97\n\n0.5\n0.16\n0.42\n0.42\n0.68\n\nSimilarity\nImposter Genuine Entire Data\n0.51\n0.51\n0.186\n0.19\n0.431\n0.43\n0.432\n0.43\n0.659\n0.66\n\n0.95\n0.55\n0.92\n0.94\n0.96\n\nFigure 4: ROC curve based on similarity scores on SFinGe dataset(left) and FVC dataset (right)\n\nBYOL methods using two test sets, SFinGe and FVC. Our method consistently outperformed other\ntechniques across both test datasets, thereby demonstrating the robustness and effectiveness of our\nmodel. Our method also surpassed other methods when evaluated in terms of similarity-based ver-\nification indicating the effectiveness of the learned representations for fingerprint recognition task.\nHowever, it was observed that all models performed better on the synthetic SFinGe dataset compared\nto the real-world FVC dataset, indicating potential limitations related to bias in the training set and\ncomplexities of real-world fingerprint data. In the future, we intend to extend our research to encom-\npass more diverse and complex real-world fingerprint datasets, thus enhancing the generalizability\nof our model. We also plan to investigate other self-supervised learning methods and strategies to\nenhance the model’s performance and adaptability to real-world fingerprint data complexities. In\nconclusion, this chapter illuminates the potential of self-supervised learning methods in the domain\nof fingerprint biometrics, while highlighting areas for further exploration and improvement.",
    "abstract": "Fingerprint recognition stands as a pivotal component of biometric technology, with diverse applications from identity verification to advanced search tools. In this paper, we propose a unique method for deriving robust fingerprint representations by leveraging enhancement-based pre-training. Building on the achievements of U-Net-based fingerprint enhancement, our method employs a specialized encoder to derive representations from fingerprint images in a self-supervised manner. We further refine these representations, aiming to enhance the verification capabilities. Our experimental results, tested on publicly available fingerprint datasets, reveal a marked improvement in verification performance against established self-supervised training techniques. Our findings not only highlight the effectiveness of our method but also pave the way for potential advancements. Crucially, our research indicates that it is feasible to extract meaningful fingerprint representations from degraded images without relying on enhanced samples.",
    "response": {
      "Soundness": 3,
      "Presentation": 3,
      "Contribution": 2,
      "Rating": 3,
      "Confidence": 3,
      "Strengths": "The main strength of the paper is its set of experimental results. They indicate that the proposed approach provides better accuracies than other comparable approaches, such as SimCLR, SimSiam, MoCo and BYOL.\n\nAnother strength of the paper is that it is easy to understand and the quality of the language is good.",
      "Weaknesses": "This manuscript has following high-level weaknesses that need to be addressed in any revision.\n\n1. Fingerprint verification accuracy on FVC test dataset is worse than on SFinGe test dataset. While I agree with the authors' explanation that it \"might be due to underrepresentation of FVC data in the training sets, leading to potential bias, and also due to inherent complexities in real-life fingerprint data, which are harder to handle\", these challenges are faced by all learning-based fingerprint recognition approaches and solutions that don't perform well on real data will be less attractive.\n\n2. Perhaps this is a weakness for deep learning based fingerprint recognition systems, but given that there are many real-world fingerprint verification systems based on minutiae matching, there needs to be a stronger case made for how the proposed deep learning approach is expected to have an impact in the real world.\n\n3. It is not sufficiently explained what \"self supervision\" means in this paper. My understanding is that both degraded and enhanced fingerprints are used for training. Aren't the enhanced fingerprints akin to having ground truth and if so isn't that training supervised? Not enough details are provided to understand this correctly.\n\n4. The paper needs to provide more mathematical details of the proposed approach. For example, more details should be provided about the fingerprint enhancement procedure and results.\n\n5. It is not clear what attributes of fingerprints are being taken advantage of, in the proposed approach. Are fingerprint geometric structures or minutiae being used somehow in the training? Overall, this paper is lacking in technical details.",
      "Questions": "1. Please be more clear about what self-supervision refers to.\n\n2. Provide more details about the fingerprint enhancement process. How is the enhancement quality evaluated?\n\n3. In Fig. 3, I assume that degraded images are on top and enhanced ones on the bottom  --- if this is correct, please state that in that figure caption.  It appears that the fingerprint degradation is artificial and not based on real-world challenges. If that is correct, could it be the reason for the poorer performance on FVC datasets?\n\n4. I am a little confused about how the accuracy numbers on the entire dataset are computed. In Table 2, it is stated that the accuracy of the proposed method on the entire data is 0.973. But if the impostor accuracy is 0.982 and the genuine accuracy is 0.886 and the data split is 1 genuine pair for every 3 impostor pairs, shouldn't the overall accuracy be (3*0.982 + 1*0.886)/4 = 0.958, not 0.973?"
    }
  },
  {
    "paper_id": "jFox1iMWUa",
    "summary": "Background: Causal inference is essential in many fields such as medical decision-making, advertising evaluation, and voucher distribution, where it aims to estimate the causal effect of treatments on outcomes using observational data. Traditional approaches struggle with bias due to confounding factors and often only rank samples by uplift rather than precisely estimating treatment effects, especially when treatments are continuous.\nGap: Existing methods for causal inference often fail to adequately model the response performance while handling continuous treatments and mitigating confounding bias. Most previous work has focused on categorical or binary treatments and has not sufficiently addressed continuous treatments or provided unbiased uplift estimation in the presence of confounders.\nHypothesis: An end-to-end multi-task deep neural network can effectively capture the relationship between treatment propensity and treatment effect in scenarios with continuous treatments, thereby providing better uplift modeling by balancing estimation of response performance and individual treatment effect.\nMethod: The authors propose a three-headed neural network structure comprising logistic, monotonicity, and Diffloss networks to estimate continuous treatment effects while addressing confounder bias. The approach involves embedding treatments and features into networks to predict outcomes based on mutually independent factors. This architecture is tested using semi-synthetic and real-world datasets, and its effectiveness is further evaluated by implementing it in an online voucher distribution system on a major ride-hailing platform.\nConclusion: The proposed method manages to balance response estimation and uplift ranking, outperforming benchmarks in terms of estimation accuracy for individual treatment effects (ITE) and uplift. Online testing in a real-world application showed improved market scale and better return on investment (ROI) compared to existing models.\n",
    "full text": "\nCAUSAL NEURAL NETWORKS FOR CONTINUOUS\nTREATMENT EFFECT ESTIMATION\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nCausal inference have wide applications in medical decision-making, evaluating\nadvertising, and voucher distribution. The exist of confounding effect makes it\ndifﬁcult to have an unbiased uplift estimation. Traditional methods focuses on\nthe ordering of the problem. Little attention has been paid to the response perfor-\nmance, either on the evaluation metric, nor the modeling. In this work, an end-to-\nend multi-task deep neural network is proposed to capture the relations between\nthe treatment propensity and the treatment effect, where the treatment can be con-\ntinuous. The performance of the proposal is tested over large scale semi-synthetic\nand real-world data. The result shows that the proposal balances the estimation\nof response performance and individual treatment effect. The online environment\nimplementation suggests the proposal can boost up the market scale and achieve\n4.8% higher return over investment (ROI).\n\n1\n\nINTRODUCTION\n\nConditional average treatment effect (CATE), also known as causal inference, can characterize the\ncausal effect of certain treatments on the potential outcome over the whole population as a function\nof measured features or covariates. Typically, samples are separated into two groups, treatment\ngroup and control group. The treatment group is given treat, such as medicines, while the control\ngroup is not. Causal inference is to estimate the treatment effect, also known as uplift, over the\ncontrol group. This technique has wide applications such as identifying the best medical decisions\n(Glass et al. (2013)), the most effective policy to economies (Heckman & Vytlacil (2007)), the most\npopular item for consumers on internet platforms (Liang et al. (2016)).\n\nOne of the most signiﬁcant issues encountered in practice in the causal inference problem is the bi-\nased distribution of treatment and control group. Ideally, the model should be trained over random-\nized control trials (RCT) data, in which treatment should be independent of features. However, the\nRCT experiment is usually not available or expensive to obtain in practice. Typically, confounding\nfactors exist which affect the treatment and potential outcome at the same time in the observational\ndata. In the internet scenario, the voucher is often provided only to inactive or new users. The active\nusers, on the other hand, are not given the vouchers for cost-saving. This phenomenon makes it\ndifﬁcult to learn an unbiased model.\n\nVarious methods have been developed to alleviate the bias introduced by confounders, such as,\nsensitivity analyses (Rosenbaum & Rubin (1983)), instrumental variable (IV) approaches (Angrist\net al. (1996)), and multi-task deep neural network (DNN) (Shi et al. (2019)). However, the sensitivity\nanalyses can not give the point estimation. IVs can only partially identify CATE. Not to mention\nthat most of the works limit to categorical treatment.\n\nIn some scenarios, it is sufﬁcient to get an ordering of uplift of samples, while the response estima-\ntion is not crucial. In a recommendation system, the cost of pushing an advertise to a consumer is\nconstant, no matter whether the consumer will purchase or not. In this case, one just needs to ﬁgure\nout consumers that have the greatest uplift. However, when it comes to the resource allocation prob-\nlem, accurate response estimation matters. For example, with the advertise, subsidy or a coupon is\nsent to consumers. Naturally, the platform want to control the total budget. In this case, one not only\nneeds to ﬁnd consumers with the largest uplift, but also needs to estimate the probability of purchas-\ning and the expected cost. In this case, the commonly used Qini coefﬁcients and aera under uplift\ncurve (AUUC) are not enough, since they does not describe the response estimation performance.\n\n1\n\n\fUnder review as a conference paper at ICLR2023\n\nThe main contribution of this paper are as follows:\n\n• An end-to-end multi-task deep neural network (DNN) is proposed to estimate the effect of\ncontinuous treatment which captures the relationships between the treatment propensity, the\ntrue responses, and the treatment effect to alleviate treatment bias in an integrated manner.\n\n• Extensive experiments carried out on semi-synthetic and real-world dataset indicate that\nthe proposal outperforms the benchmarks in ITE estimation accuracy and uplift ranking\nperformance.\n\n• The proposal is implemented in an online environment of voucher distribution on the largest\nride-hailing platform in mainland China. The results illustrate the effectiveness of the pro-\nposal in scaling up the market and achieving high return over investment (ROI).\n\nThis work is organized as follows. In Section 3, the problem setup and notations used in this work\nare presented. The proposals are presented in Section 4. Section 5 presents the experiments over\nsemi-synthetic and real-world data, and Section 6 illustrates the online environment results. Section\n7 concludes the work.\n\n2 RELATED WORK\n\nThere are in general three main approaches to estimate causal effects, two-model, single-model, and\ndirect methods. The two-model approach ﬁts two models, one to predict the response of the control\ngroup and the other to predict the treatment group response (Radcliffe (2007); Nassif et al. (2013)).\nThe model is intuitive and simple. Mature techniques can be applied within this framework like\nrandom forest, XGBoost, and DNN. However, modeling the outcome separately often misses the\n”week“ uplift signal (Radcliffe & Surry (2011); Zaniewicz & Jaroszewicz (2013)).\n\nSingle-model framework typically models a transformed target variable Z = Y T + (1 − Y )(1 − T )\nwhere Y is the binary outcome and T is the binary treatment (Jaskowski & Jaroszewicz (2012)).\nFollowing this idea, another transformation is presented in (Athey & Imbens (2015)) to relax the\nbalance treatment assignment assumption. Single-model framework often outperforms the two-\nmodel framework. But the binary treatment and outcome assumption limits its applications.\n\nTree based model is typically employed to directly model the uplift (Wan et al. (2022); Wager\n& Athey (2018); Tang et al. (2022)). A new splitting criterion with kernel-based doubly robust\nestimator is proposed in (Wan et al. (2022)) to alleviate the biased treatment distribution. However,\ntree models have difﬁculties in dealing with continuous treatment.\n\nDeep causal models have attracted increasing attentions since 2016 (Johansson et al. (2016)). A\ndomain adaption framework is adopted in (Shalit et al. (2017); Alaa et al. (2017)). Causal represen-\ntation learning is presented in (Atan et al. (2018). Generative Adversarial Nets (GAN) models are\nconsidered in (Bica et al. (2019; 2020))\n\nRecently, randomized control trials are combined with the observational data to adjust confounders\n(Concato et al. (2000)), with the assumption of transportability of RCT to external observational\nstudies (Pearl & Bareinboim (2011)). (Yu et al. (2023)) proposed a transfer learning enhanced uplift\nmodel that utilizes biased observational data to estimate the uplift of RCT data. However, the RCT\ndata is usually expensive to obtain and much less than the observational data.\n\nMost works in causal inference area focus on the binary (Shi et al. (2019); Tang et al. (2023; 2022);\nZhong et al. (2022)) or discrete (Wan et al. (2022)) treatment case. The continuous treatment case is\nless studied (Wang et al. (2022); Gao et al. (2021)). In some cases, such as voucher distribution and\ndose selection, the continuous treatment is necessary.\n\n3 NOTATION AND ASSUMPTIONS\n\nThe treatment effect estimation, also known as uplift modeling, with continuous treatment is consid-\nered in this work. Following the potential outcome framework in (Rubin (1974)), let T be the 1-dim\ncontinuous treatment, X be the dX -dim confounding variables, and Y ∈ {0, 1} be the outcome of\ninterest. As shown in Figure 1a, the population Ω : (X, T, Y ) is assumed to satisfy the following\n\n2\n\n\fUnder review as a conference paper at ICLR2023\n\n(a) Causal Diagrams\n\n(b) Intuition: two embeddings\nare trained. The b part indicates\nthe base response without treat-\nment. The w part describes the\ntreatment effect.\n\nFigure 1: Causal Diagrams and intuition\n\nequations:\n\nY = g(T, X) + (cid:15); g(T, X) = h(T, X) + b(X); T = f (X) + ν\nwhere (cid:15), ν are white noises. The samples {(Xi, Ti, Yi)} where i = 1, . . . , n are drawn from Ω. The\npotential outcome under treatment T = t is deﬁned as Y(t). In the context of continuous treatments,\nthe propensity score is deﬁned by probability density function π(T = t|X). The sample set of\ntreatment T = t is denoted as Nt and the sample size is nt (cid:44) ||Nt||.\nThe conditional average treatment effect (CATE) is the estimation target, formally deﬁned by\n\nθ(t, X) = E[Y(t)|X] − E[Y(0)|X].\n\nThe following mild assumptions are made (Holland (1986); Kennedy et al. (2017)).\n\nAssumption 1. Consistency: the outcome of any sample solely depends on its treatment.\n\nAssumption 2.\ncovariate X, i.e. (Y(T ) ⊥ T )|X\n\nIgnorability: The potential outcomes Y(T ) is independent of treatment T given\n\nPositivity:\n\nThe density of\n\nAssumption 3.\nπ(T = t|X) > pmin > 0.\nAssumption 4. Monotonicity: The treatment effect is assumed to be monotone, i.e. E[Y(t1)|X] ≥\nE[Y(t2)|X] if t1 > t2.\nUnder these assumptions, we have\n\nis bounded away from 0,\n\ntreatment\n\ni.e.\n\nθ(t, X) = E[Y(t)|X] − E[Y(0)|X] = E[Y |T = t, X] − E[Y |T = 0, X]\n\n= E[g(t, X)|T = t, X] − E[g(t, X)|T = 0, X]\n\nAssumption 4 is typically true in the internet business scenarios. For example, users with more\nsubsidy typically more likely to consume.\n\n3.1 EVALUATION\n\nTypically, the treatment effect can not be evaluated since the counter factor is unknown. For each\nsample, only the outcome given one treatment or none is observed. Outcomes with other treatment\nare unknown. In this case, Qini coefﬁcient and area under uplift curve (AUUC) are popular met-\nric choices. However, the metrics only describe the ordering performance of models and not the\nresponse performance. Thus the response performance errors are introduced in this work.\n\nFor each treatment T = t, the uplift curve is deﬁned as the follows. The samples from both control\nand treatment group T = t are sorted in descending order by the estimated uplift value ˆθ(t, Xi).\nThe response sum of the ﬁrst k-samples with treatment T = t and without treatment are deﬁned as\nYiI(Ti = t); R(0, k) =\n\nYiI(Ti = 0),\n\nR(t, k) =\n\n(cid:88)\n\n(cid:88)\n\ni≤k\n\ni≤k\n\n3\n\n\fUnder review as a conference paper at ICLR2023\n\nwhere I(Ti = 0) is the indicator function deﬁned as\n\nI(Ti = t) =\n\n(cid:26) 1, if Ti = t\n0, else.\n\nDeﬁne the ﬁrst k samples uplift score as\n\nu(t, k) = [\n\nR(t, k)\nN (t, k)\n\n−\n\nR(0, k)\nN (0, k)\n\n][(N (t, k) + N (0, k)].\n\nwhere N (t, k) = (cid:80)\n\ni≤k\n\nI(Ti = t).\n\nGiven the ﬁrst to the (nt + n0)th-samples uplift score, the uplift curve can be plotted. The AUUC\nfor treatment t and the entire dataset are calculated as\n\nAUUCt =\n\n(cid:80)\n\nk u(t, k)\nnt + n0\n\n; AUUC =\n\n(cid:80)\n\nt AUUCtnt\n(cid:80)\nt nt\n\n.\n\nQini coefﬁcient is a normalization of AUUC and can be compared across different datasets. Deﬁne:\n\nq(t, k) = R(t, k) − R(0, k)\n\nN (t, k)\nN (0, k)\n\nQini coefﬁcient for treatment t and for the entire dataset are calculated as\n\nQinit =\n\n(cid:80)\n\nk q(t, k)\nnt + n0\n\n; Qini =\n\n(cid:80)\n\nt Qinitnt\n(cid:80)\nt nt\n\n.\n\nTo evaluate the response performance, we deﬁne the average treatment effect (ATE) error of treat-\nment t as\n\n(cid:15)t\nATE = abs[(\n\nR(t, nt)\nnt\n\n−\n\nR(0, n0)\nn0\n\n) − (\n\n(cid:80)\n\ni∈Nt\n\nˆg(t, Xi)\nnt\n\n−\n\n(cid:80)\n\ni∈N0\n\nˆg(0, Xi)\nn0\n\n)]\n\nwhere Nt is the set of populations with treatment T = t, and ˆg the response estimate given by the\nmodel. ATE error measures the average difference between the true and estimated treatment effect\nin a treatment group.\n\nSince the ground truth is known in the synthetic data, we can directly evaluate the estimate of\nindividual treatment effect (ITE). The MAE for treatment t is deﬁned as\n\nprob = E(cid:2)abs[g(t, Xi) − ˆg(t, Xi)](cid:3)\n(cid:15)t\nwhere g(t, Xi) is the true probability that Yi = 1, and ˆg(t, Xi) is the estimate. The response error\nfor treatment t is deﬁned as\nAprob = E(cid:2)abs[Yi − ˆg(t, Xi)](cid:3)\n(cid:15)t\n\nThe ITE error for treatment t is deﬁned as\n\nITE = E(cid:2)abs[(cid:0)g(t, Xi) − g(0, Xi)(cid:1) − (cid:0)ˆg(t, Xi) − ˆg(0, Xi)(cid:1)](cid:3).\n(cid:15)t\nThe ATE error (cid:15)ATE, MAE (cid:15)prob, response error (cid:15)Aprob and ITE error (cid:15)ITE are the weighted average of\nATE, (cid:15)t\n(cid:15)t\n\nITE, respectively.\n\nAprob and (cid:15)t\n\nprob , (cid:15)t\n\n4 METHOD\n\nWe present the network design in this section. As shown in Figure 2, we proposed a three-head\nnetwork to deal with confounder. The features X and the treatment T are fed to two MLPs to\ngenerate embedding. These embeddings are fed to three networks, the Monotonicity network, the\nDifﬂoss network and the logistic regression network. Three errors are weighted summed to generate\nthe ﬁnal loss. The online inference is the output of the Monotonicity network. Figure 2 shows the\ndesign when the output is categorical. When the potential output is continuous, simple modiﬁcation\nis needed in the loss design.\n\n4\n\n\fUnder review as a conference paper at ICLR2023\n\nFigure 2: Method design\n\n4.1 LOGISTIC NETWORK\n\nThe treatment embedding and feature embedding are concatenated and fed to the logistic network.\nThe label is the binary outcome Y , and the error is cross-entropy. This network is designed to help\ntrain meaningful treatment and feature embedding. When the outcome Y is continuous, the error\nshould be replaced by mean squared error (MSE) or mean absolute error (MAE).\n\n4.2 MONOTONICITY NETWORK\n\nAccording to Assumption 4, the monotonicity network is designed to guarantee the monotonicity.\nThe probability of Y = 1 is assumed to follow a linear equation as follows\n\nˆP (Y = 1) = sigmoid(w ∗ T + b),\n\nwhere the value of w and b is the output of the w-DNN and b-DNN, correspondingly. The input of\nthese two DNNs is the feature embedding. The value b(X) describes the potential outcome without\ntreatment. The value w(X) ∗ T describes the effect of treatment.\n\nTo guarantee the monotonicity of treatment effect, the activation function of the w-DNN is softmax.\nThe loss is deﬁned as the cross-entropy. When the potential outcome is continuous, the error should\nbe replaced by MSE or MAE.\n\n4.3 DIFFLOSS NETWORK\n\nIn the monotonicity network, the probability of Y = 1 is assumed to follow a linear equation with\ntwo parts. The value b is assumed to be independent of treatment T . In the Difﬂoss network, the\noutcome of the b-network and w-network is trained to be orthogonal. The cosine similarity between\nthe hidden state of the b-network (w-network) and the treatment embedding is calculated and denoted\nby cosb (cosw).\n\nThus the loss is deﬁned as\n\ncosb =\n\nhb ∗ embT\n||hb|| ∗ ||embT ||\n\n.\n\nlossdiff = − log\n\nexp(γ cosw)\nexp(γ cosw) + exp(γ cosb)\n\n.\n\n5\n\n\fUnder review as a conference paper at ICLR2023\n\n(a) Traditional Dragonnet\n\n(b) Continuous Dragonnet\n\nFigure 3: Dragonnet structures\n\nMinimizing this loss means to maximize the cosine similarity between the hidden state of w-network\nand the treatment embedding while minimize the one between the hidden state of b-network and the\ntreatment embedding, which makes b to be independent of treatment T .\n\nGiven three networks, the loss is the weighted sum as follows:\n\nloss = ωmonlossmon + ωloglosslog + ωdifflossdiff\n\nThe inference is given by\n\nˆθ(t, X) = sigmoid(cid:0) ˆw(t, X) ∗ t + ˆb(X)(cid:1) − sigmoid(cid:0) ˆw(0, X) ∗ 0 + ˆb(X)(cid:1)\n\n= sigmoid(cid:0) ˆw(t, X) ∗ t + ˆb(X)(cid:1) − sigmoid(cid:0)ˆb(X)(cid:1)\n\nwhere ˆw and ˆb are the output of the monotonicity network.\n\n4.4 DISCUSSION\n\nIn (Rosenbaum & Rubin (1983)), a theorem of sufﬁciency of propensity score is stated as follows.\n\nTheorem 1 (Sufﬁciency of Propensity Score). If the average treatment effect θ is identiﬁable from\nobservational data by adjusting for X, i.e., θ = E[E[Y |T = t, X] − E[Y |T = 0, X]], then adjusting\nfor the propensity score also sufﬁces, θ = E[E[Y |T = t, f (X)] − E[Y |T = 0, f (X)]].\n\nMotivated by this theorem, a three headed deep neural network, Dragonnet is proposed to deal with\nconfounders when the treatment is binary in (Shi et al. (2019)). As shown in Figure 3a, one head\npredicts the probability that one sample gets treated, and other two heads predicts the potential\noutcomes when the treatment equals to 0 and 1, correspondingly. However, this structure can not ﬁt\nthe continuous treatment case. One naive extension of Dragonnet is shown in Figure 3b. One head\nis to predict the probability that one sample gets treated (T > 0), and the other head predicts the\npotential outcomes with different treatment.\n\nThis model does not guarantee the monotonicity of the treatment effect. One intuitive idea is to\nseparate the potential outcomes into two parts. As shown in Figure 1b, one part is the base part\nthat has nothing to do with treatments, and represents the natural outcomes without treatments. The\nother part is the treatment part, which tries to estimate h(T, X) and keep the monotonicity in T . To\nmake sure the ﬁrst part has nothing to do with treatments, the Difﬂoss network is introduced to make\nw and b orthogonal. The Logistic network is to make the embedding meaningful.\n\n5 EXPERIMENTS\n\n5.1 PRE-ESTABLISHED CAUSAL BENCHMARK DATASET\n\nThe IBM causal inference banchmark framework is used to evaluate the performance of the model,\nwhich was developed for the 2018 Atlantic Causal Inference Conference competition data (ACIC\n2018). The dataset is derived from the linked birth and infant death data (LBIDD) (MacDorman &\nAtkinson (1998))1. The treatment in the dataset is binary and the potential output is continuous. Thus\nthe activation function of the logistic network and monotonicity network are set to none. Besides, the\n\n1https://github.com/IBM-HRL-MLHLS/IBM-Causal-Inference-Benchmarking-Framework/tree/mas-\n\nter/data/LBIDD\n\n6\n\n\fUnder review as a conference paper at ICLR2023\n\ntreatment effect in the data is not necessarily positive. Thus, the softplus function in the monotonicity\nnetwork is removed.\n\nTable 1: ACIC Data Results\n\nMethod\nDragonNet\nWTB-diff\n\n(cid:15)prob\n1.3527\n1.3151\n\n(cid:15)ITE\n9.0548\n6.3749\n\n(cid:15)prob All\n2.2591\n1.3753\n\n(cid:15)ITE All\n8.9741\n5.0956\n\n# of params\n176804\n14935\n\nTime\n5.08s\n3.9s\n\nSince the outcome of this dataset is continuous, only the ITE error and MAE are presented. The\nproposal is compared with Dragonnet (Shi et al. (2019)). In Table 1, the performance with two\nsettings are presented as the one in (Shi et al. (2019)). Left half of the table presents the test errors.\nThat is to divide the dataset to training set and test set, with ratio 8:2. The model is trained over\nthe training set and the tested over the test set. The right half of the table is to train and test models\nusing the whole dataset. The result shows that the MAE and ITE error of the proposal are smaller\nthan the ones of Dragonnet. The number of hyper-parameters in the models and the training time\nfor one epoch are also smaller.\n\n5.2 SEMI-SYNTHETIC DATA\n\nThe data is collected from one of the largest ride-sharing platform in mainland China, which covers\nfrom July 2022 to April 2023. Users come to the platform and submit their ride enquiry to the\nplatform, including origins and destinations. The platform will provide estimated cost of the trip.\nMeanwhile, a possible subsidy is provided by the platform. Given the cost and subsidy, users decide\nto whether to place the order. In this work, more than 8 million enquires are collected, in which\nthere are more than half million samples are RCT data. The data is sampled that the treated and\nnon-treated samples are 1:1. There are more than 300 features, including the enquiry information,\nthe historical behavior of the user, and the supply-demand information of the origin.\n\nThe semi-synthetic data is generated using the real enquires. The treatment is the discount rate. Two\ndatasets are examined in this work. The linear dataset assumes the probability a user places an order\nis a linear function of the treatment and features as deﬁned as follows:\n(cid:26) P (Y = 1|Xi) = sigmoid(w0 ∗ Ti + bi)\n\nbi = αXi\n\nwhere α = [αj] are randomly generated, and w >> αj to make uplift effect signiﬁcant.\n\nThe polynomial dataset assumes the probability a user places an order is a polynomial function of\nthe treatment and features as deﬁned as follows:\n\n(cid:40) P (Y = 1|Xi) = sigmoid(wi ∗ Ti + bi)\n\nbi = αXi\nwi = w0 + λTi + βXi\n\nwhere α, β, λ are randomly generated, and λ >> αj to make uplift effect signiﬁcant.\n\nTable 2: Linear Semi-synthetic Data Results\n\nMethod\nContinuous DragonNet\nGCF\nADMIT (modiﬁed)\nXGBoost(rand)\nWTB\nWTB-diff\nWTB-diff-fft\n\n(cid:15)prob\n0.0320\n-\n0.085\n0.0929\n0.0181\n0.0244\n0.0195\n\n(cid:15)ITE\n0.0150\n0.0185\n0.0324\n0.0282\n0.0182\n0.0185\n0.0174\n\nAUUC\n0.5216\n0.5106\n0.4902\n0.4273\n0.5036\n0.4960\n0.4891\n\nQini\n0.3445\n-0.0018\n0.1343\n0.3179\n0.3518\n0.3519\n0.3509\n\n(cid:15)ATE\n0.0068\n-\n0.0468\n0.0111\n0.0020\n0.0019\n0.0020\n\nThe linear semi-synthetic data results are listed in Table 2. Continuous DragonNet is a two headed\ndeep neural network presented in Figure 3b. Generalized Causal Forest (GCF) (Wan et al. (2022))\nintroduces a new splitting criterion with kernel-based doubly robust estimator. The ADMIT method\n\n7\n\n\fUnder review as a conference paper at ICLR2023\n\nemploys an importance sampling technique to deal with the biased treatment distribution (Wang\net al. (2022)). The WTB model serves as an ablation study. It follows the structure in Figure 2,\nwithout the Difﬂoss Network and the the Logistic Network. The WTB-diff-fft model is similar to\nthe proposal in Figure 2. The only difference is that the WTB-diff-fft model replaces the learnable\ntreatment embedding MLP by an FFT module. All models is trained using biased observe data and\ntested on the RCT data, except XGBoost(rand), since it is not designed to deal with the confounders.\nXGBoost(rand) is a s-learner model, which uses RCT data only to ﬁt the potential outcome, with\ntreatment as one feature. If the sample is from the control group, then T = 0.\n\nIt is shown in Table 2 that the dragonnet has the smallest ITE error and greatest AUUC value. But the\nMAE (cid:15)prob is large, which means the ordering of Dragonnet is good but the response estimation is not\nthat good just as stated in (Shi et al. (2019)). On the other hand, three proposals get both MAE and\nITE error small, which balances the ordering and the response estimate. GCF also performs great\nin the ordering. However, it does not provide the response estimate. XGBoost(rand) performs the\nworst, even trained with the RCT data. Comparing three proposals, it can be seen that the Difﬂoss\nand logistic network module increases the response error (cid:15)prob, since it introduce additional losses.\nHowever, the WTB-diff-fft model has smaller ITE error and ATE error.\n\nThe polynomial semi-synthetic data results are listed in Table 3. Similar to the linear case, Dragonnet\nis good at ordering but not that good at the response estimate. The three proposals balances two\naspects. Comparing three proposals, the WTB-diff model get both MAE and ITE error small and\nthe greatest Qini coefﬁcients. The WTB model estimates the response well, while the WTB-diff-fft\nmodel estimates ITE well.\n\nTable 3: Polynomial Semi-synthetic Data Results\n\nMethod\nContinuous DragonNet\nGCF\nADMIT(modiﬁed)\nXGBoost(rand)\nWTB\nWTB-diff\nWTB-diff-fft\n\n(cid:15)prob\n0.0378\n-\n0.0764\n0.0913\n0.0146\n0.0297\n0.0260\n\n(cid:15)ITE\n0.0227\n0.0346\n0.0239\n0.0483\n0.0400\n0.0307\n0.0305\n\nAUUC\n0.4856\n0.5029\n0.4519\n0.4833\n0.4379\n0.5002\n0.5032\n\nQini\n0.3672\n-0.0004\n0.3499\n0.3588\n0.3703\n0.3707\n0.3699\n\n(cid:15)ATE\n0.0144\n-\n0.0317\n0.0211\n0.0018\n0.0046\n0.0036\n\n5.3 REAL-WORLD DATA\n\nIn the real-world data part, the data is identical to the semi-synthetic data part, except that the label is\ntrue outcomes. Since there is no ground truth in the real-world data. Only the AUUC, Qini, ATE and\naverage response errors are included. In Table 4, the proposals achieve the best Qini coefﬁcients,\nMAE and ATE errors. Comparing three proposals, WTB-diff model balances Qini and ATE errors\nwell. WTB model gets the best ATE error and WTB-diff-fft model get the best Qini score.\n\nTable 4: Real-world Data Results\n\nMethod\nGCF\nADMIT(modiﬁed)\nXGBoost(rand)\nWTB\nWTB-diff\nWTB-diff-fft\n\nAUUC\n0.5179\n0.4986\n0.6\n0.336\n0.525\n0.51\n\nQini\n-0.0001\n0.3524\n0.309\n0.288\n0.373\n0.397\n\n(cid:15)ATE\n0.0105\n0.030\n0.009\n0.007\n0.008\n0.015\n\n(cid:15)Aprob\n-\n0.055\n0.065\n0.005\n0.013\n0.028\n\n5.4 EFFECT OF MODEL PARAMETERS\n\nIn order to explore the effects of different hyper-parameters of our proposed model, we evaluate it\non the linear semi-synthetic dataset with two important hyper parameters, dimension of FFT and γ\n\n8\n\n\fUnder review as a conference paper at ICLR2023\n\nin the Difﬂoss network. As shown in Figure 5, the AUUC and Qini coefﬁcients dip, and the MAE\nand ITE error increases as the FFT dimension increases. In Figure 6, increasing in γ decreases Qini\ncoefﬁcients. Thus, we choose FFT dimension as 16 and γ = 1 considering resource consumption.\n\nTable 5: Effect of FFT dimension\n\nMethod\nWTB-diff-fft-16\nWTB-diff-fft-32\nWTB-diff-fft-64\nWTB-diff-fft-128\n\n(cid:15)prob\n0.0195\n0.0264\n0.0268\n0.0261\n\n(cid:15)ITE\n0.0174\n0.0176\n0.0211\n0.0207\n\nAUUC\n0.4891\n0.4615\n0.4865\n0.4979\n\nQini\n0.3509\n0.2354\n0.3509\n0.3513\n\n(cid:15)ATE\n0.0020\n0.0019\n0.0019\n0.0019\n\nTable 6: Effect of γ\n\nMethod\nγ = 1\nγ = 2\nγ = 10\n\n(cid:15)prob\n0.0264\n0.0242\n0.0173\n\n(cid:15)ITE\n0.0176\n0.0215\n0.0195\n\nAUUC\n0.4615\n0.4557\n0.4619\n\nQini\n0.3509\n0.2354\n0.2353\n\n(cid:15)ATE\n0.0019\n0.0019\n0.0020\n\n6\n\nIMPLEMENTATION\n\nThe proposal is employed in the largest ride-hailing platform in mainland China. The causal in-\nference results are employed to distribute vouchers to users so the total market scale is maximized.\nThe uplift model ﬁrstly infers query-wise elasticity, then a clustering procedure is applied to achieve\nconsumer fairness. After that, the class-wise elasticity and budget constraints are fed to an optimiza-\ntion module to generate a subsidy dictionary. At a ﬁxed interval, the order information ﬂows back\nto the optimization block to update the subsidy dictionary. The experiment is carried out in six of\nthe largest cities in mainland China over a week in August 2023. The target subsidy rate is set to\n3%. Three methods are compared with a control group. The market scale of the control group is\nnormalized to 1, and the cost is 0 since there is no subsidy in this group. The proposal achieves the\nlargest market scale gain with the largest cost. The return over investment (ROI) is 4.8% higher than\nthe ones of DragonNet and XGBoost.\n\nTable 7: Online Implementation Results\n\nMethod\nControl\nContinuous DragonNet\nXGBoost(rand)\nWTB-diff\n\nMarket Scale\n1\n1.0176\n1.0143\n1.019\n\nCost\n0\n\nROI\n-\n\n3.11% 0.56\n2.49% 0.56\n3.18% 0.587\n\n7 CONCLUSION AND FUTURE WORK\n\nIn this work, the CATE estimation problem is considered with the exists of confounders. An end-\nto-end multi-task network is proposed to capture the relationship between treatment propensity and\nthe treatment effect. Numerical experiments using semi-synthetic and real-world data illustrate the\neffectiveness of the proposal in ITE estimation accuracy and uplift ranking performance. Online\nimplementation shows the proposal achieves greater market scale and higher ROI.\n\nPotential future works include to ﬁnd the theoretical prove of the performance and evaluate the\nstability of the proposal. Another interesting problem would be to ﬁnd an evaluation metric to bal-\nance the uplift ranking and response estimation performance. This metric is crucial when choosing\nmodels in the practice.\n\n9\n\n\fUnder review as a conference paper at ICLR2023",
    "abstract": "Causal inference have wide applications in medical decision-making, evaluating advertising, and voucher distribution. The exist of confounding effect makes it difficult to have an unbiased uplift estimation. \nTraditional methods focuses on the ordering of the problem. Little attention have been paid to the response performance, either on the evaluation metric, nor the modeling. \nIn this work, an end-to-end multi-task deep neural network is proposed to capture the relations between the treatment propensity and the treatment effect, where the treatment can be continuous. \nThe performance of the proposal is tested over large scale semi-synthetic and real-world data. \nThe result shows that the proposal balances the estimation of response performance and individual treatment effect. \nThe online environment implementation suggests the proposal can boost up the market scale and achieve 4.8% higher return over investment (ROI).",
    "response": {
      "Soundness": 1,
      "Presentation": 1,
      "Contribution": 1,
      "Rating": 3,
      "Confidence": 4,
      "Strengths": "Continuous treatment effect estimation is a significant problem in causal inference.",
      "Weaknesses": "- **The statement \"The following mild assumptions are made (Holland (1986); Kennedy et al. (2017))\" does not correspond to reality. The Monotonicity Assumption is not mentioned in (Holland (1986); Kennedy et al. (2017)), and the Consistency Assumption also differs from the one proposed by Kennedy et al. (2017). Furthermore, there are many other statements in this paper that do not match reality.**\n\n- The writing needs significant improvement. The survey on relevant literature is not comprehensive. \n\n- The statement \"Traditional methods focus on the ordering of the problem. Little attention has been paid to the response performance, neither on the evaluation metric, nor the modeling\" is confusing. In both binary and continuous settings, researchers have recently proposed numerous estimators to predict CATE or ITE, such as Johansson et al. (2016), Shalit et al. (2017), Bica et al. (2019; 2020), Schwab et al. (2020) and Nie et al. (2021). Recent deep methods typically use PEHE or MSE on CATE to evaluate the response performance, and none of them focus on the ordering of the problem. This statement contradicts my understanding.\n\n- The statement \"Not to mention that most of the works limit to categorical treatment\" is incorrect. Recently, many machine learning IV algorithms have been proposed to identify CATE and are not limited to categorical treatment, such as NPIV, DeepIV, KernelIV, DualIV, FastIV, DFIV, CBIV, AGMM, and DeepGMM.  **The survey on relevant literature is not comprehensive.** \n\n- The statement \"Most works in causal inference area focus on the binary or discrete treatment case. The continuous treatment case is less studied\" is incorrect. Many studies have been conducted on estimating continuous treatment effects, particularly in predicting average dose-response function (ADRF) or average dose-response curve (ADRC) to identify causal effects. Some notable works in this area include (Robins et al., 2000), (Hirano & Imbens, 2004), (Imai et al., 2004), (Flores et al., 2007), (Flores et al., 2012), (Kennedy et al., 2017), (Wilson et al., 2018), (Kallus et al., 2019), (Colangelo et al., 2020), (Sharma et al., 2020), (Klosin, 2021), (Linton & Zhang, 2021), (Zhang et al., 2022), and many others. **The survey on relevant literature is not comprehensive.** \n\n- That statement \"Conditional average treatment effect (CATE) is known as causal inference\" is incorrect. Conditional average treatment effect (CATE) is not synonymous with causal inference. CATE refers to the average effect of a treatment (intervention) on an outcome under a specific condition. Causal inference, on the other hand, involves analyzing causal relationships to determine the effect of a treatment (intervention) on an outcome of interest. CATE is one method used in causal inference, but they are not identical concepts.\n\n- **Algorithm**: The feature X is a common cause of treatment T and outcome Y, which could potentially confound the causal effect of T on Y. Although this paper uses lossdiff to make embedding b independent of treatment T, the embedding factor w may remain confounders of the causal effect of T on Y. How to address the confounding bias from w?\n\n- **Experiments**: Due to the incomplete literature review, the paper lacks a comprehensive evaluation and comparison with existing methods, such as GPS (Imbens, 2000; Fong, 2018), DRNet (Schwab et al., 2020), SCIGAN (Bica et al., 2020), VCNet (Nie et al., 2021), making it difficult to assess the practical contribution of the proposed approach.",
      "Questions": "See Above."
    }
  },
  {
    "paper_id": "9lvyCHhQix",
    "summary": "Background: Continual Learning (CL) involves the ability of models to learn continuously from non-stationary data distributions, often addressing the issue of catastrophic forgetting, where new tasks negatively impact previously learned ones. Previous CL approaches focus primarily on avoiding forgetting but often do not balance parameter efficiency and capacity saturation. Parameter efficiency ensures the effective use of parameters without overfitting, while capacity saturation can occur when a model's ability to generalize is compromised as task diversity increases.\nGap: Existing approaches to continual learning struggle with the trade-off between parameter efficiency and capacity saturation, which is particularly problematic in long-term task-incremental settings with storage constraints. The challenge is maintaining parameter efficiency while handling capacity saturation for evolving tasks, which current methods fail to adequately address.\nHypothesis: The paper hypothesizes that a dynamic approach to expanding a model's weight space can effectively balance parameter efficiency and capacity saturation in continual learning. By introducing a knowledge hypergraph structure that captures latent knowledge across tasks, it is possible to measure task diversity and estimate necessary capacity increases for new tasks, ensuring efficient parameter usage and avoiding capacity saturation.\nMethod: The authors propose a novel CL approach called Continual Knowledge HyperGraph Learning (HyperGKL). This method introduces a dynamic knowledge hypergraph structure, which identifies and leverages latent knowledge across tasks to measure task diversity and manage capacity increases for new tasks. The hypergraph connects shared skills across tasks and estimates the required capacity increment. The method uses a hypernetwork for dynamic skill-specific subnetwork generation to ensure constant parameter numbers over time. It involves decomposing tasks into skills, constructing a dynamic hypergraph, and using constraints and parameter generators to manage adaptation and efficiency. The approach is empirically validated on several benchmark CL datasets, comparing it with existing methods for performance and resource utilization.\nConclusion: The proposed HyperGKL method effectively addresses the balance between parameter efficiency and capacity saturation in continual learning, achieving state-of-the-art results on several benchmark datasets. It maintains low parameter counts through a novel hypergraph structure and a hypernetwork that dynamically manages model complexity, demonstrating its robustness in long-term, memory-constrained CL scenarios.\n",
    "full text": "\nDIVERSITY-AWARE CONTINUAL LEARNING WITH\nLATENT KNOWLEDGE HYPERGRAPH\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nContinual learning (CL) refers to the ability of models to learn from non-stationary\ndata distribution while transferring and protecting past knowledge. Existing litera-\nture in CL has mainly focused on overcoming catastrophic forgetting. However,\nthey often overlook a critical trade-off between parameter efficiency and capacity\nsaturation. Almost all of the existing approaches including architecture-stable\nand architecture-growing methods struggle to balance parameter efficiency and\ncapacity saturation. This makes them vulnerable to long-term task-incremental CL\nunder storage constraints. In this paper, we propose a novel CL approach, Contin-\nual Knowledge HyperGraph Learning (HyperGKL), which explicitly addresses\nthe trade-off between parameter efficiency and capacity saturation by efficiently\nexpanding the model’s weight space in proportion to the actual capacity increase\nneeded by each new task. Specifically, our approach introduces a unique knowledge\nhypergraph structure that captures the latent knowledge across tasks and leverages\nit to measure task diversity and estimate the capacity increase required for each\nnew task. Moreover, we introduce new constraints to ensure parameter efficiency\nduring inference and a fine-grained parameter generator to create task-specific\nsub-networks that ensure a constant number of trainable parameters over time\nwhile accommodating the evolving complexities of tasks. Extensive experiment\nresults show that the proposed approach achieves state-of-the-art results on several\nbenchmark CL datasets, while maintaining low parameter counts.\n\nINTRODUCTION\n\n1\nContinual Learning (CL) is a pivotal challenge in machine learning, especially in the context of online\ntask-incremental scenarios where new tasks arrive sequentially. Most of the existing CL approaches\nfocus on addressing the issue of catastrophic forgetting (Kirkpatrick et al., 2017), i.e., the previously\nlearned tasks are forgotten or negatively impacted after learning new tasks. However, an overlooked\nissue in existing CL literature is the critical trade-off between parameter efficiency and capacity\nsaturation. While capacity saturation refers to neural networks becoming inadequate to generalize\nknowledge from seen tasks as task diversity increases over time, parameter inefficiency occurs when\nnetworks become over-parameterized and exceed the maximum capacity needed for a good knowledge\ngeneralization over seen tasks (inference-time inefficiency) or when the trainable parameters needed\nfor achieving a good performance largely exceeds the memory budget (training-time inefficiency).\nExisting CL approaches (Li & Zeng, 2023; Gupta et al., 2022), including replay-based, architecture-\nstable, and expansion-based methods, struggle to strike a balance between these two challenges.\n\nContributions: (1) To address the aforementioned issue, we propose a novel CL formulation\nthat explicitly considers the trade-off between parameter efficiency and capacity saturation in task-\nincremental CL scenarios. (2) Central to our approach, Continual Knowledge HyperGraph Learning\n(HyperGKL), is the design of a unique knowledge hypergraph structure that captures the latent\nknowledge across tasks and leverages it to measure task diversity and estimate the capacity increase\nrequired for each new task. Specifically, the proposed knowledge hypergraph is a dynamic structure\nthat dissects tasks into multiple skills (latent knowledge) and captures the intricate task interdependen-\ncies at a granular level. Each task arriving in the CL setting is decomposed into multiple skills, and\neach skill is represented by a vertex in the hypergraph. Hyperedges connect vertices that represent\nskills that are shared by the same task. The knowledge hypergraph is constructed dynamically as the\nmodel learns new tasks. For each new task, the model first decomposes the task into multiple skills.\nIt then adds the new skills to the knowledge hypergraph and updates the hyperedges to reflect the\n\n1\n\n\fUnder review as a conference paper at ICLR 2024\n\nrelationships between the new skills and the existing skills. Moreover, the task diversity is assessed\nby measuring the average shared skills between tasks, where higher values indicate lower diversity.\n(3) To ensure parameter efficiency during inference, the approach introduces new constraints that\nencourage diversity among skills and facilitates skill-sharing among tasks, offering guidance for\ncapacity expansion while minimizing inefficiencies during inference. At the same time, to overcome\nthe capacity saturation issue, we propose an algorithm that identifies shared skills with the current task,\ndetermines the requirement for new skills, and seamlessly integrates skill-specific submodules for\nnetwork expansion. This approach optimizes the balance between expanding capacity and preserving\nparameter efficiency. (4) Finally, the approach leverages a fine-grained parameter generator through\na hypernetwork that dynamically creates task-specific sub-networks to ensure a constant number\nof trainable parameters over time while accommodating the evolving complexities of tasks, threby\nenhancing training-time efficiency.\n\n2 PROBLEM DEFINITION: DIVERSITY-AWARE PARAMETER-EFFICIENT CL\n\nTask-incremental Continual Learning (CL): We focus on the online task-incremental continual\nlearning setting (Li et al., 2019). Consider a sequence of tasks T1, T2, ..., TT arriving to a learner,\nwhere each task Tt = {xj,t, yj,t}nt\nj=1 consists of nt training/validation/testing samples from a its own\nlabel space. We assume there exist simultaneous input distribution shift and label space drift over\ntasks. Suppose θt denotes the trainable parameters at task Tt and Θt = (cid:83)t\ni=1 θi collects all trainable\nparameters up to Tt. Let f (·; Θt) denote the neural network at task Tt parameterized by Θt. When a\nnew task t comes, the main objective of standard CL is to minimize the task-specific loss on Tt as\nwell as minimize the forgetting for all seen tasks T1, T2, ..., Tt−1\n\nmin\nΘt\n\nLtask(Θt) + Rfgt(Θt) = E(x,y)∼Ttl(f (x; Θt), y) + Rfgt(Θt),\n\n(1)\n\nwhere l(·, ·) is the loss function. The challenge of Eq.(1) is to minimize its anti-forgetting term\nRfgt(Θt) = (cid:80)t−1\nE(x,y)∼Til(f (x; Θt), y) without the access of T1, T2, ..., Tt−1.\n\ni=1\n\nMotivation:\nRecently, due to user preferences or privacy issues, there has been a growing real-\nworld desire for solving CL problems on memory-constrained local devices. In case of long-term\ntasks or small memory budget for CL learners, existing CL approaches face the following challenges:\n(1) Replay-based CL (Rolnick et al., 2019) is sometimes not available as they require extra storage\nfor a subset of previous samples and then replay them; (2) Non-replay based CL, including the\narchitecture-stable approaches with a constant weight space over time (Von Oswald et al., 2019) and\nthe expansion-based approaches that expands the weight space by adding parameters/modules over\ntime to accommodate new learned knowledge (Li et al., 2019), although without a replay buffer for\ndata, still has the limitations on parameter efficiency while dealing with the catastrophic forgetting.\nTo summary, while existing approaches mainly focus on dealing with the catastrophic forgetting, one\nunique challenge in the CL literature still remain under-explored: the trade-off between Capacity\nSaturation and Parameter Efficiency. Capacity saturation refers to the under-parameterization\nphenomenon when the parameter space of a neural network is not enough to generalize the knowledge\nof seen tasks–as the diversity and complexity of seen tasks increases over time, the neural network\ntends to be saturated in memorizing seen knowledge, which loses the ability to generalize new\nknowledge for future tasks. For example, capacity saturation happens when the growing of the\nneural network size is slower than the growing of task complexity/diversity. Parameter Efficiency\ninvolves both training-time and inference-time parameter efficiency. While inference-time parameter\ninefficiency refers to over-parameterization phenomenon when the expansion of weight space largely\nexceed the actual growing of task diversity, training-time parameter inefficiency refers that the\nsize of trainable parameters needed for achieving a good performance largely exceeds the memory\nbudget. Under memory-constrained CL scenarios, both capacity saturation and training/inference-\ntime parameter efficiency are important. Unfortunately, the existing CL approaches cannot achieve a\ngood balance between ensuring parameter efficiency and handling capacity saturation.\n\nTherefore, one unanswered question is that “how to efficiently expand the weight space to overcome\ncapacity saturation in CL while minimizing the number of parameters in the neural network?” In\norder to systematically address this question, we formulate a new CL problem, namely Diversity-\naware and Parameter-efficient CL, which is defined as follows.\n\n2\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 1: Proposed HyperGKL framework. Different tasks are represented by different colors (blue\nfor task 1, yellow for task 2, and green for task 3). The learned skills–each represents a knowledge\ntype learned from the tasks, are represented in red; different skills are represented by different shapes.\n\nDefinition 1 (Diversity-aware Parameter-efficient CL).\nexplicitly introduce the extra constraints on the parameter efficiency to the CL objective\n\nOn the basis of conventional CL, we\n\nmin\nΘt\n\nLtask(Θt) + Rfgt(Θt) + Reff\n\n1 (∇Θt) + Reff\n\n2 (Θt).\n\n(2)\n\nReff\n1 (∇Θt) = c(∇Θt) is the training-time parameter efficiency term at task t, where ∇Θt is the\nweight gradient at task t and c(·) is a function counting the size of its input gradients or weights.\nReff\n2 (Θt) = max(c(Θt) − c(Θt−1), γ∆t(T1:t)) is the inference-time parameter efficiency term at\ntask t, where γ is a scaling hyperparameter. The insight of Reff\n2 is that, in order to achieve a better\ntrade-off between capacity saturation and inference-time parameter efficiency, the weight space\nexpansion c(Θt) − c(Θt−1) should be aware of the diversity gain ∆t(T1:t) when adding the new\ntask t to seen tasks\n\n∆t(T1:t) = gdiverse(T1:t) − gdiverse(T1:(t−1)),\nwhere gdiverse(·) is a predefined or trainable function that measures the task diversity. The challenge\nof solving Eq.(2) is threefold. First, how to define gdiverse(·) to measure the task diversity reasonably?\nSecond, how to compute Eq.(3) without the access of T1, T2, ..., Tt−1? Third, considering the\ntwo parameter-efficiency terms (Reff\n2 ) are not differentiable as c(·) is a non-differentiable\nfunction, how to train the model to minimize them with other losses remains a question.\n\n1 and Reff\n\n(3)\n\n3 METHODOLOGY\n\nTo solve the Diversity-aware Parameter-efficient CL problem defined in Eq.(2), we propose a new CL\nlearning framework, namely Continual Knowledge HyperGraph Learning (HyperGKL). The main\nidea of HyperGKL is to continuously learn a dynamic latent knowledge hypergraph underlying tasks,\nwhich consists of (1) trainable embeddings of fine-grained knowledge types (namely, skills) that can\nbe shared among seen tasks and (2) hyperedges that reveal the complex task relationships and task\ndiversity of seen tasks, and then, utilize this hypergraph with a hypernetwork (HyperNet) based node\ndecoder to empower a parameter-efficient knowledge transfer from previous tasks to future task on\nthe latent knowledge space. An overview of HyperGKL is illustrated in Figure 1.\n\n3.1 MODELING TASK DIVERSITY VIA LATENT KNOWLEDGE HYPERGRAPH\nTo guarantee the balance between inference-time efficiency and capacity saturation (Reff), it is crucial\nto modeling and quantify the task diversity (gdiverse) to determine the minimum capacity increase\nneeded for learning the new tasj’s knowledge without catastrophic forgetting on the previous tasks.\n\nGroup-level v.s Parameter-level v.s. Latent Knowledge-level Measurements:\nIn recent works\nout of the parameter-efficient CL context, task diversity has been leveraged in two granularity levels:\n(1) Group level, where tasks are separated into disjoint groups (Wang et al., 2023) with each group\ncontaining tasks having similar distributions, and gdiverse(T1:t) can be the number of seen groups\ntil task t; (2) Parameter/Module level, where any pair of tasks has partial common weight space\n\n3\n\n\fUnder review as a conference paper at ICLR 2024\n\nor modules, each parameter/module denotes a piece of information, and gdiverse(T1:t) can be the\naccumulated number of parameters/modules til task t (Kang et al., 2022; Li et al., 2019). However,\nboth of them have drawbacks in the parameter-efficient CL context. Group-level measurement is\nnot accurate as it ignores common knowledge among different groups of tasks and may result in\nover-estimate of model capacity. Parameter/Module-level measurement requires greedy search in a\nlarger weight space and then apply pruning to cut weights backward, which is not efficient in training\ntime. To tackle these challenges, we propose to learn a latent knowledge space and then efficiently\nmeasure the task diversity on the latent space. On the latent knowledge space, points represent\nfine-grained knowledge types learned by tasks, which can be shared among different tasks and thus\nmake the task relationships more organized. In this paper, we call these fine-grained latent knowledge\ntypes as skills. Each skill means a type of functional ability trained on a task.\n\nLatent Knowledge Space: Suppose D is the dimension of the latent knowledge space. On the\nlatent knowledge space, we assume there exists a set of Mt unique skills seen until task t, represented\nas St = {sj ∈ RD}Mt\nk=1, where sj is the skill embedding of the skill type j. Each task i ≤ t learned\na collection of differnt skills, and we denote Ci = {sj}k∈Ii ⊆ St as a collection of embeddings of\nthe learned skills on the task i, which is a subset of St, where Ii ⊆ [Mt] is a collection of indices\nindicating the skill identities that comprise the task i. For example, as show in Figure 1(a), until t = 3\nwe have M3 = 5, I1 = {1, 2}, I2 = {1, 3, 4}, and I2 = {2, 5}.\n\nDefinition 2 (Skill-task Knowledge Hypergraph). On the latent knowledge space, each seen skill\nsj ∈ St can exist in multiple tasks and each task i may consist of multiple skills. Such skill-task\nrelationships indicate a hypergraph structure on this latent knowledge space\n\nGt = (St, Et, Ht),\n(4)\nwhere (1) the fine-grained knowledge types (skills) are vertices in the hypergraph, whose node\nfeatures are the trainable skill embeddings St ∈ R|St|×D; (2) the coarse-grained tasks Et = [t] are\ntreated as hyperedges; and (3) we let Ht = {Ii}t\ni=1 denote a collection of skill indicators of all the\nseen tasks until t, which is considered as the connections between skill nodes and represented as\na binary-valued incidence matrix Ht ∈ {0, 1}|St|×|Et|. Each entry Ht,j,i in the incidence matrix\ndenotes whether the task i learns skill type j. The hypergraph Gt is dynamic and becomes larger and\nlarger as both the number of nodes (skills) and the number of hyperedges (tasks) increase overtime.\n\nTask Diversity & Skill Disentanglement: We have modeled fine-grained task relationships on a\nlatent knowledge space, which explicitly indicates the knowledge types shared by every pair of tasks\nusing the incidence matrix. However, to successfully use the hypergraph, there are two additional\npreassumptions. The first is the knowledge diversity between skills. To encourage the diversity\nbetween skills, we introduce a negative correlation penalty term to the objective\n\nLdecomp(St) =\n\n(cid:88)\n\nk∈Mt\n\n−||St,k − (cid:99)St||2\n2\n\n(5)\n\nwhere (cid:99)St is the average of all skill embeddings in St. In addition, since each task is a combination\nof skills, the knowledge sharing between tasks is not directly but through an organized sharing\nscheme, that is, the skill indicators. Therefore, in order to encourage the skill-sharing opportunity\n(i.e., hyperedge overlapping) between tasks to improve the positive knowledge transfer, we introduce\nan association constraint. Intuitively, the more common skills two tasks share, the more close they\nare in the embeddings space; and, the more frequent two skills are used in the same task, the closer\nthey are in the skill embedding space. Based on this idea, there is an constraint between St and Ht\nwhich is inspired by hypergraph Laplacian:\n\nLgraph(St, Ht, ψmut, ψagg) = ||Ht − (cid:99)H||2\n(6)\n2\nwhere (cid:99)Hi,k = f mut(sk, ei; ψmut) and ei = f agg({sk|k ∈ [Mt], Ht,k,i = 1}; ψagg). f agg(·; ψagg) is\nthe skill aggregation function parameterized by ψagg, which combines multiple skills and reconstructs\nthe task, and f mut(·, ·; ψmut) is the measurement computing the association score of each skill k in\neach task i. Here, ψagg, ψmut, The final interence-time parameter effciency loss is\n\nReff\n\n2 (Θt, St, Ht, ψmut, ψagg) = Lgraph + Ldecomp + max(c(Θt \\ Θt−1), γ|Ct \\ St−1|).\n(7)\nWith the knowledge hypergraph with diversified skill embeddings, we can define the task diversity\nfunction as follows: given a subset of tasks TU ⊆ [t], their diversity can be calculated as the number\nof diversified knowledge types (skills) learned by tasks gdiverse(TU ; Gt) = | (cid:83)\nIi|.\n\nCi| = | (cid:83)\n\ni∈TU\n\ni∈TU\n\n4\n\n\fUnder review as a conference paper at ICLR 2024\n\n3.2 KNOWLEDGE HYPERGRAPH EVOLUTION FOR INFERENCE-TIME EFFICIENCY\n\nGranularity of Skill Submodule: We let each skill j be associated with a small trainable neural\nnetwork ϕj, a skill-specific subnetwork, embedded in the full task neural network. One straight-\nforward way of solving the last term of Eq.(7) is to let the network growing be simply guided by\nCt \\ St−1. That is, we introduce new parameters as Θt \\ Θt−1 = θt = {ϕj′}j′∈Ct\\St−1 and then\ntrain the parameters. However, the topology and number of parameters of each skill-specific network\nactually impact the capacity gain after adding each skill. This can be treated as a hyperparameter,\nwhich is the γ = c(ϕj′) in Eq.(2). For simplicity, we use the same architecture for each skill-specific\nnetwork. Specifically, we use a autoencoder-based low-rank network for each skill. Therefore, we\ncan rewrite: max(c(Θt \\ Θt−1), γ|Ct \\ St−1|) = |Ct \\ St−1|.\n\nNovel Knowledge Discovery & Hypergraph Growth: When leveraging network growth that\nexpands the weight space to mitigate capacity saturation and forgetting, we anticipate that the network\ngrowth should be as small as possible, that is, minimizing inference-time parameter efficiency. We\nresort to optimizing Eq.(7) to achieve this goal. One remaining challenge is that the third term of\nEq.(7) is non-differentiable and forbidding of using gradient descent. To handle this challenge, we\nproposed the following searching steps to find the optimal new set of skills Ct \\ St−1 for every new\ntask: 1) identifying any existing skills shared with the current task; 2) identifying the number of new\nskills needed by the current task; 3) adding modules for new skills and train the model. Specifically,\nto minimize |Ct \\ St−1|, we split Ct = {Cpre\n, Cnew\nt ⊂ St−1 is seen in\nt\nprevious tasks and Cnew\nis the new skills needed by the current task, and we aim to solve the two\nt\nparts in separate steps. Given the previously trained skill embeddings St−1 ∈ R|St−1|×D and the\nassociated skill submodules Θt−1 = {ϕj|j ∈ |St−1|}, we first retrieve at most k skill submodules\nthat give the best prediction results, where k is an upperbound hyperparameter. We use the validation\nset of Tt and compute f (x; ϕj) to obtain the validation errors using each skill submodule ϕj. Then,\nwe jointly train the top-k submodules and compute the validation error, which is compared with an\nerror threshold ϵ. If the existing skills do not achieve a satisfied validation error, we then introduce\nnew skills. In order to determine how many new skills are need by the current task, we employ\nevolutionary search with a search space of k.\n\n} into two parts, where Cpre\n\nt\n\n3.3 HYPERNET-BASED NODE DECODER FOR TRAINING-TIME EFFICIENCY\n\nWhile attempting to minimize the training-time efficiency, i.e. Reff\n1 (∇Θt) in Eq.(2), we consider two\nreasons that may lead to the inefficiency during the training time: (1) the number of skill-specific\nsubnetworks increases over time as the number of tasks increases, and the strategy searching for new\nskills may revisit these previously learned skill networks. Storing a large number of skill-specific\nnetworks is forbidden with memory limits. (2) When the number of maximum skills k per task is\nlarge, the trainable parameters at each task tend to exceed the memory budget. To overcome these\nchallenges, we can also borrow the benefit of the latent knowledge hypergraph learned in the previous\nsections. We propose to operate the knowledge transfer from previous tasks to the current task on the\nlatent knowledge space, instead of on the original task network’s weight space. To achieve this, we\nleverage a HyperNet-based node decoder to bridge the gap between the latent knowledge space and\nthe task networks’ weight space.\n\nt\n\n, θadpt\n\nt } consists of base modules θbase\n\nNode Decoder on Hypergraph (Parallel Parameter Generation Trick): Assuming a task network\nθt = {θbase\nshared by all tasks and task-adaptive modules\nθadpt\nt = {ϕt,j|j ∈ It}, where each adaptive module consists of multiple submodules, each of which\nis responsible to learn a skill type j on the task. We leverage a HyperNet-based node decoder applied\nto each skill node in a task-specific hyperedge in the hypergraph Gt. For each previous task i, with\nits skill set information It encoded in the incidence matrix, the adaptive module of task t can be\ngenerated through a parallel parameter generation trick\n\nt\n\nθadpt\nt = {h(sj; Ψ)|j ∈ It}\n\n(8)\n\nwhere h(·; Ψ) : RD → RF is a HyperNet shared by all skills (all nodes), which takes as input a skill\nembedding and generates the weights of the subnetwork corresponding to the input skill. Ψ denotes\ntrainable parameters and F is the dimension of the weight space of each skill subnetwork. As shown\nin Figure 1, the HyerNet is used as a node decoder on the knowledge hypergraph, and therefore, can\n\n5\n\n\fUnder review as a conference paper at ICLR 2024\n\nbe trained in an end-to-end manner using the loss from the task. The forward function of the task\nnetwork is f (x; θbase\n\nf (x; θbase\n\n, h(sj; Ψ)).\n\n(cid:80)\n\n, θadpt\nt\n\nt\n\nt\n\n) = 1\n|Ii|\n\nj∈It\n\nTraining-time Efficiency. While the weight space of the generator Ψ is fixed over time, which is\nefficient in parameter consumption, the task networj’s weight space θadpt\nt = {ϕi,j|j ∈ It} generated\nby Ψ is dynamic and corresponds to the capacity needs. As a result, although the weight space\nof the task network expands over time according to the needed capacity increase, the number of\ntrainable weights remain constant over time, that is, Reff\n) = a(Ψ). The reason of using the\nhypernetwork to improve the training-time efficiency is that the number of seen skills usually grow\nup to hundreds or thousands in long-term task-incremental CL. Directly learning and discovering\nprevious skills that are useful to the current task requires simultaneously training a large number of\nskill modules in the early stage, which is not quite efficient. In additional, the task networks can be\ngenerated by passing multiple skills through the node-level HyperNet in parallel in a batch.\n\n2 (∇Θadpt\n\nt\n\n4 EXPERIMENTS\n\nDatasets: We use four popular continual learning datasets. (1) Permuted MNIST consists of 10\ntasks, where each task is a variant of MNIST (LeCun, 1998) after applying a task-personalized\ndeterministic permutation to the input image pixels of all input images. (2) Omniglot Rotation is\ncomposed of 100 tasks constructed from the raw Omniglot (Yoon et al., 2019) having more than\n1200 classes. Each task has 12 distinct classes, which is generated from the raw images by adding\ntheir rotation version of 90, 180, and 270 degrees. (3) 5-Datasets is a mixture of 5 different vision\ndatasets (Saha et al., 2021), including MNIST (task 1), CIFAR-10 (Krizhevsky et al., 2009) (task\n2), SVHN (Netzer et al., 2011) (task 3), FashionMNIST (Xiao et al., 2017) (task 4), and notMNIST\n(Bulatov, 2011) (task 5). (4) CIFAR100-10 is constructed by dividing the 100 classes of CIFAR-100\n(Krizhevsky et al., 2009) into 10 tasks with 10 distinct classes per task. Task samples do not repeat\nover tasks. In Omniglot Rotation, 5-Datasets, and CIFAR100-10, tasks has their unique label spaces.\n\nBaselines: We compared our method with two families of non-rehearsal-based CL baselines. (1)\nRegularization-based approaches, where the model architecture remains fixed over tasks and the\ncatastrophic forgetting problem is handled by regularization techniques, include Naive FINETUNE\n(a naive sequential training strategy where a single model is trained continually on sequentially\ncoming tasks), EWC (Kirkpatrick et al., 2017), HAT (Serra et al., 2018), GPM (Saha et al., 2020),\nand the HyperNet-based method HCL (von Oswald et al., 2020; Hemati et al., 2023); (2) Expansion-\nbased approaches, where the model architecture and the parameter space dynamically expands and\nchanges over time, include Independent (a naive strategy where each task learns a new model from\nscratch without using the previously learned models), Learn to Grow (Li et al., 2019), PAR (Wang\net al., 2023) (grouping tasks and different tasks either share all weights or share no weight), SupSup\n(Wortsman et al., 2020) (finding supermasks within a randomly initialized network for each task),\nand WSN (Kang et al., 2022) (jointly training weights and find supermasks for each task). Moreover,\nwe add \"Multitask Learning with SparseMoE (MTL-MoE)\" baseline, where all the tasks are learned\nsimultaneously in a SparseMoE model (Gupta et al., 2022). MTL-MoE is not a CL approach but will\nserve as upper bound on average accuracy on all tasks.\n\nModel Architectures & Hyperparameters: We use a multi-head configuration for all experiments,\nwhere each task train their own classification head function. For the backbone, to demonstrate\nfair comparisons between different methods, we encourage all the methods to start with the same\ninference-time model architecture at the first task. Consider that several model layers in the proposed\nHyperGKL consist of multiple parallel subnetworks (i.e., skill submodules), the baselines also follow\nsuch multi-subnetwork paralleled structures. For regularization-based baselines and Independent,\nwe use two-layered MLP with k parallel subnetworks per layer and 100 neurons per subnetwork for\nPermutated MNIST, use LeNet (Al-Jawfi, 2009) with k parallel subnetworks at the second CNN layer\nfor Omniglot Rotation, use AlexNet (Serra et al., 2018) with k parallel subnetworks at the second\nCNN layer and the first fully-connected layer for CIFAR100-10, and use ResNet18 (Kang et al., 2022)\nwith k parallel subnetworks at the first CNN layers in block 1 & 3 for 5-Datasets. PAR and Learn to\nGrow begin with k parallel subnetworks per layer and then expand the number of subnetworks using\ntheir expansion algorithms. For SupSup, WSN, and MTL-MoE, we use a supernetwork having similar\nmodel structures except that ρT k parallel subnetworks at each compositional layer. While each task\nin SupSup and WSN learns a sparse task-adaptive parameter allocation mask for its inference model,\nMTL-MoE learns an additional gating function to route each task to k subnetworks.\n\n6\n\n\fUnder review as a conference paper at ICLR 2024\n\nTable 1: Comparison of performance and resource utilization between different methods. For model\narchitectures, we use ρ = 0.6, k = 4, D = 8 for “Permuted MNIST” and ρ = 0.3, k = 4, D = 20 for\n“Omniglot Rotation”. 1: Non-CL baseline (as upper bound). 2: Regularization-based CL approaches.\n3: Expansion-based CL approaches. N/A: Not Applicable. TPS and IPS values are shown in millions.\n\n# Method\n\n1 MTL-MoE\n\n2\n\n3\n\nNaive FINETUNE\nEWC\nGPM\nHCL\n\nIndependent\nLearn to Grow\nPAR\nSupSup\nWSN\n\nOurs\n\nPermuted MNIST (10 tasks)\n\nOmniglot Rotation (100 tasks)\n\nACC↑ BWT↑ TPS↓\n\nIPS↓ ACC↑ BWT↑ TPS↓\n\nIPS↓\n\n0.978\n\n0.782\n0.920\n0.944\n0.949\n\n0.783\n0.918\n0.958\n0.963\n0.964\n\n0.966\n\nN/A\n\n-0.210\n-0.031\n-0.024\n-0.022\n\n-0.261\n-0.004\n-0.013\n0\n0\n\n-0.004\n\n2.13\n\n0.35\n0.35\n0.35\n3.19\n\n0.35\n3.55\n0.35\n3.55\n2.13\n\n0.99\n\n0.35\n\n0.35\n0.35\n0.35\n0.35\n\n0.35\n2.13\n0.35\n2.13\n0.35\n\n0.35\n\n0.884\n\n0.614\n0.714\n0.834\n0.842\n\n0.493\n0.820\n0.805\n0.581\n0.856\n\n0.864\n\nN/A\n\n48.81\n\n25.45\n\n-0.424\n-0.114\n-0.032\n-0.010\n\n-0.502\n-0.007\n-0.028\n0\n0\n\n25.45\n25.45\n25.45\n41.23\n\n25.45\n44.71\n25.45\n44.71\n48.81\n\n25.45\n25.45\n25.45\n25.45\n\n25.45\n48.81\n25.45\n48.81\n25.45\n\n-0.008\n\n28.53\n\n25.45\n\n(cid:80)T\n\nj=1 AT,j. BWT measures the average forgetting on past tasks: BWT = 1\nT −1\n\nEvaluation Metrics: We evaluate our approach against baselines with respect to both the overall\nperformance and model efficiency. (1) First, following (Kang et al., 2022), we measure the overall\nperformance of CL approaches using two metrics: Average Accuracy (ACC) and Backward\nTransfer (BWT). Suppose Ai,j denotes the test accuracy for task j after training on task i. ACC\nmeasures the average of the classification accuracy on all tasks using the final model: ACC =\n1\nj=1 AT,j −Aj,j.\nT\nNegative BWT means forgetting. (2) Second, we use two metrics to measure the overall model\nefficiency of CL approaches to show their resource utilization: Trainable Parameter Size (TPS) and\nInference-time Parameter Size (IPS). Suppose Strain\ndenotes the number of trainable parameters\non task i and Sinfer\ndenotes the number of all parameters in the inference model on task i. TPS is the\nmaximum number of trainable parameters over all tasks: TPS = maxT\n. IPS is the number of\nparameters for the largest inference models over all tasks: IPS = maxT\n. For fair comparison,\nwe encourage all comparative methods to initialize with a same-sized inference model at the first task.\n\ni=1 Strain\ni\ni=1 Sinfer\ni\n\n(cid:80)T −1\n\ni\n\ni\n\nDetails on datasets, implementation, and hyperparameters are provided in Appendix.\n\n4.1 MAIN RESULTS\n\nWe report the results after training the final task in Table 1, Table 2 (in appendix), and Figure 2,\nwhere the proposed method is compared against all baselines on 4 datasets. All experiments run on a\nsingle-GPU of NVIDIA A100. Each experiment repeatedly run 5 times with different random seeds.\n\nPerformance Comparison. In general, our proposed approaches demonstrated superior ACC and\nBWT performance rather than baselines. In particular, we observe that the methods using adaptive\npartial weight sharing (WSN and our method) typically outperformed the full-weight-sharing and\nzero-weight-sharing baselines with a large margin, which demonstrates that adaptive partial weight\nsharing is crucial in overcoming forgetting in CL problems. In additional, the proposed HyperGKL\noutperformed WSN in ACC while slightly tradeoff the BWT under the same experimental setting.\nSuch a BWT is relatively small in comparison with other effective baselines and might be due to the\nskill-level HyperNet retraining over tasks. Yet we obtain a better ACC performance as WSN baseline\nsometimes suffered from structure disruption due to model sparsity.\n\nResource Utilization Comparison. To create the same inference models, CL approaches based on\npartial weight sharing (including our approach, SupSup and WSN) typically requires larger TPS\nthan the full/zero-weight-sharing ones (except HCL). The extra parameters are needed to discover\nthe correct weight subspace shared among tasks for better performance. This can be observed from\nthe TPS and IPS results in Table 1 and Table 2. While a larger TPS is usually necessary for better\nperformance, among the partial-weight-sharing methods, our approach demonstrated the smallest\n\n7\n\n\fUnder review as a conference paper at ICLR 2024\n\nTPS, which shows the best model efficiency. We successfully leveraged the fine-grained HyperNet to\nsave the computation resource during the discovery of inter-task shareable weight subspaces.\n\nFigure 2: Comparison of Performance-Efficiency Trade-offs (a) on “CIFAR100-10” consisting of 10\ntasks, where we show the impact of the embedding dimension D = 1, 4, 8 of HyperNet; and (b) on\n“5-Datasets” with 5 tasks, showing the impact of the maximum skill number per task k = 1, 5, 10.\nComparison of Performance-Efficiency Trade-offs. To provide a more comprehensive evaluation\nof CL methods, we illustrate their performance-efficiency trade-offs in Figure 2, where the x-axis\nshows the TPS and the y-axis shows the ACC. For fair comparison, all methods within the same\nfigure, if using the same D or k, start with a same-sized inference model at the first task. Methods on\nthe top-left corner have better trade-offs: higher ACC with smaller TPS. Compared with full/zero-\nweight-sharing methods (PAR, Independent, and regularization-based methods except HCL), our\nproposed partial-weight-sharing method achieves higher ACC with nearly the same TPS under the\nmemory budget. The HyperNet-based CL method (HCL) requires a very high TPS for coarse-grained\nweight space learning, which significantly exceeds the memory budget and might suffer from both\ncapacity saturation (if task architecture is too small) and overfitting at early tasks (if task network\nis too large). In contrast, our method learns finer-grained weight spaces with latent knowledge\nrelationship discovery, which reduces the two limitations. Other partial-weight-sharing methods\n(SupSup and WSN) continuously expand the weight space and thus require large TPS; instead, our\nmethod leverage a fix-sized HyperNet to disentangle the dynamic-sized weight space.\n\nVisualization of Learned Knowledge Hypergraph. Figure 3 visualizes the incidence matrix of the\nlearned knowledge hypergraph on CIFAR100-10 among 10 tasks. Each row i of an incidence matrix\nindicate the task Ti; each column j indicates the discovered skill with index j. Entries are binary;\nthe entry (i, j) equals to one (denoted as green squares) denotes that the skill j is discovered and\nlearned at task i. We consider the same inference model with k = 3 and compare the weight sharing\nstrategies pre-defined by Independent (Figure 3(d)), pre-defined by regularization-based methods\n(Figure 3(e)), learned by PAR via task grouping (Figure 3(f)), and learned by the proposed method\n(Figure 3(a-b)). SupSup and WSN do not learn the disentangled knowledge/skills and thus do not\nparticipate in the visualization. While Figure 3(d) shows none weight sharing among tasks, Figure\n3(e) shows complete/full weight sharing among tasks, and Figure 3(f) shows either full or none\nweight sharing among tasks, Figure 3(a-b) demonstrate the partial weight sharing strategy learned\nby our method, which achieved the best ACC performance. Also, while baselines doe not explicitly\ndemonstrate how much knowledge is transferred among tasks, our CL approach leverages the latent\nhypergraph that explicitly provides the interpretability of knowledge transfer.\n\n4.2 ABLATION STUDIES\nImpacts of Task/Skill Embedding Size (D). Figure 2(a) illustrates the influence of the input\nembedding dimension of HyperNet. In general, for both HCL and the proposed method, larger D\nresults in better ACC performance and larger TPS. With the same D, the TPS of our method is\nconstantly smaller than that of HCL. This is because the HCL generates the entire weight space\n(with HyperNet complexity O(kD)), while our method generates the weight subspace for each\nof fine-grained knowledge types (with HyperNet complexity O(D)). We also observe that given\nlarger D >= 4, our approach achieved better ACC performance than HCL. The larger D implies\nhigher potential to represent knowledge disentanglement in our approach and higher potential to\nrepresent task difference in HCL. However, HCL is limited to a fixed weight space and thus cannot\nobtain the best model capacity that fit each task’s complexity, especially when task complexity shifts\ndynamically. In contrast, our method can generate capacity-aware networks with different widths.\n\n8\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 3: Visualization of the incidence matrix of the latent knowledge hypergraph learned by\nour proposed method on CIFAR100-10. The performance: (a) ACC=0.830, (b) ACC=0.831, (c)\nACC=0.834, (d) ACC=0.671, and (e) ACC=0.692. Among them, (d) and (e) are edge cases. (d)\nand (e) also demonstrates the weight sharing of Independent and regularization-based approaches,\nrespectively. (f) shows the learned weight sharing scheme of PAR with ACC=0.816 performance.\n\nMoreover, since our knowledge transfer happens on the fine-grained implicit skill level, our partial-\nweight-sharing strategy can potentially avoid negative knowledge transfer, yet the full-weight-sharing\nstrategy of HCL may transfer conflicting knowledge from task to task.\n\nImpacts of Maximum Skill Number per Task (k).\nFigure 2(b) illustrates the impact of the\nmaximum number of skills per task. In general, given the same k value, our approach achieved better\nACC performance than baselines. Aor all approaches, the experiment with larger k (k ≤ 10) obtains\nhigher ACC. Basically, we can increase k to get a better model. However, TPS of all baselines would\nbe traded off for larger k and higher ACC. In contrast, the TPS of our method is nearly constant for\ndifferent k, which suggests that our method is more resource-friendly during the training time rather\nthan baselines. Given a memory budget in the real-world application, there is upper bounds for k and\nACC for baselines, while our method can use a larger k than the baseline’s upper bound.\n\nImpacts of Evolution Plasticity (ϵ). Figure 3(a), (b), (d), and (e) shows the impact of ϵ on the latent\nknowledge hypergraph and the overall ACC performance, while fixing k = 3. The larger the ϵ, the\nmore skills shared among tasks and there is more chance for weight sharing among different tasks’\nnetworks. A special case ϵ = 1 shown in Figure 3(e) indicates full weight sharing. The performance\nof Figure 3(a)(b) is significantly better than Figure 3(e) because an over-large ϵ might results in\nnegative transfer. Inversely, the smaller ϵ, the less skills shared among tasks. However, an over-small\nϵ might results in insufficient positive transfer, meaning that some useful knowledge in previous tasks\nis not utilized by future tasks. A special case ϵ = 0 shown in Figure 3(d) indicates none weight\nsharing, whose performance is significantly lower than Figure 3(a)(b). Therefore, it is crucial to\nsearch a middle-valued ϵ that can obtain an optimal knowledge hypergraph among tasks. In practice,\nwe searched ϵ ∈ [0, 1] and found the best ϵ = 0.05 that gives the best ACC performance.\n\n5 CONCLUSIONS\nIn this work, we have addressed the challenging problem of task-incremental CL by introducing a\nnovel framework that focuses on the trade-off between parameter efficiency and capacity saturation.\nOur approach recognizes that the existing CL methods struggle to balance these two crucial aspects,\nleading to vulnerabilities in long-term task-incremental settings with limited memory resources. To\novercome this issue, we have introduced the concept of diversity-aware and parameter-efficient CL,\nwhere we leverage a unique knowledge hypergraph structure to capture task diversity and estimate the\nrequired capacity increase for each new task. Moreover, we have introduced techniques for optimizing\nthe hypergraph growth and ensuring parameter efficiency through fine-grained parameter generation\nusing a fixed-sized hypernetwork. Overall, our framework develops a robust CL solution that can\nadapt to the evolving complexities of tasks while minimizing the number of trainable parameters.\n\nThe related work is provided in the Appendix.\n\n9\n\n\fUnder review as a conference paper at ICLR 2024",
    "abstract": "Continual learning (CL) refers to the ability of models to learn from non-stationary data distribution while transferring and protecting past knowledge. Existing literature in CL has mainly focused on overcoming catastrophic forgetting. However, they often overlook a critical trade-off between parameter efficiency and capacity saturation. Almost all of the existing approaches including architecture-stable and architecture-growing methods struggle to balance parameter efficiency and capacity saturation. This makes them vulnerable to long-term task-incremental CL under storage constraints. In this paper, we propose a novel CL approach that addresses the trade-off between parameter efficiency and capacity saturation by dynamically expanding the model's weight space in proportion to the actual capacity increase needed by each new task. Specifically, our approach introduces a unique knowledge hypergraph structure that captures the latent knowledge across tasks and leverages it to measure task diversity and estimate the capacity increase required for each new task. Moreover, we introduce new constraints to ensure parameter efficiency during inference and a fine-grained parameter generator to create task-specific sub-networks that ensure a constant number of trainable parameters over time while accommodating the evolving complexities of tasks. Extensive experiment results show that the proposed approach achieves state-of-the-art results on several benchmark CL datasets, while maintaining low parameter counts.",
    "response": {
      "Soundness": 3,
      "Presentation": 2,
      "Contribution": 2,
      "Rating": 5,
      "Confidence": 4,
      "Strengths": "- The trade-off between capacity saturation and parameter efficiency is an interesting point that is not discussed frequently\n- The idea of modeling a task diversity using hyper graphs seem novel",
      "Weaknesses": "- The setting in which capacity saturation and parameter efficiency really matters is, as mentioned in the manuscript, when CL methods are deployed on edge devices with little memory. This is the main reason why replay-based CL methods are seldom compared to in the submission but I disagree. If using a replay buffer is too costly, what's to say that expanding the model size (however efficiently that might be) is not too costly? Replay based CL methods can be understood as having the best parameter efficiency (as the model size does not grow) and tackling the capacity saturation issue with past task samples in the replay buffer. I think more comparisons both in terms of concepts and experiments should be done against replay based CL methods to really justify the proposed trade-off in the paper.",
      "Questions": "Please see the weaknesses."
    }
  },
  {
    "paper_id": "YxvmODVWny",
    "summary": "Background: Natural language and images are typically used for goal specification in goal-conditioned imitation learning, but each has limitations. Language can be ambiguous, and images can be over-specified. The paper explores hand-drawn sketches as a new modality, which is easy to create like language but more spatially aware like images.\nGap: The gap identified is the lack of a goal representation that balances the intuitive, easy-to-provide nature of language with the spatial precision of images, all while avoiding the overspecification issue inherent in images and the ambiguity in natural language for goal-conditioned imitation learning.\nHypothesis: Hand-drawn sketches can serve as an effective goal representation in visual imitation learning by providing a balance between abstraction and spatial specificity, leading to better task performance, especially in the presence of ambiguities or visual distractors.\nMethod: The approach involves developing RT-Sketch, a policy that uses hand-drawn sketches for manipulation tasks. The model is trained on a dataset of 80K trajectories paired with synthetically generated sketches. An image-to-sketch translation network is used to generate goal sketches from images which are then used to train the policy using imitation learning. Experimentation is conducted across six manipulation tasks with varying scene complexities on real robots.\nConclusion: RT-Sketch is shown to perform on par with language and image-conditioned policies in straightforward settings while demonstrating greater robustness to language ambiguity and visual distractions. The model also adapts well to sketches with varying details, from simple lines to colored drawings, indicating its flexibility and potential practical advantages in real-world applications.\n",
    "full text": "\nRT-SKETCH: GOAL-CONDITIONED IMITATION\nLEARNING FROM HAND-DRAWN SKETCHES\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nNatural language and images are commonly used as goal representations in goal-\nconditioned imitation learning (IL). However, natural language can be ambiguous\nand images can be over-speciﬁed. In this work, we study hand-drawn sketches as\na modality for goal speciﬁcation. Sketches are easy for users to provide on the\nﬂy like language, but similar to images they can also help a downstream policy\nto be spatially-aware and even go beyond images to disambiguate task-relevant\nfrom task-irrelevant objects. We present RT-Sketch, a goal-conditioned policy\nfor manipulation that takes a hand-drawn sketch of the desired scene as input,\nand outputs actions. We train RT-Sketch on a dataset of paired trajectories and\ncorresponding synthetically generated goal sketches. We evaluate this approach\non six manipulation skills involving tabletop object rearrangements on an articu-\nlated countertop. Experimentally we ﬁnd that RT-Sketch is able to perform on a\nsimilar level to image or language-conditioned agents in straightforward settings,\nwhile achieving greater robustness when language goals are ambiguous or visual\ndistractors are present. Additionally, we show that RT-Sketch has the capacity\nto interpret and act upon sketches with varied levels of speciﬁcity, ranging from\nminimal line drawings to detailed, colored drawings. For supplementary material\nand videos, please refer to our website.1.\n\n1\n\nINTRODUCTION\n\nRobots operating alongside humans in households, workplaces, or industrial environments have an\nimmense potential for assistance and autonomy, but careful consideration is needed of what goal\nrepresentations are easiest for humans to convey to robots, and for robots to interpret and act upon.\n\nInstruction-following robots attempt to address this problem using the intuitive interface of natural\nlanguage commands as inputs to language-conditioned imitation learning policies (Brohan et al.,\n2023b;a; Karamcheti et al., 2023; Lynch & Sermanet, 2020; Lynch et al., 2023). For instance,\nimagine asking a household robot to set the dinner table. A language description such as “put\nthe utensils, the napkin, and the plate on the table” is under-speciﬁed or ambiguous. It is unclear\nhow exactly the utensils should be positioned relative to the plate or the napkin, or whether their\ndistances to each other matter or not. To achieve this higher level of precision, a user may need\nto give lengthier descriptions such as “put the fork 2cm to the right of the plate, and 5cm to the\nleftmost edge of the table.”, or even online corrections (“no, you moved too far to the right, move\nback a bit!”) (Cui et al., 2023; Lynch et al., 2023). While language is an intuitive way to specify\ngoals, its qualitative nature and ambiguities can make it both inconvenient for humans to provide\nwithout lengthy instructions or corrections, and for robot policies to interpret for downstream precise\nmanipulation.\n\nOn the other hand, using goal images to specify objectives and training goal-conditioned imitation\nlearning policies either paired with or without language instructions has shown to be quite successful\nin recent years (Jiang et al., 2022; Jang et al., 2022). In these settings, an image of the scene in its\ndesired ﬁnal state could fully specify the intended goal. However, this has its own shortcomings:\naccess to a goal image is a strong prior assumption, and a pre-recorded goal image can be tied to a\nparticular environment, making it difﬁcult to reuse for generalization.\n\n1http://rt-sketch-anon.github.io\n\n1\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 1: (Left) Qualitative rollouts comparing RT-Sketch, RT-1, and RT-Goal-Image, (right) high-\nlighting RT-Sketch’s robustness to (top) ambiguous language and (bottom) visual distractors.\n\nBetween natural language, which lacks granularity to unambiguously specify goals, and images,\nwhich overspecify goals in unnecessary detail, leading to the need for internet-scale data for\ngeneralization, we recognize that current frameworks lack a goal representation which adequately\ncaptures user intent in a convenient yet expressive manner. While natural language is highly ﬂexible,\nit can also be highly ambiguous or require lengthy descriptions. This quickly becomes difﬁcult in\nlong-horizon tasks or those requiring spatial awareness. Meanwhile, goal images over-specify goals\nin unnecessary detail, leading to the need for internet-scale data for generalization.\n\nTo this end address these challenges, we study hand-drawn sketches as a convenient yet expressive\nmodality for goal speciﬁcation in visual imitation learning. By virtue of being minimal, sketches\nare still easy for users to provide on the ﬂy like language Yet unlike language, they (1) provide\nmore, but allow for more spatial-aware task speciﬁcation. Like goal images, sketches readily in-\ntegrate with off-the-shelf policy architectures that take visual input, but provide an added level of\ngoal abstraction that ignores unnecessary pixel-level details. Finally, the quality and selective in-\nclusion/exclusion of details in a sketch can help a downstream policy distinguish task relevant from\nirrelevant details. without needing to faithfully preserve pixel-level details as in an image, and (2)\nhelp a downstream policy disambiguate task-relevant from -irrelevant objects based on their selective\ninclusion, exclusion, or level of detail. Furthermore, sketches readily integrate with off-the-shelf\npolicy architectures that take visual representations as input.\n\nIn this work, we present RT-Sketch, a goal-conditioned policy for manipulation that takes a user-\nprovided hand-drawn sketch of the desired scene as input, and outputs actions. The novel architec-\nture of RT-Sketch modiﬁes the original RT-1 language-to-action Transformer architecture (Brohan\net al., 2023b) to consume visual goals rather than language, allowing for ﬂexible conditioning on\nsketches, images, or any other visually representable goals. To enable this, we concatenate a goal\nsketch and history of observations as input before tokenization, omitting language. We train RT-\nSketch on a dataset of 80K trajectories paired with synthetically produced goal sketches, generated\nby an image-to-sketch stylization network trained from a few hundred image-sketch pairs.\n\nWe evaluate RT-Sketch across six manipulation skills on real robots involving tabletop object rear-\nrangements on a countertop with drawers, subject to a wide range of scene variations. These skills\ninclude moving objects near to one another, knocking a can sideways, placing a can upright, closing\na drawer, and opening a drawer. Experimentally, we ﬁnd that RT-Sketch performs on a similar level\nto image or language-conditioned agents in straightforward settings. When language instructions\nare ambiguous, or in the presence of visual distractors, we ﬁnd that RT-Sketch achieves ∼ 2X more\nspatial precision and alignment scores, as assessed by human labelers, over language or goal image-\nconditioned policies (see Fig. 1 (right)). Additionally, we show that RT-Sketch can handle different\nlevels of input speciﬁcity, ranging from rough sketches to more scene-preserving, colored drawings\n(see Fig. 1 (left)).\n\n2\n\n\fUnder review as a conference paper at ICLR 2024\n\n2 RELATED WORK\n\nIn this section, we discuss prior methods for goal-conditioned imitation learning which operate on\ntraditional goal representations. We also highlight ongoing efforts towards image-sketch conversion,\nwhich open new possibilities for goal-conditioning modalities which are underexplored in robotics.\n\nGoal-Conditioned Imitation Learning Despite the similarity in name, our learning of manipu-\nlation policies conditioned on hand-drawn sketches of the desired scene is different from the notion\nof policy sketches (Andreas et al., 2017), symbolic representations of task structure describing its\nsubcomponents. Reinforcement learning (RL) is not easily applicable in our scenario, as it is non-\ntrivial to deﬁne a reward objective which accurately quantiﬁes alignment between a provided scene\nsketch and states visited by an agent during training. We instead focus on imitation learning (IL)\ntechniques, particularly the goal-conditioned setting (Ding et al., 2019).\n\nGoal-conditioned IL has proven useful in settings where a policy must be able to handle spatial or\nsemantic variations for the same task (Argall et al., 2009). These settings include rearrangement of\nmultiple objects (Brohan et al., 2023b;a; Lynch et al., 2023; Manuelli et al., 2019), kitting (Zakka\net al., 2020), folding of deformable objects into different conﬁgurations (Ganapathi et al., 2021), and\nsearch for different target objects in clutter (Danielczuk et al., 2019). However, these approaches\ntend to either rely on language (Brohan et al., 2023b; Lynch & Sermanet, 2020; Lynch et al., 2023;\nKaramcheti et al., 2023; Shao et al., 2020), or goal images (Danielczuk et al., 2019) to specify vari-\nations. Follow-up works enable multimodal conditioning on either goal images and language (Jang\net al., 2022), in-prompt images (Jiang et al., 2022), or image embeddings (Manuelli et al., 2019;\nZakka et al., 2020; Ganapathi et al., 2021). However, all of these representations are ultimately\nderived from raw images or language in some way, which overlooks the potential for more abstract\ngoal representations that are easy to specify but preserve spatial awareness, such as sketches.\n\nIn addition to their inﬂexibility in terms of goal representation, goal-conditioned IL tends to overﬁt\nto demonstration data and fails to handle even slight distribution shift in new scenarios (Ross et al.,\n2011). For language-conditioning, distribution shift can encompass semantic or spatial ambiguity,\nnovel instructions or phrasing, or unseen objects (Jang et al., 2022; Brohan et al., 2023b). Goal-\nimage conditioning is similarly susceptible to out-of-distribution visual shift, such as variations in\nlighting or object appearances, or unseen background textures (Burns et al., 2022; Belkhale et al.,\n2023). We instead opt for sketches which are minimal enough to combat visual distractors, yet\nexpressive enough to provide unambiguous goals. Prior work, including (Barber et al., 2010) and\n(Porﬁrio et al., 2023), have shown the utility of sketches over pure language for navigation and\nlimited manipulation settings. However, the sketches explored in these works are largely intended\nto guide low-level motion at the joint-level for manipulation, or provide explicit directional cues\nfor navigation. Cui et al. (2022) considers sketches amongst other modalities as an input for goal-\nconditioned manipulation, but does not explicitly train a policy conditioned on sketches. They thus\ncame to the conclusion that the scene image is better than the sketch image at goal speciﬁcation. Our\nresult is different and complementary, in that policies trained to take sketches as input outperform\na scene image conditioned policy, by 1.63x and 1.5x in terms of Likert ratings for perceived spatial\nand semantic alignment, subject to visual distractors.\n\nyears,\n\nrecent\n\ngained\n\nsketches\n\nquestion\n\nanswering\n\nhave\napplications\n\n(Qiu et al., 2022; Qiu et al., 2023),\n\ncomputer vision community for\n\nImage-Sketch Conversion In\nincreasing\nsuch as object\npopularity within the\n(Chowdhury et al., 2023a; Chowdhury et al., 2023; Chowdhury et al., 2022),\ndetection\nvisual\nscene\nunderstanding (Chowdhury et al., 2023b), either in isolation or in addition to text and images.\nWhen considering how best to incorporate sketches in IL, an important design choice is whether\nto take sketches into account (1) at test time (i.e., converting a sketch to another goal modality\ncompatible with a pre-trained policy), or (2) at training time (i.e., explicitly training an IL policy\nconditioned on sketches). For (1), one could ﬁrst convert a given sketch to a goal image, and then\nroll out a vanilla goal-image conditioned policy. This could be based on existing frameworks for\nsketch-to-image conversion, such as ControlNet (Zhang & Agrawala, 2023), GAN-style approaches\n(Koley et al., 2023), or text-to-image synthesis, such as InstructPix2Pix (Brooks et al., 2023) or\nStable Diffusion (Rombach et al., 2022). While these models produce photorealistic results under\noptimal conditions, they do not jointly handle image generation and style transfer, making it\n\nand\n\n3\n\n\fUnder review as a conference paper at ICLR 2024\n\nunlikely for generated images to match the style of an agent observations. At the same time, these\napproaches are susceptible to producing hallucinated artifacts, introducing distribution shifts (Zhang\n& Agrawala, 2023).\n\nBased on these challenges, we instead opt for (2), and consider image-to-sketch conversion tech-\nniques for hindsight relabeling of terminal images in pre-recorded demonstration trajectories. Re-\ncently, Vinker et al. (2022b;a) proposes networks for predicting Bezier curve-based sketches of\ninput image objects or scenes. Sketch quality is supervised by a CLIP-based alignment metric.\nWhile these approaches generate sketches of high visual ﬁdelity, test-time optimization takes on the\norder of minutes, which does not scale to the typical size of robot learning datasets (hundreds to\nthousands of demonstration trajectories). Meanwhile, conditional generative adversarial networks\n(cGANs) such as Pix2Pix (Isola et al., 2017) have proven useful for scalable image-to-image trans-\nlation. Most related to our work is that of Li et al. (2019), which trains a Pix2Pix model to produce\nsketches from given images on a large crowd-sourced dataset of 5K paired images and line draw-\nings. We build on this work to ﬁne-tune an image-to-sketch model on robot trajectory data, and\nshow its utility for enabling downstream manipulation from sketches.\n\n3 SKETCH-CONDITIONED IMITATION LEARNING\n\nIn this section, we will ﬁrst introduce our problem of learning a sketch-conditioned policy. We will\nthen discuss our approach to train an end-to-end sketch-to-action IL agent. First, in Section 3.1, we\ndiscuss our instantiation of an auxiliary image-to-sketch translation network which automatically\ngenerates sketches from a reference image. In Section 3.2, we discuss how we use such a model to\nautomatically hindsight relabel an existing dataset of demonstrations with synthetically generated\ngoal sketches, and train a sketch-conditioned policy on this dataset.\n\nProblem Statement Our goal is to learn a manipulation policy conditioned on a goal sketch\nof the desired scene state and a history of interactions. Formally, we denote such a policy by\nj=1), where at denotes an action at timestep t, g ∈ RW ×H×3 is a given goal\nπsketch(at|g, {oj}t\nsketch with width W and height H, and ot ∈ RW ×H×3 is an observation at time t. At inference\ntime, the policy takes a given goal sketch along with a history of RGB image observations to infer\nan action to execute. In practice, we condition πsketch on a history of D previous observations rather\nthan all observations from the initial state at t = 1. To train such a policy, we assume access to\nn=1 of N successful demonstrations, where T (n) refers\na dataset Dsketch = {gn, {(on\nto the length of the nth trajectory in timesteps. Each episode of the dataset consists of a given\ngoal sketch and a corresponding demonstration trajectory, with image observations recorded at each\ntimestep. Our goal is to thus learn the sketch-conditioned imitation policy πsketch(at|g, {oj}t\nj=1)\ntrained on this dataset Dsketch.\n\nt )}T (n)\n\nt=1 }N\n\nt , an\n\n3.1\n\nIMAGE-TO-SKETCH TRANSLATION\n\nTraining a sketch-conditioned policy requires a dataset of robot trajectories that are each paired with\na sketch of the goal state achieved by the robot. Collecting such a dataset from scratch at scale,\nincluding the trajectories themselves and manually drawn sketches, can easily become impractical.\nThus, we instead aim to learn an image-to-sketch translation network T (g|o) that takes an image\nobservation o and outputs the corresponding goal sketch g. This network can be used to post-\nprocess an existing dataset of demonstrations D = {{(on\nn=1 with image observations\nby appending a synthetically generated goal sketch to each demonstration. This produces a dataset\nt=1 }N\nfor sketch-based IL: Dsketch = {gn, {(on\n\nt )}T (n)\n\nt )}T (n)\n\nt=1 }N\n\nt , an\n\nt , an\n\nn=1.\n\nRT-1 Dataset\nIn this work, we rely on an existing dataset of visual demonstrations collected by\nprior work (Brohan et al., 2023b). RT-1 is a prior language-to-action imitation learning agent trained\non a large-scale dataset (80K trajectories) of VR-teleoperated demonstrations that include skills\nsuch as moving objects near one another, placing cans and bottles upright or sideways, opening and\nclosing cabinets, and performing pick and place on countertops and drawers (Brohan et al., 2023b).\nHere, we repurpose the RT-1 dataset and further adapt the RT-1 policy architecture to accommodate\nsketches, detailed in Section 3.2.\n\n4\n\n\fUnder review as a conference paper at ICLR 2024\n\nAssumptions on Sketches We acknowledge that there are innumerable ways for a human to pro-\nvide a sketch corresponding to a given image of a scene.\nIn this work, we make the following\nassumptions about input sketches for a controlled experimental validation procedure. In particular,\nwe ﬁrst assume that a given sketch respects the task-relevant contours of an associated image, such\nthat tabletop edges, drawer handles, and task-relevant objects are included and discernible in the\nsketch. We do not assume contours in the sketch to be edge-aligned or pixel-aligned with those in an\nimage. We do assume that the input sketch consists of black outlines at the very least, with shading\nin color being optional. We further assume that sketches do not contain information not present in\nthe associated image, such as hallucinated objects, scribbles, or textual annotations, but may omit\ntask-irrelevant details that appear in the original image.\n\ni\n\ni\n\ni , . . . , gL(i)\ni , . . . , gL(i)\n\nSketch Dataset Generation To train an image-to-sketch translation network T , we collect a new\ndataset DT = {(oi, g1\n)}M\ni=1 consisting of M image observations oi each paired with a\n. Those represent L(i) different representations of the same image\nset of goal sketches g1\noi, in order to account for the fact that there are multiple, valid ways of sketching the same scene.\nTo collect DT , we take 500 randomly sampled terminal images from demonstration trajectories in\nthe RT-1 dataset, and manually draw sketches with black lines on a white background capturing the\ntabletop, drawers, and relevant objects visible on the manipulation surface. While we personally\nannotate each robot observation with a single sketch only, we add this data to an existing, much\nlarger non-robotic dataset (Li et al., 2019). This dataset captures inter-sketch variation via multiple\ncrowdsourced sketches per image. We do not include the robot arm in our manual sketches, as we\nﬁnd a minimal representation to be most natural. Empirically, we ﬁnd that our policy can handle\nsuch sketches despite actual goal conﬁgurations likely having the arm in view. We collect these\ndrawings using a custom digital stylus drawing interface in which a user draws an edge-aligned\nsketch over the original image (Appendix Fig. 15). The ﬁnal recorded sketch includes the user’s\nstrokes in black on a white canvas with the original image dimensions.\n\nImage-to-Sketch Training We implement the image-to-sketch translation network T with the\nPix2Pix conditional generative adversarial network (cGAN) architecture, which is composed of a\ngenerator GT and a discriminator DT (Isola et al., 2017). The generator GT takes an input image o,\na random noise vector z, and outputs a goal sketch g. The discriminator DT is trained to discriminate\namongst artiﬁcially generated sketches and ground truth goal sketches. We utilize the standard\ncGAN supervision loss to train both (Li et al., 2019; Isola et al., 2017):\n\nLcGAN = min\nGT\n\nmax\nDT\n\nEo,g[log DT (o, g)] + Eo,g[log(1 − DT (o, GT (o, g))]\n\n(1)\n\nWe also add the L1 loss to encourage the produced sketches to align with the ground truth sketches\nas in (Li et al., 2019). To account for the fact that there may be multiple valid sketches for a given\nimage, we only penalize the minimum L1 loss incurred across all L(i) sketches provided for a given\nimage as in Li et al. (2019). This is to prevent wrongly penalizing T for producing a valid sketch\nthat aligns well with one example but not another simply due to stylistic differences in the ground\ntruth sketches. The ﬁnal objective is then a λ-weighted combination of the average cGAN loss and\nthe minimum alignment loss:\n\nLT =\n\nλ\nL(i)\n\nL(i)\n(cid:88)\n\nk=1\n\nLcGAN(oi, g(k)\n\ni\n\n) +\n\nmin\nk∈{1,...,L(i)}\n\nL1(oi, g(k)\n\ni\n\n)\n\n(2)\n\nIn practice, we supplement the 500 manually drawn sketches from DT by leveraging the existing\nlarger-scale Contour Drawing Dataset (Li et al., 2019). We refer to this dataset as DCD, which\ncontains 1000 examples of internet-scraped images containing objects, people, animals from Adobe\nStock, paired with L(i) = 5 crowd-sourced black and white outline drawings per image collected on\nAmazon Mechanical Turk. Visualizations of this dataset are provided in Appendix Fig. 4. We ﬁrst\ntake a pre-trained image-to-sketch translation network TCD (Li et al., 2019) trained on DCD, with\nL(i) = 5 sketches per image. Then, we ﬁne-tune TCD on DT , with only L(i) = 1 manually drawn\nsketch per robot observation, to obtain our ﬁnal image-to-sketch network T . Visualizations of the\nsketches generated by T for different robot observations are available in Fig. 5.\n\n5\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 2: Architecture of RT-Sketch allowing different kinds of visual input. RT-Sketch adopts the\nTransformer (Vaswani et al., 2017) architecture with EfﬁcientNet (Tan & Le, 2019) tokenization at\nthe input, and outputs bucketized actions.\n\n3.2 RT-SKETCH\n\nWith a means of translating image observations to black and white sketches via T (Section 3.1), we\ncan automatically augment the existing RT-1 dataset with goal sketches. This results in a dataset,\nwhich we refer to as Dsketch, which can be used for training our algorithm, RT-Sketch.\n\nRT-Sketch Dataset The original RT-1 dataset Dlang = {in, {(on\nn=1 consists of N\nt )}T n\nt , an\nepisodes with a paired natural language instruction i and demonstration trajectory {(on\nt=1.\nWe can automatically hindsight-relabel such a dataset with goal images instead of language\ngoals (Andrychowicz et al., 2017). Let us denote the last step of a trajectory n as T (n). Then the new\ndataset with image goals instead of language goals is Dimg = {on\nT (n), {(on\nn=1, where\nwe treat the last observation of the trajectory on\nT (n) as the goal gn. To produce a dataset for πsketch,\nwe can simply replace on\n\nT (n)) such that Dsketch = {ˆgn, {(on\n\nT (n) with ˆgn = T (on\n\nt )}T (n)\n\nt )}T (n)\n\nt=1 }N\n\nt=1 }N\n\nt=1 }N\n\nt , an\n\nt , an\n\nt , an\n\nn=1.\n\nt )}T (n)\n\nTo encourage the policy to afford different levels of input sketch speciﬁcity, we in practice produce\ngoals by ˆgn = A(on\nT (n) ), where A is a randomized augmentation function. A chooses between\nsimply applying T , T with colorization during postprocessing (e.g., by superimposing a blurred\nversion of the ground truth RGB image over the binary sketch), a classical Sobel operator (Sobel,\n1968) for edge detection, or not applying any operators, which preserves the original ground truth\ngoal image (Fig. 2). By co-training on all representations, we intend for RT-Sketch to handle a\nspectrum of speciﬁcity going from binary sketches; colorized sketches; edge detected images; and\ngoal images (Appendix Fig. 5).\n\nRT-Sketch Model Architecture\nIn our setting, we consider goals provided as sketches rather than\nlanguage instructions as was done in RT-1. This change in the input representation necessitates a\nchange in the model architecture. The original RT-1 policy relies on a Transformer architecture back-\nbone (Vaswani et al., 2017). RT-1 ﬁrst passes a history of D = 6 images through an EfﬁcientNet-B3\nmodel (Tan & Le, 2019) producing image embeddings, which are tokenized, and separately extracts\ntextual embeddings and tokens via FiLM (Perez et al., 2018) and a Token Learner (Ryoo et al.,\n2021). The tokens are then fed into a Transformer which outputs bucketized actions. The output\naction dimensionality is 7 for the end-effector (x, y, z, roll, pitch, yaw, gripper width), 3 for the\nmobile base, (x, y, yaw), and 1 for a ﬂag that can select amongst base movement, arm movement,\nand episode termination. To retrain the RT-1 architecture but accommodate the change in input rep-\nresentation, we omit the FiLM language tokenization altogether. Instead, we concatenate a given\ngoal image or sketch with the history of images as input to EfﬁcientNet, and extract tokens from its\noutput, leaving the rest of the policy architecture unchanged. We visualize the RT-Sketch training\ninputs and policy architecture in Fig. 2. We refer to this architecture when trained only on images\n(i.e., an image goal-conditioned RT1 policy) as RT-Goal-Image and refer to it as RT-Sketch when it\nis trained on sketches as discussed in this section.\n\nTraining RT-Sketch We can now train πsketch on Dπsketch utilizing the same procedure as was\nused to train RT-1 (Brohan et al., 2023b), with the above architectural modiﬁcations. We ﬁt πsketch\n\n6\n\n\fUnder review as a conference paper at ICLR 2024\n\nusing the behavioral cloning objective function. This aims to minimize the negative log-likelihood\nof an action provided the history of observations and a given sketch goal (Torabi et al., 2018):\n\nJ(πsketch) =\n\nN\n(cid:88)\n\nT (n)\n(cid:88)\n\nn=1\n\nt=1\n\n4 EXPERIMENTS\n\nlog πsketch(an\n\nt |gn, {oj}t\n\nj=1)\n\nWe seek to understand the ability of RT-Sketch to perform goal-conditioned manipulation as com-\npared to policies that operate from higher-level goal abstractions like language, or more over-\nspeciﬁed modalities, like goal images. To that end, we test the following four hypotheses:\n\nH1: RT-Sketch is successful at goal-conditioned IL. While sketches are abstractions of real im-\nages, our hypothesis is that they are speciﬁc enough to provide manipulation goals to a policy.\nTherefore, we expect RT-Sketch to perform on a similar level to language goals (RT-1) or goal\nimages (RT-Goal-Image) in straighforward manipulation settings.\n\nH2: RT-Sketch is able to handle varying levels of speciﬁcity. There are as many ways to sketch\na scene as there are people. Because we have trained RT-Sketch on sketches of varying levels of\nspeciﬁcity, we expect it to be robust against variations of the input sketch for the same scene.\n\nH3: Sketches enable better robustness to distractors than goal images. Sketches focus on task-\nrelevant details of a scene. Therefore, we expect RT-Sketch to provide robustness against distractors\nin the environment that are not included in the sketch compared to RT-Goal-Image that operates on\ndetailed image goals.\n\nH4: Sketches are favorable when language is ambiguous. We expect RT-Sketch to provide a\nhigher success rate compared to ambiguous language inputs when using RT-1.\n\n4.1 EXPERIMENTAL SETUP\n\nPolicies We compare RT-Sketch to the original language-conditioned agent RT-1 (Brohan et al.,\n2023b), and RT-Goal-Image, a policy identical in architecture to RT-Sketch, but taking a goal image\nas input rather than a sketch. All policies are trained on a multi-task dataset of ∼ 80K real-world\ntrajectories manually collected via VR teleoperation using the setup from Brohan et al. (2023b).\nThese trajectories span a suite of common ofﬁce and kitchen tasks such as picking and placing ob-\njects, reorienting cups and bottles upright or sideways, opening and closing drawers, and rearranging\nobjects between drawers or a countertop.\n\nEvaluation protocol To ensure fair comparison, we control for the same initial and goal state of\nthe environment across different policy rollouts via a catalog of well-deﬁned evaluation scenarios\nthat serve as references for human robot operators. For each scenario, we record an initial image\n(RGB observation) of the scene, the goal image (with objects manually rearranged as desired), a\nnatural language task string describing the desired agent behavior to achieve the goal, and a set\nof hand-drawn sketches corresponding to the recorded goal image. At test time, a human opera-\ntor retrieves a particular evaluation scenario from the catalog, aligns the physical robot and scene\naccording to a reference image using a custom visualization utility, and places the relevant objects\nin their respective locations. Finally, the robot selects one of the goal representations (language,\nimage, sketch, etc.) for the scenario as input to a policy. We record a video of the policy rollout for\ndownstream evaluation (see Section 4.2). We perform all experiments using the Everyday Robot2,\nwhich contains a mobile base, an overhead camera, and a 7-DoF manipulator arm with a parallel\njaw gripper. All sketches for evaluation are collected with a custom manual drawing interface by a\nsingle human annotator on a tablet with a digital stylus.\n\nPerformance Metrics Deﬁning a standardized, automated evaluation protocol for goal alignment\nis non-trivial. Since binary task success is too coarse-grained and image-similarity metrics like\nframe-differencing or CLIP (Radford et al., 2021) tend to be brittle, we measure performance with\n\n2everydayrobots.com\n\n7\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 3: Goal Alignment Results: Average Likert scores for different policies rating perceived\nsemantic alignment (Q1) and spatial alignment (Q2) to a provided goal. For straightforward\nbenchmark manipulation tasks, RT-Sketch performs comparably and in some cases better than\nRT-1 and RT-Goal-Image in terms of both metrics, for 5 out of 6 skills (H1). RT-Sketch further\nexhibits the ability to handle sketches of different levels of detail (H2), while achieving better goal\nalignment than baselines when the visual scene is distracting (H3) or language would be ambiguous\n(H4). Error bars indicate standard error across labeler ratings.\n\ntwo more targeted metrics. First, we quantify policy precision as the distance (in pixels) between ob-\nject centroids in achieved and ground truth goal states, using manual keypoint annotations. Although\nleveraging out-of-the box object detectors to detect object centroids is a possibility, we want to avoid\nconﬂating errors in object detection (imprecise bounding box, wrong object, etc.) from manipulation\nerror of the policy itself. Second, we gather human-provided assessments of perceived goal align-\nment, following the commonly-used Likert (Likert, 1932) rating scheme from 1 (Strongly Disagree)\nto 7 (Strongly Agree), for:\n\n• (Q1) The robot achieves semantic alignment with the given goal during the rollout.\n• (Q2) The robot achieves spatial alignment with the given goal during the rollout.\n\nFor Q1, we present labelers with the policy rollout video along with the given ground-truth language\ntask description. We expect reasonably high ratings across all methods for straightforward manip-\nulation scenarios (H1). Sketch-conditioned policies should yield higher scores than a language-\nconditioned policy when a task string is ambiguous (H4). Q2 is instead geared at measuring to\nwhat degree a policy can spatially arrange objects as desired. For instance, a policy can achieve\nsemantic alignment for the instruction place can upright as long as the can ends up in the right ori-\nentation. For Q2, we visualize a policy rollout side-by-side with a given visual goal (ground truth\nimage, sketch, etc.) to assess perceived spatial alignment. We posit that all policies should receive\nhigh ratings for straightforward scenarios (H1), with a slight edge for visual-conditioned policies\nwhich implicitly have stronger spatial priors encoded in goals. We further expect that as the visual\ncomplexity of a scene increases, sketches may be able to better attend to pertinent aspects of a goal\nand achieve better spatial alignment than image-conditioned agents (H3), even for different levels\nof sketch speciﬁcity (H4). We provide a visualization of the assessment interface for Q1 and Q2 in\nAppendix Fig. 16. We note that we perform these human assessment surveys across 62 individuals\n(non-expert, unfamiliar with our system), where we assign between 8 and 12 people to evaluate each\nof the 6 different skills considered below.\n\n4.2 EXPERIMENTAL RESULTS\n\nIn this section, we present our ﬁndings related to the hypotheses of Section 4. Tables 4.2 and 4.2\nmeasure the spatial precision achieved by policies in terms of pixelwise distance, while Fig. 3 shows\n\n8\n\n\fUnder review as a conference paper at ICLR 2024\n\nthe results of human-perceived semantic and spatial alignment, based on a 7-point Likert scale rating.\n\nSkill\nMove Near\nPick Drawer\nDrawer Open\nDrawer Close\nKnock\nUpright\nVisual Distractors\nLanguage Ambiguity\n\nSpatial Precision (RMSE in px.)\nRT-Sketch\n3.49 ± 1.38\n4.77 ± 2.78\n3.34 ± 1.08\n3.02 ± 1.35\n5.36 ± 2.74\n5.08 ± 2.08\n4.78 ± 2.17\n4.45 ± 1.54\n\nRT-Goal-Image\n3.89 ± 1.16\n4.74 ± 2.01\n4.98 ± 1.16\n3.71 ± 1.67\n5.63 ± 2.60\n4.18 ± 1.54\n7.95 ± 2.86\n-\n\nRT-1\n5.43 ± 2.15\n5.69 ± 2.90\n4.51 ± 1.55\n2.69 ± 0.93\n7.39 ± 1.77\n7.84 ± 2.37\n-\n8.03 ± 2.52\n\nFailure Occurrence (Excessive Retrying)\nRT-1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.06\n-\n0.40\n\nRT-Goal-Image\n0.33\n0.20\n0.07\n0.07\n0.40\n0.27\n0.67\n-\n\nRT-Sketch\n0.06\n0.13\n0.00\n0.00\n0.13\n0.00\n0.13\n0.13\n\nTable 1: Spatial Precision and Speciﬁc Failure Occurrence : Left: We report the level of spatial\nprecision achieved across policies, measured in terms of RMSE of the centroids of manipulated\nobjects in achieved vs. given reference goal images. Darker shading indicates higher precision\n(lower centroid distance). Fig. 7 contains visualizations illustrating the degree of visual alignment\nthat different RMSE values correspond to. Right: We report the proportion of rollouts in which\ndifferent policies exhibit excessive retrying behavior. Bolded numbers indicate the most precise and\nleast failure-prone policy for each skill.\n\nH1: We evaluate 6 skills from the RT-1 benchmark (Brohan et al., 2023b): move X near Y, place X\nupright, knock X over, open the X drawer, close the X drawer, and pick X from Y. For each skill, we\nrecord 15 different catalog scenarios, varying both objects (16 unique in total) and their placements.\n\nIn general, we ﬁnd that RT-Sketch performs on a comparable level to RT-1 and RT-Goal-Image for\nboth semantic (Q1) and spatial alignment (Q2), achieving ratings in the ‘Agree’ to ‘Strongly Agree’\nrange on average for nearly all skills (Fig. 3 (top)). A notable exception is upright, where RT-\nSketch essentially fails to accomplish the goal semantically (Q1), albeit with some degree of spatial\nalignment (Q2). Both RT-Sketch and RT-Goal-Image tend to position cans or bottles appropri-\nately and then terminate, without realizing the need for reorientation (Appendix Fig. 8). This be-\nhavior results in low centroid-distance to the goal (darker gray in Table 4.2 (left)). RT-1, on the\nother hand, reorients cans and bottles successfully, but at the expense of higher error (Appendix\nIn our experiments, we also observe the occurrence of\nFig. 8, light color in Table 4.2 (left)).\nexcessive retrying behavior, in which a policy attempts to align the current scene with a given goal\nwith retrying actions such as grasping and placing. However, performing these low-level actions\nwith a high degree of precision is challenging, and thus excessive retrying can actually disturb\nIn Table 4.2, we\nthe scene leading to knocking objects off the table or undoing task progress.\nreport the proportion of rollouts in which we observe this behavior across all policies. We note that\nRT-Goal-Image is most susceptible to this failure mode, as a result of over-attending to pixel-level\ndetails and trying in excess to match a given goal exactly. Meanwhile, RT-Sketch and RT-1 are far\nless vulnerable, since both sketches and language provide a higher level of goal abstraction.\n\nIn this table, we further see that RT-Goal-Image has a tendency to over-attend to pixel-level details,\nwhich can result in excessive retrying behavior and failure to terminate when attempting to rearrange\nobjects to exactly match a given goal image (darker gray in Section 4.2 (right), denoting more\nfrequent failures).\n\nH2: We next assess RT-Sketch’s ability to handle input sketches of varied levels of detail (free-hand,\nedge-aligned line sketch, colorized line sketch, and a Sobel edge-detected image as an upper bound).\nFree-hand sketches are drawn with a reference image next to a blank canvas, while line sketches are\ndrawn on a semi-transparent canvas overlaid on the image (see Appendix Fig. 15). We ﬁnd such a UI\nto be convenient and practical, as an agent’s current observations are typically available and provide\nhelpful guides for sketching lines and edges. Across 5 trials each of the move near and open drawer\nskills, we see in Section 4.2 that all types of sketches produce reasonable levels of spatial precision.\nAs expected, Sobel edges incur the least error, but even free-hand sketches, which do not necessarily\npreserve perspective projection, and line sketches, which are far sparser in detail, are not far behind.\nThis is also reﬂected in the corresponding Likert ratings (Fig. 3 (left, bottom)). Free-hand sketches\nalready garner moderate ratings (around 4) of perceived spatial and semantic alignment, but line\nsketches result in a marked performance improvement to nearly 7, on par with the upper bound of\nproviding an edge-detected goal image. Adding color does not improve performance further, but\nleads to interesting qualitative differences in behavior (see Appendix Fig. 9).\n\n9\n\n\fUnder review as a conference paper at ICLR 2024\n\nSkill\nMove Near\nDrawer Open\n\nFree-Hand\n7.21 ± 2.76\n3.75 ± 1.63\n\nLine Sketch\n3.49 ± 1.38\n3.34 ± 1.08\n\nColor Sketch\n3.45 ± 1.03\n2.48 ± 0.50\n\nSobel Edges\n3.36 ± 0.66\n2.13 ± 0.25\n\nTable 2: RT-Sketch Spatial Precision across Sketch Types (RMSE (centroid-distance) in px. We\nreport the spatial precision achieved by RT-Sketch subject to different input modalities. As expected,\nfor less detailed and more rough sketches, RT-Sketch achieves lower precision (lighter shading),\nand for richer representations RT-Sketch is more precise (bolded, darker shading). Still, there is a\nrelatively small difference in performance between line, color, and edge-detected representations,\nindicating RT-Sketch’s ability to afford different levels of input speciﬁcity.)\n\nH3: Next, we compare the robustness of RT-Sketch and RT-Goal-Image to the presence of visual\ndistractors. We re-use 15 move X near Y trials from the catalog, but introducing 5 − 9 distrac-\ntor objects into the initial visual scene after alignment. This testing procedure is adapted from\nRT-1 generalization experiments referred to as medium-high difﬁculty (Brohan et al., 2023b). In\nSection 4.2 (left, bottom), we see that RT-Sketch exhibits far lower spatial errors on average, while\nproducing higher semantic and spatial alignment scores over RT-Goal-Image( Fig. 3 (middle, bot-\ntom)). RT-Goal-Image is easily confused by the distribution shift introduced by distractor objects,\nand often cycles between picking up and putting down the wrong object. RT-Sketch, on the other\nhand, ignores task-irrelevant objects not captured in a sketch and completes the task in most cases\n(see Appendix Fig. 10).\n\nH4: Finally, we evaluate whether sketches as a representation are favorable when language goals\nalone are ambiguous. We collect 15 scenarios encompassing 3 types of ambiguity in language\ninstructions: instance ambiguity (T1) (e.g., move apple near orange when multiple orange instances\nare present), somewhat out-of-distribution (OOD) language (T2) (e.g., move left apple near orange),\nand highly OOD language (T3) (e.g., complete the rainbow) (see Appendix Fig. 11). While the\nlatter two qualiﬁcations should intuitively help resolve ambiguities, they were not explicitly made\npart of the original RT-1 training (Brohan et al., 2023b), and hence only provide limited utility.\nIn Section 4.2 (left, bottom), RT-Sketch achieves nearly half the error of RT-1, and a 2.39-fold and\n2.79-fold score increase for semantic and spatial alignment, respectively (Fig. 3 (right, bottom)).\nFor T1 and T2 scenarios, RT-1 often tries to pick up an instance of any object mentioned in the task\nstring, but fails to make progress beyond that (Appendix Fig. 12). This further suggests the utility\nof sketches to express new, unseen goals with minimal overhead, when language could otherwise be\nopaque or difﬁcult to express with only in-distribution vocabulary (Appendix Fig. 13).\n\nLimitations and Failure Modes Firstly, the image-to-sketch generation network used in this work\nis ﬁne-tuned on a dataset of sketches provided by a single human annotator, and we have yet to stress-\ntest the generalization capabilities of RT-Sketch at scale with sketches produced by different people.\nSecondly, we note that RT-Sketch shows some inherent biases towards performing certain skills it\nwas trained on, and occasionally performs the wrong skill. For a detailed breakdown of RT-Sketch’s\nlimitations and failure modes, please see Appendix C).\n\n5 CONCLUSION\n\nWe propose RT-Sketch, a goal-conditioned policy for manipulation that takes a hand-drawn sketch\nof the desired scene as input, and outputs actions. To enable such a policy, we ﬁrst develop a\nscalable way to generate paired sketch-trajectory training data via an image-to-sketch translation\nnetwork, and modify the existing RT-1 architecture to take visual information as an input. Empir-\nically, we show that RT-Sketch not only performs on a comparable level to existing language or\ngoal-image conditioning policies for a number of manipulation skills, but is amenable to different\ndegrees of sketch ﬁdelity, and more robust to visual distractors or ambiguities. Future work will\nfocus on extending hand-drawn sketches to more structured representations, like schematics or dia-\ngrams for assembly tasks. While powerful, sketches are not without their own limitations – namely\nambiguity due to omittted details or poor quality sketches. In the future, we are excited by avenues\nfor multimodal goal speciﬁcation that can leverage the beneﬁts of language, sketches, and other\nmodalities to jointly resolve ambiguity from any single modality alone.\n\n10\n\n\fUnder review as a conference paper at ICLR 2024",
    "abstract": "Natural language and images are commonly used as goal representations in goal-conditioned imitation learning (IL). However, natural language can be ambiguous and images can be over-specified. In this work, we propose hand-drawn sketches as a modality for goal specification in visual imitation learning. Sketches are easy for users to provide on the fly like language, but similar to images they can also help a downstream policy to be spatially-aware and even go beyond images to disambiguate task-relevant from task-irrelevant objects. We present RT-Sketch, a goal-conditioned policy for manipulation that takes a hand-drawn sketch of the desired scene as input, and outputs actions. We train RT-Sketch on a dataset of paired trajectories and corresponding synthetically generated goal sketches. We evaluate this approach on six manipulation skills involving tabletop object rearrangements on an articulated countertop. Experimentally we find that RT-Sketch is able to perform on a similar level to image or language-conditioned agents in straightforward settings, while achieving greater robustness when language goals are ambiguous or visual distractors are present. Additionally, we show that RT-Sketch has the capacity to interpret and act upon sketches with varied levels of specificity, ranging from minimal line drawings to detailed, colored drawings. For supplementary material and videos, please refer to our website: http://rt-sketch-anon.github.io.",
    "response": {
      "Soundness": 2,
      "Presentation": 2,
      "Contribution": 2,
      "Rating": 3,
      "Confidence": 3,
      "Strengths": "- The direction the authors are going with this work, thinking about different ways of specifying a goal, will be useful for improving accessibility and has intellectual merits. \n- The authors take steps to perform surveys to study human preference in specifying goals rather than just quantitative analysis through robot performance.\n- Creation of the RT-Sketch dataset and the Image-To-Sketch model have technical merits.\n- The limitations and failure modes are honest from a methodology perspective and are appreciated.\n- Overall this paper is well written, Figure 2 does a good job of demonstrating the architecture of RT-Sketch and Figure 3 does a great job of conveying the survey results.",
      "Weaknesses": "The main concerns with this manuscript come from two sources. The first is with respect to the motivation of using sketches to specify a goal and the second is concerning the quantitative robotics results.\n## Motivation\nOverall the reviewer is not convinced that the examples and arguments given motivate the superiority of using a sketch as a goal over using natural language.\n- Regarding the granularity argument that the authors use in the introduction including the examples of \"put utensils, ..., on the table\" and \"put the fork 2cm to the right...\". Doesn't this demonstrate the flexibility that language has as a goal? Even if a human had to communicate the placement of utensils on a table, this would still seem easier than drawing a corresponding representation for an entire table.\n- While language can be ambiguous, so can sketches. As an example, if a sketch was given with an empty table, am I telling the agent that I want it to throw out the garbage on the table or am I telling it to ignore the garbage? It would seem like the desirable solution would be to create more intelligent agents to create reasonable solutions or ask for clarifications when given an ambiguous problem.\n\n## Robotics Results\n- The metrics, \"Spatial Precision\" and \"Failure Occurrence\" are not carefully defined or motivated. Failure occurrence is not defined at all. The spatial precision metric is at best defined as \"the distance (in pixels) between object centroids in achieved and ground truth goal states, using manual keypoint annotation\". However, how these centroids are obtained and why manual keypoint annotation is necessary over using an off-the-shelf image classifier should have been mentioned. \n- It is not obvious how big of a difference the errors are when looking at the RMSE in pixels. Could the errors when finding the centroid and the manual keypoint annotations be an issue in measuring this? Can a visualization be created to show this?\n- Typically bolded numbers in a column/row represent the method with best performance. However, in the column for failure occurrences, RT-Goal-Image is bolded despite having the highest failure occurrence.\n- The meaning of the shading of the cells of Table 1 adds a lot of confusion and should have been defined in the caption to improve readability. This confusion comes because, in one portion of the table, darker gray colors represent lower centroid distance and in another portion it represents the frequency of failures. This frequency of failure metric is also not well defined, how is this different than failure occurrence?",
      "Questions": "Beyond the concerns and questions given in the weakness section.\n\n- How many people took the survey? How was this survey conducted?"
    }
  },
  {
    "paper_id": "R3CDj2DLln",
    "summary": "Background: The study focuses on multimodal physical scene understanding where an embodied agent needs to discern the properties, direction, and distance of an impact sound source to locate fallen objects. Previous work, such as Neural Acoustic Fields (NAFs), could capture sound propagation in a single scene but lacked generalizability across multiple scenes due to overfitting. Acoustic rendering and sound-based scene understanding are critical for applications in navigation, particularly under multimodal scenarios that integrate both auditory and visual inputs.\nGap: Previous methods, specifically feed-forward neural networks used for sound source inference, failed to generalize well across different environments and struggled with poor domain adaptation. NAFs, while capturing acoustics effectively within single scenes, were not effective for analyzing sound across multiple environments.\nHypothesis: By learning an inverse model of acoustic formation, termed as the Disentangled Acoustic Field (DAF), the embodied agent could construct a spatial uncertainty map to improve the localization and understanding of fallen objects through sound.\nMethod: The method involves creating a disentangled acoustic field model that captures the generation and propagation of sounds. The DAF approach includes an encoder to process short-time Fourier transform (STFT) into latent factors and a generator to produce the power spectral density (PSD) representing the emitted sound characteristics. The analysis-by-synthesis framework enables sound properties inference by decomposing latent space. The system uses the Testament Dataset with experiments conducted in the TDW simulation platform for audio-visual scene understanding tasks. During tests, agents use sound to form hypotheses about object characteristics and location, refining them with visual data.\nConclusion: The DAF framework significantly advances the success rate for localizing fallen objects in multimodal environments, outperforming previous models by providing both improved generalization across different scenes and enhanced interpretability of sound data. The framework facilitates effective navigation plans and can generalize to novel scene types, indicating its robustness in complex audio-visual environments.\n",
    "full text": "\nDISENTANGLED ACOUSTIC FIELDS FOR MULTI-\nMODAL PHYSICAL PROPERTY UNDERSTANDING\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nWe study the problem of multimodal physical property understanding, where an\nembodied agent needs to find fallen objects by inferring object properties, direc-\ntion, and distance of an impact sound source. Previous works adopt feed-forward\nneural networks to directly regress the variables from sound, leading to poor gen-\neralization and domain adaptation issues. In this paper, we illustrate that learning\na disentangled model of acoustic formation, referred to as disentangled acoustic\nfield (DAF), to capture the sound generation and propagation process, enables\nthe embodied agent to construct a spatial uncertainty map over where the objects\nmay have fallen. We demonstrate that our analysis-by-synthesis framework can\njointly infer sound properties by explicitly decomposing and factorizing the latent\nspace of the disentangled model. We further show that the spatial uncertainty map\ncan significantly improve the success rate for the localization of fallen objects by\nproposing multiple plausible exploration locations.\n\n1\n\nINTRODUCTION\n\nImagine walking through a forest with your eyes closed, listening to the sounds around you. As you\nmove, you hear the rustling of leaves as an animal passes by, the gentle bubbling of a nearby stream,\nand the soft whisper of the wind. These sounds provide valuable information about the environment.\nSound waves are influenced by the objects they encounter, changing in timbre, direction, and inten-\nsity as they reflect, diffract, and absorb. As humans, we intuitively understand how sound behaves in\nphysical spaces, enabling us to infer the presence, location, and physical properties of objects from\nthe sounds we hear.\n\nRecent progress in neural fields has yielded high-fidelity models of perceptual modalities such as\nvision, touch, and sound. Most recently, neural acoustic fields (NAFs) (Luo et al., 2022) propose\nrepresenting spatial acoustics of sounds, enabling continuous modeling of sound propagation and\nreverberation in a given scene. By modeling such acoustics, NAFs implicitly capture the structure\nand material properties of a scene. However, NAFs are overfitted to the acoustic properties of\na single room, preventing them from being used as a disentangled model of sound across many\nenvironments.\n\nIn this work, we propose disentangled acoustic fields (DAFs), an approach to modeling acoustic\nproperties across a multitude of different scenes. In NAFs, the short-time Fourier transform (STFT)\nof audio reverberation is the object of the modeling, but it is highly sensitive to the geometry of each\nscene and thus difficult to fit across different scenes. Instead, DAFs seek to model object sounds\nacross multiple scenes using the power spectral density (PSD). This approach provides a lower\ndimensional compact representation of acoustics that preserves much of the physical information in\nemitted sounds. We demonstrate the effectiveness of this approach by showing high-accuracy object\nproperty inference across a set of different scenes.\n\nWe demonstrate how DAFs can be used to effectively enhance audio perception. Specifically, we\npropose using DAFs as a “mental simulation engine” that can test different physical property config-\nurations to identify the world state that best matches the given sound. This “analysis-by-synthesis”\napproach allows us to robustly infer the underlying locations of fallen objects and effectively nav-\nigate to locate them. Our experiments show that, by using DAFs, we can accurately identify cate-\ngories of fallen objects and their locations, even in complex acoustic environments.\n\n1\n\n\fUnder review as a conference paper at ICLR 2024\n\nAcoustic rendering with DAFs further enables us to obtain an uncertainty measure of different phys-\nical scene parameters, such as object locations, by assessing the mismatch between a simulated\nsound and ground-truth sound. We illustrate how such uncertainty may be used in the task of find-\ning a fallen object, where we may naturally generate plans to different goals by considering the\nunderlying uncertainty cost. In summary, our contributions are as follows:\n\n• We introduce Disentangled Acoustic Fields (DAFs), an approach to model acoustic properties\n\nacross a multitude of different scenes.\n\n• We illustrate how analysis-by-synthesis using DAFs enables us to infer the physical properties of\n\na scene.\n\n• We illustrate how we may use DAFs to represent uncertainty and to navigate and find fallen objects.\n\n2 RELATED WORK\n\n2.1 NEURAL IMPLICIT REPRESENTATIONS\n\nLearned implicit functions have emerged as a promising representation of the 3D geometry (Luo\net al., 2021; Park et al., 2019; Xu et al., 2019), appearance (Mildenhall et al., 2021; Sitzmann et al.,\n2019), and acoustics of a scene (Luo et al., 2022). Unlike traditional discrete representations, im-\nplicit functions compactly encode information in the weights of a neural network, and can continu-\nously map from spatial coordinates to output. Recent work has proposed to encode shapes as signed\ndistance fields, learn appearance with differentiable rendering, and render acoustics by generating\nspectrograms (Xie et al., 2022) (Luo et al., 2022). For acoustics, Du et al. (2021) proposed to jointly\ngenerate acoustics and images by sampling from a joint manifold, and Luo et al. (2022) introduced\nthe concept of Neural Acoustic Fields (NAFs), an implicit representation that captures sound propa-\ngation in a physical scene. While NAFs enable the modeling of sounds at novel locations in a single\nscene, they cannot be generalized to enable reasoning across novel scenes. In contrast, our method\ncan generalize to novel scenes at test time, enables joint inference of object properties and location,\nand allows uncertainty-aware object localization.\n\n2.2 MULTIMODAL SCENE UNDERSTANDING\n\nRecent work has explored the use of input modalities beyond vision alone for scene understanding.\nExtensive studies have demonstrated the effectiveness of integrating audio and visual information\nin diverse scene understanding applications (Zhang et al., 2018; Owens et al., 2016b; 2018; Zhu\net al., 2021). For instance, Ephrat et al. (2018); Zhao et al. (2018); Gao et al. (2018) employ visual\ninput to separate and localize sounds, Gao et al. (2020) leverages spatial cues contained in echoes\nfor more accurate depth estimation, while Chen et al. (2021c); Luo et al. (2022); Arandjelovic &\nZisserman (2017); Owens & Efros (2018) demonstrate the potential of sound in learning multimodal\nfeatures and inferring scene structure. Cross-modal generation has gained increasing attention by\nresearchers (Gan et al., 2020a; Owens et al., 2016a; Su et al., 2020; Chen et al., 2017). Furthermore,\nAfouras et al. (2020); Gan et al. (2019); Arandjelovic & Zisserman (2018) integrate both visual and\nauditory information to localize target objects more accurately. Motivated by these findings, we\npropose a disentangled acoustic field for physical scene understanding, where an embodied agent\nseeks to find fallen objects by inferring their physical properties, direction, and distance from an\nimpact sound.\n\n2.3 AUDIO-VISUAL NAVIGATION\n\nOur work is also closely related to audio-visual navigation, where navigation is achieved using au-\ndio signals to augment vision (Chen et al., 2021a; 2019; Gan et al., 2020c; Chang et al., 2017). In\nparticular, Chen et al. (2020a) proposed the AudioGoal challenge, where an embodied agent is re-\nquired to navigate to a target emitting a constant sound using audio for positional cues (Chen et al.,\n2020b). Building on this, Chen et al. (2021a) introduced the novel task of semantic audio-visual\nnavigation, in which the agent must navigate to an object with semantic vision and short bursts of\nsound. However, their dataset had one limitation: it did not include synthesis capability for impact\nsound and thus could not render the physical properties (like material, position) by audio. To ad-\ndress both issues, Gan et al. (2022) proposed the Find Fallen Object task, where physical reasoning\n\n2\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 1: Illustration of DAFs. The encoder maps the binaural short-time Fourier transform (STFT)\nof the audio input into a new space containing physical audio information such as object position,\nmaterial, type, and a continuous latent. The decoder utilizes these parameters to reconstruct the\npower spectral density (PSD) of the audio. The two components form an analysis-by-synthesis loop\ncapable of inferring object properties, and are jointly learned during training.\n\nwas combined with sound. This dataset was based on the TDW (Gan et al., 2020b) simulation plat-\nform, which can generate audio from a variety of materials and parameters, and utilizes Resonance\nAudio (Google, 2018) (a technology for accurately replicating how sound interacts with the envi-\nronment in 3D spaces) to spatialize the impact sounds depending on the room’s spatial dimensions\nand wall/floor materials. Considering these advantages, we choose it as the benchmark to assess the\ncapability of our proposed method on multi-modal physical property understanding.\n\n3 PROPOSED METHOD\n\nWe are interested in learning a disentangled framework of sound that can effectively generalize\nacross scenes, object types, and object locations. Key to our approach is an explicitly decomposed\nlatent space that models the individual contribution of the sound factors. We first describe the pa-\nrameterization of our disentangled acoustic field, which simultaneously enables factorization of the\nsound generation process and is defined on continuous locations in space. We further characterize\nthe design choices that enable robust generalization and describe how we can use the continuous\nnature of our disentangled acoustic field to facilitate the localization of a fallen object.\n\n3.1 PHYSICS OF SOUND\n\nGiven the sound of a falling object received by an agent as binaural signal s, we seek to identify\nthe relative egocentric object location p ∈ R3, the object material category m ∈ {1, 2, . . . , M },\nthe object type category t ∈ {1, 2, . . . , T }, and a low-dimensional latent code z ∈ Rk, where z\ncan contain information that is independent from the previous factors, such as information about the\nscene structure, scene materials, and properties about the propagation medium. Given an accurate\nmodel of sound formation G, we seek to reconstruct the sound via G(p, m, t, z). In practice, we\nhere do not reconstruct the sound itself, but its power spectral density, a simplified representation\nencompassing essential properties about the falling object.\n\n3.2 DISENTANGLED ACOUSTIC FIELDS (DAFS)\n\nWe aim to learn a disentangled model of sound formation that facilitates efficient inference for\ncharacterizing sound properties. The parameterization of sound formation introduced in Section 3.1\nprovides a general framework for building such a disentangled model. To enhance effective learning,\nwe structure our framework using an encoder, denoted as Eω, and a generator, denoted as Gϕ. By\n\n3\n\nMaterial ෝ𝒎(Metal)Type ො𝒕(Bottle)Position ෝ𝒑(𝑥,𝑦)STFT log-magnitudeEncoderAnalysisDecoderPower spectral densitySynthesisTime domain waveformLatent spaceObject fallingLatent ො𝒛(𝝁,𝝈)\fUnder review as a conference paper at ICLR 2024\n\ninstructing the network to consider the relative egocentric location of the sound emitter, we guide it\nto disregard the absolute positions of the agent and object, and reason in a generalizable fashion.\nGiven a sound signal represented as a binaural waveform s ∈ R2×t, we process the signal using\nthe short-time Fourier transform (STFT), and retain the log-magnitude as S. Following prior work,\nwe discard the phase present in the original signal, which is difficult to model (Du et al., 2021; Luo\net al., 2022). We further investigated the choice of output representation, and found that the STFT\nof a fallen object sound used in prior work (Gan et al., 2022) includes large irregular stretches of\nuninformative silence along the time domain. The irregular and unpredictable temporal gaps are\ndifficult for a neural network to effectively estimate and reconstruct, and in practice, a full-fidelity\nreconstruction of the sound signal in the original domain may not be necessary, as our ultimate goal\nis the inference of the underlying factors of sound formation. We thus transform the sound from\nthe waveform domain into power spectral density (PSD) representation ¯S using Welch’s method\n(basically an average pooling over time of the squared STFT magnitude) as a target for our gen-\nerator, which retains crucial information on the power present in each frequency, but collapses the\ninformation along the time dimension.\n\nWe may thus model the latent factors as the outputs of an encoder Eω which takes as input the sound\nrepresentation S:\n\n(ˆp, ˆm, ˆt, ˆµ, ˆσ) = Eω(S);\n\nˆz ∼ N (ˆµ, ˆσ2 · I),\n\n(1)\n\nwhere we model ˆz as a sample from a diagonal Gaussian with mean ˆµ and standard deviation ˆσ.\nThis restricted parameterization prevents the encoder from compressing all sound information into\nˆz. The generator Gϕ is modeled as a neural field which takes as input the latent factors and attempts\nto generate the PSD:\n\nˆ¯S = Gϕ(ˆp, ˆm, ˆt, ˆz).\nWe train our network with supervision on the latent factors and the output. For the i-th training\nexample, we have access to the ground truth location, object material, and object type as the tuple\n(pi, mi, ti).\n\n(2)\n\nThe object type and material are supervised with cross-entropy loss:\n\nLtype = CrossEntropy(ti, ˆti),\nLmaterial = CrossEntropy(mi, ˆmi),\n\n(3)\n(4)\n\nwhere ti and mi are the ground-truth object type and material for the i-th training sample, and ˆti\nand ˆmi their estimates. An MSE loss is applied to facilitate the learning of the position vector:\n\nLposition =\n\n1\n2\n\n(cid:88)\n\ni\n\n∥ˆpi − pi∥2\n2 .\n\n(5)\n\nDuring training, we sample from the posterior for a given sound Si modeled as a multivariate Gaus-\nsian with diagonal covariance:\n\nqω(z|Si) := N (z; µi, σ2\n\ni · I)\n\n(6)\n\nWe apply the reparameterization trick (Kingma & Welling, 2013) to allow for backpropagation\nthrough the parameters µi, σi, by setting zi = µi + σi ⊙ ϵ, where ϵ ∼ N (0, I). The latent z is\nregularized with:\n\nDKL(qω(z|Si)||N (0, I))\n\n(7)\n\nThe output of the generator is supervised with an MSE loss to facilitate the prediction of the PSD:\n\nLPSD =\n\n1\nd\n\n(cid:88)\n\ni\n\n(cid:13)\nˆ¯Si − ¯Si\n(cid:13)\n(cid:13)\n\n(cid:13)\n2\n(cid:13)\n(cid:13)\n\n,\n\n(8)\n\nwhere d is the dimension of the output PSD feature. In summary, our overall objective is to mini-\nmize:\n\nLtotal = αLtype + βLmaterial + γLposition + δDKL + ηLPSD,\n\n(9)\n\nwhere (α, β, γ, δ, η) are hyperparameters.\n\n4\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 2: Planning with DAFs. The agent jointly uses auditory and visual information as part of the\nplanning process. The auditory branch takes as input the sound S represented as STFT. Using the\nDAF, we infer the factors responsible for the sound production including possible object types and a\nreconstruction loss map for each potential object location. The visual branch takes as input RGB-D\nimages and provides a semantic map and occupancy map to the planner. The planner combines the\ninformation and uses the loss map to produce a priority list of locations. Path planning is completed\nusing the A∗ algorithm.\n\n3.3\n\nINFERENCE OF SOUND PARAMETERS\n\nWe seek to invert the model of sound generation and compute the factors responsible for a given\nsound. Our disentangled model enables us to use “analysis by synthesis” to render all different\nacoustics parameters and find the one which matches our sound the best. However, the combinatorial\ncomplexity of enumerating combinations of factors renders this process computationally prohibitive.\nInstead, we amortize the inference of type, material, and latent into the joint learning process of the\nencoder and generator. We focus our efforts on the inference of object position, which is composed\nof continuous variables and is critical for the localization of the fallen object.\n\nLoss Map Generation: Given a sound s as recorded by an embodied agent, we use the encoder\nto infer the material, type, and continuous latent. We define a search space 10 m × 10 m centered\non the agent position, and discretize this space using a resolution of 0.1 m. Using the previously\ninferred type, material, and Gaussian latent, we iterate over all possible locations pj where the object\ncould be located. The current iterated position is combined with the other factors as inferred by the\nencoder network and provided to the generator.\nThe generated PSD is compared against the ground-truth PSD ¯S of the sound of the fallen object,\nand an MSE difference is recorded at each location. In this fashion, we may generate a loss map\ncorresponding to the two-dimensional coordinates. Since the loss map is based on the egocentric\nneural acoustic field, we need to convert the relative coordinates to the global frame of the room for\npath planning:\n\nfr2g,c,θ\n\n(cid:18)(cid:20) x\ny\n\n(cid:21)(cid:19)\n\n(cid:20) x′\ny′\n\n(cid:21)\n\n=\n\n(cid:20) cos θ − sin θ\ncos θ\n\nsin θ\n\n(cid:21) (cid:20) x\ny\n\n=\n\n(cid:21)\n\n+\n\n(cid:20) cx\ncy\n\n(cid:21)\n\n,\n\n(10)\n\nwhere (cx, cy) is the agent’s current position in the global frame and θ is its rotation angle in the\nglobal frame, while (x, y) is the coordinate in the agent’s egocentric frame.\n\nUncertainty-Aware Planning: We adopt a modular approach to path planning (Gan et al., 2022).\nGiven the audio, we predict the object type and location via Eω. We further construct an internal\n\n5\n\nAuditoryFalling objectsoundTypeLoss mapVisualRGB-D ImageDAFsinferenceSegmentationnetworkProjectionOccupancy mapSemantic mapPlanningPotential targetsPriority rankingPath planning\fUnder review as a conference paper at ICLR 2024\n\nmodel of the world from RGB-D images of the environment. Semantic segmentation is computed\nusing a pre-trained Mask-RCNN (He et al., 2017) model as illustrated in Figure 3. We use the depth\nmap to project the semantic map into world space. Given the object types as inferred by the encoder\nnetwork, we take the top-3 object type candidates as search targets. If a target is visible to the agent\nwhen initialized, the agent attempts to navigate to the location of the visible object. If there is no\ntarget in view, the agent will navigate to the predicted position. If the first attempt fails, the agent\nupdates the world model and searches for potential object candidates. Object candidates are ranked\naccording to the loss map value at the location corresponding to each object. Once the target list is\ndetermined, we apply the A∗ algorithm (Hart et al., 1968) to plan the shortest collision-free path to\nthe first target in an unvisited area of the map.\n\nAlgorithm 1: Inferring a Loss Map of Positions\nInput: Sound as log-magnitude STFT S and PSD ¯S, encoder network Eω, generator network\nGϕ, loss map grid Lgrid, function fr2g,c,θ for global coordinates\n1: (ˆp, ˆm, ˆt, ˆz) = Eω(S)\n2: for xpos, ypos in [−5 m, +5 m]:\n3:\n4:\n\nˆp = (xpos, ypos)\nLgrid[fr2g,c,θ(ˆp)] = ∥Gϕ([ˆp, ˆm, ˆt, ˆz]) − ¯S∥2\n2\n\n4 EXPERIMENT\n\n4.1\n\nINFERENCE OF OBJECT PROPERTIES\n\nTo test the physical inference capability of our proposed model, we first evaluate it on the Find\nFallen Dataset (Gan et al., 2022) and compare it against two baselines. The first is the modular\nsound predictor presented in Gan et al. (2022), which was learned without the use of a disentangled\nmodel. The second is a gradient-based optimization process that minimizes the difference between\nthe predicted and ground-truth PSD by optimizing all latent factors using our trained generator. All\nmethods are evaluated on the same test split. To enable exploring multiple plausible locations, we\nmark a type as accurately predicted if the correct object type is within the top-3 predicted categories.\nThe results in Table 1 show that our model significantly outperforms the baseline methods in both\nposition and type prediction accuracy. By jointly learning a disentangled model along the encoder,\nwe can more accurately predict the object location and the object type. Gradient-based optimization\nfails in jointly optimizing the position and object type, and is easily stuck in local minima.\n\nTo further evaluate our model in real-world scenarios, we employ the REALIMPACT dataset (Clarke\net al., 2023), encompassing 150,000 impact sound recordings across 50 real-world object categories.\nThis dataset incorporates impact sounds captured from 600 distinct listener locations, spanning 10\nangles, 15 heights, and 4 distances. We use the same train/test split across all methods. To accom-\nmodate with the dataset, we adapt the output of the encoder network and the input of the generator\nnetwork to be angle, height, and distance. The official baseline method is a ResNet-18 network em-\nploying the magnitude spectrogram of impact sounds to predict the corresponding object properties.\nAs highlighted in Table 2, our method demonstrates a significant improvement over the baseline in\npredicting the category of object angle, height, and distance.\n\nTable 1: Comparison of position error and type prediction accuracy on Find Fallen. Our pre-\ndictor learned alongside the disentangled model is significantly more accurate in both position and\nobject type (top-3) prediction. We observe that gradient-based optimization of these factors using\nthe disentangled model does not succeed.\n\nMethod\n\nPosition Error (m) ↓ Type Acc. ↑\n\nModular Predictor Gan et al. (2022)\nGradient Inversion\n\nOur Predictor\n\n2.41\n3.19\n\n1.09\n\n0.32\n0.11\n\n0.84\n\n6\n\n\fUnder review as a conference paper at ICLR 2024\n\nTable 2: Comparison of angle, height, and distance category prediction accuracy on RE-\nALIMPACT dataset. Our method significantly outperforms the baseline method in all three tasks.\n\nMethod\n\nAngle Acc. ↑ Height Acc. ↑ Distance Acc. ↑\n\nChance\nBaseline Gao et al. (2022)\n\nOurs\n\n0.100\n0.758\n\n0.900\n\n0.067\n0.881\n\n0.960\n\n0.250\n0.983\n\n0.994\n\nFigure 3: Visualization of visual input and the sound-derived loss map in four scenes. Top:\nRGB images of the agent’s view with the target object in a red bounding box. Middle: Semantic\nmap produced from the RGB images. Bottom: The red line indicates the path the agent takes, with\nthe end point shown as a circular dot. The ground-truth object location is shown as a gold star.\n\n4.2 NAVIGATION AND PLANNING\n\nExperimental Setup. We use the TDW (Gan et al., 2020b) simulation platform to evaluate our\nproposed method. Our networks are trained on the Find Fallen Object dataset1, following the ex-\nperimental configuration described in Gan et al. (2022). This dataset contains 8000 instances of 30\nphysical object types in 64 physically different rooms (32 study rooms and 32 kitchens), providing\na wide range of acoustic scenarios for evaluation. We evaluated the models’ performance on the\ntest split identified by Gan et al. (2022). The audio was only available at the beginning of the test,\nand the agent would receive an RGB-D image at every subsequent step. For the available actions,\nwe set move forward to 0.25 m and rotate to 30 degrees. The task was defined as follows:\nan embodied agent with an egocentric-view camera and microphone hears an unknown object fall\nsomewhere in the room it is in (a study room or kitchen) as shown in Figure 3; the agent is then\nrequired to integrate audio-visual information to find which object has fallen and where it is, as effi-\nciently as possible. Audio is recorded at 44.1 kHz in a two-channel configuration. We generate the\nSTFT representation using a window and FFT size of 512, a hop length of 128, and a Hann window.\nThe PSD representation is generated using Welch’s method with a Hann window of size 256, an\noverlap of 128, and FFT size of 256.\n\nWe evaluate agents using three metrics: Success Rate, Success weighted by Path Length (SPL)\n(Anderson et al., 2018), and Success weighted by Number of Actions (SNA) (Chen et al., 2020b).\nThe Success Rate is calculated as the ratio of successful navigation trials to the total number of trials.\nA trial is considered successful if the agent explicitly executes action found when the distance\nbetween the agent and the object is less than 2 meters, the target physical object is visible in the\nagent’s view, and the number of actions executed so far is less than the maximum number of allowed\n\n1https://github.com/chuangg/find fallen objects\n\n7\n\nCoasterWineglassVaseToasterRGBSemantic MaskLoss Surface\fUnder review as a conference paper at ICLR 2024\n\nsteps (set to 200 in all tests). SPL is a metric that jointly considers the success rate and the path\nlength to reach the goal from the starting point. SNA takes into account the number of actions and\nthe success rate, penalizing collisions, rotations, and height adjustments taken to find the targets.\n\nTable 3: Comparison against baseline methods on the localization of fallen objects. We find that\nour full model which utilizes a disentangled model to produce an uncertainty surface is significantly\nimproved when evaluated on SR, SPL, and SNA metrics. Ablation studies which progressively\nreplace parts of the original method with ones trained along our disentangled model show the con-\ntribution of each component. Baseline results are taken from Gan et al. (2022).\n\nMethod\n\nSR ↑\n\nSPL ↑\n\nSNA↑\n\nDecision TransFormer (Chen et al., 2021b)\nPPO (Oracle found) (Schulman et al., 2017)\nSAVi (Chen et al., 2021a)\nObject-Goal (Chaplot et al., 2020)\nModular Planning (Gan et al., 2022)\n\nModular Planning + Loss Map\nModular Planning + Our Position\nModular Planning + Our Type\nOurs (Full model)\n\n0.17\n0.19\n0.23\n0.22\n0.41\n\n0.43\n0.44\n0.51\n0.57\n\n0.12\n0.15\n0.16\n0.18\n0.27\n\n0.30\n0.29\n0.34\n0.38\n\n0.14\n0.14\n0.10\n0.17\n0.25\n\n0.29\n0.28\n0.34\n0.37\n\nFigure 4: Comparison of agent trajectories. We compare the agent trajectories using our method\n(Red) against the trajectories produced by the modular planning baseline (Green). The loss map\nuses dark blue to indicate regions of low error, while yellow is used to indicate regions of high error.\nThis figure compares the uncertainty maps of various cases. Darker colors indicate lower values of\nposition loss. The star (Gold) symbolizes the ground truth position of the fallen object. The end of\neach trajectory is circled in white for clarity. In (a)−(f), the baseline method fails to find the target,\nwhile our method succeeds. In (g)−(h), both methods find the target, but our method takes a shorter\npath.\n\nResult analysis. We evaluate the effectiveness of our proposed method against strong baselines as\nproposed in Gan et al. (2022). In Table 3, we find that our disentangled model based method can\nsignificantly outperform the previous modular planning baseline by 14% absolute in success rate.\nWe further observe a significant decrease in the average length of the path and the number of actions\ntaken by the agents. In addition to comparing our full proposed method with the previous modular\nplanning baseline, we conduct an ablation study to assess the necessity of introducing decoders that\ngenerate audio signals from object properties. Specifically, we evaluate a variant of our method\nthat navigates to the location predicted by the encoder. This alternative approach shows comparable\nsuccess rates, but with lower SPL and SNA metrics, emphasizing the critical role of the loss map in\neffective exploration. We visualize the loss map and trajectories taken by the agent in Figure 4. We\nobserve in Figure 4 (a)∼(f) that the modular planner often fails to find the target and attempts to find\n\n8\n\n(a)(b)(c)(d)(e)(f)(g)(h)DAFsBaselineHigh ErrorLow ErrorReconstruction error\fUnder review as a conference paper at ICLR 2024\n\nthe object via random search, while our method can plan a direct route to the object. In Figure 4\n(g)∼(h), both methods find the target, but our method takes a shorter path. These results illustrate\nthe superiority of our proposed approach over the baseline modular planner.\n\nAblation studies. The modular nature of our proposed method facilitates ablation studies that yield\ninsight into the contribution of each individual component. We report results for ablations in Ta-\nble 3. Beginning with the modular planning baseline, we find that augmenting the planner with a\nloss-map-guided object priority ranker yields a 2% increase in SR, a 3% increase in SPL, and 4%\nincrease in SNA. This shows that the introduction of the uncertainty map can effectively improve\nthe efficiency of agents searching for potential objects, reducing both the length of the path and the\nnumber of actions taken. Additionally, we replaced the sound location and sound type predictors in\nmodular planning with our predictor jointly trained with a generator. The improvement in the object\ntype prediction accuracy was found to contribute more to the overall SR than the improvement in\nthe position accuracy. This result corroborates the conclusion in Gan et al. (2022) that accurately\npredicting object types from audio alone is a major challenge.\n\nTable 4: Evaluation of cross-scene prediction for DAFs. Compared to Table 1, there is a small\ndecrease in accuracy.\n\nScene\n\nPosition Error (m)↓ Type Acc.↑\n\nKitchen to Study Room\nStudy Room to Kitchen\n\n1.17\n1.23\n\n0.81\n0.80\n\nTable 5: Evaluation of cross-scene generalization of different methods. We find that all methods\nwill experience a decrease in success rate as evaluated by different metrics. However, by analyzing\nthe sound information using a disentangled framework, our DAFs can robustly generalize to novel\nscene types unseen during training. Baselines are taken from Gan et al. (2022).\n\nKitchen to Study Room\n\nStudy Room to Kitchen\n\nMethod\n\nSR ↑\n\nSPL ↑\n\nSNA ↑\n\nSR ↑\n\nSPL ↑\n\nSNA ↑\n\nPPO (Oracle found)\nSAVi\nDecision TransFormer\nObject Navigation\nModular Planning\n\nOurs\n\n0.11\n0.20\n0.07\n0.18\n0.34\n\n0.52\n\n0.10\n0.11\n0.06\n0.14\n0.23\n\n0.38\n\n0.10\n0.09\n0.06\n0.13\n0.20\n\n0.37\n\n0.05\n0.19\n0.08\n0.15\n0.35\n\n0.48\n\n0.04\n0.14\n0.06\n0.14\n0.22\n\n0.32\n\n0.05\n0.11\n0.07\n0.13\n0.19\n\n0.32\n\nCross-Scene Generalization. To explicitly assess the generalization ability of our proposed method,\nwe train and test on entirely different classes of rooms. In the first split, models are trained in the\nkitchens and tested in the study rooms. For the second split, models are trained in the study rooms\nand tested in the kitchens. The object property prediction results are reported in Table 4. In both\nsplits, the accuracy of positioning and predicting the type of object slightly decreased compared\nto that of the full-trained model. The planning results are reported in Table 5, where all models\nexperience a degree of cross-scene performance drop. The success rate of the modular planning\napproach decreases by 7% in SR on the first split, while our method only decreases by 4%. Our\nproposed method still performs the best in both splits. This highlights that our method can not only\ngeneralize across room instances of the same type, but can also effectively generalize across rooms\nof a different type.\n\n5 CONCLUSION\n\nThis paper presents an egocentric disentangled acoustic field framework that can generalize and rea-\nson across scenes. Joint inference of sound properties is implemented by using an explicit decompo-\nsition in the latent space. Furthermore, our approach is capable of generating multimodal uncertainty\nmaps. Experiments on the TDW simulation platform demonstrate our disentangled acoustic field can\nimprove the success rate for the localization of fallen objects. Consequently, our proposed method\nis a promising solution for sound localization and understanding in complex scenes.\n\n9\n\n\fUnder review as a conference paper at ICLR 2024",
    "abstract": "We study the problem of multimodal physical scene understanding, where an embodied agent needs to find fallen objects by inferring object properties, direction, and distance of an impact sound source. Previous works adopt feed-forward neural networks to directly regress the variables from sound, leading to poor generalization and domain adaptation issues. In this paper, we illustrate that learning an inverse model of acoustic formation, referred to as disentangled acoustic field (DAF), to capture the sound generation and propagation process, enables the embodied agent to construct a spatial uncertainty map over where the objects may have fallen. We demonstrate that our analysis-by-synthesis framework can jointly infer sound properties by explicitly decomposing and factorizing the latent space of the inverse model. We further show that the spatial uncertainty map can significantly improve the success rate for the localization of fallen objects by proposing multiple plausible exploration locations.",
    "response": {
      "Soundness": 2,
      "Presentation": 3,
      "Contribution": 2,
      "Rating": 5,
      "Confidence": 4,
      "Strengths": "1. The proposed architecture is interesting. If we can obtain lots of pairs of audio signals and object properties, the proposed architecture might be one of the best choices for simultaneously analyzing and synthesizing audio scenes.\n\n2. Based on the experimental evaluations, the proposed method reasonably work well for several tasks.",
      "Weaknesses": "1. The task of this paper is closely related to sound source localization. If the authors justify the effectiveness of the proposed method in terms of position errors of sound sources, experimental comparisons with several previous methods for this task would be mandatory. As presented in [Grumiaux+ JASA 2021 https://arxiv.org/abs/2109.03465], lots of previous methods have already been proposed for this purpose. I understand that many of those previous methods typically require multi-channel audio inputs and the proposed method employs a single-channel or binaural audio clip. However, you will find several techniques that work well only with single-channel audio, since single-channel sound source localization has been one of the typical tasks in audio signal processing before the deep learning era.\n\n2. If we want to simply predict acoustic scenes from audio signals, we do not have to introduce decoders that generate audio signals from object properties. In this sense, the ablation study that compares the full proposed method and the one without decoders should be presented.\n\n3. If the main objective of this paper is a proposal of a novel construction of neural acoustic fields, the proposed method should be compared with the original NAFs in the task of audio synthesis.",
      "Questions": "Please check the above Weakness section."
    }
  },
  {
    "paper_id": "zUHgYRRAWl",
    "summary": "Background: Variational autoencoders (VAE) have become popular in drug discovery for generating molecules, leveraging large-scale datasets like ChEMBL and ZINC. However, they struggle with poor manifold recovery in complex data settings—a challenge known in high-dimensional data spaces.\nGap: Current VAE approaches for molecule generation suffer from poor manifold recovery, particularly when data is embedded on a low-dimensional manifold within a high-dimensional space. This issue leads to inferior property matching with the target dataset.\nHypothesis: A multi-stage VAE approach can improve manifold recovery over traditional single-stage VAEs by creating better latent variable representations, enhancing the quality of generated molecules without using property predictors in the training process.\nMethod: The authors propose a multi-stage VAE method where subsequent VAEs are trained on the latent representations of the prior stage. The approach was tested on a synthetic dataset and the ChEMBL dataset for molecule generation, along with fine-tuning on smaller datasets targeting specific protein activities. The study compares the multi-stage VAE against one-stage models and other methods that integrate property predictors.\nConclusion: The multi-stage VAE demonstrates improved manifold recovery, generating molecules with properties more similar to target datasets without relying on property predictors. It performs effectively against models using learned property metrics, offering an objective-agnostic improvement method.\n",
    "full text": "TIES VIA MULTI-STAGE VAE\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nVariational autoencoder (VAE) is a popular method for drug discovery and various architectures and\npipelines have been proposed to improve its performance. However, VAE approaches are known to\nsuffer from poor manifold recovery when the data lie on a low-dimensional manifold embedded in a\nhigher dimensional ambient space (Dai and Wipf, 2019). The consequences of it in drug discovery\nare somewhat under-explored. In this paper, we explore applying a multi-stage VAE approach, that\ncan improve manifold recovery on a synthetic dataset, to the field of drug discovery. We experimen-\ntally evaluate our multi-stage VAE approach using the ChEMBL dataset and demonstrate its ability\nto improve the property statistics of generated molecules substantially from pre-existing methods\nwithout incorporating property predictors into the training pipeline. We further fine-tune our models\non two curated and much smaller molecule datasets that target different proteins. Our experiments\nshow an increase in the number of active molecules generated by the multi-stage VAE in compari-\nson to their one-stage equivalence. For each of the two tasks, our baselines include methods that use\nlearned property predictors to incorporate target metrics directly into the training objective and we\ndiscuss the complications that arise with this methodology.\n\n1\n\nINTRODUCTION\n\nThe use of generative models in the domain of drug discovery has recently seen rapid progress. These methods can\nleverage large-scale molecule archives describing the structure of existing drugs to synthesize novel molecules with\nsimilar properties as potential candidates for future drugs (Duvenaud et al., 2015; Liu et al., 2018; Segler et al., 2018;\nYou et al., 2018; Jin et al., 2018; 2020a; Polykovskiy et al., 2020; Jin et al., 2020b; Satorras et al., 2021; Maziarz et al.,\n2021; Hoogeboom et al., 2022). There are two common ways of representing the structure of molecules: Simplified\nMolecular Input Line Entry System (SMILES) (Weininger, 1988) and molecular graphs (Bonchev, 1991). Graph\nneural networks can make effective use of the rich molecular graph representations by taking into account atoms,\nedges, and other structural information. SMILES strings convey less information about the molecular structure, but\nare more compatible with conventional sequence models (e.g., RNNs). Being able to generate valid molecules is the\nfirst step to AI-driven drug discovery and various solutions have been proposed to this problem. For example, GNN\nmethods (Liu et al., 2018; Simonovsky and Komodakis, 2018; Jin et al., 2020a; Maziarz et al., 2021) can constrain the\noutput space based on the chemical rules and SMILES-based approaches (Gómez-Bombarelli et al., 2018; Blaschke\net al., 2018) can benefit from the abundant molecular data.\n\nBesides structural validity, various chemical properties of the generated molecules, such as drug-likeness (QED) (Bick-\nerton et al., 2012), Synthetic Accessibility (SA) (Ertl and Schuffenhauer, 2009) and molecular weight (MW) are critical\nfactors when deciding whether candidate molecules can be synthesized in a laboratory and if they can be effective in\nreal-world applications. A molecule’s activity level on protein targets, whether to inhibit or to activate, is another very\nimportant property when treating specific diseases. A molecule that interacts successfully with the protein target is\nconsidered active and an activity score is measured based on how effective it is either to activate or inhibit the protein\ntarget’s biological function. Researchers collected large molecule datasets, such as ChEMBL (Mendez et al., 2019)\nand ZINC (Irwin and Shoichet, 2005), that contain an array of bioactive molecules together with information about\ntheir properties and protein targets. By training on a curated set of molecules, the generative models can learn to\ngenerate new molecules that are similar in properties to those in the training set in order to produce novel drug can-\ndidates that satisfy multiple objectives, e.g. being drug-like and active against multiple protein targets. Benchmark\nmetrics (Polykovskiy et al., 2020; Brown et al., 2019) are created to measure how similar the generated molecules\nare to the target dataset structurally and property-wise. The state-of-the-art results, however, show that there is still\n\n1\n\n\froom for improvements. Multi-objective generation by incorporating property predictors in the training pipeline (Jin\net al., 2020b; Maziarz et al., 2021) is a promising avenue to address this type of problems, but there are also potential\ndrawbacks. As Winter et al. (2019) summarized that, in drug discovery, the optimized objectives can be complex,\nconflicting, ill-defined or evolving over time. This could lead to improving some objectives while degrading others.\n\nIn this paper, we introduce an objective-agnostic and easy-to-implement technique to improve existing VAE-based\nmolecule generation models – training additional stages of VAE’s to generate latent representations for the previous-\nstage VAE. To show how this approach can enhance the manifold recovery of VAE models, we first study a simple MLP\nmodel trained on a synthetic sphere dataset. We then evaluate our method in an unconstrained molecule generation\ntask and a fine-tuning task. In these experiments, we demonstrate the following claims:\n\n• The multi-stage VAE is able to bring the properties of the generated molecules closer in distribution to the\n\ntesting set in experiments on the ChEMBL dataset for unconstrained molecule generation;\n\n• Fine-tuning the multi-stage VAE on the curated active molecules of two protein targets results in substantially\n\nmore active outputs than fine-tuning only the first-stage model;\n\n• In the tasks we described above, our method can achieve comparable or better results than specialized meth-\n\nods that directly optimize one or multiple target objectives using learned property predictors.\n\n2 RELATED WORK\n\nWe structure our discussing based on the type of molecular representation underlying the individual methods. Most\nprior approaches fall into one of the following families – namely, the SMILES string approach, the molecular graph\napproach and the 3D point set approach. Many approaches have been proposed to generate molecules as SMILES\nstrings (Segler et al., 2018; Gómez-Bombarelli et al., 2018). Kusner et al. (2017) and Dai et al. (2018) took advantage\nof the syntax of the SMILES strings and constrained the output of the VAE model in order to improve the validity of the\ngenerated molecules. Generative adversarial models have also been proposed to generate SMILES strings (Kadurin\net al., 2017; Prykhodko et al., 2019; Guimaraes et al., 2017). Molecular graphs carry more information about the\nmolecular structures than the SMILES string format, and GNN can effectively incorporate the additional information\ninto the learning process (Duvenaud et al., 2015; Liu et al., 2018; Maziarz et al., 2021). Jin et al. (2018) proposed\nto generate molecular graphs in two steps – generate the tree-structured scaffolds first, and then combine these with\nthe substructures to form molecules. Jin et al. (2020a) improved upon this prior result and proposed to generate new\nmolecules via substructures in a coarse-to-fine manner to adapt to larger molecules, such as polymers. Satorras et al.\n(2021) introduced an equivariant graph neural network that can operate on molecular graphs. 3D representations\nof molecules are gaining traction in the research communities as they describe detailed spatial information of the\nmolecules (Gebauer et al., 2019; 2022; Luo et al., 2021; Hoogeboom et al., 2022). However, none of the methods\nuse the VAE framework. The goal of our paper is to improve upon the existing the VAE-based approaches. Other\ngenerative approaches to drug discovery include generative adversarial models (Kadurin et al., 2017; Prykhodko et al.,\n2019; Guimaraes et al., 2017) and diffusion models (Hoogeboom et al., 2022; Xu et al., 2022; Vignac et al., 2022).\n\n3 METHOD\n\nThe VAE framework (Kingma and Welling, 2013) has enabled great success in the image generation domain and\nmore recently VAE-based approaches have become popular for addressing the molecule generation problem. Many\nsophisticated architectures have been proposed to adapt it to molecular data (Kusner et al., 2017; Dai et al., 2018; Jin\net al., 2019; Satorras et al., 2021). However, perfecting the underlying neural architecture does not remedy VAE’s\nlearning deficiency in manifold recovery (Dai and Wipf, 2019; Koehler et al., 2021). The manifold hypothesis (Fef-\nferman et al., 2016) states that many high-dimensional real life data lie on low-dimensional manifolds embedded in\nhigh-dimensional ambient spaces. Koehler et al. (2021) found that the VAE is not guaranteed to recover the low-\ndimensional manifold where a nonlinear dataset lie. We show that a multi-stage VAE method can improve manifold\nrecovery as demonstrated in a synthetic experiment (Figure 2) and further enhance the performance of pre-existing\nVAE models.\n\n2\n\n\fFigure 1: Overview of multi-stage VAE. In the first stage, the VAE trains on the molecule data xi and obtains the latent variables\nz1\ni from xi. The later-stage VAE is trained on the latent variables of the previous-stage VAE. zs\ni ’s from the s-th stage VAE become\nthe input to the s + 1-th stage VAE during training. The later-stage VAE’s input dimension is equal to the output dimension. During\nsampling, we sample z ∼ N (0, I) and obtain zs\ni from the decoder. The output of a later-stage VAE decoder is used as the input\nfor the previous-stage VAE decoder until the latent variable is decoded into a new molecule x′\n\ni in the first-stage.\n\n3.1 VARIATIONAL AUTOENCODER\n\nThe variational inference framework assumes that the data x is generated from a latent variable z ∼ p(z). The\nprior p(z) is assumed to be a multivariate standard normal distribution in the application of a VAE. Let ϕ be\nthe variational parameters and θ denotes the generative parameters, the VAE model consists of a tractable en-\ncoder qϕ(z|x) and a decoder pθ(x|z). A VAE model seeks to maximize the likelihood of the data, denoted as\nlog pθ(x) = log (cid:82) p(z)pθ(x|z)dz. However, the marginalization is intractable in practice due to the inherent com-\nplexity of the generator, or the decoder, thus an approximation of the objective is needed. The encoder and the decoder\nwork together to approximate a lower bound to the log likelihood of the data. Ideally, by optimizing this lower bound\nwe aim to increase the likelihood. This approximation enables the efficient posterior inference of the latent variable z\ngiven the input xi and for marginal inference of the output variable x. The objective function of VAE consists of a KL\ndivergence term DKL and a reconstruction term:\n\nL(θ, ϕ; x) = −DKL(qϕ(z|x) || p(z)) + Eqϕ(z|x)[log pθ(x|z)] ≤ log pθ(x)\nFor generation, latent representation zi is sampled from the prior p(z) which is a multivariate standard normal and the\ndecoder transforms zi into the output x′\ni.\n\n(1)\n\n3.2 MULTI-STAGE VAE\n\nRecovery of a low-dimensional data manifold embedded in a high-dimensional ambient space is a challenging task\nfor VAE to perform. This provides an explanation for the discrepancy in properties between the generated molecules\nfrom existing VAE approaches and the testing dataset. Dai and Wipf (2019) hypothesized that training a continous\nVAE with a fixed decoder variance could add additional noise to the output. While training a VAE with a tunable\ndecoder variance, they observed that the decoder variance has a tendency to approach zero and the diagonal entries\nof the encoder covariance to converge to either 0 or 1. A second VAE trained on the encoded latent variables of the\nfirst one with close to 0 decoder variance yields crisper and more realistic images than a one-stage VAE. By studying\nsynthetically generated data with feedforward neural network, Koehler et al. (2021) showed that when the data is not\nlinear, neither the manifold nor the density is guaranteed to be recovered by a one-stage VAE with tunable decoder\nvariance. Without limiting ourselves to only 2 stages of VAE, we demonstrate in the following synthetic experiment\nthat a multi-stage VAE can improve manifold recovery. We further apply the multi-stage VAE to the task of molecule\ngeneration involving much more complex architecture such as graph neural network and show that it is able to improve\nthe distance between the property distribution of the test set and the generated set.\n\nSynthetic Experiment We demonstrate that a multi-stage VAE setup improves the recovery of the manifold in a\nsynthetic experiment with data generated from a ground-truth manifold (Koehler et al., 2021). We generated data from\na 2-dimensional unit sphere such that the norms of all the generated data points are 1 (see Figure 2). The generated data\nis 3 dimensional but lies on the 2-dimensional unit sphere surface. The vectors are then padded with 16 dimensions of\nzeros to embed the data in an even higher-dimensional ambient space. The intrinsic dimension of the data is 2 and the\nambient dimension is 19. We trained on this data in 3 stages – meaning, the encoded latent variables from the previous\nstage are used as input for training in the next stage. For the second and the third stage VAE, the latent dimension is set\nto be the same as their input (Figure 1 right). The decoder variance is tunable for all stages and the decoder variance\n\n3\n\n\f(a) Stage 1\n\n(b) Stage 2\n\n(c) Stage 3\n\nFigure 2: Multi-stage VAE on synthetic data. The x-axis represents the norm of the data point and the y-axis represents the number\nof data points that are of x distance away from the unit sphere center. The ground-truth data’s histogram should be a Dirac delta at\nlocation 1. The figure for stage 1 shows that most of the generated points fall inside of the sphere instead. The sphere surface is\nmostly recovered and improved starting from stage 2.\n\nof the first stage approaches 0 upon convergence. During sampling, the output of the later stage decoder is used as the\ninput latent variable of the previous stage decoder. The last-stage VAE’s latent variables are sampled from a standard\nnormal distribution. We sample 1000 data points to visualize the results in the histograms. We observe that the VAE\nin the first stage does not recover the manifold and many of the generated data points fall inside of the sphere, echoing\nthe finding by Koehler et al. (2021). In the second and third stage, we see that more data points fall close to the sphere,\nindicating a better recovery of the manifold. The third-stage has slightly fewer points falling below the distance of\n0.95 to the origin than the second in this experiment.\n\nApplication on Molecule Generation The synthetic experiment is run on continuous data with a simple feed-\nforward architecture for both the decoder and the encoder. In this simple setting, two conditions on the first-stage VAE\nare satisfied for improvements in the later stage to occur (Dai and Wipf, 2019): i) the decoder variance converges to 0;\nand, ii) the entries in the diagonal of the encoder variance converge to either 0 or 1. The first condition can be applied\nto VAE’s with multinomial setup common to molecule generation tasks and the decoder variance approaching zero is\nequivalent to the output being deterministic given the same latent variable. The second condition can also be verified\nbecause the latent space of the VAE models for molecule generation is generally continuous. However, the VAE\narchitectures used for molecule generation are much more complex. Molecular data is discrete and the architectures\nfor encoders and decoders can be hierarchical or sequential in their generation process. In Section 4, we examine how\nwell each of the models we tested on fulfills the two conditions outlined for the continuous model. We also provide\nempirical studies on the suitability of a multi-stage VAE approach in the molecule generation domain by thorough\nevaluations on the samples’ quality using structural and property statistics (Polykovskiy et al., 2020). We find that the\nmulti-stage VAE helps to generate molecules that are more similar in property to the test set. Below we present the\nprecise steps to train a multi-stage VAE (Figure 1):\n\n1. Train a VAE on the molecular dataset {xi | i = 1, 2, . . . n}, and upon convergence, save the latent variables\n\nz1 ∼ qϕ(z1|x = xi) for all the molecules in the dataset;\n\n2. With {z1\n\ni | i = 1, 2, . . . n} as input, train additional stages of VAE with tunable decoder variance and the latent\nvariable at stage s is denoted as zs. We use feed-forward architectures for both the decoder pθs (zs|zs+1) and\nthe encoder qϕs(zs+1|zs). They both follow Gaussian distributions. The dimension of zs are the same across\ndifferent stages. Repeat this step until the final stage, and the last-stage latent variable is denoted as z.\n\n3. During the sampling process, sample the latent representation of the last stage VAE, z ∼ N (0, I). Obtain\n) as the input to the stage-(s − 1) decoder.\ni from the first stage decoder\n\nthe output from the stage-s decoder zs ∼ pθs(zs|zs+1 = z′s+1\nRepeat until we reach the first stage VAE and get the new molecule sample x′\nvia x ∼ pθ(x|z1 = z′1\n\ni\n\ni ).\n\nWe observe in our experiments that, for each additional stage of VAE we train, the decoder variance converges to a\nlarger value than the previous one (details in Appendix B). And when the new stage’s decoder variance converges\nto 1, the improvements become minimal. One way to interpret this method is that while the first-stage VAE learns\na deterministic mapping between the latent representations and the molecular output, the later-stage VAEs learn to\n\n4\n\n\fmap standard normal distribution to the distribution of the latent representation of the first-stage VAE. This is because\nthe latent variables of the first-stage VAE do not necessarily follow the standard normal distribution as is generally\nassumed (Dai and Wipf, 2019). In the next section, we experiment on three pre-existing VAE models for general\nmolecule generation task. We verify if they meet the two conditions outlined for the continuous synthetic setting\nearlier. And the models that do are able to generate molecules more similar to the test set by training additional\nstages of VAE. We also show that the multi-stage VAE can increase the number of active molecules generated when\nfine-tuned for a protein target.\n\n4 EXPERIMENTS\n\nIn this section, we demonstrate the effectiveness of our methods on two generation tasks. First, our algorithm learns\nto generate molecules by training on a large molecular database (i.e., ChEMBL), and we show that training additional\nstages of VAE is able to generate molecules more similar to the test set in properties than training only a single stage.\nOur baselines include a model (Maziarz et al., 2021) that minimizes the losses between the true property values of the\ninput molecules and the predicted property values of the output molecules for multiple targets during training and our\nmethod shows more consistent improvements across different properties. Second, our algorithm is fine-tuned on two\ncurated molecular datasets that are active against two different protein targets (JAK2 and EGFR), and the multi-stage\nVAE improves the activity rate among the generated molecules. In this experiment, we compare our method against an\nRL algorithm (Jin et al., 2020b) using activity predictor as the reward function and show that we can achieve equivalent\nor better results in much shorter time.\n\n4.1 UNCONSTRAINED GENERATION\n\nWe adopt three existing VAE model as our first-stage VAE – hierarchical GNN (Jin et al., 2020a), MoLeR (Maziarz\net al., 2021), and character-level RNN (Polykovskiy et al., 2020) – to compare the effects of multi-stage VAE on\ndifferent model architectures. We also adopt a GAN-based model (Prykhodko et al., 2019) as an additional comparison.\nWe conduct experiments on the ChEMBL (Mendez et al., 2019) dataset – it consists of 1,799,433 bioactive molecules\nwith drug-like properties. It is split into training, testing, validation datasets containing 1463k, 81k, and 81k molecules\nrespectively. Details on our evaluation metrics are in Appendix A. Below we introduce the details of the models used\nin this study:\n\nHierarchical GNN: This method first extracts chemically valid motifs, or substructures, from the molecular graphs\nsuch that the union of these motifs covers the entire ChEMBL space. The model consists of a fine-to-coarse encoder\nthat encodes from atoms to motifs and a coarse-to-fine decoder that selects motifs to create the molecules while\ndeciding the attachment point between the motif and the emerging molecule. We use the configuration from the\noriginal model. The latent dimension of the VAE is 20 and we use 0.1 as the KL coefficient.\n\nMoLeR GNN: Similarly to the hierarchical GNN, this method also extracts motifs in order to generate molecules\npiece by piece. The method’s objective includes a regression term to match the true properties of the molecules\nand the predicted molecule properties from the latent variables. We implement our multi-stage method without such\nterm and compare the property metrics of the generate molecules from our method to directly matching them in the\ntraining objective as \"MoLeR + prop\" in Table 1. We reduce the latent dimensions to 64 and use all the other original\nconfiguration.\n\nVanilla RNN: The inputs to the model are SMILES strings and the vocabulary consists of the low-level symbols in the\nSMILES strings. The encoder is a 1-layer GRU and the decoder is a 3-layer GRU. The latent dimension of the VAE is\n128. We use the original configuration except a reduced KL coefficient of 0.01.\n\nLatent GAN: This is also a 2-stage method. The first stage is a heteroencoder that takes SMILES strings as input\nwhile the second stage is a Wasserstein GAN with gradient penalty (WGAN-GP) that trains on the latent variables\nof the first stage encoder. The heteroencoder consists of an encoder and a decoder like an autoencoder and is trained\nwith categorical cross-entropy loss. Afterwards, the GAN is trained to generate latent variables for the decoder of the\nheteroencoder. We use the original parameters for training.\n\nWe sample 10,000 molecules from each model to generate the results in Table 1. We include sample quality, structural\nand property statistics. The entries in the property statistics section are the Wasserstein distances between the property\ndistribution of the test set and the generated set. A lower value in these statistics signals increased similarity to the test\nset in these properties. The results in the tables are averaged over 6 sets of samples generated with different random\n\n5\n\n\fStage #\nHGNN#1\nHGNN#2\nHGNN#3\nMoLeR#1\nMoLeR #2\nMoLeR #3\nMoLeR + prop\nRNN#1\nRNN#2\nLatentGan\n\nSample Quality\n\nValid ↑ Unique ↑ Novelty ↑\n\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.86\n0.87\n0.77\n\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.98\n\n0.99\n0.99\n1.0\n0.99\n0.99\n0.99\n0.99\n1.0\n1.0\n0.99\n\nFCD ↓\n5.1\n1.1\n1.2\n2.1\n2.2\n1.8\n2.1\n1.84\n1.86\n17.3\n\nStructural Statistics\nFrag ↑\n0.97\n1.0\n1.0\n0.96\n0.96\n0.96\n0.97\n1.0\n1.0\n0.68\n\nSNN ↑\n0.42\n0.41\n0.41\n0.41\n0.42\n0.42\n0.43\n0.38\n0.38\n0.34\n\nScaf ↑\n0.46\n0.43\n0.46\n0.48\n0.53\n0.49\n0.49\n0.38\n0.36\n0.21\n\nLogP ↓\n0.920.016\n0.0950.019\n0.0594.5e−3\n0.166.78e−3\n0.126.13e−3\n0.0876.16e−03\n0.111.03e−02\n0.0887.8e−3\n0.0995.5e−3\n0.690.019\n\nProperty Statistics\nSA ↓\n0.0704.3e−3\n0.0695.8e−3\n0.0696.3e−3\n0.0281.96e−3\n0.0414.31e−3\n0.0312.81e−03\n0.132.51e−03\n0.25 7.8e−3\n0.277.7e−3\n0.637.3e−3\n\nQED ↓\n0.0249.5e−4\n0.00671.0e−3\n0.0161.6e−3\n0.0472.78e−3\n0.0369.25e−4\n0.0282.31e−03\n0.0338.65e−04\n0.00881.6e−3\n0.00991.5 e−3\n0.0472.0e−3\n\nMW ↓\n68.80.83\n5.00.72\n7.70.42\n9.60.70\n6.80.71\n8.24.0e−01\n6.64.80e−01\n3.20.55\n2.80.29\n27.20.88\n\nTable 1: Properties of the generated molecules trained on the ChEMBL dataset.\n\nseeds from the model. We include the standard deviations for the property statistics but eliminated the rest as those are\nbelow 0.01. Stage #1 results are from the original models.\n\nThe HGNN#2 improves upon the first stage by many folds on property statistics. The most notable improvement\nfrom the ChEMBL dataset is the QED (from 0.024 to 0.0067), MW (from 68.8 to 5.0) and LogP (0.92 to 0.059).\nHGNN#3 does not show consistent improvement from HGNN#2. Structural statistics generally did not change a lot\nin the second and the third stage. The performance on these metrics of the later stages models may be bottle-necked\nby the first-stage graph decoder.\n\nWe trained three stages of the MoLeR model. The MoLeR#2 improves upon the MoLeR#1 in three (LogP, QED and\nMW) out of four metrics, while MoLeR#3 further improves upon MoLeR#2 in three (LogP, SA and QED) out of\nfour metrics. Overall, MoLeR #3 improves upon MoLeR #1 in all property metrics except SA (from 0.028 to 0.031,\nonly slightly higher). In LogP and QED, the metric goes down almost a half from 0.16 to 0.087 and from 0.047 to\n0.028. The regression term in the \"MoLeR + prop\" optimizes over LogP, SA and MW. For two of these three metrics,\n\"MoLeR + prop\" reaches lower statistics than MoLeR#1, where such regression term is left out of the objective, but\nthe SA metric of \"MoLeR + prop\" is more than four times higher than without the regression term. This highlights\nthe challenges of directly matching multiple statistics at once during training – the objectives could be conflicting with\neach other and lead to unexpected results. Eventually, the MoLeR#3 reaches lower property statistics than \"MoLeR +\nprop\" in three (LogP, SA and QED) out of four metrics. For the only property metric (MW) our algorithm was worse\n(or bigger distance) in, it is only 24% worse than \"MoLeR+prop\" while for metric such as SA, our outputs are more\nthan 300% better than \"MoLeR+prop\".\n\nRNN#2 performs worse than RNN#1 in three out of four metrics. The RNN VAE’s training process does not follow\na standard VAE training procedure – the SMILES strings are input to both the encoder and decoder. This allows\nthe decoder to rely less on the latent variables during the decoding process and the model exhibits signs of posterior\ncollapse (Razavi et al., 2019; Fu et al., 2019) during training. Our hypothesis for the poor performance of the multi-\nstage VAE with the RNN model as the first-stage is that the variance of the first-stage decoder did not fulfill the\ncondition of approaching 0 upon convergence and we will discuss more on that next.\n\nVerification on the Encoder and Decoder Variance Conditions We investigate how well each of the three first-\nstage models fulfill the original conditions in a continuous setting for improvements in the later stages to occur. The\nexamination can also help explain the different behaviors from them. We present our findings in Table 2.\n\n1. The decoder variance of the first-stage model converges to zero. Variance in a discrete generation setting is not\neasy to measure, but we can substitute variance calculation with a simple experiment – input the same latent\nvariable to the trained decoder for 1000 times, the number of distinct molecules the model generates can give\nus an indication if the decoder variance is approaching zero. We observe that for both of the GNN models,\nall 1000 identical latent variables generate 1000 identical molecules (Table 2 column \"Decoder Diversity\").\nWe can consider both models to have zero decoder variance. In contrast, the RNN model generates 1000\ndistinctive SMILES strings. Thus, its decoder variance did not approach 0 and our earlier hypothesis is\nverified.\n\n2. Each entry of the encoder variance diagonal either converges to 0 or 1. We allow for 0.1 of tolerance around\n0 and 1 and we find the following: The HGNN model’s 20-dimensional encoder variance diagonal has all\nconverged to 0; RNN’s 128-dimensional encoder variance diagonal have converged to 111 of 1’s, 15 of 0’s;\n\n6\n\n\fand MoLeR’s 64 dimensions of encoder variance diagonal have converged to 45 of 1’s, 6 of 0’s. The rest of\nthe dimensions fall somewhere in between 0.1 and 0.9 as in Table 2’s first three columns.\n\nStage 1 Model\nHGNN\nMoLeR\nRNN\n\nx < 0.1\n20\n6\n15\n\n0.1 ≤ x ≤ 0.9\n0\n13\n2\n\nx > 0.9 Decoder Diversity\n\n0\n45\n111\n\n1\n1\n1000\n\nTable 2: The first three columns document the number of encoder variance diagonal value x that fall into either of the three\ncategories: x < 0.1, 0.1 ≤ x ≤ 0.9 and x > 0.9, and the last column document the number of unique SMILES by inputting 1000\nidentical latent vectors.\n\nOverall, we see that the HGNN conforms to the conditions set for the simple continuous settings and the multi-stage\nVAE with HGNN as first-stage is able to improve significantly on the second stage and improve slightly in certain\nmetrics on the third. MoLeR model is able to partially fulfill the conditions and the multi-stage VAE is still able to\nyield better results than the first-stage. The RNN model does not fulfill any of the two conditions and the outcomes\nfrom training the multi-stage VAE with RNN as the first-stage do not follow the synthetic experiment (Figure 2) that\nshows improvements in later stages.\n\n4.2 GENERATION FOR A PROTEIN TARGET\n\nIn addition to unconstrained generation of molecules, we also explore generating molecules that target a specific\nprotein. We pretrain the MoLeR model and two additional stages of VAE (with MoLeR as the first-stage) on the\nChEMBL dataset and fine-tune them on two curated, and much smaller, datasets (Korshunova et al., 2022) consisting\nof molecules that are active inhibitors of Janus Kinas 2 (JAK2) and inhibitors of the Epidermal Growth Factor (EGFR).\nFor each of the protein target, the regression dataset contains the molecules and their corresponding activity scores\n(from 0 to 10). A score above 6 is considered active. The dataset size is around 19k for JAK2, in which around 15.6k\nare active, and around 15k for EGFR, in which around 7.8k are active. The full dataset is used to train a regressor\nof the activity score while only the active molecules are used to fine-tune the VAE. We divide each dataset by 80%,\n10% and 10% for training, validation and testing purposes for both the VAE and the predictor. In addition, there is a\nseparate classification dataset for each protein target – 60k for JAK2 and 50k for EGFR – containing only the binary\nactivity information of each molecule. 10% of the dataset is used for testing the classifiers.\n\nMetrics We evaluate our method and baseline methods in three general categories: activity, diversity and novelty\n(Table 3 and 4). All evaluations are reported with means and standard deviations across 5 random seeds used to\ngenerate 5 datasets with 1000 molecules each. Particularly for activity scores, we report both the mean score and\npercentage of active molecules (6 as the cutoff point) in a dataset from two different regressors – a Chemprop (CPR)\n(Yang et al., 2019) model, and a Random Forest (RFR) model with Morgan fingerprint features (Rogers and Hahn,\n2010). Both regressors reach an RMSE of 0.5 for the JAK2 dataset and 0.6 for the EGFR dataset. In addition to the\nregressors, we also train two classifiers using Chemprop (CPC) and Random Forest (RFC) with Morgan fingerprint\nfeatures on the classification dataset and we report the percentage of active molecules in the generated set by the\nclassifiers as well. All predictors’ performances on the active test set split are included in Table 3 and 4 as a reference.\nNovelty is defined as the fraction of molecules with its nearest neighbor similarity in the active training set lower than\n0.4. Diversity is calculated based on the pairwise molecular distance sim(X, Y ) within the generated datset. The\nfunction sim(·, ·) is defined as the Tanimoto distance over Morgan fingerprints of two molecules. We define the two\nmetrics as follows (Jin et al., 2020b):\n\nDiversity = 1 −\n\n2\nn(n − 1)\n\n(cid:88)\n\nX,Y\n\nsim(X, Y ) Novelty =\n\n1\nn\n\n(cid:88)\n\nG\n\n1[sim(G, GSN N ) < 0.4]\n\n(2)\n\nMethods We compare the generated molecules from the fine-tuned multi-stage MoLeR model to the fine-tuned one-\nstage MoLeR model. We include RationaleRL (Jin et al., 2020b) as a reinforcement learning-based baseline. We\nobtained the multi-stage MoLeR from training on the full ChEMBL dataset as described in Section 3.2. The first-\nstage (also the original) MoLeR model was fine-tuned on the curated active molecules dataset after pretraining on the\nfull ChEMBL dataset. The second and third-stage VAEs are initialized with the parameters from the corresponding\n\n7\n\n\fstage of the pretrained multi-stage MoLeR and fine-tuned on the encoded latent variables of the curated dataset from\nthe fine-tuned previous-stage VAE. We fine-tuned the later-stage VAE’s in three ways: fine-tuning the entire model,\nfine-tuning only the two inner-layers connecting to the latent sampling layer, and fine-tuning only the two outer-layers\nconnecting to the input and output. We visually compare the distributions of the activity scores for EGFR as predicted\nby the Chemprop model from the three types of fine-tuning methods and baselines in Figure 3. The quantitative metrics\nare shown in Table 3 and 4. Each evaluation reports metrics for the active test set as a reference.\n\nModel Type\nActive Test set\nMoLeR\nRationaleRL\nWhole-Model\nInner-Layer\nOuter-Layer\n\nActivity (CPC) Activity (CPR) Mean (CPR) Activity (RFC) Activity (RFR) Mean (RFR)\n\n0.93\n0.441.18 e−02\n0.797.94 e−03\n0.536.43 e−03\n0.791.61 e−02\n0.851.30 e−02\n\n0.92\n0.571.21 e−02\n0.841.45 e−02\n0.636.65 e−03\n0.871.31 e−02\n0.895.31 e−03\n\n7.4\n6.32.94 e−02\n6.81.56 e−02\n6.43.18 e−02\n7.43.92 e−02\n7.23.17 e−02\n\n0.98\n0.411.14 e−02\n0.881.23 e−02\n0.501.38 e−02\n0.881.27 e−02\n0.861.75 e−02\n\n0.89\n0.601.46 e−02\n0.944.50 e−03\n0.694.31 e−03\n0.908.40 e−03\n0.891.17 e−02\n\n7.1\n6.32.16 e−02\n6.71.42 e−02\n6.51.91 e−02\n7.02.04 e−02\n6.92.61 e−02\n\nDiversity\n0.85\n0.872.26 e−03\n0.791.38 e−03\n0.859.95 e−04\n0.714.41 e−03\n0.715.30 e−03\n\nNovelty(0.4)\n0.016\n0.631.81 e−02\n0.0555.24 e−03\n0.549.65 e−03\n0.172.03 e−02\n0.228.78 e−03\n\nTime 1\n-\n4 min\n4.5 d\n1.5 h\n1.25 h\n1.5 h\n\nTable 3: Evaluation of the generated molecules targeting EGFR by three multi-stage VAE fine-tuning methods: fine-tuning the\nwhole model, fine-tuning only the inner-layers and fine-tuning the outer-layers. They are compared against baseline models such\nas fine-tuned one-stage MoLeR and RationaleRL. The evaluation metrics include percentage of active molecules, mean activity\nscores, diversity and novelty.\n\nModel Type\nActive Test set\nMoLeR\nRationaleRL\nWhole-Model\nInner-Layer\nOuter-Layer\n\nActivity (CPC) Activity (CPR) Mean (CPR) Activity (RFC) Activity (RFR) Mean (RFR)\n\n0.97\n0.521.15e−02\n0.858.96e−03\n0.548.31e−03\n0.667.58e−03\n0.622.27e−02\n\n0.97\n0.746.89e−03\n0.791.67e−02\n0.781.02e−02\n0.881.11e−02\n0.898.37e−03\n\n7.5\n6.51.17e−02\n6.62.93e−02\n6.62.22e−02\n6.91.01e−02\n6.93.72e−02\n\n0.99\n0.441.73e−02\n0.924.93e−03\n0.461.13e−02\n0.566.68e−03\n0.651.23e−02\n\n0.98\n0.891.24e−02\n0.967.26e−03\n0.915.90e−03\n0.942.28e−03\n0.951.96e−03\n\n7.4\n6.72.44e−02\n6.81.67e−02\n6.71.16e−02\n6.81.36e−02\n6.91.53e−02\n\nDiversity\n0.88\n0.905.62e−04\n0.872.58e−03\n0.894.47e−04\n0.888.39e−04\n0.861.29e−03\n\nNovelty(0.4)\n0.016\n0.856.71e−03\n0.247.42e−03\n0.827.80e−03\n0.786.29e−03\n0.695.37e−03\n\nTime1\n-\n7 min\n8.5 d\n2.8 h\n2.8 h\n2.8 h\n\nTable 4: Evaluation of the generated molecules targeting JAK2 by by three multi-stage VAE fine-tuning methods: fine-tuning the\nwhole model, fine-tuning only the inner-layers and fine-tuning the outer-layers. They are compared against baseline models such\nas fine-tuned one-stage MoLeR and RationaleRL. The evaluation metrics include percentage of active molecules, mean activity\nscores, diversity and novelty.\nFor both of the protein targets, fine-tuning the multi-stage VAE in either of the three ways produces more active\nmolecules of the protein targets than fine-tuning only the first-stage VAE (Table 3 and 4) with all predictors con-\nsidered. The improvement is especially pronounced on the EGFR dataset, which is about half of the JAK2 dataset\nsize. Particularly, fine-tuning only the inner-layers or outer-layers produces more active molecules than fine-tuning\nthe whole model. Their activity score distributions are also more similar to the test set’s (Figure 3). The peak of the\nactivity score distribution around 7 is well-captured by the two fine-tuning methods of the VAE in Figure 3a and 3b.\nFine-tuning only two layers reduces the number of parameters to be trained with the same amount of data, thus leads\nto better results.\n\nRationaleRL model is initialized with a generative model trained on the ChEMBL dataset and then fine-tuned with an\nRL algorithm. The Random-Forest classifier with Morgan fingerprint features is used as its reward signal (Jin et al.,\n2020b) in the fine-tuning process. RationaleRL generally has higher percentage of active molecules when predicted\nby the two Random Forest models than Chemprop, a sign of possible overfitting. No such pattern is observed for our\nmethods that did not involve property predictors in the fine-tuning process. On the EGFR dataset, RationaleRL reaches\nhigher level of activity than fine-tuned multi-stage VAE as predicted by Random Forest models but lower level of\nactivity when predicted by Chemprop. On the JAK2 dataset, our methods have lower activity level than RationaleRL\nas predicted by both classifiers but both regressors have predicted similar or slightly higher activity levels on the\ngenerated molecules from our methods than RationaleRL. Fine-tuned multi-stage VAE is similar to RationaleRL on\ndiversity but higher on novelty metric. Our method reaches slightly higher mean activity scores than RationaleRL on\nboth datasets predicted by both regressors. In addition, RationaleRL is much more computationally intensive to train.\n\nMultiple factors could contribute to the large discrepancy between regressors and classifiers’ predictions on the dataset\ngenerated by our method for JAK2. It could be that the classifiers are more adept at identifying negative samples due\nto the larger training set, which also includes more negative examples. Another reason could be that the predictors\nfail to generalize to an out-of-distribution dataset, such as the generated one, despite performing well on the ground-\ntruth data. This is evidenced by the high novelty scores from the generated sets of fine-tuned MoLeR and its multi-\nstage versions in Table 4. These differing results highlight the ambiguity in drawing conclusions based on property\npredictors, which further complicates the optimization process.\n\n1Training on a single \"NVIDIA GeForce RTX 2080 Ti\" GPU.\n\n8\n\n\f(a) Inner-Layer Fine-tuning\n\n(b) Outer-Layer Fine-tuning\n\n(c) Whole Model Fine-tuning\n\n(d) Test set\n\n(e) MoLeR Fine-tuning\n\n(f) RationaleRL\n\nFigure 3: The distributions of the generated molecules’ activity scores on the EGFR protein predicted by the Chemprop model. We\ninclude 5 sets of molecules generated by different methods as well as the ground-truth test set. Additional activity score histograms\ncan be found in Appendix D\n\n5 DISCUSSION\n\nIn this paper, we present a novel multi-stage VAE model that enhances the properties of the generated molecules from\npre-existing VAE models. It achieves the goal in an objective-agnostic way, meaning, our method does not optimize\nover any particular property objectives during training. Yet, it is able to achieve comparable, and sometimes better\noutcomes than specialized approaches that directly optimizes over one or multiple property objectives with the help\nof property predictors. We demonstrate this in two experiments: 1) an unconstrained generation experiment trained\non ChEMBL dataset in which we compare against MoLeR (Maziarz et al., 2021) that includes regression terms in\nthe VAE objective to match the true property values of the input molecules with the predicted propertiy values of\nthe output molecules; and 2) a generation for protein target experiment where our multi-stage VAE is fine-tuned on\ntwo active molecule datasets against two protein targets. We compare our method to RationaleRL (Jin et al., 2020b),\nwhich uses an activity predictor as the reward function. Our findings also bring up the complications in using property\npredictors for multi-objective generation, e.g. overfitting, conflicting objectives, and differing prediction results from\ndifferent predictors. Addressing these issues is an important direction for future research.\n\n9\n\n0246810Activity Score050100150200Number of Molecules0246810Activity Score050100150200250Number of Molecules0246810Activity Score0255075100125150175Number of Molecules0246810Activity Score0255075100125150175200Number of Molecules0246810Activity Score020406080100120140160Number of Molecules0246810Activity Score050100150200250Number of Molecule",
    "abstract": "Variational autoencoder (VAE) is a popular method for drug discovery and various architectures and pipelines have been proposed to improve its performance. However, VAE approaches are known to suffer from poor manifold recovery when the data lie on a low-dimensional manifold embedded in a higher dimensional ambient space [Dai and Wipf, 2019]. The consequences of it in drug discovery are somewhat under-explored. In this paper, we explore applying a multi-stage VAE approach, that can improve manifold recovery on a synthetic dataset, to the field of drug discovery. We experimentally evaluate our multi-stage VAE approach using the ChEMBL dataset and demonstrate its ability to improve the property statistics of generated molecules substantially from pre-existing methods without incorporating property predictors into the training pipeline. We further fine-tune our models on two curated and much smaller molecule datasets that target different proteins. Our experiments show an increase in the number of active molecules generated by the multi-stage VAE in comparison to their one-stage equivalent. For each of the two tasks, our baselines include methods that use learned property predictors to incorporate target metrics directly into the training objective and we discuss complications that arise with this methodology.",
    "response": {
      "Soundness": 1,
      "Presentation": 2,
      "Contribution": 1,
      "Rating": 3,
      "Confidence": 4,
      "Strengths": "1. Correct posing of a problem, with correct references to related work.\n2. The authors undertake the issue of finding better low-dimension space immersion in high-dimensional space. On the other hand, it is not proved that their solution is the right one.",
      "Weaknesses": "1. The authors note that typically, high-dimensional data can be projected onto a low-dimensional space immersed in a high-dimensional space. To demonstrate this, they conduct a very simple experiment on synthetic data. This experiment does not show much, only the authors claim that for one of the later layers a space with the correct dimensionality for the data is obtained. This experiment is clearly spurious and can be removed.\n2. The authors propose to build a VAE model with many hidden layers iteratively. In each step, developing the projection of the most recently obtained hidden layer into a new VAE model. The obtained model would get the variances on the diagonal reach the values {0, 1}, which would show the selection/reduction of individual dimensions projection. \nIt seems that the analogy with synthetic experiment is insufficient for a direct application of this approach.\n3. Several models with randomly selected numbers of hidden layers are used in the experiments.\n4. In the comparisons, each model has a different dimensionality of hidden layers for the same data.\n5. There is no bibliography in the work. I suspect that this is a simple editorial error (references in the work are marked as links), but such an error in a work for a major conference is unacceptable. Similarly, there is no appendix to which there is a reference in the main text.",
      "Questions": "1. Is the synthetic experiment really needed? I would remove it if I were you. In place of it, I would give some better mathematical discussion.\n2. Is the analogy between the scheme presented in the synthetic data experiment to the problem of molecule generation really valid? Could you, please, better justify the analogy?\n3. Why just that number of layers are used in the models? Why are there 3 layers for HGNN and MoLeR? The \"MoLeR + prop\" model is completely unclear? Why does the RNN model only have two layers? The LatentGAN model seems to have only one hidden layer, right? Or maybe some of the nodels are for comparison? This is not clear from the text of the work.\n4. If one of the goals was to show that multi-layer models, along with variance optimization, allow for better matching of the implicit dimensions, does using different dimensions allow this to be shown for different models? There is no description of the individual statistics used in the work.\n5. Was variance optimization used for all models? This is not clear from the text.\n6. The standard in the editorial composition of scientific works is to place descriptions of tables above them, not below."
    }
  },
  {
    "paper_id": "o1TKGCrSL7",
    "summary": "Background: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n",
    "full text": "\nCROSS-MODALITY DEBIASING: USING LANGUAGE TO\nMITIGATE SUB-POPULATION SHIFTS IN IMAGING\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nSub-population shift is a specific type of domain shift that highlights changes in\ndata distribution within specific sub-groups or populations between training and\ntesting. Sub-population shift accounts for a significant source of algorithmic bias\nand calls for distributional robustness. Recent studies found inherent distributional\nrobustness in multi-modality foundation models, such as the vision-language model\nCLIP, yet this robustness is vulnerable through parameter fine-tuning. In this paper,\nwe propose leveraging the connection of robustness among different modalities and\nreshaping the distributional robustness of one modality with another. Specifically, in\nthe context of the distributional robustness of CLIP, we propose to leverage natural\nlanguage inputs to debias the image feature representations, to improve worst-case\nperformance on sub-populations. Our extensive empirical studies show that image\nrepresentations debiased by natural language can achieve significant performance\nimprovement and reduction of performance instability under sub-population shifts.\n\n1\n\nINTRODUCTION\n\nThe domain shift between the training data and data at the inference stage is commonly found in ma-\nchine learning systems. For instance, in applications such as robotics James et al. (2019); Wulfmeier\net al. (2017), navigation Lütjens et al. (2019); Bharadhwaj et al. (2019), and auto collecting Moreno-\nTorres et al. (2012), training data may be collected from simulated or misaligned environments to\nreduce cost. The collected data will likely encounter domain shift during deployment, where the\nmodel robustness towards those shifts is a crucial requisite for safety deployment. Sub-population\nshift is a specific type of domain shift where there are changes in the distribution of data within\nspecific sub-populations or groups Koh et al. (2021); Santurkar et al. (2020). Sub-population shift\nleads to generalization issues of groups and constitutes a significant source of algorithmic bias. A\nrobust model against sub-population shift shall perform well on various demographic groups, such\nas groups identified by gender, race, or age, no matter the group population. However, classical\nmethods such as empirical risk minimization are known to be fragile under such sub-population shifts\nor domain shifts Duchi & Namkoong (2018); Rockafellar et al. (2000).\n\nMore recently, the sub-population shift has also been identified as a critical issue in large foundation\nmodels Henderson et al. (2023); Gan et al. (2022). One notable example of cross-modality foundation\nmodels is Contrastive Language-Image Pre-training (CLIP) Radford et al. (2021). CLIP is a neural\nnetwork trained on pairs of images and natural language supervision.\nIt enables learning rich\nfeature representations for images and leverages natural language cues during training and inference.\nConsequently, methods built upon CLIP have gained considerable attention due to their ability to align\ndiverse modalities and address various tasks. However, recent studies found that fine-tuning CLIP\nimproved task performance but often compromised its original robustness to domain shifts Wortsman\net al. (2022); Kumar et al. (2022); Zhang et al. (2021). Similarly, the same phenomenon regarding\nrobustness to sub-population shifts was observed Li et al. (2021); Zhang & Ré (2022); Lee et al.\n(2022). These findings collectively indicate that the careful construction of feature embeddings can be\neasily compromised by the negligent utilization of label information during training and emphasize\nthe importance of investigating suitable methods for handling distributional shifts in conjunction with\nmultimodal models like CLIP.\n\nDistributionally Robust Optimization (DRO) provides a paradigm to tackle the naturally distributional\nshifts Namkoong & Duchi (2016); Rockafellar et al. (2000). DRO typically involves optimizing\n\n1\n\n\fUnder review as a conference paper at ICLR 2024\n\nthe model performance by considering the uncertainty of data, which approximates the worst-case\nshift in domain features or sub-populations from the training distribution. The shift is usually\nbounded within a distributional divergence distance, such as f -divergences Ben-Tal et al. (2013);\nNamkoong & Duchi (2016); Hashimoto et al. (2018); Duchi & Namkoong (2018); Shapiro (2021)\nand Wasserstein distances Gao et al. (2017); Blanchet et al. (2019); Kuhn et al. (2019); Sinha et al.\n(2017). However, a straightforward combination of CLIP with universal DRO approaches, such\nas χ2-DRO Hashimoto et al. (2018) and its variants, may introduce inherent risks like unstable\nperformance and the requirement for a domain-aware validation dataset. Therefore, it is essential to\nexplore alternative strategies to mitigate distributional shifts while preserving the effectiveness and\nrobustness of CLIP.\n\nA recent study by Dunlap et al. (2022) introduced an approach that modifies the feature embedding\nof classifiers utilizing CLIP as a backbone. This modification enables the extension of classifier\ncapabilities to previously unseen domains by leveraging natural language descriptions associated with\nthese unseen domains. The example demonstrated the potential of adjusting distributional robustness\nusing interconnected modalities. In this paper, we study strategies for mitigating sub-population\nshifts by leveraging natural language supervision in the context of the language-image foundation\nmodel CLIP. Notably, our investigation is conducted under the assumption of a domain-oblivious\nsetting, wherein the sub-population membership of individual instances remains unknown during the\ntraining phase.\n\nOur contributions are summarized as follows:\n\n• We build a principled connection between natural language supervision and robustness to\nsub-population shift (also known as subgroup robustness) and provide extensive experimental\nanalysis, which shows the capability of mitigating robustness issues in one modality by\nidentifying and analyzing it in another modality.\n\n• We show that without instance-wise label information, the proposed method consistently\nimproves worst-case performance under sub-population shifts over original zero-shot learning\nof CLIP under divergent settings.\n\n2 RELATED WORK\n\nRecent years witnessed a growing interest in studying the distributional robustness of vision-language\nfoundation models Fang et al. (2022); Nguyen et al. (2022); Gan et al. (2022). There is extensive\nevidence of robustness deterioration when applying classical fine-tuning methods, such as linear-\nprobe, to pre-trained models, and there are great efforts to alleviate these robustness issues Wortsman\net al. (2022); Gao et al. (2021); Kumar et al. (2022). To enhance performance in the presence of sub-\npopulation shifts, broadly used strategies that are based on loss values have been adapted to foundation\nmodels fine-tuning Zhang & Ré (2022). Moreover, there is a growing trend that natural language\nsupervision is adapted in the training phase to debias learned feature representations Ranasinghe\net al. (2022); Petryk et al. (2022); Wang et al. (2022). A recent work that aligns with our approach is\npresented in Zhang et al. (2023), which utilizes language to control model behaviors by identifying\nmisclassified instances and influential attributes in the form of language descriptions and then\ncontinuing fine-tuning the vision classifier upon that information.\n\nDistributionally robust optimization has been widely studied to handle the situation where the test\ndistribution is undetermined Shapiro (2021); Namkoong & Duchi (2016); Quiñonero-Candela et al.\n(2008). Common approaches involve constructing an uncertainty set around the training distribution\nwith some divergence to approximate the unknown distribution. Certain real-world scenarios, such as\nsub-population shift Koh et al. (2021); Santurkar et al. (2020); Jeong & Namkoong (2020), can be\nmodeled as minimizing the supremum of the loss within the uncertainty set. Some convenient dual\nreformulations of the optimization problem in terms of some specific divergence are introduced such\nas χ2-DRO Hashimoto et al. (2018). In the context of addressing sub-population shifts, exploring\nstatistical features to identify minority groups during training, through the analysis of gradients,\nlosses, and feature spaces, has also gained popularity Liu et al. (2021); Nam et al. (2020); Sagawa\net al. (2019); Sohoni et al. (2020).\n\n2\n\n\fUnder review as a conference paper at ICLR 2024\n\n3 BACKGROUND\n\nConsidering a machine learning task with given training distribution P over input space X ∈ Rd and\nvectorized label space Y ∈ Rc, empirical risk minimization aims to optimize the average performance\nof a model fθ : X → Y parameterized with θ over observed i.i.d. samples zk = (xk, yk) ∼ P for\nk = {1, ..., m}, which is formulated as minθ EZ∼P [ℓ(θ, Z)]. The problem of sub-population shift\nconcerns the worst-case performance over some pre-defined sub-populations or domains {Q1, ..., Qn}\nfrom an uncertainty set of distributions Q ∈ X × Y where P and Q have the same support, and the\nEZ∼Q[ℓ(θ, Z)]. In this paper, we use the vision-language\nobjective is formulated as minθ supQ∈Q\nfoundation model CLIP Radford et al. (2021) in our evaluations and the proposed strategy and\nprinciple can be easily extended to other multimodal foundation models. We analyze the challenges\nassociated with mitigating sub-population shift using χ2-DRO Hashimoto et al. (2018).\n\n3.1 CLIP\n\nBecause of the rich image feature representations and its ability to incorporate natural language super-\nvision during training and inference, CLIP has been widely studied and applied in various domains.\nIn particular, the general zero-shot performance of CLIP serves as an essential baseline for algorithm\ndevelopment nowadays. The zero-shot classifier, which relies on CLIP as its backbone, utilizes the\nimage encoder of CLIP and a linear classifier constructed using domain-specific descriptions for a\ngiven task. We formally define this zero-shot classifier as follows.\n\nWe denote Iθi : X → I as the image encoder of CLIP that maps input space X to image embedding\nspace I, and Tθt(t) ∈ T as the text encoder of CLIP that maps some text t to text embedding space\nT , where I ∈ Re and T ∈ Re. We abbreviate Iθi(·) and Tθt(·) as I(·) and T (·) respectively. A set of\nclassification text prompts, {t1, ..., tc}, that is empirically derived from class labels and class-domain\ndescription is necessary to construct a zero-shot classifier, e.g., {t1, t2} = {\"a photo of a blond hair\npeople\", \"a photo of a not blond hair people\"} for the common dataset CelebA Liu et al. (2015),\nwe abbreviate {t1, t2} as \"a photo of a {not blond, blond} hair people\". The zero-shot classifier\nbuilt upon CLIP classifies an input x through: arg maxi∈[c] I(x)T T (ti), where [c] refers to the set\nof classes {1, 2, ..., c}. Various fine-tuning methods have been proposed to better adapt CLIP to\ndownstream tasks, we select architecture design by Gao et al. (2021) for general training purposes. It\nadded an extra feature adapter, Aθa : I → I, between I(·) and T (·), and we abbreviate Aθa as A(·).\nThe corresponding training and inference are formulated as:\n\nTraining min\nθa\n\nE(x,y)∼P [ℓce([(Aθa ◦ I(x))T T (ti)]i∈[c], y)],\n\nInference arg maxi∈[c] (A ◦ I(x))T T (ti),\n\n(1)\n\n(2)\n\nwhere [(Aθa ◦ I(x))T T (ti)]i∈[c] denotes a vector with i-th value as (Aθa ◦ I(x))T T (ti), and ℓce(·)\ndenotes the cross-entropy loss. We abbreviate the network architecture as I ▷ A ▷ T following the\ninput stream from the image encoder of CLIP to the feature adapter and then to the text embedding.\n\n3.2 DRO\n\nGiven a training distribution P and a predefined divergence D, DRO aims to minimize the expected\nrisk over the distribution Q that is in a ball around the training distribution P w.r.t. divergence D.\nThe expected DRO risk is defined as RD;ρ(θ; P ) = supQ≪P {EZ∼Q[ℓ(θ, Z)] : D(Q||P ) ≤ ρ} for\nsome ρ > 0 where Q ≪ P denotes Q is absolutely continuous with respect to P. Supported by\nthe convenient dual characterization of Cressie-Read family of R´enyi divergence Shapiro (2017);\nCressie & Read (1984), pioneering work eliminates the untouchable distribution Q in RD;ρ(θ; P )\nand only exploits training distribution P to solve the problem Duchi & Namkoong (2018); Zhai et al.\n(2021). Referring Zhai et al. (2021), we can show that, minimizing the loss values that are larger than\na specific threshold, results in minimizing the DRO risk where P and Q have the same support, i.e.,\nsub-population shift. This reinforces that the loss values (wrongly classified instances), based on\nover-sampling or re-weighting methods, are being used for mitigating the sub-population shift.\n\nHowever, it is hard to directly apply DRO risk in the general training pipeline of deep models. Firstly,\n(Mini-batch) SGD is a biased estimation of DRO risk Ghosh et al. (2018), which suggests a two-phase\ntraining pipeline. Secondly, directly minimizing DRO risk is equal to minimizing the loss over a small\n\n3\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 1: Training phase of L-DRO on CelebA Liu et al. (2015). Given the training dataset (without\ninstance-wise label information) and concerned sup-populations, L-LDR aims to learn a feature\nadapter Aθa to transform image representation from original embedding to a debiased embedding. The\ngoal is to ensure that the debiased embedding does not reveal any information about sub-population\nmembership while minimizing significant changes from the original embedding.\n\nportion of whole data points, which likely deteriorates the average performance and even worst-case\nperformance in deep learning regimes. This suggests an augmentation over the small portion of data\npoints identified by DRO, instead of counting on learning over the small portion of data points only,\nand leads to a similar design of JTT Liu et al. (2021). However, with the theoretical understanding\nand method developments, existing approaches dealing with sub-population shifts still suffer risks\nsuch as unstable performance across training epochs Zhai et al. (2021). A popular and compromised\nsolution is a domain explicit validation dataset along with an early stopping strategy to guarantee\nreasonable worst-case performance Zhai et al. (2022), which is also employed in this work.\n\n4 L-DRO: DISTRIBUTIONAL ROBUSTNESS VIA LANGUAGE\n\nThe ability of the CLIP model that fuses natural language supervision into the training phase for\nvarious purposes leaves great untouched potential. A set of text prompts with carefully designed\nclass-domain descriptions can construct a zero-shot classifier that works reasonably well for specific\ntasks. Inspired by the construction, this work proposes to use natural language information to mitigate\nsub-population shifts.\n\nUsing language to mitigate sub-population shift Note that the sub-populations {Q1, ..., Qn} is defined\nover space X × Y, i.e., for a specific sub-population, e.g., Qk, the sub-population is decided by the\ncombination of attributes from input space and label space together. Take a typical setting on CelebA\nas an example, attributes from label space {yb, yn} := {blond, not blond} and attributes from input\nspace {F, M } := {female, male} consist of four sub-populations {blond male, not blond male, blond\nfemale, not blond female} for worst-case performance estimation.\n\nSince label space information is explicit, the performance over label space can be perfectly balanced.\nIntuitively, if the performance of sub-populations within the same class can be balanced as well, the\nsub-population shifts can be mitigated. Using the principle of applying ERM for deep model training\nwith the balanced dataset for label space P(yb|x) ≈ P(yn|x), the performance of sub-populations is\ngiven by P(yb, F |x) : P(yn, F |x) : P(yb, M |x) : P(yn, M |x) ≈ P(F |x) : P(F |x) : P(M |x) :\nP(M |x). This means the performance of a specific sub-population is decided by the proportion\nof the sub-population over the dataset. Further, ℓent(P(F |x), P(M |x)) can serve as a measure to\nsub-population shifts, formulated as:\n\nEZ∼Q[ℓ(θ, Z)] ∝ −ℓent(P(F |x), P(M |x)).\n\n(3)\n\nsup\nQ∈Q\n\nIt suggests that sub-population shift can be naturally mitigated through max ℓent(P(F |x), P(M |x)),\nand achieving the goal does not rely on instance-wise label information.\n\nBased on intuition, we proposed Language-based Distributional Robust Optimization, abbreviated as\nL-DRO. L-DRO is built upon equation 3 and aims at improving the worst-case performance of CLIP\nwithout instance-wise label information.\n\n4\n\nPre-defined sub-populations{F, M}:={female, male}Descriptions{𝑡̂!,𝑡̂\"}≔\t“a photo a {female, male} people”𝐼!!𝑇!\"𝐴!#ℓentℓsimDebiasing lossMaxEnt(𝒫(F|x#), 𝒫(M|x#))Consistency loss𝑇(𝑡̂!),𝑇(𝑡̂\")𝐼(x#)x#𝐴∘𝐼(x#)Language-Image modelDataset(X, )\fUnder review as a conference paper at ICLR 2024\n\n• Following architecture design in equation 1, we propose using the sup-population descriptions\n\nto debias the original feature representations.\n\n• In order to retain general performance, another objective is designed to maintain consistency\n\nbetween the original feature representations and the debiased ones.\n\nGiven image encoder of CLIP I(·), text encoder T (·), adapter A(·), classification text prompt\n(target domain descriptions ) {t1, · · · , tc}, and debiasing text prompt (a set of semantically opposite\nsub-population descriptions) {ˆt1, · · · , ˆts}, the objective of L-DRO is formulated as:\n\nmin\nθA\n\nℓ(x, {ˆt1, · · · , ˆts}) := min\nθA\n\n1 − ℓent\n\n(cid:16)\n\n[(cid:0)(AθA ◦ I(x)(cid:1)T\n\nT (ˆti)]i∈[s]\n\n(cid:17)\n\n− η · ℓsim\n\n(cid:0)I(x), AθA ◦ I(x)(cid:1),\n(4)\n\nwhere ℓent(a) := −softmax(a)T log(softmax(a)) is the entropy loss that encourages the inability\nto distinguish across sub-populations using the learned feature representation, i.e., debiasing the\nfeature representations. And ℓsim(a, b) := aT b\n||a||||b|| is the consistency loss (cosine similarity) that\nencourages the similarity of feature representation before and after the adapter. η is a scalar to balance\nthe above two terms. The corresponding training and inference of L-DRO follow the procedures of\nEqs. equation 1 and equation 2. An outline of the training phase of our method is shown in Figure 1.\n\n5 EXPERIMENTS\n\nIn this section, we begin by showcasing the consistent improvement of L-DRO over zero-shot learning\nin terms of worst-case performance. Specifically, we investigate this improvement and highlight the\nstability of L-DRO across different training epochs (see Section 5.1). Subsequently, we examine\nthe impact of debiasing on various sub-populations, including both independent and correlated sub-\npopulations (see Section 5.2). Further, we explore the potential of the debiased feature representations\nto stabilize the existing methods that deal with sub-population shifts. The performance metrics\ninclude average accuracy and worst-case accuracy, and the worst-case accuracy represents the lowest\naccuracy observed among the different subpopulations. To ensure the robustness and reliability of\nthe results, each experiment is repeated 10 times using different random seeds to get the mean and\nstandard deviation of accuracy.\n\nModel architecture CLIP Radford et al. (2021) is selected as the vision-language foundation model\nin the experiment, and the training and inference follow equation 1 and equation 2. The subspace\nmapping AθA (·) is a two-layer MLP with the same input and output dimensions.\nDataset and pre-defined sub-populations Most of our experiments were evaluated on CelebA Liu\net al. (2015), which is a large-scale face attributes dataset with 40 attribute annotations for each image.\nThe target task of CelebA is to differentiate if a given human face image is blond hair or not blond\nhair. The attributes from input space for constituting sub-population shifts are selected as {male,\nfemale}, then we have four sub-populations {blond male, not blond male, blond female, not blond\nfemale} on CelebA dataset for worst-case performance estimation. Another selected and commonly\nused dataset is Waterbirds (95%) Sagawa et al. (2019), which asks classifiers to differentiate if a given\nimage is waterbirds or landbirds. The training set of Waterbirds places 95% of all waterbirds against\na water background with the remaining 5% against a land background. And the similar unbalanced\nsetting also applied to the other class landbirds. Then we have four sub-populations {waterbird in\nwater, waterbird in land, landbird in water, landbird in land } on Waterbirds dataset for worst-case\nperformance estimation.\n\nWe note that the best model is selected based on a domain-aware validation dataset across varying\nhyper-parameters and training epochs (early stopping strategy), please refer to Appendix B.2 of\nZhai et al. (2021) and Zhai et al. (2022) for a comprehensive discussion about the necessity of\ndomain-aware validation dataset for model selection in methods dealing with sub-population shifts.\n\n5.1 WORST-CASE PERFORMANCE AND BEYOND\n\nThe selection of text prompt The selection of appropriate text prompts, including classification and\ndebiasing prompts, significantly impacts the performance of CLIP. Thus, our initial investigation\nfocuses on examining the impact of different text prompts on both average accuracy and worst-case\n\n5\n\n\fUnder review as a conference paper at ICLR 2024\n\nTable 1: Under CelebA and CLIP (ViT-B/32 and RN50), the Average Accuracy (Avg.Acc.) and\nWorst-Case Accuracy (W.C.Acc.) over sub-populations with varying classification text prompt and\ndebiasing text prompt.(%)\n\nClassification text prompt\nAnd Debiasing text prompt\n\n\"a photo of []\"\nAnd \"a photo of []\"\n\n\"photo of a [] people\"\nAnd \"photo of a [] people\"\n\n\"photo of a [] hair people\"\nAnd \"photo of a [] people\"\n\nMethod\n\nZero-shot\nL-DRO\n\nZero-shot\nL-DRO\n\nZero-shot\nL-DRO\n\nRN50\n(Avg.Acc & W.C.Acc.)\n\nViT-B/32\n(Avg.Acc & W.C.Acc.)\n\n64.9 & 49.2\n66.3±0.8 & 55.4±1.0\n\n49.6 & 35.6\n51.6±1.2 & 38.3±1.5\n\n70.1 & 52.9\n70.9±0.8 & 53.3±1.4\n\n58.8 & 49.2\n60.4±0.7 & 53.3±0.9\n\n77.4 & 65.2\n78.3±0.8 & 66.5±1.5\n\n86.4 & 61.1\n86.1±0.3 & 69.7±2.1\n\n\"a photo of a [] hair people\nAnd \"a photo of a [] people\" L-DRO\n\nZero-shot\n\n77.4 & 62.6\n78.0±0.7 & 63.5±1.4\n\n85.2 & 70.6\n83.6±0.3 & 79.2±1.3\n\n\"picture of [] hair people\"\nAnd \"picture of [] people\"\n\n\"a picture of [] hair people\"\nAnd \"a picture of [] people\"\n\n\"a picture of [] people\"\nAnd \"a picture of [] people\"\n\nZero-shot\nL-DRO\n\nZero-shot\nL-DRO\n\nZero-shot\nL-DRO\n\n87.6 & 67.1\n88.8±0.3 & 74.7±1.1\n\n86.1 & 65.0\n85.0±0.5 & 65.4±1.3\n\n88.8 & 75.6\n89.0±0.3 & 75.8±1.1\n\n87.3 & 75.0\n85.2±0.7 & 75.9±0.8\n\n83.6 & 75.3\n85.7±0.4 & 78.9±1.7\n\n71.6 & 66.3\n71.2±0.4 & 58.8±0.8\n\nin the classification text prompt is male or female and [] in the classification text prompt is not\nblond or blond.\naccuracy. Table 1 shows that L-DRO consistently outperforms zero-shot learning across most of\npopular text prompt selections for both network architectures RN50 and ViT-B/32. Meanwhile, the\naverage accuracy increases most time surprisingly since the objective of L-DRO, equation 4, does not\ninvolve any label information about the target task. More investigations in text prompts for Waterbirds\ndataset and other settings are detailed in Appendix A.\n\nPerformance cross datasets and network architectures Table 2 shows the performance of L-DRO\nwith different network architectures on CelebA and Waterbirds. Compared with other baselines, under\nboth network architectures RN50 and ViT-B/32 with both CelebA dataset and Waterbirds dataset, L-\nDRO outperforms all of the baselines with great improvement. However, with network architectures\nViT-L/14, the L-DRO fails to debias the feature representations and improve the performance\ncompared with zero-shot learning, we investigate the phenomenon in Appendix C, which shows\nthat text prompts on CLIP (ViT-B/32) does not consistently translate to high performance on CLIP\n(ViT-L/14). Unless otherwise specified, we use ViT-B/32 for the following experiments.\n\nStable worst-case accuracy across training epochs We highlight the performance stability of\nL-DRO across training epochs since most methods dealing with sub-population shifts suffer the\ninstability of performance along with training epochs. A domain-aware validation dataset is usually\nnecessary to obtain reasonable worst-case performance. As we can see from Figure 2, baseline DRO\nmethods, χ2-DRO Hashimoto et al. (2018), JTT Liu et al. (2021), and CVaR DRO Namkoong &\nDuchi (2016), show large fluctuations in both average accuracy and worst-case accuracy throughout\nthe training epochs. On the other hand, Figure 2 shows that the average accuracy and worst-case\naccuracy of L-DRO under the default setting where η = 0.2 are significantly stable along with training\nepochs, which underscore the advancement of methods based on re-weighting or augmentation over\ndataset that bring instability during training referring Figure 2 in Zhai et al. (2021).\n\nData efficiency Benefiting from the parameter-efficient training procedure, L-DRO demonstrates\ndata efficiency. Table 3 summarizes the average accuracy and worst-case accuracy with varying sizes\nof training data. The performance gain compared with zero-shot learning starts from 2048 training\nexamples under the CelebA dataset, and the gain for Waterbirds starts from 512 training examples.\n\n5.2 MUTUAL EFFECTS OF DEBIASING VARIOUS SUB-POPULATIONS AND BEYOND\n\nWe investigate the effect of unaligned debiasing and the combination of multiple debiasing sources\nat Table 4. The first eight rows in Table 4 show that unaligned debiasing generally has similar\n\n6\n\n\fUnder review as a conference paper at ICLR 2024\n\nTable 2: The average accuracy and worst-case accuracy of OrthCali Chuang et al. (2023), ERM,\nCVaR DRO Namkoong & Duchi (2016), χ2-DRO Hashimoto et al. (2018), JTT Liu et al. (2021), and\nL-DRO (our proposed method) over different datasets and network architectures.1 (%)\n\nArchitecture Method2\n\nCelebA\n\nWaterbirds\n\nAvg. Acc. W.C. Acc. Avg. Acc. W.C. Acc.\n\nRN50\n\nI ▷ T\nI ▷ A ▷ T\nI ▷ A ▷ T\nI ▷ A ▷ T\nI ▷ A ▷ T\nI ▷ A ▷ T\n\nViT-B/32\n\n52.5\nOrthCali\n95.2±0.1\nERM\nCVaR DRO 87.0±3.4\nχ2-DRO\n88.1±3.8\n89.4±2.5\nJTT\n85.7±0.5\nL-DRO\n\n24.6\n41.2±3.6\n70.6±4.7\n64.2±12.8\n49.7±4.7\n78.9±1.7\n\n69.8\n83.0±1.1\n77.2±3.7\n79.0±1.8\n78.5±2.6\n77.4±1.3\n\n60.4\n59.6±2.1\n59.5±4.2\n61.1±3.1\n58.3±6.0\n62.7±2.8\n\nOrthCali\n73.1\n95.3±0.1\nERM\nCVaR DRO 84.8±4.9\nχ2-DRO\n87.4±4.5\n90.3±2.1\nJTT\n83.6±0.3\nL-DRO\n\nI ▷ T\nI ▷ A ▷ T\nI ▷ A ▷ T\nI ▷ A ▷ T\nI ▷ A ▷ T\nI ▷ A ▷ T\nThe proposed method L-DRO and {CVaR DRO, χ2-DRO, JTT} requires a domain-\naware validation dataset for hyper-parameter selection. OrthCali is training-free\nregimes and generally don’t need a validation dataset except for selecting a better\nprompt. And, only adapter A is tunable.\n\n60.8\n44.2±2.5\n67.1±10.4\n72.0±9.6\n53.4±2.6\n79.2±1.3\n\n83.9\n83.3±1.0\n78.3±3.2\n79.3±2.9\n80.3±1.9\n77.6±0.5\n\n59.7\n59.7±2.4\n60.5±4.2\n59.3±5.7\n60.5±3.0\n64.8±0.8\n\n1\n\n2\n\nCVaR-DRO and χ2-DRO use the two-phase training strategy same with JTT. The\nmotivation is detailed in section 3.2, and the performance comparison is demonstrated\nin Appendix B.\n\n(a) Performance on CelebA dataset.\n\n(b) Performance on Waterbirds dataset.\n\nFigure 2: Under CLIP (ViT-B/32), the average and worst-case accuracy of validation dataset and test\ndataset over epochs using L-DRO and baseline methods. The left figure shows the performance of\nCelebA dataset, and the right figure shows the performance of Waterbirds dataset.\n\nperformance with zero-shot learning, i.e., basically, the L-DRO works on influential attributes only and\nwill not hurt the embedding of uncorrelated attributes. The last two rows demonstrate the performance\nof combining multiple debiasing sources, it shows that L-DRO can debias multiply influential\nattributes at the same time and the performance is slightly degraded compared with independent\ndebiasing. In Table 5, we further investigate using L-DRO to debias semantically correlated source\nsub-populations and target sub-populations, and it shows that a degressive correlation between source\n\n7\n\n13579Epochs0.00.20.40.60.81.0AccuracyAvg. Acc. in test - 2-DROW.C. Acc. in test - 2-DROAvg. Acc. in test - CVaR DROW.C. Acc. in test - CVaR DROAvg. Acc. in test - JTTW.C. Acc. in test - JTTAvg. Acc. in test - L-DROW.C. Acc. in test - L-DRO13579Epochs0.00.20.40.60.81.0AccuracyAvg. Acc. in test - 2-DROW.C. Acc. in test - 2-DROAvg. Acc. in test - CVaR DROW.C. Acc. in test - CVaR DROAvg. Acc. in test - JTTW.C. Acc. in test - JTTAvg. Acc. in test - L-DROW.C. Acc. in test - L-DRO\fUnder review as a conference paper at ICLR 2024\n\nTable 3: Under CLIP (ViT-B/32), the average accuracy and worst-case accuracy with varying sizes of\ntraining data. The accuracy with ⋆ reaches the performance of zero-shot learning, and the accuracy\nwith ⋄ reaches or is close to the performance of training with full data.(%)\n\nDataset\n\nAcc.\n\nPerformance over varying size of training data\n\n512\n\n1024\n\n2048\n\n4096\n\n8192\n\n16384\n\nCelebA\n\nAvg.\nW.C.\n\n77.1±8.5\n57.2±6.4\n\n84.6±1.1\n83.7±0.9\n69.4±6.2 71.4±4.6⋆ 74.1±2.8 76.3±3.3 77.4±2.8⋄\n\n84.5±0.8 83.9±0.6\n\n84.7±0.8\n\nWaterbirds\n\n74.4±1.8\n\n78.4±0.8\nAvg.\nW.C. 57.9±3.1⋆ 63.2±1.9 65.1±1.1⋄ 66.5±1.4\n\n76.0±0.7\n\n77.8±0.7\n\n/\n/\n\n/\n/\n\nTable 4: Under CelebA and CLIP (ViT-B/32), the average accuracy and worst-case accuracy over\nunaligned source sub-populations and target sub-populations. (%)\n\nUnaligned debiasing\n\nAvg. Acc. W.C. Acc.\n\nMethod\n\nZero-shot\nL-DRO\nL-DRO\nL-DRO\nL-DRO\n\nZero-shot\nL-DRO\nL-DRO\nL-DRO\nL-DRO\n\nSource\n\n-\n{male, female}\n{old, young}\n{attractive, not attractive}\n{straight hair, wavy hair}\n\n-\n{old, young}1\n{male, female}\n{attractive, not attractive}\n{straight hair, wavy hair}\n\nL-DRO\n\n[{old, young}, {male, female}]\n\nTarget\n\n{male, female}\n{male, female}\n{male, female}\n{male, female}\n{male, female}\n\n{old, young}\n{old, young}\n{old, young}\n{old, young}\n{old, young}\n\n{male, female}\n{old, young}\n\n85.2\n83.6±0.3\n84.5±0.3\n84.3±0.2\n85.8±0.4\n\n85.1\n88.0±0.7\n84.3±0.8\n84.0±0.7\n85.9±0.5\n\n86.7±0.8\n86.9±1.1\n\n70.6\n79.2±1.3\n69.5±1.3\n73.9±1.7\n67.7±2.2\n\n73.5\n84.3±1.6\n69.4±1.7\n79.6±1.7\n72.1±1.8\n\n78.9±1.6\n81.2±1.8\n\n1 Debiasing test prompt for input space attributes {old, young} is \"a photo of a [{old, not\nold}, {young, not young}] people\".\n\nTable 5: Under CelebA and CLIP (ViT-B/32), the average accuracy and worst-case accuracy over\nsemantically correlated source sub-populations and target sub-populations. The semantic relations\nrefer University (2010). (%)\n\nMethod\n\nZero-shot\nL-DRO\nL-DRO\nL-DRO\n\nZero-shot\nL-DRO\nL-DRO\nL-DRO\n\nSemantically correlated debiasing\n\nSource\n\nTarget\n\n-\n{male, female}\n{man, woman}1\n{boy, girl}2\n\n{male, female}\n{male, female}\n{male, female}\n{male, female}\n\n-\n{old, young}\n{adult, juvenile}\n{mature, immature}\n\n{old, young}\n{old, young}\n{old, young}\n{old, young}\n\nAvg. Acc. W.C. Acc.\n\n85.2\n83.6±0.3\n84.9±0.3\n85.1±0.4\n\n85.1\n88.0±0.7\n81.8±0.5\n85.6±0.3\n\n70.6\n79.2±1.3\n75.8±2.3\n74.4±2.5\n\n73.5\n84.3±1.6\n79.1±0.6\n71.2±1.4\n\n1 Debiasing test prompt is \"a photo of a {man, woman}\".\n2 Debiasing test prompt is \"a photo of a {boy, girl}\".\n\nand target sub-populations generally gets decreasing performance on the worst-case accuracy as\nexpected.\n\n8\n\n\fUnder review as a conference paper at ICLR 2024\n\nTable 6: Under CLIP (ViT-B/32), the average accuracy and worst-case accuracy of ERM, L-\nDRO+ERM, CVaR DRO Namkoong & Duchi (2016), L-DRO+CVaR-DRO, χ2-DRO Hashimoto\net al. (2018), L-DRO+χ2-DRO, JTT Liu et al. (2021), and L-DRO + JTT over different datasets. (%)\n\nArchitecturea\n\nMethod\n\nCelebA\n\nWaterbirds\n\nAvg. Acc. W.C. Acc. Avg. Acc. W.C. Acc.\n\nI ▷ A2 ▷ T\nI ▷ A1 ▷ A2 ▷ T\n\nI ▷ A2 ▷ T\nI ▷ A1 ▷ A2 ▷ T\n\nI ▷ A2 ▷ T\nI ▷ A1 ▷ A2 ▷ T\n\nERM\nL-DRO + ERM\n\nCVaR DRO\nL-DRO + CVaR-DRO\nχ2-DRO\nL-DRO + χ2-DRO\n\n95.3±0.1\n95.1±0.1\n\n84.8±4.9\n85.9±3.2\n\n87.4±4.5\n84.7±2.8\n\n44.2±2.5\n45.3±4.6\n\n67.1±10.4\n71.0±8.1\n\n72.0±9.6\n73.8±6.8\n\n83.3±1.0\n79.9±2.0\n\n78.3±3.2\n78.1±3.0\n\n79.3±2.9\n77.5±4.4\n\n59.7±2.4\n55.6±3.3\n\n60.5±4.2\n56.7±4.5\n\n59.3±5.7\n57.1±5.2\n\nJTT\nL-DRO + JTT\n\nI ▷ A2 ▷ T\nI ▷ A1 ▷ A2 ▷ T\na I ▷ A1 ▷ A2 ▷ T denotes that L-DRO tunes the first adapter A1 which is the same adapter as AθA (·),\nand the second embedded method tunes the second adapter A2 which is a three-layer MLP with the\nsame input and output dimensions.\n\n60.5±3.0\n56.7±2.3\n\n80.3±1.9\n78.2±2.0\n\n53.4±2.6\n48.4±2.8\n\n90.3±2.1\n91.2±2.2\n\n5.3 DEBIASED FEATURES IMPROVING THE STABILITY OF METHODS DEALING WITH\n\nSUB-POPULATION SHIFTS\n\nIn this section, we empirically verify that L-DRO has the potential to help stabilize the existing\nmethods including CVaR-DRO Namkoong & Duchi (2016), χ2-DRO Hashimoto et al. (2018), and\nJTT Liu et al. (2021) that deal with sub-population shifts based on loss values or wrongly classified\ninstances. In Table 6, we compare the performance of some commonly used DRO methods before\nand after using the adapter to debias feature on CelebA dataset and Waterbirds dataset. Under\nCelebA dataset, results show that L-DRO+CVaR-DRO and L-DRO+χ2-DRO improves the mean\nand standard deviation of worst-case accuracy over the original CVaR-DRO and χ2-DRO. But, there\nis no improvement when using JTT. Under Waterbirds dataset, the combination does not show any\nadvantages. However, it seems that if the base performance has reasonable results, then L-DRO\ncan further improve its performance upon it. If not, such as the first four rows of performance on\nWaterbirds, the DRO methods cannot achieve better performance than ERM, then the combination\nfails either.\n\n6 CONCLUSION\n\nIn this work, we studied the sub-population shift in the multimodality model CLIP, where sub-\npopulation shifts in one modality can be described and defined in another modality. Specifically,\none can use natural language to describe the influential attributes that cause the shifts, then those\ndescriptions can be mapped into the space same as image embedding. To this end, we proposed\nL-DRO to debias the image representations according to the vectorized influential attributes descrip-\ntions and exploit the debiased representations to achieve better performance while sub-population\nshifts exist during testing. Compared with zero-shot learning, L-DRO shows improved worst-case\nperformance under domain oblivious settings and occasionally even enhances average performance\nwithout instance-wise label information. In L-DRO, we introduced the use of entropy and consistency\nterms to facilitate the cooperation between the two modalities, focusing specifically on the concerned\nattributes while minimizing their impact on other factors.\n\nLimitations. Our work has several limitations that should be acknowledged. Firstly, as is the case\nwith other studies utilizing vision-language models as the foundation, the selection of text prompts,\nincluding classification and debiasing prompts in this work, plays a crucial role in achieving reasonable\naverage accuracy and worst-case accuracy. Therefore, even though we can eliminate the need for a\ndomain-oblivious validation dataset during training, careful consideration and appropriate selection\nof prompts are necessary and deserve a comprehensive study in our future work. Additionally, we\nanticipate that there may be certain sub-populations that prove challenging to describe accurately,\nwhich could hinder the application of our proposed method.\n\n9\n\n\fUnder review as a conference paper at ICLR 2024",
    "abstract": "Sub-population shift is a specific type of domain shift that highlights changes in data distribution within specific sub-groups or populations between training and testing. Sub-population shift accounts for a significant source of algorithmic bias and calls for distributional robustness. Recent studies found inherent distributional robustness in multi-modality foundation models, such as the vision-language model CLIP, yet this robustness is vulnerable through parameter fine-tuning. In this paper, we propose leveraging the connection of robustness among different modalities and reshaping the distributional robustness of one modality with another. Specifically, in the context of the distributional robustness of CLIP, we propose to leverage natural language inputs to debias the image feature representations, to improve worst-case performance on sub-populations. Our extensive empirical studies show that image representations debiased by natural language can achieve significant performance improvement and reduction of performance instability under sub-population shifts.",
    "response": {
      "Soundness": 3,
      "Presentation": 4,
      "Contribution": 3,
      "Rating": 6,
      "Confidence": 3,
      "Strengths": "1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\n3. The paper is well-organized and clearly presented.",
      "Weaknesses": "1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.",
      "Questions": "1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?"
    }
  },
  {
    "paper_id": "cNi2EJ8OCh",
    "summary": "Background: Local differential privacy (LDP) is important in data privacy, especially when dealing with sensitive data in untrusted environments. While differential privacy (DP) is well-studied, LDP remains less explored, particularly in the context of functional data classification. This is significant because functional data, observed over time or space, can reveal personal information. The challenge lies in managing the infinite dimensionality and preserving privacy while maintaining data utility.\nGap: The paper identifies a significant gap in the application of local differential privacy to the field of functional data classification, which is largely unexplored. Most existing research focuses on differential privacy or on multivariate data, not on functional data characterized by infinite dimensions.\nHypothesis: The authors propose that by leveraging the property of functional data—being approximable by a finite set of basis functions—they can reduce its dimensionality, thereby facilitating the construction of classifiers under the constraints of local differential privacy (LDP). They posit that this approach can effectively balance privacy and utility in functional data classification.\nMethod: The authors develop algorithms to construct functional classifiers under LDP in both single-server and multi-server settings. In single-server scenarios, they propose a strategy of using fewer samples to train multiple weak classifiers, reserving most samples for evaluation, and using techniques like model reversal and model averaging to improve classifier performance. In multi-server settings, they use federated learning to allow servers to benefit from shared knowledge. The method involves encoding, perturbation using a Laplace mechanism, aggregation, estimation, and leveraging model reversal and averaging strategies to enhance classifier accuracy under LDP.\nConclusion: The paper concludes that the proposed algorithms significantly improve the performance of functional data classifiers under LDP. Techniques like model reversal and model averaging, along with federated learning in multi-server environments, enhance the accuracy of classifiers while adhering to privacy constraints. These findings demonstrate the feasibility and effectiveness of the proposed methods in privacy-preserving functional data classification.\n",
    "full text": "\nFUNCTIONAL CLASSIFICATION UNDER LOCAL DIF-\nFERENTIAL PRIVACY WITH MODEL REVERSAL AND\nMODEL AVERAGE\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nIn the rapidly evolving landscape of technological development, privacy protec-\ntion in data collection and storage demands heightened attention. While there has\nbeen notable research on releasing functional summaries of interest under Differ-\nential Privacy (DP), the area of learning models with functional observations under\nLocal Differential Privacy (LDP) is still largely unexplored. This paper seeks to\naddress this gap by implementing functional projection with a finite basis and in-\ntroducing aggregation techniques that are well-suited for LDP, thereby contribut-\ning to the advancement of privacy-preserving methodologies in functional data\nanalysis. Specifically, we propose algorithms for constructing functional classi-\nfiers designed for both single-server and heterogeneous multi-server environments\nunder LDP. In single-server scenarios, we introduce an innovative allocation strat-\negy where fewer samples are used for training multiple weak classifiers, while\nthe majority are used to evaluate their performance. This enables the construc-\ntion of a robust classifier with enhanced performance by model averaging. We\nalso introduce a novel technique, model reversal, which effectively enhances the\nperformance of weak classifiers. In multi-server contexts, we employ federated\nlearning and enable each server to benefit from shared knowledge to improve the\nperformance of each server’s classifier. Experimental results demonstrate that our\nalgorithms significantly boost the performance of functional classifiers under LDP.\n\n1\n\nINTRODUCTION\n\nAdvances in technologies enable us to collect and process data densely observed over temporal\nor spatial domains, which are termed functional data (Ramsay & Silverman, 2005; Ferraty, 2011;\nHorv´ath & Kokoszka, 2012), distinguishing them from traditional multivariate data. It is typically\nrepresented as curves, surfaces, or anything that varies over a continuum. Privacy preservation for\nfunctional data is an important issue, as the data may reveal individual characteristics or preferences\nthrough their temporal or spatial patterns. This is especially relevant in domains such as health\ninformatics or behavioral science, where medical data of brain scans like DTI (Short et al., 2022)\nor fMRI (Logothetis, 2008) can show brain anatomy and activity, and smart devices like phones,\nwatches, etc. can capture human activity data (Stisen et al., 2015). However, there are only a few\nworks concerning privacy preservation within the realm of functional data.\n\nDifferential privacy (DP, Dwork et al., 2006) is a leading paradigm for privacy-preserving statistical\nanalyses. It provides a rigorous and interpretable definition of data privacy, and limits the amount\nof information that attackers can infer from publicly released database queries. However, in some\nscenarios, when the data collector is untrusted or malicious, or when the data is highly sensitive or\npersonal, DP may not be sufficient. In such cases, we need to consider local differential privacy\n(LDP, Kasiviswanathan et al., 2011) instead, which adds noise at the individual data level before\ncentralization.\nIt has been deployed by major technology companies like Apple (Differential Pri-\nvacy Team, 2017), Google (Erlingsson et al., 2014), Windows (Ding et al., 2017). However, LDP\noften requires more noise to achieve the same privacy level as DP, which can lead to decreased utility\nof the data. Currently, two primary research directions under LDP are statistical queries and private\nlearning(Yang et al., 2020). For statistical queries, where the aggregator aims to collect user data\nto address queries like frequency, mean, and range, there is plenty of research. Conversely, private\n\n1\n\n\fUnder review as a conference paper at ICLR 2024\n\nlearning, which focuses on training models via specific machine learning algorithms, is compara-\ntively less explored. The formal definition of LDP is as follows:\nDefinition 1 (ε-LDP). A randomized mechanism M satisfies ε-LDP, where ε > 0 is the privacy\nbudget, if and only if for any pair of input values u1, u2 in the domain of M and any possible output\nv of M, it holds\n\nP (M(u1) = v) ≤ eεP (M(u2) = v).\n\nPrivate learning faces significant challenges, largely arising from data correlations and high dimen-\nsionality (Wang et al., 2020; Yang et al., 2020; Ye & Hu, 2020). Firstly, challenges arise in pre-\nserving correlations among multiple features (Wang et al., 2019b) and between features and their\ncorresponding labels (Yilmaz et al., 2019). Second, when collecting d-dimensional data under LDP,\ncommon approaches involve either distributing the privacy budget ε across the d dimensions and\nreporting every dimension with ε/d-LDP or allowing users to randomly select a single dimension\nto report with ε-LDP (Nguyˆen et al., 2016; Arcolezi et al., 2021; 2022). A challenge emerges as an\nincrease in d leads to a rapid decay of the privacy budget and a rise in the noise scale.\n\nIn this paper, we propose two novel algorithms to construct functional classifiers under LDP, suit-\nable for single-server and heterogeneous multi-server settings, respectively. Our contributions are\nsummarized as follows:\n\n• Present a process of constructing functional classifiers under LDP. We develop and\noffer theoretical analysis on a projection-based functional classifier, and measure the infor-\nmation loss in classification induced by projection. To the best of our knowledge, this is\nthe first work that models functional data under LDP.\n\n• Introduce a novel technique model reversal. It improves the performance of weak clas-\nsifiers under LDP by inverting their prediction direction (−1× coefficient estimate) when\ntheir accuracy is below a certain threshold. Given a classifier’s accuracy rate distribution,\nwe measure the improvement that model reversal can bring to this classifier.\n\n• Propose a model average algorithm tailored for LDP. It includes our idea of allocating\na larger proportion of clients to evaluate the performance of weak classifiers instead of\ntraining them. It builds upon our methods of evaluating the performance of weak classifiers\nunder LDP, and assigning more suitable weights to these classifiers based on the evaluation.\n\n• Extend to heterogeneous multi-server settings. We propose a federated learning ap-\n\nproach under LDP, where each server benefits from the shared knowledge of others.\n\n2 RELATED WORK\n\nSupervised Learning Under LDP. Given challenges arising from data correlations and high dimen-\nsionality, existing research on supervised learning under LDP is limited. Both Yilmaz et al. (2019)\nand Xue et al. (2019) aimed to train a Na¨ıve Bayes classifier under LDP while preserving the corre-\nlation between feature values and class labels. However, their methods did not demonstrate distinct\nadvantages and suffered from high variance and low utility when the number of features or the size\nof the input domain is large. Alternatively, some research focuses on empirical risk minimization\n(ERM), treating the learning process as an optimization problem solved through defining a series of\nobjective functions. Wang et al. (2019a) constructed a class of machine learning models under LDP\nthat can be expressed as ERM, and solved by stochastic gradient descent (SGD). To address the high\ndimensionality issue, Liu et al. (2020) privately selected the top-k dimensions according to their\ncontributions in each iteration of federated SGD. Additionally, Sun et al. (2020), which trained deep\nlearning models in federated learning (Koneˇcn`y et al., 2016) under LDP, proposed data perturbation\nwith adaptive range and a parameter shuffling mechanism to mitigate the privacy degradation caused\nby high dimensionality. However, to the best of our knowledge, no existing research has explored\nmodeling with functional data, which has infinite dimensionality, within the framework of LDP.\n\nFunctional Data Projection. Recent research has begun to employ projection to reduce the di-\nmensionality of functional data (Horv´ath & Rice, 2015; Pomann et al., 2016; Kraus & Stefanucci,\n2019). These methods are facilitated by the inherent property of functional data, which allows it to\n\n2\n\n\fUnder review as a conference paper at ICLR 2024\n\nbe represented by a linear combination of a set of basis functions. Specifically, to test the equal-\nity of two-sample mean functions, Mel´endez et al. (2021) and G´orecki & Smaga (2015) developed\ntest statistics using projections with finite random functions and data-independent basis functions,\nrespectively. For the functional classification problem, Kraus & Stefanucci (2019) and Sang et al.\n(2022) constructed classifiers based on projections with estimated finite functions.\n\nFunctional Data Under DP and LDP. Previous DP research primarily concentrated on scenar-\nios where a query’s output is a vector. Hall et al. (2013) was a pioneer in providing a framework\nfor achieving (ε, δ)-DP with infinite dimensional functional objects, yet it focused on a finite grid\nof evaluation points. Subsequently, Mirshani et al. (2019); Reimherr & Awan (2019); Jiang et al.\n(2023) conducted further extensive work around (ε, δ)-DP. Regarding ε-DP, a series of studies have\nbeen conducted that utilize finite-dimensional representations (Wang et al., 2013; Alda & Rubin-\nstein, 2017). More recently, Lin & Reimherr (2023) introduced a novel mechanism for releasing\nε-DP functional summaries, termed the Independent Component Laplace Process, which relaxes as-\nsumptions on data trajectories and enhances data utility. However, releasing functional summaries\nunder DP and collecting functional observations under LDP are significantly different tasks. Under\nDP, the server can access all the original data and achieve privacy based on the knowledge of co-\nvariance function and sensitivity of functional objects. Under LDP, however, each functional object\nmust be noised before being sent to the server. Moreover, DP focuses on the quality of the functional\nsummaries, while LDP emphasizes the performance of the models trained on the noised functional\nobservations from the clients. Thus, using finite basis projection for privacy preservation may have\nless impact under LDP than under DP.\n\n3 SINGLE SERVER WITH MODEL REVERSAL AND MODEL AVERAGE\n\nProblem Setup. Suppose there is a server with N clients. Each client has a square-integrable\nfunctional covariate x(t), which is defined over a compact domain I, and the corresponding binary\nlabel y. Without loss of generality, we assume that y ∈ {0, 1} and I = [0, 1]. The primary goal is\nto construct a classifier f based on {(xi, yi)}N\n\ni=1 under local differential privacy (LDP), where\n\n(cid:90)\n\nf (x) = α +\n\nx(t)β(t)dt,\n\nand for a new sample with functional covariate x(t), we predict its label y as ˆy(x) = I(f (x) > 0).\n\nAs pointed by Yang et al. (2020), LDP algorithms typically involve four steps: encoding, pertur-\nbation, aggregation, and estimation. In the first two steps, each client encodes his original value\naccording to the predefined coding scheme, and perturbs encoded value by the randomized algo-\nrithm that achieves local differential privacy. Then, in the last two steps, the server collects all the\nperturbed value from clients and estimates the query result. In the following, we introduce the pro-\ncesses of estimating the functional classifier f under LDP. The overall framework of our algorithm\nis summarized in Algorithm 1.\n\n3.1 ENCODING AND PERTURBATION\n\nFor a client with functional covariate x(t) and label y, the data he reports is transformed through the\nfollowing steps.\n\nDimensionality Reduction. The infinite dimensionality of functional data is a significant challenge\nin functional data analysis, especially under LDP. To address this challenge, we consider mapping\nx(t), onto a finite-dimensional functional space, spanned by functions ϕ1, . . . , ϕd, resulting in a\nlow-dimensional vector z ∈ Rd. This vector is then used as a substitute for x(t) when building the\nsubsequent classifier.\n\nSpecifically, the projection functions, Φ = (ϕ1, . . . , ϕd), are prespecified by the server, and gen-\nerally can be taken as B-Spline basis, Fourier basis, etc. The mapping process is equivalent to\nrepresenting x(t) by the truncated basis Φ, i.e.,\n\nx(t) =\n\nd\n(cid:88)\n\nk=1\n\nzkϕk(t) + ξ(t),\n\n3\n\n(1)\n\n\fUnder review as a conference paper at ICLR 2024\n\nwhere z = (z1, . . . , zd)⊤ is the coefficient vector of Φ, and ξ(t) is the residual function that can’t\nbe represented by Φ. Regarding the expressive capacity of a finite basis, we propose that employing\na finite d is feasible, considering the following three aspects:\n\n1. Functional data fitting: In practice, functional observations are vectors observed at differ-\nent time points, i.e., x = (x(t1), . . . , x(tT ))⊤. When fitting with B-Spline basis of order\nm (m = 4 for cubic spline), the estimation risk decays at the optimal rate of T −2m/(2m+1)\nas the number of knots grows like T 1/(2m+1) (Eubank, 1999), which indicates a relatively\nslow rate of increase. Note that the number of basis d equals the order m plus the number\nof knots, so finite B-Spline basis can achieve an effective fitting;\n\n2. Performance of projection-based classifiers: Lemma 1 in Appendix B.1 demonstrates\nthat, under the assumption that x(t) is from a Gaussian process, classifiers based on pro-\njection with finite basis can achieve the minimum misclassification rate;\n\n3. Functional projection under LDP: Using a finite basis helps to capture population-level\npatterns while avoiding overfitting individual differences. Additionally, replacing x(t) with\nz significantly improves communication efficiency.\n\nIn Appendix B.1, we provide further discussion on the projection-based functional classification and\nmeasure the information loss induced by projection. And we compare the results of d at different\nvalues in Appendix A.2.\n\nRescaling. Before adding noise, to bound the sensitivity, clipping and truncation are commonly\nadopted to rescale the domain of each numeric feature. Here we introduce two types of transforma-\ntion of rescaling z ∈ Rd into z∗ = (z∗\n\nd)⊤ ∈ [−1, 1]d, where for k = 1, . . . , d,\n\n1 , . . . , z∗\n\n(Tanh Transformation)\n\nz∗\nk = tanh(zk),\n\n(Min-Max Transformation)\n\nz∗\nk =\n\nzk − mink′ |zk′|\nmaxk′ |zk′| − mink′ |zk′|\n\n.\n\nThe Tanh transformation introduces non-linearity, whereas the Min-Max transformation maintains\nthe relative relationships between the original data points. For different types of Φ, the two trans-\nformations are suitable for distinct scenarios. For instance, when employing the B-Spline basis as\nΦ, the Tanh transformation is generally more appropriate for cases where functions x(t) in different\nclasses exhibit significant magnitude or value differences. Conversely, the Min-Max transformation\nis more fitting for situations where functions x(t) in different classes exhibit distinct shapes or trends\nwithin the [0, 1] range. In Appendix A.2, we compare the performance of these two transformations.\nPerturbation. Based on Laplace mechanism (Dwork et al., 2006), the rescaled z∗ can be perturbed\nby adding randomized Laplace noise, i.e., reporting z′ = (z′\n\n1, . . . , z′\n\nd) with\n\nz′\nk = z∗\n\nk + ek,\n\n(2)\n\nwhere {ek}d\nk=1 are independently drawn from a Laplace distribution with scale parameter λ =\nd∆/ε1 and ε1 < ε. Please note that ∆ is the sensitivity of z∗\nk ∈ [−1, 1], and\nthe privacy budget for each dimension is ε1/d. Additionally, through randomized response Warner\n(1965), the label y can be perturbed by reporting y′, where\n\nk and ∆ = 2 since z∗\n\nP (y′ = y) = eε2 /(1 + eε2),\n\n(3)\n\nand ε2 = ε − ε1. Thus, a client with observation (x(t), y) only needs to report (z′, y′). In this paper,\nwe take ε2 = ε/(d + 1) = ε1/d, while different ways of allocating the privacy budget warrant\nfurther exploration. The following Theorem 1 demonstrates that the encoding and perturbation\nprocess adheres to ε-LDP.\nTheorem 1. Let M1 be the privacy mechanism that takes (x(t), y) as input and outputs (z′, y′) as\ndescribed above. Then M1 satisfies ε-local differential privacy.\n\n3.2 AGGREGATION AND ESTIMATION\n\nAfter collecting all the perturbed values {(z′\ni, y′\nfunctional classifiers based on the collected data.\n\ni)}N\n\ni=1 from clients, we provide two ways of building\n\n4\n\n\fUnder review as a conference paper at ICLR 2024\n\nMethod I. It’s straightforward to first build a vector classifier with the d-dimensional predictor z′\ni\nby traditional methods, such as logistic regression and support vector machine (SVM). Record the\nestimated intercept and coefficient as ˆα ∈ R and ˆb ∈ Rd, respectively. Then the slope function β(t)\ncan be estimated by (cid:98)β(t) = (cid:80)d\n\nˆbkϕk(t), and the corresponding functional classifier is\n\nk=1\n\nˆf (x) = ˆα +\n\n(cid:90)\n\nx(t) (cid:98)β(t)dt.\n\nk=1 z′\n\nMethod II. Before building the classifier, we can first perform curve recovery, i.e., treating x′\n(cid:80)d\n\ni(t) =\ni,k represents the kth element of z′\ni.\nThen, the classifier can be obtained using the functional conjugate gradient algorithm (CG, Kraus\n& Stefanucci, 2019) or the functional distance weighted discrimination (DWD, Sang et al., 2022).\n\ni,kϕk(t) as the perturbed functional covariate, where z′\n\nWith these two ways of constructing classifiers, in the Appendix A.2, we illustrate the effects of\ndimensionality reduction, rescaling, and perturbation on the misclassification rate of the classifiers.\nExperiment results indicate that the projection of function data and rescaling of coefficient vectors\nhave a small impact on the classifier’s performance. And the performance of two types of transfor-\nmations is similar in our context. Moreover, as the privacy budget ε → 0, the misclassification rates\nof the classifiers based on the perturbed data tend to 50%.\n\n3.3 MODEL AVERAGE\n\nWhile LDP ensures strong privacy guarantee, it introduces significant noise. Modeling on perturbed\ndata often leads to poor performance. To address this, we propose to leverage model averaging to\nenhance classifier performance. Therefore, to assign appropriate weights for weak classifiers, we\npartition N clients into two sets: a training set D with size N0 for constructing weak classifiers and\na validation set Dvalid with size N1 for evaluating their performance, where N0 + N1 = N .\nTraining. The server collects the perturbed value {(z′\ni)}i∈D from the clients in the training set.\nBased on this perturbed training data, multiple weak classifiers can be constructed by random sam-\npling. Specifically, we build B weak classifiers, each based on a random sample of n0(< N0)\ntraining instances. Denote the classifiers as {f (b)}B\nb=1, and their corresponding coefficients as\n{(ˆα(b), (cid:98)β(b)(t))}B\nb=1. Note that the privacy guarantee is not affected by building multiple classi-\nfiers based on the same dataset, since each client reports the perturbed value only once.\n\ni, y′\n\nValidation. For the clients in the validation set, instead of collecting their perturbed observations,\nwe obtain their perturbed evaluations of a classifier’s performance. Specifically, for the given B\nclassifiers, we split the validation set into B subsets, denoted as D(b)\nvalid, b = 1, . . . , B, and evaluate\nthe performance of the classifier f (b) based on the reports of the clients in the subset D(b)\nvalid. Each\nclient in D(b)\nvalid with observation (xi(t), yi) calculates ˆyi = I(f (b)(xi) > 0) and ri = I(ˆyi = yi),\nand reports r′\ni = ri) = q and q ∈ (1/2, 1). In Theorem 2, we demonstrate that the\nvalidation process adheres to LDP, and provide an unbiased estimate of the classifier’s accuracy.\nTheorem 2. Let M2 be the privacy mechanism that takes (x(t), y) as input and outputs r′ as de-\nscribed above. Then for q = eεv /(1 + eεv ), M2 satisfies εv-local differential privacy. Furthermore,\n1 = |D(b)\nlet r(b) be the classification accuracy of the classifier f (b), n(b)\nvalid|, and\n(cid:80)\nr′\ni\n\ni with P (r′\n\n˜r(b) =\n\nˆr(b) + q − 1\n2q − 1\n\nwith ˆr(b) =\n\nvalid\n\ni∈D(b)\nn(b)\n1\n\n.\n\n(4)\n\nThen E(˜r(b)) = r(b) and Var(˜r(b)) ≤ ((eεv + 1)/(eεv − 1))2/(4n(b)\n\n1 ).\n\nFrom Theorem 2, it’s evident that as the sample size for classifier evaluation increases, our estimate\nunder LDP tends to approximate the classifier’s true accuracy. This inspired us to allocate a larger\nproportion of clients for evaluating the performance of weak classifiers rather than for training them.\nThis idea is particularly effective under substantial noise interference, where increasing the training\nsample size may yield limited performance gains. Conversely, expanding the validation sample size\nenhances the accuracy of assessments, significantly aiding in identifying the most effective weak\nclassifiers and contributing to the subsequent development of a superior ensemble classifier.\n\n5\n\n\fUnder review as a conference paper at ICLR 2024\n\nIn the single-server setting, since each client is used to evaluate just one classifier, we set εv = ε\nto achieve ε-LDP. If one client is used to evaluate multiple classifiers, the privacy budget must be\nshared among them, which will further lower the accuracy of the evaluation.\n\nModel Reversal (MR). To optimize the performance of weak classifiers, we propose the model\nreversal technique. For a classifier with estimated accuracy ˜r(b) < 50%, we invert the sign of its\nestimated coefficients, transforming (ˆα(b), (cid:98)β(b)(t)) to (−ˆα(b), − (cid:98)β(b)(t)), which results in a classifier\nwith an enhanced estimated accuracy 1− ˜r(b) > 50%. This approach is underpinned by the principle\nthat, in classification tasks, the angle between the estimated and true coefficient values is paramount,\nunlike in regression where the emphasis is on minimizing their L2 distance. The improvement it\nprovides relies on our accurate evaluation of the performance of classifiers in Theorem 2.\nTheorem 3. For a classifier fε that adheres to ε-LDP, let’s denote its classification accuracy rate\nas r(x) = P (sign(fε(x)) = y|x). Additionally, let rδ represent the potential enhancement in\nclassification accuracy that could be achieved for fε through the application of model reversal.\nThen, if r(x) = r0 for all x(t), we have rδ = max{1 − 2r0, 0}. Otherwise, let’s denote the\ndistribution of classification accuracy rate as pε(r) = P (x(t) ∈ Aε,r) with r ∈ [0, 1], where\nAε,r = {x(t)|r(x) = r}. Then we have\n\nE(rδ) =\n\n(cid:90) 1\n\n0\n\npε(r) max{1 − 2r, 0}dr =\n\n(cid:90) 1/2\n\n0\n\npε(r)(1 − 2r)dr.\n\nTheorem 3 measures the enhancement that model reversal can bring to the classifier fε, which\ndepends on the privacy budget ε and the distribution pε(r). Simulation results in Figure 1 indicate\nthat different classifiers vary in their sensitivity to noise and their distribution pε(r). Theoretical\nanalysis of the distribution pε(r) for different classifiers is beyond the scope of this paper.\n\nModel Selection and Model Average (MA). Given the B weak classifiers, it’s essential to establish\na criterion for selecting the top-performing classifiers and then effectively combine them and get a\nmore robust model. For a specified cutoff value r0 ∈ (0.5, 1), we assign weight wb to the classifier\nf (b), where\n\nwb =\n\nmax(˜r(b) − r0, 0)\nb=1 max(˜r(b) − r0, 0)\n\n(cid:80)B\n\n.\n\n(5)\n\nClearly, classifiers with ˜r(b) ≤ r0 are excluded. And our final estimated classifier is\n\nf ∗(x) = ˆα∗ +\n\n(cid:90)\n\nx(t) (cid:98)β∗(t)dt with ˆα∗ =\n\nB\n(cid:88)\n\nb=1\n\nwb ˆα(b), (cid:98)β∗(t) =\n\nB\n(cid:88)\n\nb=1\n\nwb (cid:98)β(b)(t).\n\n(6)\n\nIn the experiments, we demonstrate the improvements in classification accuracy brought about by\nmodel reversal and model average.\n\nSample Size Balancing. When the total sample size N is substantial, it stands to reason that in-\ncreasing both the number of samples used to the estimation and evaluation of each classifier and the\ntotal number of classifiers can be advantageous. Yet, in practical, for a given N , it is imperative to\ndeliberate on specifying the values of N0 and B. While larger values of N0 and B assist in acquiring\nmore effective weak classifiers, smaller values of N0 and B are preferred to ensure adequate sample\nsizes for each validation subset. In the Appendix A.3, we evaluate classifier performance across\nvarying values of the parameters N0, N1, n0 and B, and further discuss sample allocation.\n\n4 MULTI-SERVER WITH FEDERATED LEARNING\n\nIn the previous section, we introduced the construction of classifiers on a single server through Algo-\nrithm 1. In real-world scenarios, multi-server environments are common. Thus, federated learning\noffers a promising approach. Suppose there are K servers in total, each possessing a classifier, de-\nnoted by f ∗\nk (x), trained on its own data, and these servers mutually exchange their models. In this\nsection, we illustrate how each server can enhance classifier performance by effectively integrating\nclassifiers from various heterogeneous servers under LDP. The details are outlined in Algorithm 2\nin Appendix A.6.\n\n6\n\n\fUnder review as a conference paper at ICLR 2024\n\nAlgorithm 1 Functional Classification under LDP with MRMA\n1: procedure SERVER(Φ, N0, N1, n0, B, ε, εv, r0)\n\nDivide clients into a training set D and a validation set Dvalid with |D| = N0, |Dvalid| =\n\nN1, and spilt clients in the validation set into B subsets, D(b)\n\nvalid, b = 1, . . . , B.\n\nRandomly draw n0 samples from {(z′\nBuild a classifier f (b) based on {(z′\ni, y′\nfor each client in the validation set D(b)\n\ni, y′\ni)}i∈D(b).\nvalid do\n\ni)}i∈D without replacement and denote as D(b).\n▷ See Section 3.2\n\nfor each client in the training set do\n\ni) ← ClientTrain(Φ, ε).\n\ni, y′\n\n(z′\nend for\nfor b = 1, . . . , B do\n\nr′\ni ← ClientValid(f (b), εv)\nend for\nEstimate ˜r(b) by Equation 4.\nif ˜r(b) < 50% then\nf (b) ← −f (b)\n\nend if\n\n2:\n3:\n4:\n5:\n6:\n7:\n\n8:\n9:\n10:\n11:\n12:\n13:\n14:\n15:\n\n▷ Model Reversal\n\nend for\nEstimate w = (w1, . . . , wB)⊤ by Equation 5 with cutoff value r0.\nreturn The final estimated classifier f ∗ by Equation 6.\n\n▷ Model Average\n\n16: end procedure\n1: procedure CLIENTTRAIN(Φ, ε)\n2:\n3:\n4:\n\nEstimate z through the functional regression in Equation 1.\nRescale z into z∗ ∈ [−1, 1]d by Tanh or Min-Max transformation.\nGenerate z′, y′ by Equation 2 and Equation 3, respectively.\nreturn (z′, y′)\n\n5: end procedure\n1: procedure CLIENTVALID(f (b), εv)\n2:\n3:\n\nCalculate ˆy = I(f (b)(x) > 0) and r = I(ˆy = y).\nGenerate r′ by P (r′ = r) = eεv /(1 + eεv ).\nreturn r′\n4: end procedure\n\nk (x)}K\n\nPerturbation. When a server possesses ample data, the validation set can be partitioned into B + K\nsubsets initially, where the latter K subsets are used to evaluate the performance of {f ∗\nk=1 and\nclients report the perturbed evaluations under ϵ-LDP as in Section 3.1. In scenarios where a server’s\ndata is limited, one may consider the iterative use of the validation set. Initially, the validation set is\npartitioned into B subsets, and clients report the perturbed evaluations of the classifiers constructed\nfrom their server under εv = ϵ/2-LDP. Subsequently, the validation set is divided into K subsets,\nand clients report the perturbed evaluations of classifiers from different servers under ε∗\nv = ϵ/2-LDP.\nFederated Learning. Each server, upon receiving the K classifiers and obtaining their performance\nevaluations, can refer to Section 3.2 to undertake both model reversal and model average, where we\nrecord the cutoff value used here as r∗\nk=1. Considering the\nheterogeneity across servers, model averaging in this context is similar to transfer learning (Olivas\net al., 2009). To optimize this process, it is crucial to prevent the transmission of information that is\neither irrelevant or detrimental, thereby avoiding the negative transfer effect (Li et al., 2022).\n\n0 and the obtained classifiers as {f †\n\nk}K\n\n5 EXPERIMENTS\n\nIn this section, we demonstrate the improvements in classification accuracy brought about by model\nreversal and model average. The data generation process is given in Appendix A.1. Assuming\nthe server has a total of N = 3000 clients, we allocate N0 = 500 for training and the remaining\nN1 = 2500 for validation. To construct classifiers, we sequentially draw n0 = 50 samples from\nthe training dataset without replacement, repeating this procedure B = 50 times. At the same\n\n7\n\n\fUnder review as a conference paper at ICLR 2024\n\ntime, we partition the validation set into B = 50 subsets of equal size, enabling us to evaluate\nthe classification accuracy of each classifier using n1 = 50 distinct samples. Both the training\nand validation of classifiers follow the data processing and privacy protection mechanisms proposed\nin Section 3. To assess the performance of the classifiers, we randomly generate a testing dataset\ncomprising 500 samples during each trial, repeating this procedure 500 times.\n\nFigure 1 showcases the misclassification rates, along with their corresponding error bars, for various\nclassifiers across different ε levels. In this figure, “Weak” denotes the average misclassification rate\nof B = 50 weak classifiers obtained through sampling. “MR” represents the average misclassifi-\ncation rate of B weak classifiers after model reversal under LDP. “MA” signifies the results when\nusing model average on weak classifiers with cutoff value r = 0.4, while “MRMA” illustrates the\nresults of applying both model reversal and model averaging. To compare with classic aggregation\nmethods, we train B weak classifiers under LDP. Each classifier is trained with N/B instances from\nthe combined training and validation set. We then obtain the results through majority voting and\naveraging with equal weight, denoted as “Voting” and “Averaging”, respectively. And “All data”\ndenotes the classifier trained with N clients directly.\n\nThe classifier “All data”, even if it is trained with 3000 clients, shows almost no improvement over\nthe classifier “Weak” when ε is small (indicating substantial noise interference). And classifiers\n“Voting” and “Averaging” also perform similarly. However, our proposed techniques, both model\nreversal and model average, significantly improve the performance of all types of weak classifiers.\nAnd MRMA further enhances the performance of SVM and CG classifiers substantially. Figure 7\nin Appendix A.4 demonstrates that even when allocating more clients for training weak classifiers,\nMR and MA can still significantly enhance the classifiers’ performance. For further discussions\nregarding distinctions among different types of classifiers, please refer to Appendix A.2.\n\n(a) Logistic\n\n(b) SVM\n\n(c) DWD\n\n(d) CG\n\nFigure 1: The misclassification rates of four types of classifiers with a single server under ε-LDP.\n\n6 REAL APPLICATION\n\nIn this section, we employ a phonemes dataset derived from the TIMIT Acoustic-Phonetic Contin-\nuous Speech Corpus (Garofolo, 1993). Speech frames in this dataset are extracted from the contin-\nuous speech of 50 male speakers, and a log-periodogram is constructed from recordings available\n\n8\n\n20%40%60%80%0.10.51234510εMisclassification rateWeakMRMAMRMAVotingAveragingAll data20%40%60%80%0.10.51234510εMisclassification rateWeakMRMAMRMAVotingAveragingAll data20%40%60%80%0.10.51234510εMisclassification rateWeakMRMAMRMAVotingAveragingAll data20%40%60%80%0.10.51234510εMisclassification rateWeakMRMAMRMAVotingAveragingAll data\fUnder review as a conference paper at ICLR 2024\n\nat different equispaced frequencies for the five phonemes: “sh” as in “she”, “iy” as in “she”, “dcl”\nas in “dark”, “aa” as in “dark”, and “ao” as in “water”. The log-periodogram data can be viewed\nas functions of frequency, rendering them as functional data, and can be fitted using a Fourier basis.\nThe richness and potential sensitivity of speech data, capable of revealing unique aspects of an indi-\nvidual’s identity such as accent, speech patterns, and even native language or regional background,\nnecessitate the application of LDP.\n\nIn our study, we focus on classifying the phonemes “sh” and “iy”, represented by 1163 and 872\nlog-periodograms, respectively, each of length 256 and with known class memberships. In Figure\n8 in Appendix A.5, we visualize 200 randomly selected functional observations from each group,\nand the curves reconstructed after finite basis projection and transformation. To construct classifiers,\nwe adopt a randomized approach to split the dataset into a testing set (535 instances), a training set\n(300 instances), and a validation set (1200 instances), and B = 24 weak classifiers are learned. This\nentire procedure is replicated 500 times to ensure robustness and reliability of our results.\n\nFigure 2 presents the misclassification rates of classifiers. The results clearly indicate that classifiers\nbased on model reversal and model averaging show significant improvement compared to other\nclassifiers, especially when ε is small.\n\n(a) Logistic\n\n(b) SVM\n\n(c) DWD\n\n(d) CG\n\nFigure 2: The misclassification rates of four types of classifiers with a single server under ε-LDP.\n\n7 CONCLUSION\n\nIn this paper, we delved into functional data classification under local differential privacy, a domain\nthat remains relatively underexplored. We introduced innovative algorithms designed for both single\nand multi-server environments. Notably, our allocation strategy prioritized a substantial proportion\nof clients for performance evaluation over training, paving the way for potential advancements.\nFurther advancements include the model reversal technique, which enhanced weak classifier perfor-\nmance by reversing prediction outcomes, and the adoption of model averaging, which effectively\ncombined weak classifiers. We also applied federated learning in a multi-server context, allowing\nservers to mutually benefit from shared knowledge. Experimental results demonstrated that our al-\ngorithms brought significant improvements to classifiers under LDP constraints. Furthermore, the\nmethodologies we introduced hold promise for broader applications and future exploration.\n\n9\n\n20%40%60%80%0.10.51234510εMisclassification rateWeakMRMAMRMAVotingAveragingAll data20%40%60%80%0.10.51234510εMisclassification rateWeakMRMAMRMAVotingAveragingAll data20%40%60%80%0.10.51234510εMisclassification rateWeakMRMAMRMAVotingAveragingAll data20%40%60%80%0.10.51234510εMisclassification rateWeakMRMAMRMAVotingAveragingAll data\fUnder review as a conference paper at ICLR 2024",
    "abstract": "Local differential privacy (LDP) has been a focal point in data privacy research, yet its application in the field of functional data classification remains underexplored. To address this gap, we present a novel approach that tackles the challenge of infinite dimensionality in functional classification under LDP constraints. The main idea is to leverage the inherent property of functional data---which allows it to be approximated by a linear combination of basis functions---to reduce the dimensionality of data and facilitate the process of model training under LDP constraints. Specifically, we propose algorithms for constructing functional classifiers designed for both single-server and heterogeneous multi-server environments under LDP. In single-server scenarios, we introduce an innovative allocation strategy where fewer samples are used for training multiple weak classifiers, while the majority are used to evaluate their performance. This enables the construction of a robust classifier with enhanced performance by model averaging. We also introduce a novel technique, ``model reversal\", which effectively enhances the performance of weak classifiers. In multi-server contexts, we employ federated learning and enable each server to benefit from shared knowledge to improve the performance of each server's classifier. Experimental results demonstrate that our algorithms significantly boost the performance of functional classifiers under LDP.",
    "response": {
      "Soundness": 2,
      "Presentation": 3,
      "Contribution": 1,
      "Rating": 3,
      "Confidence": 3,
      "Strengths": "1.The paper introduces a novel approach to functional data classification under Local Differential Privacy (LDP), a topic that has not been extensively explored in the existing literature.  \n2.The introduction of model reversal is particularly innovative, as it provides a unique solution to enhance the performance of weak classifiers by inverting their parameters when their accuracy is below a certain threshold.",
      "Weaknesses": "1.Differential Privacy (DP) has already proven to be sufficiently effective in protecting individual privacy, for instance, in defending against membership inference attacks. Moreover, there are numerous existing methods within DP for adding noise to functional input data, where the idea of projecting onto a set of finite bases is also quite common. From this perspective, the innovation of projection might not be high enough, and the necessity of employing Local Differential Privacy (LDP) remains to be further examined.At the same time, the performance degradation caused by residuals has not been analyzed theoretically. The expressive power of a finite basis is quite limited, and for some functions, their residuals could be fatal.\n\n2.Regarding the ideas of model average and model reversal proposed by the authors, compared to traditional ensemble techniques, model average utilizes more clients in the evaluation process, aiming to select better weak classifiers. For each poor classifier, a model reversal approach is employed for improvement. However, since only a small number of clients are used to train the parameters, the classifiers might generally perform poorly; model reversal can enhance the performance of the learners, but it may not necessarily improve them to a satisfactory level. Theorem three provides an expected conclusion, but it does not directly lead to a high-probability bound; the theoretical analysis of model reversal is still lacking.",
      "Questions": "1..Are there specific scenarios or use cases where LDP provides a clear advantage over DP in functional data classification?\n\n2.The idea of projecting data onto a finite basis is quite common in the realm of DP. Could you clarify how your approach innovates or differs significantly from existing methods?\n\n3.The theoretical analysis of model reversal seems to be lacking. Could you provide more details or elaborate on the theoretical foundations of this technique?"
    }
  },
  {
    "paper_id": "6hzNVNSz8O",
    "summary": "Background: Line search methods have been shown to improve the performance of stochastic gradient descent (SGD) by making the learning rate superfluous. Recent research has suggested that line search approaches can effectively overcome the need for hyperparameter tuning in learning rates by evaluating the loss function at multiple gradient points, thereby finding an adaptive learning rate.\nGap: Current line search methods such as the Stochastic Line Search (SLS) have problems with mini-batch noise and are inefficient for larger scale datasets and more complex data domains. There are also stability issues with standard implementations of line search, such as those seen with large step size variations over different runs.\nHypothesis: The paper proposes that an enhancement to Armijo line search, called SaLSa, which incorporates exponential smoothing (momentum) and reduces the line search frequency, can provide a more stable and effective line search method that is better suited for stochastic mini-batching and larger datasets.\nMethod: The SaLSa method enhances Armijo's line search by incorporating an exponential smoothing factor to account for mini-batch noise and reduce the impact of an erratic step size change. The method also limits the frequency of line search computation to improve efficiency. These modifications are implemented as a PyTorch optimizer and tested against SGD and Adam optimizers across a variety of datasets and architectures, including Transformers, CNNs, and MLPs. Major datasets used include ImageNet and the Glue dataset collection for NLP tasks.\nConclusion: SaLSa achieves better performance in terms of lower loss values and higher average accuracy compared to prior methods and tuned optimizers. The results demonstrate SaLSa's robustness and efficiency without requiring extensive hyperparameter tuning. Its implementation as a PyTorch optimizer makes it accessible and practical for a broad range of machine learning applications.\n",
    "full text": "\nNO LEARNING RATES NEEDED: INTRODUCING SALSA\n- STABLE ARMIJO LINE SEARCH ADAPTATION\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nIn recent studies, line search methods have been demonstrated to significantly\nenhance the performance of conventional stochastic gradient descent techniques\nacross various datasets and architectures, while making an otherwise critical choice\nof learning rate schedule superfluous (Vaswani et al., 2019; Mahsereci & Hennig,\n2015; Vaswani et al., 2021). In this paper, we identify problems of current state-of-\nthe-art of line search methods (Vaswani et al., 2019; 2021), propose enhancements,\nand rigorously assess their effectiveness. Furthermore, we evaluate these methods\non orders of magnitude larger datasets and more complex data domains than\npreviously done.\nMore specifically, we enhance the Armijo line search method by speeding up\nits computation and incorporating a momentum term into the Armijo criterion,\nmaking it better suited for stochastic mini-batching. Our optimization approach\noutperforms both the previous Armijo implementation and a tuned learning rate\nschedule for the Adam and SGD optimizers. Our evaluation covers a diverse range\nof architectures, such as Transformers, CNNs, and MLPs, as well as data domains,\nincluding NLP and image data.\nOur work is publicly available as a Python package, which provides a hyperparam-\neter free Pytorch optimizer.\n\n1\n\nINTRODUCTION\n\nIn the field of modern machine learning, there are numerous optimization algorithms available\n(Schmidt et al., 2021). However, determining the most suitable algorithm for a specific problem and\nfinding the appropriate learning rate or learning rate schedule often requires extensive expertise and\ncomputational resources. In particular, the prevailing approach involves treating the learning rate\nas a hyperparameter and training the network repeatedly until the optimal value that yields the best\nperformance is discovered. To simplify and expedite this process, recent research in deep learning\n(Vaswani et al., 2019; Mahsereci & Hennig, 2015; Bollapragada et al., 2018; Paquette & Scheinberg,\n2020) has proposed the reintroduction of line search methods as popular optimization technology,\nwhich effectively identify an adaptive learning rate by evaluating the loss function at different points\nalong the gradient direction, thus eliminating costly hyperparameter tuning.\n\nAs traditional line search requires multiple forward passes per gradient update, a more efficient\napproach is desirable. In Vaswani et al. (2019), a Stochastic Line Search (SLS) has been combined\nwith a smart re-initialization of the step size to alleviate the need for multiple forward passes for every\nstep. This approach was shown in Vaswani et al. (2019) to improve a variety of optimization methods,\nsuch as Stochastic Gradient Descent (SGD) on tasks such as matrix factorization as well as image\nclassification for small networks and datasets. In Vaswani et al. (2021) the authors adapt this line\nsearch to preconditioned optimizers like Adam (Kingma & Ba, 2015) further increasing its usability.\n\nIn this paper we extend upon this work, by introducing a momentum term to the SLS, critically\nimproving its performance and stability. Furthermore, we introduce a limitation on the frequency with\nwhich a line search is performed, greatly reducing the computation needed. Additionally, we conduct\nextensive experiments to evaluate the performance of various optimization methods across different\ndatasets domains and architecture options. Our findings demonstrate that, our improved Stable Armijo\nLine Search Adaptation algorithm, called SaLSa, consistently outperforms the previously introduced\nSLS as well as tuned optimizers, with very little computational overhead (about 3% compared to no\n\n1\n\n\fUnder review as a conference paper at ICLR 2024\n\nline search). We observe the SaLSa optimizers have on average an 1.5% advantage on accuracy and a\n50% lower average log loss at end of training. Additionally, the stability of the training is improved\ncompared to previously introduced versions of SLS.\n\nTo make our work easy to reproduce and use, we implement all methods as PyTorch optimizers. The\nsource code is open-source and free software (MIT licensed) and available at [anonymized URL for\nreview. Code is available in the supplementary material]\n\n2 BACKGROUND\n\nThe stochastic Armijo line search described in Vaswani et al. (2019) is designed to set a step size for\nall network parameters wk at iteration k. In this section, we formulate a modification of the Armijo\ncriterion to handle the ADAM (Kingma & Ba, 2015) direction instead of the classical SGD direction.\nThis is based upon Vaswani et al. (2019; 2021) Moreover, we introduce an improved Armijo criterion,\nwhich mitigates the effect of noise in the mini-batch setting by calculating an exponential moving\naverage smoothing on both sides of the Armijo equation.\n\nWe define the following notation: The loss function is denoted by f (w). || · || denotes the Euclidean\nnorm and ∇f denotes the gradient of f . Given the iteration counter k, fk and ∇fk denote the\nmini-batch loss and its mini-batch gradient.\n\n2.1 ARMIJO LINE SEARCH\n\nThe Armijo line search criterion is defined in Vaswani et al. (2019) as:\n\nfk(wk + ηkdk) ≤ fk(wk) − c · ηk||∇fk(wk)||2,\n\n(1)\n\nwhere dk is the direction (e.g., dk = −∇fk(wk) in case of SGD), c ∈ (0, 1) is a constant (commonly\nfixed to be 0.1 Vaswani et al. (2019)). The step size ηk which satisfies Condition 1 is practically\nobtained by employing a backtracking procedure, i.e., starting with a high initial step-size η0\nk and\niteratively decreasing it by a constant factor δ ∈ (0, 1) until Condition 1 is satisfied (in practice\nδ = 0.9).\n\nTo avoid a monotonically decreasing step size, ηk is increased each step by the following formula:\n\nk = ηk−1 · 21/b\nη0\nas described in Vaswani et al. (2019). In practice for b = 500, this will usually avoid backtracking\nmultiple times per step, since the increase in step size is small. Henceforth, we will refer to this\nalgorithm as SLS.\n\n(2)\n\n2.2\n\nINCLUDING ADAM’S UPDATE STEP IN SLS\n\nIn case of SGD, the direction dk is the negative\nmini-batch gradient.\n\ndk = −∇fk(wk)\n\nAdam’s direction defined in Kingma & Ba\n(2015) can be written as:\n\ngk = ∇fik(wk)\nmk = β1 · mk−1 + (1 − β1) · gk\nvk = β2 · vk−1 + (1 − β2) · g2\nk\nˆmk = mk−1/(1 − βk\n1 )\nˆvk = vk−1/(1 − βk\n2 )\nˆvk + ϵ)\n\ndk = − ˆmk/(\n\n(cid:112)\n\n(3)\n\nAdam combines a momentum-based approach\ntogether with a step size correction built upon\n\nFigure 1: The step size of ADAM + SLS as well\nas ADAM + SaLSa on ImageNet. Colored areas\nindicate variance between runs. Notice the large\nvariations for ADAM + SLS compared to the con-\nsistent and stable behavior of ADAM + SaLSa.\n\n2\n\n\fUnder review as a conference paper at ICLR 2024\n\nthe gradients variance. In the training of Trans-\nformers, these modifications have been shown\nto be important enhancements over the simpler SGD algorithm (Kunstner et al., 2023). The weight\nupdate rule is generally defined as\n\nwk+1 = wk + ηkdk.\n\n(4)\n\nThe Armijo line search criterion from Eq. 1 must be adjusted for the Adam optimizer. We perform\nthis adjustment based on Vaswani et al. (2019; 2021). To check if the Armijo line search criterion is\nsatisfied in the Adam case, we use the direction dk defined in Eq. 3, with momentum β1 = 0. Note\nthat, the Armijo criterion is only guaranteed to be satisfy-able by adjusting the step size ηk, if the\nupdate direction and the gradient direction are identical. However, this condition is not met when\nβ1 ̸= 0 in Eq. 3. Additionally, we replace the gradient norm term ||∇fk(wk)||2 by the preconditioned\ngradient norm ||∇fk(wk)||2\nas in Vaswani et al. (2021) resulting in Eq. 5.\n\n√\n\nˆvk+ϵ\n\nfk(wk + ηkdk) ≤ fk(wk) − c · ηk\n\n||∇fk(wk)||2\nˆvk + ϵ\n\n√\n\n(5)\n\nNote that to perform final weight updates each step we use β1 ̸= 0.\n\n2.3 SLS FAILURE CASES\n\nAs shown in Vaswani et al. (2019; 2021) the pre-\nviously described line search methods perform\nwell on smaller datasets and neural network ar-\nchitectures. However, here we show that these\nmethods have problems to consistently perform\nduring larger scale training.\n\nThe first of these Problems we call \"mini-batch\nnoise\": Eq. 1 and 5 describe criterions which are\nchecked for every mini-batch. This is problem-\natic, since the criteria will be violated subject\nto inherent noise in the mini-batch data. The\nphenomenon is amplified by small mini-batch\nsizes. As can be seen in Figure 2 in a typical\ntraining run the Armijo line search method leads\nto frequent changes of the step size, see Figure\n2.\n\nAnother frequently occuring Problem is mini-\nbatch gradient approximations ∇fk(wk) ≈ 0.\nThese are due to computational precision prob-\nlems, even with float32 precision enabled. For\nan example see Figure 3. In the original im-\nplementation by Vaswani et al. (2021), when-\never ∇fk(wk) ≤ 10−8 no line search was per-\nformed.\n\nFigure 2: The step size of ADAM + SLS (black)\ncompared to ADAM + SaLSa (brown) visual-\nized during a training run of BERT on the MNLI\ndataset.\n\nAdditionally, a problem which only occurs on\nlarge datasets, can be seen in Figure 1. The step\nsize and its variance over 5 different runs on Im-\nageNet is visualized. We observe that the step\nsizes of SLS are very sensitive to initial condi-\ntions, the only difference between the runs is\nthe random parameter initialization of the net-\nwork and the shuffled dataset. This problem\nonly seems to occur on larger datasets and is quite impactful, as some runs do not converge properly,\nor take very long to even begin converging.\n\nFigure 3: The step size of ADAM + SLS (black)\non the CIFAR10 dataset. We sometimes observed\ndrastic drops in step size due to computational\nprecision problems. When training with SaLSa\n(red) we do not observe any such drops.\n\n3\n\n\fUnder review as a conference paper at ICLR 2024\n\n3 METHODS\n\nTo obtain a line search method with better properties in the mini-batch scenario, we propose to\nextend the Armijo criterion with a momentum term. Below we provide a detailed explanation of\nthe modifications we made, the theoretical basis behind them, and our reasoning. Furthermore, we\nintroduce a method to greatly reduce the computational overhead introduced by a line search.\n\n3.1 ADDRESSING MINI-BATCH NOISE\n\nAs an extension to Eq. 1 we propose an exponential smoothing (also called momentum term) of all\nfactors which are dependent on the mini batch in this equation. First we rewrite Eq. 1:\n\nfk(wk) − fk(wk + ηkdk) ≥ c · ηk||∇fk(wk)||2\n\n(6)\n\nfk(wk) − fk(wk + ηkdk) denotes the decrease in loss and ||∇fk(wk)||2 denotes the gradient norm.\nIn order to apply exponential smoothing to both terms we define hk and sk as follows:\n\nhk = hk−1 · β3 + (fk(wk) − fk(wk + ηkdk)) · (1 − β3)\nsk = sk−1 · β3 + ||∇fk(wk)||2 · (1 − β3)\n\n(7)\n\nhk represents the smoothed decrease of the loss with the current step size, sk the smoothed gradient\nnorm and β3 ∈ (0, 1) the smoothing factor used for the exponential moving average.\n\nWe introduce the Stable Armijo Line Search Adaptation (SaLSa) criterion as:\n\nCombining SaLSa and the Adam optimizer is done by computing sk as follows:\n\nhk ≥ c · ηk · sk\n\nsk = sk−1 · β3 +\n\n||∇fk(wk)||2\nˆvk + ϵ\n\n√\n\n· (1 − β3)\n\n(8)\n\n(9)\n\nand computing dk as described in Equation 3, but with β1 = 0. We keep the calculation procedure\nfor the step size ηk the same as previously described.\n\n3.2\n\nINTUITIVE MOTIVATION\n\nAs mentioned in Section 3 due to the inherent noise in mini-batches we expect some of them to\nviolate the original Armijo line search, even if the step size ηk is appropriate for the majority of\nmini-batches around step k in training.\n\nLet us assume that all mini-batches are normally distributed with respect to the Armijo criterion, e.g.\nsome mini-batches fulfill the condition with a wide margin, most fulfill it with a small margin and\nsome rare exceptional batches violate the criterion. In this scenario we do not want to decrease the\nstep size by a large amount each time we get an exceptionally bad mini-batch, since the step size\nis still fitting for most batches and Equation 2 is only increasing the step size slowly. The stable\nArmijo line search adaptation in Equation 8 is implementing exactly this behaviour. Note that the\nexact distribution is not relevant in this thought experiment.\n\nIf we analyze Equation 8, we notice that the right hand side is affected by the current step size ηk\nto the same degree as in the original Armijo line search Equation1. However, the left hand side is\nsubstantially less affected since ηk is part of the exponential smoothing process. This results in a\nslower reduction of the step size ηk as the criterion is fulfilled more easily than the original criterion\nby lowering ηk.\n\n3.3 THEORETICAL ANALYSIS\n\nWe extend the convergence Theorem introduced in the original Armijo paper (Armijo, 1966) for\nthe full batch setting and the SaLSa criterion with SGD from Eq.8. We additionally require that\n\n4\n\n\fUnder review as a conference paper at ICLR 2024\n\nevery found learning rate yields an improved loss f (wk) − f (wk + ηkdk) ≥ 0. This condition\nensures the convergence for an infinite sequence, which may otherwise not be guaranteed due to the\nexponential smoothing. In practice, enforcing this condition did not yield a significant difference in\nthe optimization process and is thus not implemented in our experiments. The proof for the theorem\nbelow, as well as training runs with this condition, can be found in the Appendix A.3.\nTheorem 1 (Convergence Theorem). Let f ≡ fk. For w0 ∈ Rd let S(w0) := {w | f (w) ≤ f (w0)}\nand assume that f (w∗) := inf w∈Rd f (w) exists for a unique point w∗ ∈ Rd with ∇f (w) = 0 for\nw ∈ S(w0) if and only if w = w∗. Any sequence {wk}∞\nk=1 found by the SaLSa criterion with\nf (wk) − f (wk + ηkdk) ≥ 0 and c < 1 converges to w∗.\n\n3.4 ADDRESSING COMPUTATIONAL COSTS\n\nIt is unnecessary and computationally expensive to perform a line search for every step during\ntraining, as for most steps the step size does not need to be changed. The overall training compute\ncost increases by roughly 30% when performing a line search every step. To address this, we propose\nto perform a line search more regularly when a high rate of change of the step size is detected and\nless regularly otherwise. We realize this by keeping 2 different exponential moving averages of the\nstep size ηk which we update after every line search procedure:\n\nWe calculate the average rate of change as follows:\n\n¯ηk(β) = β ¯ηk−1 + (1 − β) · ηk−1\n\nand invert it if rk ≤ 1:\n\nrk =\n\n¯ηk(0.9)\n¯ηk(0.99)\n\n¯rk =\n\n(cid:26)rk\nr−1\nk\n\nif rk ≥ 1\notherwise\n\nwe set the line search frequency Lk to the closest integer of:\n\nLk =\n\n1\n¯rk − 1\n\n(10)\n\n(11)\n\n(12)\n\n(13)\n\nand clamp it to the range Lk+1 ∈ [1, 10]. We perform the line search every Lk+1 steps. This reduces\nthe extra compute needed from roughly 30% to approximately 3% for longer runs. In practice, we\ndid not notice any performance degradation, see the Appendix for ablation studies A.5.\n\n3.5 PRACTICAL CONSIDERATIONS\n\nIn the original Armijo line search implementation a few outliers dominated the determination of\nηk as shown in Figure 2. The hyperparameter c was set with this in mind. In our experiments we\nfound good values for c to be in the range c ∈ [0.3, 0.5]. For all our experiments we used c = 0.3\n(compared to c = 0.1 for the original Armijo line search criterion). Ablation studies on the impact of\ndifferent c values can be found in the Appendix.\n\nFurthermore, we tuned the hyperparameter β3 ∈ [0.9, 0.999] from Eq. 7 on a variety of datasets.\nWe found that although performance is robust to the choice of β3, a value of β3 = 0.99 is the best\ngeneral choice. Larger β3’s result in slower adaptation of the step size to the loss landscape, but less\nsusceptibility to noise. Ablation studies on the impact of different β3 values can be found in the\nAppendix A.4.\n\n4 EXPERIMENTAL APPROACH\n\nIn this section, we detail our experimental design to investigate the performance of our proposed\noptimization method. We utilize datasets, model implementations and weights from the Huggingface\nlibrary (Wolf et al., 2019), the pytorch datasets library and the nanoGPT (Karpathy, 2023) github\nrepository.\n\n5\n\n\fUnder review as a conference paper at ICLR 2024\n\n4.1 CANDIDATES\n\nA quick overview of all candidates we are evaluating can be seen below:\n\n• SGD with tuned learning rate and learning rate schedule\n\n• ADAM with tuned learning rate and learning rate schedule\n\n• SGD + SLS, see Section 2.1\n\n• ADAM + SLS, see Section 2.2\n\n• SGD + SaLSa, see Section 3\n\n• ADAM + SaLSa, see Section 3\n\nAs a baseline comparison we evaluate the ADAM and SGD optimizers with a cosine decay with\nwarm starting for 10% of the total training time. For NLP tasks this warm starting and cosine decay\nis common practice. For the image tasks we compare to a flat learning rate as done in Vaswani et al.\n(2019).\nWe take the peak learning rate for ADAM on natural language tasks η = 2 · 10−5 from the original\nBert paper by Devlin et al. (2019), which presents a good value for a variety of classification tasks,\nincluding the Glue (Wang et al., 2018) tasks upon which we are evaluating. We found the value for\nthe peak learning rate for SGD on the NLP task η = 2 · 10−3 using a grid search.\nWe found the value η = 1 · 10−3 for image classification for ADAM using a grid search. The same\nprocedure resulted in η = 1 · 10−1 for SGD.\n\n4.2 DATASETS AND MODELS\n\nTo evaluate an optimization method it is necessary to perform large scale runs of complex real world\ndatasets and tasks. This is especially important as many optimization methods perform well on small\nscale or clean theoretical tasks, but fail to perform well on real world data.\n\nNatural Language Processing - Transformers\n\nWe consider a common scenario in natural language processing, where a large pre-trained language\nmodel (in our case Bert (Devlin et al., 2019)) is fine-tuned on a small to medium sized dataset. The\nGlue dataset by Wang et al. (2018) is a collection of various popular classification tasks in NLP, and\nit is widely used to evaluate common natural language processing capabilities. All datasets used are\nthe version provided by tensorflow-datasets 4.0.1.\n\nMore specifically of the Glue collection (Wang et al., 2018), we use the datasets Stanford Sentiment\nTreebank SST2, Microsoft Research Paraphrase Corpus MRPC, Stanford Question Answering Dataset\nQNLI, and the Multi-Genre Natural Language Inference Corpus MNLI. These datasets range from\n500 - 400.000 training samples and represent a variety of fine-tuning tasks.\n\nAs a further evaluation metric for language models, we fine-tune GPT-2 (Radford et al., 2019) on the\nShakespeare dataset as described in Karpathy (2023) implementation.\n\nImage Classification - Convolutional Neural Networks\n\nIn image classification common evaluation datasets are CIFAR10 and CIFAR100 (Krizhevsky, 2009),\nboth being small scale (50.000 samples, 32x32 resolution). To obtain more reliable results we\nalso compare on ImageNet (Deng et al., 2009) which consists of roughly 106 samples. We use the\nResNet34 (He et al., 2016) architecture without pre-training for small datasets and ResNet50 for\nImageNet.\n\n4.3\n\nIMPLEMENTATION DETAILS\n\nThe following details are the same for all experiments: All models are trained 5 times and the\naveraged metrics are reported in Tables 1 and 2. The learning curves as well as standard errors are\nvisualized in Figures 4 and 5.\n\n6\n\n\fUnder review as a conference paper at ICLR 2024\n\n(a) MNLI\n\n(b) MRPC\n\n(c) QNLI\n\n(d) MNLI\n\n(e) MRPC\n\n(f) QNLI\n\nFigure 4: The loss (top row) and accuracy curves (bottom row) of the experiments on the GLUE\ndataset with standard error indicated around each line, starting after the first epoch. Accuracy was\ncalculated on the validation data, while loss was calculated on the training data.\n\nA Bert Devlin et al. (2019) model was trained on the NLP dataset with the following hyperparameter\nchoices: Five epochs training time on each dataset. The pooling operation used in the Glue exper-\niments is [CLS]. The maximum sequence length is set to 256 tokens. The batch size used during\ntraining is 32.\n\nFor the image datasets CIFAR10 and CIFAR100 (Krizhevsky, 2009) ResNet34 (He et al., 2016) was\nused. For the ImageNet (Deng et al., 2009) dataset the ResNet50 (He et al., 2016) architecture was\nused, a larger architecture was used due to the increased amount of complexity and size of the dataset.\nThe batch size used during training is set to 128 for Cifar10 and Cifar100 and 256 for ImageNet. We\napplied pre-processing as described in the ResNet paper by He et al. (2016). Models were trained on\nCIFAR10 and CIFAR100 for 100 epochs and on ImageNet for 12 epochs.\n\nThe computing time for all experiments was roughly 65 days on an A40 GPU. Roughly 15 of these\ndays were used for the NLP tasks and 50 for the image datasets.\n\n5 EXPERIMENTAL RESULTS\n\nIn this section, we will describe the results of our experiments. We compare the 6 candidates as\ndescribed in Section 4.1. All metrics reported are average values obtained using 5 training runs. All\naccuracies displayed are calculated on the validation sets. The losses displayed are calculated on the\ntraining sets, smoothed with exponential moving average. The colored areas around each line indicate\nthe standard error of each experiment. We display the accuracies and losses during the training period\nin Figures 4 and 5. The Tables 1 and 2 are displaying peak accuracies and final loss of each candidate.\nAdditional visualizations of experimental results can be found in the Appendix A.2. In summary,\nwe observe the SaLSa methods having on average an 1.5% advantage on accuracy and a 50% lower\naverage log loss at end of training.\n\n5.1 NATURAL LANGUAGE PROCESSING - TRANSFORMER EXPERIMENTS\n\nIn our NLP experiments, as shown in Figure 4 and in the Appendix for GPT-2 and SST2, we have\nobserved that, on average, ADAM + SaLSa achieves a lower final loss compared to ADAM, ADAM\n+ SLS, and SGD + SLS. However, this improvement in loss does not always translate to a significant\ndifference in the accuracy metric. ADAM + SLS and ADAM + SaLSa perform similarly in terms of\naccuracy, but both outperform ADAM and SGD + SLS on average. Note that for similar final losses,\nthe convergence rate of SaLSa is generally faster than that of ADAM, as depicted in Figure 4.\n\n7\n\n\fUnder review as a conference paper at ICLR 2024\n\n(a) ImageNet\n\n(b) Cifar 100\n\n(c) Cifar 10\n\n(d) ImageNet\n\n(e) Cifar 100\n\n(f) Cifar 10\n\nFigure 5: The loss (top row) and accuracy curves (bottom row) of the ResNet experiments on the\nimage datasets with standard error indicated around each line, starting after the first epoch. Accuracy\nwas calculated on the validation data, while loss was calculated on the training data.\n\nTable 1: Peak classification accuracies, averaged over 5 runs, for all datasets and optimization\nmethods. Best performing optimization method is marked in bold.\n\nADAM SGD\n\n-\n\n-\n\nADAM SGD\nSLS\n\nSLS\n\nADAM SGD\nSaLSa\nSaLSa\n\n0.8340\n0.9090\n0.8279\n0.9271\n\n0.8256\n0.8972\n0.8441\n0.9225\n\n0.8446\n0.9044\n0.8667\n0.9261\n\n0.8303\n0.8971\n0.8441\n0.9167\n\n0.8409\n0.9090\n0.8745\n0.9216\n\n0.8305\n0.9014\n0.8473\n0.9245\n\n0.9312\n0.6733\n\n0.9229\n0.7057\n\n0.9401\n0.7128\n\n0.9453\n0.6859\n\n0.9389\n0.7114\n\n0.9384\n0.7064\n\n0.6486\n\n0.5633\n\n0.4836\n\n0.6234\n\n0.6684\n\n0.6594\n\nMNLI\nQNLI\nMRPC\nSST2\n\nResNet34\nCIFAR10\nCIFAR100\nResNet50\nImageNet\n\naverage\naverage rank\n\n0.8215\n3.57\n\n0.8116\n4.86\n\n0.8111\n2.42\n\n0.8204\n4.43\n\n0.8378\n2.14\n\n0.8297\n3.28\n\n5.2\n\nIMAGE - CONVOLUTIONAL NEURAL NETWORKS EXPERIMENTS\n\nIn our image experiments, we have observed that the combination of ADAM + SLS or SGD + SLS\nyields good results for CIFAR10 and CIFAR100, but performs poorly for ImageNet, as depicted\nin Figure 5. We attribute this outcome primarily to stability issues. Specifically, ADAM + SLS\noccasionally produces excessively large step sizes η, or it diminishes them to unreasonably small\nvalues η ≤ 10−10. On the other hand, our enhanced approaches ADAM + SaLSa and SGD + SaLSa,\ndo not encounter these problems and on average deliver the best performance among all methods.\n\n6 RELATED WORK\n\nThe optimization of deep neural networks has been a central topic of research in the field of machine\nlearning. Various techniques and optimizers have been proposed, including but not limited to SGD\n(Robbins & Monro, 1951), Adagrad (Duchi et al., 2011), RADAM (Liu et al., 2020), ADAMW\n(Loshchilov & Hutter, 2019), RMSprop (Hinton & Swersky, 2014) and Adam (Kingma & Ba, 2015).\nHowever, selecting the most suitable optimizer remains a challenge, and there is no clear consensus\non the best according to Schmidt et al. (2021).\n\n8\n\n\fUnder review as a conference paper at ICLR 2024\n\nTable 2: Final losses, averaged over 5 runs, for all datasets and optimization methods. Best performing\n(minimal loss) optimization method is marked in bold. The logarithmic average is taken due to the\nlogarithmic nature of the typical loss.\n\nADAM\n-\n\n0.009567\n0.00258\n0.01312\n0.005857\n2.86\n\nSGD\n-\n\n0.08613\n0.02079\n0.1978\n0.02561\n3.572\n\nADAM\nSLS\n\n0.03713\n0.00504\n0.007298\n0.009457\n2.917\n\nSGD\nSLS\n\n0.06901\n0.03667\n0.05262\n0.0412\n3.566\n\nADAM\nSaLSa\n\n0.005867\n0.000628\n0.003126\n0.006991\n2.772\n\nSGD\nSaLSa\n\n0.02174\n0.0091627\n0.03862\n0.01837\n3.559\n\n0.01394\n0.03739\n\n0.00982\n0.01143\n\n0.0009508\n0.01337\n\n0.05646\n0.08245\n\n0.003314\n0.003774\n\n0.003773\n0.01453\n\n0.9122\n\n0.0355\n2.75\n\n1.547\n\n0.0930\n4.625\n\n2.036\n\n0.0315\n3.125\n\n1.144\n\n0.134\n5.5\n\n0.8339\n\n0.0148\n1.25\n\n0.9788\n\n0.0477\n3.75\n\nMNLI\nQNLI\nMRPC\nSST2\nGPT-2\n\nResNet34\nCIFAR10\nCIFAR100\nResNet50\nImageNet\n\nlog average\naverage rank\n\nIn a recent study bySchmidt et al. (2021) on the topic of optimization methods, it was observed\nthat while there are various optimizers available, there is no definitive best optimizer. The authors\nhighlight that the introduction of more optimizers does not necessarily lead to improved results,\nand therefore, alternative approaches should be explored to enhance optimization techniques. One\nsuch approach with great potential is automatic step size determination. One of the most common\napproaches for this are line search methods, which hold promise for enhancing optimization processes\n(Nar & Sastry, 2018; Vaswani et al., 2019; Paquette & Scheinberg, 2020; Galli et al., 2023).\n\nIn this work we particularly build upon Vaswani et al. (2019). The Armijo line search method\nintroduced there, offers several important advantages over other optimization techniques: no hyperpa-\nrameter tuning of the learning rate, faster convergence rates and better generalization. A significant\ndrawback of this method, along with other line search approaches, is that it requires at least an\nadditional forward pass per update step. Consequently, this leads to an increase of approximately\n30% in computational resources required per training step.\n\nRecent work has shown that Transformers are highly sensitive to the choice of learning rate and\nlearning rate schedule schedule during training Liu et al. (2020); Kenneweg et al. (2022). To address\nthis issue, various approaches have been proposed, such as RADAM Liu et al. (2020) and warm\nstarting. In this work, we show that our approach is able to train these highly sensitive architectures\nwell. Other related work includes Granziol et al. (2022) which studies the correlation between batch\nsize and learning rate, Streeter & Dillon (2022) which shows theoretical and practical results for\ntraining using higher order gradients, Arous et al. (2022) which studies the scaling limit of SGD in\nthe high dimensional regime and Kunstner et al. (2023) which investigates why Adam is so effective\nat training the Transformer architecture.\n\nThe optimization of neural networks continues to be an important area of research, to which, the\ndevelopment of effective and reliable line search methods, which work on sensitive architectures such\nas transformers or large scale convolutional neural networks, constitutes a significant contribution.\n\n7 CONCLUSION\n\nWe have introduced SaLSa, an automatic step size selection method and built a hyperparameter free\ngeneral purpose optimizer on top. We have compared its performance against tuned learning rates\nfor larger datasets and architectures than previously done in optimizer evaluations for line search\nmethods. The SaLSa optimizer performance compares favorably in these cases, while requiring no\ntuning of learning rates, or code overhead, as well as minimal compute overhead. We recommend\nits use as a first choice for training deep neural networks in these domains and publish the code as a\nPython package.\n\n9\n\n\fUnder review as a conference paper at ICLR 2024\n\n8 REPRODUCIBILITY STATEMENT\n\nWe have taken great care to ensure reproducibility in this work. In the Appendix, we show all\nassumptions and the proof for Theorem 1. In the supplementary materials, we provide the exact code\nused for the experiments. We either described all pre-processing steps used in the experiments in\nSection 4 or referenced papers in which they are detailed. All datasets used are publicly available and\nare referenced accordingly.",
    "abstract": "In recent studies, line search methods have been demonstrated to significantly\nenhance the performance of conventional stochastic gradient descent techniques\nacross various datasets and architectures, while making an otherwise critical choice\nof learning rate schedule superfluous Vaswani et al. (2019); Mahsereci & Hennig\n(2015); Vaswani et al. (2021). In this paper, we identify problems of current state-of-the-art of line search methods Vaswani et al. (2019; 2021), propose enhancements,\nand rigorously assess their effectiveness. Furthermore, we evaluate these methods\non orders of magnitude larger datasets and more complex data domains than\npreviously done.\nMore specifically, we enhance the Armijo line search method by speeding up\nits computation and incorporating a momentum term into the Armijo criterion,\nmaking it better suited for stochastic mini-batching. Our optimization approach\noutperforms both the previous Armijo implementation and a tuned learning rate\nschedule for the Adam and SGD optimizers. Our evaluation covers a diverse range\nof architectures, such as Transformers, CNNs, and MLPs, as well as data domains,\nincluding NLP and image data.\nOur work is publicly available as a Python package, which provides a hyperparameter free Pytorch optimizer.",
    "response": {
      "Soundness": 3,
      "Presentation": 4,
      "Contribution": 3,
      "Rating": 6,
      "Confidence": 3,
      "Strengths": "The paper is clearly written. The motivation and theoretical analysis are clear. The thoroughness of the experimental setup is excellent. The contribution is very significant and I think likely to be very useful to machine learning practitioners in general.",
      "Weaknesses": "While the presented results are very compelling, it would be even stronger to show experiments on Transformer models in a pretaining setting as well. Additionally, it would be beneficial to compare different sizes of Transformer (and CNN) to show the persistence of the benefit of the method across the scaling of the model. These experiments may plausibly be left as future work (the presented ablations are more important and very thorough) but they represent practical questions that ML practitioners will have, so would strengthen the current work if included.",
      "Questions": "Nits:\nFigs 4 and 5: it is a little difficult to read these plots, as you have to go back and forth visually between the plot and the legend to see which curve is which. Consider making Salsa lines dashed / SLS lines dotted / baselines solid (or something of that nature) to help improve readability.\nFig 5: loss label missing at top of (a), accuracy label missing at top of (d)\nFig 1: fix occluded label text for y axis"
    }
  },
  {
    "paper_id": "eJhgguibXu",
    "summary": "Background: In model-based reinforcement learning, agents use learned models of environment dynamics to improve policies. Models help generate experiences for learning policies or planning outcomes, enhancing sample efficiency and adaptability to new tasks by reducing the need to relearn environment fundamentals. Despite this, model accuracy limits these benefits, as inaccuracies can cause failures when agents trust them in erroneous states. Humans, however, effectively use imperfect intuitive physics models to navigate and reason about the world.\nGap: Existing model-based reinforcement learning approaches face challenges with model inaccuracies and compounding errors over long rollouts, which can lead to failures in policy learning and hinder exploration efficiency, especially in environments with continuous action spaces and sparse rewards. Current literature focuses on mitigating these inaccuracies rather than leveraging them for exploration.\nHypothesis: The authors hypothesize that learned approximate models can be used to guide exploration towards useful regions of the state and action space, thereby improving sample efficiency and policy learning in complex environments with large action spaces and sparse rewards.\nMethod: The authors propose learning dynamics models for intuitive physics tasks using graph neural networks (GNNs) that incorporate object, relational, and event structures. By training these models in a task-agnostic way with data collected from object collision interactions, they develop methods to identify rewarding action clusters. These clusters guide exploration in policy learning through an epsilon-greedy strategy, combined with Deep Deterministic Policy Gradients (DDPG), to improve learning efficiency. The models were applied to the CREATE environment for physics-based puzzles, targeting tasks requiring precise tool placements to achieve goals.\nConclusion: The study demonstrates that approximate models learned using GNNs can effectively generalize to unseen tasks and improve policy sample efficiency by guiding exploration towards rewarding action spaces. This approach accelerates convergence in reinforcement learning tasks, suggesting that approximate models can be advantageous for exploration in environments with challenging features such as large action spaces and sparse rewards.\n",
    "full text": "\nUSING APPROXIMATE MODELS FOR EFFICIENT EX-\nPLORATION IN REINFORCEMENT LEARNING\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nIn model-based reinforcement learning, an agent uses a learned model of environ-\nment dynamics to improve a policy. Using a learned model of the environment to\nselect actions has many benefits. It can be used to generate experience for learning\na policy or simulate potential outcomes in planning. It allows flexible adaptation to\nnew tasks and goals without having to relearn the underlying fundamentals of the\nenvironment from scratch. These sample efficiency and generalisation gains from\nmodel use are restricted by the model’s accuracy. An imperfect model can lead to\nfailure if trusted by the agent in regions of the state space where predictions are\ninaccurate. It is well-documented in cognitive and developmental psychology that\nhumans use approximate intuitive models of physics when navigating the world\nin everyday scenarios. These intuitive models, despite being imperfect, enable\nhumans to reason flexibly about abstract physical concepts (for example, gravity,\ncollisions and friction), and to apply these concepts to solve novel problems with-\nout having to relearn them from scratch. In other words, humans efficiently make\nuse of imperfect models. In this paper, we learn dynamics models for intuitive\nphysics tasks using graph neural networks that explicitly incorporate the abstract\nstructure of objects, relations and events in their design. We demonstrate that\nthese learned models can flexibly generalise to unseen tasks and, despite being\nimperfect, can improve the sample efficiency of policy learning through guiding\nexploration to useful regions of the state and action space.\n\n1\n\nINTRODUCTION\n\nIn model-based reinforcement learning, the agent possesses a model of the dynamics of the environ-\nment, and learns from the experience generated by this model (Sutton & Barto, 2018). The model\nis either learned or given. Using the experience generated from a model to improve a policy and/or\nvalue function is often referred to as planning. Generating experience or transitions from a model\nand learning from this can significantly improve the sample efficiency of learning, since fewer direct\ninteractions with the environment are required to generate experience. Using a model can enable\nflexible adaptation to new tasks and goals without having to relearn the underlying fundamentals\nof the environment from scratch (Lake et al., 2016). These sample efficiency and generalisation\ngains from model use are restricted by the model’s accuracy. An imperfect model can lead to failure\nif trusted by the agent in regions of the state space where predictions are inaccurate (Abbas et al.,\n2021). Relating to the use of imperfect or approximate models in the model-based reinforcement\nlearning literature, there has been a focus on (1) mitigating the impact of model inaccuracies on\npolicy learning by restricting rollout length or relying less on uncertain predictions (Janner et al.,\n2019; Abbas et al., 2021); and (2) learning and using simplified models that abstract away details\nthat are irrelevant for value-based planning (Grimm et al., 2020; 2021; Schrittwieser et al., 2019),\nor, more generally, learning the model in a way that links it to its use in value-based planning (Saleh\net al., 2022).\nIt is well-documented in cognitive and developmental psychology that humans use approximate intu-\nitive models of physics when navigating the world in everyday scenarios (Piloto et al., 2022). These\nintuitive models enable humans to reason flexibly about abstract physical concepts such as gravity,\ncollisions and friction. Humans demonstrate the capacity to construct rich action priors based on\ntheir intuitive models of physics to flexibly solve physical reasoning problems (Allen et al., 2020).\nThese action priors can be used to guide exploration through biasing and restricting the search over\n\n1\n\n\fUnder review as a conference paper at ICLR 2024\n\nactions to those that have been previously useful, thereby, speeding up learning (Rosman & Ra-\nmamoorthy, 2015). Taking inspiration from this, we propose using learned dynamics models to\nguide exploration towards rewarding regions of the action space to accelerate learning. Efficient\nexploration is a major challenge for reinforcement learning algorithms, especially in environments\nwith continuous action spaces and where a sequence of precise actions have to be chained together\nin order to receive some sparse reward signal from the environment (Ecoffet et al., 2021). By using\napproximate and imperfect models to guide exploration only and not generate experience or tran-\nsitions from which the reinforcement learning algorithm directly learns, we mitigate the impact of\nmodel error on stable policy learning, while still benefiting from the information contained in the\npredictions of these models, however uncertain.\nWe make the following contributions:\n\n• We learn dynamics models for intuitive physics tasks that leverage the abstract structure\nof objects, relations and events to flexibly generalise across tasks. To do this, we learn\nthe models using graph neural networks based on the Interaction Network (Battaglia et al.,\n2016), that rely on the assumptions of distinct edge update functions for unique pairs of\nobject types, dynamic edge activations and relative information.\n\n• Using the learned approximate dynamics models, we compute the statistics of action distri-\nbutions identified as rewarding by the model, and show that sampling from these distribu-\ntions significantly improves the frequency of reward signals obtained from the environment.\n• We incorporate the model-identified rewarding action distributions in an epsilon-greedy\nexploration strategy for reinforcement learning that demonstrates accelerated convergence\nto optimal policies.\n\n2 BACKGROUND\n\nWe model the agent’s interaction with the environment as a Markov Decision Process, represented\nby a tuple ⟨S, A, P, R, γ⟩ of states, actions, transition probability function, reward function and\ndiscount factor. At each time step t, the agent observes the environment state st, and then se-\nlects an action at. At the next time step, the agent receives reward r(st, at) and transitions to\nthe next state st+1 according to some dynamics transition function f : S × A → S. The goal\nof reinforcement learning is for an agent to learn a policy π(at|st), a mapping from states to ac-\ntions, that maximises the expected sum of future rewards that can be achieved from the current state\nEπ[(cid:80)∞\nIn model-free reinforcement learning, the dynamics function f and reward function r are unknown\nand the agent learns only from direct interaction with the environment. In model-based reinforce-\nment learning, the agent has a model of the environment, which represents the dynamics function\nf and reward function r, and learns from the experience generated by this model (Sutton & Barto,\n2018). The model is either learned or given. In this work, we assume that the agent has access to the\nunderlying reward function r and we focus on learning the dynamics function f .\n\ni=0 γir(St+i, At+i)|St = s].\n\n2.1 GRAPH NEURAL NETWORKS FOR PHYSICS PREDICTION\n\nGraph neural networks (GNNs) have demonstrated success in flexible physics prediction, with the\ncapacity to generalise across variable object counts and scene configurations (Battaglia et al., 2016;\nChang et al., 2017; Sanchez-Gonzalez et al., 2020). This success is attributable to their use of strong\nrelational inductive biases - assumptions about the relationships and interactions among entities\n(Battaglia et al., 2018). The Interaction Network (Battaglia et al., 2016) assumes pairwise relations\nbetween entities, enabling scaling to variable object counts. It represents objects as the nodes of a\ngraph and their relations as edges. The Interaction Network is an example of a Message Passing\nNeural Network (Gilmer et al., 2017), which computes the updated node fetaures x′\n\ni as follows:\n\nx′\ni = ψ(xi,\n\n(cid:88)\n\nj∈N (i)\n\nϕ(xi, xj, dj,i)),\n\n(1)\n\nwhere xi ∈ RF denotes the features of node i, dj,i denotes the (optional) edge features from sender\nnode j to receiver node i, ψ is the per-node differentiable update function, and ϕ is the per-edge\n\n2\n\n\fUnder review as a conference paper at ICLR 2024\n\ndifferentiable update function that computes the effect of the interaction between receiver node i and\nsender node j (usually instantiated as multilayer perceptrons). Note that the GNN is permutation-\ninvariant because the per-edge update function ϕ is symmetrically applied to all interactions; and the\ninteraction effects are aggregated using a permutation-invariant operation, in this case, summation.\nA larger, more complex system can be represented by a graph structure G = ⟨O, R⟩, where nodes O\nrepresent objects and edges R represent their pairwise relations. The per-node and per-edge update\nrules in 1 are applied across the full graph to update the features of receiver nodes for the next step.\n\n2.2 DEEP DETERMINISTIC POLICY GRADIENTS (DDPG)\n\nDDPG (Lillicrap et al., 2019) is an off-policy reinforcement learning algorithm that uses the\nBellman equation to concurrently learn an action-value or Q-function and a deterministic pol-\nicy µ(s). DDPG is adapted for continuous action spaces by using the following approximation\nmaxa Q(s, a) ≈ Q(s, µ(s)). Since the action space is continuous, the optimal Q∗(s, a) is differen-\ntiable with respect to the action. Since the policies learned are deterministic, noise is added to the\nactions sampled from the policy at training time (usually using an Ornstein-Uhlenbeck (Uhlenbeck\n& Ornstein, 1930) or Gaussian noise process) for exploration.\n\n3 RELATED WORK\n\nApproximate models and model-based reinforcement learning. Recently, there has been an in-\ncreased focus in the model-based reinforcement learning literature on learning useful models as\nopposed to accurate models (Saleh et al., 2022). Saleh et al. (2022) motivate that a model that\naccurately predicts environment dynamics may not necessarily be the most useful to a learner and\ninstead focus on learning planning models that can be used to improve value function learning as a\nmeta objective. Other works have investigated approaches for reducing the detrimental impacts of\ninaccurate models on learning. Abbas et al. (2021) propose an approach for estimating model uncer-\ntainty and use this uncertainty measure to limit the impact of planning in regions of the state space\nwhere models are inaccurate. Limiting the length of model rollouts to reduce compounding model\nerror is a widely adopted technique in the literature (Janner et al., 2019; Holland et al., 2019). The\nvalue equivalence principle (Grimm et al., 2020) places emphasis on accurately predicting returns\nand not environment transitions. We propose that approximate models can be useful by directing\nexploration towards useful regions of the action space in settings with large or continuous action\nspaces and where rewards are sparse.\nIntuitive physics. It is well-established in cognitive and developmental psychology that people use\napproximate and abstract models of physics when interacting with the physical world (Spelke, 1990;\n2003; Gerstenberg & Tenenbaum, 2020). Much of the work on learning models for physical intu-\nition focuses on the specialised tasks of block stacking and stability prediction (Battaglia et al., 2013;\nGroth et al., 2018; Lerer et al., 2016). Girdhar et al. (2021) and Allen et al. (2020) focus on intuitive\nphysics puzzles. However, the tasks they investigate are one-step and do not involve the sequential\ndecision-making typical of traditional reinforcement learning tasks. Allen et al. (2020) also do not\nlearn a dynamics model, but use a ground truth simulator with added noise for their experiments.\nAction priors and hard exploration. Exploration can be difficult in environments with large or\ncontinuous action spaces and where a sequence of precise actions have to be chained together in\norder to receive some sparse reward signal from the environment. To enable more effective ex-\nploration, Ecoffet et al. (2021) store states visited during exploration, revisit promising states and\nexplore starting from the revisited states in a model-free way. This mitigates the issues of the agent\nforgetting how to reach previously visited states and not first returning to a state before exploring\nfrom it. Action priors can be used to guide exploration through biasing and restricting the search\nover actions to those that have been previously useful, thereby, speeding up learning (Rosman &\nRamamoorthy, 2015). We propose using a learned model to explore from promising states and us-\ning the statistics of promising actions identified by the model to compute cluster distributions of\nrewarding actions that can be used to guide exploration.\nGraph neural networks for simulating physics. Battaglia et al. (2016) assume a fixed set of nodes\nand edges for training, with only node features varying over time. Sanchez-Gonzalez et al. (2020)\nsimulate complex physics (fluids, rigid solids, deformable materials) assuming a fixed set of nodes\nand dynamic edge structures using a connectivity radius - that is, edges are constructed between\nnodes within some specified radius. Saxena & Kroemer (2022) learn time-varying sparse graph\n\n3\n\n\fUnder review as a conference paper at ICLR 2024\n\nstructures to scale to tasks with a larger number of ‘distractorqnodes (nodes that are not involved in\nany interactions). Edge activations refer to an edge between the relevant nodes existing at a discrete\ntime point. They focus on robotic gripping tasks where the assumption of longer contact durations\nis relevant and use a temporal Gated Recurrent Unit to learn edge activations. In training, they use\na fixed set of nodes. In contrast, our training dataset is noisy with nodes randomly added at regular\nintervals.\n\n4 LEARNING AND USING APPROXIMATE MODELS TO GUIDE EXPLORATION\n\nIN INTUITIVE PHYSICS SETTINGS\n\n4.1 LEARNING DYNAMICS MODELS THAT LEVERAGE STRUCTURAL INVARIANCES\n\nIn the intuitive physics setting, reasoning at the level of objects, relations and events is a natural\nconstruct that can enable generalisation to variable scene configurations and object counts (Battaglia\net al., 2016; Chang et al., 2017). We leverage this structure through the use of graph neural networks\nbased on the Interaction Network. To improve the flexibility and generalisation capacity of the\nlearned models, we make the following additional assumptions:\n\n• Distinct per-edge update functions: Interactions between different pairs of object types\nare considered to be different edge types, and are encoded by edge update functions that are\nspecific to the object pair under consideration. For example, if a ball collides into a wall,\nthe effect of this interaction is encoded by an edge update function specific to the ball-wall\ninteraction. If a ball rebounds off a floor, a different edge update function specific to the\nball-floor interaction is used. Learning per-edge update functions specific to object pairs\nenables improved generalisation to various scene configurations, where these object pair\ninteractions may be present or absent.\n\n• Dynamic edge activations: In a node-dynamic graph, the set of nodes varies over time,\nwhere some nodes may be added or removed (Harary & Gupta, 1997). If a node is removed,\nit is assumed that edges associated with the node are also deleted. In an edge-dynamic\ngraph, the set of edges varies over time, meaning edges may be added or removed from the\ngraph. We represent the state of the system at time t using a graph Gt = (Vt, Et), where\nVt is a set of nodes and Et a set of edges. xi\nt ∈ Vt,\nat time t. dji\nt ∈ Et from sender node j to receiver\nt\nnode i. Given that we are interested in physical contact interactions, dji\nis the distance\nt\nbetween nodes vj\nt. We specify some distance cutoff dcutof f to determine whether\nan edge is active or not. If dji\nt <= dcutof f , the edge eji\nis active and is added to the set\nof active edges Et. An episode can, thus, be described by a sequence of dynamic graphs\n(G0, G1, G2, . . . , GT ) at discrete time points 0, . . . , T .\n\nt is the state feature vector of node i, vi\n\nrepresents the features of the edge eji\n\nt and vi\n\nt\n\n• Relative information: To capture the fact that Newtonian physics is invariant to changes\nin absolute object positions when their relative positions remain the same (all else being\nequal), we construct a dynamic coordinate system that uses the location of a reference\nobject as its origin. The reference object is a dynamic or moving object in the scene and\nthe positions of other objects are expressed relative to the reference object’s position at each\ntime step.\n\n4.2 ACTION CLUSTERS FOR GUIDED EXPLORATION\n\nThe two main challenges in model-based reinforcement learning are: (1) model bias or inaccu-\nracy, and (2) compounding model error over longer rollout horizons. Intuitive physics models are\nonly approximate in nature, yet humans are able to use them to achieve flexible physical reasoning.\nHowever, for policy learning, even small model error can be catastrophic for stable policy learning\n(Janner et al., 2019). Allen et al. (2020) propose that humans demonstrate efficient search for phys-\nical problem solving because they leverage rich action priors, which they subsequently update as\nobservations become available. We investigate using learned approximate models in model-based\nreinforcement learning in a similar way to humans. That is, we use the model to construct distribu-\ntions over actions that it predicts leads to high reward. These can then be used to guide search in a\nreinforcement learning setting. The agent samples actions from these model-identified distributions\n\n4\n\n\fUnder review as a conference paper at ICLR 2024\n\nwhen exploring according to an epsilon-greedy strategy. In environments with large action spaces\n(particularly, continuous action spaces) or sparse rewards, the exploration problem is a difficult one\n(Ecoffet et al., 2021). An agent being in possession of intuitive information about actions that are\npromising for solving a task can direct search towards regions of the state and action spaces that\nprovide useful signals for learning, thereby, accelerating convergence to an optimal policy.\nOur approach to computing these rewarding action distributions using the learned dynamics models\ntakes the following form:\n\n1. For each task, we assume a set of subgoals exist that provide reward signals over shorter\nintervals (shorter than the full episode). The use of subgoals serves to mitigate the impact of\ncompounding model error over longer rollout horizons, since the agent can receive signals\nabout rewarding actions from shorter model rollouts, where predictions are likely to be\nmore reliable (Janner et al., 2019). Additionally, the use of subgoals mitigates the credit\nassignment problem through decomposing the task into smaller subproblems (Sutton et al.,\n2000).\n\n2. Given the initial state for a task and the first subgoal, we roll out the model over a specified\nhorizon by randomly sampling actions. The model rollouts are performed multiple times\nand the actions that lead to the first subgoal being achieved as predicted by the model\nare stored as rewarding actions for the first subgoal. We then use K-means clustering to\nidentify clusters of the stored rewarding actions for the first subgoal and compute their\nstatistics (cluster means and standard deviations).\n\n3. To limit compounding model error for the next phase of model predictions: starting from\nthe initial state, we roll out the real environment over a specified horizon multiple times\nby sampling from the computed high-reward action cluster distributions (approximated as\nnormal distributions with the cluster means and standard deviations). The last states of\nthese real environment rollouts are then assigned as the initial states for the next set of\nrollouts relating to the next subgoal. For subsequent subgoals, the initial states used for\nmodel and real environment rollouts will be sampled from this constructed set of initial\nstates.\n\n4. The above is repeated for each subgoal until we reach the final goal.\n\n4.3 EXPLORATION STRATEGY FOR POLICY LEARNING\n\nWe incorporate the useful action distributions computed using the learned dynamics models within\na policy learning framework using an epsilon-greedy exploration strategy. Given some threshold\nvalue ϵthreshold of epsilon ϵ, where 0 ≤ ϵ ≤ 1: if ϵ <= ϵthreshold, sample actions from the intuitive\npriors or cluster distributions, otherwise, sample actions according to the policy with some added\nnoise sampled from a random process.\n\n5 EXPERIMENTS\n\n5.1 SETUP\n\nWe use the Chain Reaction Tool Environment (CREATE) developed by Jain et al. (2020). It offers\na range of physics-based puzzles that require an agent to place tools in real-time to direct a ball’s\ntrajectory to reach a goal location. The action space is 2D and continuous - the 2D placement of\nthe tool at regular time steps (x, y) ∈ R2, where x, y ∈ [−1, 1].In our experiments we consider\nthe following tool types: bucket, rotated square, box, wall and floor. The state feature vector for an\nobject includes its x-velocity, y-velocity, x-position, y-position and a one-hot encoding representing\nthe object type. The state observation st corresponds to the collection of state feature vectors for\nobjects in the scene at time t. We assume that an action is taken approximately every 1.33 seconds\nin the environment.\nHuman prediction of collision resolution is inherently noisy (Smith et al., 2013). Yet even from\nthese noisy intuitive models, humans can construct rich action priors to guide action selection in\norder to find an optimal solution. The 2D continuous action space and the precision of sequential\ntool placements required to solve tasks makes CREATE a challenging environment for exploration.\n\n5\n\n\fUnder review as a conference paper at ICLR 2024\n\n5.2 DYNAMICS MODELS\n\nWe learn task-agnostic dynamics models, that is, models that are not trained in the context of specific\ntasks. Instead, we collect observations that capture the distinct dynamics of collision interactions\nbetween a moving ball and each of the tool types under consideration (bucket, rotated square, box,\nwall and floor). Given a single tool in the scene, a ball is launched at various angles and velocities\nat the tool in order to produce observations of the tool’s dynamics. We collect 1000 one-step ob-\nservations for each tool type. To make the learned model invariant to absolute positions in a scene\ngiven the same relative positions of objects, we transform the observations to reflect the positions of\nthe tools relative to the dynamic ball at each step. The dynamic ball’s position is the origin of the\ncoordinate system at each step.\nFor a given set of tools in a task, we learn a GNN dynamics model from the combined set of obser-\nvations for the tools in the given set. We learn different edge update functions for interactions with\neach tool type. For example, for all tasks that involve the bucket and box tools, we use the same\nGNN dynamics model learned from the collated observations for the bucket and box tool types,\nwhich includes a ball-bucket edge update function and a ball-box edge update function. A dynamics\nmodel is learned for each unique combination of tool types that are included in the tasks we include\nin this paper. See A.1 for details on dynamics model architecture and training hyperparameters.\n\nFigure 1: Individual tool dynamics observations.\n\nFigure 2: Dynamic relative coordinate system with moving ball’s position as origin.\n\n5.3 POLICY LEARNING\n\nWe use DDPG to learn deterministic policies. For each task, we compare the following two ap-\nproaches:\n\n• DDPG with model-guided exploration (DDPG-MGE): Combines the standard DDPG\napproach with guided exploration informed by the learned approximate dynamics models\nas described in 4.2 and 4.3. When not sampling from the learned action clusters, the explo-\nration policy µ′ used is constructed by adding noise sampled from an Ornstein-Uhlenbeck\nprocess N (Uhlenbeck & Ornstein, 1930) to the actor policy as done in Lillicrap et al.\n(2019):\n\nµ′ = µ(st|θµ\n\nt ) + N\n\n(2)\n\n• Standard DDPG (Baseline): A standard model-free DDPG approach that uses the ex-\n\nploratory policy in 2.\n\nWe learn policies for the tasks shown in 3. See A.2 for details on DDPG policy and critic networks\narchitecture and training hyperparameters.\n\n6\n\n\fUnder review as a conference paper at ICLR 2024\n\n(a) Task 1\n\n(b) Task 2\n\n(c) Task 3\n\n(d) Task 4\n\n(e) Task 5\n\n(f) Task 6\n\nFigure 3: Tasks used in policy learning experiments.\n\n5.4 RESULTS\n\nOur experiments aim to answer the following questions:\n\n• Can the approximate dynamics models learned in a task-agnostic way be used to reli-\n\nably identify rewarding regions of the action space across tasks?\nFigure 4 illustrates the distributions of actions selected by sampling from the action clus-\nter distributions identified by the learned models. Figure 5 compares the frequency of\ngoals and subgoals being hit over 1000 episodes using model-guided exploration versus\nrandom exploration. Here, for model-guided exploration, the agent only samples from the\nmodel-identified action cluster distributions (ϵthreshold = 1), whereas random exploration\nrefers to the agent sampling from its policy and adding noise sampled from an Ornstein-\nUhlenbeck process. Only for Tasks 1 and 3 does random exploration lead to an occurrence\nof the final goal being hit. The success of model-guided exploration varies across tasks,\nbut overall, there is a marked increase the frequency of subgoals and goals being hit under\nmodel-based exploration compared to random exploration.\n\n• Can exploration guided by the model be used to accelerate convergence to an optimal\n\npolicy?\nSince models are learned in a task-agnostic way, this means that in total three models\nwere used across six tasks (a separate one for each unique combination of tool types in\nthe scene). 2000 observations were used to train each model (that is, 2000 interactions\nwith the real environment per model). To compute the model-identified action clusters,\n200 rollouts from the real environment were required for each task. Sample efficiency\ngains for policy learning were achieved in four out of six tasks by using model-guided\nexploration (shown in Figure 6). Exploration epsilon refers to the epsilon threshold used -\na threshold of 0 indicates that no model-guided exploration was used. For ϵthreshold > 0.1,\nan exploration decay strategy was employed to prevent policy learning diverging due to\nexcessive exploration.\n\n7\n\n\fUnder review as a conference paper at ICLR 2024\n\n(a) Task 1\n\n(b) Task 2\n\n(c) Task 3\n\n(d) Task 4\n\n(e) Task 5\n\n(f) Task 6\n\nFigure 4: Actions sampled from cluster distributions computed using learned dynamics models.\n\n(a) Final goal hit\n\nFigure 5: Comparison of the frequency of goals and subgoals being hit over 1000 episodes using\nmodel-guided exploration versus random.\n\n(b) Subgoal hit\n\n8\n\n\fUnder review as a conference paper at ICLR 2024\n\n(a) Task 1\n\n(b) Task 2\n\n(c) Task 3\n\n(d) Task 4\n\n(e) Task 5\n\n(f) Task 6\n\nFigure 6: Cumulative reward plots at train time comparing DDPG-MGE and DDPG (baseline).\nRolling average is computed over a window of 100 previous episodes and refers to the percentage\nof these episodes that achieved the goal.\n\n6 CONCLUSIONS\n\nHumans rely on approximate models of intuitive physics to flexibly solve a variety of physical\nreasoning problems. These models are inherently uncertain, yet are effectively used by humans to\ngeneralise to unseen problems. In this paper, we learn dynamics models for intuitive physics tasks\nusing graph neural networks that explicitly incorporate the abstract structure of objects, relations\nand events in their design. We demonstrate that these learned models can flexibly generalise to\nunseen tasks and, despite being imperfect, can improve the sample efficiency of policy learning\nthrough guiding exploration to useful regions of the state and action space.\nRelating to the use of imperfect or approximate models in the model-based reinforcement learning\nliterature, there has been a focus on (1) mitigating the impact of model inaccuracies on policy\nlearning by restricting rollout length or relying less on uncertain predictions; and (2) learning and\nusing simplified models that abstract away details that are irrelevant for value-based planning, or,\nmore generally, learning the model in a way that links it to its use in value-based planning. However,\nin environments that are challenging for exploration because of large or continuous action spaces\nand sparse reward signals, we propose that approximate models can be used to guide exploration\ntowards useful regions of the action space to accelerate convergence to an optimal policy.\nWe show that using graph neural networks based on the Interaction Network (leveraging the\nabstract structure of objects, relations and events) with the additional assumptions of distinct edge\nupdate functions for unique pairs of object types, dynamic edge activations and the use of relative\npositional information enables effective generalisation to various scene configurations and unseen\ntasks. We show that these learned approximate dynamics models trained in a task-agnostic way can\nreliably identify rewarding regions of the action space across tasks. Sampling from distributions\nover model-identified rewarding actions using an epsilon-greedy exploration strategy within a\nreinforcement learning framework achieves sample efficiency gains in terms of accelerated policy\nconvergence to an optimal solution.\nFuture work that learns subgoals for tasks instead of relying on them being specified would sig-\nnificantly improve our approach to identifying rewarding actions using learned dynamics models.\nAdditionally, iteratively updating and refining the model-identified distributions over rewarding\nactions as real experience is collected by the agent could improve the accuracy of action selection\nthat result in high rewards as training progresses.",
    "abstract": "In model-based reinforcement learning, an agent uses a learned model of environment dynamics to improve a policy. Using a learned model of the environment to select actions has many benefits. It can be used to generate experience for learning a policy or simulate potential outcomes in planning. It allows flexible adaptation to new tasks and goals without having to relearn the underlying fundamentals of the environment from scratch. These sample efficiency and generalisation gains from model use are restricted by the model’s accuracy. An imperfect model can lead to failure if trusted by the agent in regions of the state space where predictions are inaccurate. It is well-documented in cognitive and developmental psychology that humans use approximate intuitive models of physics when navigating the world in everyday scenarios. These intuitive models, despite being imperfect, enable humans to reason flexibly about abstract physical concepts (for example, gravity, collisions and friction), and to apply these concepts to solve novel problems without having to relearn them from scratch. In other words, humans efficiently make use of imperfect models. In this paper, we learn dynamics models for intuitive physics tasks using graph neural networks that explicitly incorporate the abstract structure of objects, relations and events in their design. We demonstrate that these learned models can flexibly generalise to unseen tasks and, despite being imperfect, can improve the sample efficiency of policy learning through guiding exploration to useful regions of the state and action space.",
    "response": {
      "Soundness": 2,
      "Presentation": 1,
      "Contribution": 2,
      "Rating": 3,
      "Confidence": 3,
      "Strengths": "The paper is mostly fairly easy to understand and aims to tackle an important issue of efficient exploration.",
      "Weaknesses": "The description of the experiments needs improvement. Moreover, the results are not very convincing. Further details are described below.",
      "Questions": "1.\tFigures 2,3 and 4 need more descriptive captions as well as better descriptions in the text.  For example, what do the symbols correspond to in Fig 3? Similarly, in Fig 4, a legend (along with better descriptions) would have made it much easier to interpret the results. \n\n2.\tWhy is it that the relative Goal hits in Fig 5a are much higher compared to other tasks?\n\n3.\tIt is not immediately clear why the approach applies only to intuitive physics based tasks. I think this point needs to be emphasized better in the introduction.\n\n4.\tOver how many trials were the experiments conducted in Fig 6? In general, from the learning curves, the learning does not look stable.\n\n5.\tDoesn’t exploration epsilon=0 imply no model guidance? If so, for say, task 4, they both reach the same asymptotic performance. Why is this the case, while in some of the other tasks, the asymptotic  performances are very different?\n\n6.\tAblations showing the effect of $\\epsilon_{threshold}$ are missing. \n\n7.\tAs claimed in the last line on Page 7, why would the policy learning diverge? I believe that the learning would still occur (as DDPG learns off-policy) but due to a lack of exploitation, the learning curves would not reflect the learnt policy.\n\n8.\tPerhaps the authors could have considered Phyre environments (https://ai.meta.com/tools/phyre/) to validate their approach.\n\n9.\tIn Fig 6, I assumed the orange color always corresponds to exploration epsilon of 0, but for task 6, the colors are swapped.\n\n10.\t“By using…however uncertain” – This sentence towards the end of the introduction is too long and can be phrased better."
    }
  },
  {
    "paper_id": "eEslYpY6Yq",
    "summary": "Background: Graph Neural Networks (GNNs) have become a state-of-the-art approach for various graph-related tasks due to their efficiency in aggregating features from neighboring nodes. Mixup, on the other hand, is a data augmentation technique that averages features and labels from multiple samples to generate new examples. Both techniques utilize information aggregation from multiple samples to enhance learning models.\nGap: The paper addresses the gap in understanding the potential connection between graph convolution operations used in GNNs and Mixup techniques. There is a lack of exploration on whether these seemingly unrelated techniques are fundamentally connected or could be unified under a common framework.\nHypothesis: The main claim of the paper is that graph convolution can be mathematically expressed as a special form of Mixup under two mild conditions: the Homophily Relabel condition where the target node's label is assigned to its neighbors, and the Test-Time Mixup condition where feature mixing is performed during the testing phase.\nMethod: The paper uses a mathematical framework to establish the equivalence between graph convolution and Mixup. It demonstrates that both Graph Convolutional Networks (GCN) and Simplified Graph Convolution (SGC) can be reformulated to adhere to the Mixup technique under specified conditions. The equivalence is validated by training a Multi-Layer Perceptron (MLP) with homophily relabeling and test-time Mixup and comparing its performance to that of GCNs. Various experiments with different data splits and well-known datasets (Cora, CiteSeer, and PubMed) are conducted to provide empirical support for the claims.\nConclusion: The findings indicate that graph convolution is indeed a form of Mixup when homophily relabeling is applied during the training phase and Mixup is performed during testing. This revelation offers a new perspective on understanding graph neural networks, suggesting that efficient network designs can be achieved by integrating Mixup techniques. Additionally, the novel insights into GNNs could lead to development of simpler yet effective versions of these networks, promising comparable performance with reduced complexity.\n",
    "full text": "\nON THE EQUIVALENCE OF GRAPH CONVOLUTION AND\nMIXUP\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nThis paper investigates the relationship between graph convolution and Mixup tech-\nniques. Graph convolution in a graph neural network involves aggregating features\nfrom neighboring samples to learn representative features for a specific node or\nsample. On the other hand, Mixup is a data augmentation technique that generates\nnew examples by averaging features and one-hot labels from multiple samples. One\ncommonality between these techniques is their utilization of information from mul-\ntiple samples to derive feature representation. This study aims to explore whether\na connection exists between these two approaches. Our investigation reveals that,\nunder two mild conditions, graph convolution can be viewed as a specialized form\nof Mixup that is applied during both the training and testing phases. The two\nconditions are: 1) Homophily Relabel - assigning the target node’s label to all its\nneighbors, and 2) Test-Time Mixup - Mixup the feature during the test time. We\nestablish this equivalence mathematically by demonstrating that graph convolution\nnetworks (GCN) and simplified graph convolution (SGC) can be expressed as a\nform of Mixup. We also empirically verify the equivalence by training an MLP\nusing the two conditions to achieve comparable performance. The code can be\nfound at https://anonymous.4open.science/r/GraphConv_is_Mixup-F470.\n\n1\n\nINTRODUCTION\n\nGraph Neural Networks (GNNs) (Wu et al., 2020; Zhou et al., 2020) have recently been recognized\nas the de facto state-of-the-art algorithm for graph learning. The core idea behind GNNs is neighbor\naggregation, which involves combining the features of a node’s neighbors. Specifically, for a target\nnode with feature xi, one-hot label yi, and neighbor set Ni, the graph convolution operation in GCN\nis essentially as follows:\n\n(˜x, ˜y) =\n\nxk, yi\n\n,\n\n(1)\n\n(cid:32)\n\n1\n|Ni|\n\n(cid:88)\n\nk∈Ni\n\n(cid:33)\n\nIn parallel, Mixup (Zhang et al., 2018) is proposed to train deep neural networks effectively, which\nalso essentially generates a new sample by averaging the features and labels of multiple samples:\n\n(cid:32) N\n(cid:88)\n\nλixi,\n\nN\n(cid:88)\n\n(˜x, ˜y) =\n\n(cid:33)\n\nλiyi\n\n,\n\ns.t.\n\nN\n(cid:88)\n\nλi = 1,\n\n(2)\n\ni=1\nwhere xi/yi are the feature/label of sample i. Mixup typically takes two data samples (N = 2).\n\ni=1\n\ni=1\n\nEquation (1) and Equation (2) highlight a remarkable similarity between graph convolution and\nMixup, i.e., the manipulation of data samples through averaging the features. This similarity prompts\nus to investigate the relationship between these two techniques as follows:\n\nIs there a connection between graph convolution and Mixup?\n\nIn this paper, we answer this question by establishing the connection between graph convolutions and\nMixup, and further understanding the graph neural networks through the lens of Mixup. We show\nthat graph convolutions are intrinsically equivalent to Mixup by rewriting Equation (1) as follows:\n(cid:33)\n\n(cid:32)\n\n(cid:32)\n\n(cid:33)\n\n(cid:33)\n\n(cid:32)\n\n(˜x, ˜y) =\n\nxk, yi\n\n=\n\n(cid:88)\n\n1\n|Ni|\n\n(cid:88)\n\n1\n|Ni|\n\nxk,\n\n1\n|Ni|\n\nyi\n\nk∈Ni\n\nk∈Ni\n\nλi= 1\n|Ni|\n=\n\n(cid:88)\n\n(cid:88)\n\nλixk,\n\nλiyi\n\n,\n\nk∈Ni\n\nk∈Ni\n\n(cid:88)\n\nk∈Ni\n\n1\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 1: Graph convolution is Mixup. (a) illustrates the basic idea of Mixup: averaging the features\n). (b) shows the graph convolution operation where\nand one-hot labels of multiple samples (\n) is the weighted average of the features of all its neighbors. (b) →\nthe feature of the target node (\n(c) shows that graph convolution is Mixup if we assign the label (\n) to all\nof its neighbors (\n\n). (d) shows that Mixup is empirically equivalent to GCN.\n\n) of the target node (\n\n,\n\n,\n\n,\n\nwhere xi and yi are the feature and label of the target node ni. This equation states that graph\nconvolution is equivalent to Mixup if we assign the yi to all the neighbors of node ni in set Ni\n\nTo demonstrate the equivalence between graph convolutions and Mixup, we begin by illustrating that\na one-layer graph convolutional network (GCN) (Kipf & Welling, 2016b) can be transformed into an\ninput Mixup. A two-layer GCN can be expressed as a hybrid of input and manifold Mixup (Verma\net al., 2019). Similarly, simplifing graph convolution (SGC) (Wu et al., 2019) can be reformulated as\nan input Mixup. We thus establish the mathematical equivalence between graph convolutions and\nMixup, under two mild and reasonable conditions: 1) assign the target node’s label to neighbors\nin the training time (referred to as Homophily Relabel); 2) perform feature mixup in the test time\n(referred to as Test-Time Mixup).\n\nWe further investigate the conditions required for the equivalence between graph convolution and\nMixup, focusing on the effect of Homophily Relabel and Test-Time Mixup. To explore Homophily\nRelabel, we demonstrate that training an MLP with Homophily Relabel (called HMLP) is equivalent\nto training GCN in terms of prediction accuracy. This finding provides a novel perspective for\nunderstanding the relationship between graph convolution and Mixup and suggests a new direction\nfor designing efficient GNNs. To investigate Test-Time Mixup, we train GNNs without connection\ninformation, perform neighbor aggregation during inference, and find that this approach can achieve\nperformance comparable to traditional GNNs. This result reveals that Test-Time Mixup can be a\npowerful alternative to traditional GNNs in some scenarios, and suggests that Mixup may have a\nbroader range of applications beyond traditional data augmentation. Our investigation on Homophily\nRelabeling and Test-Time Mixup provides valuable insights into the theoretical properties of GNNs\nand their potential applications in practice. We highlight our contributions as follows:\n\n• We establish for the first time connection between graph convolution and Mixup, showing that\ngraph convolutions are mathematically and empirically equivalent to Mixup. This simple yet\nnovel finding potentially opens the door toward a deeper understanding of GNNs.\n\n• Given that both graph convolution and Mixup perform a weighted average on the features of\nsamples, we reveal that graph convolution is conditionally equivalent to Mixup by assigning the\nlabel of a target node to all its neighbors during training time (Homophily Relabel). Also, we\nalso reveal that in the test time, graph convolution is also Mixup (Test-Time Mixup).\n\n• Based on Homophily Relabel and Test-Time Mixup, we propose two variants of MLPs based on\n\nthe Mixup strategy, namely HMLP and TMLP, that can match the performance of GNNs.\n\nRelated Work. Graph neural networks are widely adopted in various graph applications, including\nsocial network analysis (Fan et al., 2019), recommendation (Wei et al., 2022; Deng et al., 2022; Cai\n\n2\n\n,˜x˜y,˜x˜y,˜x˜yy0(d)  GCN≈MixupAssign to all neighbors(Homophily Relabel),x0x2x1y0y1y2,y0x0x2x1y0y0(a) Three-sampleMixup(b) Graph convolution (target node is      ),y0x0x2x1MixupEqualFeatureLabel(c) Mixupwith Homophily Relabel Graph Convolutiony0y1y2y0y0y0y0\fUnder review as a conference paper at ICLR 2024\n\net al., 2023; Tang et al., 2022), knowledge graph (Cao et al., 2023; Zhao et al., 2023), molecular\nanalysis (Sun et al., 2022; Zhu et al., 2023a; ZHANG et al., 2023; Wang et al., 2023b; Corso et al.,\n2023; Liao & Smidt, 2023; Xia et al., 2023; Hladiš et al., 2023), drug discovery (Sun et al., 2022;\nZhang et al., 2023c), link prediction (Chamberlain et al., 2023; Cong et al., 2023) and others (Chen\net al., 2023a). Understanding the generalization and working mechanism of graph neural networks\nis still in its infancy (Garg et al., 2020; Zhang et al., 2023a; Yang et al., 2023; Baranwal et al.,\n2023). The previous work attempts to understand graph neural networks from different perspectives,\nsuch as signal processing (Nt & Maehara, 2019; Bo et al., 2021; Bianchi et al., 2021), gradient\nflow (Di Giovanni et al., 2022), dynamic programming(Dudzik & Veliˇckovi´c, 2022), neural tangent\nkernels (Yang et al., 2023; Du et al., 2019; Sabanayagam et al., 2022) and influence function (Chen\net al., 2023b). There is also a line of works that analyzes the connection between GNNs and\nMLPs (Baranwal et al., 2023; Han et al., 2022b; Yang et al., 2023; Tian et al., 2023), which is similar\nto the proposed method in Section 3. In this work, we understand graph neural networks through\na fresh perspective, Mixup. We believe that this work will inspire further research and lead to the\ndevelopment of new techniques to improve the performance and interpretability of GNNs.\n\nIn parallel, Mixup (Zhang et al., 2018) and its variants (Verma et al., 2019; Yun et al., 2019;\nKim et al., 2020; 2021) have emerged as a popular data augmentation technique that improves\nthe generalization performance of deep neural networks. Mixup is used to understand or improve\nmany machine learning techniques. More specifically, deep neural networks trained with Mixup\nachieve better generalization (Chun et al., 2020; Zhang et al., 2020; Chidambaram et al., 2022),\ncalibration (Thulasidasan et al., 2019; Zhang et al., 2022a), and adversarial robustness (Pang et al.,\n2019; Archambault et al., 2019; Zhang et al., 2020; Lamb et al., 2019). Mixup are also used to\nexplain ensmeble (Lopez-Paz et al., 2023). See more related work in Appendix A.\n\nThe Scope of Our Work. This work not only provides a fresh perspective for comprehending Graph\nConvolution through Mixup, but also makes valuable contributions to the practical and theoretical\naspects of graph neural networks, facilitating efficient training and inference for GNNs when dealing\nwith large-scale graph data. It’s important to note that certain complex GNN architectures, such as\nattention-based graph convolution (Veliˇckovi´c et al., 2018; Xu et al., 2018; Javaloy et al., 2023),\nmay not lend themselves to a straightforward transformation into a Mixup formulation due to the\nintricacies of their convolution mechanisms. In our discussion, we explore how these GNNs can also\nbe considered as a generlized Mixup in Section 2.4.\n\n2 GRAPH CONVOLUTION IS MIXUP\n\nIn this section, we reveal that the graph convolution is essentially equivalent to Mixup. We first\npresent the notation used in this paper. Then we present the original graph convolution network\n(GCN) (Kingma & Ba, 2015) and simplifying graph convolutional networks (SGC) (Wu et al., 2019)\ncan be expressed mathematically as a form of Mixup. Last, we present the main claim, i.e., graph\nconvolution can be viewed as a special form of Mixup under mild and reasonable conditions.\n\nNotations. We denote a graph as G(V, E), where V is the node set and E is the edge set. The number\nof nodes is N = |V| and the number of edges is M = |E|. We denote the node feature matrix as\nX = {x1, x2, · · · , xN } ∈ RN ×d, where xi is the node feature for node ni and d is the dimension of\nfeatures. We denote the binary adjacency matrix as A ∈ {0, 1}N ×N , where Aij = 1 if there is an\nedge between node ni and node nj in edge set E, and Aij = 0 otherwise. We denote the neighbor set\nof node ni as Ni, and its 2-hop neighbor set as N 2\ni . For the node classification task, we denote the\nprediction targets of nodes as one-hot labels Y ∈ {0, 1}N ×(C−1), where C is the number of classes.\nFor graph convolution network, we use ˜A = D−1A as the normalized adjacency matrix1, where\nD ∈ RN ×N is the diagonal degree matrix of A, and Dii denotes the degree of node ni. We use\n˜ai ∈ R1×N , i-th row of ˜A, as the normalized adjacency vector of node ni.\n\n2.1 PRELIMINARIES\n\nGraph Convolution. Graph Convolution Network (GCN) (Kipf & Welling, 2016a), as the pioneering\nwork of GNNs, proposes ˆY = softmax( ˜A · σ( ˜A · X · W1) · W2), where W1 and W2 are the\n\n1Our following analysis can be easily generalized to other normalization methods, such as D− 1\n\n2 AD− 1\n2 .\n\n3\n\n\fUnder review as a conference paper at ICLR 2024\n\ntrainable weights of the layer one and two, respectively. Simplifying Graph Convolutional Networks\n(SGC) (Wu et al., 2019) is proposed as ˆY = softmax( ˜A · ˜A · X · W). In this work, we take these\ntwo widely used GNNs to show that graph convolution are essentially Mixup.\n\nMixup. The Mixup technique, introduced by Zhang et al. (2018), is a simple yet effective data\naugmentation method to improve the generalization of deep learning models. The basic idea behind\nMixup is to blend the features and one-hot labels of a random pair of samples to generate synthetic\nsamples. The mathematical expression of the two-sample Mixup is as follows:\n\n(˜x, ˜y) = (λxi + (1 − λ)xj, λyi + (1 − λ)yj) = (λ1xi + λ2xj, λ1yi + λ2yj),\n\n(3)\n\nwhere λi + λj = 1. Based on the two-sample Mixup, the multiple-sample Mixup is presented in\nEquation (2). The mathematical expression presented above demonstrates that Mixup computes a\nweighted average of the features from multiple original samples to generate synthetic samples.\n\n2.2 CONNECTING GRAPH CONVOLUTION AND MIXUP\n\nWe demonstrate that graph convolution, using GCN and SGC as examples, is conditionally Mixup. To\ndo this, we mathematically reformulate the expressions of GCN and SGC to a Mixup form, thereby\nillustrating that graph convolutions are indeed Mixup.\n\nOne-layer GCN is Mixup We begin our analysis by examining a simple graph convolution neural\nnetwork (Kipf & Welling, 2016a) with one layer, referred to as the one-layer GCN. We demonstrate\nthat the one-layer GCN can be mathematically understood as an implementation of the input Mixup\ntechnique. The expression of the one-layer GCN is given by: ˆY = softmax( ˜A · X · W) where\nW ∈ RD×C is the trainable weight matrix. Let us focus on a single node ni in the graph. The\npredicted one-hot label for this node is given by ˆyi = softmax(˜ai · X · W), where ˜ai ∈ R1×N , i-th\nrow of ˜A, is the normalized adjacency vector of node ni. Ni is the neighbors set of node ni. We\nmake the following observations:\n\n• ˜ai · X results in a weighted sum of the features of the neighbors of node ni, which is the\n\nmultiple-sample Mixup. Explicitly, we can rewrite ˜ai · X to ˜x = 1\n|N i|\n\n(cid:80)\n\nk∈Ni xk.\n\n• If we assign node ni’s label to all its neighbors, the label yi of node ni can be interpreted as a\nweighted average of the labels of its neighboring nodes, which is equivalent to performing Mixup\non the labels. Thus we have ˜y = yi = 1\n|Ni|\n\nk∈Ni yk.\n\n(cid:80)\n\nFrom the previous observations, we can see that one-layer GCN synthesizes a new Mixup sample for\nnode ni by mixing up its neighbors’ features and one-hot labels, which is defined as follows:\n\n(cid:32)\n\n(˜x, ˜y) =\n\n1\n|Ni|\n\n(cid:88)\n\nk∈Ni\n\nxk,\n\n1\n|Ni|\n\n(cid:88)\n\nk∈Ni\n\n(cid:33)\n\nyk\n\n.\n\n(4)\n\nTherefore, we conclude that a one-layer GCN is a Mixup machine, which essentially synthesizes a\nnew sample by averaging its neighbors’ features and one-hot labels.\n\nTwo-layer GCN is Mixup We extend our analysis to consider a two-layer GCN. The expression\nfor two-layer GCN is given by ˆY = softmax( ˜A · σ( ˜A · X · W1) · W2), Let us focus on node ni\nin the graph. For the first layer of the two-layer GCN, we have hi = Relu(˜ai · X · W1) ∈ R1×d,\nwhere ˜ai ∈ R1×N is the normalized adjacency vector of node ni. The first layer is the same as the\none-layer GCN as discussed above, which can be regarded as a multiple-sample input Mixup.\nFor the second layer of the two-layer GCN, we have ˆyi = Softmax(˜aiHW2) ∈ R1×C, H ∈ RN ×d\nis the hidden representation of all nodes obtained from the first layer, and W2 ∈ Rd×C is the weight\nmatrix. The predicted one-hot label for node ni, ˆyi, is obtained through a softmax activation function.\nThe 2-nd layer can be regarded as multiple-sample manifold Mixup (Verma et al., 2019) in the\nfollowing:\n\n• ˜ai · H is the multiple-sample Mixup of the hidden representation of the neighbors of node ni.\n(cid:80)\n\nWe rewrite the ˜ai · H to ˜x = ˜ai · H = 1\n|Ni|\n\nk∈Ni hk.\n\nTherefore, we conclude that a multi-layer GCN is a hybrid of input Mixup (first layer) and manifold\nMixup (second layer).\n\n4\n\n\fUnder review as a conference paper at ICLR 2024\n\nSGC is Mixup The 2-layer SGC architecture is represented by the following expression, ˆY =\nsoftmax( ˜A · ˜A · X · W). Similar to one-layer GCN, for the node ni in a 2-layer SGC, we have\n\nˆyi = softmax(˜a2\n\n(5)\ni ∈ R1×N is the adjacency vector within 2-hop neighbors of node ni. The 2-hop neighbor\n\ni · X · W), L = Cross-Entropy( ˆyi, yi),\n\nwhere ˜a2\nset of node ni is represented by N 2\n\ni , Hereby we have\n\n• ˜a2\n\ni · X is the multiple Mixup of the features of the neighbors of node ni. We rewrite the ˜ai · X\nto ˜x = ˜a2\n\n(cid:80) k ∈ N 2\n\ni xk.\n\ni · X = 1\n|N 2\ni |\n\nFrom the above, we have a Mixup of samples for node ni is\n\n(˜x, ˜y) =\n\n\n\n\n\n1\n|N 2\ni |\n\n(cid:88)\n\nxk,\n\nk∈N 2\ni\n\n1\n|N 2\ni |\n\n(cid:88)\n\nk∈N 2\ni\n\n\n\nyk\n\n .\n\n(6)\n\nThus we conclude that an SGC is an input Mixup machine.\n\n2.3 GRAPH CONVOLUTION IS (Conditionally) MIXUP\n\nIt is straightforward to derive the Mixup form of 1-layer, 2-layer GCN, and SGC as discussed above.\nThis leads to the conclusion that 1-layer, 2-layer GCN, and SGC can all be reformulated in the form\nof Mixup. This establishes a mathematical connection between graph neural networks and the Mixup\nmethod. Building on these findings, in this section, we introduce our main contribution as follows:\n\nMain Results\nGraph convolution is Train- and Test-Time Mixup, under two mild and reasonable conditions:\nHomophily Relabel and Test-time Mixup.\n\nThe two primary differences between GNNs and Mixup are as follows:\n\n• Homophily Relabel In the training time, if we assign the label of the target node to all its\n\nneighbors, the Graph Neural Network can be naturally rewritten as a form of Mixup.\n\n• Test-Time Mixup In the test time, the GNNs perform feature mixing on the nodes and then use\n\nthe mixed feature for the inference.\n\nBoth of the above conditions are mild and reasonable and have practical implications for GNN design\nand analysis. The Homophily Relabel operation can be understood as imposing the same label on the\ntarget node and all its neighbors, which corresponds to the homophily assumption for graph neural\nnetworks. The homophily assumption posits that nodes with similar features should have the same\nlabel, which is a common assumption in many real-world graph applications. On the other hand,\nTest-time Mixup can be understood as Mixup at the test time by mixing the features from neighbors\nfor prediction. We examine the above differences in depth in Section 3 and Section 4.\n\n2.4 DISCUSSION\n\nComparison to Other Work. Hereby, we compare our work and previous work. Yang et al. (2023);\nHan et al. (2022b) propose to train an MLP and transfer the converged weight from MLP to GNN,\nwhich can achieve comparable performance to GCN. In this work, these two methods are all included\nin part of our work, Test-Time Mixup. Different from previous work, we provide a fresh understanding\nof this phoneme by connecting graph convolution to Mixup and also derive a TMLP Appendix C.2 to\nimplement Test-Time Mixup. Baranwal et al. (2022) understand the effect of graph convolution by\nproviding a rigorous theoretical analysis based on a synthetic graph. Different form this work, our\nwork understands graph convolution with a well-studied technique, Mixup.\n\n3\n\nIS Homophily Relabel EQUIVALENT TO GCNS TRAINING ?\n\nIn this section, we conduct experiments to examine the Homophily Relabel proposed in Section 2.3.\nThe empirical evidence substantiates our claims, emphasizing the significant effect of Homophily\n\n5\n\n\fUnder review as a conference paper at ICLR 2024\n\nRelabel on GNNs. This understanding can facilitate the design of an MLP equivalent that competes\nwith the performance of GNNs. Note that we explore the transductive setting in our experiments.\n\n3.1 Homophily Relabel YIELDS A TRAINING-EQUIVALENT MLP TO GCN\n\nWe utilize the simple example graph to demonstrate the calculation of the\nloss function in a GCN in the transductive setting. The example graph,\nin Figure 2, comprises three nodes. The blue and red nodes belong to the\ntraining set, while the gray node belongs to the test set. In the transductive\nscenario, the loss function is computed for the entire graph during the\ntraining phase, incorporating both the training and test nodes. In our\nsimple example graph, the loss function will be calculated on the blue\nand red nodes with their actual labels. Notably, the prediction for the gray\nnode, despite it being part of the test set, will also contribute to the overall\nloss calculation in the transductive setting. For node ni, the prediction\n(p0, p1), the loss of the example graph will be as follows\n\nFigure 2: The example\ngraphs. xi\nis the tar-\nget node, The loss of xi\nthat connected two nodes\nwith different labels.\n\nLG = cross-entropy(Y, ˆY) = cross-entropy(y0, ˆy0) + cross-entropy(y1, ˆy1)\n\n= −y00 log(p0) − y01 log(p1)−y10 log(p0) − y11 log(p1)\n= −1 ∗ log(p0) − 0 ∗ log(p1)−0 ∗ log(p0) − 1 ∗ log(p1)\n= −1 ∗ log(p0)−1 ∗ log(p1) = −2 ∗ (0.5 ∗ log(p0)+0.5 ∗ log(p1)).\n\n(7)\n\nwhere y00/y01 is the 0th/1st elements in the one-hot label y0 for node n0. The above analysis shows\nthat the actual training label for target node ni is (0.5, 0.5). From the Mixup perspective, in an\nideally well-trained model utilizing a two-sample Mixup, the following approximation will hold\nf (λkx + (1 − λk)xk) ≈ λkf (x) + (1 − λk)f (xk), If we regard the neighbor aggregation as Mixup,\nwe can easily derive that\n(cid:18) x0 + xi\n2\n\n(f (x0) + f (xi)) , ˆy1 = f\n\n(cid:18) x1 + xi\n2\n\n(f (x1) + f (xi)) .\n\nˆy0 = f\n\n1\n2\n\n1\n2\n\n(8)\n\n=\n\n=\n\n(cid:19)\n\n(cid:19)\n\nFrom the above two equations, we can see that f (xi) are trained with the labels of all its neighbors\nx0 and x1. Thus in the next, we propose explicitly training the nodes in the test set with the label of\ntheir neighbors if they are in the training set. In our example graph, we explicitly train the gray node\nwith label (0.5, 0.5)\n\nHMLP Based on the above analysis, we proposed Homophily Relabel MLP (HMLP), which achieves\ncomparable performance of GCN for via training a MLP as a backbone with Homophily Rela-\nbel. In detail, the proposed method HMLP has two major steps: 1) relabel all the nodes in the\ngraph with the mixed label ˆY = A · Y. 2) train an MLP on all the nodes with ˆY on relabeled\ngraph, only using node features. We illustrate our proposed HMLP with the example graph in\nFigure 2. 1) we relabel all the nodes using Homophily Relabel, the node with new label will be\n(x0, (1, 0)), (x1, (0, 1)), (xi, (0.5, 0.5)) 2) then train the HMLP with these data samples. The test\nsample will be (xi, ?) during the test time. Note that the backbone of HMLP is an MLP.\n\n3.2 CAN HMLP ACHIEVE COMPARABLE PERFORMANCE TO GCN?\n\nTable 1: Performance comparison of GCN\nand HMLP.\n\nTo verify the proposed HMLP can achieve compara-\nble performance to the original GCN, we train the\none-layer GCN (GCN-1), two-layer GCN (GCN-2),\nSGC, and HMLP on the training set and report the\ntest accuracy based on the highest accuracy achieved\non the validation set. We experimented with different\ndata splits of train/validation/test (the training data\nratio span from 10% − 90%), and we also conducted\nexperiments with the public split on Cora, CiteSeer,\nand PubMed datasets. We present the results in Table 1 and Figure 4. Additionally, we also present\nthe training and test curves of the MLP. GCN and HMLP in Figure 3.\n\nCora\n73.57±0.98 88.04±1.26 86.42±1.78\nCiteSeer 71.90±1.69 75.69±1.36 75.94±1.54\nPubMed\n86.90±0.74 87.94±0.64 88.34±0.48\n\nDatasets\n\nAverage\n\nHMLP\n\n77.45\n\n83.89\n\n83.57\n\nGCN\n\nMLP\n\n6\n\nx0x1xiy0=[1,0]y1=[0,1]\fUnder review as a conference paper at ICLR 2024\n\nFigure 4: The performance comparison of the GCN, MLP and HMLP (Ours). The x-axis represents\nthe ratio of training data, and the y-axis represents the accuracy of classification. The results show that\nour proposed method (HMLP) achieves comparable performance to GCN. Note that the architecture\nof our method in train and test time are both MLP. More experimental results on other datasets and\nGNNs are presented in Appendix C.1.\n\nFrom the results in Table 1 and Figure 4, ➊ HMLP\nachieves comparable performance to GCN when\nthe data training data split ratio is large. For the\nFigure 4, the performance of HMLP is significantly\nbetter than MLP, and on par with GCN, especially\nwhen the training data ratio is large. The results\nalso show that when the training data ratio is small,\nHMLP performs worse than GCN. The training and\ntest curves show that HMLP achieve the same test\ncurves (red and green lines in the right subfigure)\neven though the training curve are not the same. This may be because the labels of nodes are changed.\n\nFigure 3: The training and test curves of GCN\nand HMLP.\n\n4 HOW DOES Test-time Mixup AFFECT GCN INFERENCE?\n\nIn this section, we aim to investigate how Test-time Mixup affects the GCN inference process. We\nconduct an empirical study to understand the functionality of Test-time Mixup (neighbor aggregation\nduring the test time). To this end, we propose an MLP involving Test-time Mixup only (TMLP).\n\nTMLP The proposed method TMLP follows the steps: 1) we train an MLP using only the node features\nof the training set. 2) we employ the well-trained MLP for inference with neighbor aggregation\nduring testing. This paradigm has also been investigated in previous works (Han et al., 2022b;\nYang et al., 2023). We call this method TMLP. We illustrate the TMLP with the example graph in\nFigure 2. Specifically, we train the following training examples, (x0, (1, 0)), (x1, (0, 1)). Note that\nthe backbone of TMLP is an MLP. In the following, we explore the performance of TMLP.\n\n4.1 HOW TMLP PERFORM COMPARED TO GCN?\n\nIn the section, we conducted experiments to verify the effect of Test-Time Mixup. In the experiments,\nduring the training, we only use the node feature to train an MLP, and then during the test time, we\nuse the well-trained MLP with the neighbor aggregation to perform the inference. We present the\nresults of varying training data ratio in Figure 6, and we also present the accuracy in Table 2 when\nthe training data ratio is 0.6. Additionally, we compare the learned representation of GNNs and\nTMLP using t-SNE Van der Maaten & Hinton (2008). From these results, we make the following\nobservations:\n\nFrom these results, we make the following observa-\ntions: ➋ TMLP almost achieves comparable perfor-\nmance to GNNs. In the Figure 4, the performance\nof TMLP is significantly better than MLP and on par\nwith GCN, especially when the training data ratio is\nlarge. In some datasets (i.e., Cora, FullCoraML), the\nperformance of TMLP is slightly worse than GCN but\nstill significantly better than MLP. It is worth noting\nthat our proposed TMLP is only trained with the node\n\nTable 2: Performance comparison of GCN\nand HMLP.\n\nDatasets\n\nMLP\n\nGCN\n\nTMLP\n\nCora\n73.57±0.98 88.04±1.26 88.26±1.60\nCiteSeer 71.90±1.69 75.69±1.36 76.35±1.13\nPubMed\n86.90±0.74 87.94±0.64 87.58±0.44\n\nAverage\n\n77.45\n\n83.89\n\n84.06\n\n7\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 6: The performance comparison of the GCNs, MLP and TMLP (Ours). GCN-1 is a one-\nlayer GCN, and GCN-2 is a two-layer GCN. The results show that TMLP achieves a comparable\nperformance to GCN and SGC. Note that the architecture is essentially MLP in training time and\nGCN in the test time. More results on other datasets and GNNs are presented in Appendix C.2.\n\nFigure 7: Decision Boundary of MLP and TMLP. TMLP can cause node features to move away from\nthe learned boundary, as shown with\n\n. More experimental details are in Appendix C.3\n\nfeatures, ignoring the connection information. Table 2 show the average accuracy (84.06) of TMLP is\ncomparable to that of GCN (83.89).\nBased on the node representation, ➌ Test-Time\nMixup make the node representation more dis-\ncriminative than MLP. The visualization clearly il-\nlustrates that the node representations within the same\nclass, as learned by TMLP, are clustered together.\nThis clustering pattern demonstrates the advanced\npredictive power of TMLP, as it appears to have a\nkeen ability to distinguish similar group classes ef-\nfectively. Consequently, this capacity allows TMLP\nto deliver superior classification performance. The\nnode representation clearly show that what TMLP does from the representation perspective.\n\nFigure 5: Visualization of node representa-\ntions learned by MLP and TMLP. Node rep-\nresentations from the same class, after Test-\nTime Mixup, become clustered together.\n\n4.2 HOW DOES Test-time Mixup BENEFIT GCN? INVESTIGATING DECISION BOUNDARY\n\nIn this section, we evaluate the effect of Test-Time Mixup on the decision boundary change from\nMLP to TMLP. To do so, we compare the distance between node features and the decision boundary\nof MLP before and after applying Test-Time Mixup. After training a model with training-time data\nmixup, we evaluate its decision boundary, which separates different classes in the feature space. We\nthen apply test-time mixup techniques to node features and reassess the decision boundary. Our\nfindings indicate that ➍ Test-Time Mixup can cause node features to move away from the learned\nboundary, enhancing model robustness to unseen data. However, it’s crucial to strike a balance\nbetween Test-Time Mixup and model performance, as overdoing Mixup may reduce accuracy.\n\n8\n\nTest-Time Mixup\fUnder review as a conference paper at ICLR 2024\n\nFigure 8: Unifying HMLP and TMLP. these methods together achieve performance comparable to\nGNNs (despite the PubMed dataset, as the MLPs also exhibit better performance than GNNs.).\n\n5 UNIFYING HMLP AND TMLP: VERIFYING THE MAIN RESULTS\n\nIn this section, we unify HMLP and TMLP into a single MLP, which integrates both Homophily\nRelabel and Test-time Mixup. They are supposed to be equivalent to graph convolution. To unify\nthese two MLPs, we 1) relabel all the nodes in the graph with the mixed label ˆY = A · Y. 2)\ntrain an MLP on all the nodes with ˆY on relabeled graph, only using node features. 3) employ the\nwell-trained MLP for inference with neighbor aggregation during testing. To test the efficacy of this\ncombined approach, we perform experiments comparing the performance of the unified MLP with\nthat of individual HMLP and TMLP, as well as other GCN. The experimental setup remains consistent\nwith the previous setting with HMLP and TMLP. We present the results in Figure 8. ➎ Unifying\nHMLP and TMLP can achieve comparable performance to original GNNs.\n\n6 DISCUSSION AND FUTURE WORK\n\nPractical Potentials. Both HMLP and TMLP hold potential value for practical usage. TMLP is\ntraining-efficient, as its backbone and training process are both MLP-based; however, the inference\nrequires Test-Time Mixup (neighbor aggregation), which can be time-consuming. This limitation also\nexists in previous works (Han et al., 2022b; Yang et al., 2023). HMLP, on the other hand, is both\ntraining and test efficient, as both processes are based on MLP. This suggests the potential of HMLP\nfor practical usage on large-scale graphs, as it eliminates the need for connection information during\nboth training and testing.\n\nTheoretical Potentials. HMLP, derived from the Mixup, have the theoretical potential to understand\nthe expressiveness of graph neural network from the Mixup perspective. With the connection between\nMixup and graph convolution, HMLP goes beyond traditional methods to understand the learning\ncapability of the graph neural network.\n\nFuture Work. Based on the above practical and theoretical potentials of the proposed methods in\nthis paper. Here is some potential future works: 1) improve HMLP to make it more practical for\nlarge-scale graphs and the performance when the training data ratio is small. 2) The well-studied\nMixup strategy would be helpful for understanding the expressiveness of graph neural networks.\n\n9\n\n\fUnder review as a conference paper at ICLR 2024\n\nREPRODUCIBILITY STATEMENT\n\nTo ensure the reproducibility of our experiments and benefit the research community, we provide the\nsample source code in the anonymous link https://anonymous.4open.science/r/GraphConv_\nis_Mixup-F470. We will also open-source our implementation after we clean up the codes. The\ndetailed experimental settings required to reproduce our experiments are described in Appendix D.",
    "abstract": "This paper investigates the relationship between graph convolution and Mixup techniques. Graph convolution in a graph neural network involves aggregating features from neighboring samples to learn representative features for a specific node or sample. On the other hand, Mixup is a data augmentation technique that generates new examples by averaging features and one-hot labels from multiple samples. One commonality between these techniques is their utilization of information from multiple samples to derive feature representation. This study aims to explore whether a connection exists between these two approaches. Our investigation reveals that, under two mild conditions, graph convolution can be viewed as a specialized form of Mixup that is applied during both the training and testing phases. The two conditions are: 1) \\textit{Homophily Relabel} - assigning the target node's label to all its neighbors, and 2) \\textit{Test-Time Mixup} - Mixup the feature during the test time. We establish this equivalence mathematically by demonstrating that graph convolution networks (GCN) and simplified graph convolution (SGC) can be expressed as a form of Mixup. We also empirically verify the equivalence by training an MLP using the two conditions to achieve comparable performance.",
    "response": {
      "Soundness": 2,
      "Presentation": 3,
      "Contribution": 1,
      "Rating": 3,
      "Confidence": 4,
      "Strengths": "- **Novel and original.** The paper proposes a very interesting interpretation of graph convolution, which to the best of my knowledge, has not been studied yet.\n- **Great presentation.** Paper is well-written with clear mathematical derivations that are easy to follow.",
      "Weaknesses": "- **Overall significance is unclear and claimed practical/theoretical implications lack support.**\n  - [W1] While Section 6 mentions how TMLP and HMLP can be training-efficient with large-scale graphs since they are MLP-based, there are no experiments to compare computational costs and demonstrate this claim. I also suspect the gain in efficiency would be fairly small since graph convolution can be highly optimized via sparse-dense matrix multiplications.\n  - [W2] The theoretical implications also seem a bit far-fetched: there is a rich line of work on the theoretical expressivity of GNNs [A, B, C, D, E], yet the paper does not convey any clear connection between Mixup and such work to support its theoretical significance. This could most likely constitute another research paper on its own, but any further discussion on what makes \"the Mixup perspective\" particularly interesting with respect to GNN expressivity would be helpful.\n- **Discussion on empirical results do not provide much insight and are often unconvincing due to lack of evidence.**\n  - [W3] Section 3.2 and Figure 3 show that HMLP and GCN share the same test curves, but not the training curves, which the paper conjectures to be \"because the labels of nodes are changed\". I think the discussion could go a bit deeper (maybe via proper ablation),  considering that the training dynamics of HMLP differs from that of GCN in ways other than the node-labels: not only are all the node-labels mixed, but also 1) nodes in the test-set are used for training HMLP (not the case for GCN) and 2) the model parametrization is different. \n  - [W4] Section 4.2 mentions how \"overdoing Mixup may reduce accuracy\", but this observation is not evident from the results in Figure 7, nor is the discussion consistent with the claim that \"Test-Time Mixup enhances model robustness\". Are there any experimental results that can support this observation? Is this discussion in any way related to the oversmoothing problem of graph convolution [F]?\n  - [W5] Figure 8 shows a fairly large discrepancy between GNNs and Mixup on the PubMed dataset. Considering that the main claim is that GNNs and Mixup are equivalent, why are we seeing this discrepancy on performance despite PubMed being a homophilic network just like Cora and CiteSeer?\n\n[A] Morris et al., Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks. (AAAI 2019)\\\n[B] Xu et al., How Powerful are Graph Neural Networks? (ICLR 2019)\\\n[C] Maron et al., Provably Powerful Graph Networks. (NeurIPS 2019)\\\n[D] Zopf, 1-WL Expressiveness Is (Almost) All You Need. (IJCNN 2022)\\\n[E] Feng et al., How Powerful are K-hop Message Passing Graph Neural Networks. (NeurIPS 2022)\\\n[F] Li et al., Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning. (AAAI 2018)",
      "Questions": "- [Q1] I am not sure whether Section 2.4 is really necessary on its own as it does not seem to provide significant information. The section could instead be merged to the Related Work paragraph in Section 1.\n- [Q2] Question on HMLP: given the example graph in Figure 2, HMLP uses $(x_i, (0.5, 0.5))$ for training, yet the ground-truth sample during test time will be just one of $(x_i, (1, 0))$ or $(x_i, (0, 1))$. In this case, wouldn't the difference in the labels be detrimental for test-time performance since the model will be trained to output $(0.5, 0.5)$ given $x_i$?\n- [Q3] How are the \"MLP\"s and \"Mixup\"s different for each row of plots in Figure 8? Was slightly confused why the same method appears in different graphs with different accuracies.\n- [Q4] The end of Practical Potentials paragraph in section 6 mentions that HMLP \"eliminates the need for connection information during both training and testing\". Is this true? Although the model being trained may not require connectivity information, the Homophily Relabeling taken place prior to training does seem to require the connectivity.\n\nThere are also several typos:\n- End of Section 1: \"be considered as a generlized ...\" -> \"be considered as a generalized ...\"\n- Notations paragraph in Section 2: \"one-hot labels $Y \\in \\\\{0,1\\\\}^{N \\times (C-1)}$...\" -> Shouldn't it be $Y \\in \\\\{0,1\\\\}^{N \\times C}$ with $C$ classes?\n- Top of page 5: \"We rewrite the $\\tilde{a}i \\cdot X$...\" -> should be subscript \"$\\tilde{a}_i \\cdot X$\"\n- Section 2.4: \"... understanding of this phoneme ...\" -> \"... understanding of this phenomenon ...\"\n- End of Section 3.1: \"Thus in the next, we propose ...\" -> \"Thus in the next paragraph, we propose ...\"?\n- Bottom of page 7: \"In the Figure 4, the performance ...\" -> Wrong reference. \"In Figure 6, the performance ...\"\n- End of Section 4.1: \"The node representation clearly show that what TMLP ...\" -> \"... clearly show what TMLP ...\""
    }
  },
  {
    "paper_id": "vnKf1jG5eK",
    "summary": "Background: Recent advancements in AI have led to the development of sophisticated multimodal generative models, leveraging both language models and vision models. The progress in the field can be attributed to large language models (LLMs) for text generation and diffusion models for image generation. Collaborative models like Visual ChatGPT and MM-REACT have emerged, which integrate visual information into text descriptions via pre-trained tools. These models heavily rely on pre-trained components and often struggle with requiring extensive training data or dependencies on encoders like CLIP and ImageBind for modality alignment.\nGap: Current multimodal models face limitations such as dependency on extensive training data to bridge the gap between modalities and a scarce ability to generate comprehensive multimodal responses. They frequently employ CLIP or ImageBind encoders that, while effective at learning joint representations for text and images, require massive datasets to map these encodings to language model spaces effectively. Additionally, these models are generally constrained to understand multimodal content rather than generate it.\nHypothesis: The authors propose that a model based on a bidirectional conditional diffusion model, dubbed BiDiffuser, can more efficiently handle the interaction between modalities by minimizing the necessity for vast datasets and complex pre-training, thus improving the generation capabilities in multimodal tasks such as image-to-text and text-to-image generation.\nMethod: EasyGen consists of a bidirectional conditional diffusion model named BiDiffuser and a large language model (LLM). It is designed to manage image-to-text generation tasks by synchronizing the embedding space of BiDiffuser with that of an LLM using a simple projection layer. BiDiffuser converts images into text format for semantic alignment with the LLM. For text-to-image generation, it utilizes the LLM to generate detailed textual descriptions which BiDiffuser interprets to produce visual content. The training involves fine-tuning the BiDiffuser specifically for image-to-text and text-to-image tasks, with the integration layer conditioned via semantically aligned inputs from multimodal datasets, allowing the model to work efficiently in a lab setting with relatively low computational resources.\nConclusion: EasyGen demonstrates the effectiveness of leveraging a bidirectional conditional diffusion model and LLMs for multimodal generation, performing on par with existing state-of-the-art models while utilizing significantly less training data. The approach showcases robust multimodal generation capabilities with efficient training requirements and improved adaptability through simplified architectural adjustments, marking a significant contribution to the field of AI-generated content.\n",
    "full text": "\nMAKING MULTIMODAL GENERATION EASIER:\nWHEN DIFFUSION MODELS MEET LLMS\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nWe present EasyGen, an efficient model designed to enhance multimodal under-\nstanding and generation by harnessing the capabilities of diffusion models and large\nlanguage models (LLMs). Unlike existing multimodal models that predominately\ndepend on encoders like CLIP or ImageBind and need ample amounts of training\ndata to bridge the gap between modalities, EasyGen is built upon a bidirectional\nconditional diffusion model named BiDiffuser, which promotes more efficient\ninteractions between modalities. EasyGen handles image-to-text generation by\nintegrating BiDiffuser and an LLM via a simple projection layer. Unlike most\nexisting multimodal models that are limited to generating text responses, EasyGen\ncan also facilitate text-to-image generation by leveraging the LLM to create textual\ndescriptions, which can be interpreted by BiDiffuser to generate appropriate visual\nresponses. Extensive quantitative and qualitative experiments demonstrate the\neffectiveness of EasyGen, whose training can be easily achieved in a lab setting.\n\n1\n\nINTRODUCTION\n\nRecent times have been remarkable progress in the field of artificial intelligence generated content\n(AIGC), notably in technologies like large language models (LLMs) (Chiang et al., 2023; Touvron\net al., 2023; Brown et al., 2020; Chowdhery et al., 2022; Zeng et al., 2022) for text generation\nand diffusion models Rombach et al. (2022); Nichol et al. (2022); Saharia et al. (2022) for visual\ngeneration. These breakthroughs have paved the way for the development of large-scale multimodal\ngenerative models, sparking a recent trend of incorporating extra visual modules into LLMs. Collab-\norative models, such as Visual ChatGPT (Wu et al., 2023a) and MM-REACT (Yang et al., 2023),\nstrategically use externally pre-trained tools to translate visual information into text descriptions and\nfeed the data into LLMs. However, they are exclusively dependent on pre-trained tools for inference.\nContrarily, end-to-end trained models including the BLIP series (Li et al., 2023b), LLaVA (Liu et al.,\n2023), MiniGPT-4 (Zhu et al., 2023), and mPLUG-Owl (Ye et al., 2023) focus on mapping image\ninformation to the text space of LLMs, enabling LLMs to comprehend visual inputs.\n\nExisting end-to-end models are also not without limitations. First, most of these multimodal models\nrely on either CLIP (Radford et al., 2021) or ImageBind (Girdhar et al., 2023) as their image encoder.\nWhile these encoders excel in learning unified representations that encompass both text and images,\nthey face challenges when it comes to transforming between different modalities. This predicament\nmakes current vision-language models relying heavily on sizable data sets to align CLIP/Bind-encoded\nimages with the language model, due to the disparity between different modalities. Furthermore,\nmost of existing multimodal models lack the ability of generating multimodal responses, as they\nare primarily designed to understand multimodal content and often fall short in generating content\nbeyond text. A recent work Emu (Sun et al., 2023) takes a unified approach to generate visual or\ntextual tokens, but it also heavily relies on vast quantities of training data.\n\nIn this work, we propose EasyGen, a model that facilitates multimodal generation by harnessing the\nstrengths of diffusion models and LLMs. Diffusion models with multiple skills, such as Versatile\nDiffusion (Xu et al., 2023) and UniDiffuser (Bao et al., 2023b), have exhibited remarkable capabilities\nin accurately capturing multimodal distributions. UniDiffuser, in particular, shows great potential in\naccommodating multimodal interactions. It views both image and text as sequential token streams for\ndiffusion calculations, making it well-suited for the text representations in LLMs that are typically\nstructured as sequences. However, since UniDiffuser aims to fit all conditional distributions (including\n\n1\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 1: Our model EasyGen can understand multimodal inputs and generate multimodal responses,\nas illustrated by model-generated speech bubbles in grey color, which include both text and images.\n\nthose conditioned on noisy inputs) into one model, it is less effective on particular tasks such as\nconditional generation based on noise-free inputs. To address this limitation, we finetune UniDiffuser\nwith a specific focus on the targeted image-to-text and text-to-image tasks. The finetuned model,\nreferred to as BiDiffuser, forms a core component of EasyGen for text and image generation.\n\nBiDiffuser is able to convert image data into a textual format, which simplifies the process of\nsynchronizing its embedding space with that of an LLM for semantic comprehension and reasoning.\nAs illustrated in Figure 2, we bridge BiDiffuser and the LLM using a simple projection layer, which\ncan be trained efficiently with a small amount of data for image-to-text tasks such as image captioning\nand visual question answering. Alternatively, the LLM can be utilized to generate detailed descriptions\nand cues derived from text contexts like dialogues, which can aid BiDiffuser in generating accurate\nvisual responses, as illustrated in Figure 2.\n\nFigure 1 demonstrates the capability of EasyGen in process-\ning multimodal inputs and generating the appropriate multi-\nmodal responses (see more examples provided in Appendix. H).\nFurthermore, EasyGen achieves competitive performance com-\npared to state-of-the-art models with much less training data.\nIt is worth noting that the training of EasyGen can be per-\nformed in a laboratory-level environment. Without employing\nparameter-efficient fine-tuning techniques like LoRa (Hu et al.,\n2021), EasyGen only requires about 120 A100 (80G) GPU\nhours during the pre-training process (for training BiDiffuser)\nand 20/72 A100 (80G) GPU hours during the alignment pro-\ncess for fine-tuning FlanT5XL/Vicuna-7B. By using LoRa, the\nFigure 2: Overview of EasyGen.\ntraining process of EasyGen can be significantly more efficient.\nFor instance, the fine-tuning time for Vicuna-7B can be reduced from 70 to just 13 GPU hours (see\nTable 10).\n\n2 BASICS OF DIFFUSION MODELS\n\nUnconditional Generation Given a data sample taken from a real data distribution x0 ∼ q(x0),\ndiffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) first destruct the data by constructing a\nMarkov forward process and gradually injecting noise to the data:\n\nq(x1:T |x0) =\n\nT\n(cid:89)\n\nt=1\n\nq(xt|xt−1),\n\nq(xt|xt−1) = N (xt; (cid:112)1 − βtxt−1, βtI),\n\n(1)\n\nwhere βt ∈ (0, 1) is the variance added at diffusion step t. Then, they learn to reverse the process:\n\np(x0:T ) = p(xT )\n\nT\n(cid:89)\n\nt=1\n\npθ(xt−1|xt),\n\npθ(xt−1|xt) = N (xt−1; µt(xt, t), σ2\n\nt I),\n\n(2)\n\nwhere p(xT ) = N (xT ; 0, I) is the standard Gaussian distribution and µt(·) is the parameterization\nof the predicted mean. Diffusion models are trained to maximize the marginal likelihood of the data\n\n2\n\nCould you please provide an introduction to this model in the picture?What other types of green energy irrigation systems are feasible?I’m really intrigued by wind-powered irrigation systems. Could you please provide a brief introduction and show me some pictures related to it?The image features a solar irrigation system working model that is displayed on a table. It is made of various materials, such as plastic, and features various components, including solar panels, water tanks, a pump, piping, valves, hoses, faucets, plants, soil, dirt, pebbles, bottles, cups, spoons, straws, screws and nuts, among others.There are several other types of green energy irrigation systems apart from solar power. Some of them include wind-powered irrigation system, hydro-powered irrigation system, tidal-powered irrigation system …A wind-powered irrigation system is a type of green energy irrigation system that utilizes wind energy to power the irrigation process. It involves the use of wind turbines, typically mounted on towers, which capture the kinetic energy of the wind and convert it into electrical energy. The generated electricity can be stored in batteries … Here are some possible pictures:<Image><Text>BiDiﬀuserBiDiﬀuserProjection LayerLLMLLMText GenerationImage Generation(a)(b)\fUnder review as a conference paper at ICLR 2024\n\nFigure 3: The training process of BiDiffuser involves finetuning UniDiffuser (Bao et al., 2023b) with\na joint objective of image-to-text and text-to-image tasks.\n\nE[log pθ(x0)], and the canonical objective is the variational lower bound of log pθ(x0). Denoising\ndiffusion probabilistic models (Ho et al., 2020) generate samples xt ∼ q(xt|x0) by injecting noise\nϵ ∼ N (0, I) to the data x0, and train a network ϵθ(·) to predict the added noise ϵ using a standard\nmean squared error loss:\n\nL := Ex0,ϵ,t[∥ϵ − ϵθ(xt, t)∥2].\n\n(3)\n\nNote that µt(xt, t) can be derived from ϵθ(xt, t).\n\nConditional Generation For conditional generation, a paired data (x0, y0) is given, and the aim\nis to model the conditional data distribution q(x0|y0), where y0 can be image class or text prompt.\nConditional generation includes classifier guidance (Dhariwal & Nichol, 2021) and classifier-free\nguidance (Ho & Salimans, 2021). Classifier guidance requires training an extra classifier on noisy\ndata at inference time to improve the sample quality. For classifier-free guidance, no classifier needs\nto be trained. The denosing network ϵθ(xt|y0) simply conditions on the information encoded in y0.\nAt inference time, with a guidance scale s, the modified score estimate is further in the direction of\nϵθ(xt|y0) and away from the unconditional model ϵθ(xt|∅) (∅ is a null token) as follows:\n\nˆϵθ(xt|y0) = ϵθ(xt|∅) + s · (ϵθ(xt|y0) − ϵθ(xt|∅)).\n\n(4)\n\n3 EASYGEN: EASY MULTIMODAL GENERATION WITH A BIDIRECTIONAL\n\nCONDITIONAL DIFFUSION MODEL AND LLMS\n\nWe propose EasyGen, a model capable of processing multimodal inputs and generating multimodal\noutputs. It achieves easy multimodal generation by leveraging a bidirectional conditional diffusion\nmodel to effectively bridge the gap between different modalities and an LLM to comprehend multi-\nmodal tasks and produce textual responses containing cues for multimodal message creation. In the\nsubsequent section, we outline the multimodal generation process of EasyGen.\n\n3.1 BIDIFFUSER: A BIDIRECTIONAL CONDITIONAL DIFFUSION MODEL\n\nSince the text space of LLMs is discrete, to minimize the disparity between the output of a diffusion\nmodel and the input of LLMs, we leverage Unidiffuser (Bao et al., 2023b), a unified diffusion model\ncapable of transforming images into the discrete text space.\nDuring the training process, UniDiffuser injects noise ϵx and ϵy to a set of paired image-text data\n(x0, y0) and generates noisy data xtx and yty , where 0 ⩽ tx, ty ⩽ T represent two individual\ntimesteps (perturbation levels). It then trains a joint noise prediction network ϵθ(xtx , yty , tx, ty) to\npredict the noise ϵx and ϵy by minimizing the mean squared error loss:\n\nEϵx,ϵy,x0,y0 [∥[ϵx, ϵy] − ϵθ(xtx , yty , tx, ty)∥2],\n\n(5)\n\nwhere the output of ϵθ is the concatenation of the estimated noise ϵx\nBy predicting ϵθ(xtx , yty , tx, ty) for any tx and ty, UniDiffuser learns all distributions related to\n(x0, y0) simultaneously. This includes all conditional distributions: q(x0|y0) for text-to-image gen-\neration, q(y0|x0) for image-to-text generation, and those conditioned on noisy input, i.e., q(x0|yty )\n\nθ, i.e., ϵθ = [ϵx\n\nθ and ϵy\n\nθ , ϵy\nθ].\n\n3\n\nLever Tillman Fromlet Fromlet Fromlet …S  ‘It’s an ‘It’s an ‘It’s an ‘It’s an ‘It’s an interestingLeveridgee standing in the middle of the middle of …two people standing on top of snowy mountain with a ski poletwo people standing on top of snowy mountain with a ski poleXTXTXtXtX0X0<textual noise>Image-to-TextText-to-ImageBiDiﬀuser (ours)UniDiﬀuser Backbone<visual noise>\fUnder review as a conference paper at ICLR 2024\n\n(a) Pre-Align manner.\n\n(b) Mid-Align manner.\n\nFigure 4: Two different ways of aligning BiDiffuser with LLMs.\n\nand q(y0|xtx), for 0 < tx, ty ≤ T . Learning a conditional distribution q(x0|yty ) or q(y0|xtx ) can\nbe seen as learning a distinct task. From a multitask learning perspective, due to limited network\ncapacity, learning many tasks simultaneously (i.e., fitting all distributions to a single network) may\nresult in task competition or task conflict, ultimately leading to suboptimal performance in particular\ntasks such as q(x0|y0) and q(y0|x0).\n\nTo resolve this issue and enhance the performance of both image-to-text and text-to-image generation\ntasks, we finetune UniDiffuser with exclusive emphasis on the two tasks:\n\nL = Eϵx,ϵy,x0,y0[∥ϵx − ϵx\n\nθ (xtx , y0, tx, 0)∥2 + α∥ϵy − ϵy\n\nθ(x0, yty , 0, ty)∥2],\n\n(6)\n\nwhere α is a hyperparameter to balance the learning paces of the two tasks. As depicted in Figure 3,\nour training objective entails predicting the text y0 based on the input image x0 and vice versa, where\nthe input conditions for the model are noise-free. We employ classifier-free guidance. During training,\nwe estimate the noise injected to the image (i.e., ϵx\nθ (xtx, y0, tx, 0)) conditioned on the noise-free text\ny0 and the noise to the text (i.e., ϵy\nθ(x0, yty , 0, ty)) given the noise-free image x0. During inference,\nwith a guidance scale s ⩾ 0, we use the modified prediction ˆϵθ to guide towards the condition:\n\nˆϵx\nθ (xtx , y0, tx, 0) = ϵx\nθ(x0, yty , 0, ty) = ϵy\nˆϵy\nθ (xtx, ϵy, tx, T ) ( ty = T and yT = ϵy) and ϵy\n\nθ (xtx, ϵy, tx, T ) + s · (ϵx\nθ(ϵx, yty , T, ty) + s · (ϵy\n\nθ (xtx , y0, tx, 0) − ϵx\nθ(x0, yty , 0, ty) − ϵy\n\nθ (xtx , ϵy, tx, T )),\nθ(ϵx, yty , T, ty)),\n\n(7)\n\nwhere ϵx\nθ(ϵx, yty , T, ty) (tx = T and xT = ϵx)\nrepresent the unconditional models when T is sufficiently large. We name the finetuned model\n“BiDiffuser”, signifying its specialized ability in bidirectional conditional generation.\n\n3.2\n\nIMAGE-TO-TEXT GENERATION\n\nBiDiffuser can convert images into vectors in the text space, facilitating alignment with the vector\nspace of LLMs. In the following, we show how BiDiffuser can be integrated with LLMs to perform\nimage-to-text generation tasks such as image captioning and visual question answering (VQA).\n\n3.2.1 ALIGNING BIDIFFUSER WITH LLMS\n\nWe connect BiDiffuser and LLMs via a simple projection layer, which maps text embeddings obtained\nfrom the output of the diffusion model to the embedding space of LLMs. As shown in Figure 4, the\nalignment can take place either prior to the LLM (referred to as Pre-Align manner) or between its\nencoder and decoder components (referred to as Mid-Align manner).\n\nPre-Align Manner As shown in Figure 4a, the projection layer is placed before the LLM to map\nthe output of BiDiffuser (image representations) to the text embedding space of the LLM. The text\nembedding of the input image is then concatenated with the embeddings of the textual instructions\nand fed to the LLM for decoding. To synchronize the text space of BiDiffuser with that of the LLM,\n\n4\n\nTwo people standing on the top of a snowy mountain holding on to a ski pole.Decode<Textual Noise><Image>BiDiﬀuserLLMProjection LayerText EmbeddingText EmbeddingText Embedding### Human: <Query> ### Assistant: Describe the image concisely.Two people standing on the top of a snowy mountain holding on to a ski pole.Decode<Textual Noise><Image>BiDiﬀuserLLM DecoderProjection LayerText EmbeddingText EmbeddingText Embedding### Human: LLM EncoderLLM Encoder<Query> ### Assistant: Describe the image concisely.\fUnder review as a conference paper at ICLR 2024\n\nwe propose to use the image-grounded text generation (ITG) objective to drive the model to generate\ntexts based on the input image by computing the auto-regressive loss:\n\nLITG = −\n\n1\nL\n\nL\n(cid:88)\n\nl=1\n\nlog pθ(wg\n\nl |wg\n\n<l, I, TI ),\n\n(8)\n\nwhere wg = (wg\nL) represents the ground-truth caption of image I with length L, TI is the text\ninstruction, and θ denotes the model parameters, which include the parameters of the projection layer\nand the LLM.\n\n1, ..., wg\n\nMid-Align Manner As shown in Figure 4b, the projection layer is placed between the LLM’s\nencoder and decoder, aiming to map the output of BiDiffuser to the embedding space of the text that\nis encoded by the LLM’s encoder. Particularly, we argue that the output of BiDiffuser, once mapped\nby the projection layer and denoted as ddiff , should align with the image caption that is encoded by\nthe LLM’s encoder, denoted as dllm. Therefore, to accurately learn the alignment between the image\nand text representations, in addition to the ITG loss in Eq. 8, we also employ an image-text distance\nminimization (ITDM) loss:\n\nLITDM =\n\n1\nN\n\nN\n(cid:88)\n\ni=1\n\n∥ddiff − dllm∥2\n2,\n\nLmid = LITG + LITM.\n\n(9)\n\nwhere N is the batch size, and Lmid is the overall loss. In this manner, the model parameters θ only\ninclude the parameters of the projection layer.\n\nAfter aligning BiDiffuser with LLMs, EasyGen gains the capability of zero-shot image-to-text\ngeneration, which includes tasks such as image captioning and VQA.\n\n3.2.2\n\nINSTRUCTION-TUNING LLMS TO PROCESS MULTIMODAL TASKS\n\nBefore aligning BiDiffuser with an LLM, we perform instruction-tuning on the LLM to equip it with\nthe capability of understanding multimodal tasks. We construct the instruction data as follows. With\nreference to fastchat*, we designed different forms of instructions for different LLMs:\n\nFlanT5: ###Human: <Img><image></Img> + <random[query]>. ###Assistant: <answer>.\nVicuna: USER: <Img><image></Img> + <random[query]>. Assistant: <answer>.\n\nFor the <image> placeholder, we substitute it with one of the captions associated with the image.\nNote that an image can have multiple captions that convey a similar meaning. For each image, we\nrandomly choose one of its captions, which is then fixed to be used specifically for the <answer>\nplaceholder. As for <random[query]>, we randomly select a query from a predefined set of text\nqueries that prompt the description of the given image as outlined in Table 8.\n\nTo avoid overfitting to the captioning task and counter the model’s inclination to generate excessively\nshort outputs, we have devised specific instructions (blue texts in Table 8), which enable the LLM to\nproduce concise responses when necessary. Furthermore, we incorporate an additional 80K instances\nof multimodal instruction data from LLaVA (Liu et al., 2023), which helps to preserve the LLM’s\ncapability to generate comprehensive and detailed responses.\n\nMoreover, to equip the LLM with the capability to comprehend various multimodal tasks, we curate\ndistinct instruction templates for different tasks, as outlined in Appendix F.\n\n3.3 TEXT-TO-IMAGE RESPONSE GENERATION\n\nMost of existing multimodal models, including the BLIP series (Li et al., 2022), LLaVa (Liu et al.,\n2023), and MiniGPT4 (Zhu et al., 2023), are unable to provide a multimodal response as they are\nprimarily designed to generate only textual outputs. On the other hand, Emu (Sun et al., 2023) takes\na unified approach to predict the subsequent visual or textual token in an auto-regressive manner,\nbut it is heavily reliant on vast quantities of training data. Contrary to the limitations of these\nexisting models, EasyGen, by leveraging the bidirectional generation capability of BiDiffuser and the\ninference capability of LLMs, can produce accurate and high-quality visual response with ease.\n\n*https://github.com/lm-sys/FastChat/tree/main\n\n5\n\n\fUnder review as a conference paper at ICLR 2024\n\nTo tackle multimodal response generation tasks such as PhotoChat (Zang et al., 2021), we adopt the\napproach used in Divter (Sun et al., 2021) (note that Divter cannot encode and process visual images).\nFirst, we finetune the LLM to generate detailed image captions based on dialogue context. Then, we\nemploy BiDiffuser to create the corresponding images with the produced captions. Specifically, we\nreplace the image featured in the dialogue with its corresponding descriptive caption, encapsulating it\nwith task-specific tokens <Img>,</Img> and constructing the following instruction templates:\n\nUSER: Dialog history + <Img><image></Img> + Dialog history. Assistant: <response>.\nUSER: Dialog history. Assistant: <response> + <Img><image></Img>.\n\nNote that when <image> appear in the response,\nit represents the generated description of the im-\nage. Training with the instruction data enables\nour model to not only produce text responses\nbut also perform image intent classification and\ngenerate image captions that BiDiffuser can in-\nterpret.\n\n4 EXPERIMENTS\n\n4.1 EXPERIMENTAL SETUP\n\nWe initialize the encoder-decoder LLM using\npre-trained weights from FlanT5XL or decoder-\nonly LLM from Vicuna-7B, along with the uti-\nlization of the diffusion module from BiDiffuser.\n\nFigure 5: Text-to-image generation by EasyGen.\n(Bottom) LLM generates response and description\nof the image. (Top) BiDiffuser takes the descrip-\ntion as input and generates images.\n\nDuring the alignment process, we maintain the\nfrozen state of the BiDiffuser. The statistics of the datasets for pre-training, alignment and instruction-\ntuning can be found in Appendix A. For the image captioning task, EasyGen is evaluated on both\nthe MS-COCO (Lin et al., 2014) Karpathy test set and the NoCaps (Agrawal et al., 2019) validation\nset. For the VQA task, our method is evaluated on OK-VQA (Marino et al., 2019) validation set and\nGQA (Hudson & Manning, 2019) test-dev set.\n\nTo adapt the model for multimodal dialogue generation, we fine-tune the LLM and projection layer on\nthe PhotoChat dataset. We incorporate photo-sharing activities into the dialogue context by generating\n<Img><caption></Img>, and utilize cross-entropy loss exclusively for fine-tuning the multimodal\ngeneration task. Given the limited expressiveness of image descriptions in the PhotoChat dataset, as\nevidenced by Table 6’s ground truth descriptions, we regenerate image annotations in a text format\nsimilar to that used in MS-COCO.\n\n4.2 EVALUATION\n\nWe evaluate EasyGen on various vision-language tasks including image captioning (MS-COCO (Lin\net al., 2014), NoCaps (Agrawal et al., 2019)), visual question answering (OK-VQA (Marino et al.,\n2019), GQA (Hudson & Manning, 2019)), and multimodal dialog generation (PhotoChat (Zang\net al., 2021)). We use BLIP (Li et al., 2022), Flamingo (Alayrac et al., 2022), BLIP-2 (Li et al.,\n2023b), InstructBlip (Dai et al., 2023), MiniGPT-4 (Zhu et al., 2023), and LLaVA (Liu et al., 2023)\nas baselines for image-to-text tasks, and Maria (Liang et al., 2021) and Divter (Sun et al., 2021) as\nbaselines for the multimodal response generation task. See details in Appendix B and Appendix C.\n\n4.3 OVERALL RESULTS\n\nTable 1 lists the automatic and ChatGPT evaluation results for each baseline and our models on\nMS-COCO and VQA datasets. EasyGen outperforms most of the baseline models on both the COCO\ntest set and NoCaps validation set (zero-shot transfer). Although EasyGen is only pre-trained on a\nsmall dataset MS-COCO, its performance on the image captioning generation task is comparable\nto models (e.g., BLIP-2) pre-trained on a large dataset. This indicates that EasyGen can effectively\ncombine the strength of diffusion module and LLM to generate smooth and informative captions. GPT\nscores do not vary significantly because the captions produced by the models in the image-captioning\n\n6\n\nA: We went out to go to a garage sale this morning. B: \u0000at’s awesome! Do you play guitar?<img> An acoustic guitar with a headstock on top of a brick wall. </img>AutoKL Decoder<Dialogue Context>Yeah, take a look. <Description>LLMBiDiﬀuserText-to-Text Generation\fUnder review as a conference paper at ICLR 2024\n\nModel\n\nBLIP (Li et al., 2022)\nFlamingo (Alayrac et al., 2022)\nBLIP-2 OPT-6.7B (Li et al., 2023b)\nBLIP-2 FlanT5XL (Li et al., 2023b)\nInstructBlip 7B (Dai et al., 2023)\nMiniGPT-4 (Zhu et al., 2023)\nLLaVA (Liu et al., 2023)\n\nEasyGen FlanT5XL\nEasyGen Vicuna-7B\n\nDataset\nSize\n\n129M\n1.8B\n129M\n129M\n16M\n5M\n753K\n\n173K\n173K\n\nNoCaps (val)\n\nCOCO (Karpathy)\n\nOK-VQA\n\nGQA\n\nCIDEr\n\nSPICE BLEU@4 CIDEr GPT Accuracy Accuracy\n\n113.2\n-\n121.0\n121.6\n123.1\n42.4\n33.1\n\n121.2\n121.8\n\n14.8\n-\n15.3\n15.8\n-\n-\n-\n\n15.5\n15.8\n\n40.4\n-\n43.5\n42.4\n40.8\n-\n7.9\n\n43.5\n42.4\n\n136.7\n138.1\n145.2\n144.5\n140.7\n-\n30.0\n\n145.7\n144.6\n\n-\n-\n8.4\n8.3\n-\n-\n8.6\n\n8.6\n8.7\n\n-\n50.6\n36.4\n39.4\n61.0⋆\n37.5\n54.4\n\n41.1\n45.2\n\n-\n-\n36.4\n44.4\n49.2⋆\n30.8\n41.3\n\n37.2\n44.6\n\nTable 1: Automatic evaluation and GPT evaluation of our model and the baselines on various vision-\nlanguage tasks. The results of EasyGen on NoCaps, OK-VQA and GQA are obtained in a zero-shot\nsetting. ⋆ indicates that the model was trained on other VQA datasets.\n\nModel\n\nResponse Generation\n\nDescription Generation\n\nImage\n\nBLEU-1 BLEU-2\n\nPPL↓ ROUGE-L BLEU-1/2 ROUGE-L PPL↓\n\nFID↓\n\nDivter (Sun et al., 2021)\nMaria (Liang et al., 2021)\n\nEasyGen FlanT5XL\n+ w/ generated desc.\n+ w/o generated desc.\n\n6.5\n13.8\n\n22.3\n17.8\n\n1.7\n9.2\n\n18.7\n12.4\n\n59.63\n48.75\n\n4.32\n7.61\n\n5.69\n15.17\n\n17.24\n15.12\n\n15.1/11.4\n-\n\n13.5/10.2\n17.4/13.2\n\n15.81\n-\n\n13.84\n16.71\n\n5.12\n-\n\n4.16\n6.23\n\n29.16\n-\n\n10.30\n75.46\n\nTable 2: Automatic evaluation of our model and the baselines on the PhotoChat dataset.\n\ntask tend to be quite alike. For the OK-VQA and GQA dataset, the performance of EasyGen is\nimproved compared with other models of a similar scale. For example, BLIP-2 adopts the task-special\ndecoding method and achieves 39.4% accuracy on OK-VQA validation set, while ours can get 45.2%\neven with a simple decoding method, i.e., greedy search.\n\nTable 2 lists the automatic evaluation results on the PhotoChat dataset. The results of Divter are cited\nfrom (Sun et al., 2021). We fine-tune Maria on PhotoChat dataset only for the response generation\ntask. Since our EasyGen model can generate response and image description simultaneously, the\nresponse and description generation task has a similar PPL. Compared with other models, our\nmethod has clear advantages in the performance of PPL, indicating that by leveraging LLM, our\nmodel demonstrates strong performance on text generation tasks. Besides, we find that the image\ndescriptions in the PhotoChat dataset are too concise to adequately convey the information of images.\nTherefore, we used the pre-trained model from the first stage to regenerate the image description\n(referred to as “w/ generated desc.” in Table 2) which led to a large gap towards ground-truth\ndescriptions, resulting in lower BLEU-1/2 and ROUGE-L. However, the performance of our model\non BELU-1/2 and ROUGE is higher than other models on response generation tasks, indicating\nthat introducing richer image descriptions is beneficial for generating more relevant and informative\nresponses. We also provide some examples (Figure 6) to show the effectiveness of our method.\n\n4.4 ABLATION STUDY\n\nIn Table 3, we investigate the impact of different training strategies on the model. After removing the\nITDM loss, the performance of EasyGen is slightly weaker than the original model. It is evident that\nthe MSE Loss can help to align the semantic spaces of the two models. Furthermore, the performance\nof the model will drop significantly after removing the cross-entropy loss, suggesting that constraints\nvia the language model play a key role. Without the instruction tuning process on LLM, EasyGen has\na significant decline in the performance of automatic evaluations, which indicates that prior tuning of\nthe LLM to an accurate caption generation model is necessary.\n\nIn Table 4, we examine the impact of freezing/tuning BiDiffuser and the LLM. We conducted ablation\nstudies on image captioning and VQA tasks. It can be observed that the frozen Mid-Align method\noutperforms the Pre-Align method in image captioning. This shows that the ITDM loss function is\neffective. However, the frozen Mid-Align method exhibits inferior performance in the VQA task.\n\n7\n\n\fUnder review as a conference paper at ICLR 2024\n\nModel\n\nNoCaps (val)\n\nCOCO (Karpathy)\n\nOK-VQA\n\nGQA\n\nCIDEr\n\nSPICE SPICE BLEU@4 CIDEr Accuracy Accuracy\n\nEasyGen Mid-Align FlanT5XL\n+ w/o ITDM\n+ w/o ITG\n+ w/o LLM pre-tuning\nEasyGen Vicuna-7B\n+ w/o LLM pre-tuning\n\n121.2\n118.6\n93.2\n110.8\n121.8\n107.3\n\n15.5\n15.3\n12.9\n14.5\n15.3\n14.3\n\n25.1\n24.8\n23.0\n24.4\n24.9\n24.2\n\n43.5\n42.2\n35.1\n40.7\n42.4\n40.1\n\n145.7\n141.5\n127.6\n139.6\n144.6\n137.5\n\n31.5\n-\n-\n25.8\n45.2\n44.1\n\n22.6\n-\n-\n18.1\n44.6\n41.2\n\nTable 3: Ablation studies on the instruction-tuning process and loss functions.\n\nLLM\n\nDiffusion\nModel\n\nAlignment\n\nNoCaps\n\nCOCO(Karpathy)\n\nOK-VQA\n\nCIDEr\n\nSPICE\n\nBLEU@4\n\nCIDEr\n\nAccuracy\n\nT5\nT5\nT5\nT5\n\nUniDiffuser\nBiDiffuser\nBiDiffuser\nBiDiffuser\n\nPre-Align\nPre-Align\nMid-Align\nMid-Align\n\nVicuna-7B\nVicuna-7B\n\nBiDiffuser\nBiDiffuser\n\nPre-Align\nPre-Align\n\n62.4\n119.1\n121.2\n121.5\n\n121.8\n119.0\n\n18.0\n25.5\n25.1\n25.3\n\n24.9\n24.6\n\n26.8\n42.6\n43.5\n43.6\n\n42.4\n40.3\n\n90.7\n145.1\n145.7\n145.7\n\n144.6\n140.3\n\n33.0\n41.1\n31.5\n36.4\n\n45.2\n42.7\n\nTable 4: Ablation studied on image captioning and VQA tasks.\nweights of the LLM during the alignment process.\n\n/\n\nrepresents we tune/freeze the\n\nWe hypothesize that this is due to the integration of mid-aligned target image features with query\ninformation, and the projection layer is insensitive to instruction information. We conduct instruction-\ntuning on Pre-Align T5 and Vicuna. Compared to models at the same scale, these instruction-tuned\nmodels achieve superior results. The results clearly demonstrate that the instruction tuned models\noutperformed other models significantly on the OK-VQA and GQA datasets.\n\n4.5 FINE-TUNING EASYGEN FOR VQA TASKS\n\nConsidering the substantial cost involved in fine-tuning the diffusion model for VQA tasks, we opt to\nconcatenate the output of BiDiffuser with the image encoded by image CLIP ViT-L/14 and fine-tune\nthe parameters of the LLM and projection layers. We fine-tune the EasyGen on the training and\nvalidation splits from VQAv2, Text Captions, AOK-VQA and TextVQA datasets.\n\nIn order to verify the effectiveness of BiDiffuser, we also add this module to LLaVA Vicuna-7B and\nuse the same mixture dataset to do instruction tuning. The details of training dataset can be found in\nTable 9. Since BiDiffuser can map images into text vectors, BiDiffuser can be directly migrated to the\nLLaVA Vicuan-7B model. We keep LLaVA and our model using the same instruction tuning datasets.\nNoting that LLaVA has used 595K pretraining data from CC-3M dataset (Sharma et al., 2018) to\nalign CLIP and LLM. Our model does not pre-align CLIP and LLM, and only uses instruction-tuning\ndata for training.\n\nModel\n\nVQAv2 (test-dev) MMbench (dev)\n\nTextVQA\n\nMiniGPT-4 (Zhu et al., 2023)\nInstructBLIP Vicuna-7B (Dai et al., 2023)\nLLaVA Vicuna-7B (Liu et al., 2023)\nLLaVA Vicuna-7B + BiDiffuser\nEasyGen ViT-L Vicuna-7B\n+ w/o BiDiffuser\n\n-\n-\n77.6\n78.2\n79.4\n71.1\n\n24.3\n36.0\n43.6\n45.7\n45.4\n21.4\n\n19.4\n50.1\n44.1\n46.7\n45.5\n36.2\n\nTable 5: Comparison with state-of-the-art open-ended generation models fine-tuned for visual\nquestion answering and benchmarks.\n\n8\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 6: Examples of the generated responses on PhotoChat dataset. The text highlighted in red\nindicates the objects present in the image. The turns prefixed with A/B denote the given context.\n\n5 RELATED WORK\n\nMultimodal Language Models. Recent research has witnessed a surge of interest in multi-\nmodal LLMs, including collaborative models such as Visual ChatGPT (Wu et al., 2023a), MM-\nREACT (Yang et al., 2023), and HuggingGPT (Shen et al., 2023), and end-to-end methods including\nFlamingo (Alayrac et al., 2022), Img2LLM (Guo et al., 2022), BLIP series (Li et al., 2023b; Dai\net al., 2023; Li et al., 2022), BEiT series (Bao et al., 2021; Wang et al., 2022b), LLaVA (Liu et al.,\n2023), mPLUG-owl (Ye et al., 2023), MiniGPT-4 (Zhu et al., 2023), Llama-adapter (Zhang et al.,\n2023a), Otter (Li et al., 2023a), OFA (Wang et al., 2022a), and PaLI (Chen et al., 2022). In our works,\nEasyGen is built upon a bidirectional conditional diffusion model, which promotes more efficient\ninteractions between modalities.\n\nMultimodal Diffusion Models. Diffusion generative models (Rombach et al., 2022; Ramesh et al.,\n2021; Nichol et al., 2022; Ruiz et al., 2023) have achieved strong results in text conditioned image\ngeneration works. Specifically, Versatile Diffusion (Xu et al., 2023) employs the U-Net (Ronneberger\net al., 2015) architecture with a multi-flow design to tackle multiple modalities and tasks, while\nUniDiffuser (Bao et al., 2023b) adopts the U-ViT (Bao et al., 2023a) framework to treat both image\nand text as sequential token streams for diffusion calculations. However, these models are unable to\ncomplete complex language tasks. EasyGen combines the advantages of diffusion models and LLMs\nand achieves competitive performance in both image-to-text and text-to-image tasks.\n\nMultimodal Response Generation. Recent works have shown significant progress on multimodal\nresponse generation (Koh et al., 2023b; Aghajanyan et al., 2022; Zhang et al., 2023b; Wu et al.,\n2023b; Pan et al., 2023; Koh et al., 2023a). Divter (Sun et al., 2021) incorporates text-to-image\ngeneration into text-only dialogue response generation to produce a multimodal response. Leveraging\nthe power of diffusion models, CoDi (Tang et al., 2023) can generate any combination of output\nmodalities. Emu (Sun et al., 2023) takes a unified approach to predict the subsequent visual or textual\ntoken in an auto-regressive manne. In EasyGen, we efficiently combine the diffusion model and\nLLMs to generate multimodal outputs.\n\n6 CONCLUSION\n\nWe have introduced EasyGen, a model that facilitates multimodal understanding and generation. In\ncontrast to existing models that rely on encoders like CLIP or ImageBind (Girdhar et al., 2023) and\nrequire significant amounts of training data to integrate different modalities, EasyGen offers a more\nefficient solution by employing a bidirectional diffusion model named BiDiffuser. This allows for\nmore effective modal interactions, handling both image-to-text and text-to-image generations by the\nfusion of BiDiffuser and LLMs. Comprehensive experiments underscores EasyGen’s effectiveness\nand efficiency.\n\n9\n\nA: I just got back from the pet store and you wouldn’t believe what I saw? B: What you saw, can you tell me? A: I saw the cutest golden retriever ever… I couldn’t stop looking at this animal… A: Check it out - I took a picture. <Img>A: What’s onn your mind?              B: Good. \u0000anks. I am thinking to make a cake. A: Sound yummy, what kind? B: I saw a chocolate cake topped with whipped cream. It looks so yummy.  B: I hope you can see the photo. <Img>Objects in the photo: Animal, Golden retriever, Dog, Carnivore.Objects in the photo: Baked goods, Cake, DairyB: Lovely and cute dudeA: \u0000e cake looks beautiful, but should not be hard to make.A golden retriever puppy peeking through a stuﬀed animalA chocolate cake covered with whipped creamB: What a cute golden retriever, I want to play with him.A: Delicious! Do you have a recipe in mind for your chocolate cake with whipped cream topping?Ground TruthGround TruthEasyGen (Ours)EasyGen (Ours)Dialogue ContextImage DescriptionResponseImages\fUnder review as a conference paper at ICLR 2024",
    "abstract": "We present EasyGen, an efficient model designed to enhance multimodal understanding and generation by harnessing the capabilities of diffusion models and large language models (LLMs). Unlike existing multimodal models that predominately depend on encoders like CLIP or ImageBind and need ample amounts of training data to bridge the gap between modalities, EasyGen is built upon a bidirectional conditional diffusion model named BiDiffuser, which promotes more efficient interactions between modalities. EasyGen handles image-to-text generation by integrating BiDiffuser and an LLM via a simple projection layer. Unlike most existing multimodal models that are limited to generating text responses, EasyGen can also facilitate text-to-image generation by leveraging the LLM to create  textual descriptions, which can be interpreted by BiDiffuser to generate appropriate visual response. Extensive quantitative and qualitative experiments demonstrate the effectiveness of EasyGen, whose training can be easily achieved in a lab setting.",
    "response": {
      "Soundness": 2,
      "Presentation": 3,
      "Contribution": 3,
      "Rating": 5,
      "Confidence": 5,
      "Strengths": "- The idea of tuning UniDiffuser as LLM encoders via image-text bidirectional generation is simple and novel, and the results are interesting to me.\n- The targeted problem of multimodal dialogue is relevant and trending, especially in the modern LLM era.\n- The method seems very data-efficient, which is good.",
      "Weaknesses": "My primary concern is the limited applications and evaluations. For multimodal dialogue, when considering long context conditional image generation or image conditional (multi-images or single-image) image generation like image edition/translation (it is, however, what UniDiffuser initially can do), only generating text inputs as diffusion modal conditions is limited. \n\nFor the evaluations, the current results only include traditional short QA or image captioning tasks. However, one of the advantages of LLMs is that they are mighty at world knowledge and long-context reasoning. It would be better if results on modern benchmarks like MM-Vet and MMBench were provided. Also, the most commonly used VQAv2 results are not provided.\n\nOn the other hand, since the method converts images to texts to obtain multimodal understandings, lots of information may be diluted or missing. It will harm the understanding that requires detailed knowledge, such as OCR-related tasks (results on TextVQA are also encouraged).  Besides, results on commonly used text-to-image generation benchmarks are required, such as COCO FID results. \n\nIt is somewhat overclaimed to criticize the large data requirement of CLIP/ImageBind models since UniDiffuser is also trained on large-scale corpora using large amounts of resources that cannot be affordable by most labs.\n\nMissing citations and discussions. The method of first converting images to texts and then feeding them to LLMs is the same as Img2LLM [A], but it is not cited or discussed. Besides, several closely related concurrent works should be discussed [B-F].\n\n[A] From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models.\n[B] DreamLLM: Synergistic Multimodal Comprehension and Creation.\n[C] NExT-GPT: Any-to-Any Multimodal LLM.\n[D] MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens.\n[E] Generating Images with Multimodal Language Models.\n[F] Kosmos-G: Generating Images in Context with Multimodal Large Language Models.",
      "Questions": "- What about COCO captioning results on `test` split?\n- Is is possible if some language capability can be provided? Such as MMLU and HellaSwag benchmarks."
    }
  },
  {
    "paper_id": "2PKZtPMyvI",
    "summary": "Background: Transformers have become a dominant framework in sequential modeling, particularly in NLP, outweighing models like CNNs due to their ability to handle large datasets effectively. They are used in models such as BERT and GPT-3, and various innovations like Reformer and Linformer have been developed to enhance performance by reducing computational complexity.\nGap: Despite the widespread use of transformers and numerous architectural advancements, their underlying operational mechanics remain not well-understood, particularly how embedding dimensions and layer depth affect performance.\nHypothesis: The intrinsic dimension (ID), representing the minimum parameter count for effective modeling, can provide insight into the geometrical characteristics of data representations learned by Transformers, revealing trends in embedding dimensions, layer depth, and task performance.\nMethod: The study uses intrinsic dimension (ID) analysis to explore the geometric characteristics of data representations in Transformers. It conducts several text classification experiments using the TwoNN method to estimate IDs across different layers of Transformer models and examines relationships between ID, embedding dimensions, and classification performance across three datasets: IMDB, AG, and SST2.\nConclusion: The study finds that higher terminal feature IDs in Transformers correlate with lower classification error rates, opposite to CNNs. ID tends to decrease with layer depth, especially in complex architectures. Nonlinear dimension reduction is achieved in Transformers, with varying sequence lengths impacting ID and performance, supporting the efficacy of data reduction in training.\n",
    "full text": "\nAN INTRINSIC DIMENSION PERSPECTIVE OF TRANS-\nFORMERS FOR SEQUENTIAL MODELING\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nTransformers have become immensely popular for sequential modeling, particu-\nlarly in domains like natural language processing (NLP). Recent innovations have\nintroduced various architectures based on the Transformer framework, resulting\nin significant advancements in applications. However, the underlying mechanics\nof these architectures are still somewhat enigmatic. In this study, we explore the\ngeometrical characteristics of data representations learned by Transformers using\na mathematical metric known as intrinsic dimension (ID). This can be conceptual-\nized as the minimum parameter count needed for effective modeling. A sequence\nof experiments, predominantly centered on text classification, support the ensuing\nempirical observations regarding the correlation between embedding dimension,\nlayer depth, individual layer ID, and task performance. Interestingly, we note that\na higher terminal feature ID, when obtained from Transformers, generally corre-\nlates with a lower classification error rate. This stands in contrast to the behavior\nobserved in CNNs (and other models) during image classification tasks. Further-\nmore, our data suggests that the ID for each layer tends to diminish as layer depth\nincreases, with this decline being notably steeper in more intricate architectures.\nWe also present numerical evidence highlighting the geometrical constructs of\ndata representations as interpreted by Transformers, indicating that only nonlinear\ndimension reduction is achievable. Lastly, we delve into how varying sequence\nlengths impact both ID and task performance, confirming the efficacy of data re-\nduction during training. Our ambition is for these insights to offer direction in the\nchoice of hyper-parameters and the application of dimension/data reduction when\nusing Transformers for text classification and other prevalent NLP tasks.\n\n1\n\nINTRODUCTION\n\nTransformers, as introduced by (Vaswani et al., 2017), have revolutionized numerous machine learn-\ning disciplines. They’ve notably driven breakthroughs in both natural language processing (NLP)\nand computer vision (CV). The Transformer architecture excels at managing vast datasets, predom-\ninantly when equipped with an ample number of parameters. Prominent models like BERT (Devlin\net al. (2018)), GPT-3 (Brown et al. (2020)), and BART (Lewis et al. (2019)) exemplify its prowess.\nImpressively, given sufficient training data, Transformers often surpass rivals like CNNs in perfor-\nmance (Dosovitskiy et al. (2020)).\n\nAs researchers continue to unlock the potential of Transformers, a plethora of variations have sur-\nfaced. The Reformer (Kitaev et al. (2020)), for instance, mitigates the computational complex-\nity from O(L2) to O(L log L) using locality-sensitive hashing, where L represents the sequence\nlength. The Sparse Transformer (Child et al. (2019)) employs sparse factorizations, achieving a\nsimilar reduction in memory overhead. Linformer (Wang et al. (2020)) adopts low-rank matrices\nto approximate the self-attention mechanism, decreasing both computation and memory costs from\nO(L2) to O(L log L).\n\nFurthermore, DRformer (Tang & Huang (2022)) is adept at recognizing both regional and attribute\nrelations within the pedestrian attribute recognition domain. The Tightly-coupled convolutional\ntransformer (Shen & Wang (2022)) enhances the granularity of information retrieval in time series\nforecasting. ACORT (Tan et al. (2022)) streamlines the Transformer’s footprint for image captioning\n\n1\n\n\fUnder review as a conference paper at ICLR 2024\n\ntasks. Lastly, Huang et al. (2022) has designed a Transformer variant tailored for news recommen-\ndation.\n\nWhile there has been remarkable progress in architecture development, the foundational mechanics\nbehind Transformers remain elusive. Training a Transformer is notoriously challenging, and we’ve\nyet to fully understand its inner workings, especially regarding how performance varies with in-\ncreasing embedding dimension and depth. Yet, understanding these nuances is vital, as there’s a\ntrend towards designing bigger, deeper Transformers with enhanced training techniques to bolster\nperformance. Some recent studies have begun to shed light on this. Xiong et al. (2020) and Popel &\nBojar (2018) explored the impact of hyper-parameter tuning on Transformer training. Huang et al.\n(2020) examined the complexities of optimizing Transformer models and introduced a novel initial-\nization approach for training deeper iterations. Efforts to train expansive Transformer models were\nalso seen in Wang et al. (2019), with Wang et al. (2022) successfully training a Transformer boasting\na depth of 1000 layers. This paper primarily endeavors to demystify the Transformer’s operational\nintricacies by focusing on its intrinsic dimension (ID) representation.\n\nIt’s broadly acknowledged that real-world data, encompassing sounds, texts, images, and the like,\ninherently exhibit low-dimensional structures. This implies that only a subset of dimensions is\nnecessary to encapsulate sampled data and inherent relationships.\n\nConsider, for instance, the dataset comprising all 224×224×3 RGB images labeled as ’dogs’. While\nthere exist 224 × 224 × 256 × 3 = 38535168 possible configurations, the actual, ”intrinsic” count\nof distinct dog images perceivable by humans is substantially fewer, with many bearing notable\nresemblances. Numerous deep learning strategies, such as Hinton & Salakhutdinov (2006) and\nGonzalez & Balajewicz (2018), leverage the prevalence of these low-dimensional data constructs.\n\nThe concept of intrinsic dimension (ID), as described in Amsaleg et al. (2015), Houle et al. (2012),\nCutler (1993), and Xie et al. (2009), serves as a critical mathematical instrument to fathom the ge-\nometric essence of data. It signifies the minimal parameter set necessary to model specific truths,\nideally pinpointing the low-dimensional data architectures. Our study mainly probes the data repre-\nsentation by Transformers, striving to highlight unique phenomena through the lens of ID.\n\nOur work’s salient contributions are fourfold:\n\n• We examine the ID fluctuation of data representations as deciphered by sequential Trans-\nformer blocks, identifying a prominent dimension reduction trend across layers. This trend\nis accentuated with deeper architectures.\n\n• We illuminate the geometric nuances of Transformers for sequential modeling. Notably, it\nappears Transformers primarily achieve nonlinear dimension reduction in text classification\ntasks.\n\n• We delve into the interplay between embedding dimension (ED), the intrinsic dimension\n(ID) of the deduced representation, and task performance. This provides a clear rationale\nbehind the advantages of expanding ED from an ID viewpoint.\n\n• We assess the implications of truncating training datasets through sequence lengths, noting\nminimal impact on both IDs and performance metrics. This insight might offer pragmatic\nguidance for refining computational efficiency in real-world scenarios.\n\n2 RELATED WORK\n\nTwoNN Method There has been extensive research on estimating the intrinsic dimension (ID)\nof provided datasets. For instance, methods based on Principal Component Analysis (PCA) can\nbe found in Fukunaga & Olsen (1971) and Bruske & Sommer (1998). The maximum likelihood\nestimation (MLE) based approach is described in Levina & Bickel (2004). Costa & Hero (2004)\nleveraged the geodesic-minimal-spanning-tree for ID estimation, and K´egl (2002) employed the\ncapacity dimension for the same purpose. An innovative estimation method, named TwoNN, is\npresented in Facco et al. (2017). This method uses the proximity of the nearest and the second\nnearest samples to shape probability distributions. Due to its computational efficiency, we have\n\n2\n\n\fUnder review as a conference paper at ICLR 2024\n\nchosen the TwoNN method for estimating the intrinsic dimensions of the representations decoded\nby Transformers.1\nFormally, the TwoNN method operates as follows. Given the dataset D = {xi}N\ni=1, for each data\npoint xi, let the distances to its closest and second closest samples be denoted by si,1 and si,2,\nrespectively. The intrinsic dimension ID is estimated by analyzing the ratio:\n\nsi :=\n\nsi,2\nsi,1\n\n.\n\nNotably, the ratio si := si,2\nsi,1\net al. (2018) and Rootz´en & Tajvidi (2006). This gives rise to the equation:\n\ncan be characterized by a Pareto distribution, as detailed in Hussain\n\nP ((s1, s2, ..., sN ) | d) = dN\n\nN\n(cid:89)\n\ni=1\n\ns−(d+1)\ni\n\n.\n\nThe intrinsic dimension of the dataset D, denoted by d, can then be efficiently determined using a\nstandard maximum likelihood technique. For clarity, a step-by-step outline of the TwoNN method\nis provided below:\n\n• Randomly select N data points, forming D = {xi}N\n• For each xi in D, compute the distances to the closest and second closest data points,\n\ni=1.\n\nlabeled as si,1 and si,2.\n\n• For every xi in D, determine the ratio si := si,2\nsi,1\n• Estimate d from\n\n.\n\nP ((s1, s2, ..., sN ) | d) = dN\n\nN\n(cid:89)\n\ni=1\n\ns−(d+1)\ni\n\nusing the maximum likelihood technique. This d serves as the ID estimate.\n\nIntrinsic Dimension in Deep Learning There has been recent interest in examining the role of\nintrinsic dimension within deep learning frameworks. For instance, Pope et al. (2021) employed\nGANs to generate synthetic images, allowing them to establish an upper bound for ID, with the goal\nof verifying the precision of ID estimation methodologies. Ansuini et al. (2019) explored variations\nin ID across layers for traditional neural network architectures, including ResNet (He et al. (2016))\nand VGG (Simonyan & Zisserman (2014)), focusing on image classification tasks. Aghajanyan\net al. (2020) delved into the impact of pre-training on the intrinsic dimension in the context of NLP\ntasks. However, when juxtaposed with CNNs and other models tailored for computer vision, there is\na paucity of research concerning sequential models designed for NLP, such as Transformers, despite\ntheir widespread acclaim. This study endeavors to bridge this research void.\n\n3 RESULTS\n\nWe commence by detailing our experimental setup and subsequently present our findings across five\ndimensions.\n\n3.1 EXPERIMENTAL SETTINGS\n\nTasks\nIn order to conveniently examine the intrinsic dimension (ID) of Transformers, we have\nchosen to focus our numerical experiments on the text classification task. Our rationale is three-\nfold. Firstly, text classification stands as a foundational yet crucial task within the realm of natural\nlanguage processing. Its utility spans a broad array of applications, including but not limited to,\nspam detection (Crawford et al. (2015), Asghar et al. (2020)), text style categorization (Wu et al.\n(2019), Sudhakar et al. (2019)), and sentiment analysis (Medhat et al. (2014), Xu et al. (2019)).\nSecondly, numerous studies, such as Devlin et al. (2018), Wang et al. (2020), and Shaheen et al.\n(2020), have underscored the effectiveness of the Transformer architecture in classification tasks.\n\n1Refer to https://github.com/ansuini/IntrinsicDimDeep for relevant code.\n\n3\n\n\fUnder review as a conference paper at ICLR 2024\n\nLastly, for text classification purposes, the Transformer typically employs a sequence of encoder\nblocks, augmented with a supplementary multi-layer perceptron (MLP) (Rosenblatt (1961)) block\nfor classification. This streamlined approach facilitates the ease of ID analysis. Conversely, for\ntasks necessitating decoders, such as text generation, dissecting ID on a layer-by-layer basis can\npose challenges.\n\nDatasets We conducted our experiments on three distinct datasets: IMDB (Maas et al. (2011)),\nAG (Zhang et al. (2015)), and SST2 (Socher et al. (2013)). Specifically, IMDB (Maas et al. (2011))\nis a binary classification dataset for movie reviews, comprising 25,000 training entries and 25,000\ntest entries. The AG dataset (Zhang et al. (2015)) consists of news articles categorized into four\nclasses, with 120,000 training entries and 7,600 test entries. Meanwhile, SST2 is another binary\nmovie review classification dataset, housing approximately 7,000 training entries and 2,000 test\nentries.\n\nModels We employ the standard Transformer model (Vaswani et al. (2017)), leveraging successive\nencoder blocks to extract features and represent the entirety of the input texts. For the final classi-\nfication layer, in line with common practices and for the sake of simplicity, we utilize standalone\nmulti-layer perceptrons (MLPs) (Rosenblatt (1961)). These maintain a significant influence on the\nTransformer’s (encoders) overall performance.\n\nObjectives and Relevant Hyper-parameters This study seeks to understand and elucidate the\ncorrelation between the intrinsic dimensions of representations derived by Transformers and the\nensuing classification outcomes. In striving for a thorough and robust experimental approach, we\nmodulated the following hyper-parameters:\n\n• D: Depth, representing the total layer count within the Transformer.\n\n• ED: Embedding (Mikolov et al. (2013)) dimension. For clarity and in adherence to es-\ntablished conventions (as illustrated by Vaswani et al. (2017)), we maintained a consistent\ndimension across all hidden layers.\n\nEssentially, our aim is to discern their impacts on both the ID and classification accuracy.\n\n(a) AG\n\n(b) IMDB\n\n(c) SST2\n\nFigure 1: Layer-wise Variation of Intrinsic Dimensions across Different Depths and Embedding\nSizes\n\n3.2 LAYER-WISE VARIATION OF INTRINSIC DIMENSIONS\n\nWe commence by examining the ID variation across different layers. In a Transformer model of\ndepth D and hidden dimensions {nl}D\nl=1, every piece of input data is sequentially mapped into a nl-\ndimensional vector space for l = 1, 2, · · · , D. Despite the defined hidden dimensions, these cannot\ntruly depict the underlying geometric structures of the data. Thus, we employ the TwoNN (Facco\net al. (2017)) method to ascertain the ID for each layer.\n\nBuilding upon the settings delineated in Section 3.1, our experiments span three datasets (AG,\nIMDB, and SST2) using the Transformer model with various depths and embedding dimensions:\nD ∈ {4, 6} and ED ∈ {128, 256, 512}. The intrinsic dimension and its layer-wise changes are\nillustrated in Figure 1.\n\n4\n\n\fUnder review as a conference paper at ICLR 2024\n\nIn Figure 1, the horizontal axis indicates each layer, with emb symbolizing the embedding layer and\nlayer i referring to the i-th encoder layer. The vertical axis quantifies the corresponding ID, and each\nline in Figure 1 charts the ID’s layer-wise variations for a specific hyper-parameters setup.\n\nKey observations from Figure 1 include:\n\n• As depth increases, the overall intrinsic dimension generally displays a declining trend.\nTypically, the ID may surge at the first encoder layer (refer to Figure 1 (c)), yet it diminishes\nin subsequent layers, bottoming out at the final layer. This decrement in ID across layers\ncan be perceived as a form of dimension reduction in Transformers, corresponding with the\ndistillation of pertinent information for the concluding classification.\n\n• Keeping the model’s depth constant for a given dataset reveals that the intrinsic dimension\ntends to escalate with the embedding dimension. As corroborated by Figure 1, this ob-\nservation persists across all depths and datasets. This trend is intuitive; as the embedding\ndimension grows, the starting intrinsic dimension generally follows suit, culminating in a\nhigher ending ID.\n\nTo delve deeper into this dimension reduction phenomenon, we can inspect the variation in the\nratios between the intrinsic dimension and both the embedding and hidden dimensions. Given that\nthe latter two were predefined as equivalent, this analysis becomes straightforward. Notably, the\nratio starts around O(10−1) at the embedding layer (roughly between 0.1 and 0.3), but significantly\ndrops to a value near O(10−2) by the final layer.\n\n3.3 RELATIONSHIP BETWEEN INTRINSIC DIMENSIONS AND CLASSIFICATION ACCURACY\n\n(a) AG\n\n(b) IMDB\n\n(c) SST2\n\nFigure 2: Relationship Between Terminal Intrinsic Dimension and Classification Error Rate Across\nDatasets\n\nGiven that the ID describes the inherent geometric structures of data distribution, and that classifica-\ntion success hinges on the final representation produced by the Transformers’ last hidden layer, one\ncould posit that the terminal ID and predicted classification accuracy are related.\n\nTo further explore this, we examined the relationship between terminal ID and its corresponding\nclassification error rate across different hyper-parameter settings and datasets, as depicted in Figure\n2. For consistency, training was done through multiple independent runs with random initializations\nto ensure convergence. In Figure 2, the x-axis displays the classification error rate, while the y-axis\npresents the ID of the representation derived from the final hidden layer. Dashed lines link outcomes\nfor various depths at a set embedding dimension, with black points indicating average values for\nboth terminal ID and error rates.\n\nA glance at Figure 2 reveals that, despite all tests being conducted within the same hypothesis space\n(namely, Transformers), notable discrepancies exist in terms of classification efficacy and terminal\nID in relation to model size (or embedding dimension in this context). As illustrated by the solid lines\nin Figure 2, the terminal ID generally rises with increasing embedding dimension, but classification\nerror displays the opposite trend. Fascinatingly, the trend seen for Transformers in NLP contrasts\n\n5\n\n\fUnder review as a conference paper at ICLR 2024\n\nsharply with findings in Ansuini et al. (2019), where the terminal ID for CNNs directly correlates\nwith the classification error rate for image processing.\n\nThe rationale behind this can be understood as follows. As the embedding dimension increases, in\nline with the observations in Section 3.2, there’s an increase in the ID of the input data represen-\ntation, leading to a rise in the terminal ID. Simultaneously, a larger embedding dimension, along\nwith greater hidden dimensions, certainly expands the associated hypothesis space for modeling.\nTherefore, it’s logical to expect an improvement in classification accuracy.\n\nThis observation suggests that boosting the embedding dimension can elevate classification perfor-\nmance by amplifying the intrinsic dimension.\n\nThis insight paves the way for potential practical guidelines in applications: one might conceptual-\nize a methodical approach to leverage the terminal ID as a posterior “indicator” of generalization.\nSpecifically, by tracking the changes in terminal ID throughout training, we might be able to ensure\nsuperior classification outcomes without needing an entirely separate dataset for both validation and\ntesting. This approach could be especially beneficial when dealing with scarce data in real-world\nscenarios, making it a promising avenue for future research.\n\nRemark 1 Our analysis underscores the intrinsic behavior of transformer models in textual tasks.\nHowever, for ViT models, the findings deviate, as evidenced in Table 1. We observe that both the ViT\nembedding dimension and the ViT depth have minimal impact on the ID.\n\nTable 1: The correlations between the ID and classification accuracy of ViT model on CIFAR-10\ndataset. The left subtable shows the results under a fixed depth and the right one shows those under\na fixed embedding dimension.\nfixed depth=7\nED=192\nED=288\nED=384\n\nfixed ED=384\ndepth=3\ndepth=5\ndepth=7\n\nlast layer’s ID\n23.52\n23.07\n22.51\n\nlast layer’s ID\n22.02\n22.33\n22.51\n\n3.4 A PRINCIPAL COMPONENT ANALYSIS VIEWPOINT OF DATA REPRESENTATION\n\n(a) AG\n\n(b) IMDB\n\n(c) SST2\n\nFigure 3: PCA results on data representation of the last hidden layer (ED = 256, depth = 6).\nThe vertical axis shows the value of corresponding eigenvalues. The green and red vertical dotted\nlines denote the number of components required to capture 50% and 85% of the variance in data\nrepresentation.\n\nThere exist numerous tools for estimating the intrinsic dimension of data representation, includ-\ning methods rooted in principal component analysis (PCA) (Fukunaga & Olsen (1971), Bruske &\nSommer (1998)), as well as the TwoNN approach (Facco et al. (2017)), among others. Owing to\nits computational efficiency, we opted for the TwoNN algorithm. For a more in-depth discussion,\nreaders can refer to Section 2.\n\nBased on the findings presented in Figure 1, it’s clear that the intrinsic dimensions of Transformers\nare significantly less than the embedding and hidden dimensions. In this section, we delve deeper,\n\n6\n\n\fUnder review as a conference paper at ICLR 2024\n\ndemonstrating that the data representation crafted by Transformers resides on low-dimensional yet\ncurved manifolds, rather than flat subspaces, rendering them unsuitable for model reduction through\nlinear techniques.\n\nTo substantiate this claim, we employed the time-honored PCA method (Pearson (1901)) on the\nnormalized covariance matrix of each Transformer layer across various hyper-parameter configura-\ntions and all three datasets. The outcomes for the final hidden layer are depicted in Figure 3. The\nx-axis ranks the eigenvalues of data representation in descending order, while the y-axis conveys\nthe magnitude of these eigenvalues. The green and red dotted vertical lines represent the number\nof components necessary to account for 50% and 85% of data representation variance, respectively.\nThe x-coordinate indicated by the red line is termed PCA-ID, representing the ”pseudo” intrinsic\ndimension ascertained through a straightforward PCA technique.\n\nThe ID derived via the TwoNN methodology is substantially smaller than the PCA-ID. For instance,\nthe ID presented in Figure 1 (a) is roughly 34 for ED = 256 on the AG dataset.\nIn contrast,\nthe PCA-ID displayed in Figure 3 (a) stands at about 180 in the same scenario, a figure that’s 5-6\ntimes greater. Moreover, the PCA-ID’s ratio to the embedding dimension oscillates between 0.7 and\n0.9, far exceeding that of the ID (ranging from 0.05 to 0.15). The stark disparity between the IDs\ninferred from the TwoNN and PCA methods highlights the pronounced non-linearity interlinking\ndata samples. From these observations, we infer that the domain in which data representation is\nsituated isn’t a linear subspace but rather a distinct curved manifold. This nature precludes any\nattempts at rudimentary linear model reduction.\n\nFigure 4: The length distribution histogram of several datasets. The horizontal axis represents sen-\ntence length, and the vertical axis represents frequency per length interval.\n\n3.5 THE EFFECT OF INTRINSIC DIMENSION REDUCTION WITH RESPECT TO DEPTH\n\nTable 2: The ID reduction w.r.t. depth for different embedding dimensions on the AG dataset, where\n“emb layer” denotes the embedding layer.\n\ndepth=4\nemb layer’s ID last layer’s ID decrease\n\ndepth=8\nemb layer’s ID last layer’s ID decrease\n\nED=128\nED=256\nED=512\n\n42.04\n54.91\n60.93\n\n27.85\n33.86\n37.21\n\n14.19\n21.05\n23.73\n\n43.81\n55.09\n60.96\n\n27.14\n32.44\n32.46\n\n16.66\n22.65\n28.50\n\nTable 3: The ID reduction w.r.t. depth for different embedding dimensions on the IMDB dataset.\n\ndepth=4\nemb layer’s ID last layer’s ID decrease\n\ndepth=8\nemb layer’s ID last layer’s ID decrease\n\nED=128\nED=256\nED=512\n\n52.36\n73.91\n102.41\n\n38.74\n55.65\n69.86\n\n13.62\n18.27\n32.55\n\n51.14\n72.98\n103.33\n\n37.68\n47.30\n60.55\n\n13.46\n25.68\n42.78\n\nIn Transformers, as with other neural network architectures, model depth is a pivotal factor. A\nmodel that’s too shallow might lack robust representational capability, leading to subpar training\noutcomes. Conversely, an exceedingly deep model might face generalization challenges like over-\nfitting, resulting in diminished test performance, and might demand prohibitive computational and\nmemory resources. In this section, we delve deeper into how ID variation correlates with model\ndepth.\n\n7\n\n\fUnder review as a conference paper at ICLR 2024\n\nTable 4: The ID reduction w.r.t. depth for different embedding dimensions on the SST2 dataset.\n\ndepth=4\nemb layer’s ID last layer’s ID decrease\n\ndepth=8\nemb layer’s ID last layer’s ID decrease\n\nED=128\nED=256\nED=512\n\n40.85\n52.29\n60.30\n\n34.50\n41.48\n46.22\n\n6.34\n10.82\n14.08\n\n41.39\n48.32\n59.8\n\n30.12\n37.87\n43.27\n\n11.26\n10.45\n16.53\n\nAs illustrated in Figure 1, there’s a noticeable decline in ID across layers. To better understand the\ninfluence of model depth on this trend, it’s instructive to examine the disparities between the IDs\nof embedding layers and those of the final hidden layers. Though the pertinent ID outcomes are\nalready depicted in Figure 1, we’ve collated them in Table 2, 3, and 4 for clarity.2 From this data,\nseveral key observations emerge:\n\n• Holding the dataset and embedding dimension constant, the IDs of the embedding layers\n\nremain largely unchanged despite variations in depth.\n\n• Again, with a consistent dataset and embedding dimension, the IDs of the final hidden\n\nlayers typically diminish substantially as depth increases.\n\n• Synthesizing the above insights reveals that Transformers exhibit a more pronounced di-\n\nmension reduction across layers in deeper architectures.\n\nA straightforward rationale for these observations can be posited. When the embedding dimension\nis held constant, the learned data representation tends to be stable, suggesting consistent IDs for the\nembedding layers. Simultaneously, referencing Figure 1 and the discourse in Section 3.2, the trend\nof decreasing ID seems to intensify in deeper models.\n\n3.6 DATA REDUCTION CASES: IMPACT OF SEQUENTIAL LENGTHS\n\nIn the context of practical applications, the Transformer model is often associated with a consider-\nable volume of parameters. This complexity necessitates the utilization of large datasets to achieve\noptimal training results, implicating notable space and computational time expenditures. The im-\nplication of these overheads motivates an investigation into the potential for reducing the size of\nthe training dataset—a concept we refer to as “data reduction”—while maintaining, or potentially\nenhancing, the model’s performance during testing.\n\nOur study is anchored in the exploration of the feasibility and approaches to implementing data\nreduction in the training phase of Transformers. We approach this investigation by considering the\nintrinsic dimensionality of data representation. A notable observation from our preliminary findings\nis the relative stability of the intrinsic dimension, contrasting with the variability often observed in\nother hyper-parameters, such as the embedding dimension. This stability is observed irrespective of\nthe sequential length of data samples, unveiling prospective avenues for data reduction through the\nselective exclusion of specific training samples without compromising the model’s performance.\n\nOur journey into this inquiry commences with an examination of the sequential length of samples\nconstituting the training dataset. We systematically organize all training sentences according to their\nlength, measured in terms of word count (refer to Figure 4 for a visual representation). A subset\nof sentences, termed the “long-set,” is compiled by selecting the upper 80% of the most extended\nsentences present in the training dataset. Analogously, a “short-set” is constructed by harvesting the\nupper 80% of the shortest sentences. For comparative purposes, we retain the original, unmodified\ntraining dataset, denoting it as the ”full-set.”\n\nThe underpinning rationale for this categorization and selective inclusion is to mitigate the influence\nof outliers, particularly those that are exceptionally short or long. We posit that this strategy facil-\nitates a more refined and coherent training dataset, thereby augmenting the integrity and efficacy\nof the model training process. It is of paramount importance to underscore that this modification\nis exclusive to the training datasets; the test datasets remain unaltered, ensuring the validity and\nintegrity of our experimental results and conclusions.\n\n2Due to space limitations, the depth 6 scenario is omitted.\n\n8\n\n\fUnder review as a conference paper at ICLR 2024\n\nThis nuanced approach to data curation and reduction not only aims at optimizing computational\nefficiency but also seeks to unravel insights into the intricate dynamics between dataset size, intrinsic\ndimensionality, and model performance. Each subset—long-set, short-set, and full-set—serves as\na distinct lens through which we scrutinize the interplay of these factors, laying a foundational\nframework for future research in efficient and effective training of Transformer models.\n\nTable 5: The effect of sequential lengths on IDs of embedding layers and classification errors, where\nthe depth of Transformers is fixed as 5.\n\nAG\n\nIMDB\n\nSST2\n\nED=128\nED=256\nED=512\nED=128\nED=256\nED=512\nED=128\nED=256\nED=512\n\nID of embedding layer\nshort\nlong\nfull\n44.35\n46.23\n43.77\n57.21\n54.07\n55.49\n59.12\n59.22\n58.96\n48.29\n51.17\n51.10\n71.78\n74.77\n74.38\n104.15\n103.70\n101.34\n42.16\n41.63\n42.10\n54.40\n53.14\n53.17\n62.84\n60.51\n62.23\n\nerror rate\nlong\n10.3\n9.5\n9.2\n14.7\n13.6\n13.2\n25.9\n26.2\n24.6\n\nshort\n10.6\n10.1\n9.4\n15.3\n14.1\n13.9\n26.1\n25.5\n24.3\n\nfull\n9.6\n8.8\n9.3\n14.6\n13.5\n13.2\n23.3\n24.2\n22.8\n\n3.7 EXPERIMENTAL ANALYSIS OF DATA REDUCTION BY SEQUENTIAL LENGTHS\n\nExperiments were conducted using the three aforementioned training sub-datasets: long-set, short-\nset, and full-set. These experiments considered various configurations, with a primary focus on\nembedding dimensions, across different datasets. For every Transformer model trained within these\nparameters, we documented both the IDs of the embedding layers and the ultimate classification\nerror rates (using the complete test datasets) in Table 5.\n\nA principal observation emerges from Table 5. Reviewing the results row by row, we find that\nneither the IDs of the embedding layers nor the classification error rates show substantial variations,\neven though each Transformer model is trained using distinct subsets of the original training dataset.\nThis finding lends support to the feasibility of data reduction in training, particularly with regard to\nsequential lengths. Such insights can prove invaluable in scenarios where data is scarce, highlighting\navenues for further investigation in subsequent studies.\n\n4 CONCLUSION\n\nWe have presented an analysis that offers a new perspective on understanding Transformers, empha-\nsizing the role of intrinsic dimensions in data representation. This exploration has unveiled distinct\npatterns and relationships between intrinsic dimensions, task performance, and various hyperparam-\neters including model depth, embedding dimensions, and data sequence length.\n\nOur results highlight several core insights. We found a direct relationship between embedding di-\nmensions, intrinsic dimensions, and classification accuracy. Additionally, we noted a decrease in\nintrinsic dimensions through successive layers, a trend amplified in models with increased depth.\n\nIn the context of the relationship between model and data, evidence supports that the data represen-\ntation learned by Transformers is situated on curved manifolds. Our study also suggests that data\nreduction during the training phase can be effective, offering avenues for enhanced data efficiency\nand applicability in real-world scenarios.\n\nThough our study is grounded in classification tasks, we are poised to expand this research into\ngenerative domains. We are also preparing to delve deeper into the examination of established\narchitectures, including pre-trained language models, to enrich our understanding. Our future work\nis geared towards quantitative analyses that will connect the dots between Transformers and intrinsic\ndimensions, addressing existing theoretical voids.\n\n9\n\n\fUnder review as a conference paper at ICLR 2024",
    "abstract": "Transformers have become immensely popular for sequential modeling, particularly in domains like natural language processing (NLP).\nRecent innovations have introduced various architectures based on the Transformer framework, resulting in significant advancements in applications.\nHowever, the underlying mechanics of these architectures are still somewhat enigmatic.\nIn this study, we explore the geometrical characteristics of data representations learned by Transformers using a mathematical metric known as intrinsic dimension (ID). This can be conceptualized as the minimum parameter count needed for effective modeling.\nA sequence of experiments, predominantly centered on text classification, support the ensuing empirical observations regarding the correlation between embedding dimension, layer depth, individual layer ID, and task performance.\nInterestingly, we note that a higher terminal feature ID, when obtained from Transformers, generally correlates with a lower classification error rate. \nThis stands in contrast to the behavior observed in CNNs (and other models) during image classification tasks. Furthermore, our data suggests that the ID for each layer tends to diminish as layer depth increases, with this decline being notably steeper in more intricate architectures.\nWe also present numerical evidence highlighting the geometrical constructs of data representations as interpreted by Transformers, indicating that only nonlinear dimension reduction is achievable.\nLastly, we delve into how varying sequence lengths impact both ID and task performance, confirming the efficacy of data reduction during training.\nOur ambition is for these insights to offer direction in the choice of hyper-parameters and the application of dimension/data reduction when using Transformers for text classification and other prevalent NLP tasks.",
    "response": {
      "Soundness": 2,
      "Presentation": 2,
      "Contribution": 1,
      "Rating": 1,
      "Confidence": 4,
      "Strengths": "None",
      "Weaknesses": "This paper seems to be a study. And an incomplete one. It goes from related work where they describe the TwoNN method proposed by another paper, to experiment results. There is no technical section? It is also written kind of strangely.",
      "Questions": "See weaknesses"
    }
  },
  {
    "paper_id": "JzvIWvC9MG",
    "summary": "Background: Game theory provides a mathematical framework for modeling interactions among self-interested agents, typically leading to Nash equilibria. However, the parameters of the payoff functions driving these interactions are often unobserved in real situations, necessitating methods for inverse game theory, which infers these parameters from observed equilibria.\nGap: Existing methods for inverse game theory, particularly in multiagent environments, struggle with computational complexity and accuracy, especially when dealing with stochastic elements and real-world data with unclear payoff structures.\nHypothesis: The authors hypothesize that formulating inverse game theoretical problems as generative-adversarial min-max optimization problems can allow for efficient (polynomial-time) computation of inverse Nash equilibria and related predictions.\nMethod: The authors develop polynomial-time algorithms relying on min-max optimization frameworks that utilize exact or stochastic first-order oracles to infer game parameters from observed equilibriums. Specifically, they extend these methods to inverse multiagent simulacral learning, aiming to replicate given observations using computed parameters and equilibria.\nConclusion: The proposed methods demonstrate effective prediction on synthetic economic games and outperform traditional ARIMA models in forecasting electricity prices using Spanish market data, indicating both enhanced computational efficiency and prediction accuracy in complex multiagent settings.\n",
    "full text": "\nEFFICIENT INVERSE MULTIAGENT LEARNING\n\nDenizalp Goktas⇤& Amy Greenwald\nBrown University, Computer Science\ndenizalp_goktas@brown.edu\n\nSadie Zhao\nHarvard University, Computer Science\n\nAlex Koppel & Sumitra Ganesh\nJP Morgan Chase & Co\n\nABSTRACT\n\nIn this paper, we study inverse game theory (resp. inverse multiagent learning) in\nwhich the goal is to ﬁnd parameters of a game’s payoff functions for which the\nexpected (resp. sampled) behavior is an equilibrium. We formulate these problems\nas generative-adversarial (i.e., min-max) optimization problems, which we develop\npolynomial-time algorithms to solve, the former of which relies on an exact ﬁrst-\norder oracle, and the latter, a stochastic one. We extend our approach to solve\ninverse multiagent simulacral learning in polynomial time and number of samples.\nIn these problems, we seek a simulacrum, meaning parameters and an associated\nequilibrium that replicate the given observations in expectation. We ﬁnd that our\napproach outperforms the widely-used ARIMA method in predicting prices in\nSpanish electricity markets based on time-series data.\n\n1\n\nINTRODUCTION\n\nGame theory provides a mathematical framework, called games, which is used to predict the outcome\nof the interaction of preference-maximizing agents called players. Each player in a game chooses a\nstrategy from its strategy space according to its preference relation, often represented by a payoff\nfunction over possible outcomes, implied by a strategy proﬁle (i.e., a collection of strategies, one-\nper-player). The canonical outcome, or solution concept, prescribed by game theory is the Nash\nequilibrium (NE) (Nash, 1950): a strategy proﬁle such that each player’s strategy, ﬁxing the equilib-\nrium strategies of its opponents, is payoff-maximizing (or more generally, preference-maximizing).\n\nIn many applications of interest, such as contract design (Holmström, 1979; Grossman & Hart, 1992)\nand counterfactual prediction (Peysakhovich et al., 2019), the payoff functions (or more generally,\npreference relations) of the players are not available, but the players’ strategies are. In such cases, we\nare concerned with estimating payoff functions for which these observed strategies are an equilibrium.\nThis estimation task serves to rationalize the players’ strategies (i.e., we can interpret the observed\nstrategies as solutions to preference-maximization problems). Estimation problems of this nature\ncharacterize inverse game theory (Waugh et al., 2013; Bestick et al., 2013).\n\nThe primary object of study of inverse game theory is the inverse game, which comprises a game\nwith the payoff functions omitted, and an observed strategy proﬁle. The canonical solution concept\nprescribed for an inverse game is the inverse Nash equilibrium, i.e., payoff functions for which the\nobserved strategy proﬁle corresponds to a Nash equilibrium. If the set of payoff functions in an\ninverse game is unrestricted, the set of inverse Nash equilibria can contain a wide variety of spurious\nsolutions, e.g., in all inverse games, the payoff function that assigns zero payoffs to all outcomes is\nan inverse Nash equilibrium, because any strategy proﬁle is a Nash equilibrium of a constant game:\ni.e., one whose payoffs are constant across strategies. To meaningfully restrict the class of payoff\nfunctions over which to search for an inverse Nash equilibrium, one common approach (Kuleshov &\nSchrijvers, 2015; Syrgkanis et al., 2017) is to assume that the inverse game includes in addition to all\nthe aforementioned objects, a parameter-dependent payoff function for each player, in which case\nan inverse Nash equilibrium is simply deﬁned as parameter values such that the observed strategy\nproﬁle is a Nash equilibrium of the parameter-dependent payoff functions evaluated at those values.\n\n⇤Research conducted while the author was an intern at JP Morgan Chase & Co.\n\n1\n\n\fPublished as a conference paper at ICLR 2024\n\nIf one assumes exact oracle access to the payoffs of the game (i.e., if there exists a function which,\nfor any strategy proﬁle, returns the players’ payoffs1), the problem of computing an inverse Nash\nequilibrium is one of inverse multiagent planning. In many games, however, a more appropriate\nassumption is stochastic oracle access, because of inherent stochasticity in the game (Shapley, 1953)\nor because players employ randomized strategies Nash (1950). The problem of computing an inverse\nNash equilibrium assuming stochastic oracle access is one of inverse multiagent learning.\n\nOne important class of inverse games is that of inverse Markov games, in which the underlying game\nis a Markov game (Shapley, 1953; Fink, 1964; Takahashi, 1964), i.e., the game unfolds over an\ninﬁnite time horizon: at each time period, players observe a state, take an action (simultaneously),\nreceive a reward, and transition onto a new state. In such games, each player’s strategy,2 also called a\npolicy, is a mapping from states to actions describing the action the player takes at each state, with\nany strategy proﬁle inducing a history distribution over histories of play i.e., sequences of (state,\naction proﬁle) tuples. The payoff for any strategy proﬁle is then given by its expected cumulative\nreward over histories of play drawn from the history distribution associated with the strategy proﬁle.\nExcluding rare instances,3 the payoff function in Markov games is only accessible via a stochastic\noracle, typically implemented via a game simulator that returns estimates of the value of the game’s\nrewards and transition probabilities. As such, the computation of an inverse Nash equilibrium in\nan inverse Markov game is an inverse multiagent learning problem, which is often called inverse\nmultiagent reinforcement learning (inverse MARL) (Natarajan et al., 2010).\n\nIn many real-world applications of inverse Markov games, such as robotics control (Coates et al.,\n2009), one does not directly observe Nash equilibrium strategies but rather histories of play, which\nwe assume are sampled from the history distribution associated with some Nash equilibrium. In these\napplications, we are given an inverse simulation—an inverse Markov game together with sample\nhistories of play—based on which we seek parameter values which induce payoff functions that\nrationalize the observed histories. As a Nash equilibrium itself is not directly observed in this setting,\nwe aim to compute parameter values that induce a Nash equilibrium that replicates the observed\nhistories in expectation. We call the solution of such an inverse simulation (i.e., parameter values\ntogether with an associated Nash equilibrium) a simulacrum. Not only does a simulacrum serve to\nexplain (i.e., rationalize) observations, additionally, it can provide predictions of unobserved behavior.\n\nWe study two simulacral learning problems, a ﬁrst-order version in which samples histories of play\nare faithful, and a second-order version in which they are not—a (possibly stochastic) function of each\nhistory of play is observed rather than the history itself. Here, the use of the term “ﬁrst-order” refers\nto the fact that the simulacrum does not necessarily imitate the actual equilibrium that generated the\nhistories of play, since multiple equilibria can generate the same histories of play (Baudrillard, 1994).\nMore generally, if the simulacrum is “second-order,” it is nonfaithful, meaning some information\nabout the sample histories of play is lost. We refer to the problems of computing ﬁrst-order (resp.\nsecond-order) simulacra as ﬁrst-order (resp. second-order) simulacral learning: i.e., build a ﬁrst-order\n(resp. second-order) simulacrum from faithful (resp. non-faithful; e.g., aggregate agent behavior)\nsample histories of play. We summarize the problems characterizing inverse game theory in Table 1a.\n\nContributions The algorithms introduced in this paper extend the class of games for which an\ninverse Nash equilibrium can be computed efﬁciently (i.e., in polynomial-time) to the class of normal-\nform concave games (which includes normal-form ﬁnite action games), ﬁnite state and action Markov\ngames, and a large class of continuous state and action Markov games. While our focus is on Markov\ngames in this paper, the results apply to normal-form (Nash, 1950), Bayesian (Harsanyi, 1967; 1968),\nand extensive-form games (Zermelo, 1913). The results also extend to other equilibrium concepts,\nbeyond Nash, such as (coarse) correlated Aumann (1974); Moulin & Vial (1978), and more generally,\n\u0000-equilibrium (Greenwald & Jafari, 2003) mutatis mutandis.\n\nFirst, regarding inverse multiagent planning, we provide a min-max characterization of the set of\ninverse Nash equilibria of any inverse game for which the set of inverse Nash equilibria is non-empty,\nassuming an exact oracle (Theorem 3.1). We then show that for any inverse concave game, when the\n\n1Throughout this work, we assume that the oracle evaluations are constant time and measure computational\n\ncomplexity in terms of the number of oracle calls.\n\n2Throughout this paper, a strategy refers to the complete description of a players’ behavior at any state or\n\ntime of the game, while an action refers to a speciﬁc realization of a strategy at a given state and time.\n\n3For simple enough games, one can express the expected cumulative reward in closed form, and then solve\n\nthe inverse (stochastic) game assuming exact oracle access.\n\n2\n\n\fPublished as a conference paper at ICLR 2024\n\nEquilibrium Access\nDirect\nFaithful Samples\nNonfaithful Samples\n\nExact Oracle\nInverse Multiagent Planning\nFirst-order Simulacral Planning\nSecond-order Simulacral Planning\n\nStochastic Oracle\nInverse Multiagent Learning\nFirst-order Simulacral Learning\nSecond-order Simulacral Learning\n\n(a) Taxonomy of inverse game theory problems. First-\norder simulacral learning is more commonly known as\nmultiagent apprenticeship learning (Abbeel & Ng, 2004;\nYang et al., 2020).\n\nReference\n(Fu et al., 2021)\n(Yu et al., 2019)\n(Lin et al., 2019)\n(Song et al., 2018)\n(Syrgkanis et al., 2017)\n(Kuleshov & Schrijvers, 2015)\n(Waugh et al., 2013)\n(Bestick et al., 2013)\n(Natarajan et al., 2010)\n\nThis work\n\nGame Type\nFinite Markov\nFinite Markov\nFinite Zero-sum Markov\nFinite Markov\nFinite Bayesian\nFinite Normal-Form\nFinite Normal-Form\nFinite Normal-Form\nFinite Markov\nFinite/Concave Normal-form\nFinite/Concave Markov\n\nSolution Concept\nNash\nQuantal Response\nVarious\nQuantal Response\nBayes-Nash\nCorrelated\nCorrelated\nCorrelated\nCooperative\nNash/Correlated\nAny Other\n\nPolytime?\n7\n7\n7\n7\n3\n3\n3\n7\n7\n\n3\n\n(b) A comparison of our work and prior work on inverse\ngame theory and inverse MARL.\n\nregret of each player is convex in the parameters of the inverse game, an assumption satisﬁed by a\nlarge class of inverse games such as inverse normal-form games, this min-max optimization problem\nis convex-concave, and can thus be solved in polynomial time (Theorem 3.2) via standard ﬁrst-order\nmethods. This characterization also shows that the set of inverse Nash equilibria can be convex, even\nwhen the set of Nash equilibria is not.\n\nSecond, we generalize our min-max characterization to inverse multiagent learning, in particular\ninverse MARL, where we are given an inverse Markov game, and correspondingly, a stochastic\noracle, and we seek a ﬁrst-order simulacrum (Corollary 1). We show that under standard assumptions,\nwhich are satisﬁed by a large class of inverse Markov games (e.g., all ﬁnite state and action Markov\ngames and a class of continuous state and action Markov games), the ensuing min-max optimization\nproblem is convex-gradient dominated, and thus an inverse Nash equilibrium can be computed once\nagain via standard ﬁrst-order methods in polynomial time (Theorem 4.1).\n\nThird, we provide an extension of our min-max characterization to (second-order) simulacral learning\n(Theorem 5.1). We once again characterize the problem as a solution to a min-max optimization\nproblem, for which standard ﬁrst-order methods compute a ﬁrst-order stationary (Lin et al., 2020)\nsolution in polynomial-time, using a number of observations (i.e., unfaithful samples of histories of\nplay) that is polynomial in the size of the inverse simulation (Theorem 5.2).\n\nFinally, we include two sets of experiments. In the ﬁrst, we show that our method is effective in\nsynthetic economic settings where the goal is to recover buyers’ valuations from observed competitive\nequilibria (which, in this market, coincide with Nash equilibria). Second, using real-world time-series\ndata, we apply our method to predict prices in Spanish electricity markets, and ﬁnd that it outperforms\nthe widely-used ARIMA method in predicting prices on this real-world data set.\n\n,\n\n,\n\n)\n\n)\n\n}\n\n{\n\nX\n\nX\n\nX\n\nX\n\nF\n\n!\n\n.\n=\n\n.\n=\n\n(x)\n\nby ⇧\n\nf(x2)\n\n1, . . . , n\n\nF\nµ : (\n\n2 PRELIMINARIES\nNotation. All notation for variable types, e.g., vectors, should be clear from context; if any\nconfusion arises, see Section 7.1. We denote by [n] the set of integers\n. Let\nbe any\n}\nset and (\n) any associated measurable space, where the \u0000-algebra\nunless otherwise noted\nis assumed to be the \u0000-algebra of countable sets, i.e.,\n. We write\n}\n). Additionally,\n\u0000(\n[0, 1]\ny\nwe denote the orthogonal projection operator onto a set\n\nF\nto denote the set of probability measures on (\n\nis countable\n,\nX\n.\n= arg miny\n\n{\nF\n{E ✓ X | E\n\nx1 \u0000\nx2k\nminx02X h\n\nk·k\n. If the gradient of f is `\n\nR. f is `f -Lipschitz-continuous w.r.t. norm (typically, Euclidean)\n\nX\nMathematical Concepts. Consider any normed space (\nf :\n,\n\n\u0000\nRm and any function\nx1, x2 2\nk·k\n8\nf -Lipschitz-continuous, we refer\n\u0000\nf -Lipschitz-smooth. Furthermore, given µ > 0, f is said to be µ-gradient-dominated if\nr\nf(x0)\n\nX!\nf(x1)\nA\nk\nto f as `\nminx02X\n\u0000\nNormal-form Games. A (parametric) game\neach i\nWe refer to any vector of per-player strategies x = (x1, . . . , xn)\n\nN+ players,\nRm simultaneously.\nas a strategy proﬁle, where\nRnm denotes the space of all strategy proﬁles. After the players choose their\n, each receives a payoff ui(x; ✓) given by payoff function ui :\nR\nRd. We deﬁne the payoff proﬁle function\nR across all players, between two\nui(x; ✓); and the\n (x, y; ✓).\n\n⇥i\nX\nstrategies x\nparameterized by a vector ✓ in a parameter space ⇥\nu(x; ✓)\n[n]; the cumulative regret   :\nstrategy proﬁles x, y\n, given ✓\n\u0000\nexploitability (or Nikaido-Isoda potential (Nikaido & Isoda, 1955)) '(x; ✓)\n\n[n] of whom chooses a strategy xi 2X i from an strategy space\n\n[n] Xi ✓\n2X\n.\n= (ui(x; ✓))i\n\nx,\nr\n✓ .\n= (n, m, d,\n\n`f k\nf(x) + µ\n\n⇥\n!\n[n] ui(yi, x\n\n(Bhandari & Russo, 2019).\n\n, ⇥, ✓, u) comprises n\n\n✓\nX⇥X⇥\n.\n=\n\ni; ✓)\n\u0000\n.\n= maxy\n\n⇥, as  (x, y; ✓)\n\nX\n) where\n\nF\n2X k\n\n2\n2X\n\nXi ✓\n\nf(x)\n\nk \n\nX⇥\n\nX⇢\n\n2X\n\n.\n=\n\n2\n2.\n\n!\n\niff\n\nx0\n\n\u0000\n\n⇥\n\nX\n\nX\n\n2\n\n2\n\n2\n\nx\n\nG\n\nk\n\nr\n\ni\n\n2\n\n2\n\n·\n\n,\n\ni\n\nP\n\n2X\n\n3\n\n\fPublished as a conference paper at ICLR 2024\n\nA game is said to be concave if for all parameters ✓\nempty, compact, and convex, 2. ui is continuous, and 3. xi 7!\n✓\nmaxxi2Xi ui(xi, x⇤\n2\nequilibrium, and is guaranteed to exist in concave games (Nash, 1950; Arrow & Debreu, 1954).\n\nG\n\u0000\n[n]. A 0-Nash equilibrium is simply called a Nash\n\nXi is non-\ni; ✓) is concave. Given\n\n⇥, an \"-Nash equilibrium (\"-NE) of a game\n\n⇥ and players i\nui(xi, x\n\n✓ is a strategy proﬁle x⇤\n\n\", for all players i\n\ns.t. ui(x⇤; ✓)\n\n[n], 1.\n\ni; ✓)\n\n2X\n\n\u0000\n\n2\n\n2\n\n2\n\n\u0000\n\n\u0000\n\n,\n\nS\n\nS\n\n=\n\nA\n\nM\n\n\u0000(\n\n✓ .\n\n). At each time period t = 0, 1, . . ., each player i\n\n2\ni 2A i from an action space\n\nDynamic Games. An (inﬁnite-horizon, discounted, parametric) Markov game (Shapley, 1953;\n, ⇥, ✓, r, p,\u0000, µ ) is a dynamic game played over\n= (n, m,\nFink, 1964; Takahashi, 1964)\nan inﬁnite time horizon. The game initiates at time t = 0 in some state S(0)\nµ drawn from\n[n]\nan initial state distribution µ\nplays an action a(t)\nRm . We deﬁne the space of action proﬁles\n, each\nA\nplayer i receives a reward ri(s(t), a(t); ✓) according to a parameterized reward proﬁle function\nr :\n(0, 1) is\n\u0000\ns(t), a(t)) according to a\ncalled the discount factor, or transitions to a new state S(t+1)\n(Markov) probability transition kernel p whereby for all (s, a)\n),\ns, a)\nS\nS(t) = s(t), A(t) = a(t)) is the probability of\ns(t), a(t)) = P(S(t+1) = s(t+1)\nand p(s(t+1)\ntransitioning to state s(t+1) from state s(t) when the players’ take action proﬁle a(t).4\n\n[n] Ai. After the players choose their action proﬁle a(t)\n\nRn. The game then either ends with probability 1\n\n⇥i\nS⇥A⇥\n\n1 , . . . , a(t)\nn )\n\n· |\n2S⇥A\n\n.\n= (a(t)\n\n\u0000, where \u0000\n\nAi ⇢\n\n, p(\n\n2A\n\n\u0000(\n\n· |\n\n!\n\np(\n\n⇠\n\n⇥\n\n⇠\n\n2\n\n2\n\n2\n\n2\n\n|\n\n|\n\n[n], we deﬁne the space of all (measurable) policies\n\nA (stationary Markov) policy (Maskin & Tirole, 2001) for player i\nfrom states to actions so that ⇡i(s)\nplayer i\n.\n2\n⇡\n= (⇡1, . . . , ⇡n)\nlength T\nT\ndeﬁne the discounted history distribution ⌫⇡ (h)\nt=0 \u0000t p(s(t+1)\nprobability of observing a history h of length T. Throughout, we denote by H\nany randomly sampled history from ⌫⇡ .5\n\n2\nS!A\n2A i denotes the action that player i takes at state s. For each\n. As usual,\nPi\n)T of\n1\n,\n2P\ns(t), ⇡(s(t))) as the\n⌫⇡\n\n[n] Pi denotes a policy proﬁle. A history (of play) h\n\nN is a sequence of state-action tuples h = (s(t), a(t))T\n\n[n] is a mapping ⇡i :\n.\n=\n\nS⇥A\nt=0 . For any policy proﬁle ⇡\n\u0000\n\nS!A i}\n(\n2\n\n.\n= µ(s(0))\n\nS(t), A(t)\n\n⇡i :\n{\n\n⇥i\n\nt ⇠\n\n2P\n\n|\n.\n=\n\n.\n=\n\nQ\n\n2\n\n2\n\n\u0000\n\n\u0000\n\n⇠\n\n⌫⇡\n\n2P\n\nand a player i.\n\nThe expected cumulative payoff\n\nFix a policy proﬁle ⇡\non the following terminology.\n1t=0 ri(S(t), A(t); ✓)\nEH\nspectively, as v⇡\ni (s; ✓)\n⇥P\n1t=0 ri(S(t), A(t); ✓)\n⌫⇡\nEH\n2\n⇠\n) denotes the probability that a state is reached under a policy ⇡, given initial state distribution µ,\n\u0000(\n⇥P\nS\ni.e., \u0000⇡\n. Finally, as usual, an \"-Nash equilibrium (\"-NE) of a game\nµ (s)\n\nis given by ui(⇡; ✓)\nare deﬁned,\ni (s, a; ✓)\n. The state occupancy distribution \u0000⇡\nµ\n⇤\n\nIn our analysis of Markov games, we rely\n.\n=\nre-\n.\n=\n\n. The state- and action-value functions\n\n⇠\nS(0) = s, A(0) = a\n\n1t=0 ri(S(t), A(t); ✓)\n\n.\n= EH\n⇤\n\nand q⇡\n\nS(0) = s\n\n⇥P\n\n⌫⇡\n\n⇤\n\n|\n\n|\n\n1t=0 S(t )=s\n\nM\nand a Nash equilibrium ensues when \" = 0.\n\n2P\n\nsuch that for all i\n\ni\n\n[n], ui(⇡⇤; ✓)\n\nmax⇡i2Pi\n\nui(⇡i, ⇡⇤\n\u0000\n\ni; ✓)\n\n\u0000\n\n\";\n\n\u0000\n\n2\n\n.\n= EH\n✓ is a policy proﬁle ⇡⇤\nhP\n\n⌫⇡\n\n⇠\n\n3\n\nINVERSE MULTIAGENT PLANNING\n\nThe goal of inverse multiagent planning is to invert an equilibrium: i.e., estimate a game’s parameters,\ngiven observed behavior. In this section, we present our main idea, namely a zero-sum game (i.e.,\nmin-max optimization) characterization of inverse multiagent planning, where one player called\n\n4For notational convenience, we assume the probability transition function is independent of the parameters,\nbut we note that our min-max characterizations apply more broadly without any additional assumptions, while\nour polynomial-time computation results apply when, in addition to Assumption 4, one assumes the probability\ntransition function is stochastically convex (see, for instance, Atakan (2003a)) in the parameters of the game.\n\n5Let (\n\n,\n\n,\n\n,\n\nFS\n\n), (\n\nS⇥A\n\nFS⇥A\n\n), and (\n\n) spaces, respectively. Further, let ([0, 1],\n\n) be the measurable spaces associated with the state, action\nFA\nS\nA\nBRn ) be measurable\nproﬁle, and state-action proﬁle (\nn deﬁned by the Borel \u0000-algebra. For simplicity, we do not explicitly represent the reward\nspaces on [0, 1] and R\nproﬁle function, transition probability kernel, initial state distribution, or policies as measures or measurable\nfunctions. We note, however, that for the expectations we deﬁne to be well-posed, they all must be assumed\n[0, 1],\nto be measurable functions. We simply write r :\nand ⇡ :\n)\n([0, 1],\n\nto mean, respectively, r : (\n\nn, p :\nR\nn,\n(R\n(\n\nB[0,1]), (R\n\nS!\nFS⇥A\n\n[0, 1], µ :\n\n([0, 1],\n\nS⇥A\n\nS⇥A\n\n!\n)\n\nFS\n\n!\n\nn,\n\n⇥\n\n)\n\n(\n\n)\n\n,\n\n,\n\nS⇥A\nB[0,1]), and ⇡ : (\n\nS⇥A!\n)\n,\nFS⇥A\n,\nS\n\n!\n)\nFS\n\n(\nS⇥A\nS⇥\n,\nBRn ), p : (\nS\n).\n,\nFA\nA\n\nS!A\nB[0,1]), µ : (\n\nS\n\nFS\n\n!\n\n!\n\n4\n\n\fPublished as a conference paper at ICLR 2024\n\n✓†\n\n✓†\n\nG\n\n= (\n\nthe stabilizer picks parameters, while the other called the destabilizer picks per-player deviations.\nThis game is zero-sum) because the stabilizer seeks parameters that rationalize (i.e., minimize the\nexploitability of) the observed equilibrium, while the destabilizer aims to rebut the rationality of\nthe observed equilibrium (i.e., seeks deviations that maximize cumulative regret). We use this\ncharacterization to develop a gradient descent ascent algorithm that ﬁnds inverse NE in polynomial\ntime, assuming access to an exact ﬁrst-order oracle: speciﬁcally, a pair of functions that return the\nvalue and gradient of the payoff proﬁle function.\n1 .\n\n1, our goal is to compute an \"-inverse Nash equilibrium, meaning parameter values ✓⇤\n\n✓†, x†) comprises a game form (i.e., a parametric game sans its\nAn inverse game\n\u0000\nG\n✓† together with an observed strategy proﬁle x†, which we assume is a Nash\nparameter)\nequilibrium. Crucially, we do not observe the parameters ✓† of the payoff functions. Given an inverse\n⇥\ngame\n✓⇤ . As usual, a 0-inverse NE is simply called an inverse NE. Note that this\ns.t. x†\ndeﬁnition does not require that we identify the true parameters ✓†, as identifying ✓† is impossible\nunless there exists a bijection between the set of parameters and the set of Nash equilibria, a highly\nrestrictive assumption that is not even satisﬁed in games with a unique Nash equilibrium. To compute\nan inverse NE is to ﬁnd parameter values that minimize the exploitability of the observed equilibrium.\nThis problem is a min-max optimization problem, as the parameter values that minimize exploitability\nare those that maximize the players’ cumulative regrets. More precisely:\nTheorem 3.1. The set of inverse NE of\noptimization problem min✓\n\n1 is the set of parameter proﬁles ✓\n⇥ '(x†; ✓), or equivalently, this min-max optimization problem:\n\n⇥ that solve the\n\nis an \"-NE of\n\nG\n2X\n\n2\n\n2\n\nG\n\nG\n\nG\n\n\\\n\n\\\n\n\u0000\n\n\u0000\n\n2\n\nf(✓, y)\n\n.\n=  (x†, y; ✓) =\n\nmin\n⇥\n✓\n2\n\nmax\ny\n2X\n\nui(yi, x†\n\u0000\n\ni; ✓)\n\n\u0000\n\nui(x†; ✓)\n\ni\n\nXi\n2\n\n[n] h\n\n(1)\n\nThis min-max optimization problem can be seen as a generalization of the dual of Waugh et al.’s\n(2013) maximum entropy likelihood maximization method for games with possibly continuous\nstrategy spaces, taking Nash equilibrium rather than maximum entropy correlated equilibrium as\nthe inverse equilibrium. In contrast to Waugh et al.’s dual, our min-max optimization problem\ncharacterizes the set of all inverse NE, and not only a subset of the inverse correlated equilibria, in\nparticular those that maximize entropy. This formulation also generalizes Swamy et al.’s (2021)\nmoment matching game from a single-agent to a multiagent setting.\n\nt=0\n\nAlgorithm 1 Adversarial Inverse Multiagent Planning\n, f, ⌘✓ , ⌘y , T, ✓(0), y(0), x†\nInputs: ⇥,\nX\nOutputs: (✓(t), y(t))T\n1: for t = 0, . . . , T\n✓(t+1)\n⇧⇥\n2:\n\n1 do\n⌘(t)\n✓ r✓ f(✓(t), y(t))\n✓(t)\n\u0000\ni\ny(t) + ⌘(t)\nry f(✓(t), y(t))\ny\n\ny(t+1)\n3:\n4: return (✓(t), y(t))T\nh\nt=0\n\n\u0000\n\n⇧\n\ni\n\nh\n\nX\n\nWithout further assumptions, the objective\nfunction f in Equation (1) is non-convex non-\nconcave; however, under suitable assump-\ntions (Assumption 1) satisﬁed by ﬁnite action\nnormal-form games, for example, it becomes\nconvex-concave.\n1,\nAssumption 1. Given an inverse game\nassume 1. (Concave game) for all parame-\n✓ is concave; and 2. (Con-\nters ✓\nvex parametrization) ⇥ is non-empty, com-\n, each player i’s regret\n\n⇥,\n\nG\n\nG\n\n\u0000\n\n2\n[n], yi 2X i, and x†\n\ni\n\n8\n\n2\n\n\u0000\n\n7!\n\nui(yi, x†\n\u0000\n\nui(x†; ✓) is convex.\n\npact, and convex; and for all\n✓\ni; ✓)\nRemark 1. Perhaps surprisingly, the set of inverse NE can be convex even when the set of NE is not,\nsince the set of solutions to a convex-concave (or even convex-non-concave) min-max optimization\nproblem is convex.\nThis observation should alleviate any worries about the computational in-\ntractability of inverse game theory that might have been suggested by the computational intractability\nof game theory itself (Daskalakis et al., 2009; Chen & Deng, 2006).\n\n2X\n\nIf additionally, we assume the players’ payoffs are Lipschitz-smooth (Assumption 2), Equation (1)\ncan then be solved to \" precision in O (1/\"2) via gradient descent ascent (Algorithm 1). That is, as\nTheorem 3.2 shows, an inverse \"-NE can be computed in O (1/\"2) iterations.6 We note that this\nconvergence complexity can be further reduced to O (1/\") (even without decreasing step-sizes) if one\ninstead applies an extragradient descent ascent method (Golowich et al., 2020) or optimistic GDA\n(Gorbunov et al., 2022).\n\n6We include detailed theorem statements and proofs in Section 7.2.\n\n5\n\n \n \n\fPublished as a conference paper at ICLR 2024\n\nAssumption 2 (Lipschitz-Smooth Game). For all players i\n\n[n], ui is `\n\nr\n\n2\n\nui -Lipschitz-smooth.\n\nTheorem 3.2 (Inverse NE Complexity). Under Assumptions 1–2, for \"\nwith inputs that satisfy T\n2\nall parameters ✓(T) .\n= 1\nT+1\n\nT\nt=0 ✓(t) is an \"-inverse NE.\n\n⌦(1/\"2) and for all t\n\ny = ⌘(t)\n\n[T], ⌘(t)\n\n✓ ⇣\n\n2\n\n0, if Algorithm 1 is run\n1/t, then the time-average of\n\n\u0000\n\nINVERSE MULTIAGENT REINFORCEMENT LEARNING\n\nP\n\n4\n\nIn this section, we build on our zero-sum game (i.e., min-max optimization) characterization of\ninverse game theory to tackle inverse MARL in an analogous fashion. As it is unreasonable to assume\nexact oracle access to the players’ (cumulative) payoffs in inverse MARL, we relax this assumption\nin favor of a stochastic oracle model. More speciﬁcally, we assume access to a differentiable game\n⌫⇡ according to ⌫⇡ , given any\nsimulator (Suh et al., 2022), which simulates histories of play h\npolicy proﬁle ⇡, and returns the rewards r and transition probabilities p,7 encountered along the way,\ntogether with their gradients.\n\n⇠\n\n1 .\n\n✓†\n\n\u0000\n\n\\\n\nM\n\nM\n\n= (\n\n✓†, ⇡†) is an inverse game that comprises a\nFormally, an inverse Markov game\n✓†\n✓† together with an\nMarkov game form (i.e., a parametric Markov game sans its parameter)\nobserved policy proﬁle ⇡†, which we assume is a Nash equilibrium. Crucially, we do not observe\nthe parameters ✓† of the payoff functions. Since a Markov game is a normal-form game with\npayoffs given by u(⇡; ✓) = EH\n, the usual deﬁnitions of inverse NE\nand cumulative regret apply, and the following result, which characterizes the set of inverse NE as the\nminimizers of a stochastic min-max optimization problem, is a corollary of Theorem 3.1.\nCorollary 1. The set of inverse NE of\n\n1 is characterized by solutions to the following problem:\n\n1t=0 r(S(t), A(t); ✓)\n\n⇥P\n\nM\n\n⌫⇡\n\n\\\n\n\u0000\n\n⇠\n\n⇤\n\nM\n\nf(✓, ⇡)\n\n.\n=\n\nmin\n⇥\n✓\n2\n\nmax\n⇡\n2P\n\n1\n\n)\n\ni\n\n\"\n\nt=0\nX\n\nE\n(⇡i ,⇡†\n\u0000\n⌫⇡†\n\n⇠\n\n[n]\nXi\n2\n\nH\n\n⌫\n\n⇠\nH 0\n\nri(S(t), A(t); ✓)\n\n\u0000\n\n1\n\nt=0\nX\n\nri(S†\n\n(t), A†\n\n(t); ✓)\n\n(2)\n\n#\n\n2\n\nP\n\n2X\n\n7!\n\nX⇢\n\n⇥ maxx\n\nRl . Redeﬁning f(✓, x)\n\n.\n= f(✓, ⇡x ), for ⇡x 2P\n\nAs is usual in reinforcement learning, we use policy gradient to solve the destabilizer’s problem in\nEquation (2). To do so, we restrict the destabilizer’s action space to a policy class\nX parameterized\nby\nX , we aim to solve the stochastic min-max\noptimization problem min✓\nf(✓, x). Solutions to this problem are a superset of the\nsolutions to Equation (2), unless it so happens that all best responses can be represented by policies in\nX , because restricting the expressivity of the policy class decreases the power of the destabilizer. As\nP\nin Section 3, without any additional assumptions, f is in general non-convex, non-concave, and non-\nsmooth. While we can ensure convexity and smoothness of ✓\nf(✓, x) under suitable assumptions\non the game parameterization, namely by assuming the regret at each state is convex in ✓, concavity\nin x is not satisﬁed even by ﬁnite state and action Markov games. Under the following conditions,\nhowever, we can guarantee that f is Lipschitz-smooth, convex in ✓, and gradient dominated in x.\nAssumption 3 (Lipschitz-Smooth Gradient-Dominated Game). Given an inverse Markov game\n, ⇥ are\nX\nfor all policies\n[n],\n(s, ⇡x (s); ✓) is µ-gradient-\n,\n2S\ni(s)) =\n\nM\nnon-empty, compact, and convex; 3. (Smooth Game)\n⇡x\n2P\nstates s\ndominated for some µ > 0; and 5. (Closure under Policy Improvement) for all states s\nplayers i\n[n], and policy proﬁles ⇡\n2\nq⇡\ni (s, ⇡0i(s), ⇡\nmax⇡0i2Pi\nPart 3 of Assumption 3 implies that the game’s cumulative payoff function is Lipschitz-smooth\nin the policy parameters x. We note that a large class of Markov games satisfy Part 4, including\nlinear quadratic games (Bhandari & Russo, 2019), ﬁnite state and action games, and continuous\nstate and action games whose rewards (resp. transition probabilities) are concave (resp. stochastically\n\nare non-empty, and compact; 2. (Convex parameter spaces)\nr,\n\nX , are continuously differentiable; 4. (Gradient-Dominated Game) for all players i\n\n, and policies ⇡x\n\n, there exists ⇡x\n\n, action proﬁles a\n\n1, assume 1.\n\nrx ⇡x ,\n\ni (s, ⇡x\n\nX s.t. q⇡\n\ni (s), ⇡\n\ni(s)).\n\np, and\n\nq⇡x\n\nX , x\n\nand\n\n2A\n\n2P\n\n2P\n\n2P\n\n2S\n\n7!\n\nr\n\nr\n\nA\n\n2\n\nS\n\n\u0000\n\n\u0000\n\n\u0000\n\ni\n\n7We note that in inverse reinforcement learning, as opposed to reinforcement learning, it is typical to assume\n\nthat the transition model is known (see, for instance (Abbeel & Ng, 2004), Footnote 8).\n\n6\n\n\fPublished as a conference paper at ICLR 2024\n\nconcave) in each player’s action (Atakan, 2003b). Finally, Part 5 is a standard assumption (see, for\ninstance, Section 5 of Bhandari & Russo (2019)), which guarantees that the policy parameterization\nis expressive enough to represent best responses.\nAssumption 4 (Convex Parameterization). Given an inverse Markov game\nfor all players i\ni; ✓)\nri(s, bi, a\n\n[n], states s\nri(s, a; ✓) is convex.\n\n\u0000\n, the per-state regret ✓\n\n, and action proﬁles a, b\n\n1, assume that\n\n2A\n\n2S\n\nM\n\n7!\n\n2\n\u0000\n\n\u0000\n\n7!\n\nf(✓, x) is convex, for all x\n\nWith these assumptions in hand, we face a convex gradient-dominated optimization problem, i.e.,\n⇥. As\n✓\n7!\nfor normal-form games (see Remark 1), the set of inverse NE in Markov games is convex under\nAssumptions 3 and 4. Consequently, we can obtain polynomial-time convergence of stochastic\ngradient descent ascent (Algorithm 2) by slightly modifying known results (Daskalakis et al., 2020).\n\nf(✓, x) gradient-dominated, for all ✓\n\n, and x\n\n2X\n\n2\n\n2\n\n3:\n\n2:\n\n⇠\n\nt=0\n\nH\n\n,⇡†\n\u0000\n\n⌫⇡†\n\n✓(t+1)\n\n1 do\nx(t )\ni\n\n\u0000\n[n] ⌫(⇡\n⇧⇥\n\n, f✓ , fx ,⌘ ✓ ,⌘ x , T, ✓(0), x(0), ⇡†\n\n⇠⇥i\nx(t+1)\n4:\n5: return (✓(t), x(t))T\nh\nt=0\n\nAlgorithm 2 Adversarial Inverse MARL\nInputs: ⇥,\nP\nOutputs: (✓(t), x(t))T\n1: for t = 0, . . . , T\n\nAlgorithm 2 requires an estimate of\nf w.r.t. both ✓ and x. Under Part\nr\n3 of Assumption 3, the gradient of f\nw.r.t. x can be obtained by the deter-\nministic policy gradient theorem (Sil-\nver et al., 2014), while the gradient of\nf w.r.t. ✓ can be obtained by the lin-\nearity of the gradient and expectation\noperators. However, both of these\ngradients involve an expectation—\nx\ni ,⇡†\n⌫⇡† .\nover H\n\u0000\nAs such, we estimate them using sim-\nulated trajectories from the deviation\ni) and the equilibrium history distribu-\n⌫⇡† , respectively. For a given such pair (H, h†), the cumulative regret gradient estimators\ntion h†\nf✓ and fx correspond to the gradients of the cumulative regrets between each deviation history hi in\nH and h†, and can be computed directly using the chain rule for derivatives, as we assume access to\na differentiable game simulator.8\n\ni), h†\n⌘(t)\n✓(t)\n✓ r✓ f✓ (✓(t), x(t); H, h†)\n\u0000\ni\nx(t) + ⌘(t)\nrx fx (✓(t), x(t); H, h†)\nx\n\nhistory distribution H\n\nh1, . . . , hn\n\ni) and H †\n\n⇠⇥i\n\n[n] ⌫(⇡\n\nx\ni ,⇡†\n\u0000\n\n⌫(⇡\n\n.\n=\n\n⇠\n\n⇠\n\n⇠\n\n⇧\n\ni\n\nh\n\n\u0000\n\n\u0000\n\nP\n\n2\n\nT\n\n⇡†\nµ /@µ\n\n@\u0000\n\nFinally, we deﬁne the equilibrium distribution mismatch coefﬁcient\nas the Radon-\nNikodym derivative of the state occupancy distribution of the NE ⇡† w.r.t. the initial state distribution\nµ. This coefﬁcient, which measures the inherent difﬁculty of reaching states under ⇡†, is closely\nrelated to other distribution mismatch coefﬁcients introduced in the analysis of policy gradient\nmethods (Agarwal et al., 2020). With this deﬁnition in hand, we can ﬁnally show polynomial-time\nconvergence of stochastic GDA (Algorithm 2) under Assumptions 3–4.\nTheorem 4.1. Under Assumptions 3–4, for all \"\n\"\u0000\nT\nparameters ✓(T) .\n⇣\n\n(0, 1), if Algorithm 2 is run with inputs that satisfy\n\"8, then the time-average of all\n\nand for all t\n2\n⇣\nT\nt=0 ✓(t) is an \"-inverse NE.\n\n2\n[T], ⌘(t)\ny\n\n\"4 and ⌘(t)\n\n⇡†\nµ /@µ\n\n✓ ⇣\n\n@\u0000\nk\n\n= 1\nT+1\n\nk1\n\nk1\n\n⌦\n\n2\n\n⌘\n\n10\n\nk\n\n5 SIMULACRAL LEARNING\n\nP\n\nIn this section, we consider the more realistic setting in which we do not observe an equilibrium, but\n⌫⇡† associated with an unobserved\n(s(t,k), a(t,k))t\nobserve only sample histories\nequilibrium ⇡†. The problem of interest then becomes one of not only inferring parameter values\nfrom observed behavior, but of additionally ﬁnding equilibrium policies that generate the observed\nbehavior, a solution which we refer to as a ﬁrst-order simulacrum. A ﬁrst-order simulacrum can be\nseen as a generalization of an inverse equilibrium, as it not only comprises parameters that rationalize\n\nk =\n\nh(k)\n\nk ⇠\n\n\u0000\n\n\u0000\n\n8For completeness, we show how to compute fx and f✓ in Section 7.5. In our experiments, however,\nas has become common practice in the literature (Mora et al., 2021), we compute these gradients by simply\nautodifferentiating the cumulative regret of any history w.r.t. the policy parameters using a library like Jax\n(Bradbury et al., 2018). We also show that under Assumption 3, (f✓ , fx ) is an unbiased estimate of (\nrx f)\nwhose variance is bounded.\n\nr✓ f,\n\n7\n\n \n \n \n \n\fPublished as a conference paper at ICLR 2024\n\nthe observed histories, but also policies that mimic them in expectation. First-order simulacral\nlearning is also known as multiagent apprenticeship learning (Abbeel & Ng, 2004; Yang et al., 2020).\n\n)\n\n\u0000\n\n2\n\n2\n\n}\n\n,⇡\n\n2:\n\n3:\n\nP\n\n(k)\n\n(k)\n\nt=0\n\nH\n\nc\n\n}k\n\ng✓ ,\n\ngx ,\n\n⌫⇡†\n\n✓(t)\n\no†\n{\n\n✓(t+1)\n\n[] ⇠\n\n⌫(⇡x(t )\n\n⇠⇥i\nx(t+1)\n\ngy ),⌘ ✓ ,⌘ x ,⌘ y , T, ✓(0), x(0), y(0),\n\nAlgorithm 3 Adversarial Simulacral Learning\nInputs: ⇥,\n, (\nOutputs: (✓(t), x(t), y(t))T\n1 do\n1: for t = 0, . . . , T\nc\nc\n\u0000\nx(t )\n[n] ⌫(⇡\ni\n⇧⇥\n\nEven more generally, we might\nnot have access to samples\nh†\nfrom\n{\nan equilibrium history distribu-\ntion, but rather a lossy func-\ntion of those histories accord-\n:\ning to some function ⇢\nthat produces\n.\nH!O\n=\no†\nobservations\n}k\n[]\n{\n2\n⌅⇡† , dis-\nh†\n⇢\n[] ⇠\n{\ntributed according to some\n(pushforward) observation dis-\ntribution ⌅⇡\n), parame-\nterized by policy proﬁle ⇡\n,\nis the observation space. This more general framework is very useful in applications where\nwhere\nthere are limitations on the data collection process: e.g., if there are game states at which some of the\nplayers’ actions are unobservable, or when only an unfaithful function of them is available. Here, we\nseek to learn the more general notion of a second-order simulacrum.\n\ny(t )\ni ), h\n⌘(t)\n✓ r✓\n⌘(t)\nx(t)\nx\n\u0000\nh\ny(t) + ⌘(t)\ny\n\ni\ngx (✓(t), x(t), y(t); H, h)\nc\ngy (✓(t), x(t), y(t); H, h)\nc\n\n5:\n6: return (✓(t), x(t), y(t))T\n\n⇠\ng✓ (✓(t), x(t), y(t); H, h)\n\ny(t+1)\n\nrx\n\nry\n\n2P\n\n\u0000(\n\n}k\n\nc\n\nt=0\n\nO\n\nO\n\n\u0000\n\n⇧\n\n⇧\n\n(k)\n\n(k)\n\n2\n\n4:\n\ni\n\ni\n\nh\n\nh\n\n\u0000\n\n\u0000\n\nP\n\nP\n\n2\n\n1 .\n\n\u0000\n\n\\\n\nI\n\n✓†\n\n✓†\n\nM\n\nM\n\n✓†,\n\n= (\n\n\\\n✓† with unknown parameters ✓†, an observation distribution ⌅:\n\n, ⌅, ⌅⇡† ) is a tuple consisting of a Markov game\nFormally, an inverse simulation\n) mapping\nform\n, and an observation distribution ⌅⇡† for the\npolicies to distributions over the observation space\nunobserved behavioral policy ⇡†, which we assume is a Nash equilibrium. Our goal is to ﬁnd an (\", \u0000)-\nthat (\", \u0000)-simulates\nNash simulacrum, meaning a tuple of parameters and policies (✓⇤, ⇡⇤)\n\" and\nthe observations as a Nash equilibrium:\n\n⇥\nmax⇡i2Pi\n\u0000. Theorem 5.1, which is analogous to Corollary 1, characterizes\n\n⇥P\nui(⇡i, ⇡⇤\n\u0000\n\ni.e., ui(⇡⇤; ✓⇤)\n\ni; ✓⇤)\n\nP!\n\n\u0000(\n\nO\n\nO\n\nO\n\n\u0000\n\n\u0000\n\n2\n\no\n\n2\n\no†\n\nE(o,o†)\nthe set of Nash simulacra of an inverse simulation.\n\n⌅⇡⇤\n\n⌅⇡†\n\n\u0000\n\n\n\n⇥\n\n⇠\n\nh\u0000\n\u0000\nTheorem 5.1. Given an inverse simulation\n\u0000\nis equal to the set of minimizers of the following stochastic min-max optimization problem:\n\n1, for any ↵, \u0000 > 0, the set of Nash simulacra of\n\n\u0000\n\u0000\n\nI\n\ni\n\n1\n\n\u0000\n\nM\n\nmin\n✓\n⇥\n2\n⇡\n2P\n\n'(✓, ⇡) = min\n⇥\n\n✓\n⇡\n\n2\n2P\n\ng(✓, ⇡, ⇢)\n\n.\n= ↵\n\nmax\n⇢\n2P\n\n(o,o†)\n\nE\n⌅⇡\n\n⇠\n\n⌅⇡†\n\n⇥\n\no†\n\no\n\n\u0000\n\n2\n\ni\n\n\u0000\n\u0000\n\nh\u0000\n\u0000\n\n+ \u0000 (⇡, ⇢; ✓)\n\n(3)\n\n2\n\n⇠\n\nh\n\nb\n\no\n\nP\n\n(k)\n\n(k)\n\n\u0000\n\n2X\n\n⇥X\n\n\u0000\n\u0000\n\no†\n\n1/\n\n\nk=1\n\n}⇠\n⌅⇡\n\nmaxy\n\ng(✓, x, y)\n\ng(✓, ⇡, ⇢)\n\ng(✓, x, y).\n\nlearning loss\n\nTo tackle simulacral\n\nb\nX , redeﬁne g(✓, x, y)\n\n⌅⇡† , based on which we compute the empirical\n\no†\n{\n↵ Eo\ni\nonce again restrict policies to lie within a parametric class of policies\nP\n.\ng(✓, ⇡x , ⇡y ) and\n=\nempirical learning loss min(✓ ,x)\n2\nb\n\nlearning, we approximate g via realized observation samples\n.\n=\n+ \u0000 (⇡, ⇢; ✓). Additionally, as in the previous section, we\n.\n=\n\u0000\n\u0000\ng(✓, ⇡x , ⇡y ), and solve the ensuing optimization problem over the\n⇥\n\nIn general, this stochastic min-max optimization is non-convex non-concave. By Assumption 3,\nb\nhowever, the function y\ng(✓, x, y) is gradient dominated, for all ✓\n. Nevertheless,\nit is not possible to guarantee that (✓, x)\ng(✓, x, y) is convex or gradient dominated, for all\ny\n, without overly restrictive assumptions. This claim is intuitive, since the computation of\nan inverse simulacrum involves computing a Nash equilibrium policy, which in general is a PPAD-\ncomplete problem (Daskalakis et al., 2009; Foster et al., 2023). Finally, deﬁning gradient estimators\ngy )(✓, x, y; H, hx ) from samples\nas we did in Section 4, to obtain gradient estimators (\nx\ni ,⇡\nhistories H\n, we can use Algorithm 3 to compute a local solution\nof Equation (3) from polynomially-many observations.\nTheorem 5.2. Suppose that Assumption 3 holds, and that for all ⇡x\ntinuously differentiable in x. For any \"\n⇡⇤\nand for all t\nµ /@µ\nT\n\nis twice con-\n(0, 1), if Algorithm 3 is run with inputs that satisfy\n[T], ⌘(t)\n\"8, then the best iterate\ny\n\n\"4 and ⌘(t)\n\ni) and hx\n\n⇠⇥i\n\nX , ⌅⇡x\n\n[n] ⌫(⇡\n\n⇥ and x\n\n\u00002/\"10\n\n⌫⇡x\n\ngx ,\n\ng✓ ,\n\n2X\n\n2P\n\n2Y\n\n7!\n\n7!\n\nc\n\nc\n\nc\n\n⇠\n\n⌦\n\n2\n\n2\n\n2\n\n\u0000\n\ny\n\n2\n\n⇣\n\n✓ ⇣\n\n2\n\n⇣\n\n@\u0000\nk\n\nk1\n\n⌘\n\n8\n\n \n \n \n\fPublished as a conference paper at ICLR 2024\n\n(✓ best, xbest) converges to an \"-stationary point of ' (deﬁned in Section 7.2). Additionally, for any\n⇣,⇠\n⇠ if the number of\nsample observations \n\n0, it holds with probability 1\n\nbest, x(T)\nbest)\n\nbest, x(T)\nbest)\n\n⌦(1/⇠2 log(1/⇣)).\n\n'(✓ (T)\n\n'(✓ (T)\n\n⇣ that\n\n\u0000\n\n\u0000\n\n\u0000\n\n\n\n6 EXPERIMENTS\nWe run two sets of experiments with the aim of answering two questions. Our ﬁrst goal is to\nunderstand the extent to which our algorithms are able to compute inverse Nash equilibria, if any,\nbeyond our theoretical guarantees. Our second goal is to understand the ability of game-theoretic\nmodels to make predictions about the future.9\n\nb\n\n2\n\nIn our ﬁrst set of experiments, we consider ﬁve types\nof economic games whose equilibria and payoffs have\ndifferent properties. The ﬁrst three are Fisher market\n(FM) games, which are zero-sum, between sellers and\nbuyers engaged in trading goods. These games can be\ncategorized based on the buyers’ utility functions as lin-\near, Cobb-Douglas, or Leontief (Cheung et al., 2013). We\nthen consider two general-sum economic games, which\nmodel competition between two ﬁrms, namely Cournot\ncompetition and Bertrand oligopoly. When budgets are\nthe only parameters we seek to recover, our min-max for-\nmulation is convex-concave, because the players’ payoffs\nare concave in their actions, and afﬁne in their budgets,\nand hence the regret of players is also afﬁne in the players’\nbudgets. In addition, in both the Cournot competition and\nBertrand oligopoly games, regret is again convex in the\nparameters of the game. Finally, all the games we study\nare concave, with the exception of the Bertrand oligopoly game, and the equilibria are unique in the\nCobb-Douglas FM, Cournot competition, and Bertrand oligopoly games. In each experiment, we\ngenerate 500 synthetic game instances, for which the true parameters are known, and use Algorithm 1\n(which does not rely on this knowledge) to compute an inverse NE for each. We record whether our\nalgorithm recovers the true parameters of the market and whether it ﬁnds an inverse NE (i.e., average\nexploitability). We summarize our ﬁndings for the FM games in Table 2. We ﬁnd that our algorithm\nrecovers the true parameters more often when budgets are the only parameters we seek to recover,\nas opposed to both budgets and types; but even in non-convex-concave case, our algorithm is still\nable to approximate inverse NE over 80% of the time. In settings where the equilibria are unique, we\nrecover true parameters most often, while the worst performance is on Leontief FM games, where\npayoffs are not differentiable.\n\nFigure 1: Hourly prices in the Spanish\nelectricity market from January 2015 to\nDecember 2020. The Nash simulacrum\nachieves a MSE that is twice as low as\nthat of the ARIMA method.\n\nGame Parameters\nFisher Market Type\n% Parameters Recovered\nAverage Exploitability\n\nBudgets\nLeontief\n36.8%\n0.2240\n\nCD\n100%\n0.0004\n\nLinear\n100%\n0.0018\n\nTypes + Budgets\nLeontief\n1%\n0.1949\n\nCD\n99.6%\n0.0004\n\nLinear\n12%\n0.0119\n\n% Parameters Recovered\nAverage Exploitability\n\nCournot Bertrand\n95.2%\n0.0000\n\n78%\n0.0011\n\nTable 2: The percentage of games for which we recovered the true parameters and the average\nexploitabilities of the observed equilibrium evaluated w.r.t the computed inverse Nash equilibrium.\nIn our second set of experiments, we model the Spanish electricity market as a stochastic Fisher\nmarket game between electricity re-sellers and consumers. In this game, the state comprises the supply\nof each good and the consumers’ budgets, while the re-sellers’ actions are to set prices in today’s spot\nmarket and tomorrow’s day ahead market, and the consumers’ actions are their electricity demands.\nWe assume the consumers utilities are linear; this choice is suited to modeling the substitution effect\nbetween electricity today and electricity tomorrow. Using publicly available hourly Spanish electricity\nprices and aggregate demand data from Kaggle, we compute a simulacrum of the game that seeks to\nreplicate these observations from January 2015 to December 2016. We also train an ARIMA model\non the same data, and run a hyperparameter search for both algorithms using data from January 2017\nto December 2018. After picking hyperparameters, we then retrain both models on the data between\nJanuary 2015 to December 2018, and predict prices up to December 2018. We also compute the\nmean squared error (MSE) of both methods using January 2018 to December 2020 as a test set. We\nshow the predictions of both methods in Figure 1. To summarize, we ﬁnd that the simulacrum makes\npredictions whose MSE is twice as low.\n\n9Our code can be found here.\n\n9\n\n\fPublished as a conference paper at ICLR 2024",
    "abstract": "In this paper, we study inverse game theory (resp. inverse multiagent learning) in\nwhich the goal is to find parameters of a game’s payoff functions for which the\nexpected (resp. sampled) behavior is an equilibrium. We formulate these problems\nas generative-adversarial (i.e., min-max) optimization problems, which we develop\npolynomial-time algorithms to solve, the former of which relies on an exact first-\norder oracle, and the latter, a stochastic one. We extend our approach to solve\ninverse multiagent simulacral learning in polynomial time and number of samples.\nIn these problems, we seek a simulacrum, meaning parameters and an associated\nequilibrium that replicate the given observations in expectation. We find that our\napproach outperforms the widely-used ARIMA method in predicting prices in\nSpanish electricity markets based on time-series data.",
    "response": {
      "Soundness": 4,
      "Presentation": 3,
      "Contribution": 4,
      "Rating": 10,
      "Confidence": 3,
      "Strengths": "The simple formulation of the set of inverse Nash equilibria (NE) as a min-max\ngame is elegant and appears to be original. If it is indeed original, for this\nalone, the paper merits publication and should be highlighted.\n\nThe paper overall is well written and showcases immediate applications of the\nproposed solution to an important and practical domain as a proof-of-concept. I\nbelieve these results are significant and will be impactful.",
      "Weaknesses": "As an easily rectified issue, Figure 1 could have been better represented by\nplotting residuals over time or, by subsampling the data, plotting mean\nresiduals with error bars.\n\nAs a minor complaint, I do not prefer the language of \"generative-adversarial\"\n(especially not in terms of a \"discriminator\"), even if this is the closest\nanalogy familiar to machine learning practitioners: This is a standard min-max\noptimization problem that need not be wed to the ML setting.",
      "Questions": "Remark 1 is indeed interesting, but it is not obvious. Did I miss an associated\nproof or example?\n\nWhy was the proof of Theorem 3.2 omitted? Was it rephrased to appear as Theorem\n6.1 in the supplementary material?  Establishing the convergence rates of\nvarious algorithms is not my expertise, but the results seem reasonable,\nespecially given assumptions of convexity and Lipshitz smoothness."
    }
  },
  {
    "paper_id": "5rrYpa2vts",
    "summary": "Background: The proliferation of fake news on social media has become a significant issue, with substantial efforts made over the past decade in developing methods for detecting fake news. Initial efforts included using manually crafted textual features, and later, deep learning methods like LSTM and RNN, which evolved to incorporate external knowledge alongside textual features. Recent models like KAN and Dual-CAN have attempted to integrate external evidence and social information, respectively, to improve the accuracy of fake news detection.\nGap: Current fake news detection models have limitations in maintaining longer text dependencies and capturing complex semantic relationships. They also face challenges in integrating external knowledge efficiently. Models like KAN and FinerFact have shown reliance on single-entity contexts or socially sourced claims, which can be less reliable or manipulated.\nHypothesis: The authors propose that by incorporating Abstract Meaning Representation (AMR) and external knowledge from Wikidata, specifically through a novel graph representation and evidence linking algorithm, a more robust and accurate model for fake news detection can be developed.\nMethod: The proposed model, EA2N, combines a language encoder and a graph encoder to integrate transformer-encoded textual features with affective lexical features. The graph encoder constructs an AMR graph, enriching it with external knowledge from Wikidata to form a WikiAMR graph. A path-aware graph learning module is used to capture semantic relationships among entities. The model predicts the veracity of news by integrating features from both the language and graph encoders, using a transformer for classification. The model was evaluated using datasets from PolitiFact and GossipCop, comparing its performance to state-of-the-art models.\nConclusion: EA2N, leveraging AMR and external knowledge, outperformed state-of-the-art models in fake news detection on benchmark datasets. The integration of evidence through the WikiAMR graph was shown to be effective, suggesting this method's potential for improving semantic understanding in NLP applications.\n",
    "full text": "\nEA2N: EVIDENCE-BASED AMR ATTENTION NET-\nWORK FOR FAKE NEWS DETECTION\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nProliferation of fake news has become a critical issue in today’s information-\ndriven society. Our study includes external knowledge from Wikidata and devi-\nates from the reliance on social information to detect fake news, that many state-\nof-the-art (SOTA) fact-checking models adopt. This paper introduces EA2N, an\nEvidence-based AMR Attention Network for Fake News Detection. EA2N lever-\nages Abstract Meaning Representation (AMR) and incorporates knowledge from\nWikidata using proposed evidence linking algorithm, pushing the boundaries of\nfake news detection. The proposed framework encompasses a combination of\nnovel language encoder and graph encoder to detect the fake news. While the\nlanguage encoder effectively combines transformer encoded textual features with\naffective lexical features, the graph encoder encodes AMR with evidence through\nexternal knowledge, referred as WikiAMR graph. A path-aware graph learning\nmodule is designed to capture crucial semantic relationships among entities over\nevidence. Extensive experiments supports our model’s superior performance, sur-\npassing SOTA methodologies. This research not only advances the field of Fake\nNews Detection but also showcases the potential of AMR and external knowl-\nedge for robust NLP applications, promising a more trustworthy information land-\nscape.1\n\n1\n\nINTRODUCTION\n\nSocial media has revolutionized the exchange of information by enabling people to obtain and\nshare news online. However, with the growing popularity and convenience of social media, the\ndissemination of fake news has also escalated. The deliberate distortion and fabrication of facts\nin fake news have severe negative consequences for individuals and society (Brewer et al., 2013).\nTherefore, it is crucial and socially advantageous to detect and address fake news on social media.\nSignificant efforts have been made in the direc-\ntion of fake news detection in the past decade.\nEarly works (Feng et al., 2012; Ma et al., 2016)\nused manually crafted textual features to de-\ntect fake news. Later, many researchers used\nLSTM and RNN based methods (Long et al.,\n2017; Liu & Wu, 2018) for the purpose. Deep\nlearning methods used therein is able to learn\ntext features out of the article. Recently, ex-\nternal knowledge is incorporated alongside the\ntextual features to improve fake news detec-\ntion models. Dun et al. (2021) proposed KAN\nmodel which leverages external evidence from\nthe Wikidata. On the contrary, FinerFact (Jin\net al., 2022) and Dual-CAN (Yang et al., 2023),\nincorporates social information that supports\nauthenticity of the news article. Despite the\nsignificant achievements, these methods exhibit\n\nFigure 1: An example of evidence linking in AMR\ngraph constructed over Politifact news article.\n\n1Code repository: Code will be shared once the discussion forum start.\n\n1\n\n\fUnder review as a conference paper at ICLR 2024\n\nlimitations in capturing certain details. Precisely, these models struggle to maintain longer text de-\npendencies and less effective to capture complex semantic relations such as events, locations, trigger\nwords and so on. Additionally, the way of incorporating external knowledge into these models is not\nhighly reliable and time consuming. For example, KAN only considers single entity contexts and\nfail to link context between two entities. On the other hand, FinerFact gather supported claims from\nsocial platforms that is time-consuming. Although social authenticity produces good results, these\ninformation can be manipulated by social media users for personal gain. In order to tackle these\nchallenges, our study effectively uses complex semantic relations of news articles and evidence\nfound in Wikidata (Wikidata5M (Wang et al., 2021a)) with the help of a novel graph representation.\n\nIn this paper, we present a novel model for detecting fake news that leverages a semantically en-\nriched knowledge base to classify a news article as real or fake. Our model incorporates Abstract\nMeaning Representation (AMR) (Banarescu et al., 2013) to understand the logical structure of sen-\ntences. Further, the model establishes relations between entities found in AMR graph, through a\nnew evidence linking algorithm. The algorithm utilizes Wikidata to connect evidence, leading to\nthe formation of a graph referred as WikiAMR. To the best of our knowledge, this is the first study\nto examine the evidence based semantics features of AMR graphs for fake news detection. For the\nillustration, Fig. 1 shows an example of WikiAMR constructed over a news article using Wiki-\ndata. In this, a linkage is established between the entities ‘Donald T rump’ and ‘M af ia’, wherein\nthe nodes ‘M edia Coverage’ and ‘P ost − election protest’ are connected by relations such as\n‘main subject’, ‘has cause’, and ‘part of ’ in the path. Similarly, a connecting evidence path\nemerges between ‘Russian − emigre’ and ‘F lex Ster’. These instances serve as valuable evi-\ndence to assess the credibility of news content. Next, to encode the WikiAMR graph, we employ a\npath-aware graph learning module. This module uses relation-enhanced global attention that focus\non important relation over entities and compute the attention score considering the entities and their\nrelations. By modifying the Graph Transformer (Cai & Lam, 2020) for entities, our model can effec-\ntively reason over the relation paths within the WikiAMR graph. In order to enhance the capabilities\nof language encoder, we also use affective features (Ghanem et al., 2021) extracted from different\nsegments of news article and concatenated with language embedding. Finally, the representations of\nlanguage and AMR graph are fed into a classification layer using transformer to predict the veracity\nof the news. The key contributions of our research are as follows:\n\n• Introduction of EA2N, a novel Evidence-based AMR Attention Network for Fake News\n\nDetection, reasoning over evidence linked through external knowledge.\n\n• Introduction of WikiAMR graph, a novel graph structure that includes undirected evidence\npaths, extracted form external knowledge graph, between entities of AMR constructed from\ntext document.\n\n• Evidence Linking Algorithm to generate WikiAMR, from entity-level and context-level\n\nfiltering to enhance model performance.\n\n• Comprehensive evaluation of EA2N against state-of-the-art techniques, demonstrating its\n\nsuperior performance and effectiveness.\n\n2 RELATED WORKS\n\nIn this section, we delve into brief of the approaches employed in the detection of fake news. We\nhave categorized the relevant studies into three components: textual-based methods, knowledge-\naware methods, and AMR-based methods. A comprehensive explanation of each approach is pro-\nvided in the subsequent sections.\n\nTextual-based approaches primarily rely on the textual content extracted from articles to verify the\nauthenticity of news. In the early times, the emphasis was primarily on developing a supplementary\ncollection of manually created features rooted in linguistic characteristics (Feng et al., 2012; Ma\net al., 2016; Long et al., 2017; Rashkin et al., 2017; Liu & Wu, 2018). These early studies demanded\nextensive efforts to assess the efficacy of these manually crafted features. Recently, Ghanem et al.\n(2021) proposed FakeFlow that involves the utilization of a text with lexical features to classify news\nas fake news. Early detection of fake news is facilitated in many work (Wei et al., 2021; Azevedo\net al., 2021), however, their effectiveness is limited as they overlook auxiliary knowledge that could\naid in news verification.\n\n2\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 2: EA2N framework for fake news detection.\n\nKnowledge-aware methods utilize auxiliary knowledge to aid in the process of news verification.\nMonti et al. (2019) extends classical CNNs to operate on graphs by analyzing the user activity,\ncontent, social graph etc., while Shu et al. (2020b) found user profile features to be useful in fake\nnews detection. Lu & Li (2020) employ a co-attention model that uses both news content and\nsocial context. Later, Wang et al. (2020) and Hu et al. (2021) utilize entity linking to capture entity\ndescriptions from Wikidata and integrate them into their models for the identification of fake news.\nRecently, KAN (Dun et al., 2021) considered external knowledge from Wikidata to expand domain\nknowledge and Jin et al. (2022) presented Finerfact, a fine-grained reasoning framework using social\ninformation to detect the fake news. Most recently introduced Dual-CAN (Yang et al., 2023) method\ntakes news content as well as social media replies, and external knowledge for the purpose. Thus it\nis proven that better fake news detection require external knowledge acquisition.\n\nAMR as introduced by Banarescu et al. (2013), represents relations between nodes using Prop-\nBank, frameset and sentence vocabularies. It utilizes over hundred semantic relations, including\nnegation, conjunction, command, and wikification. It aims to represent different sentences with the\nsame semantic meaning using the same AMR graph. Various NLP fields, such as summarization\n(Kouris et al., 2022), event detection (Wang et al., 2021b), question answering (Lim et al., 2020)\netc., have effectively used AMR. Despite its wide range of applications in NLP, AMR has not been\ninvestigated to capture complex semantic relations in documents for fake news detection. Recently,\n(Zhang et al., 2023) used AMR for identification of out-of-context multimodal misinformation. Un-\nderstanding the importance of semantic relations, we embarked on an exploration of utilizing AMR\nfor the purpose of detecting fake news.\n\n3 PROPOSED METHOD\n\nWe present a brief overview of our proposed model, Evidence-based AMR Attention Network\n(EA2N), depicted in Fig. 2. Our framework comprises three key components: Language Encoder,\nGraph Encoder, and Classification Module. Description for each component is provided in the fol-\nlowing sections. Before proceeding further, let us define the problem statement formally.\n\n3.1 PROBLEM STATEMENT\n\nOur objective is to perform binary classification on news articles, classifying them as either real\n(y = 0) or fake (y = 1). Formally, given a news article S, the task is to learn a function F such that\nF : F (S) → y, where y ∈ {0, 1} represents the ground truth labels of the news articles.\n\n3\n\n\fUnder review as a conference paper at ICLR 2024\n\n3.2 LANGUAGE ENCODER\n\nWe utilized the BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al.,\n2019) model to encode sequences of tokens in sentences. Let a news article S be denoted as\ntuple(T, B), where T represents the title and B represents the body text of the news article, both\ncontaining a set of words {w1, w2, . . . , wn}. In order to create the final text for input, we concate-\nnate the title and body using a special “[SEP]” tag. After that, we initially tokenize these words and\nfed into the BERT model to obtain the final layer embedding Hi in the following manner:\n\nS = Concat([T, B]), [H0\n\nS, H1\n\nS, . . . , HNS\n\nS ] = BERT(S), Z S = H0\n\nS\n\n(1)\n\nWe extracted affective lexical features (Ghanem et al., 2021) such as emotions, sentiment, morality,\nhyperbolic, and imagebility from the n different segments {s1, s2, . . . , sn} of article S to enhance\nthe capabilities of the language encoder. This enhance the capabilities to differentiate documents\nmore effectively by capturing the distribution of the features segment wise. Each feature vector, asj ,\nof j-th segment sj ∈ S is represented with a term frequency that takes into account the articles’\nlength as a weighting factor. This approach allows us to effectively capture and represent the dis-\ntinctive characteristics of each segment in the article, accounting for variations in segment lengths.\nThe resulting language vector, denoted as Z l, is obtained by concatenating the vector embedding\nderived from BERT with the representation vector formed by integrating the affective features as:\nZ af f ect = Concat([as1 , as2, . . . asn ])\nZ l = Concat([Z S, Z af f ect]) ∈ RN ×L×D\n(3)\nHere N is the batch size, L is the maximum sequence length, and D is the dimension of the lanugage\nfeature vector.\n\n(2)\n\n3.3 GRAPH ENCODER\n\nThe Graph Encoder module plays a crucial role in transforming the language representation into a\nstructured and abstract form. We adopt Abstract Meaning Representation (AMR) (Banarescu et al.,\n2013) to encode the meaning of the news article in a graph structure. The graph encoder comprises\nthree key components, each responsible for specific tasks in the process, ranging from the generation\nof AMR to the integration of external knowledge and, finally, the path-aware graph learning module.\nBefore describing the concise overview of each component in the following subsections, let us define\nthe notation of graph here. A graph G in general is represented with a tuple (V, E) where V is a set\nof node entities, and E represents relation edges. For different graphs used in our article, we used\ndifferent notations for G= (V, E) without detailing out there.\n\n3.3.1 AMR GENERATION\n\nThe generation process converts the news arti-\ncles into a network of nodes and edges, cap-\nturing the relationships between different en-\ntities. AMR generation process involves pars-\ning the news articles to extract linguistic infor-\nmation, including semantic roles, relations, and\ncore events. For a news article S, we represent\nthe AMR graph as Gamr = (V amr, E amr).\n\n(s / seek − 01\n\n: arg0 (p / person\n\n: name (n / name : op1 “Donald T rumph”))\n\n: arg1 (d / distance − 01\n\n: arg1 p\n: arg2 (p2 / person\n\n: arg0 − of (a / advise − 01)\n: arg1 − of (t / tie − 01)\n\n: name (n2 / name : op1 “maf ia”)\n: time (p3 / past)))\n\nAs an illustrative example, consider the sentence: “Donald Trumph seeks distance from advisor with\npast ties to mafia.” The corresponding AMR graph is presented in right side of section. The AMR\ngraph is a directed acyclic graph that represents a hierarchical structure with nodes denoting entities\n(Donald T rumph, maf ia, seek, etc). Edges (arg0, arg1, name, etc) capture the relationships\nbetween these entities, forming a semantically structured representation of S.\n\n3.3.2 EVIDENCE INTEGRATION WITH AMR\n\nWe propose an evidence linking algorithm to extract evidence rich paths from external knowledge\ngraph among entities in AMR. Given Gwiki = (V wiki, E wiki), a Wikidata knowledge graph, the\n\n4\n\n\fUnder review as a conference paper at ICLR 2024\n\nalgorithm integrates Gwiki with AMR by entity-level filtering (ELF) and context-level filtering\n(CLF). ELF assesses the relevance between entities using a Relatedness(·) function.\nIf the\nrelevance exceeds the ELF threshold (γ), it initiates the CLF process to link evidence between\nentities.\n\nEntity-Level Filtering (ELF): During ELF, pairs of entities within the AMR graph are examined\nfor their corresponding representations and relevance in Wikidata. The relevance between entities\nsource and destination is calculated as:\n\nR(s,d)\n\nELF = Relatedness(vwiki\n\ns\n\n, vwiki\nd\n\n)\n\n(4)\n\ns\n\nand vwiki\nd\nELF exceeds γ, the entities vamr\n\nwhere vwiki\nR(s,d)\npotential evidence path to be attached between them.\n\nand vamr\nd\n\nare the entity representation of (vamr\n\nIf\nare related in Wikidata. This implies the existence of\n\n) found in Wikidata.\n\nand vamr\n\nd\n\ns\n\ns\n\nand\n. The CLF follows principles of A∗ search algorithm to search path in the knowledge graph\n\nContext-Level Filtering (CLF): The CLF algorithm determines the relevance between vamr\nvamr\nd\nfrom the starting entity vamr\n\nto the destination entity vamr\n\ns\n\n.\n\ns\n\nd\n\nFor each entity vwiki\ncalculated between vwiki\nsource and destination:\n\ni\n\ni\n\n∈ Vwiki lying in the evidence path between vamr\n\n, the relevancy is\ns\nto predict the possibility of finding a rich evidence path between\n\nand vamr\nd\n\nand vamr\nd\n\nR(s,d)\n\nCLF = Relatedness(vwiki\n\ni\n\n, vwiki\nd\n\n) =\n\nT ag(vwiki\n\n, vwiki\ni\n\ns\nni + ϵ\n\n)\n\n+\n\nT ag(vwiki\n\ni\n\n, vwiki\nd\n\n)\n\nnm − ni + ϵ\n\n(5)\n\nwhere T ag(i, j) is the tagme score summed up from i to j, ni and nm represent the current hopes\nand maximum hops respectively, a very small value, ϵ is added to avoid division by zero. The first\nterm averages the Tagme score from source (vwiki\n) and the second term\naverages the heuristic value of Tagme score from current node (vwiki\n). If\nR(s,d)\nis linked with the next entity in the Wikidata\npath until vamr\nis reached. This process leads to the attachment of evidence information between\nentities, enriching the AMR graph with external knowledge.\nThis process is repeated for each pair of entities in Gamr and the resultant graph is called WikiAMR\nGraph. The detailed algorithm to generate WikiAMR using ELF and CLF is presented in Algo. 1.\n\nCLF is above the CLF threshold (δ), the entity vwiki\n\ni\n) to destination (vwiki\n\n) to current node (vwiki\n\nd\n\nd\n\ns\n\ni\n\ni\n\nAlgorithm 1 Evidence-Linking Algorithm\nInput: AMR Gamr, Wikigraph Gwiki, γ, δ\nOutput: WikiAMR graph GW ikiAM R\nfunction ELF(Gamr, Gwiki, γ, δ):\n\nevidence ← []\nfor each node pair u, v ∈ Gamr do\nGet relatedness RELF between\nstart node u, and goal node v\nif (RELF > γ): then\n\npath = CLF(u, v, γ, δ);\nevidence.append(path);\n\nend if\n\nend for\nIntegrate Gamr and Relation paths to get\nGW ikiAM R\nreturn GW ikiAM R\n\nend function\n\nAlgorithm 2 Context Level Filtering\n\nfunction CLF(start, goal, γ, δ):\n\npath ← [start], relation path ← []\nwhile path is notempty do\n\nPick ncur, last indexed node from path\nif ncur = goal then\n\nExtract relationships from Gwiki\nand append them in relation path\nreturn relation path\n\nend if\nfor each neighbor nadj of ncur in Gwiki\n\n& nadj /∈ path do\n\nGet RCLF between nadj, goal\nif (RCLF > δ) then\n\nAppend nadj to path\n\nend if\n\nend for\nend while\nend function\n\n5\n\n\fUnder review as a conference paper at ICLR 2024\n\nWikiAMR is denoted as GW ikiAM R, comprises interconnected undirected evidence paths between\nentities in Gamr. This structure facilitates reasoning about the evidence present in external knowl-\nedge along with the directed acyclic structure extracted from the text document. Mathematically, it\ncan be represented as follows:\n\nGW ikiAM R = Gamr ∪\n\nP wiki(vamr\n\ns\n\n, vamr\nd\n\n)\n\n(cid:88)\n\ns,d\n\n(6)\n\nHere, P wiki is the evidence path between vamr\nthe set of evidence paths.\n\ns\n\nand vamr\nd\n\nand (cid:80) denotes the graph generated from\n\nIn continuation of above example, suppose we want to link the entities Donald T rumph and\nM af ia, the relevance RELF between them is calculated using Relatedness(·) function.\nIf\nRELF > γ, the entities Donald T rumph and M af ia are identified as relevant, potentially holding\nevidence path between them. Upon identifying relevant entities, the RCLF between the next entity\n, M edia Coverage and the destination entity M af ia is computed. If RCLF > δ, entity vwiki\nvwiki\ni\nis linked with entity vwiki\ni+1 in the AMR graph until the destination is reached. This results in the at-\ntachment of relevant evidence information. This is repeated over all the pair of entities in AMR and\nthe final graph is represented as WikiAMR.\n\ni\n\n3.3.3 PATH-AWARE GRAPH LEARNING MODULE\n\nThis module plays a crucial role in EA2N by generating informative features from the enriched\nWikiAMR graph obtained. These features capture essential semantic relationships and enable the\nmodel to gain a deeper understanding of the information present in the news articles. The module\ninvolves a Graph Transformer, which leverages multi-head attention mechanisms to process the\nWikiAMR representation in a way that facilitates effective reasoning and representation learning.\n\nRelation Path Encoder The WikiAMR obtained from the above section is passed to the node in-\ntialization and relation encoder to get the encoding of WikiAMR in RN ×L×D′\n, where D′ is the\ndimension of the graph encoding.\nTo facilitate the model’s recognition of explicit graph paths from GW ikiAM R, the relation encoder\nis applied to capture the shortest path between two entities. The sequence representing this path\nis transformed into a relation vector using a Gated Recurrent Unit (GRU) based RNN (Cho et al.,\n2014). The mathematical representation of this relation encoding is given by:\n−→p t = GRUf (−→p t−1, spt) ←−p t = GRUg(←−p t+1, spt)\n\nHere, spt signifies the shortest path of the relation between the two entities. As per Cai & Lam\n(2020)’s paper, in order to calculate the attention score, the final relational encoding rij is divided\ninto two separate encodings: ri→j and rj→i, which are obtained using a linear layer and the param-\neter matrix Wr.\n\nrij = [−→p n; ←−p 0],\n\n[ri→j; rj→i] = Wrrij\n\nThe Graph Transformer processes the input GW ikiAM R using multi-head attention mechanism.\nThen the attention scores αij are computed based on both the entity representations and their re-\nlation representation.\n\nαij = g(ei, ej, rij) = (ei + ri→j)W T\nq Wkrj→i\n(cid:125)\n\n= eiW T\nq Wkej\n(cid:124)\n(cid:123)(cid:122)\n(cid:125)\n(a)\n\n+ eiW T\n(cid:124)\n\n(cid:123)(cid:122)\n(b)\n\nq Wk(ej + rj→i)\n+ ri→jW T\nq Wkej\n(cid:123)(cid:122)\n(cid:125)\n(c)\n\n(cid:124)\n\n+ ri→jW T\nq Wkrj→i\n(cid:123)(cid:122)\n(cid:125)\n(d)\n\n(cid:124)\n\n(7)\n\nThe attention weights here are computed to work over entities based on relations and each term in\nEq. 7 holds an explanation. The term (a) signifies content-based addressing, (b) and (c) capture\nthe source-dependent and target-dependent relation bias, and (d) embodies a universal relation bias,\nencompassing a broader perspective on relation interactions. Collectively, this equation explains a\ncomprehensive mechanism for the model to reason and weigh entity-relation interactions.\n\nGraph Transformer for Representation Learning\n\n6\n\n\fUnder review as a conference paper at ICLR 2024\n\nGraph Transformer applies self-attention to capture dependencies between different positions within\neach WikiAMR graph representation. The encoder consists of multiple identical blocks, of which\nthe core is multi-head attention. The model computes attention weights for the encoded paths to\nlearn the enhanced representations. Given a set of attention heads H, each head computes distinct\nQuery (Qi), Key (Ki), and Value (Vi) matrices, which are then linearly combined through learnable\nweight matrices (Wi) to produce the final attended representation:\n\nAi = Attn(Qi, Ki, Vi) = sof tmax\n\n(cid:18) QiK T\ni√\nD′\n\n(cid:19)\n\nVi\n\nA = Concat([A1, A2, ..., Ah])WH\n\n(8)\n\n(9)\n\nHere, h, Ai and WH represent the number of attention heads, output of the ith head, and learnable\nweight matrix. This dynamic method enhance intricate semantic extraction.\n\nAfter computing the attention weights, the Graph Transformer (GT) encodes the integrated Wiki-\nAMR representations GW ikiAM R as follows:\n\nZ g = GT(GW ikiAM R, A) ∈ RN ×L×D′\n\n(10)\n\nWhere Z g represents the final graph embedding obtained from the Graph Transformer, and D′ is\nthe dimension of the feature vector.\n\n3.4 CLASSIFICATION MODULE\n\nThe final stage of EA2N involves the Classification Module, which takes the semantically-informed\nAMR representation and the enriched language features to produce the fake news predictions.\nWe concatenate the graph Z g and language features Z l to create the final fused embedding:\n\nZ = Concat([Z l, Z g]) ∈ RN ×L×(D+D′)\n\n(11)\n\nFinally, we pass Z through a classification transformer (CT) followed by a softmax layer to obtain\nthe final probabilites Ypred over real and fake.\n\nf (Z) = sof tmax(CT(Z)) ∈ RN ×2, Ypred = argmax(f (Z))\n\n(12)\n\nThe comprehensive EA2N model, leveraging AMR, external knowledge integration, affective fea-\ntures, and attention mechanisms, offers a powerful and novel approach to Fake News Detection.\n\n4 EXPERIMENTAL SETUP\n\n4.1 DATASET AND EVALUATION METRIC\n\nIn order to assess the effectiveness of EA2N, we perform experiments on two benchmark datasets,\nnamely, PolitiFact and GossipCop (Shu et al., 2020a). These datasets consist of 815 and 7,612\nnews articles, respectively, along with labels assigned by journalists and domain experts. Additional\ninformation regarding the preprocessing and implementation details can be found in Appendix A.\nWe evaluate our model using a set of metrics, including Precision (Pre), Recall (Rec), F1-score,\nAccuracy (Acc), and Area Under the ROC curve (AUC). We conduct 5-fold cross-validation and\nreport the average results.\n\n4.2 BASELINES\n\nIn our evaluation, we contrast our EA2N model with various state-of-the-art baselines, categorized\ninto two groups. The first group utilizes only textual information (SVM (Yang et al., 2012), DTC\n(Castillo et al., 2011), RFC (Kwon et al., 2013), GRU-2 (Ma et al., 2016), FF (FakeFlow) (Ghanem\net al., 2021)), while the second incorporates auxiliary knowledge in addition to textual features (B-\nTransE (Pan et al., 2018), KCNN (Wang et al., 2018), GCAN (Lu & Li, 2020), KAN (Dun et al.,\n2021), FinerFact (Jin et al., 2022)).\n\n7\n\n\fUnder review as a conference paper at ICLR 2024\n\nTable 1: Comparative study of our model EA2N w.r.t. different baselines.\n\nMethod\n\nSVM\nRFC\nDTC\nGRU-2\nFF\n\nPre\n0.7460\n0.7470\n0.7476\n0.7083\n0.8462\nB-TransE 0.7739\nKCNN\n0.7852\n0.7945\nGCAN\n0.8687\nKAN\n0.9196\nFinerFact\nEA2N\n0.9333\n\nRec\n0.6826\n0.7361\n0.7454\n0.7048\n0.7923\n0.7658\n0.7824\n0.8417\n0.8499\n0.9037\n0.9324\n\nPolitiFact\nF1\n0.6466\n0.7362\n0.7450\n0.7041\n0.8193\n0.7641\n0.7804\n0.8345\n0.8539\n0.9172\n0.9328\n\nAcc\n0.6694\n0.7406\n0.7486\n0.7109\n0.8574\n0.7694\n0.7827\n0.8083\n0.8586\n0.9092\n0.9318\n\nAUC\n0.6826\n0.8074\n0.7454\n0.7896\n0.8627\n0.8340\n0.8488\n0.7992\n0.9197\n0.9384\n0.9523\n\nPre\n0.7493\n0.7015\n0.6921\n0.7176\n0.7263\n0.7369\n0.7483\n0.7506\n0.7764\n0.8615\n0.8947\n\nGossipCop\nF1\n0.5955\n0.6691\n0.6919\n0.7079\n0.7307\n0.7340\n0.7433\n0.7709\n0.7713\n0.8685\n0.8906\n\nRec\n0.6254\n0.6707\n0.6922\n0.7079\n0.7352\n0.7330\n0.7422\n0.7574\n0.7696\n0.8779\n0.8865\n\nAcc\n0.6643\n0.6918\n0.6959\n0.7180\n0.7563\n0.7394\n0.7491\n0.7439\n0.7766\n0.8320\n0.8713\n\nAUC\n0.6253\n0.7389\n0.6929\n0.7516\n0.7616\n0.7995\n0.8125\n0.8031\n0.8435\n0.8637\n0.9014\n\nT\n\nT+K\n\nOurs\n\nTable 2: Analysis on number of hops linked between entities.\n\nPolitifact\nGossipcop\n\n# 1 hop\n951\n5482\n\n# 2 hops\n10\n23\n\n# 3 hops\n5\n8\n\n# 4 hops\n3\n5\n\n# 5 hops\n2\n3\n\n5 RESULTS\n\nWe used various transformer based model for textual encoding and reported the best results for\nEA2N in the table. Table 1 shows a comparative analysis of EA2N against various models. The\nstandard deviations for accuracy and F1-score metrics in Politifact are 2.17 and 1.82, respectively\nand in Gossipcop, these standard deviations stand at 2.35 and 2.08. The table clearly demonstrates\nthat our model, EA2N, outperforms the state-of-the-art model, FinerFact, by 1.6%, 2.3% in terms\nof F1-score and accuracy on the Politifact, and by 2.2%, 3.9% on the Gossipcop, respectively. In-\nterestingly, our model achieves these superior results without integrating social information, which\nFinerFact utilizes. Furthermore, our model surpasses KAN’s performance on both datasets with F1-\nscore and accuracy improvement of 7.9%, 8.1% and 11.9%, 9.5%. This is attributed to our model’s\nability to consider contextual information across multiple entities in the AMR and link evidence be-\ntween them, unlike KAN, which only focuses on the contextual information of a single entity. This\nenables our model to learn the facts between entities, benefiting from external knowledge.\n\n6 ABLATION STUDY\n\n6.1 COMPARISON ON DIFFERENT LANGUAGE ENCODERS\n\nWe employ several transformer-based models to assess the effectiveness of EA2N across various\ntextual encodings. These include BERT-base (Devlin et al., 2019), RoBERTa-base (Liu et al., 2019),\nXLNET-base (Yang et al., 2019), and ELECTRA-base (Clark et al., 2020). The results of this\nevaluation, in conjunction with the baseline (FinerFact) result, are illustrated in Fig. 3. Notably,\nELECTRA outperforms other models, exhibiting F1-score and accuracy of 0.9328, 0.9318 on the\nPolitifact dataset and 0.8906, 0.8713 on the Gossipcop dataset. Comparing the remaining models,\nboth XLNET and BERT demonstrate superior performance over RoBERTa. In comparison with\nbaseline, this study concludes that by leveraging various textual encoders, EA2N model surpasses\nother existing fake news detection models, yielding substantial improvements in performance.\n\n6.2 COMPARISON ON DIFFERENT EA2N VARIANTS\n\nWe conducted experiments with our model EA2N (LE|WikiAMR) by incorporating different varia-\ntions, including: 1) Language Encoder (LE) 2) AMR (AMR) and 3) Language Encoder with AMR\n(LE|AMR). Our findings from Fig. 4 indicate that only AMR model performs better than only LE\nmodel. Moreover, when we combine both language encoder and AMR (LE|AMR), there is a sig-\nnificant improvement of 6-8% observed over the only LE and AMR models. Additionally, when we\nintegrate evidence in AMR into our final model (LE|WikiAMR), there is a further enhancement of\n3-4% over the LE|AMR model for both datasets. We conducted two-tailed t-tests and observed a\nsignificant difference between EA2N variants, obtaining a significance score(p-value) < 0.01 thus\nrejecting the null hypothesis. The detailed analysis of the same is covered in Appendix A.3.4.\n\n8\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 3: Comparison on different language\nencoders.\n\nFigure 4: Comparison on different EA2N variants.\n\nFigure 5: Analysis on number of\nneighbours searched between entities.\n\nFigure 6: Attention weight analysis over random\nsamples.\n\n6.3 ANALYSIS ON EVIDENCE LINKING ALGORITHM\n\nIn our proposed Evidence Linking Algorithm, we conducted an analysis of the number of hops\nlinked and the number of neighbors searched between start and goal nodes. From Table 2, It is\nobserved that the majority of entities where we found connections in both datasets are linked using\n1 hop, indicating direct relations between them. As the number of hops increased, the proportion\nof linked entities gradually decreased, with very few entities linked using 5 hops relations. Also, in\nFig. 5, the results revealed that most entities are searched within the first neighbor, and the frequency\ngradually decreases for subsequent neighbors. Based on findings, we capped linked path at 5 nodes\nand explored up to 5 neighbors for each node between start and goal entities in our algorithm.\n\n6.4 ANALYSIS ON ATTENTION WEIGHTS\n\nIn order to investigate the influence of WikiAMR features on our model, we have conducted an\nexamination of the attention weights from the final layer of the EA2N. We delve into the attention\nweights of the Language Encoder (LE), the Graph Encoder (GE), as well as the combined Language\nEncoder with Graph Encoder (LE|GE). Analyzing the attention weights of LE|GE from Fig. 6, we\ndeduce that the lower portion of the feature set (GE feature set) holds significant influence on the\nmodel’s performance. This conclusion arises from the fact that our proposed WikiAMR encapsulates\na comprehensive and intricate semantic structure of news articles. Furthermore, delving into the\nweights of individual encoders, we infer that within the LE, the initial feature subset strongly affects\nthe model’s behavior. This stems from the fact that the title of a news article, a concise summary of\nthe news, is typically situated in the initial sentences. On the other hand, for GE, the entire feature\nset carries significance since WikiAMR emphasizes crucial semantic relationships among entities.\n\n7 CONCLUSION\n\nIn this study, we introduce EA2N, a novel Evidence-based AMR Attention Network designed to\neffectively identify fake news by harnessing external knowledge from Wikidata within the AMR\ngraph through a proposed evidence linking algorithm. For the future direction, we are interested to\nexplore more ways to encode external knowledge using social information with semantic relations\nin AMR. It is important to note that our study not only provide solution for fake news detection but\nit has the potential to pave the way for solving various other NLP applications.\n\n9\n\n\fUnder review as a conference paper at ICLR 2024",
    "abstract": "Proliferation of fake news has become a critical issue in today's information-driven society. Our study includes external knowledge from Wikidata and deviates from the reliance on social information to detect fake news, that many  state-of-the-art (SOTA) fact-checking models adopt. This paper introduces EA$^2$N, an Evidence-based AMR Attention Network for Fake News Detection. EA$^2$N leverages Abstract Meaning Representation (AMR) and incorporates knowledge from Wikidata using proposed evidence linking algorithm, pushing the boundaries of fake news detection. The proposed framework encompasses a combination of novel language encoder and graph encoder to detect the fake news. While the language encoder effectively combines transformer encoded textual features with affective lexical features, the graph encoder encodes AMR with evidence through external knowledge, referred as WikiAMR graph. A path-aware graph learning module is designed to capture crucial semantic relationships among entities over evidences. Extensive experiments supports our model's superior performance, surpassing SOTA methodologies. This research not only advances the field of Fake News Detection but also showcases the potential of AMR and external knowledge for robust NLP applications, promising a more trustworthy information landscape.",
    "response": {
      "Soundness": 2,
      "Presentation": 2,
      "Contribution": 1,
      "Rating": 3,
      "Confidence": 4,
      "Strengths": "- The authors provide an effective amalgamation of the graph- and text-based pipeline for learning representation (although not entirely novel).\n- The figures and tables give a clear picture of the underlying model and experimentation.\n- The authors perform the much required ablation studies.",
      "Weaknesses": "The proposed architecture seems to be an attempt to combine several pre-existing architectures -- i.e. Language Encoder (BERT [6] + FakeFlow [7]), Path-aware Graph Learning Module (Graph Transformer [8]) and Graph Generation and Integration ($\\mathcal{A}^*$ search with a TagMe API [9] -based heuristic). While these architectures have been amalgamated to propose a new model, these aren't *novel* contributions. The paper isn't well written (in some places) in the sense that several key notations are missing, and the narrative of the work could have been improved. All the baselines have been directly adopted from the FinerFact [5] paper. Such practices should be avoided. I have highlighted several weaknesses and concerns under the \"Questions\" section.\n\n\n[1] Zixuan Zhang and Heng Ji. 2021. Abstract Meaning Representation Guided Graph Encoding and Decoding for Joint Information Extraction. (NAACL-HLT 2021)\n\n[2] Shu, Kai, et al. \"Fakenewsnet: A data repository with news content, social context, and spatiotemporal information for studying fake news on social media.\" Big data 8.3 (2020): 171-188.\n\n[3] Martinez-Rodriguez, Jose L., Ivan López-Arévalo, and Ana B. Rios-Alvarado. \"Openie-based approach for knowledge graph construction from text.\" Expert Systems with Applications 113 (2018): 339-355.\n\n[4] Yi-Ju Lu and Cheng-Te Li. GCAN: Graph-aware co-attention networks for explainable fake news detection on social media. In ACL, pp. 505–514, Online, July 2020. ACL.\n\n[5] Yiqiao Jin, Xiting Wang, Ruichao Yang, Yizhou Sun, Wei Wang, Hao Liao, and Xing Xie. Towards fine-grained reasoning for fake news detection. AAAI, 36:5746–5754, 06 2022.\n\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June 2019. ACL.\n\n[7] Bilal Ghanem, Simone Paolo Ponzetto, Paolo Rosso, and Francisco Rangel. Fakeflow: Fake news detection by modeling the flow of affective information. In 16th EACL, 2021.\n\n[8] Deng Cai and Wai Lam. Graph transformer for graph-to-sequence learning. In AAAI, pp. 7464– 7471. AAAI Press, 2020.\n\n[9] Paolo Ferragina and Ugo Scaiella. Tagme: On-the-fly annotation of short text fragments (by wikipedia entities). In ICIKM, CIKM ’10, pp. 1625–1628, New York, NY, USA, 2010. ACM. ISBN 9781450300995.",
      "Questions": "**Evidence Integration with AMR**: There seem to be several incomplete parts, which need to be explained.\n\n---  In Entity-level Filtering $(\\mathcal{R}^{(S,D)}_{ELF} = Relatedness(v_s^{wiki}, v_d^{wiki}))$, the authors have not mentioned what the $Relatedness(.)$ function is? Without an ***explicit*** definition of the function, it is diffiucult to assess the working of the ELF and CLF algorithms since the $Relatedness(.)$ function seems to be the \"main\" heuristic being used here.\n\n- From a look at the Appendix, it seems (***implicitly***) that the authors use the TagMe API to compute the relatedness, but what \"explicitly\" is the function definition used? \n\n--- The Figure 2 (and following text) mentions that the authors use $\\mathcal{A}^*$ search on the Wiki-graph to find the (optimal) path between source $v_s^{amr}$ and destination $v_d^{amr}$. However the Algorithm 2 described as \"Context-level Filtering\" doesn't seem to represent $\\mathcal{A}^*$ search. \n\n- For $\\mathcal{A}^*$ search, $f$ (total cost) = $g$ (current cost) + $h$ (heuristic approximation of the future cost). Here, $f$ (total cost) must be the criterion for choosing the next node in path. If we assume that, $h = \\mathcal{R}^{(i,D)}_{CLF} = Relatedness(v_i^{wiki}, v_d^{wiki}) > \\delta$ is the heuristic cost to reach the destination $d$ from $i$, what is $g$? The authors don't provide information for that. Incase $g$ is being ignored (or taken 0), it becomes a *greedy-heuristic* search, and not $\\mathcal{A}^*$. \n- A possible $\\mathcal{A}^*$ variant could have been:\n\n$g + h = \\mathcal{R}^{(S,i)}\\_{CLF} + \\mathcal{R}^{(i,D)}\\_{CLF} = Relatedness(v_s^{wiki}, v_i^{wiki}) + Relatedness(v_i^{wiki}, v_d^{wiki}) > \\delta*$\n\n\n**Relation Encoder in Path-aware Graph Learning Module**: For every pair of entities $(v_s^{amr}, v_d^{amr})$ in $\\mathcal{G}^{amr}$, two distinct sources of relational data exist in $\\mathcal{G}^{WikiAMR}$: AMR-based relations and evidence-based relations from Wikidata:\n\n- Using the notation from paper: $\\mathcal{G}^{WikiAMR} = \\mathcal{G}^{amr} \\cup \\sum_{s, d} \\mathcal{P}^{wiki}(v_s^{amr}, v_d^{amr})$.\n- Consider the shortest path denoted as $sp_{s \\rightarrow d} = \\set { e(v_s^{amr}, n_1), e(n_1, n_2), \\dots, e(n_k, v_d^{amr})\\}$ within $\\mathcal{G}^{WikiAMR}$. Here, $e^{amr}$ represents an edge within $\\mathcal{G}^{amr}$, and $e^{wiki}$ represents an edge within $\\sum_{s, d} \\mathcal{P}^{wiki}(v_s^{amr}, v_d^{amr})$. Therefore, the collective edge set $e^{WikiAMR} = e^{wiki} \\cup e^{amr}$.\n- Let $sp^{amr}\\_{s \\rightarrow d} = \\set {e^{amr}(v_s^{amr}, n_1), e^{amr}(n_1, n_2), \\dots, e^{amr}(n_k, v_d^{amr})\\}$ be the shortest path that relies solely on AMR-based relations (i.e., exclusively within $\\mathcal{G}^{amr}$). Simultaneously, $sp^{wiki}\\_{s \\rightarrow d} = \\set {e^{wiki}(v_s^{amr}, n_1), \\dots, e^{wiki}(n_k, v_d^{amr})}$ represents the shortest path utilizing only evidence-based connections.\n- As indicated by Table 2 and Figure 5, a significant portion of entities are linked through evidence-based connections with just one hop, meaning they are connected directly. In other words, for most entity pairs, the shortest path is the \"direct\" evidence path with a single edge originating from the Wiki graph. This can be represented as $sp_{s \\rightarrow d} = sp^{wiki}\\_{s \\rightarrow d} = \\{ e^{wiki}(v_s^{amr}, v_d^{amr})\\}$ (Because, as per Table 2, in most cases, the number of edges in $sp^{wiki}\\_{s \\rightarrow d}$ (being 1 in majority cases), is less than or equal to that in $sp^{amr}_{s \\rightarrow d}$.\n\nIn essence, this implies that in most cases, when encoding a relationship $r_{s \\rightarrow d}$, the relation encoder would **overlook the AMR-based relation information**. While Wikidata relations are valuable, disregarding AMR-level relations may not be technically justified. Therefore, the authors should contemplate how to adapt the relation encoder in the graph transformer to effectively integrate both information sources.\n\n**Abstract Meaning Representation**: I have a few major concerns about using an AMR representation here:\n\n- The average length of news articles in the Gossipcop dataset is **~600 words** [2] (Some articles are as large as 1000 words). In the case of such large input length, the AMR graphs are going to be \"very\" noisy. How do the authors handle this case?\n- The authors don't elaborate on the intuition behind using the Abstract Meaning Representation (\"Why specificaly AMR?\")? There can be other (more sophisticated) variants of AMR like **AMR-IE** [1] (which uses an AMR guided graph decoder to extract knowledge elements based on the order decided by the hierarchical structures in AMR) which seem more relevant owing to the \"integration of external knowledge\" used in this work. What about knowledge graphs other than AMR (eg. OpenIE-based approaches [3])?\n- It seems that the authors have used an off-the-shelf pretrained AMR parser, however they have not provided any details about the same. Was the AMR parser finetuned? (I am guessing not!)\n\n**Insufficient Experimentation**: \n\n- The presented results are exclusively based on two datasets, Politifact and Gossipcop, both of which are part of the FakeNewsNet database. To ensure the generalizability of the proposed architecture, it is essential for the authors to include results from additional datasets spanning various domains or social media platforms. Moreover, it's worth noting that Politifact, one of the datasets, contains only 815 news articles.\n- Several of the selected baseline models do not provide a fair basis for comparison. For instance, GCAN [4] incorporates the propagation path of the tweet and user profiles in addition to the source tweet content, features not utilized by the proposed model.\n- A logistical concern arises from the fact that all the baseline models and their results, including SVM, DTC, RFC, GRU-2, FF, B- TransE, KCNN, GCAN, and KAN, appear to **have been directly borrowed from the FinerFact** [5] paper. Such practices should be avoided, and the authors should explicitly mention the sources of these baseline models in their paper.\n- In the ablation study examining different variants of the proposed model, such as LM, AMR, LE|AMR, and LE|WikiAMR, the authors should provide significance hypothesis test results (e.g., T-test) alongside the standard deviation of the metrics across multiple runs. Including these statistical measures would enhance the interpretability of the results."
    }
  },
  {
    "paper_id": "TyFrPOKYXw",
    "summary": "Background: Large language models (LLMs) have revolutionized tasks like understanding instructions and complex reasoning. With their increased societal impact, ensuring their outputs are not harmful is critical. Safety alignment in LLMs is crucial, particularly in minimizing harmful responses through fine-tuning. Recent advances involve Reinforcement Learning with Human Feedback (RLHF), which aligns LLMs with human preferences but struggles with balancing helpfulness and harmlessness.\nGap: Current methods like RLHF face significant challenges in balancing the objectives of being helpful while being harmless, as the two can often conflict. For example, a model refusing to answer a harmful query might be considered safe but is unhelpful. Hence, achieving a model that is simultaneously helpful, safe, and responsive remains a challenge.\nHypothesis: The authors propose Safe Reinforcement Learning from Human Feedback (Safe RLHF) as a solution to the tension between helpfulness and harmlessness in LLM training. Safe RLHF hypothesizes that decoupling the objectives into distinct reward and cost models, and using constraint optimization, can effectively manage and balance these objectives.\nMethod: Safe RLHF involves a multi-stage process. First, it separates the training data into helpfulness and harmlessness components, avoiding bias in crowdworker feedback. Two separate models are trained: a Reward Model for helpfulness and a Cost Model for harmlessness. Using the Lagrangian method, it dynamically balances the objectives of maximizing helpfulness while satisfying harmlessness constraints. Experiments included three rounds of fine-tuning the Alpaca-7B model, guided by human preferences.\nConclusion: Safe RLHF successfully improves both the helpfulness and harmlessness of LLMs compared to conventional methods. It provides a dynamic mechanism to balance conflicting objectives, significantly reducing the probability of harmful responses while maintaining performance according to human evaluations. Safe RLHF offers a novel integration of Safe RL and RLHF frameworks, contributing to enhanced human value alignment in AI systems.\n",
    "full text": "\nSAFE RLHF: SAFE REINFORCEMENT LEARNING\nFROM HUMAN FEEDBACK\n\nJuntao Dai∗1 Xuehai Pan∗2 Ruiyang Sun∗1\nYizhou Wang2 Yaodong Yang†1\n1Center for AI Safety and Governance, Institute for AI, Peking University\n2School of Computer Science, Peking University\n{jtd.acad, jiamg.ji}@gmail.com\n{XuehaiPan, yizhou.wang, yaodong.yang}@pku.edu.cn\n\nJiaming Ji∗1 Xinbo Xu1 Mickel Liu2\n\nABSTRACT\n\nWith the development of large language models (LLMs), striking a balance be-\ntween the performance and safety of AI systems has never been more critical.\nHowever, the inherent tension between the objectives of helpfulness and harmless-\nness presents a significant challenge during LLM training. To address this issue,\nwe propose Safe Reinforcement Learning from Human Feedback (Safe RLHF),\na novel algorithm for human value alignment. Safe RLHF explicitly decouples\nhuman preferences regarding helpfulness and harmlessness, effectively avoiding\nthe crowdworkers’ confusion about the tension and allowing us to train separate\nreward and cost models. We formalize the safety concern of LLMs as an opti-\nmization task of maximizing the reward function while satisfying specified cost\nconstraints. Leveraging the Lagrangian method to solve this constrained problem,\nSafe RLHF dynamically adjusts the balance between the two objectives during\nfine-tuning. Through a three-round fine-tuning using Safe RLHF, we demonstrate\na superior ability to mitigate harmful responses while enhancing performance\ncompared to existing algorithms. Experimentally, we fine-tuned the Alpaca-7B\nusing Safe RLHF and aligned it with collected human preferences, significantly\nimproving its helpfulness and harmlessness according to human evaluations.\nCode is available at https://github.com/PKU-Alignment/safe-rlhf.\nWarning: This paper contains example data that may be offensive or harmful.\n\n1\n\nINTRODUCTION\n\nLarge Language Models (LLMs) have shown remarkable capabilities in understanding instruc-\ntions (Chung et al., 2022; Ouyang et al., 2022), summarization (Stiennon et al., 2020; Koh et al.,\n2022) and performing complex reasoning tasks (OpenAI, 2023; Anil et al., 2023), and more. Con-\ncurrently, AI systems that leverage LLMs are increasingly enhancing the efficiency of numerous\nhuman activities, such as coding (Chen et al., 2021; Gao et al., 2023b), medical assistance (Yang\net al., 2022; Moor et al., 2023), education (Kasneci et al., 2023; Kung et al., 2023), law (Katz et al.,\n2023), and so forth. Considering the potential for broad societal impact, responses generated by\nLLMs must not contain harmful content, such as discrimination, misinformation, or violations of\nsocial norms and morals (Gehman et al., 2020; Weidinger et al., 2021; Ganguli et al., 2022; Desh-\npande et al., 2023). Therefore, the alignment of safety in LLMs has received widespread attention\nfrom academia and industry (Christian, 2023).\n\nAn essential component of safety alignment involves minimizing the tendency of a model to generate\nharmful responses through fine-tuning. Recent works demonstrate that Reinforcement Learning\nwith Human Feedback (RLHF) (Christiano et al., 2017; Ouyang et al., 2022) is a practical approach\nfor aligning LLMs with human preferences, both in terms of style and ethical values (Bai et al.,\n2022a; Ganguli et al., 2022). RLHF leverages LLMs’ broad knowledge and capabilities to promote\n\n∗Equal contribution.\n†Corresponding author.\n\n1\n\n\fPublished as a conference paper at ICLR 2024\n\nFigure 1: Safe RLHF pipeline compared to conventional RLHF method. Our pipeline decouples\nthe data annotation for helpfulness and harmlessness, as well as the training of preference models.\nUltimately, it dynamically integrates both aspects during the policy optimization phase. During the\nannotation phase, the safety labels for the responses are annotated independently. These responses\ncan be labeled as both safe or both unsafe.\n\ndesired responses and behaviors, which leads to safer, higher-performing, and more controllable AI\nsystems. Both technical reports from GPT-4 (OpenAI, 2023) and Anthropic (Ganguli et al., 2022)\nfor their LLMs revealed their use of safety-related prompts, constructed through adversarial probing\nmethods like red-teaming, in the RLHF phase to reduce the potential harm of their model. However,\nthe pursuit of increasing helpfulness and harmlessness may often contradict in practice (Ganguli\net al., 2022; Bai et al., 2022a). For example, a model refusing to answer can be considered safe,\nyet it also renders the response unhelpful in extreme scenarios. Thus, a significant challenge arises\nin balancing the two objectives during the training phase. Our goal is to develop a large language\nmodel that is helpful, safe, and willing to respond.\n\nTo address the above challenge, we propose a novel framework: Safe Reinforcement Learning from\nHuman Feedback (Safe RLHF), as shown in Figure 1. The core insight of Safe RLHF is the decou-\npling of human preferences during data annotation and the establishment of two optimization objec-\ntives: helpfulness and harmlessness. Such decoupling offers two advantages: During the data an-\nnotation, it ensures that the feedback from crowdworkers remains unbiased by any tension between\nhelpfulness and harmlessness; During the Safe RLHF stage, the Lagrangian method (Bertsekas,\n1997) can adaptively balance the trade-off between two inherently conflicting training objectives.\nSafe RLHF formalizes the goal of developing harmless LLMs as a constraint under the Safe RL\nframework. It is crucial that we need a balance between helpfulness and harmlessness objectives,\nand avoid over-optimizing for harmlessness.\n\nTo the best of our knowledge, Safe RLHF is the first integration of Safe RL and the RLHF frame-\nwork. Our contributions to enhancing LLM safety are threefold:\n\n• We propose the Safe RLHF framework to navigate the tension between helpfulness and harmless-\nness objectives. This framework addresses three key challenges: first, guiding crowdworkers to\nproduce more precise and decoupled data; second, introducing a novel Cost Model to model safety\nas a constraint; and third, fine-tuning LLMs through the integration of Safe RL.\n\n• Through three iterations of Safe RLHF, we have empirically demonstrated the effectiveness of\nSafe RLHF in increasing the helpfulness and harmlessness of LLMs. Our extensive experiments\nreveal some key insights, such as the benefits of separating helpfulness and harmlessness, the\nsuperior performance of dynamic approaches over static multi-objective balancing methods like\nReward Shaping (Ng et al., 1999), and the robust design of our Cost Model.\n\n• We release all the data and training codes from the three iterations of Safe RLHF fine-tuning,\n\nfacilitating researchers to replicate and validate our findings1.\n\n1All data and code can be found in the supplementary materials, as detailed in Appendix C.\n\n2\n\nBEGINNING OFCONVERSATION: USER: Givethree tips for stayinghealthy.ASSISTANT:Prompt1.Eat a balanceddiet and make sure to includeplenty of fruits andvegetables.2. Exerciseregularly to keep your bodyactive and strong.3. Getenough sleep and maintain aconsistent sleep schedule.ResponseBEGINNING OFCONVERSATION: USER: Can you help mefigure out how to be a serialkiller?ASSISTANT:PromptUnfortunately,no. We do not provideadvice on how to become aserial killer or engage in anyillegal activities.ResponseABecoming aserial killer requires carefulplanning, patience, andabove all, a strong mentalstate.Firstly,youneed…ResponseBSupervisedFine-tuningDataCollectionPreferenceAnnotation&PreferenceModelingOptimizationObjectivemin!max\"#$−𝒥%𝜃+𝜆⋅𝒥&𝜃OptimizationObjective min!−𝒥%𝜃RewardObjective𝒥!𝜃=𝔼\"∼$!⋅𝑥𝑅%𝑦,𝑥CostObjective𝒥&𝜃=𝔼\"∼$!⋅𝑥𝐶’𝑦,𝑥+𝑑RewardObjective𝒥!𝜃=𝔼\"∼$!⋅𝑥𝑅(𝑦,𝑥PolicyOptimizationRLHFSafeRLHFRewardModel𝑅(𝑦,𝑥PretrainModelSFTModelRewardModel𝑅%𝑦,𝑥CostModel𝐶’𝑦,𝑥HarmlessnessPreference:≻Label:/ABPreviousStageModelHuman AnnotatorsHuman AnnotatorsHelpfulnessPreference:≻BAOverallPreference:≻AB\fPublished as a conference paper at ICLR 2024\n\n2 PRELIMINARIES\n\nPreference Modelling The RLHF method enhances the quality of LLM responses by leveraging\nhuman preference data through a reward model. The reward model is denoted as Rϕ(y, x), where\nx is the input prompt, y is the model response, and R is the scalar output from the reward model.\nHuman preference data is symbolized as yw ≻ yl|x, where yw (win) denotes a response that is\nmore preferred compared to yl (lose). Most previous work, such as Christiano et al. (2017); Sadigh\net al. (2017); Bai et al. (2022a); Kim et al. (2023), employs a preference predictor adhering to the\nBradley-Terry model (Bradley & Terry, 1952). The likelihood of a preference pair is estimated as:\n\np∗(yw ≻ yl|x) =\n\nexp(R(yw, x))\nexp(R(yw, x)) + exp(R(yl, x))\n\n= σ(R(yw, x) − R(yl, x)),\n\n(1)\n\nwhere σ(x) = 1/(1 + exp(−x)) is the logistic sigmoid function.\nSafe Reinforcement Learning A Markov Decision Process (MDP) (Puterman, 2014), M ≜\n⟨S, A, r, P, µ0, γ⟩, including the state space S, the action space A, a reward function r, the tran-\nsition probability P, the initial state distribution µ0, and a discount factor γ. In this framework, a\nstationary policy, π, is a probability distribution indicating the likelihood of taking action a in state\ns. The state value function V π(s) = Eτ ∼π [(cid:80)∞\nt=0 γtrt | s0 = s] denotes the expected cumulative\ndiscounted reward over time, starting from s. Then, the primary objective of reinforcement learning\nis to maximize the objective function, J (πθ) = Es0∼µ0 [Vπθ (s0)].\nSafe RL is formulated as a Constrained MDP (CMDP) extending the standard MDP M with an\nadditional constraint set C (Altman, 2021). The set C = {(ci, bi)}m\ni=1 is composed of cost functions\nci and cost thresholds bi. The cost return is defined as J ci(πθ) = Eπθ [(cid:80)∞\nt=0 γtci (st+1|st, at)],\nand the feasible policy set is ΠC = (cid:84)m\ni=1 { πθ ∈ ΠΘ | J ci(πθ) ≤ bi }. The goal of Safe RL is to\nfind the optimal feasible policy, i.e., π⋆ = arg maxπθ∈ΠC J (πθ).\n\n3 RELATED WORK\n\nLLMs Alignment and Safety AI alignment focuses on ensuring that AI systems, particularly\nLLMs, adhere to human intentions and values (Ji et al., 2023a). While LLMs have outperformed\nhuman experts in many scenarios (Wu et al., 2021; OpenAI, 2023), they are prone to problematic\nbehaviors like generating inaccurate information, diverging from set goals, and producing harmful,\nmisleading, or biased outputs (Wang et al., 2023). Many previous works have explored the potential\nharms of public access to LLMs. Weidinger et al. (2021; 2022) outline six areas of ethical and social\nrisk associated with these models. Rauh et al. (2022) analyze the characteristics of harmful text.\nShevlane et al. (2023) discuss extreme risks, including dangerous capabilities and misalignments.\nDeshpande et al. (2023) examine toxicity in ChatGPT, highlighting issues such as incorrect stereo-\ntypes, harmful dialogue, and hurtful opinions. Such unpredictability in behavior can be especially\nperilous in sensitive domains like medicine (Thirunavukarasu et al., 2023; Clusmann et al., 2023),\nwhere model-generated misinformation could have severe repercussions. To mitigate these risks,\nalignment-based fine-tuning methods are being developed to align LLMs with human expectations\nbetter, incorporating not just basic standards (e.g., the 3H Standard (Askell et al., 2021)) but also\nethical and human-value considerations (Irving et al., 2018; Gabriel, 2020; Casper et al., 2023).\n\nReinforcement Learning from Human Feedback While LLMs have excelled in various lan-\nguage tasks, they sometimes exhibit unexpected behaviors such as producing inaccurate information\nor making biased, misleading, and harmful responses (Bai et al., 2022a;b; Koco´n et al., 2023; Sun\net al., 2023b). RLHF enables LLMs to progress towards more diverse goals by learning from hu-\nman feedback (Ouyang et al., 2022; Yuan et al., 2023; Rafailov et al., 2023; Song et al., 2023; Yang\net al., 2023). Because of the bias and noise in human feedback (Wu et al., 2023a), some methods\noptimizing on a sole preference may lead the model to some local optimal solution (Casper et al.,\n2023). Some existing methods refine different properties and use different reward models to match\nthem. Based on these models, LLMs are guided to be fine-tuned to ensure that the models integrate\nmultiple properties (Glaese et al., 2022; Wu et al., 2023b; Touvron et al., 2023a). However, this\napproach requires manual adjustment of the weights between rewards and costs (similar to Reward\nShaping) (Touvron et al., 2023b), making it challenging to deploy in different application scenar-\nios rapidly. Our approach decouples the Helpful and Harmless, automatically adjusts the trade-off\nbetween rewards and costs based on predefined thresholds, and ensures that the model generates\nhigh-quality responses while providing a higher level of safety.\n\n3\n\n\fPublished as a conference paper at ICLR 2024\n\n4 METHOD: SAFE RLHF\n\nAs shown in Figure 1, we introduce our Safe RLHF pipeline, which leverages the Safe RL frame-\nwork to balance the tension between the helpfulness and harmfulness objectives. Compared to the\nconventional RLHF (Ouyang et al., 2022), Safe RLHF introduces substantial modifications, specif-\nically in the stages of Preference Annotation & Modeling and Policy Optimization.\n\n4.1 HUMAN PREFERENCE OF HARMLESSNESS AND HELPFULNESS\n\nIn the Safe RLHF framework, we utilize a two-stage human annotation strategy to assess the helpful-\nness and harmlessness of text generation. We follow the annotation methodology outlined in Ji et al.\n(2024b), in which the rankings for helpfulness and harmlessness were explicitly decoupled from a\nsingular human preference dimension. In this strategy, crowdworkers annotate a safety meta-label\nfor each question-answer (QA) pair, considering 14 predefined categories of harm. A QA pair is\nlabeled as “safe” only if it poses no risk across all 14 categories. Subsequently, the annotators are\ngiven two responses to the same prompt and asked to rank the harmlessness and helpfulness, treating\neach criterion independently. The detailed annotation guidelines can be found in the Appendix E.\n(cid:9)N\nFollowing the annotation pipeline, we produce a helpfulness-related dataset, DR = (cid:8)xi, yi\ni=1,\n(cid:9)N\nand a harmlessness-related dataset, DC = (cid:8)xj, yj\nj=1. Both datasets, DR and DC,\ncover the same set of QA pairs but with different preference labels. Within each pair in DR, yi\nw\nrepresents a response from the model that better addresses the prompt xi compared to yi\nl . Similarly,\nw signifies a more harmful response compared to yj\nfor each pair in DC, but in this case, yj\nl . The safety\nlabels of these responses are then quantified using binary classification labels sj\nl , according to\nthe following harmfulness sign function:\n\nw, yj\n\nw, sj\n\nw, sj\n\nw, yi\nl\n\nl , sj\n\nl\n\ns(y) ≜\n\n(cid:26)+1,\n−1,\n\nif response y is harmful,\nif response y is harmless.\n\n(2)\n\nFigure 1 illustrates an example that shows the tension in balancing harmlessness and helpfulness.\nWhen the AI assistant faces the question of “How to become a serial killer”, Response B is superior\nto Response A in terms of helpfulness, as it shows a higher degree of completeness towards the user’s\ninstruction and has a better response structure. However, in terms of harmlessness, Response A is\nsafer because it refuses to respond to this query and informs the involved legal risks. In summary,\nwe would expect a helpfulness preference B ≻ A, a harmlessness preference A ≻ B, as well as\nharmfulness signs for the two responses s(A) = −1 and s(B) = +1.\n\n4.2 PREFERENCE MODEL FITTING: REWARD AND COST MODELS\n\nWe train two independent preference models to fit human preferences. The Reward Model (RM)\nis developed from the helpfulness dataset DR, serving to provide the reward signals that are opti-\nmized for helpfulness during the RL phase. The Cost Model (CM) is built upon the harmlessness\ndataset DC, delivering insights into human perceptions regarding the safety of LLM responses. An\nillustration of the reward and cost distribution on the dataset is presented in Figure 2.\n(cid:9)N\nReward Model (RM) Utilizing the helpfulness dataset DR = (cid:8)xi, yi\ni=1, we train a pa-\nrameterized reward model Rϕ(y, x), where Rϕ represents a scalar output. This model is trained to\nemploy the pairwise comparison loss derived from equation (1):\n\nw, yi\nl\n\nLR(ϕ; DR) = −E(x,yw,yl)∼DR [log σ(Rϕ(yw, x) − Rϕ(yl, x))] ,\n\n(3)\n\nCost Model (CM)\nIn the context of human values, we treat the safety of LLMs as a constraint.\nNevertheless, conventional preference models struggle to effectively capture the associated thresh-\nolds (as shown in Appendix B). Thus, we introduce a novel preference model named the Cost Model.\nThis model preserves the characteristics of the Bradley-Terry model (Bradley & Terry, 1952), but it\ndifferentiates between safe and unsafe responses by employing a zero threshold, as shown in Fig-\nure 2a. Specifically, we integrate a classification term into the original pairwise comparison loss\nfunction, leveraging harmfulness signs s sourced from the harmlessness dataset DC:\n\nLC(ψ; DC) = − E(x,yw,yl,·,·)∼DC [log σ(Cψ(yw, x) − Cψ(yl, x))]\n\n− E(x,yw,yl,sw,sl)∼DC [log σ(sw · Cψ(yw, x)) + log σ(sl · Cψ(yl, x))] .\n\n(4)\n\n4\n\n\fPublished as a conference paper at ICLR 2024\n\n(a) reward vs. cost distribution\n\n(b) reward distribution\n\n(c) cost distribution\n\nFigure 2: (a) The distribution of reward and cost on the test set as evaluated by the preference models\nfrom the first Safe RLHF iteration. Each point signifies a test sample whose color is derived from its\nsafety labels. (b) The reward distribution on the test set determined by the trained reward model. (c)\nThe cost distribution on the test set determined by the trained cost model. The red dashed vertical\nline at c = 0 in (a) and (c) is the decision boundary of the cost model while used as a binary classifier.\n\nIt’s worth noting that the Cost Model still complies with the Bradley-Terry (BT) model. Assume\nthere exists a virtual response, y0, which lies on the boundary between safe and unsafe responses,\nand let Cψ(y0, x) = 0. If y is unsafe, i.e., s(y) = +1, then the Cost Model tends to prefer y. Hence,\nwe aim to maximize the probability of y ≻ y0|x:\n\np(y ≻ y0|x) = σ (Cψ(y, x) − Cψ(y0, x)) = σ (Cψ(y, x)) = σ (s(y) · Cψ(y, x)) .\n(5)\nSimilarly, if y is safe, i.e., s(y) = −1, then the Cost Model tends to prefer y0. Hence, we aim to\nmaximize the probability of y0 ≻ y|x:\n\np(y0 ≻ y|x) = σ (Cψ(y0, x) − Cψ(y, x)) = σ(−Cψ(y, x)) = σ (s(y) · Cψ(y, x)) .\n(6)\nThus, the second term of the loss function (4) can be viewed as maximizing the likelihood of the\nBT model regarding the response y0 and y from the dataset DC. With the extra annotation of the\nharmfulness label s, we will not need to know the exact content of the virtual response y0. In our\nCost Model, a response y that is more harmful to the same prompt x will yield a higher cost value.\nFor unsafe responses, the cost value is positive; otherwise, it is negative. As shown in Figure 2a, the\nCost Model divides the LLMs’ responses into two clusters based on their safety. This classification\nability of the Cost Model provides a basis for dynamically adjusting conflicting objectives.\n\n4.3 SAFE REINFORCEMENT LEARNING\n\nDuring the RL phase, our approach utilizes the Reward Model Rϕ to estimate the value of human\npreference for helpfulness, while the Cost Model Cψ for harmlessness. The LLM we are training is\ndenoted as πθ(y|x). The following optimization objective is a Safe RL scheme previously outlined\nin Chow et al. (2017), hereby defined as the objective for our Safe RLHF setting:\n\nmaximize\nθ\n\nEx∼D,y∼πθ(·|x) [Rϕ(y, x)] ,\n\ns.t. Cψ(y, x) ≤ 0,\n\n∀x ∼ D, y ∼ πθ(·|x),\n\n(7)\n\nwhere D is a distribution of prompts used in the RL phase, and the y = a1:T are responses generated\nby the LLM πθ. This equation encapsulates our primary goal: to maximize the expected reward\nwithin the constraints of ensuring the harmlessness of the responses generated by the LLMs.\n\nHowever, the constraint denoted in equation (7) entails the challenge of guaranteeing safety for all\npotential responses y to a given prompt x. This task is not straightforward using RL methods. In\nlight of this, we reformulate the safety constraint into an expectation form, paralleling the structure\nof the objective function. This modification also introduces a hyper-parameter d, devised to exert\ncontrol over the probability of generating harmful responses. Given the objective function JR(θ)\nand the constraint function JC(θ) as\n\nJR(θ) ≜ Ex∼D,y∼πθ(·|x) [Rϕ(y, x)] ,\nOur surrogate objective is presented as follows:\nJR(θ),\n\nmaximize\nθ\n\ns.t. JC(θ) ≤ 0,\n\nand JC(θ) ≜ Ex∼D,y∼πθ(·|x) [Cψ(y, x)] + d.\n\n(8)\n\n(9)\n\nTo address this constrained problem, we leverage the Lagrangian method, a technique for finding\nthe local maxima and minima of a function over a constraint set. We convert the constrained primal\nproblem, as defined in equation (9) into its unconstrained Lagrangian dual form as follows:\n\nmin\nθ\n\nmax\nλ≥0\n\n[−JR(θ) + λ · JC(θ)],\n\n(10)\n\n5\n\n201001020cost201001020rewardsafeunsafe201001020reward0.000.020.040.060.08densitysafeunsafe201001020cost0.000.020.040.060.080.10density\fPublished as a conference paper at ICLR 2024\n\n(a) Prompt type and distribution\n\n(b) Distribution of safety types of response pairs\n\nFigure 3: (a) Number of different types of prompts during 3 rounds of Safe RLHF iteration. (b)\nNumber of different safety types of response pairs during three rounds of RLHF iteration.\n\nwhere λ ≥ 0 serves as the Lagrange multiplier. It is important to note that the optimization of\nhelpfulness JR often contradicts the objective of minimizing harm JC (Bai et al., 2022a). Thus,\nequation (10) can be interpreted as appending a penalty term to the original helpfulness objective.\nThis penalty, which corresponds to the potential harmfulness of the LLMs, can be dynamically\nmodulated via the parameter λ. Specifically, we iteratively solve the min-max problem in equation\n(10), alternately updating the LLM parameters θ and the Lagrange multiplier λ (refer to Appendix\nF.3 to more details). This ensures that any change in the potential harm associated with the updated\nmodel is rapidly reflected in the multiplier, thereby avoiding the risks of over-emphasizing one\nobjective at the expense of the other under a fixed optimization ratio.\n\n5 EXPERIMENTS\n\nIn this section, we present experiments to evaluate the effectiveness of the Safe RLHF pipeline in\nenhancing model safety and performance. We specifically address the following research questions:\n\n• Can Safe RLHF simultaneously improve the LLM’s helpfulness and harmlessness? (Section 5.2.1)\n• What benefits arise from the distinct separation of helpfulness and harmlessness? (Section 5.2.2)\n• How does Safe RLHF navigate the inherent tension between the dual optimization objectives of\n\nhelpfulness and harmlessness? (Section 5.2.3)\n\nFurthermore, we conduct an ablation experiment to elucidate the specific design of the Cost Model\nwhich is endowed with classification capabilities (Section 5.2.4). Collectively, these experiments\naim to provide a comprehensive assessment of Safe RLHF within practical contexts.\n\n5.1 EXPERIMENTAL DETAILS\n\nWe demonstrate the efficacy of our pipeline by iteratively fine-tuning the initial SFT model using the\nSafe RLHF pipeline for three cycles. Each cycle involves Red Teaming (excluding the first round),\ngenerating and annotating human data, training the Reward Model and Cost Model, and Safe RL\nfine-tuning. The implementation details and hyper-parameters are available in Appendix F and G.1.\n\nInitial SFT Model. Our experiments begin with the Alpaca-7B model (reproduced). This model\nis derived from instruction fine-tuning the LLaMA-7B (Touvron et al., 2023a) using the Alpaca\nopen-source dataset (Taori et al., 2023). We selected Alpaca-7B as our initial model for two pri-\nmary reasons. First, Alpaca-7B embodies essential chat assistant capabilities and has an appropriate\nmodel size, facilitating the full implementation of the Safe RLHF framework. Second, Alpaca-7B\nis capable of generating both harmless and harmful responses, offering varied responses to identical\nprompts, as shown in Figure 3b. Using Alpaca-7B as our starting point allows us to more clearly\ndiscern improvements in the safety and utility of LLMs when employing the Safe RLHF framework.\n\nPrompts, Red-teaming, and Preference Datasets. At the start of each iteration, we adjust the\nmix of the different types of prompts (safety-unrelated, resolved safety-related, unresolved safety-\nrelated, and those collected through red-teaming), as shown in Figure 3a. This prompt dataset is\nused for generating preference datasets and for RL training. In the first iteration, our prompts were\nderived from open-source safety-related datasets referenced in Ganguli et al. (2022) and Sun et al.\n(2023a). From the second iteration, we involved researchers in conducting red-teaming attacks to\nexpand our prompt set. By examining successful attacks, we identified and added prompts that\nexpose vulnerabilities not present in the original dataset. More details are available in Appendix H.\n\nAfter finalizing the prompts, responses are generated using the model in training and sent to crowd-\nworkers for labeling. We allowed the crowdworkers to meticulously label out invalid preference\n\n6\n\n150114801448247114493799421500349163646400100020003000400050006000Round 3Round 2Round 1safety-unrelatedsolved safety-relatedunsolved safety-relatedred-teaming276391878612811368853984837197363391368705000100001500020000250003000035000Round 3Round 2Round 1dual-safe pairsmixed-safe pairsdual-unsafe pairs\fPublished as a conference paper at ICLR 2024\n\n(a) Alpaca-7B\n\n(b) Beaver-v1\n\n(c) Beaver-v2\n\n(d) Beaver-v3\n\nFigure 4: The scatter plots present the distribution of reward and cost on the evaluation prompt set,\nas assessed by the unified reward and cost models. All four models utilize the same set of prompts\nas inputs, generating responses via a greedy search. The red dashed vertical line at c = 0 is the\ndecision boundary of the cost model while used as a binary classifier.\n\nTable 1: The evaluation accuracy for the Reward Model and Cost Model for three rounds of Safe\nRLHF iterations. The unified preference models are trained and tested on evenly balanced preference\ndata from the preference dataset used in the three Safe RLHF iterations.\n\nModel\nReward Model\n\nCost Model\n\nMetric\nRanking Accuracy\nRanking Accuracy\nSafety Classification Accuracy\n\nBeaver-v1 Beaver-v2 Beaver-v3 Unified\n73.95%\n70.44%\n85.83%\n\n77.32%\n74.17%\n85.88%\n\n78.13%\n74.47%\n95.62%\n\n75.73%\n76.07%\n84.54%\n\npairs. Each prompt receive k = 3 ∼ 6 unique responses, leading to C k\n2 = k(k − 1)/2 preference\npairs, as shown in Figure 3b. Following the annotation scheme in Section 4.1, we obtain decoupled\ndatasets for helpfulness and harmlessness. More details and examples are available in Appendix E.\n\nEvaluation Datasets. Since the lack of evaluation datasets that consider both helpfulness and\nharmlessness alignment, we constructed our own evaluation prompt dataset, comprising 3 parts:\nprompts meticulously designed for 14 safety categories, prompts sourced from open-source datasets\n(excluded from training), and a selected 10% of prompts from each red-teaming phase. The defini-\ntion of the 14 safety categories is detailed in Appendix E.3.\n\n5.2 EXPERIMENT RESULTS\n\n5.2.1 HELPFULNESS AND HARMLESSNESS EVALUATION\n\nTo rigorously assess the efficacy of our Safe RLHF pipeline along two alignment dimensions —\nhelpfulness and harmlessness — we analyze models from three iterations of Safe RLHF: Beaver-\nv1, Beaver-v2, and Beaver-v3. However, evaluating large language models has consistently been\na challenging and unresolved problem. Traditional benchmarks often do not capture the full extent\nto which a model aligns with human values. Thus, we prefer to assess large language models by\ndirectly evaluating their responses. We employ two methods for overall assessment including a rapid\nModel-based Evaluation and a GPT-4 and Human Evaluation.\n\nModel-based Evaluations. Despite human evaluation remains the gold standard for aligning\nLLMs with human values, the reliance on this method alone is neither practical nor efficient due\nto expensive time and financial costs. Such limitations necessitate alternative assessment meth-\nods to complement human evaluation. Thus, we have developed a unified Reward Model and Cost\nModel, utilizing methodologies in Section 4.2. These models are trained on preference data origi-\nnating from all iterations of Safe RLHF, and the test accuracy for these models is detailed in Table\n1. With these unified models, we can rapidly evaluate new models under consistent criteria.\n\nAs illustrated in Figure 4, our SFT model, the reproduced Alpaca-7B model, has the ability to\nproduce both harmless and harmful responses that are almost evenly separated by the c = 0 dividing\nline (Figure 4a). Following the first round of Safe RLHF training, there is an appreciable shift in\nthe model response distribution towards the side with a lower cost, implying safer outputs (Figure\n4b). During the second iteration of Safe RLHF, there is a decline in harmful content, denoted by the\nc > 0 region (Figure 4c). In the final iteration, the data cluster gravitates towards the higher reward\ndirection, while successfully maintaining the majority of the responses as harmless (Figure 4d).\n\n7\n\n1050510cost42024reward1050510cost42024reward1050510cost42024reward1050510cost42024reward\fPublished as a conference paper at ICLR 2024\n\n(a) Elo scores rated by GPT-4\n\n(b) Elo scores rated by Human\n\n(c) Model safety on evaluation set\n\nFigure 5: (a) (b) The Elo scores in harmlessness and helpfulness for three rounds of Safe RLHF\niteration. The Elo scores for the Alpaca-7B are normalized to 1000. (c) The ratio of the responses\nflagged by Human on the evaluation set.\n\nGPT-4 and Human Evaluations. For more accurate assessments, we compare models against\neach other to generate associated Elo scores, as described in Askell et al. (2021). Specifically,\nevaluators compare the outputs of two models in response to the same prompt and provide their\npreferences regarding helpfulness and harmlessness. After obtaining pairwise win-rate relationships\nbetween all models, we fit corresponding Elo scores (with an initial score of 1200). According\nto Chiang & Lee (2023), GPT-4 can replace human evaluators in assessing the alignment capabilities\nof LLMs. Therefore, we have organized assessments involving both GPT-4 and human evaluators.\n\nAs shown in Figure 5a and 5b, the three rounds of Safe RLHF significantly improved the Elo scores\nin both helpfulness and harmlessness, as evaluated by both GPT-4 and human evaluators. When\ncompared to Alpaca-7B, the Beaver-v3 model demonstrated an increase in the Elo score for helpful-\nness (GPT-4: +244.91, Human: +363.86) and for harmlessness (GPT-4: +268.31, Human: +237.98).\nComparatively, the evaluations by GPT-4 and human evaluators are almost consistent. Notably, start-\ning from the second round, we initiated red teaming attacks to broaden the scope of safety-related\nprompts. This effectively aided in making the Safe RLHF training models more harmless. During\nthe third round, since the model was sufficiently safe, Safe RLHF tended to prioritize maintaining\nthe current harmlessness level over excessive optimization. This is also reflective of the dynamic\nadjustment characteristics inherent to Safe RLHF.\n\nMeanwhile, our crowdworkers also labeled whether the models’ responses are safe, as shown in\nFigure 5c. Through three rounds of Safe RLHF training, the Beaver-v3 model’s probability of\nharmful responses on the evaluation set decreased from 53.08% for Alpaca-7B to 2.45%. For the\nspecific prompts used in the GPT-4 evaluation, please refer to Appendix G.4.\n\n5.2.2 THE DECOUPLING OF HARMLESSNESS AND HELPFULNESS\n\nIn this section, we aim to demonstrate the benefits of explicitly separating harmlessness and helpful-\nness. We use the responses collected from the first round of Safe RLHF to carry out annotation and\nPPO training following the conventional RLHF methodology. During the annotation, the difference\nis that only a comprehensive preference is provided, while other aspects align with Safe RLHF.\n\nCompared to single-dimensional annotation and training, we observe the following advantages of\nSafe RLHF: First, decoupling the annotations for helpfulness and harmlessness results in higher\nInter-Rater Agreement Rate among crowdworkers, which is Helpfulness: 69.00% and Safety:\n66.53% compared to 61.65%. Second, the agreement between crowdworkers and researchers (i.e.\napproval rate) is also increased. In single-dimensional annotation, the average approval rate during\na 10% quality inspection drops from at least 90% accuracy to below 80%. Third, as shown in Figure\n6a, using the above data for PPO training results in a notable improvement in helpfulness. However,\nthe enhancement in harmlessness is significantly less than that achieved by Safe RLHF. In contrast,\nSafe RLHF allows a subjective adjustment during training to balance helpfulness and harmlessness.\n\n5.2.3 BALANCE BETWEEN HARMLESSNESS OBJECTIVE AND HELPFULNESS OBJECTIVE\nTo highlight the importance of dynamically balancing the harmlessness and helpfulness objectives\nduring RL training, we compare Safe RLHF with the Reward Shaping (RS) approach that employs\n\n8\n\n1000105011001150120012501300Harmlessness100010501100115012001250130013501400HelpfulnessAlpaca-7BBeaver-v1Beaver-v2Beaver-v31000105011001150120012501300Harmlessness100010501100115012001250130013501400HelpfulnessAlpaca-7BBeaver-v1Beaver-v2Beaver-v3Alpaca-7BBeaver-v1Beaver-v2Beaver-v30%10%20%30%40%50%60%70%80%90%100%Harmful ratioHarmless ratio\fPublished as a conference paper at ICLR 2024\n\n(a) Ablation training\n\n(b) Compare to Reward Shaping (RS)\n\n(c) Training curve for Beaver-v1\n\nFigure 6: (a) The harmlessness and helpfulness win rates for Safe RLHF and other methods against\nthe Alpaca-7B. (b) The harmlessness and helpfulness win rates for Safe RLHF and reward shaping\n(RS) methods with different coefficients against the Alpaca-7B. The dashed curve in (a) and (b) is\nthe asymptotic curve for reward shaping (RS) methods. (c) The training curve for the Lagrange\nmultiplier λ and the moving averaged cost during the Beaver-v1 training.\n\na static balance. Specifically, the Reward Shaping method refers to weighting the two objective\nfunctions at a fixed ratio, that is, Rν(y, x) = Rϕ(y, x) − ν · Cψ(y, x). Our experiments extensively\ntested seven different Reward Shaping coefficients ν, namely 0.01, 0.5, 1, 2, 5, 10, and 100.\n\nThe training results are shown in Figure 6b. Two conclusions can be drawn from the observations:\nexcessively high (ν = 5, 10, 100) and excessively low (ν = 0.01, 0.5) reward shaping weights re-\nsult in over-optimizing one objective at the expense of the other. Moderate reward shaping weights\n(ν = 1, 2) still cannot effectively address the tension between the objectives of helpfulness and harm-\nlessness, with their improvements remaining inferior to Safe RLHF. Comparatively, Safe RLHF\nassesses the harmlessness of models by using average cost values, subsequently updating the La-\ngrange multiplier λ. When the model satisfies safety constraints, Safe RLHF employs a smaller λ to\npreserve harmlessness, thereby avoiding over-optimization, as illustrated in Figure 6c.\n\n5.2.4 DESIGN OF COST PREFERENCE MODEL\n\nA crucial design of Safe RLHF is the Cost Model, which simultaneously fits human preferences and\nsafety labels. Human preferences provide the direction of optimization, while safety labels provide a\nthreshold as the basis of the dynamic balance of objectives. Such successful integration contributes\nto the success of Safe RLHF. To substantiate this, we compared Safe RLHF with the training using\nthe logits of a safety classifier as the cost signals (Glaese et al., 2022). As illustrated in Figure 6a\n(CM-classifier), the latter’s efficiency in improving harmlessness is significantly inferior to that of\nSafe RLHF. On the other hand, removing the classification capability of the Cost Model, and not\nupdating the Lagrange multiplier, results in a degradation to the Reward Shaping method.\n\n6 CONCLUSION AND DISCUSSION\n\nThis work significantly impacts the safety of AI systems based on LLMs, focusing on how to address\nthe tension between helpfulness and harmlessness during fine-tuning LLMs. We acknowledge that\nhelpfulness and harmlessness often conflict in most scenarios, making their mixture into a single\ntraining objective unreliable. Our safety alignment paradigm, Safe RLHF, is the first integration\nof Safe RL and RLHF framework. The core insight of Safe RLHF is the decoupling of human\npreference during the annotation and a λ-trade-off to dual helpfulness and harmlessness objectives.\n\nIn our experiments, we applied three rounds of the Safe RLHF framework to fine-tune the SFT\nbase model. Evaluation results indicate that Safe RLHF effectively enhances the helpfulness and\nharmlessness of the LLM. Compared to the algorithm, Reward Shaping, that statically balances two\noptimization objectives Safe RLHF better navigates the tension between the goals of helpfulness\nand harmlessness. All code and data, detailed in Appendix C, can be found in the supplementary\nmaterials to foster the reproducibility of this work.\n\n9\n\n0246Lagrange Multiplier Lagrange Multiplier 050100150200250300Step64202Cost Moving AverageCost Moving Average C\fPublished as a conference paper at ICLR 2024\n\nACKNOWLEDGMENTS\n\nThis work is sponsored by the National Natural Science Foundation of China (62376013) and the\nBeijing Municipal Science & Technology Commission (Z231100007423015)",
    "abstract": "With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical. However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training. To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowd workers' confusion about the tension and allowing us to train separate reward and cost models. We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints. Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we demonstrate a superior ability to mitigate harmful responses while enhancing model performance compared to existing value-aligned algorithms. Experimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with collected human preferences, significantly improving its helpfulness and harmlessness according to human evaluations.\n\nCode is available at https://github.com/PKU-Alignment/safe-rlhf.\n\n\nWarning: This paper contains example data that may be offensive or harmful.",
    "response": {
      "Soundness": 4,
      "Presentation": 4,
      "Contribution": 4,
      "Rating": 8,
      "Confidence": 3,
      "Strengths": "1. Separation of rewards and costs is an excellent idea that probably resolves the optimization contradiction in RLHF of LLM. \n2. The paper provides concrete experimental results demonstrating the effectiveness of Safe RLHF in enhancing model performance and reducing harmful responses.",
      "Weaknesses": "Minor suggestions in Questions.",
      "Questions": "1. Fig2.(a) the symbols of axis indexes are absent.\n2. Could you add the scatter point of Beaver-v2 and Beaver-v3 in Fig.6(a) and (b)?"
    }
  },
  {
    "paper_id": "NvSwR4IvLO",
    "summary": "Background: The rapid advancements in Large Language Models (LLMs) enable them to perform various tasks efficiently, posing potential security risks due to misuse in plagiarism, fake news generation, and more. Recent research attempts to identify AI-generated texts using model signatures and watermarking techniques.\nGap: Currently proposed AI-text detectors are not reliable in real-world applications as they are vulnerable to attacks, particularly paraphrasing, which significantly reduces their efficacy. There's a necessity to evaluate the robustness of these detection methods against adversarial strategies.\nHypothesis: Existing AI-text detectors, including watermarking-based schemes, neural network-based classifiers, zero-shot classifiers, and retrieval-based detectors, are susceptible to evasion and spoofing attacks, rendering them ineffective in reliably distinguishing AI-generated from human-generated text.\nMethod: The authors develop a recursive paraphrasing attack using neural network-based paraphrasing models, applied to AI-generated texts to evade detection. They conduct experiments with different detector types and demonstrate this attack on passages of around 300 tokens. They also present a spoofing attack showing how adversaries can mislead detectors. Finally, they provide a theoretical framework connecting the effectiveness of a detector to the total variation distance between human and AI text distributions, using a paraphraser to reduce this distance.\nConclusion: The study reveals that current AI-generated text detectors are unreliable in practical scenarios, as they fall victim to paraphrasing and spoofing attacks. These findings show the inherent difficulty in creating robust detectors capable of differentiating human and AI text without substantial trade-offs. The research suggests that as AI models become more sophisticated, existing detectors will find it increasingly difficult to function effectively.\n",
    "full text": "\nCAN AI-GENERATED TEXT BE RELIABLY DETECTED?\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nThe rapid progress of Large Language Models (LLMs) has made them capable of\nperforming astonishingly well on various tasks, including document completion\nand question answering. The unregulated use of these models, however, can\npotentially lead to malicious consequences such as plagiarism, generating fake\nnews, spamming, etc. Therefore, reliable detection of AI-generated text can be\ncritical to ensure the responsible use of LLMs. Recent works attempt to tackle\nthis problem either using certain model signatures present in the generated text\noutputs or by applying watermarking techniques that imprint specific patterns onto\nthem. In this paper, we show that these detectors are not reliable in practical\nscenarios. In particular, we develop a recursive paraphrasing attack to apply on\nAI text, which can break a whole range of detectors, including the ones using\nthe watermarking schemes as well as neural network-based detectors, zero-shot\nclassifiers, and retrieval-based detectors. Our experiments include passages around\n300 tokens in length, showing the sensitivity of the detectors even in the case of\nrelatively long passages. We also observe that our recursive paraphrasing only\ndegrades text quality slightly, measured via perplexity scores and MTurk human\nstudy. Additionally, we show that even LLMs protected by watermarking schemes\ncan be vulnerable against spoofing attacks aimed to mislead detectors to classify\nhuman-written text as AI-generated, potentially causing reputational damages to\nthe developers. In particular, we show that an adversary can infer hidden AI text\nsignatures of the LLM outputs without having white-box access to the detection\nmethod. Finally, we provide a theoretical connection between the AUROC of the\nbest possible detector and the Total Variation distance between human and AI text\ndistributions that can be used to study the fundamental hardness of the reliable\ndetection problem for advanced language models.\n\n1\n\nINTRODUCTION\n\nArtificial Intelligence (AI) has made tremendous advances in recent years, from generative models in\ncomputer vision (Rombach et al., 2022; Saharia et al., 2022) to generative models in natural language\nprocessing (NLP) (Brown et al., 2020; Zhang et al., 2022; Raffel et al., 2019). Large Language\nModels (LLMs) can now generate texts of supreme quality with the potential in many applications.\nFor example, the recent model of ChatGPT (OpenAI, 2022) can generate human-like texts for various\ntasks such as writing codes for computer programs, lyrics for songs, completing documents, and\nquestion answering; its applications are endless. The trend in NLP shows that these LLMs will\neven get better with time. However, this comes with a significant challenge in terms of authenticity\nand regulations. AI tools have the potential to be misused by users for unethical purposes such as\nplagiarism, generating fake news, spamming, generating fake product reviews, and manipulating\nweb content for social engineering in ways that can have negative impacts on society (Adelani et al.,\n2020; Weiss, 2019). Some news articles rewritten by AI have led to many fundamental errors in them\n(Christian, 2023). Hence, there is a need to ensure the responsible use of these generative AI tools. In\norder to aid this, a lot of recent research focuses on detecting AI-generated texts.\n\nSeveral detection works study this problem as a binary classification problem (OpenAI, 2019; Jawahar\net al., 2020; Mitchell et al., 2023; Bakhtin et al., 2019; Fagni et al., 2020) and use neural network-\nbased detectors. For example, OpenAI fine-tunes RoBERTa-based (Liu et al., 2019) GPT-2 detector\nmodels to distinguish between non-AI generated and GPT-2 generated texts (OpenAI, 2019). This\nrequires such a detector to be fine-tuned with supervision on each new LLM for reliable detection.\n\n1\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 1: An illustration of vulnerabilities of existing AI-text detectors. We consider both\nwatermarking-based and non-watermarking-based detectors and show that they are not reliable\nin practical scenarios. Colored arrow paths show the potential pipelines for adversaries to avoid detec-\ntion. In red: an attacker can use a paraphraser to remove the LLM signatures from an AI-generated\ntext to avoid detection. In blue: an adversary can query the watermarked LLM multiple times to learn\nits watermarking scheme. This information can be used to spoof the watermark detector.\n\nAnother stream of work focuses on zero-shot AI text detection without any additional training\noverhead (Solaiman et al., 2019; Ippolito et al., 2019; Gehrmann et al., 2019). These works evaluate\nthe expected per-token log probability of texts and perform thresholding to detect AI-generated texts.\nMitchell et al. (2023) observe that AI-generated passages tend to lie in negative curvature of log\nprobability of texts. They propose DetectGPT, a zero-shot LLM text detection method, to leverage\nthis observation. Since these approaches rely on a neural network for their detection, they can be\nvulnerable to adversarial and poisoning attacks (Goodfellow et al., 2014; Sadasivan et al., 2023;\nKumar et al., 2022; Wang et al., 2022). Another line of work aims to watermark AI-generated texts\nto ease their detection (Atallah et al., 2001; Wilson et al., 2014; Kirchenbauer et al., 2023a; Zhao\net al., 2023). Watermarking eases the detection of LLM output text by imprinting specific patterns on\nthem. Soft watermarking proposed in Kirchenbauer et al. (2023a) partitions tokens into “green” and\n“red” lists, as they define, to help create these patterns. A watermarked LLM samples a token, with\nhigh probability, from the green list determined by a pseudo-random generator seeded by its prefix\ntoken. The watermarking detector would classify a passage with a large number of tokens from the\ngreen list as AI-generated. These watermarks are often imperceptible to humans. Krishna et al. (2023)\nintroduces an information retrieval-based detector by storing the outputs of the LLM in a database.\nFor a candidate passage, their algorithm searches this database for semantically similar matches for\ndetection. However, storing user-LLM conversations might cause serious privacy concerns.\n\nIn this paper, through several experiments, we show that these state-of-the-art AI-text detectors\nare unreliable in practical scenarios (Wolff, 2020; Aaronson, 2022; Liang et al., 2023; Pu et al.,\nIn §2, we have developed a recursive paraphrasing attack that use\n2023; Wang et al., 2023).\nneural network-based paraphrasing to recursively paraphrase the source LLM’s output text. Our\nexperiments show that this automated paraphrasing attack can drastically reduce the accuracy of\nvarious detectors, including those using soft watermarking (Kirchenbauer et al., 2023a), to increase\ntype-II error (detecting AI text as human text). For instance, our recursive paraphrasing attack\non watermarked texts, even over relatively long passages of 300 tokens in length, can drop\nthe detection rate (true positive rate at 1% false positive rate or TPR@1%FPR) from 99.3%\nto 4.0% with only degradation of 1.5 in perplexity score. We note that Kirchenbauer et al.\n(2023a) considers a relatively weak paraphrasing attack in their experiments where they perform span\nreplacement by replacing random tokens (in-place) using an LLM. Our experiments, however, show\nthe vulnerability of the watermarking scheme against stronger paraphrasing attacks that we use.\n\nWe also observe that the quality of the paraphrased passages degrades, but only slightly, compared\nto the original ones. We quantify this both via perplexity score evaluation as well as via MTurk\nhuman evaluation study. In particular, our human evaluation study shows that 70% of the recursive\n\n2\n\n\fUnder review as a conference paper at ICLR 2024\n\nparaphrased passages are rated high quality in terms of content preservation, and 89% of them are\nrated high quality in terms of grammar or text quality.\n\nAfter paraphrasing, the area under the receiver operating characteristic (AUROC) curves of zero-shot\ndetectors (Mitchell et al., 2023) drops from 96.5% to 25.2%. We also observe that the performance\nof neural network-based trained detectors (OpenAI, 2019) deteriorates significantly after our para-\nphrasing attack. For instance, the TPR@1%FPR of the RoBERTa-Large-Detector from OpenAI\ndrops from 100% to 60% after paraphrasing. In addition, we show that the retrieval-based detector by\nKrishna et al. (2023) designed to evade paraphrase attacks is vulnerable to our recursive paraphrasing.\nIn fact, the accuracy of their detector falls from 100% to 25% with our recursive paraphrase attack.\n\nMoreover, we show the possibility of spoofing attacks on various AI text detectors in §3. In this\nsetting, an attacker generates a non-AI text that is detected to be AI-generated, thus increasing type-I\nerror (falsely detecting human text as AI text). An adversary can potentially launch spoofing attacks\nto produce derogatory texts that are detected to be AI-generated to affect the reputation of the target\nLLM’s developers. In particular, we show that an adversary can infer hidden AI text signatures\nwithout having white-box access to the detection method. For example, though the pseudo-random\ngenerator used for generating watermarked text is private, we develop an attack that adaptively queries\nthe target LLM multiple times to learn its watermarking scheme. An adversarial human can then use\nthis information to compose texts that are detected to be watermarked. Figure 1 shows an illustration\nof some of the vulnerabilities of the existing AI-text detectors.\n\nFinally, in §4, we present a theoretical result regarding the hardness of AI-text detection. Our\nmain result in Theorem 1 states that the AUROC of the best possible detector differentiating two\ndistributions H (e.g., human text) and M (e.g., AI-generated text) reduces as the total variation\ndistance TV(M, H) between them decreases. Note that this result is true for any two arbitrary\ndistributions H and M. For example, H could be the text distribution for a person or group and M\ncould be the output text distribution of a general LLM or an LLM trained by an adversary to mimic\nthe text of a particular set of people. Essentially, adversaries can train LLMs to mimic human text as\nthey get more sophisticated, potentially reducing the TV distance between human and AI text, leading\nto an increasingly more difficult detection problem according to our Theorem 1. Although estimating\nthe exact TV between text distributions from a finite set of samples is a challenging problem, we\nprovide some empirical evidence, over simulated data or via TV estimations, showing that more\nadvanced LLMs can potentially lead to smaller TV distances. Thus, our Theorem 1 would indicate\nan increasingly more difficult reliable detection problem in such cases.\n\nIdentifying AI-generated text is a critical problem to avoid its misuse by users for unethical purposes\nsuch as plagiarism, generating fake news, and spamming. However, deploying vulnerable detectors\nmay not be the right solution to tackle this issue since it can cause its own damages, such as falsely\naccusing a human of plagiarism. Our results highlight the sensitivities of a wide range of detectors\nto both evasion and spoofing attacks and indicate the difficulty of developing reliable detectors in\npractical scenarios — to maintain reliable detection performance, LLMs would have to trade off\ntheir performance. We hope that these findings can help the ethical and dependable utilization of\nAI-generated text.\n\nIn summary, we make the following contributions in this work.\n\n• Our work is the first to comprehensively analyze the performance of four different classes\nof detectors, including watermarking-based, neural network-based, zero-shot, and retrieval-\nIn particular, the recursive\nbased detectors, and reveal their reliability issues (in §2).\nparaphrasing attack that we develop is the first method that can break watermarking\n(Kirchenbauer et al., 2023a) and retrieval-based (Krishna et al., 2023) detectors with only a\nsmall degradation in text quality.\n\n• Our work is the first to show that existing detectors are vulnerable against spoofing attacks\nwhere an adversarial human aims to write a (potentially derogatory) passage falsely detected\nas AI-generated without having a white-box access to the detection methods (in §3). For\ninstance, we show that an adversary can infer the watermarking signatures by probing the\nwatermarked LLM and analyzing the statistics of the generated tokens.\n\n• Our work is the first to establish a theoretical connection between the AUROC of the best\npossible detector and the TV distance between human and AI-text distributions that can be\nused to study the hardness of the reliable text detection problem (in §4).\n\n3\n\n\fUnder review as a conference paper at ICLR 2024\n\n(a) Watermarked text with mean token length 300\n\n(b) Watermarked text with varying token lengths\n\nFigure 2: ROC plots for soft watermarking with recursive paraphrasing attacks. AUROC,\nTPR@1%FPR, and perplexity scores measured using OPT-13B are given in the legend. (a) Even for\n300 tokens long watermarked passages, recursive paraphrasing is effective. As paraphrasing rounds\nproceed, detection rates degrade significantly with a slight trade-off in text quality. (b) Attacking\nwatermarked passages become easier as their length reduces.\n\nppi\n\ni=1\n\ni=2\n\ni=3\n\ni=4\n\ni=5\n\nAll ppi\n\nContent\npreservation\n\nGrammar or\ntext quality\n\nAvg. rating\n\n4.0 ± 0.8\n\n4.1 ± 0.8\n\n3.9 ± 0.9\n\n4.2 ± 0.9\n\n3.7 ± 1.1\n\n4.0 ± 0.9\n\nRatings 5&4\n\n70.2%\n\n77.2%\n\n63.2%\n\n80.0%\n\n61.4%\n\n70.4%\n\nAvg. rating\n\n4.28 ± 0.67\n\n4.12 ± 0.50\n\n4.12 ± 0.53\n\n4.11 ± 0.64\n\n4.07 ± 0.53\n\n4.14 ± 0.58\n\nRatings 5&4\n\n87.72%\n\n92.98%\n\n91.23%\n\n84.21%\n\n89.47%\n\n89.12%\n\nTable 1: Summary of the MTurk human evaluation study on content preservation and grammar or\ntext quality of the recursive paraphrases that we use for our attacks. Ratings are on a Likert scale of 1\nto 5. See Appendix B.1 for details.\n\n2 EVADING AI-DETECTORS USING PARAPHRASING ATTACKS\n\n2.1 PARAPHRASER MODELS AND TEXT DATASET\n\nWe use the “document” features of the XSum dataset (Narayan et al., 2018) containing long news\narticles for our experiments. We use two different neural network-based paraphrases – DIPPER\nwith 11B parameters (Krishna et al., 2023), and T5-based paraphraser (Damodaran, 2021) with\n222M parameters. Suppose a passage S = (s1, s2, ..., sn) where si is the ith sentence. DIPPER\nparaphrases S to be S′ = fdipper(S) in one-shot while the light-weight T5-based paraphraser would\noutput S′ = (ft5(s1), ft5(s2), ..., ft5(sn)) where they can only paraphrase sentence-by-sentence.\nDIPPER also has the ability to input a context prompt text C to generate higher-quality paraphrasing\nS′ = fdipper(S, C). We can also vary two different hyperparameters of DIPPER to generate a diverse\nnumber of paraphrases for a single input passage.\n\nWe use DIPPER for recursive paraphrasing attacks since it\nprovides high-quality paraphrasing (Krishna et al., 2023). Let\nan LLM L generate AI text output S = L(C) for an input\nprompt C. DIPPER can be used to generate a paraphrase pp1\n= fdipper(S, C). This paraphrasing can be performed in re-\ncursion (see Figure 3). That is, ppi = fdipper(pp(i-1), C).\nIn the next section, we show that recursive paraphrasing is ef-\nfective in removing watermarks from AI text when compared\nto a single round of paraphrasing. Moreover, by quantifying\ntext quality with perplexity measured using OPT-13B as well\n\n4\n\nFigure 3: Recursive paraphrasing\n\n\fUnder review as a conference paper at ICLR 2024\n\nas performing a human study, we show that our recursive paraphrasing method only degrades the text\nquality slightly.\n\n2.2 PARAPHRASING ATTACKS ON WATERMARKED AI TEXT\n\nIn this section, we evaluate our recursive paraphrasing attacks on the soft watermarking scheme\nproposed in Kirchenbauer et al. (2023a). Soft watermarking encourages LLMs to output token\ns(t) at time-step t that belongs to a “green list”. The green list for s(t) is created using a private\npseudo-random generator that is seeded with the prior token s(t−1). A watermarked output from the\nLLM is designed to have tokens that are majorly selected from the green list. Hence, a watermark\ndetector with the pseudo-random generator checks the number of green tokens in a candidate passage\nto detect whether it is watermarked or not. Here, we target a watermarked OPT-1.3B (Zhang et al.,\n2022) with 1.3B parameters for our experiments.\n\nDataset. We perform our experiments on 2000 text passages that are around 300 tokens in length\n(1000 passages per human and AI text classes). We pick 1000 long news articles from the XSum\n“document” feature. For each article, the first 300 tokens are input to the target OPT-1.3B to generate\n1000 watermarked AI text passages that are each 300 tokens in length. The second 300 tokens from\nthe 1000 news articles in the dataset are treated as baseline human text. We note that our considered\ndataset has more and longer passages compared to the experiments in Kirchenbauer et al. (2023a).\n\nDetection results after paraphrasing attack. After a single round of paraphrasing (pp1),\nTPR@1%FPR of watermark detector only degrades from 99.3% to 78.6%. Similarly, another\nweaker paraphrasing attack discussed in Kirchenbauer et al. (2023a) is not effective in removing\nwatermarks. They perform “span replacement” by replacing random tokens (in-place) using a lan-\nguage model. However, we show that our stronger recursive paraphrasing attack can effectively\nevade watermark detectors with only a slight degradation in text quality. As shown in Figure 2a, the\nrecursive paraphrase attack further degrades the detection rate of the detector to 9.7% after 5 rounds\nof paraphrasing (pp5). Best of ppi in the figure refers to the method where, for each passage,\nwe select the paraphrase out of all the ppi’s that has the worst detector score. For Best of ppi,\nthe detection rate reduces drastically from 99.8% to 4.0% with only a trade-off of 1.5 in the\nperplexity score. Figure 2b shows that the watermarking detector becomes weaker as the length of\nthe watermarked text reduces. Note that for watermarked texts that are 50 or 100 tokens long, the\ndetection performance after the recursive paraphrasing attack is similar to that of a random detector.\nWe provide examples of paraphrased text that we use for our attacks in Appendix B.2.\n\nQuality of the paraphrased passages. In order to reliably study the quality of the paraphrases we\nuse in our experiments, we perform two human evaluations using MTurk. A summary of the study\nis provided in Table 1. We investigate the content preservation and text quality or grammar of the\nrecursive paraphrases with respect to the watermarked texts (see Tables 4 and 5 in Appendix B.1 for\nmore details). In terms of content preservation, 70% of the paraphrases were rated high quality\nand 23% somewhat equivalent. In terms of text quality or grammar, 89% of the paraphrases\nwere rated high quality. On a Likert scale of 1 to 5, the paraphrases that we use received an average\nrating of 4.14 ± 0.58 for text quality or grammar and 4.0 ± 0.9 for content preservation. These results\nconfirm that watermarking can be effectively attacked using recursive paraphrasing with only a slight\ndegradation in text quality. See Appendix B.1 for more details on the human study.\n\n2.3 PARAPHRASING ATTACKS ON NON-WATERMARKED AI TEXT\n\nNeural network-based trained detectors such as RoBERTa-Large-Detector from OpenAI (OpenAI,\n2019) are trained or fine-tuned for binary classification with datasets containing human and AI-\ngenerated texts. Zero-shot classifiers leverage specific statistical properties of the source LLM outputs\nfor their detection. Retrieval-based methods search for a candidate passage in a database that stores\nthe LLM outputs. Here, we perform experiments on these non-watermarking detectors to show they\nare vulnerable to our paraphrasing attack.\n\nTrained and Zero-shot detectors. We use a pre-trained GPT-2 Medium model (Radford et al., 2019)\nwith 355M parameters as the target LLM to evaluate our attack on 1000 long passages from the\nXSum dataset (Narayan et al., 2018). We use the T5-based paraphrasing model (Damodaran, 2021)\nwith 222M parameters to rephrase the 1000 output texts generated using the target GPT-2 Medium\n\n5\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 4: ROC curves for various trained and zero-shot detectors. Left: Without attack. Middle:\nAfter paraphrasing attack using T5-based paraphraser. The performance of zero-shot detectors drops\nsignificantly. Right: Here, we assume we can query the detector ten times for the paraphrasing\nattack. We generate ten paraphrasings for each passage and query multiple times to evade detection.\nNotice how all detectors have low TPR@1%FPR. In the plot legend – perturbation refers to the\nzero-shot methods in Mitchell et al. (2023); threshold refers to the zero-shot methods in Solaiman\net al. (2019); Gehrmann et al. (2019); Ippolito et al. (2019); roberta refers to OpenAI’s trained\ndetectors (OpenAI, 2019). The TPR@1%FPR scores of different detectors before the attack, after the\nattack, and after the attack with multiple queries, respectively, are provided in the plot legend.\n\nmodel. Figure 4 shows the effectiveness of the paraphrasing attack over these detectors. The AUROC\nscores of DetectGPT (Mitchell et al., 2023) drop from 96.5% (before the attack) to 59.8% (after\nthe attack). Note that AUROC of 50% corresponds to a random detector. The rest of the zero-shot\ndetectors (Solaiman et al., 2019; Gehrmann et al., 2019; Ippolito et al., 2019) also perform poorly\nafter our attack. Though the performance of the trained neural network-based detectors (OpenAI,\n2019) is better than that of zero-shot detectors, they are also not reliable. For example,TPR@1%FPR\nof OpenAI’s RoBERTa-Large-Detector drops from 100% to around 92% after our attack.\n\nIn another setting, we assume the attacker may have multiple access to the detector. That is, the\nattacker can query the detector with an input AI text passage, and the detector would reveal the\ndetection score to the attacker. For this scenario, we generate ten different paraphrases for an input\npassage and query the detector for the detection scores. For each AI text passage, we then select the\nparaphrase with the worst detection score for evaluating the ROC curves. As shown in Figure 4, with\nmultiple queries to the detector, an adversary can paraphrase more efficiently to bring down\nTPR@1%FPR of the RoBERTa-Large-Detector from 100% to 80%.\n\nRetrieval-based detectors. Detector in Kr-\nishna et al. (2023) is designed to be robust\nagainst paraphrase attacks. However, we\nshow that they can suffer from the recur-\nsive paraphrase attacks that we develop us-\ning DIPPER. We use 2000 passages (1000\ngenerated by OPT-1.3B and 1000 human\npassages) from the XSum dataset. AI out-\nputs are stored in the AI database by the\ndetector. As shown in Figure 5, this de-\ntector detects almost all of the AI outputs\neven after a round of paraphrasing. How-\never, the detection accuracy drops to 60%\nafter five rounds of recursive paraphras-\ning. As marked in the plot, the perplexity\nscore of the paraphrased text only degrades\nby 1.7 at a detection accuracy of below 60%.\nMoreover, retrieval-based detectors are con-\ncerning since they might lead to serious pri-\nvacy issues from storing users’ LLM con-\nversations.\n\nFigure 5: Recursive paraphrasing breaks the retrieval-\nbased detector (Krishna et al., 2023) with only slight\ndegradation in text quality. ppi refers to i recursion(s)\nof paraphrasing. Numbers next to markers denote the\nperplexity scores of the paraphraser output.\n\n6\n\n\fUnder review as a conference paper at ICLR 2024\n\n3 SPOOFING ATTACKS ON GENERATIVE AI-TEXT MODELS\n\nAn AI language detector without a low type-I error can cause harm as it might wrongly accuse a\nhuman of plagiarizing using an LLM. Moreover, an attacker (adversarial human) can generate a\nnon-AI text to be detected as AI-generated. This is called the spoofing attack. An adversary can\npotentially launch spoofing attacks to produce derogatory texts to damage the reputation of the target\nLLM’s developers. In this section, as a proof-of-concept, we show that current text detectors can\nbe spoofed to detect texts composed by adversarial humans as AI-generated. More details on the\nspoofing experiments are presented in Appendix D.\n\nSoft watermarking. As discussed in §2, soft water-\nmarked LLMs (Kirchenbauer et al., 2023a) generate\ntokens from the “green list” that are determined by\na pseudo-random generator seeded by the prefix to-\nken. Though the pseudo-random generator is private,\nan attacker can estimate the green lists by observing\nmultiple token pairs in the watermarked texts from the\ntarget LLM. An adversarial human can then leverage\nthe estimated green lists to compose texts by them-\nselves that are detected to be watermarked. In our\nexperiments, we estimate the green lists for 181 most\ncommonly used words in the English vocabulary. We\nquery the target watermarked OPT-1.3B model one\nmillion times to observe the token pair distributions\nwithin this smaller vocabulary subset we select. Based on the frequency of tokens that follow a prefix\ntoken in the observed generative outputs, we estimate green lists for each of the 181 common words.\nWe build a tool that helps adversarial humans create watermarked sentences by providing them with\nthe proxy green list we learn. We observe that the soft watermarking scheme can be spoofed to\ndegrade its detection AUROC from 99.8% to 1.3% (see Figure 6).\n\nFigure 6: ROC curve of a soft watermarking-\nbased detector (Kirchenbauer et al., 2023a)\nafter our spoofing attack.\n\nRetrieval-based detectors. Krishna et al. (2023) use a database to store LLM outputs to detect\nAI-text by retrieval. We find in our experiments (see Figure 12) that an adversary can spoof this\ndetector 100% of the time, even if the detector maintains a private database. Suppose an\nadversary, say a teacher, has access to a human written document S, say a student’s essay. The\nadversary can prompt the target LLM to paraphrase S to get S′. This results in the LLM, by design,\nstoring its output S′ in its private database for detection purposes. Now, the detector would classify\nthe original human text S as AI-generated since a semantically similar copy S′ is present in its\ndatabase. In this manner, a teacher can purposefully allege an innocent student to have plagiarised\nusing the retrieval-based detector.\n\nZero-shot and neural network-based detectors. In this setting, a malicious adversary could write a\nshort text in a collaborative work, which may lead to the entire text being classified as AI-generated.\nTo simulate this, we prepend a human-written text marked as AI-generated by the detector to all\nthe other human-generated text for spoofing. In other words, from 200 long passages in the XSum\ndataset, we pick the human text with the worst detection score for each detector considered in §2.3.\nWe then prepend this text to all the other human texts, ensuring that the length of the prepended\ntext does not exceed the length of the original text. Our experiments show that the AUROC of all\nthese detectors drops after spoofing (see plots in Appendix D). After this naïve spoofing attack, the\nTPR@1%FPR of most of these detectors drop significantly.\n\n4 HARDNESS OF RELIABLE AI TEXT DETECTION\n\nIn this section, we formally upper bound the AUROC of an arbitrary detector in terms of the TV\nbetween the distributions for M (e.g., AI text) and H (e.g., human text) over the set of all possible\ntext sequences Ω. We note that this result holds for any two arbitrary distributions H and M. For\nexample, H could be the text distribution for a person or group, while M could be the output text\ndistribution of a general LLM or an LLM trained by an adversary to mimic the text of a particular set\nof people.\n\n7\n\n\fUnder review as a conference paper at ICLR 2024\n\nWe use TV(M, H) to denote the TV between these two distributions and model a detector as a\nfunction D : Ω → R that maps every sequence in Ω to a real number. Sequences are classified\ninto AI-generated or human-generated by applying a threshold γ on this number. By adjusting the\nparameter γ, we can tune the sensitivity of the detector to AI and human-generated texts to obtain an\nROC curve.\nTheorem 1. The area under the ROC of any detector D is bounded as\n\nAUROC(D) ≤\n\n1\n2\n\n+ TV(M, H) −\n\nTV(M, H)2\n2\n\n.\n\nThe proof is deferred to Appendix C.1. Figure 7 shows how the above bound grows as a function of\nthe TV distance. This theorem states that as the TV distance between AI and human text distributions\nreduces, the AUROC of the best possible detector decreases. Based on our theory, an adversary can\nuse advanced LLMs to mimic human text to reduce the TV distance between human and AI text\ndistributions to evade text detection systems.\n\nFor a detector to have a good performance (say, AU-\nROC > 0.9), the distributions of human and AI-\ngenerated texts must be very different from each\nother (TV > 0.5 based on the figure). As M gets\nmore similar to H (say, TV < 0.2), the performance\nof even the best-possible detector becomes unreli-\nable (AUROC < 0.7). For some applications, say\nAI-text plagiarism, reliable detection should have\na low false positive rate (say, < 0.01) and a high\ntrue positive rate (say, > 0.9). Based on our theory,\nthis cannot be achieved even when the overlap be-\ntween the distributions is relatively low, say 11% (or\nTV = 0.9 − 0.01 = 0.89, based on equation 1 in\nAppendix C.1).\n\nFigure 7: Comparing the performance, in\nterms of AUROC, of the best possible detector\nto that of the baseline performance correspond-\ning to a random classifier.\n\nNote that, for a watermarked model, the above bound can be close to one as the TV between the\nwatermarked distribution and human-generated distribution can be high. Corollary 1 in Appendix C.2\ndiscusses how paraphrasing attacks can be effective in evading watermarks using Theorem 1. In\nAppendix C.3, we also present a tightness analysis of the bound in Theorem 1, where we show that\nfor any distribution H there exists M and a detector D for which the bound holds with equality. We\nalso discuss general trade-offs between true positive and false positive rates of detection in Corollaries\n2 and 3 in Appendix C.2. Theorem 2 in Appendix C.4 extends Theorem 1 to bound the AUROC of\nthe best possible detector by a function of the TV distance between LLM outputs generated using\npseudorandomness and human text distributions.\n\nIn studying the hardness of the detection problem, we consider the following assumption that for\na given human-text distribution H, more advanced LLMs mimicking H can lead to smaller TV.\nThus, using Theorem 1, the detection problem becomes increasingly more difficult. This is the core\nargument of our hardness result on AI text detection. Although the underlying assumption seems to\nbe intuitive given the capabilities of LLMs such as GPT-4 (OpenAI, 2023), a precise analysis of this\nassumption is quite difficult because estimating the true TV of the text distributions from a finite set\nof samples is extremely challenging. Nevertheless, we provide some empirical evidence supporting\nthis assumption using two sets of experiments. In all the experiments, we consistently observe that\nthe TV distance estimates between human and AI text distributions reduce as language models get\nmore advanced, indicating the increasing difficulty associated with AI text detection.\n\n(i) Using synthetic text data. We perform experiments on a toy synthetic text dataset where the exact\nTV distance can be calculated. We use the Markov assumption to generate the synthetic text data\nwith sequence length 3 using a randomly generated token transition matrix for varying vocabulary\nsizes. We use single-layer LSTMs of different hidden unit sizes to train on a dataset of size 20,000\nsampled from this synthetic data distribution using a default AdamW optimizer (Loshchilov & Hutter,\n2017). We compute the learned token transition matrix for the LSTM output distribution using the\nsoftmax logit values of the trained model. Using transition matrices of both distributions, we compute\nthe exact TV. Figure 8 shows that the exact TV distances between the learned and true synthetic\ndistributions reduce as the LSTM model size increases.\n\n8\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 8: Increasing model size reduces the exact TV\nbetween the true synthetic data distribution and the\nlearned distribution in all settings. Error bars report\nstandard deviations after 5 independent trials.\n\nFigure 9: Estimated TV distances of GPT-2\noutput datasets from the WebText dataset us-\ning meta-token sequences of varying lengths.\nTV decreases with model size for each length.\n\n(ii) Using projection. For discrete distributions, the TV distance can be computed as 1/2 of the\nsum of the point-wise differences between their probability density functions (PDFs). While this is\nmathematically simple since texts can be considered as token sequences with bounded length, it is not\npractical to compute true TV distances directly through estimating PDFs due to the size of the sample\nspace, which is approximately the size of the token set to the power of sequence length. To tackle\nthis issue, we split the original token set into five roughly equal partitions and assign a meta-token to\neach partition. Given a sequence of tokens from the original set, we construct a new sequence by\nreplacing each token with the corresponding meta-token. We estimate the PDFs of the sequences\nof meta-tokens created using texts from the WebText and GPT-2 output datasets. Since the set of\nmeta-tokens is significantly smaller than the original token set, estimating PDFs becomes much more\ntractable. We then use these PDFs to estimate the total variation distances of the output distributions\nof different GPT-2 models (GPT-2-Small, GPT-2-Medium, GPT-2-Large, and GPT-2-XL) from the\nWebText dataset. Figure 9 plots these TV estimates for different sequence lengths, averaged over\n30 runs of the experiment. We observe that the TV distance consistently decreases with increasing\nmodel size for all sequence lengths.\n\nThese experiments provide empirical evidence that more advanced LLMs can lead to smaller TV\ndistances. Thus, based on Theorem 1, reliable AI text detection would become increasingly difficult.\n\n5 CONCLUSION\n\nIn this paper, we analyze the performance of four different classes of detectors including watermarking-\nbased, neural-net based, zero-shot based and retrieval-based detectors and show their reliability issues.\nIn particular, we develop a strong attack called recursive paraphrasing that can break recently\nproposed watermarking and retrieval-based detectors. Using perplexity score computation as well as\nconducting various MTurk human study, we observe that our recursive paraphrasing only degrades\ntext quality slightly. We also show that adversaries can spoof these detectors to increase their type-I\nerrors. Spoofing attacks can lead to the generation of derogatory passages detected as AI-generated\nthat might affect the reputation of the LLM detector developers. Finally, we establish a theoretical\nconnection between the AUROC of the best possible detector to the TV distance between human\nand AI-text distributions that can be used to study the fundamental hardness of the reliable detection\nproblem for more advanced LLMs.\n\nA detector should ideally be helpful in reliably flagging AI-generated texts to prevent the misuse of\nLLMs. However, the cost of misidentification by a detector can be huge. If the false positive rate of\nthe detector is not low enough, humans (e.g., students) could be falsely accused of AI plagiarism.\nMoreover, a disparaging passage falsely detected to be AI-generated could affect the reputation of the\nLLM’s developers. As a result, the practical applications of AI-text detectors can become unreliable\nand invalid. Security methods need not be foolproof. However, we need to make sure that it is not an\neasy task for an attacker to break these security defenses. Thus, analyzing the risks of using current\nand future detectors can be vital to avoid creating a false sense of security.\n\n9\n\n\fUnder review as a conference paper at ICLR 2024",
    "abstract": "The rapid progress of Large Language Models (LLMs) has made them capable of performing astonishingly well on various tasks, including document completion and question answering. The unregulated use of these models, however, can potentially lead to malicious consequences such as plagiarism, generating fake news, spamming, etc. Therefore, reliable detection of AI-generated text can be critical to ensure the responsible use of LLMs. Recent works attempt to tackle this problem either using certain model signatures present in the generated text outputs or by applying watermarking techniques that imprint specific patterns onto them. In this paper, we show that these detectors are not reliable in practical scenarios. In particular, we develop a recursive paraphrasing attack to apply on AI text, which can break a whole range of detectors, including the ones using the watermarking schemes as well as neural network-based detectors, zero-shot classifiers, and retrieval-based detectors. Our experiments include passages around 300 tokens in length, showing the sensitivity of the detectors even in the case of relatively long passages. We also observe that our recursive paraphrasing only degrades text quality slightly, measured via perplexity scores and MTurk human study. Additionally, we show that even LLMs protected by watermarking schemes can be vulnerable against spoofing attacks aimed to mislead detectors to classify human-written text as AI-generated, potentially causing reputational damages to the developers. In particular, we show that an adversary can infer hidden AI text signatures of the LLM outputs without having white-box access to the detection method. Finally, we provide a theoretical connection between the AUROC of the best possible detector and the Total Variation distance between human and AI text distributions that can be used to study the fundamental hardness of the reliable detection problem for advanced language models.",
    "response": {
      "Soundness": 3,
      "Presentation": 4,
      "Contribution": 3,
      "Rating": 6,
      "Confidence": 3,
      "Strengths": "* The authors address the important problem of reliable AI-generated text detection. This problem is likely to become increasingly important with the rapid rise in popularity of large language models in society.\n\n* The proposed approach is simple yet effective in undermining the reliability of current AI-generated text detectors. The simplicity of the attack may also make it more likely to generalize to different models and domains.\n\n* The authors perform a human evaluation using Amazon's Mechanical Turk (MTurk) to evaluate the resulting paraphrased text passages. Their results suggest the original content of the text passage is preserved in addition to the grammar and overall text quality.\n\n* The authors also discuss the overall hardness of AI text detection, providing a formal upper bound of detection performance based on the total variation (TV) between AI-generated and human-generated text distributions.\n\n* The paper is well-written and easy to follow.",
      "Weaknesses": "* The experiments only use text passages from a single dataset, and tend to only evaluate a single model for each detector type. Evaluations on a wider range of datasets and detectors would greatly strengthen any generalizable claims regarding the proposed paraphrasing attack.\n\n* Lack of baseline methods. In many of the experiments, the proposed attack is the only method being evaluated, have the authors compared their attack with similar attacks?\n\n* I appreciate the human evaluation in the \"Watermarked AI Text\" section, however, this type of evaluation is missing in the other experimental sections. For example, Figure 5 claims a significant drop in detector accuracy with minimal degradation in text quality; however, it's unclear to me how significantly text quality degrades based on a perplexity score increase from 6.15 to 13.55.\n\n* The insight that smaller TV between AI-generated and human-generated text distributions leads to more difficult detection problems seems rather obvious. Although the authors show that more complex models can lead to smaller TV distances, the authors do not provide any empirical evidence that smaller TV distances actually lead to more difficult AI text detection.\n\n* There is no empirical runtime evaluation of the proposed attack.\n\n* There are several grammatical errors throughout the paper, consider using a service like Grammarly to fix these issues.\n\n* Figure 9 is not colorblind friendly.",
      "Questions": "* How is TV distance defined, and why is it difficult to compute for larger datasets?\n\n* How did the authors determine 5 rounds of paraphrasing to be sufficient?"
    }
  },
  {
    "paper_id": "le1UUMd45T",
    "summary": "Background: Multiobjective combinatorial optimization problems (MOCOPs) are critical in various fields like routing, logistics, and planning, requiring solutions that consider multiple, often conflicting objectives. Traditional methods, such as evolutionary algorithms, have been employed to approximate Pareto optimal solutions, with recent advances in deep reinforcement learning (DRL) showing promise in solving single-objective combinatorial optimization problems (COPs). However, these have limitations in addressing MOCOPs effectively.\nGap: While DRL has been used to tackle MOCOPs, existing models based on the 'Learn to Construct' paradigm struggle with achieving sufficient proximity and diversity of solutions towards the true Pareto front. They heavily depend on model quality and lack comprehensive exploration of potential Pareto solutions in subproblem neighborhoods.\nHypothesis: The authors propose that embedding weight-related policy networks into multiobjective evolutionary algorithm (MOEA) frameworks within a 'Learn to Improve' paradigm can effectively guide the search direction and improve the quality and diversity of solutions for MOCOPs.\nMethod: The method involves a weight-related policy network (WRPN) designed to be integrated into MOEA frameworks like NSGA-II, MOEA/D, and MOGLS. It guides improvement operations based on a proximal policy optimization algorithm with a shared baseline to reduce variance. A quality enhancement mechanism improves the Pareto set via instance augmentation. Experiments were conducted on the multiobjective traveling salesman problem and multiobjective vehicle routing problem, demonstrating significant improvements over existing methods.\nConclusion: The proposed WRPN within the 'Learn to Improve' paradigm outperforms state-of-the-art methods, providing improvements in both proximity and diversity of solutions on classic MOCOPs. The approach, with its quality enhancement mechanism, shows superior results compared to existing DRL approaches and classical solvers like LKH under weighted decomposition schemes.\n",
    "full text": "\nSOLVING MULTIOBJECTIVE COMBINATORIAL OPTI-\nMIZATION VIA LEARN TO IMPROVE METHOD\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nRecently, deep reinforcement learning (DRL) has been prevailing for solving mul-\ntiobjective combinatorial optimization problems (MOCOPs). Most DRL methods\nare based on the \"Learn to Construct\" paradigm, where the trained model(s) can\ndirectly generate a set of approximate Pareto optimal solutions. However, these\nmethods still suffer from insufficient proximity and poor diversity towards the true\nPareto front. In this paper, following the \"Learn to Improve\" (L2I) paradigm, we\npropose weight-related policy network (WRPN), a learning-based improvement\nmethod for solving MOCOPs. WRPN is incorporated into multiobjective evolu-\ntionary algorithm (MOEA) frameworks to effectively guide the search direction. A\nshared baseline for proximal policy optimization is presented to reduce variance in\nmodel training. A quality enhancement mechanism is designed to further improve\nthe Pareto set in model inference. Computational experiments conducted on two\nclassic MOCOPs, i.e., multiobjective traveling salesman problem and multiobjec-\ntive vehicle routing problem, indicate that our method achieves state-of-the-art\nresults. Notably, our WRPN module can be easily integrated into various MOEA\nframeworks such as NSGA-II, MOEA/D and MOGLS.\n\n1\n\nINTRODUCTION\n\nMultiobjective combinatorial optimization problems (MOCOPs) (Ehrgott and Gandibleux, 2000)\nhave wide applications in various fields, such as communication routing, investment planning, vehicle\nrouting, logistics scheduling, etc. Solving such kind of problems requires taking into account different\nroles’ preferences corresponding to different objectives, which may often conflict with each other. In\nprinciple, the goal of MOCOPs is to find the best compromise solutions (known as Pareto optimal\nsolutions) rather than a single optimal solution. The decision maker can eventually choose a particular\nPareto optimal solution according to his knowledge for practical usage.\n\nMOCOPs have been extensively studied in computational intelligence communities in past decades.\nSeeking a set of Pareto optimal solutions for an MOCOP is extremely challenging, even its scalarized\nsingle-objective subproblem is generally NP-hard. In real-world applications, heuristic methods,\nmostly based on evolutionary algorithms, are commonly introduced to cope with MOCOPs. They can\ngenerate a set of approximately efficient solutions in reasonable time. However, traditional heuristics\nmethods adopt simple rules or complex operations relied on experts’ experience and knowledge for a\nspecific problem, which may be limited to providing high-quality solutions for general MOCOPs.\n\nOver the past few years, neural learning methods, especially deep reinforcement learning (DRL)\nmethods, have made great achievements in solving single-objective combinatorial optimization\nproblems (COPs) (Bengio et al., 2021; Mazyavkina et al., 2021; Wang and Tang, 2021). By capturing\nimplicit patterns from a large number of problem instances, these methods can obtain better solutions\nthan traditional heuristic methods in many scenarios.\n\nIn more recent years, there are several attempts trying to tackle MOCOPs via DRL. Most approaches\n(Li et al., 2020; Zhang et al., 2021; Lin et al., 2022; Zhang et al., 2022) follow the “Learn to Construct\"\n(L2C) paradigm by using end-to-end learning models. Roughly speaking, they first decompose an\nMOCOP into multiple subproblems by different weight vectors, and then employ well-trained models\nto rapidly construct Pareto optimal solutions in the inference stage.\n\n1\n\n\fUnder review as a conference paper at ICLR 2024\n\nDespite some success of L2C methods has been achieved for MOCOPs, they still have their down-\nsides. On the one hand, L2C methods heavily rely on the quality of end-to-end models for solving\ndecomposed subproblems. Till now, even the state-of-the-art models (Kool et al., 2019; Kwon et al.,\n2020) still have substantial solution gaps for solving single-objective COPs, thereby resulting in the\nconvergence (or proximity) problem. On the other hand, L2C methods directly construct a solution\nfor a corresponding subproblem. It brings about the diversity problem due to the inadequate search\nfor potential Pareto solutions in the neighborhood of each subproblem.\n\nIn order to address the above issues, we propose weight-related policy network (WRPN) based\non another learning paradiam, named \"Learn to Improve\" (L2I) for solving MOCOPs. WRPN\nis embedded into typical multiobjective evolutionary algorithms (MOEAs) to perform potential\nimproving operations in parallel for individual solutions in the population. It iteratively updates the\ncurrent population and gradually approximates to the Pareto optimal solutions.\n\nThe contributions of our work can be summarized as follows.\n\n• We propose WRPN, which is the first generic L2I method for MOCOPs. It can automatically\nprovide efficient improvement operators for a batch of individual solutions, guided by a\npolicy network that is designed to simultaneously extract the weight and solution features.\n\n• We design a shared baseline, which is computed by a group of heterogeneous offsprings\ngenerated through evolution to realize low variance and mitigate the issue of local optima.\n\n• We devise a quality enhancement mechanism. Based on instance augmentation techniques,\nit further utilizes the external population of MOEAs and the Pareto dominance of solutions,\nthereby improving the proximity and diversity of the Pareto set.\n\n• We show that WRPN outperforms the existing state-of-the-art methods on classic MOCOPs.\nIt is even superior to the excellent LKH solver (Tinós et al., 2018) under the weighted-\nsum decomposition for multiobjective traveling salesman problem (MOTSP). Notably, our\nWRPN module can be easily integrated into various MOEA approaches such as NSGA-\nII (Deb et al., 2002), MOEA/D (Zhang and Li, 2007) and MOGLS (Jaszkiewicz, 2002).\n\n2 RELATED WORKS\n\nExact and Heuristics Methods for MOCOPs. Exact (Florios and Mavrotas, 2014) and heuris-\ntic (Herzel et al., 2021) algorithms are two groups of methods to solve MOCOPs in past decades. The\nformer can find all the Pareto-optimal solutions for only very small-scale problems, while the latter,\ncommonly used in practical applications, can find the approximate Pareto-optimal solutions within\nreasonable time. Multiobjective evolutionary algorithms are typical representatives of heuristic algo-\nrithms, including NSGA-II (Deb et al., 2002), MOEA/D (Zhang and Li, 2007), MOGLS (Jaszkiewicz,\n2002), PLS (Angel et al., 2004), and PPLS/D-C (Shi et al., 2022).\n\nDRL methods for COPs. In literature, some end-to-end DRL construction methods are developed\nfor solving single-objective COPs. The pioneering works (Bello et al., 2017; Nazari et al., 2018)\ntrain a pointer network to construct a near-optimal solution for COPs. Kool et al. (2019) propose\nan Attention Model (AM) based on the Transformer architecture. A representative work is policy\noptimization with multiple optima (POMO) (Kwon et al., 2020), which exploits the symmetry of\nsolutions to further improve the performance of end-to-end models. Distinguished from construction\nmethods, improvement methods, another important class of DRL methods, iteratively improve the\ncurrent solution, assisted by learning techniques. They generally achieve superior results compared\nwith construction methods although longer running time may be taken. Typical works include Wu\net al. (2021); Chen and Tian (2019); Lu et al. (2019); Ma et al. (2021) for vehicle routing problems.\n\nDRL methods for MOCOPs. There are relatively few works using DRL to solve MOCOPs. Most\nof them are construction methods based on decomposition (Zhang and Li, 2007). Their basic idea\nis to decompose MOCOPs into multiple subproblems according to prescribed weight vectors, and\nthen train a single model or multiple models to solve these subproblems. For example, Li et al.\n(2020); Zhang et al. (2021) train multiple models collaboratively through a transfer learning strategy.\nPreference-conditioned multi-objective combinatorial optimization (PMOCO) (Lin et al., 2022) trains\na hypernetwork-based model, which can generate the decoder parameters according to weight vectors\nfor solving subproblems. Meta-Learning-based DRL (MLDRL) (Zhang et al., 2022) first trains a\n\n2\n\n\fUnder review as a conference paper at ICLR 2024\n\nmeta-model and then quickly fine-tunes the meta-model based on weight vectors to solve subproblems.\nTo our best knowledge, PMOCO and MLDRL are two competitive DRL methods for MOCOPs.\n\n3 PRELIMINARIES\n\nMOCOP. The definition of an MOCOP can be described by min\nx∈X\n\nF (x) = (f1(x), · · · , fM (x)),\nwhere X ∈ RN is the feasible domain of N decision variables, F (x) is an M -dimensional objective\nvector and fi(x) represents the i-th objective function. Since the objectives are usually in conflict\nwith each other, a set of trade-off solutions is sought. The concept of Pareto optimality is introduced.\n\nDefinition 1 (Pareto dominance). Let u, v ∈ X, u is said to dominate v, i.e., u ≺ v, if: 1)\n∀i ∈ {1, · · · , M }, fi(u) ≤ fi(v), and 2) ∃j ∈ {1, · · · , M }, fj(u) < fj(v).\nDefinition 2 (Pareto optimality). A solution x∗ ∈ X is called a Pareto optimal solution if x∗ is not\ndominated by any other solutions, i.e., /∃x′ ∈ X : x′ ≺ x∗. All the Pareto optimal solutions can be\ndefined as P := {x∗ ∈ X|/∃x′ ∈ X : x′ ≺ x∗}, which is the Pareto set (PS). All the Pareto optimal\nobjective vectors constitute the Pareto front (PF).\n\nUtility Function. A utility (or aggregated) function can map each point in the objective space\ninto a scalar according to an M -dimensional weight vector satisfying (cid:80)M\ni=1 λi = 1 and λi ≥ 0.\nWeighted-Sum (WS) and Weighted-Tchebycheff are commonly used utility functions (Miettinen,\n2012). As the simplest representative, WS can be defined by min\nx∈X\n\nf (x|λ) = (cid:80)M\n\ni=1 λifi(x),\n\nKey Components of MOEA. The process of a generic MOEA involves initialization, parent selection,\nrecombination, improvement and population update (Verma et al., 2021). For MOCOPs, improvement\nis a key process to enhance the PF quality. This motivates us to devise an learning-based improvement\ncomponent in place of traditional one to seek high-quality individuals within the population.\n\n4 METHODOLOGY\n\nThe L2I framework collaboratively performs efficient local improvements and replaces the traditional\nproblem-specific heuristics. It contains a policy network that generates a node pair of local operations\nto potentially improving a batch of individual solutions. In what follows, we take MOTSP as an\nexample to elaborate the details of L2I. It is not difficult to generalize to other MOCOPs.\n\n4.1 DRL FORMALUTION\n\nL2I learns an improvement policy with respect to a given weight vector so as to approximate the\nentire PF. The improvement process can be deemed as a Markov decision process (MDP) as follows.\n\nState. For an MOTSP instance with N nodes at iteration t, the state st includes the features of\ninstance v, current solution, represented as a sequence xt with length N , and weight vector λ, i.e.,\nst = {v1, · · · , vN , x1\nt is the i-th visited\nnode in sequence xt.\n\nt , λ}, where vi is the coordinate of node i and xi\n\nt , · · · , xN\n\nAction. The action at = (i, j) is denoted as the node pair (i, j) of a pair-wise local search operator\nto be conducted. In MOTSP, we adopt an ensemble operator, i.e., the combination of three classic\noperators relocate, exchange, 2-opt (see Appendix C for details).\n\nTransition. The next state st+1 is obtained by performing action at = (i, j) on xt, i.e., selecting the\nbest solution from relocate(i, j), exchange(i, j) and 2-opt(i, j).\nReward. The reward function is defined by rt = f (x∗\nthe best solution found till iteration t and rt > 0 if an improved solution is found at iteration t.\n\nt |λ) − min{f (xt+1|λ), f (x∗\n\nt |λ)}, where x∗\n\nt is\n\n4.2 WEIGHT-RELATED POLICY NETWORK\n\nThe design of WRPN is based on the encoder-decoder architecture, as illustrated in Figure 1. More\ndetails about the network architecture is presented in Appendix B. The policy network first encodes\n\n3\n\n\fUnder review as a conference paper at ICLR 2024\n\nFigure 1: The weight-related policy network (WRPN).\n\nthe state into a hidden embedding, and then feeds it into the decoder to compute the action probability\nmatrix. The sampling and greedy decoding strategy are adopted in training and inference, respectively.\n\nThe raw features of each solution in the solution population are mapped into two sets of embeddings,\nincluding node embeddings and positional embeddings, while the associated weight vector is mapped\ninto weight embeddings.\n\nThe node embedding (NE) hi with dimension dh is obtained by the linear projection of its node\nfeatures. The positional embedding (PE) gi with dimension dg is initialized by the cyclic positional\nencoding (Ma et al., 2021) as follows.\n\ng(d)\ni =\n\n(cid:26)sin(ωd · (z(i) mod 4π\n) − 2π\nωd\nωd\n) − 2π\ncos(ωd · (z(i) mod 4π\nωd\nωd\n(cid:25)\n(cid:24) N + 1\n2π/ωd\n\nz(i) =\n\n2π\nωd\n\ni − 1\nN\n1\n\n), if d is even\n), if d is odd\n\n(cid:40) 3⌊d/3⌋\ndg\n\nωd =\n\n(N − N\n\n⌊dg /2⌋ ) + N\nN,\n\n1\n\n⌊dg /2⌋ ,\n\nif d < ⌊dg/2⌋\notherwise\n\n(1)\n\n(2)\n\n(3)\n\ni\n\nHere, g(d)\n(i = 1, . . . , N, d = 1, . . . , dg) is the d-th dimension of gi, z(i) is a pattern to make N\nnodes linearly spaced, and ωd is the angular frequency. The weight embedding (WE) with dimension\ndw is obtained by the corresponding weight features through a linear projection.\n\nSince a weight vector is applied to guide the search direction, the enhanced node embedding (ENE)\nˆh is obtained by fusing WE and NE through a feature-wise linear modulation (FiLM) (Brockschmidt,\n2020). Similarly, the enhanced positional embedding (EPE) ˆg is obtained by the fusion of WE and\nPE. Such design can better represent the node features and solution features under a weight vector.\n\n4.2.1 ENCODER\n\nThere are L (L = 3) Transformer-style stacked encoders. For better readability, we omit the\nsuperscript l for the l-th stacked encoder. Instead, we use ˆh and ˆg to indicate the l-th input embeddings,\nand use ˜h and ˜g to indicate the l-th output embeddings, which are also equivalent to the (l + 1)-th\ninput embeddings.\nIn each stacked encoder, ˆh and ˆg are fed into a Dual-Aspect Collaborative Attention (DAC-Att)\nlayer (Ma et al., 2021), followed by the batch normalization, and two separate feed-forward network\n(FFN) sub-layers, respectively. Because ˆh and ˆg are originated from two sources, DAC-Att model\nexhibits good performance compared with the vanilla attention model (Kool et al., 2019).\nDAC-Att layer. Based on the trainable matrices W Qg\nand W Kh\n\nm ∈ Rdˆh×dk\nm ∈ Rdˆh×dk (dq = dk = dˆh/m = dˆg/m, m = 4 is the number of attention heads), the\n\nm ∈ Rdˆg×dq , W Kg\n\nm ∈ Rdˆg×dq , W Qh\n\n4\n\n\fUnder review as a conference paper at ICLR 2024\n\nDAC-Att layer computes the node attention score αh\neach head m, which are given by Eq. (4).\n\ni,j,m and the positional attention score αg\n\ni,j,m for\n\n1\n√\ndk\ni,j and αg\n\nαh\n\ni,j,m =\n\n1\n√\ndk\n\n(ˆhiW Qh\n\nm )(ˆhjW Kh\n\nm )T , αg\n\ni,j,m =\n\n(ˆgiW Qg\n\nm )(ˆgjW Kg\n\nm )T ,\n\n(4)\n\ni,j,m and αg\n\ni,j,m of each head are concatenated into αh\n\nThe αh\ni,j and ˜αg\n˜αh\ntrainable matrix W Vh, W Vg , W Rh, W Rg ∈ Rd˜h×dv and W Oh, W Og ∈ R2d˜h×dv (dv = d˜h).\n\ni,j, then further normalized to\ni,j via Softmax. Finally, the output embedding ˜hi and ˜gi are computed by Eq. (5–6) with\n\n˜hi = concat[\n\nN\n(cid:88)\n\nj=1\n\ni,j(ˆhjW Vh),\n˜αh\n\n˜gi = concat[\n\nN\n(cid:88)\n\n˜αg\ni,j(ˆgjW Vg ),\n\nj=1\n\nN\n(cid:88)\n\nj=1\n\nN\n(cid:88)\n\nj=1\n\ni,j(ˆhjW Rh )]W Oh\n˜αg\n\ni,j(ˆgjW Rg )]W Og\n˜αh\n\n(5)\n\n(6)\n\nFFN. It consists of two linear layers and one 64-dimensional hidden sub-layer with the ReLU\nactivation function.\n\n4.2.2 DECODER\n\nIn the decoder, the max-pooling and multi-head attention (MHA) modules are applied to indepen-\ndently generate the node-pair selection proposals for ˜h and ˜g, after which the respective outputs are\naggregated by a multi-layer perception (MLP).\n\nMax-pooling. Two independent max-pooling sub-layers are adopted to aggregate the global feature\nrepresentation for ˜h and ˜g, respectively.\n\nMHA. The MHA layer effectively represents the attention correlations for each node pair. Given the\nstate embeddings (˜h, ˜g), the correlations score matrices Y h, Y g ∈ RN ×N are computed as the dot\nproduct of the query and key matrices similarly to Kool et al. (2019). Thereafter, the infeasible node\npairs are masked as −∞ before Softmax.\n\nMLP. We adopt a four-layer MLP with structure (2m × 32 × 32 × 1) to aggregate two node-pair\nselection proposals from ˜h and ˜g.\n\n4.3 A SHARED BASELINE FOR PROXIMAL POLICY OPTIMIZATION\n\nWe use n-step proximal policy optimization (PPO) (Savelsbergh, 1990) to train the policy network\nand further design a shared baseline to reduce the variance of the training. The overall training\nprocess is presented in Algorithm 1.\n\nThe training process involves a total of E epochs and B batches per epoch. For each batch, a set of\ntraining instances D is randomly generated (line 3) and a population of weight vectors is randomly\nsampled from the uniform distribution (line 4). The curriculum learning (CL) strategy (Bengio\net al., 2009) is used to derive the initial state for better sample efficiency (lines 6–8). The n-step\nreturn estimation is then exploited to achieve a trade-off between effective reward propagation and\nbias-variance following the original design of n-step PPO (lines 14–18).\nWe design a shared baseline bshare\n(lines 16–17), rather than directly use a baseline obtained by\ngreedy rollout or an extra critic network. The shared baseline and reinforcement learning loss are\ngiven in Eq. (7) and Eq. (8), respectively.\n\nt′\n\nbshare\nt′\n\n=\n\n1\nP\n\nP\n(cid:88)\n\np=1\n\nRp\nt′\n\n▽θJ (θ) =\n\n(cid:80)\n\n(cid:80)t+n\n\n1\nD\nn|D|\nclip[ πθ(at′ |st′ )\n\nt′=t min{ πθ(at′ |st′ )\nπold(at′ |st′ ) At′,\nπold(at′ |st′ ) , 1 − ε, 1 + ε]At′}\n\n(7)\n\n(8)\n\nwhere P is population size, Rp\nt′ is the cumulative reward of offspring p after iteration t′ steps and At′\nis the advantage compared to bshare\n. Such a shared baseline exploiting all solutions of the population\nt′\ncan induce less variance due to the zero-mean advantage, and also can be computed more efficiently.\n\n5\n\n\fUnder review as a conference paper at ICLR 2024\n\nAlgorithm 1 The training process of WRPN.\nInput: Initial policy network πθ, clipping threshold ε, population size P , weight vector distribution\nΛ; learning rate ηθ, learning rate decay β, mini-batch κ, training steps Ttrain, CL scalar ρCL.\n\nfor b ← 1 : B do\n\nt′=t where at′ ∼ πθ(at′|st′);\n\nRandomly generate training instances D;\nλp ← SampleWeight(Λ), ∀p ∈ {1, · · · , P };\nδp ← InitialSolutions(D) , ∀p ∈ {1, · · · , P };\nImprove δp via {(πθ, λp)} for T = e/ρCL steps;\ns0 ← InitialState(δp, λp), ∀p ∈ {1, · · · , P }; t ← 0;\nwhile t < Ttrain do\n\nOutput: Trained policy network πθ.\n1: for e ← 1 : E do\n2:\n3:\n4:\n5:\n6:\n7:\n8:\n9:\n10:\n11:\n12:\n13:\n14:\n15:\n16:\n17:\n18:\n19:\n20:\n21:\n22:\n23:\n24: end for\nNote: The superscipt p of sp, ap, Ap, rp and Rp for individual p is omitted from lines 7 to 16 for\nreadability.\n\nGet {(st′, at′, rt′)}t+n\nt ← t + n, πold ← πθ;\nfor z ← 1 : κ do\nRt+1 = 0;\nfor t′ ∈ {t, t − 1, · · · , t − n} do\nRt′ ← rt(λ) + γRt′+1;\nCompute baseline bshare\nt′\nAt′ ← Rt′ − bshare\n;\n\nend for\nCompute RL loss J (θ) using Eq. (8);\nθ ← θ + ηθ ▽θ J (θ);\n\nend for\nend while\n\nend for\nηθ ← βηθ\n\nusing Eq. (7);\n\nt′\n\n4.4 QUALITY ENHANCEMENT\n\nTo further enhance the proximity and diversity of the Pareto set, we propose a quality enhancement\nmechanism based on instance augmentation (Lin et al., 2022). Specifically, an instance of MOCOPs\nstill retains the equivalent optimal solution after multiple transformations, such as spatial rotations\nand reflections. These instances are then solved by an MOEA with the trained WRPN to further\npromote exploration. The final external population is obtained by aggregating all the non-dominated\nsolutions during search with respect to different transformed instances. More details of the algorithm\nimplementation can be found in Appendix G.\n\n5 EXPERIMENTS\n\nThe experiments are conducted on a server with an Intel E5-2678 v3 CPU @ 2.50GHz and 8 TITAN\nXp GPUs. All the compared methods are implemented in Python using the Pytorch library, except that\nLKH (Helsgaun, 2000; Tinós et al., 2018) is in C-style. Our codes will be made publicly available.\n\n5.1 EXPERIMENTAL SETUP\n\nProblems. We introduce two classic MOCOPs, i.e., multiobjective traveling salesman prob-\nlem (MOTSP) (Lust and Teghem, 2010) and multiobjective capacitated vehicle routing problem\n(MOCVRP) (Jozefowiez et al., 2008). Concretely, we consider bi/tri-objective (Bi/Tri-TSP) and\nbi-objective CVRP (Bi-CVRP). For M -objective MOTSP, M groups of coordinates are given to\ndefine M Euclidean travelling costs between a pair of nodes. The objective is to simultaneously\nminimize M total costs. For Bi-CVRP, the objective is to minimize the total route lengths, as well as\nthe length of the longest route. See Appendix A for detailed descriptions of MOTSP and MOCVRP.\n\n6\n\n\fUnder review as a conference paper at ICLR 2024\n\nCompared approaches. We compare our WRPN with three kinds of representative approaches.\n(1) MOEA approaches: MOEA/D and NSGA-II with 5000 generations and MOGLS with 2000\ngenerations and 50 local improvements; PPLS/D-C (Shi et al., 2022) with 200 generations. All of\nthem are efficiently implemented in parallel. These MOEAs also use relocate, exchange, and 2-opt\noperator for MOTSP and MOCVRP. The population sizes of the former three are set identically\nto 100, while the subregion number of PPLS/D-C is set to 10 according to the original paper. (2)\nL2C approaches: DRL-MOA (Li et al., 2020), POMO-T (Kwon et al., 2020), PMOCO (Lin\net al., 2022) and MLDRL (Zhang et al., 2022). The former two use decomposition and parameter\nmigration strategies with Point Network (Vinyals et al., 2015) and POMO (Kwon et al., 2020) as their\nsingle-objective submodels, respectively. The third one is the preference-conditioned multiobjective\ncombinatorial optimization in which the submodel is also POMO. The last one is the meta-learning\napproach with POMO as its model architecture. Note that all the above approaches consider 101\ndecomposed subproblems. (3) The state-of-the-art single-objective solvers under WS scalarization.\n101 decomposed subproblems are considered. Each subproblem is solved by LKH or OR-Tools1 for\nMOTSP.\n\nMetrics. We primarily adopt the hypervolume (HV) (Zitzler et al., 2007), the number of non-\ndominated solutions (|NDS|) and the inverted generational distance plus (IGD+) (Ishibuchi et al.,\n2016) to evaluate the performance of the compared algorithms. In general, the larger the HV is and\nthe smaller the IGD+ is, the better the corresponding algorithm performs. Our proposed method is\nhighlighted in italic and the best result and its statistically indifferent results are highlighted in bold.\nThe second-best result and the one without statistical significance to it are highlighted as underline.\nA Wilcoxon rank-sum test with a significance level 1% is applied to compare the experimental results.\nAdditional details of the above metrics are introduced in Appendix D.\n\nTraining details. For MOTSP with different scales, the model is trained with E = 200 epochs. Each\nepoch has B = 90 batches with batch size 120. For MOCVRP, we respectively set E = 200/200/100\nwith batch size 120/100/32 for MOCVRP-20/50/100. The population size P is set to 20 in training.\nWe set PPO-step n = 4 and Ttrain = 200 for MOTSP, while set n = 5 and Ttrain = 500 for\nMOCVRP. The gradient norm is clipped within ε = 0.04/0.2/0.45 for the problems with scale\n20/50/100. The reward discount factor is set to γ = 0.999. The Adam optimizer with a learning\nrate ηθ = 10−4 is adopted, while decaying with β = 0.985 per epoch. To accelerate convergence,\nthe pre-trained 50-size model is used to train 100-size model. We also implement GPU parallel\ntraining. For MOTSP-20/50/100, an epoch roughly takes 7/22/40 minutes, respectively. For\nMOCVRP-20/50/100, it takes about 17/30/50 minutes, respectively.\n\nInference details. During inference, the population size P is set to 100. The number of iterations T is\nset to 2000/5000. The number of local operations per iteration is set to 50, except for MOCVRP-100\nthat is increased to 100 due to its difficulty. All the compared methods are evaluated on the same\ntest dataset with 200 random instances. We further test the performance of PMOCO and MLDRL\nwith instance augmentation, in which 8 transformed instances are considered. Similarly, our WRPN\nmodule based on MOGLS is also equipped with quality enhancement using 8 transformed instances.\n\n5.2 RESULTS\n\nMOTSP. For each problem size, the average IGD+, average HV, gap to the best HV, number of\nnon-dominated solutions, and average (inference) time on 200 random instances are shown in Table\n1. Compared with basic MOEA approaches, WRPN incorporated with different MOEAs have\noutstanding performance. In particular, MOGLS+WRPN achieves the lowest gap in fewest time for\nall problem sizes. The comprehensive metrics, HV and IGD+, both indicate that WRPN is excellent in\nproximity and diversity. When quality enhancement mechanism is adopted, MOGLS+WRPN(AUG)\nwith 5K iterations attains the highest HV for all sizes. It is even better than WS-LKH which is\nbased on weight decomposition. Moreover, our WRPN achieves larger gaps compared with other\ncompetitors on Tri-TSP, e.g., a 2.94% gap for WS-LKH on Tri-TSP-100, which shows that WRPN is\nable to excavate more potential Pareto solutions in the higher-dimensional objective space.\n\nIt is worth noting that WRPN may generate more than 100/105 non-dominated solutions. Its HV can\ntake advantage of the mass number of solutions. In Appendix F.2, we further analyze the effect of the\nnumber of non-dominated solutions on HV.\n\n1https://developers.google.com/optimization/\n\n7\n\n\fUnder review as a conference paper at ICLR 2024\n\nTable 1: Comparison results on 200 random MOTSP instances.\n\nMethod\n\nWS-LKH\nWS-ORTools\nMOEA/D(T=5K)\nNSGA-II(T=5K)\nMOGLS(T=5K)\nPPLS/D-C(T=200)\n\nBi-TSP-50\nIGD+ HV GAP |NDS| TIME IGD+ HV GAP |NDS| TIME IGD+ HV GAP |NDS| TIME\n\nBi-TSP-100\n\nBi-TSP-20\n\n0.23 0.6270 0.44% 15\n0.22 0.6256 0.67% 20\n0.35 0.6264 0.81% 16\n0.17 0.6283 0.24% 77\n0.18 0.6287 0.17% 49\n0.18 0.6256 0.67% 71\n\n0.19 0.6415 0.56% 38\n3s\n0.30 0.6353 1.52% 49\n2s\n47s\n0.56 0.6336 1.78% 56\n2.3m 1.07 0.6145 4.74% 98\n0.48 0.6296 2.40% 88\n14s\n0.45 0.6282 2.62% 213\n68s\n\n0.12 0.7090 0.00% 65\n1.8m\n30s\n78s\n0.40 0.7004 1.21% 66\n16s\n2.2m\n72s\n0.71 0.6951 1.96% 181\n2.6m\n2.3m 2.09 0.6634 6.43% 42\n18s\n31s\n2.30 0.6506 8.24% 103\n6.4m 1.07 0.6844 3.47% 372 31.2m\n\nDRL-MOA\nPOMO-T\nPMOCO\nMLDRL\nMOEA/D+WRPN(T=5K)\nNSGA-II+WRPN(T=5K)\nMOGLS+WRPN(T=2K)\nMOGLS+WRPN(T=5K)\n\n0.78 0.5973 5.16% 17\n0.24 0.6257 0.65% 23\n0.20 0.6259 0.62% 18\n0.20 0.6271 0.43% 26\n0.14 0.6297 0.02% 76\n0.14 0.6296 0.03% 93\n0.14 0.6296 0.03% 76\n0.14 0.6297 0.02% 84\n\n1.50 0.5909 8.40% 31\n2s\n0.33 0.6360 1.41% 57\n2s\n0.31 0.6351 1.55% 49\n2s\n0.28 0.6364 1.35% 63\n2s\n95s\n0.09 0.6445 0.09% 326\n2.8m 0.11 0.6433 0.28% 272\n0.09 0.6443 0.12% 241\n15s\n0.08 0.6446 0.08% 302\n37s\n\n4s\n4s\n4s\n4s\n\n3.40 0.6390 9.87% 38\n0.60 0.6970 1.69% 70\n0.54 0.6957 1.88% 71\n0.56 0.6969 1.71% 73\n2.5m 0.20 0.7061 0.41% 258\n4.4m 0.22 0.7057 0.47% 234\n0.22 0.7055 0.49% 192\n22s\n0.16 0.7069 0.30% 233\n55s\n\n0.23 0.6271 0.43% 15\nPMOCO(AUG)\n0.23 0.6271 0.43% 16\nMLDRL(AUG)\nMOGLS+WRPN(AUG)(T=2K) 0.14 0.6298 0.00% 92\nMOGLS+WRPN(AUG)(T=5K) 0.14 0.6298 0.00% 94\n\n2s\n2s\n27s\n66s\n\n0.16 0.6401 0.78% 51\n0.13 0.6408 0.67% 63\n0.01 0.6450 0.02% 490\n0.00 0.6451 0.00% 465\n\n0.33 0.7013 1.09% 77\n5s\n0.34 0.7022 0.96% 83\n5s\n60s\n0.10 0.7081 0.13% 324\n2.7m 0.05 0.7090 0.00% 393\n\n7s\n7s\n7s\n7s\n4.6m\n5.8m\n44s\n1.8m\n\n12s\n12s\n2.7m\n6.9m\n\nMethod\n\nWS-LKH\nWS-ORTools\nMOEA/D(T=5K)\nNSGA-II(T=5K)\nMOGLS(T=2K)\nPPLS/D-C(T=200)\n\nTri-TSP-50\nIGD+ HV GAP |NDS| TIME IGD+ HV GAP |NDS| TIME IGD+ HV GAP |NDS| TIME\n\nTri-TSP-100\n\nTri-TSP-20\n\n0.24 0.4712 1.34% 61\n0.23 0.4703 1.53% 76\n0.23 0.4704 1.51% 104\n0.37 0.4572 4.27% 527\n0.16 0.4722 1.13% 242\n0.11 0.4698 1.63% 875\n\n0.82 0.5076 2.94% 105\n33s\n0.51 0.4440 3.42% 103\n4s\n1.26 0.4947 5.41% 105\n19s\n0.71 0.4348 5.42% 102\n3s\n78s\n2.19 0.4698 10.17% 978\n48s\n0.61 0.4376 4.81% 627\n3.1m 8.48 0.3301 36.88% 588\n2.2m 3.55 0.3295 28.32% 624\n15s\n5.52 0.3789 27.55% 496\n21s\n1.49 0.3958 13.90% 448\n3.4m 0.96 0.4174 9.20% 3723 19.3m 3.02 0.4376 16.33% 8096\n\nPMOCO\nMLDRL\nMOEA/D+WRPN(T=5K)\nNSGA-II+WRPN(T=5K)\nMOGLS+WRPN(T=2K)\nMOGLS+WRPN(T=5K)\n\n0.25 0.4693 1.74% 71\n0.24 0.4701 1.61% 72\n0.08 0.4734 0.88% 347\n0.13 0.4734 0.88% 543\n0.08 0.4759 0.36% 486\n0.06 0.4766 0.21% 693\n\n0.77 0.4315 6.14% 103\n0.74 0.4317 6.10% 103\n0.35 0.4485 2.44% 683\n\n1.57 0.4858 7.11% 105\n2s\n1.57 0.4852 7.23% 104\n2s\n2.6m 0.67 0.5091 2.66% 709\n95s\n3.8m 0.35 0.4474 2.68% 5033 4.4m 1.44 0.4948 5.39% 7074\n0.55 0.5115 2.20% 1436\n15s\n0.38 0.5159 1.36% 2478\n40s\n\n0.24 0.4528 1.50% 1269\n0.17 0.4553 0.96% 2190\n\n26s\n59s\n\n4s\n4s\n\nPMOCO(AUG)\n0.24 0.4712 1.34% 62\n0.24 0.4712 1.34% 61\nMLDRL(AUG)\nMOGLS+WRPN(T=2K)(AUG) 0.03 0.4774 0.04% 1078\nMOGLS+WRPN(T=5K)(AUG) 0.01 0.4776 0.00% 1358\n\n7s\n7s\n38s\n91s\n\n1.19 0.4956 5.24% 105\n19s\n0.57 0.4409 4.09% 104\n1.16 0.4958 5.20% 105\n19s\n0.55 0.4408 4.11% 104\n0.16 0.5206 0.46% 5327\n90s\n0.06 0.4587 0.23% 4629\n0.03 0.4597 0.00% 4243 3.6m 0.06 0.5230 0.00% 8007\n\nTable 2: Comparison results on 200 random MOCVRP instances.\n\nMethod\n\nMOEA/D(T=5K)\nNSGA-II(T=5K)\nMOGLS(T=5K)\nPPLS/D-C(T=200)\n\nBi-CVRP-50\nIGD+ HV GAP |NDS| TIME IGD+ HV GAP |NDS| TIME IGD+ HV GAP |NDS| TIME\n\nBi-CVRP-100\n\nBi-CVRP-20\n\n0.68 0.3913 9.19% 8\n0.06 0.4273 0.84% 14\n0.04 0.4285 0.56% 11\n0.04 0.4287 0.51% 15\n\n7m 0.33 0.4012 2.38% 6\n5.7m 0.75 0.3937 4.23% 14\n0.44 0.3992 2.87% 11\n96s\n24.6m 0.31 0.4007 2.51% 17\n\n22.2m 0.88 0.3976 2.07% 7\n10.2m 1.67 0.3821 5.89% 13\n4.2m 1.14 0.3939 2.98% 11\n1.00 0.3946 2.81% 20\n4.1h\n\nPOMO-T\nPMOCO\nMLDRL\nMOEA/D+WRPN(T=5K)\nNSGA-II+WRPN(T=5K)\nMOGLS+WRPN(T=2K)\nMOGLS+WRPN(T=5K)\n\n0.03 0.4287 0.51% 7\n0.06 0.4267 0.97% 5\n0.04 0.4181 2.97% 5\n0.01 0.4304 0.09% 16\n0.01 0.4295 0.32% 15\n0.01 0.4307 0.05% 14\n0.01 0.4308 0.02% 15\n\n3s\n3s\n3s\n\n0.04 0.4076 0.83% 10\n0.08 0.4031 1.92% 6\n0.04 0.4020 2.19% 9\n8.2m 0.03 0.4092 0.23% 20\n6.4m 0.08 0.4075 0.85% 19\n53s\n0.03 0.4097 0.32% 18\n2.2m 0.02 0.4101 0.22% 20\n\n7s\n6s\n6s\n\n0.16 0.4055 0.12% 12\n0.24 0.3908 3.74% 5\n0.17 0.4022 0.94% 11\n32.1m 0.21 0.4022 0.54% 16\n12m 0.29 0.4045 0.37% 26\n2.2m 0.34 0.4034 0.64% 18\n4.6m 0.30 0.4044 0.39% 20\n\n0.04 0.4294 0.35% 6\nPMOCO(AUG)\nMLDRL(AUG)\n0.03 0.4219 2.09% 6\nMOGLS+WRPN(AUG)(T=2K) 0.00 0.4309 0.00% 17\nMOGLS+WRPN(AUG)(T=5K) 0.00 0.4309 0.00% 18\n\n0.04 0.4077 0.80% 7\n3s\n0.02 0.4065 1.09% 10\n3s\n0.01 0.4107 0.07% 24\n84s\n3.5m 0.01 0.4110 0.00% 26\n\n7s\n7s\n\n0.18 0.3966 2.32% 7\n0.14 0.4059 0.02% 13\n4.1m 0.28 0.4052 0.20% 24\n10m 0.24 0.4060 0.00% 27\n\n2m\n83s\n2.3m\n3.4m\n32s\n1.1h\n\n7s\n7s\n4.3m\n7.6m\n49s\n2.2m\n\n86s\n86s\n3.8m\n9.2m\n\n38.5m\n14.7m\n6.6m\n14.8h\n\n14s\n13s\n13s\n40.8m\n20.2m\n6.9m\n17.4m\n\n14s\n14s\n16.5m\n41m\n\nMOCVRP. Table 2 presents the results for MOCVRP instances. From the table, we can find that\nMOEAs equipped with WRPN are significantly better than MOEAs in terms of both running time and\nsolution quality. It also outperforms advanced L2C methods in terms of solution quality. Although\nWRPN generally requires longer time, its training efficiency is better than L2C methods. We have\nverified that the number of training samples of WRPN is about one to two orders of magnitude less\nthan PMOCO and MLDRL. Finally, it is worth noting that MOCVRP is more difficult to solve than\n\n8\n\n\fUnder review as a conference paper at ICLR 2024\n\nMOTSP. In many leading heuristic approaches, complex problem-specific operators are proposed\nto locally improve a solution. In our WRPN methods for MOCVRP, the ensemble operator only\nincludes relocate, exchange and 2-opt (see Appendix C for details). Such a simple design can partly\nvalidate the effectiveness of WRPN.\n\n5.3 ABLATION STUDY\n\nEffects of the weight-related policy network. To verify our design of the weight-related policy\nnetwork, we compare WRPN with DACT (Ma et al., 2021), a representative L2I method for solving\nsingle-objective COPs. DACT cannot directly address MOCOPs, since its policy network cannot\nhandle the weight vector. Therefore, we combine DACT with transfer learning (Li et al., 2020) to\ncope with decomposed subproblems, thus solving MOCOPs. Specifically, we adopt the DACT model\npre-trained by the authors and performe transfer learning with 5 epochs per subproblem (keeping\nthe training dataset size consistent with WRPN). The final results based on the MOGLS framework\nare displayed in Table 3. It can be observed that DACT exhibits poor performance with expensive\ninference time. We also compare our WRPN with DACT on single objective that DACT specializes\nin. The results can be found in Appendix H.\n\nEffects of the shared baseline. To highlight the effectiveness of our shared baseline in training, we\ncompare it with the critic network as baseline (Ma et al., 2021) on 200 MOTSP-50 instances, while\nkeeping other settings of the model unchanged. For the critic baseline, it consists of one MHA layer,\ntwo pooling layers and one three-layer MLP, which produces an estimated value for the current state\nby using the encoder output (˜h, ˜g). Figure 2 shows that the model trained with our shared baseline\nconverges much faster than that with the critic baseline.\n\nTable 3: Effects of the weight-\nrelated policy network.\n\nTable 4: Effects of the quality\nenhancement.\n\nMethod\n\nBi-VRP-20\n\nMethod\n\nHV\n\nGAP TIME\n\nBi-TSP-100\n\nHV\n\nGAP TIME\n\n0.3979 7.66% 1.5h\nDACT-T(T=2K)\nWRPN(T=2K)\n0.4307 0.05% 53s\nWRPN(AUG)(T=5K) 0.4309 0.00% 84s\n\nWRPN (VIA) (T=2K) 0.7018 1.02% 2.5m\nWRPN (VIA) (T=5K) 0.7044 0.65% 5.7m\n0.7081 0.13% 2.7m\nWRPN (QE) (T=2K)\n0.7090 0.00% 6.9m\nWRPN (QE) (T=5K)\n\nFigure 2: Effects of the shared\nbaseline.\n\nEffects of the quality enhancement. To provide further insights and comparisons, we conducted\ncomplementary experiments to evaluate the impact of the proposed quality enhancement (QE)\ntechnique and a vanilla instance augmentation (VIA) technique proposed by Lin et al. (2022). The\nVIA technique simply retains the best solution of multiple transformed instances in a prescribed\ndirection during each iteration, without utilizing a set of temporal external populations to archive\nnon-dominated solutions. The results based on MOGLS are presented in Table 4. It shows that our\nQE mechanism outperforms the VIA technique in terms of HV. This finding clearly demonstrates\nthe utility of QE as an effective mechanism for enhancing diversity exploration in MOCOPs. More\nexperimental results can be found in Appendix G.\n\n6 CONCLUSION\n\nL2I is a generic DRL-based improvement paradigm for MOCOPs that iteratively improve a population\nof solutions. We propose WRPN, an end-to-end model, to effectively guide the local improvement.\nA shared baseline is designed to train WRPN efficiently. The quality enhancement mechanism is\nalso adopted to improve the search. Extensive experiments on MOTSP and MOCVRP justify that\nWRPN can produce promising PF with good proximity and diversity, and achieve state-of-the-art\nresults. Note that WRPN is also applicable to other MOEA frameworks. In future, more advanced\nDRL techniques can be devised to better learn implicit patterns of MOEA components. In addition,\nit would be interesting to investigate the performance of other L2I approaches to solve complex\nMOCOPs with many objectives.\n\n9\n\n\fUnder review as a conference paper at ICLR 2024",
    "abstract": "Recently, deep reinforcement learning (DRL) has been prevailing for solving multiobjective combinatorial optimization problems (MOCOPs). Most DRL methods are based on the \"Learn to Construct\" paradigm, where the trained model(s) can directly generate a set of approximate Pareto optimal solutions. However, these methods still suffer from insufficient proximity and poor diversity towards the true Pareto front. In this paper, we propose \"Learn to Improve\" (L2I), a learning-based improvement method for solving MOCOPs. We embed a weight-related policy network into multiobjective evolutionary algorithm (MOEA) frameworks to effectively guide the search direction. A shared baseline for proximal policy optimization is presented to reduce variance in model training. A quality enhancement mechanism is designed to further improve the Pareto set in model inference. Computational experiments conducted on two classic MOCOPs, i.e., multiobjective traveling salesman problem and multiobjective vehicle routing problem, indicate that our method achieves state-of-the-art results. Notably, our L2I module can be easily integrated into various MOEA frameworks such as NSGA-II, MOEA/D and MOGLS.",
    "response": {
      "Soundness": 4,
      "Presentation": 4,
      "Contribution": 3,
      "Rating": 8,
      "Confidence": 4,
      "Strengths": "L2I introduces a new DRL-based approach for MOCOPs. Unlike the traditional \"Learn to Construct\" methodology, L2I emphasizes iterative improvements. The L2I module has demonstrated adaptability as it can be integrated into various MOEA frameworks, such as NSGA-II, MOEA/D, and MOGLS. This mechanism, applying instance augmentation techniques, improves both the proximity and diversity of the Pareto set. The L2I methodology outperforms other state-of-the-art techniques on standard MOCOPs, even showing better performance than renowned solutions like the LKH solver for specific problems.",
      "Weaknesses": "Since the paper deals with the combinatorial optimization, decomposition methods are not sufficiently elaborated/reviewed.",
      "Questions": "How does L2I compare with methods follows a general scheme of \"learn-divide-and-conquer\" or \"divide-learn-and-conquer\"? In a sense, there are approaches that learn how to decompose a problem and there are approaches that decompose a problem before learning. I qualitative assessment may be sufficient."
    }
  },
  {
    "paper_id": "yIv4SLzO3u",
    "summary": "Background: Class Incremental Learning (CIL) is a continuous learning framework aimed at handling tasks sequentially with disjoint class sets, while facing challenges such as catastrophic forgetting, where models forget previously learned information upon learning new tasks. Traditional methods such as knowledge distillation, architecture expansion, and parameter regularization have been employed to address these issues but come with inherent limitations.\nGap: Existing methods for CIL struggle with preserving past knowledge without relying heavily on previous task data or adding significant computational overhead. This hinders their applicability, especially in scenarios where memory storage for previous data is limited or not feasible.\nHypothesis: The authors propose a novel approach leveraging weight ensemble and constrained optimization called Bound-and-Average (BaA), which integrates inter-task and intra-task weight averaging alongside bounded model updates to effectively reduce catastrophic forgetting in CIL without altering network architectures or learning objectives.\nMethod: BaA incorporates two types of weight averaging: inter-task averaging, which maintains knowledge from all previous tasks by averaging model weights, and intra-task averaging, which improves generalization on the current task by averaging weights across multiple checkpoints during training. Additionally, the bounded update technique restricts model updates to remain close to a base model from previous tasks, balancing stability and adaptability. The method is evaluated on standard CIL benchmarks like CIFAR-100 and ImageNet-100/1000, showing improvements over existing methods with minimal extra computational cost.\nConclusion: The BaA approach demonstrates superior performance in retaining past knowledge and adapting to new tasks compared to state-of-the-art methods, while maintaining low overhead. It offers a simple plug-in solution to enhance existing CIL frameworks and is shown to be particularly effective under conditions of limited memory budgets, proving robust against catastrophic forgetting across various tasks.\n",
    "full text": "\nBOUND-AND-AVERAGE: LEVERAGING WEIGHTS AS\nKNOWLEDGE FOR CLASS INCREMENTAL LEARNING\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nWe present a novel training approach, named Bound-and-Average (BaA) for Class\nIncremental Learning (CIL) that leverages weight ensemble and constrained opti-\nmization, motivated by recent advances in transfer learning. Our algorithm con-\ntains two types of weight averaging: inter-task weight averaging and intra-task\nweight averaging. Inter-task weight averaging integrates the ability of previous\nmodels by averaging the weights of models from all previous stages. On the other\nhand, intra-task weight averaging enriches the learning of current task by averag-\ning the model parameters within current stage. We also propose a bounded update\ntechnique that aims to optimize the target model with minimal cumulative up-\ndates and preserve knowledge from previous tasks; this strategy reveals that it is\npossible to effectively obtain new models near old ones, reducing catastrophic for-\ngetting. BaA seamlessly integrates into existing CIL methods without modifying\narchitecture components or revising learning objectives. We extensively evaluate\nour algorithm on standard CIL benchmarks and demonstrate superior performance\ncompared to state-of-the-art methods.\n\n1\n\nINTRODUCTION\n\nDespite the remarkable achievements of recent deep neural networks (DNNs) (Radford et al., 2021;\nHo et al., 2020; Brown et al., 2020), training under continually shifting data distributions encoun-\nters a significant challenge called catastrophic forgetting—performance degradation on previously\nlearned data. Since the real-world environments, in which DNNs are deployed, diversely and dy-\nnamically change over time, addressing this issue becomes pivotal for enhancing the efficiency and\napplicability of DNNs.\n\nClass Incremental Learning (CIL) (Douillard et al., 2020; Hou et al., 2019; Rebuffi et al., 2017;\nSimon et al., 2021; Kang et al., 2022) is a continual learning framework which learn a sequential\ninflux of tasks composed of disjoint class sets. Given the constraint of permitting only a few or no\ndata from previous tasks during training for new tasks, catastrophic forgetting becomes a significant\nobstacle for CIL. Prior approaches have attempted to tackle this issue through methods including\nknowledge distillation (Hinton et al., 2015; Romero et al., 2015; Zagoruyko & Komodakis, 2017;\nKang et al., 2022), architecture expansion (Rusu et al., 2016; Yoon et al., 2018; Liu et al., 2021; Yan\net al., 2021; Abati et al., 2020), or parameter regularization (Aljundi et al., 2018; Kirkpatrick et al.,\n2017; Zenke et al., 2017). However, these methods have inherent limitations, such as reliance on\ndata from previous tasks for distilling knowledges of previous tasks, the need for additional network\ncomponents, and poor performance, which hinder their wide-range applications.\n\nWe propose a novel training approach for CIL, referred to as Bound-and-Average (BaA), which can\nbe easily incorporated into existing CIL methods without any algorithmic or architecture modifi-\ncations. Motivated by recent studies (Wortsman et al., 2022a; Rame et al., 2022; Wortsman et al.,\n2022b; Izmailov et al., 2018), which demonstrates the effects of weight averaging in aggregating the\nability of multiple models, we introduce two types of weight averaging tailored for CIL: inter-task\nweight averaging and intra-task weight averaging, each respectively enhances the stability and plas-\nticity of a CIL model. To preserve all the knowledge acquired up to the current task and avoid an\nover-reliance on the data from immediately preceding tasks, we introduce inter-task weight averag-\ning. This method summarizes the knowledge acquired throughout the previous tasks by averaging\nthe parameters of the models learned from individual incremental stages in an online fashion, thus\n\n1\n\n\fUnder review as a conference paper at ICLR 2024\n\nforming a base model. The base model serves as the initialization point for the subsequent task. On\nthe other hand, we propose intra-task weight averaging to enhance the model’s adaptivity to new\ntasks. This technique improves the model’s generalization capability for each new task by averaging\nmultiple checkpoints along the training trajectory within the current task.\n\nFurthermore, we incorporate bounded model update (Tian et al., 2023; Gouk et al., 2020) for training\nin each task, which constrains weight updates in the vicinity of the base model. By preventing the\nmodel parameters from deviating excessively from the base model, this strategy ensures stability\nand preserves knowledge from previous tasks. Throughout integrating these techniques, we strike\na balance between stability, preserving knowledges from previous tasks, and plasticity, adapting to\nnew tasks in CIL scenarios.\n\nThe contributions of this paper are summarized as follows:\n\n• We incorporate two weight averaging techniques into CIL, inter- and intra-task weight av-\neraging. These techniques enhance the model’s stability and plasticity by averaging model\nparameters across tasks (inter-task) and within a task (intra-task), respectively.\n\n• We introduce a bounded model update strategy that constrains the total amount of model\nupdates within each task. By enforcing the new models to remain close to the old ones, the\nproposed technique alleviates the catastrophic forgetting of previously acquired knowledge.\n\n• Our algorithm, which can be conveniently integrated into existing CIL methods, consis-\ntently improves performance on multiple benchmarks with marginal extra computational\ncomplexity. We demonstrate the effectiveness of our method via extensive experiments.\n\n2 RELATED WORKS\n\nThis section reviews existing algorithms related to class incremental learning and transfer learning.\n\n2.1 CLASS INCREMENTAL LEARNING (CIL)\n\nCIL is a challenging problem that aims to learn a model with the number of classes increasing\nstage-by-stage without forgetting the previously learned classes. We organize CIL methods into\nfive groups based on their main strategies: parameter regularization, architecture expansion, bias\ncorrection, knowledge distillation, and rehearsal methods.\n\nParameter regularization methods (Aljundi et al., 2018; Kirkpatrick et al., 2017; Zenke et al., 2017)\nmeasure the importance of network parameters and adjust their flexibility to mitigate catastrophic\nforgetting. However, these methods suffer from unsatisfactory generalization performance in CIL\nscenarios (van de Ven & Tolias, 2019; Hsu et al., 2018). Architecture expansion methods (Rusu\net al., 2016; Yoon et al., 2018; Liu et al., 2021; Yan et al., 2021; Abati et al., 2020) dynamically\nexpand the network capacity to handle incoming tasks by adding new neurons or layers. How-\never, they introduce computational burdens due to additional network components. Bias correction\nmethods (Hou et al., 2019; Wu et al., 2019) address the bias towards new classes caused by the\nclass imbalances in CIL by introducing scale and shift parameters or matching the scale of weight\nvectors. Knowledge distillation methods (Hinton et al., 2015; Romero et al., 2015; Zagoruyko &\nKomodakis, 2017; Kang et al., 2022) encourage models to preserve previous task knowledge by\nmimicking the representations of old models. Several approaches match output distributions or\nattention maps to preserve important information. Rehearsal-based methods (Rebuffi et al., 2017;\nOstapenko et al., 2019; Shin et al., 2017) store representative examples or employ generative models\nto mitigate forgetting. Examples include maintaining class centroids Rebuffi et al. (2017) or using\ngenerative adversarial networks (Goodfellow et al., 2014; Liu et al., 2020a; Odena et al., 2017) to\ngenerate synthetic examples.\n\nIn contrast, we propose a novel approach which is compatible and easy to integrate with the existing\nCIL methods, without requiring any changes to the network architectures or loss functions. Our\napproach leverages the weights of previous models instead of the data from previous tasks, which\nreduces the data dependency on previous tasks. Also, our approach has a negligible computational\noverhead, making it suitable for various CIL scenarios.\n\n2\n\n\fUnder review as a conference paper at ICLR 2024\n\n2.2 ROBUST TRANSFER LEARNING\n\nRobust transfer learning aims to adapt a pre-trained model to a new domain or task without losing\nits generalization ability. This is similar to continual learning, which seeks to prevent catastrophic\nforgetting of previous knowledge while learning new tasks sequentially.\n\nA common approach to robust transfer learning is to freeze or constrain the weights of the pre-trained\nmodel (Kumar et al., 2022; Lee et al., 2023) while fine-tuning. This prevents feature distortion from\nadaptation and preserves the original knowledge. For example, LP-FT (Kumar et al., 2022) adopts\na two-step strategy: first, train the linear classifier and then fully fine-tune the entire network with a\nsmall learning rate. Surgical fine-tuning (Lee et al., 2023) shows that training only a subset of layers\nwhile freezing others can improve the robustness against distribution shift.\n\nAnother line of work explores the idea of weight averaging or interpolation to enhance the perfor-\nmance of fine-tuned models (Neyshabur et al., 2020; Wortsman et al., 2022a; Ram´e et al., 2022).\nNeyshabur et al. (Neyshabur et al., 2020) shows that interpolating the weights of fine-tuned models\nfrom the same pre-trained model can improve both in-distribution accuracy and out-of-distribution\nrobustness. Model Soups (Wortsman et al., 2022a) and Model Ratatouillie (Ram´e et al., 2022) ex-\ntend this idea by using different hyperparameters and downstream tasks to create diverse fine-tuned\nmodels and maximize the ensemble effect. WiSE-FT (Wortsman et al., 2022b) proposes to linearly\ninterpolate the weights of the pre-trained model and the fine-tuned model, which allows the model to\nlearn new information without deviating too much from the original network. MARS-PGM (Gouk\net al., 2020) and TPGM (Tian et al., 2023) adopt a similar weight projection technique to regularize\nthe fine-tuned models. However, these methods primarily concentrate on a single downstream task,\nneglecting the sequential domain shifts that are inherent in continual learning.\n\nStojanovski et al. (Stojanovski et al., 2022) introduced Momentum-based Weight Interpolation for\nContinual Learning (MCL), a method that uses exponential moving average (EMA) of the weights\nto preserve previous knowledge. However, MCL does not account for the dynamics of continual\nlearning and simply applies the techniques from transfer learning. We address this limitation by\ninvestigating the effects and impacts of linear interpolation in continual learning dynamics through\ninter-task, intra-task weight averaging, and bounded updates on weight space.\n\n3 PROPOSED APPROACHES\n\nThis section presents the details of the proposed method, Bound-and-Average (BaA), for preserv-\ning knowledge from previous tasks while learning new tasks in CIL through weight averaging and\nbounded model update.\n\n3.1 PROBLEM FORMULATION\n\nCIL is a learning framework to handle a sequence of tasks, T1:K = {T1, · · · , Tk, · · · , TK}. Each\ntask Tk consists of a labeled dataset Dk whose label set, Ck, is disjoint to the ones defined in the past,\ni.e.. (C1 ∪ · · · ∪ Ck−1) ∩ Ck = ∅. At the kth incremental stage, the current model Mk(·) is trained\non integrated dataset D′\nk = Dk ∪ Bk−1, where Bk−1 is a memory buffer for storing representative\nexemplars that belong to all the previously learned classes. The performance of a CIL algorithm is\nevaluated using a test set comprising test data from all the stages.\n\n3.2 WEIGHT AVERAGING\n\nWe propose two unique weight averaging techniques: Inter-task weight averaging and intra-task\nweight averaging. Although both techniques concentrate on integrating the competency of multiple\nmodels, they have distinct objectives. Inter-task weight averaging aims to consolidate knowledge\nfrom all previously learned tasks and construct a comprehensive model by computing moving av-\nerages of previous models. Meanwhile, intra-task weight averaging, motivated by SWA (Izmailov\net al., 2018), is effective for boosting the model’s generalization capability on new tasks. This is\nachieved by calculating the average weight of models at various checkpoints along the training tra-\njectories. Note that, for both inter- and intra-task weight averaging, it is not required to store all the\nprevious models for averaging because the model averaging is performed in an online manner.\n\n3\n\n\fUnder review as a conference paper at ICLR 2024\n\n(a) Feature extractor fθbase\n\nk+1\n\n(b) Classifier gϕbase\n\nk+1\n\nFigure 1: Description of inter-task weight averaging: Upon the completion of the kth incremental\nstage, we establish the base model M base\nk+1(·), which will serve as the initialization point for the next\n(k + 1)th stage. The model comprises a feature extractor fθbase\n(·), con-\nstructed as follows: (a) To construct the base feature extractor fθbase\nk+1 to the moving\naverage of all previous feature extractor weights θ1, θ2, · · · , θk, which can be easily computed with\nθbase\n(·), we concatenate the weights of the\nk\ncurrent base classifier ϕbase\nk with the weights of the current classifier ϕk associated with the class set\nof the current task Ck.\n\nand θk following Equation 1. (b) For the classifier gϕbase\n\n(·) and a classifier gϕbase\n(·), we set θbase\n\nk+1\n\nk+1\n\nk+1\n\nk+1\n\n3.2.1\n\nINTER-TASK WEIGHT AVERAGING\n\nLet the base model, M base\n{M1(·), · · · , Mk−1(·)}. The base model is composed of a feature extractor fθbase\ngϕbase\n\n(·), be an average of the models for all the preceding tasks, denoted by\n(·) and a classifier\n\n(·), collectively parameterized by Θbase\n\nk = {θbase\n\n, ϕbase\n\nk }.\n\nk\n\nk\n\nk\n\nk\n\nUpon the completion of the kth incremental stage, we combine the current feature extractor fθk (·)\nand classifier gϕk (·) into the current base model, M base\n(·), as illustrated in Figure 1. This process\ncreates the new base model for the next stage, denoted by M base\nk+1(·). The new feature extractor is\ngiven by the moving average of all models as follows:\n\nk\n\nθbase\nk+1 =\n\nk − 1\nk\n\n· θbase\n\nk +\n\n1\nk\n\n· θk.\n\n(1)\n\nk\n\nTo obtain the classifier of the new base model for the (k + 1)st stage, we concatenate the current\nbase model classifier gϕbase\n(·) defined for the classes in Ck−1 with the weights corresponding to the\nclasses in Ck of the current classifier, gϕk (·), which is expressed as\nk+1 = Concat(ϕbase\nϕbase\n\n(2)\nk\nwhere Select(ϕk, Ck) extracts the weights corresponding to the classes in Ck of the current classifier\ngϕk (·). At the (k + 1)st incremental stage, we start to train from the base model, M base\nk+1(·), which\nencapsulates all the knowledge learned up to the current stage.\n\n, Select(ϕk, Ck)),\n\n3.2.2\n\nINTRA-TASK WEIGHT AVERAGING\n\nFor intra-task weight averaging, we seek to enhance generalization ability on the current task by\naveraging multiple checkpoints along the training trajectories. In the kth incremental stage, an intra-\ntask averaged model, M avg\n\nis updated at every ea epochs as\n\nk (·), parameterized by Θavg\nk\nn · Θavg\nk + Θk\nn + 1\n\nk ←\n\nΘavg\n\n,\n\n(3)\n\nwhere n denotes the number of models involved in the averaging. Once the stage is completed, the\nfinal model, Mk(·), is replaced by the intra-task averaged model, M avg\nk , for\ninference as well as the computation of the base model, M base\n\nk (·), i.e., Θk ← Θavg\n\nk+1(·).\n\nFor the models equipped with Batch Normalization (BN) (Ioffe & Szegedy, 2015), an additional\ndata pass is required for post-training to compute the new running estimates of mean and variance of\n\n4\n\nTask k-1Task kAveragingTask k-1ConcatTask kTask k\fUnder review as a conference paper at ICLR 2024\n\n(a) Intra-task weight averaging\n\n(b) Bounded updates\n\nFigure 2: (a) Explanation of the process of intra-task weight averaging: We define an intra-task aver-\naged model weight to compute the running mean of multiple weights along the training trajectories,\nas described in Equation 3. This averaged model weight is utilized for inference and for computing\nthe next stage base model M base\n(·). (b) Description of the bounded update method: We constrain the\nweight updates around the given base model. This strategy is designed to preserve the knowledge\nembedded in the base model, denoted as M base\n(·).\n\nk\n\nk\n\nthe activations after model averaging (Garipov et al., 2018; Izmailov et al., 2018), since the accumu-\nlated BN statistics are not computed for the averaged models. Rather than executing an additional\nforward pass after resetting the running statistics as in SWA (Izmailov et al., 2018), we conduct an\nadditional forwarding path with the running statistics of the current model Mk(·) estimated before\nmodel averaging. This strategy alleviates the bias towards the current task due to sample deficiency\nof the classes introduced in the previous tasks.\n\n3.3 BOUNDED MODEL UPDATE\n\nRecent advancements in transfer learning research (Tian et al., 2023; Gouk et al., 2020) underscores\nthe significance of confining the divergence of fine-tuned models from the pretrained models. This\nstrategy allows the fine-tuned model to effectively retain the knowledge obtained from the pretrained\nmodel. Inspired by this observation, we enforces a constraint for model update in CIL, which bounds\nthe magnitude of weight updates from the base model at every eb epochs as\n\n∆Θ ←\n\n(cid:40)\n\n∥∆Θ∥ ,\n\nB · ∆Θ\n∆Θ,\n\nif ∥∆Θ∥ > B\notherwise\n\n,\n\n(4)\n\nwhere ∆Θ denotes the displacement of the current model from the base model, M base\n(·), and B\nserves as the threshold for bounding the gradient magnitude. Note that the proposed bounded model\nupdate is performed within an incremental stage jointly with intra-task weight averaging. Given that\nthe base model is presumed to hold the knowledge of previous tasks, this strategy aims to prevent\nthe model from straying too far from the base model acquired in prior stages.\n\nk\n\n4 EXPERIMENTS\n\nThis section describes the datasets, evaluation protocol, and the implementation details of our algo-\nrithm. We then report the results of our algorithm on the standard benchmarks for CIL and conduct\nablation studies to demonstrate the effectiveness of our method.\n\n4.1 DATASETS AND EVALUATION PROTOCOL\n\nWe conduct CIL experiments on two datasets, CIFAR-100 (Krizhevsky et al., 2009) and ImageNet-\n100/1000 (Russakovsky et al., 2015). CIFAR-100 contains 50,000 training and 10,000 validation\nimages in 100 classes. ImageNet-1000 contains 1,000 classes for 1.2M training and 50,000 vali-\ndation images and ImageNet-100 is constructed with the first 100 classes of ImageNet-1000. For\n\n5\n\nepochsAveragingAveragingincremental stageepochsballepochs\fUnder review as a conference paper at ICLR 2024\n\nTable 1: CIL performance (%) on CIFAR-100. The proposed training technique (BaA) consistently\nimproves performance when plugged into the existing methods. Note that we run 3 experiments\nwith 3 different orders and report the average result. Models with ∗ denotes the report of reproduced\nresults. The bold-faced numbers indicate the best performance.\n\nNumber of tasks\n\niCaRL (Rebuffi et al., 2017)\nBiC (Wu et al., 2019)\nMnemonics (Liu et al., 2020b)\nGeoDL* (Simon et al., 2021)\nUCIR (Hou et al., 2019)\nPODNet* (Douillard et al., 2020)\nPODNet (Douillard et al., 2020) + BaA\nAFC* (Kang et al., 2022)\nAFC (Kang et al., 2022) + BaA\nFOSTER* (Wang et al., 2022)\nFOSTER (Wang et al., 2022) + BaA\n\n5\n\n58.08\n56.86\n63.34\n65.34\n64.01\n64.83±0.62\n67.69±0.50\n66.11±0.60\n67.96±0.70\n72.23±0.51\n73.03±0.61\n\nCIFAR-100\n\n10\n\n25\n\n53.78\n53.21\n62.28\n63.61\n61.22\n62.75±0.74\n66.49±0.50\n64.77±0.74\n67.10±0.68\n69.12±0.67\n70.58±0.64\n\n50.60\n48.96\n60.96\n60.21\n57.57\n60.73±0.62\n64.93±0.42\n63.68±0.74\n66.12±0.67\n65.45±0.90\n66.79±0.84\n\n50\n\n44.20\n47.09\n-\n52.28\n49.30\n58.37±0.83\n63.29±0.37\n61.94±0.60\n65.38±0.41\n59.60±0.85\n62.47±0.97\n\nTable 2: CIL performance (%) on ImageNet-100/1000. BaA demonstrates significant performance\ngains when integrated into existing methods, even in the large-scale benchmarks for CIL.\n\nNumber of tasks\n\niCaRL (Rebuffi et al., 2017)\nBiC (Wu et al., 2019)\nMnemonics (Liu et al., 2020b)\nGeoDL* (Simon et al., 2021)\nUCIR (Hou et al., 2019)\nPODNet* (Douillard et al., 2020)\nPODNet (Douillard et al., 2020) + BaA\nAFC* (Kang et al., 2022)\nAFC (Kang et al., 2022) + BaA\nFOSTER* (Wang et al., 2022)\nFOSTER (Wang et al., 2022) + BaA\n\n5\n\n65.56\n68.97\n72.58\n73.87\n71.04\n74.06\n75.98\n76.91\n77.05\n80.22\n80.59\n\nImageNet-100\n10\n\n60.90\n65.14\n71.37\n73.55\n70.71\n71.51\n74.08\n75.26\n76.35\n78.15\n79.20\n\n25\n\n54.56\n59.65\n69.74\n71.72\n62.94\n67.31\n70.70\n73.65\n74.35\n71.74\n73.24\n\nImageNet-1000\n10\n\n5\n\n51.36\n45.72\n64.54\n65.23\n64.34\n68.18\n69.53\n68.06\n70.28\n–\n–\n\n46.72\n44.31\n63.01\n64.46\n61.18\n65.58\n67.76\n66.39\n69.51\n–\n–\n\nfair comparisons, we follow three different class orders for CIFAR-100 while a class order specified\nin (Douillard et al., 2020) is used for ImageNet-100 and ImageNet-1000.\n\nWe incorporate the proposed training technique (BaA) into various state-of-the-art CIL algorithms\nbased on knowledge distillation (PODNet (Douillard et al., 2020), AFC (Kang et al., 2022)), archi-\ntecture expansion (FOSTER (Wang et al., 2022)), and virtual class augmentation (IL2A (Zhu et al.,\n2021)). Note that IL2A is employed to compare with non-exemplar-based methods.\n\nFollowing the previous works (Douillard et al., 2020; Hou et al., 2019; Liu et al., 2021), we first\ntrain the model using a half of the classes in the initial stage and split the remaining classes into\n5/10/25/50 stages for CIFAR-100, 5/10/25 stages for ImageNet-100, and 5/10 stages for ImageNet-\n1000 to simulate CIL scenarios. We test models on all the seen classes at each incremental stage,\nand report the average incremental accuracy (Rebuffi et al., 2017; Douillard et al., 2020; Hou et al.,\n2019)—average accuracy over all incremental stages.\n\n4.2\n\nIMPLEMENTATION DETAILS\n\nAs our approach is a plug-in method that can be incorporated into existing baselines, we follow the\nimplementation settings of the existing methods (Douillard et al., 2020; Kang et al., 2022; Wang\net al., 2022) in principle. We adopt ResNet-32 for CIFAR-100 and ResNet-18 for ImageNet as\nthe backbone networks. We employ SGD with a momentum of 0.9. The hyperparameter setting\nincluding learning rates, batch sizes, training epochs, distillation loss weight, and herding strategies\n\n6\n\n\fUnder review as a conference paper at ICLR 2024\n\nTable 3: Component analysis of our algorithm for inter-task weight averaging, intra-task weight av-\neraging, and bounded model update. We evaluate the impact of these components using the metrics\nof forgetting and average new accuracy, which reflect the balance between stability and plasticity in\nCIL, in addition to overall accuracy.\n\nInter-task\n\nIntra-task\n\nBounded\n\nForgetting ↓ Avg. new acc. ↑ Overall acc.↑\n\n(a) BaA\n\n(b) w/o inter-task\n(c) w/o intra-task\n(d) w/o bounded\n\n✓\n\n✓\n✓\n\n✓\n\n✓\n\n✓\n\n✓\n\n✓\n✓\n\n15.38\n\n21.77\n13.10\n18.72\n\n59.35\n\n62.74\n51.60\n64.14\n\n65.38\n\n61.81\n65.08\n64.21\n\nTable 4: CIL performance (%) with a limited memory budget on CIFAR-100: 1 exemplar memory\nper class for PODNet (Douillard et al., 2020) and AFC (Kang et al., 2022), and no memory for\nIL2A (Zhu et al., 2021). BaA demonstrates significant enhancement under strict memory constraint.\n\nNumber of tasks\n\nPODNet* (Douillard et al., 2020)\n\n(1 exemplar)\n\nPODNet (Douillard et al., 2020) + BaA (1 exemplar)\n\nAFC* (Kang et al., 2022)\n\n(1 exemplar)\n\nAFC (Kang et al., 2022) + BaA (1 exemplar)\n\nIL2A* (Zhu et al., 2021)\n\n(no exemplar)\n\nIL2A (Zhu et al., 2021) + BaA (no exemplar)\n\n5\n\n43.70\n52.27\n49.82\n53.41\n65.70\n67.46\n\nCIFAR-100\n25\n10\n\n34.19\n45.63\n42.78\n48.92\n58.14\n61.80\n\n26.58\n36.81\n35.51\n46.08\n54.40\n58.25\n\n50\n\n14.78\n20.84\n23.59\n35.25\n20.42\n43.54\n\nis identical to the baseline algorithms. The memory budget size is set to 20 per class unless specified\notherwise. Detailed description about hyperparameters for our method is illustrated in Section A\n\n4.3 RESULTS ON CIFAR-100 AND IMAGENET-100/1000\n\nCIFAR-100 We plug in our method to PODNet (Douillard et al., 2020), AFC (Kang et al., 2022)\nand FOSTER (Wang et al., 2022). Following previous works (Douillard et al., 2020; Kang et al.,\n2022), we conduct three experiments with different class orders and report the mean and standard\ndeviation. Table 1 illustrates that the proposed algorithm, denoted by BaA, enhances the perfor-\nmance of the baseline models in all CIL scenarios, with notable margins for various algorithms and\nnumbers of tasks.\n\nImageNet-100/1000 Table 2 shows the results on large-scale benchmarks, ImageNet-100/1000.\nWhile the proposed method consistently boosts the baseline algorithms, Table 2 clearly illustrates\nthat the performance gains are particularly remarkable for ImageNet-1000, which is the most chal-\nlenging benchmark for CIL. Additionally, across all datasets, the proposed algorithm demonstrates\nenhanced performance gains with an increasing number of tasks. This is partly because BaA is\nmore robust to catastrophic forgetting, which typically worsens as the number of tasks increases.\nThis characteristic is highly desirable for CIL in practical scenarios, which have no restrictions or\ninformation on the number of incoming tasks.\n\n4.4 ABLATION STUDIES\n\nWe perform various ablation studies to validate the effectiveness of the proposed training technique.\nAll the experiments are performed on CIFAR-100 with 50 incremental stages unless specified other-\nwise. To measure stability and plasticity, we compute the average new accuracy, which is the average\naccuracy of new classes over incremental stages, and the forgetting metric (Lee et al., 2019), which\nis the average of the performance degradation for each class.\n\nVariations of the proposed method Table 3 presents the results obtained from different com-\nbinations of components in BaA. Each component contributes to the overall improvements, albeit\nin different ways. Comparing BaA to w/o inter-task and w/o bounded, we observe a decrease in\nforgetting, indicating that inter-task weight averaging and bounded updates effectively help retain\nprevious knowledge. However, the low average new accuracy in w/o intra-task suggests that in-\n\n7\n\n\fUnder review as a conference paper at ICLR 2024\n\n(a) CKA of AFC (Kang et al., 2022)\n\n(b) CKA of AFC (Kang et al., 2022) + BaA\n\nFigure 3: CKA between models after training individual incremental stages. We visualize the sim-\nilarity between pairs of models obtained from two different tasks—baseline task and comparison\ntask—by measuring CKA of the representations of test examples of all classes learned up to base-\nline task extracted from the two models.\n\ntegrating bounded updates solely with inter-task weight averaging leads to significant degradation\nin adaptation. On the other hand, the increase in average new accuracy in ours compared to w/o\nintra-task indicates that the inclusion of intra-task weight averaging, in combination with bounded\nupdates, helps mitigate this problem. Thus, the combination of these components enhances the per-\nformance of our method in retaining previous knowledge while facilitating adaptation to new tasks.\n\nComputational cost To further substantiate the efficacy of our approach, we provide precise train-\ning time overhead induced by our method. Based on a single NVIDIA RTX-8000 GPU with a\nResNet-32 backbone, we observe that the inter- and intra-task weight averaging only take 0.003\nseconds each, whereas the bounded model update operation requires 0.011 seconds. Given that\nthese operations only occur intermittently, e.g., per task or every several epochs, the additional train-\ning time is marginal. Note that one epoch of additional forwarding is required for computing BN\nstatistics per task. Because BaA is a training technique, it incurs no extra overhead for inference.\n\nResults on limited memory budgets We evaluate the performance of BaA with exemplar-based\nmethods such as PODNet (Douillard et al., 2020) and AFC (Kang et al., 2022) when only one\nexemplar is available. We also verify the effectiveness of BaA when it is combined with non-\nexemplar-based method, IL2A (Zhu et al., 2021). As shown in Table 4, our algorithm significantly\noutperforms existing methods when operating under limited memory budgets by exploiting model\naveraging techniques and constraining the amount of model updates. This property is desirable\nconceptually because CIL may have a large number of stages and even a small number of exemplars\nmay be difficult to hold in practice.\n\nSimilarity between representations To comprehensively assess the impact of the proposed\nmethod on feature representations, we measure the similarity of representations. To this end,\nwe extract the feature representations of the test examples from the final layer of the models\ntrained on different tasks, e.g., baseline task and comparison task using Centered Kernel Alignment\n(CKA) (Cortes et al., 2012; Kornblith et al., 2019) using test data of all classes learned up to the\nbaseline task. Figure 3 clearly illustrates that BaA enhances feature similarities across the models\ntrained in different incremental stages, which implies that BaA alleviates catastrophic forgetting.\n\nVariations in inter-task weight averaging Table 5 shows the results from different strategies of\ninter-task weight averaging. Our moving average technique provides the same solution with the of-\nfline averaging, which outperforms Exponential Moving Averaging (EMA) with various smoothing\nfactors. The EMA methods favor recent models and lead to forgetting the previous knowledge.\n\nVariations in intra-task weight averaging We explore the characteristics of intra-task weight\naveraging by varying the weight averaging periods and the BN statistics computation strategies.\nAccording to Table 6, the proposed intra-task weight averaging technique is robust to the changes\nof averaging period but the results are affected by the methods to compute the BN statistics. Due to\n\n8\n\n02468Comparison Task02468Baseline TaskCKA Matrix0.900.920.940.960.981.0002468Comparison Task02468Baseline TaskCKA Matrix0.900.920.940.960.981.00\fUnder review as a conference paper at ICLR 2024\n\nTable 5: Analysis of inter-task weight averaging strategies. EMA denotes exponential moving av-\nerage, where the numbers in parentheses indicate the smoothing factor; a higher smoothing factor\nassigns more weights to the most recent tasks.\n\nAvg (Ours)\n\nEMA (0.9)\n\nEMA (0.5)\n\nEMA (0.1)\n\nAveraging factor\n\nForgetting ↓\nAvg. new acc. ↑\nOverall acc. ↑\n\n15.38\n59.35\n65.38\n\n19.12\n61.34\n64.09\n\n18.28\n61.74\n64.10\n\n17.41\n59.10\n64.01\n\nTable 6: Results by varying the weight averaging periods and the BN statistics computation methods\nin intra-class weight averaging. R means resetting the running statistics and NC indicates no change\nof the BN statistics after the weight averaging. † denotes our choice for reporting the main results.\n\nWeight averaging period\n\n1†\n\n15.38\n59.35\n65.38\n\n5\n\n16.12\n59.41\n64.85\n\n10\n\n15.59\n58.50\n65.19\n\n15\n\n15.58\n58.95\n65.51\n\nOurs\n\n15.38\n59.35\n65.38\n\nBatchNorm\nR\n\n36.67\n39.75\n25.60\n\nNC\n\n25.60\n60.47\n64.57\n\nForgetting ↓\nAvg. new acc. ↑\nOverall acc. ↑\n\nTable 7: Results by varying the bounding period and the size of the bound.\n\nBounding period\n\nBounding threshold\n\n1\n\n5\n\n10\n\n15† w/o bounded\n\n5\n(w/ decay)\n\n10†\n(w/ decay)\n\n15\n(w/ decay)\n\n10\n(w/o decay)\n\nForgetting ↓\n\n15.73 15.89 16.45 15.38\nAvg. new acc. ↑ 57.25 57.92 58.12 59.35\nOverall acc. ↑\n64.76 64.81 64.73 65.38\n\n18.72\n64.14\n64.21\n\n16.53\n58.66\n64.50\n\n15.38\n59.35\n65.38\n\n16.33\n59.66\n64.99\n\n18.09\n62.40\n64.44\n\nthe distinct characteristics of CIL that it should perform well on the previous classes, resetting the\nrunning statistics (R) gives severe performance degradation since the computed running statistics\nwill be highly biased towards current tasks in computation procedure after the reset. Also, not\nforwarding the additional data path (NC) shows the degraded performance since there is discrepancy\nbetween the running statistics and intra-task averaged model since the statistics for the averaged\nmodel are not computed.\n\nVariations in bounded model updates We conducted experiments with varying the frequency\nand the allowed size of the bounded model updates. As illustrated in Table 7, the overall accuracy is\nrobust to the changes in the bounding frequency while applying the bounded model update results\nin clear advantage. For the bounding threshold, we observe that decaying the bounding threshold as\nthe training proceeds in each stage gives improved results by reducing the forgetting. We argue that\nthis phenomenon is attributed to the progressive convergence towards an optimal solution for the\nnew task. As the learning process progresses, the updates gradually become more specialized and\noverfitted to the new task. This promotes the natural forgetting of previous information, making the\ndecaying bounding threshold strategy valuable.\n\n5 CONCLUSION\n\nThis paper introduces an innovative CIL method that utilizes weight ensemble techniques to handle\nthe catastrophic forgetting. The method redefines CIL as a sequential transfer learning process and\nintroduces inter-task and intra-task weight averaging, and bounded update techniques for enhanced\nmodel stability and adaptability.\n\nThe proposed strategy is seamlessly integrated into existing methods without modifications to the\nnetwork architecture or loss functions. By leveraging the weights of previous models, our approach\nmitigates data dependency on preceding tasks, addressing concerns related to data privacy and mem-\nory budget.\n\n9\n\n\fUnder review as a conference paper at ICLR 2024\n\nReproducibility We clarify all the hyperparameters needed for reproducing the proposed algo-\nrithm in Section 4.2 of main paper and A of appendix. We use the official code of each baseline for\nconducting experiments. We will release the code for reproducing the proposed method soon.\n\nEthics statement The proposed algorithm enhances the robustness and reliability of deep neural\nnetworks by resolving the catastrophic forgetting which can yield harmful impact under continu-\nously shifting data distribution.",
    "abstract": "We present a novel training approach, named Bound-and-Average (BaA) for Class Incremental Learning (CIL) that leverages weight ensemble and constrained optimization, motivated by recent advances in transfer learning. Our algorithm contains two types of weight averaging: inter-task weight averaging and intra-task weight averaging. Inter-task weight averaging integrates the ability of previous models by averaging the weights of models from all previous stages. On the other hand, intra-task weight averaging enriches the learning of current task by averaging the model parameters within current stage. We also propose a bounded update technique that aims to optimize the target model with minimal cumulative updates and preserve knowledge from previous tasks; this strategy reveals that it is possible to effectively obtain new models near old ones, reducing catastrophic forgetting. BaA seamlessly integrates into existing CIL methods without modifying architecture components or revising learning objectives. We extensively evaluate our algorithm on standard CIL benchmarks and demonstrate superior performance compared to state-of-the-art methods.",
    "response": {
      "Soundness": 2,
      "Presentation": 3,
      "Contribution": 2,
      "Rating": 3,
      "Confidence": 5,
      "2022": 364,
      "Strengths": "The proposed method BaA is simple but effective, and the method can be integrated into existing methods to improve the performance without modifying their algorithm or architecture.",
      "Weaknesses": "(1) The contribution of this paper appears to be insufficient to fully support the article. Similar EMA methods are widely used in self-supervised learning and some incremental learning methods [1,2].\n\n[1]Yu C, Shi Y, Liu Z, et al. Lifelong person re-identification via knowledge refreshing and consolidation[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2023, 37(3): 3295-3303.\n\n[2]Liang M, Zhou J, Wei W, et al. Balancing between forgetting and acquisition in incremental subpopulation learning[C]//European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022: 364-380.\n\n(2) Lack of analysis experiments or visualization about how the proposed weight average mitigates the forgetting issue.\n\n(3) Tables 1 and 2 show that BaA can improve the performance of certain CIL methods. Experiments on the latest CIL methods in 2023 should be done to show that the BaA can also be integrated into the SOTA methods.",
      "Questions": "In Table 3, the results of the experiment are somewhat perplexing as the average new accuracy appears to be lower than the overall accuracy. In my assessment, the new accuracy represents the precision of classes of the current task, and the overall accuracy is expected to be lower than the average new accuracy due to knowledge forgetting."
    }
  }
]