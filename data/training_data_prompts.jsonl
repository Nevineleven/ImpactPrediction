{"paper_id": "bUGzjiUsIq", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Traditional multi-label classification relies heavily on fully annotated datasets, which are difficult and often impractical to obtain, especially in domains requiring specific expert knowledge. Incomplete annotations are common in multi-label tasks due to the large number of possible labels and complexities involved in providing full annotations.\nGap: The problem addressed in this paper is learning from partially annotated multi-label classification datasets where only a subset of positive classes is annotated, leading to issues of scarcity in positive annotations and severe label imbalance.\nHypothesis: The authors propose that by integrating reinforcement learning with partial annotation using a Policy Gradient (PAPG) approach, they can effectively overcome the limitations of traditional methods and improve performance in partially annotated multi-label classification by balancing exploration and exploitation.\nMethod: The authors introduce PAPG, a framework that combines the exploration strengths of reinforcement learning with the exploitation capabilities of supervised learning. The model is structured as a Markov Decision Process (MDP) with local and global rewards. The local reward comes from a value network trained on annotated data and the global reward is based on recall metrics. The training process involves iterative updates between a policy network and a value network, using data enhancement and an adaptive threshold to adjust training data.\nConclusion: The proposed PAPG framework effectively handles the challenges of partially annotated multi-label classification, demonstrating superior performance over traditional methods and baselines. The framework shows robustness and generalizes well across different datasets and domains, substantially improving F1 scores and other metrics compared to existing methods.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The problem, partially annotated multi-label classifcation, is novel.\\n\\n2. A novel method named PAPG is proposed, which can overcome the challenges associated with a scarcity of positive annotations and severe label imbalance. \\n\\n3. Experiments validate the effectiveness of the proposed PAPG method.\", \"Weaknesses\": \"1. The motivation might be unreasonable. As stated in abstract, \\\"This task encounters challenges associated with a scarcity of positive annotations and severe label imbalance\\\". These challenges are indeed problem to deal with, but in my opinion, the main challenges correspond to that the negative samples are unknown or we do not know which are negative samples among the remaining unlabeled samples. In other words, the main challenges are the same as PU learning.\\n\\n2. For the two challenges mentioned in this paper, i.e., a scarcity of positive annotations and severe label imbalance, it is unclear how can the proposed PAPG method overcome them.\\n\\n3. As stated in Introduction, \\\"Consequently, many advanced methods of PU learning (Su et al., 2021; Acharya et al., 2022; Luo et al., 2021) cannot readily adapt to our multi-label settings\\\". In my opinion, if we focus each label one by one, it is a PU learning problem. Thus, the partially annotated multi-label classifcation problem can be solved via binary relevance strategy where the base classifier is trained with any off-the-shelf PU learning methods.\\n\\n4. For multi-label classification, it is very important to model the correlations among labels. But it is unclear how can the proposed PAPG method model label correlations.\\n\\n5. The writtting can be greatly improved. For exampe:\\n(1) The title is inappropriate.\\n(2) Errors exist in notations (especially the first paragraph in section 3.1).\\n(3) What does RARL mean in the last sentence of this paper?\", \"Questions\": \"If author disagree with my comments in Weaknesses, please clarify them in the rebuttal phase.\"}"}]}
{"paper_id": "vsW5vJqBuv", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Empowering embodied AI, such as robots, has become crucial as these systems transition from solving specific, constrained problems to handling a variety of tasks in the physical world, such as household chores and healthcare. Traditional methods often struggle with the inherent diversity and complexity of open-ended task goals that are multifaceted and dynamic.\nGap: Current AI techniques are limited in their ability to handle open-ended tasks due to the traditional goal-conditioned frameworks that expect specific end-states. This restriction impedes the flexibility required for robots to adapt to novel tasks not encountered during training, which is necessary for more realistic and practical applications.\nHypothesis: The authors propose that combining diffusion models with training-free guidance can enhance the adaptability and competency of embodied AI systems in solving open-ended tasks. The diffusion models can generate various potential future states without the constraints of pre-defined goals, allowing for more flexible planning and control.\nMethod: The proposed method, DOG (Diffusion for Open-ended Goals), uses diffusion models to understand world dynamics from offline data without goal-conditioning and performs energy minimization for planning during inference. The method involves training with Markov decision frameworks and using classifier-guided diffusion models for conditional score functions. The approach was tested on diverse tasks such as maze navigation, robot movement control, and robotic arm manipulation, demonstrating its ability to handle unscripted objectives dynamically.\nConclusion: DOG significantly advances the flexibility and competence of embodied AI, enabling agents to plan and execute actions for open-ended task goals not defined during training. The framework allows for zero-shot transfer to new tasks and shows robust performance in handling various types of goals, representing a potential leap in achieving more human-like intelligence in embodied systems.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 2, \"Strengths\": \"Using diffusion models to perform Goal-Conditioned Reinforcement Learning (GCRL) looks like a good idea, although the authors are not the first to put it forward, and are not clear about the novelty they bring.\", \"Weaknesses\": \"- The paper is very unclear about the \\\"training-free\\\" property of the framework, which seems to be the main contribution with respect to Janner et al. (2022) and Liang et al. (2023).\\n\\n- The paper is unclear about the difference between defining an energy function by hand and defining a goal-related reward function by hand.\\n\\n- The experiments about using language-based prompts are not explained at all\\n\\n- The paper is written poorly and often in a non-scientific way with a lot of unnecessary over-statements (see below).\", \"Questions\": \"- could the authors explain the difference between defining energy functions for partial goals, negative goals, sequence goals, hybrid goals as they do in the bottom of page 3 and defining goal-related reward functions that do the same, as often done in GCRL? If they argue that this makes a difference, can they backup this claim with comparative experiments?\\n\\n- \\\"This way of defining a goal is fundamentally different from the conventional way of treating the goal as a variable\\\" -> could the authors explain why it is \\\"fundamentally different\\\"? You could have a goal represented with \\\"Clean = True\\\", this would be using a boolean variable, or \\\"clean level = X\\\", that would be using a scalar variable. Doesn't your energy function (c or g?) compute something similar?\\n\\n- p6: the list of examples should be reordered to match the order of the figure (or vice versa). Besides, the corresponding energy functions should be given. The examples themselves need to be better explained.\\n\\nFor instance \\\"our objective isn\u2019t state-specific but aims at maximizing or minimizing some certain property,\\\" this is a completely unspecific sentence, and an over-statement. Can your system do whatever you ask it to do? What are the limits?\\n\\n- in the related work, the authors connect training-free guidance to (Yu et al., 2023b): \\\"and training-free guidance (Yu et al., 2023b)\\\" But the paragraph where they speak of training-free guidance (p2) does not even mention (Yu et al., 2023b). This point must be explained more clearly, as it seems to be the central contribution of the paper.\\n\\n-could the authors extend the related work and better explain the difference betwen their work and Janner et al. (2022) and Liang et al. (2023)? Putting it before the experimental study would help, in my opinion.\\n\\n- Table 1: why is the authors' method performing worse than competitors in the medium dataset?\\n\\n- could the authors explain how they transformed language-based prompts into a goal-related energy function?\\n\\n\\n- Appendix F, HER is not an executor function, it is a method to leverage failed trajectories in GCRL. I don't understand the authors' description at all, we clearly do not see HER the same way...\\n\\n\\n## Over-statements:\\n\\n- p2: \\\"This work is a trial to approach the ultimate embodied AI that can assist people with diverse tasks such as healthcare, driving, and housework, though further endeavor is, of course, still largely demanded.\\\" So, is this work a successful trial? Clearly, it does not approach this ultimate embodied AI it describes. This is a huge over-statement, and the authors should describe in the introduction the goals they truly manage to achieve.\\n\\n- p5: \\\"By doing so, it expedites the sampling process and ensures adherence to defined objectives, proficiently fulfilling our requirements and presenting a viable resolution to our challenges\\\" -> only politicians write this way! :) Be concrete about the positive sides of your method, and do not be afraid of listing limitations, you are doing science, not selling something\\n\\n- p6: \\\"Fig. 4h exemplifies our algorithm\u2019s prowess\\\" -> sorry, I don't consider this as a prowess.\\n\\n- p7: This holds considerable significance for offline RL: what makes you believe this is so? Without properly backing up this claim with results, I see this as an over-statement.\\n\\n- \\\"Our work introduces a novel way of solving open-ended embodied tasks, which may shed light on cognitive neuroscientific research on understanding human intelligence (Taylor et al., 2016).\\\" -> This is a huge over-statement. Can the authors explain in which way they believe their work can concretely shed light on cognitive neuroscientific research on understanding human intelligence?\\n\\n\\n## Local issues and typos ##\\n\\n- p2: These works are worthy of recognition on the path to embodied AI that can truly tackle open-ended tasks.\\nDo the authors use \\\"task\\\" and \\\"goal\\\" interchangeably?\\n\\n- p2: from somewhere.). -> from somewhere).\\n\\n- Bayesian formula -> Bayes formula\\n\\n- p3, in (1) the energy function is written g, but in the examples below it is written c\\n\\n\\n- \\\"Generative models such as diffusion models for image generation can \u201cdraw\u201d the pixels with any color, whereas a real-world painter can only use the colors available in the palette\\\". -> This makes no sense. How can a generative model draw an image with pixels outside the computer's palette?\\n\\n- \\\"This is different from traditional machine learning tasks, where the focus is on finding a single optimal output for a given input.\\\" -> By traditional, do you mean supervised learning? Reinforcement learning is traditional (rooted in Bellman, 1957) but it corresponds to what you describe in the previous sentence.\\n\\n- \\\"Oftentimes we care about multiple steps rather than a single state, e.g, asking a robot to move in circles. Therefore, tackling\\nopen-ended task goals in embodied AI becomes challenging.\\\" -> this is very poorly written. Ask yourself what is your point and write what your point is, avoid being only allusive.\\n\\n- p3: dimension (Here -> here\\n\\n- discrete actions.) -> actions).\\n\\n- you don't need a \\\"nanny\\\" robot to clean or tidy a house, a anny is to take care about children.\\n\\n- p4: the task is achieve -> is to achieve\\n\\n- p5: concatate -> concatenate\\n\\n- p5: plan executing -> execution\\n\\n- p5: Several optional can be made for the plan executor -> there are several options...\\nBy the way, these options should be described in the main text\\n\\n- p5: Since our executor to multiple-step actions, -> something is missing\\n\\n- p5: to the Appendix: which one?\\n\\n- p5: the generating policy consistent with D4RL: what do you mean? I don't understand.\\n\\n- p6: we present 100 episodes both from the same start point (marked as green star). -> why \\\"both\\\"? It makes no sense.\\n\\n- p7: the the mean\\n\\n- p8: which including sub steps: -> includes\\n\\n- Appendix B, try to fit Table 2 into Appendix B.2\\n\\n- Appendix C, about Mujoco environment: \\\"using a specific algorithm and acculumating 1M samples\\\" -> which  specific algorithm do you mean?\\n\\n- Appendix F: \\n\\n\\\"by , takes\\\" -> something is missing\\n\\nwe adopt the (Ma et al. (2022) as ...\\\" -> something is missing\"}"}]}
{"paper_id": "rkplYfqUr0", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Language model (LM) prompting is a popular approach for solving natural language processing (NLP) tasks. These models can perform tasks through a process known as in-context learning, where they are primed with examples to adjust to new tasks without fine-tuning. Recent studies have debated the mechanics behind in-context learning and its sensitivity to prompt variations, requiring precise demonstrations for optimal performance.\nGap: Discriminative prompting, where the model predicts labels based on input, is prone to miscalibration and brittle when prompt variations occur. This setup's reliance on finding the right training examples and the high variance in performance based on prompt structure necessitate a more robust approach.\nHypothesis: The authors propose that using a generative prompting framework for zero-shot text classification can address the issues of miscalibration and brittleness. They hypothesize that estimating the likelihood of generating input conditioned on natural language descriptions of labels can lead to better performance and robustness compared to discriminative methods.\nMethod: The authors introduce GEN-Z, a generative zero-shot text classification framework. GEN-Z computes the likelihood of input text given label descriptions, which include additional context. The method aggregates results from multiple paraphrases of label descriptions to reduce variance. Experiments were conducted using six open-source language model families across 19 classification tasks. The authors manually crafted label descriptions and generated paraphrases using ChatGPT to improve task performance.\nConclusion: GEN-Z outperforms traditional zero-shot and few-shot baselines, demonstrating robustness to prompt variations and better performance overall. The approach enables personalized classification by integrating contextual information about the evaluation set, such as the author's demographics or the text's domain.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"- This paper is well-written and easy to follow.\\n- The empirical analysis is comprehensive and solid regarding datasets and backbone models.\\n- The proposed method is simple yet effective, providing a solid and lightweight framework to solve zero-shot text classification problems.\", \"Weaknesses\": \"The proposed method, presentation, and empirical analysis are well self-explained. I don't have much concern about it. \\n\\nMy main concern is whether the problem of \\\"zero-shot text classification with some human/ChatGPT labeled data\\\" is meaningful. If one would like to do some annotation, either with human annotators or ChatGPT, why not directly annotate some labeled samples for few-shot text classification? Thus, in my opinion, to show that this problem setup and the proposed framework are meaningful, the authors need to somehow show that this zero-shot framework is more effective than a few-shot text classification framework given the same amount of annotation effort.\", \"Questions\": \"Do you think the proposed framework can be used to amplify few-shot classification systems? I think it may make better sense in assisting a few-shot classification system since adding a few more training samples may not be as effective as providing these label descriptions given the same annotation efforts.\"}"}]}
{"paper_id": "4N7v4w2r3b", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The robustness of neural network proxies is critical as these models are increasingly used in various domains to optimize or monitor quantities of interest. Proxies often serve as stand-ins for true objectives that are costly or difficult to evaluate directly. Despite their practicality, proxies are susceptible to 'gaming,' where optimization pressure leads them to fail in representing the true objective accurately. This issue has been acknowledged in several domains, including education and public policy, and poses significant risks when neural networks are tasked with capturing human values and preferences.\nGap: The robustness of neural network proxies under optimization pressure remains largely unexplored and unsolved. Existing research indicates that proxies may collapse under optimization pressure, but there is limited work on systematically evaluating proxy robustness and developing robust methods.\nHypothesis: Proxy robustness is a solvable problem that can be incrementally improved, and devising comprehensive benchmarks can uncover new properties of the proxy gaming problem, aiding in the development of methods to enhance proxy robustness.\nMethod: The paper introduces \"ProxyBench,\" a benchmark to assess the robustness of neural network proxies against optimization pressure in the text domain, particularly in Reinforcement Learning from Human Feedback (RLHF) settings. The benchmark includes: (1) gold reward models, representing true objectives; (2) a dataset of example prompts paired with preferred responses to train proxy models; and (3) various optimization methods, including best-of-n (BoN), reinforcement learning (RL), and white-box adversarial optimization. Multiple experiments are conducted using baselines and variations of proxy models, evaluating them under different optimization pressures to measure the divergence between proxy and gold rewards and identify gaming.\nConclusion: The study finds that proxy models are prone to gaming under optimization pressure, and this vulnerability increases with the complexity and scale of both the policy and the optimization process. Larger proxy models exhibit more robustness, but common methods like adversarial training are insufficient, as they do not significantly improve robustness across all settings. This underscores the need for new techniques to enhance the robustness of proxy models, and the open-sourcing of the benchmark and experiment code aims to facilitate further research in this area.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 4, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"The strengths of the work is listed below:\\n1. The introduction of ProxyBench is a substantial contribution, as it provides a practical and comprehensive benchmark for assessing the robustness of neural network proxies against optimization pressure, especially in the text domain.\\n2. The paper conducted extensive experimental studies, and the insights from the statistics are very meaningful.\\n3. This work uncovers the relationship between proxy model size and its robustness, contributing to the understanding of how different factors like model parameters, training data, and training steps affect proxy stability.\\n4. The introduction of metrics for evaluating proxy reward model robustness and detailed analysis on the performance of baseline methods are practical.\\n5. The exploration of different optimization strategies, including best-of-n, reinforcement learning, and white-box optimization, indicates a deep understanding of the landscape and provides a nuanced perspective on potential vulnerabilities.\", \"Weaknesses\": \"1. Assumption of Gold Model Accuracy (Section 4.1): The paper assumes that the ensemble of functions trained on human preferences accurately reflects the true objective. However, there's an inherent assumption that the datasets used encompass all aspects of \\\"helpful and harmless\\\" which might not be comprehensive. There is also a risk that the ensemble could inherit systemic biases or errors present in the training datasets.\\n2. Training the proxy model on a fixed dataset without further interaction with the gold model may not reflect the iterative nature of learning and adapting to new data.\\n3. There is a missing relevant work \\\"TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization, ICLR2023\\\".\\n4. The paper notes a significant correlation of gamed examples across different proxy models. This suggests potential inherent weaknesses or biases in the training process or data. However, the paper does not provide a detailed analysis of the underlying causes of this correlation or strategies to mitigate it.\\n5. The appearance of phase shifts in reward curves during RLHF optimization highlights that current methods may not capture the complexity of policy learning dynamics. The paper's methods might be missing other, subtler forms of proxy gaming.\", \"Questions\": \"I do not have other questions. Please refer to the weakness colum above.\"}"}]}
{"paper_id": "Mdk7YP52V3", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Heteroskedastic regression models allow modeling of non-constant output noise, unlike traditional homoskedastic models which assume noise is constant across inputs. Such models can be useful in scenarios where output noise varies with input, highlighting areas where a model \"knows it fails.\" However, real-world applications have shown that overparameterized heteroskedastic neural networks are prone to overfitting, either by fitting every data point too closely or by considering targets as pure noise.\nGap: There is a lack of comprehensive theoretical understanding of the failure modes of overparameterized heteroskedastic regression models, particularly in their propensity to overfit data in extreme cases. Prior studies have not provided a model-agnostic theoretical framework to explain these phenomena, which indicates a gap in understanding the foundational reasons behind the observed pathologies in model behavior.\nHypothesis: The paper hypothesizes that the observed overfitting and model instabilities in overparameterized heteroskedastic regression models can be explained through a theoretical lens using statistical physics. It posits that these issues are not unique to neural network architectures but are inherent to the field theory of an overparameterized conditional Gaussian likelihood model.\nMethod: The authors employ a field-theoretical approach derived from statistical physics to study the pathologies in heteroskedastic regression models. They propose a nonparametric free energy (NFE) framework to model the behaviors of overparameterized models, suggesting that solutions to the differential equations derived (from this field theory) can explain the behaviors observed in practice. They also present a phase diagram to describe the model's behavior under varying regularization strengths and empirically validate the framework against real-world data using neural networks and numerical solutions to the derived equations.\nConclusion: The study finds that overparameterized heteroskedastic models exhibit phase transitions\u2014sharp changes in behavior under varying regularization strengths\u2014a finding that aligns with the NFE solutions. This highlights the necessity for appropriate regularization in these models. Furthermore, the authors suggest a more efficient regularization tuning approach aligning with numerical insights. Their theoretical framework helps explain issues with these models, generic across architectures and datasets, potentially leading to improved regularization strategies in practical applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"1. (main) Conceptually interesting: a richer understanding of regularization for both mean and deviation.\\n2. Uncertain quantification: an important topic, this paper provides some ideas about how to tame overparametrization\", \"Weaknesses\": \"1. (main) Is there a sharp phase transition? This paper lacks quantitative measurement and results. It would be interesting to calculate, at least under certain simple generative models, the free energy and check if there is a first-order/second-order phase transition. \\n2. (main) Technically speaking, not sure which part of the paper is innovative---for example, Eqns 7--10, are they new or semi-new (i.e., similar derivations are obtained in a different context)?  I would be skeptical that Eqns 7--10 are entirely new. Also, the proposed regularization is well-studied in the literature.\\n3. Data experiments are simple, but I am mostly fine with that, since this paper is mainly proof-of-concept.\\n4. It is a bit handwaving when transitioning from parameter norm regularization to gradient regularization\\n$$\\n\\\\int \\\\alpha || \\\\nabla \\\\hat \\\\mu(x) ||_2^2 dx, \\\\qquad \\\\int \\\\beta || \\\\nabla \\\\hat \\\\Lambda(x) ||_2^2 dx.\\n$$\\nI feel that there are missing gaps between parametric models vs nonparametric models, though the idea can be understood intuitively.\", \"Questions\": \"See the above section\"}"}]}
{"paper_id": "8fQlGQkj0S", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large language models have shown improved predictive performance when provided with in-context samples. In-context learning (ICL) has been connected with theories relating to Bayesian inference and gradient descent. However, there exist two distinct modes: task learning and task retrieval, which have not been thoroughly examined together in prior analyses.\nGap: Existing analyses largely overlook the impact of pre-training distributions on ICL and lack explanations covering both task retrieval and learning within the same framework.\nHypothesis: The hypothesis is that by modeling both pretraining data and in-context samples using generative models, researchers can derive risk upper bounds for task retrieval and learning, providing clear separations and understandings between these two modes.\nMethod: The authors propose generative models for pretraining data and in-context samples, using a Gaussian mixture model to reflect task diversity. They assume the pretraining data is drawn from noisy task distributions centered around clean tasks. They then explore in-context prediction using a Bayes-optimal next-token predictor as equivalent to the posterior mean of the label conditioned on in-context samples, deriving risk upper bounds for ICL. Numerical computations and experiments on Transformers validate their findings.\nConclusion: The paper concludes that the task learning risk decreases quadratically with more in-context samples, while task retrieval risk follows a U-shaped pattern, suggesting an optimal number of in-context samples exists for task retrieval. These findings provide a new theoretical framework distinguishing between in-context task learning and retrieval modes.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"- The authors highlight the task retrieval and task learning aspects of ICL. \\n- The authors provide risk bounds for both the setup under a mixture of linear Gaussian generative model.\", \"Weaknesses\": \"- The generative model is simplistic as it is limited to linear Gaussian mixture. In contrast HMM based ICL is already studied in other works already mentioned in the paper. The authors should explain the importance of studying this model. \\n\\n- The authors leave out some area of works that are related, e.g. meta-learning and retrieval augmented learning. A few recent examples of the latter are -  'A Statistical Perspective on Retrieval-Based Models' by Basu et al., 'Generalization and stability in in-context learning' Li  et al. \\n\\n- The dependence of U shaped risk bound on component reweighting and shifting is not discussed properly. Furtheremore, U shape is also observed through the bias-variance tradeoff in optimization literature. The authors don't connect the U shaped mention here with the bias-variance tradeoff. \\n\\n- The novelty in deriving the posterior distributions for the mixture of Gaussian distribution is unclear to me.\", \"Questions\": \"Please look at the weakness section.\"}"}]}
{"paper_id": "lkIRFglmTp", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Reinforcement learning tasks are often modeled as Markov decision processes (MDPs), which assume that the state observed contains all necessary information. However, real-world decision processes often do not satisfy the Markov property, leading to non-Markovian observations in partially observable environments. This challenge is traditionally addressed by augmenting observation spaces with history data to approximate the Markov property. However, current methods for this augmentation lack generality and often require task-specific hand-designed features. The partially observable Markov decision process (POMDP) framework accommodates such non-Markovian observations but optimal behavior typically depends on the history of states, which is difficult to manage without sophisticated mechanisms.\nGap: Existing approaches for handling partial observability in reinforcement learning often rely on manual augmentation of state information or assumptions that may not hold broadly across different tasks. Specifically, there is a lack of practical methods to autonomously detect and resolve partial observability in a general and scalable way without predefining state features or heavily relying on expert intervention.\nHypothesis: The authors hypothesize that the \u03bb-discrepancy, defined as the difference between TD(\u03bb) value functions for different lambda values, is a reliable measure to detect partial observability in decision processes. Furthermore, minimizing this \u03bb-discrepancy through optimized memory functions can resolve partial observability and improve decision-making in reinforcement learning.\nMethod: The paper introduces \u03bb-discrepancy, a measure to quantify the non-Markovianity in decision processes based on the difference between temporal difference (TD) learning value functions for different lambda parameters. The authors conducted theoretical analyses proving that the \u03bb-discrepancy reliably indicates partial observability and empirically demonstrated strategies to minimize it. The method involves using these discrepancy measures as an optimization target by implementing memory functions that summarize historical data to enhance the agent's observational inputs. Two main optimization strategies were developed: one using closed-form solution techniques in a planning setting and another using hill-climbing in online learning where only estimates are available.\nConclusion: The authors conclude that \u03bb-discrepancy is a robust measure for identifying and resolving partial observability in reinforcement learning environments. By targeting the reduction of \u03bb-discrepancy, they demonstrate that agents can use augmented memory functions to approximate the Markov property effectively, thereby enhancing decision-making capabilities. The study's findings suggest that \u03bb-discrepancy can be practically applied without the need for full state observability, offering generalizable solutions across various POMDPs.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"- The paper is generally well written\\n- The paper is well motivated -- a metric denoting the Markovness of a POMDP is a very useful tool\\n- The approach is novel\", \"Weaknesses\": \"- The experiment section is lacking, only comparing against a memory-free baseline\\n- The tested environments and models are very low dimensional, and it's not clear such a method would scale to more interesting problems\\n- If I am understanding this correctly, the proposed metric is flawed in that errors in the Q function (not the state estimator) will result in a $\\\\lambda$-discrepancy. For example, given a perfect state estimator $M^*(o_1, o_2, ..., o_n) = s_n$ and two imperfect Q functions $Q_\\\\theta, Q_\\\\phi$: $ \\\\lVert Q_{\\\\theta}^{\\\\lambda_1}(s_n) - Q_{\\\\phi}^{\\\\lambda_2}(s_n) \\\\rVert > 0$. In this case, I do not see how $\\\\lambda$-discrepancy is a better metric of Markovness than the traditional TD(0) Q learning error.\", \"Questions\": \"- What makes the $\\\\lambda$-discrepancy specific to partial observability? A policy trained on a fully-observable MDP will also have a $\\\\lambda$-discrepancy if the Q function is even slightly less-than-optimal.\"}"}]}
{"paper_id": "MeB86edZ1P", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Neuromorphic computing with spiking neural networks (SNNs) offers a promising path for energy-efficient AI applications. However, unlike human brains, which can learn various tasks throughout life, these neural networks suffer from catastrophic forgetting when learning new tasks. Traditional approaches inspired by neuroscience like episodic replay or synaptic metaplasticity do not guarantee complete knowledge preservation across neuron populations, which is crucial for neuromorphic computing. Machine learning methods involving orthogonal projection are mathematically grounded but lack a neural execution mechanism.\nGap: There is a need to develop a method for spiking neural networks that prevents catastrophic forgetting and is aligned with both biological principles and mathematical rigor, particularly suitable for neuromorphic computing, which traditional methods fail to adequately address.\nHypothesis: Lateral connections and Hebbian learning can be utilized to project neural activity traces into orthogonal subspaces, thus preserving old knowledge while learning new tasks in a neuromorphic-friendly manner.\nMethod: The authors propose a method called Hebbian Learning based Orthogonal Projection (HLOP) for SNNs, which involves using lateral connections and Hebbian learning to extract principal neural activity subspaces, enabling orthogonal projection. This method updates synaptic weights without influencing old tasks by modifying presynaptic activity traces and utilizes recurrent lateral connections with skew-symmetric weights for orthogonal projection. Experiments conducted with different SNN training methods and datasets showed that HLOP effectively prevents forgetting.\nConclusion: The proposed HLOP method consistently outperforms existing approaches, achieving nearly zero forgetting in SNNs across various settings. The work demonstrates that Hebbian learning and neural circuits provide strong continual learning capabilities by enabling orthogonal projection of activity traces. This method opens a path for developing high-performance, continual learning systems in neuromorphic computing.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1.The paper introduces for the first time a method that fully utilizes neuronal operations to approximate orthogonal projection matrices, and achieves the best performance surpassing multiple baseline methods on various datasets in continual learning task.\\n2.The implementation based on Hebbian learning can seamlessly integrate with different training methods and aligns well with the parallel local learning approach of neuromorphic chips.\\n3.HLOP can be directly applied to SNNs, providing a new approach for continual learning in SNNs.\", \"Weaknesses\": \"1.The method lacks sufficient detail in the methodology section. Providing complete formulas or a specific illustrative example would enhance understanding. \\n2.The baselines compared in the study include EWC (Kirkpatrick et al., 2017), HAT (Serra et al., 2018), and GPM (Saha et al., 2021). It would be valuable to investigate if there are recent works that achieve better results than these methods.\\n3.Although the continual learning approach in this study relies solely on neuronal operations without directly utilizing past data, the increasing number of subspace neurons with each new task learned implies a form of data compression and storage to some extent.\\n4. Although the paper uses Hebbian learning to achieve a pure neuronal operation method, it seems that the newly added subspace neurons do not participate in the model's forward process. If this is the case, then despite being a pure neuronal operation approach, it is essentially just an estimation method for orthogonal projection matrices.\\n5. While the paper demonstrates the effectiveness of HLOP, it does not provide an explanation or analysis of why the weight matrices obtained through Hebbian learning can serve as a substitute for the orthogonal projection matrix.\", \"Questions\": \"See weaknesses.\"}"}]}
{"paper_id": "532tcx7IHF", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Reinforcement Learning (RL) is a prominent research area with applications in smart manufacturing, autonomous driving, among others. Existing open-source RL projects like Stable-baselines3 and Tianshou offer algorithm implementations but face challenges such as long-term sustainability, comprehensive ecosystem development, and resource-intensive nature.\nGap: Existing RL libraries and frameworks often lack a comprehensive and modular approach, fail to decouple RL algorithms fully, and do not support long-term evolution plans. Many libraries focus primarily on model training, neglecting essential components such as model evaluation and deployment, and lack exhaustive benchmark testing data.\nHypothesis: RLLTE proposes a long-term evolution, extremely modular, and open-source framework for RL. It aims to set a new standard for RL engineering practices by offering a complete ecosystem that includes model training, evaluation, deployment, and a benchmark hub, along with a LLM-empowered copilot.\nMethod: RLLTE decouples RL algorithms into primitives from the exploitation-exploration perspective, offering modular components such as encoders, storage, policy, and distribution. It supports data augmentation for improved sample efficiency and generalization. RLLTE also includes an evaluation toolkit, deployment tools, and integrates LLM to create a copilot for reduced learning curves. The framework is designed to be flexible, accommodating various hardware for enhanced scalability.\nConclusion: RLLTE is poised to advance RL research and application by providing a comprehensive and modular framework that adapts to evolving needs. It fills the gaps left by existing RL projects by offering a long-term evolution plan, complete algorithm decoupling, and a robust ecosystem for academia and industry.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"* Modular and customizable design, allowing for easy algorithm development and improvement.\\n* Long-term evolution plan, ensuring the framework stays up-to-date with the latest research.\\n* Comprehensive ecosystem, covering various aspects of RL research and application.\\n* Built-in support for data augmentation techniques, improving sample efficiency and generalization ability.\\n* Multi-hardware support, accommodating diverse computing hardware configurations.\", \"Weaknesses\": \"* The paper does not provide a thorough comparison of RLLTE with other existing RL frameworks.\\n* The proposed LLM-empowered copilot is in its early stages and may not be as effective as expected.\\n* The paper does not discuss potential limitations or challenges in implementing the proposed framework.\", \"Questions\": \"* How does RLLTE compare to other existing RL frameworks in terms of performance, modularity, and ease of use?\\n* What are the specific advantages of using RLLTE over other RL frameworks for different types of RL problems?\\n* How does the proposed LLM-empowered copilot improve the overall RL research and application process?\\n* Are there any potential limitations or challenges in implementing the proposed RLLTE framework that the authors have not discussed?\"}"}]}
{"paper_id": "JTcaziw7G1", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large language models (LLMs) have shown impressive abilities in answering queries based on pre-trained knowledge. However, for information not included in pretraining data, information retrieval can augment the generation capabilities of LLMs. Private information, which needs to be retrieved, is increasingly stored in distributed settings by organizations and individuals.\nGap: The problem addressed is the leakage of information about queries and database content during information retrieval using neural embeddings unless both are stored locally. The existing methods do not ensure privacy when the database is distributed across multiple servers.\nHypothesis: The authors propose that it is possible to achieve private retrieval augmented generation (PRAG) for LLMs by using multi-party computation (MPC) techniques that ensure secure query transmission and retrieval from distributed servers without any server observing the client\u2019s query or the database content.\nMethod: The proposed approach, PRAG, uses a combination of secret sharing and MPC to securely transmit queries to a distributed set of servers containing a private database. It leverages a novel MPC-friendly protocol for inverted file approximate search (IVF) that allows for efficient document searches. The method is designed to achieve sublinear communication complexity, enabling private and scalable information retrieval useful for LLMs without centralizing data or exposing sensitive information. The experiments involve synthetic and real data to evaluate retrieval accuracy and speed across various parameters, demonstrating the method's efficacy.\nConclusion: PRAG successfully provides a privacy-preserving solution for dense information retrieval that remains efficient in terms of communication complexity. The approach allows for the secure augmentation of LLMs with external information while maintaining the confidentiality of both queries and database content. This lays a foundation for more secure LLM applications, though the computational costs suggest a need for further optimization.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1.\\tAddress the privacy issue in the LLM augment generation process.\\n2.\\tThe design provides security guarantees for both users and external information supplier.\", \"Weaknesses\": \"1.\\tThe design is ad-hoc, just combining existing protocols together without design efforts.\\n2.\\tThe approximate search design in Sec 2.5 may lead to the leakage of centroids, which is not well-elaborated.\\n3.\\tThe design is very slow and impractical.\", \"Questions\": \"1.\\tIt seems that such design can also be implemented with other linear secret sharing schemes, why do you choose to use the Shamir\u2019s shares (instead of arithmetic secret shares, replicated secret shares, etc.)?\\n\\n2.\\tThe authors claim that it is possible to use perturbations to mitigate the leakage in the approximate search design, but it is unclear what the suitable parameters is for this noise.\\n\\n3.\\tAlso, it is unclear the security level after adding the noise, a proof sketch is required to understand whether the countermeasure is effectively addressing the leakage.\\n\\n4.\\tThe proposed scheme becomes very slow (at least 50s needed) when the database size goes to a million scale, but such database is quite common in the real-world. This renders the impracticality of proposed design.\\n\\n5.\\tEven if we take the approximate design into consideration, it is still slow with a large k, I am wondering whether a large k is also common in the real-world systems because the LLM may want to embed more external resources to produce a general response for queries.\"}"}]}
{"paper_id": "nmBjBZoySX", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Graph Neural Networks (GNNs) have gained prominence for representation learning on graph-structured data due to their superior performance in various tasks. Advancements in GNNs focus on simplifying graph structures and compressing GNN models to mitigate high computational costs associated with large graphs. A recent trend inspired by the Lottery Ticket Hypothesis posits that within GNNs, there exist sparse subnetworks that can perform as well as full networks, termed Graph Lottery Tickets (GLT).\nGap: Existing GLT methods rely excessively on trial-and-error for pruning rate tuning, lack flexibility due to irreversible pruning paradigms, and face scalability issues when applied to deep GNNs, as they assume static topology across layers, limiting their applicability in larger and deeper GNN scenarios.\nHypothesis: The paper proposes an Adaptive, Dynamic, and Automated framework for identifying Graph Lottery Tickets ($\\text{AdaGLT}$) that aims to address existing limitations by introducing layer-adaptive sparse structures, integrating pruning with training for dynamic workflows, and automatically capturing graph lottery tickets across different sparsity levels without extensive tuning.\nMethod: $\\text{AdaGLT}$ method includes: 1) Layer-adaptive sparsification, allowing different sparse structures for each layer to facilitate deeper GNNs. 2) Integrating pruning and training processes for dynamic workflows that include both pruning and potential restoration. 3) Automated pruning scheduling which obviates the need for manual configuration of pruning ratios. Theoretical proofs ensure that AdaGLT mitigates over-smoothing and achieves improved sparse structures in deep GNN settings. Extensive experiments demonstrate AdaGLT's superiority over competitors across multiple datasets and settings, particularly in deep GNN scenarios.\nConclusion: AdaGLT effectively mitigates the limitations of current GLT approaches by providing a scalable, automated, and dynamic method for identifying sparse structures within GNNs. It outperforms state-of-the-art methods in efficiency and adaptability, demonstrating the ability to uncover GLT with significant graph and weight sparsity in variety of contexts including deeper GNNs.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"1. The paper is well-written and organized. As compared to previous UGS, the proposed AdaGLT jointly sparsifies both graphs and GNN weights and adopts an adaptive layer sparsification process. In addition, it offers the practitioner the chance to adopt automatic pruning and dynamic restoration for extracting the graph lottery tickets.\\n\\n2. The effective combination of automation, layer adaptivity, and dynamic pruning/restoration provides better properties as compared to previous methods.\", \"Weaknesses\": \"Given the contribution of UGS and other series of graph lottery tickets work, this automated tool sounds like incremental work. However, the analysis of scalability issues when applied to deep GNNs is worth reading.\\n\\nMost experiments are conducted on small and transductive graph/tasks. The ogbn-arxiv is a small graph and ogbn-protein is a medium graph actually but are claimed as large graphs, more reasonablely large graphs should be considered. Also, how about inductive settings like GraphSAGE or other SOTA ones?\\n\\nOther than the above aspects, I think the other perspectives of this paper are clear.\", \"Questions\": \"See weaknesses. I wonder whether the contribution applied to an inductive setting or evolving graphs with increasing nodes or edges.\"}"}]}
{"paper_id": "vVoWRFV5Y4", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The Quadratic Assignment Problem (QAP) is addressed in the context of solving combinatorial optimization problems using reinforcement learning. Previous works have successfully applied reinforcement learning to problems like the traveling salesman problem (TSP) and vehicle routing problems (VRP) by using both constructive and improvement designs. However, existing reinforcement learning approaches have struggled to outperform classical optimization techniques for specific problem classes due to the maturity and optimization of traditional solvers. The QAP is known to be particularly challenging because it is both NP-hard to solve exactly and to approximate, making it an appealing candidate for the application of novel methods like deep reinforcement learning (DRL).\nGap: There is no known method to exactly solve QAP instances for sizes larger than 30, whereas other combinatorial optimization problems such as the TSP can be solved for much larger instances with existing techniques. Additionally, previous attempts to use machine learning approaches for QAP are scarce and have not yet demonstrated success over classical methods.\nHypothesis: The authors propose that a deep reinforcement learning approach, specifically using a novel double pointer network with policy gradient methods, can effectively provide competitive solutions to the QAP without needing instance-specific retraining, and can do so in a computationally efficient manner compared to traditional solvers.\nMethod: The authors present a method based on deep reinforcement learning using a double pointer network architecture. This network alternates between selecting a location and a facility, leveraging reinforcement learning as a sequential decision-making process. The model is trained using the Advantage Actor-Critic (A2C) algorithm on synthetic QAP instances, where locations are embedded using one-dimensional convolutions and facilities using graph convolutional networks to capture flows. The network uses an attention mechanism to guide the selection process, and results are evaluated using a local search heuristic as a baseline.\nConclusion: The proposed deep reinforcement learning approach achieves solutions to the QAP that are on average within 7.5% of a high-quality local search heuristic, and occasionally outperform it. The method demonstrates a significant reduction in computation time compared to traditional optimization techniques like Gurobi, especially as the problem size increases. While not yet surpassing established solvers like the swap heuristic, the approach offers promising performance gains, particularly in runtime, and shows potential for scaling to larger problems.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1- This paper is well motivated as the QAP has been rarely studied using DRL;\\n2- It seems the proposed method works on the self-generated dataset.\", \"Weaknesses\": \"1- It seems that from the DRL based heuristic perspective, the objective function in QAP does not make difference compared with the ones in ILP, both of which are just captured by a reward in DRL. In this sense, the QAP in this paper is almost the same as the vehicle and customer assignment in VRPs with linear objectives, such as\\n[a] Deep Reinforcement Learning for Solving the Heterogeneous Capacitated Vehicle Routing Problem. IEEE T Cybernetics;\\n[b] Learning to Solve Vehicle Routing Problems with Time Windows through Joint Attention. Arxiv;\\n[c] Solving NP-hard Min-max Routing Problems as Sequential Generation with Equity Context. Arxiv.\\nThose works also involve sequentially or parallelly selecting vehicles and customers, while using (advanced) Transformers rather than pointer network.\\n\\n2- Besides, the Transformers in 'Learning Improvement Heuristics for Solving Routing Problems. TNNLS' outputs a probability matrix, which could also be tailored to the probability of facility and location pair with proper masking.\\n\\n3- The evaluation is quite simple, which solely focuses on one single problem with randomly generated instances. And the baselines are also insufficient and not strong enough.\\n\\nOverall, the method falls short of novelty, and the evaluation is inadequate, which is below the standard of an ICLR publication.\", \"Questions\": \"Please see the above weakness.\"}"}]}
{"paper_id": "RtDok9eS3s", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The transformer architecture, introduced in 2017 by Vaswani et al., has been a cornerstone of various advancements in deep learning. It typically involves stacking identical transformer blocks, which are complex structures made up of attention and MLP sub-blocks, skip connections, and normalization layers. Despite their complexity, these blocks have undergone little modification since their initial design. Simplification of these blocks could help bridge the gap between theoretical studies, which often assume simpler architectures, and practical implementations. Signal propagation theory has previously been employed to study and enhance deep architectures like transformers by mitigating signal degeneracies.\nGap: Standard transformer blocks, although crucial for neural network architectures, are intricate and somewhat rigid, making them sensitive to minor architectural changes. This complexity can lead to drastic impacts on training efficiency and even render models untrainable under slight alterations. There's a compelling need to determine how much these standard blocks can be simplified without compromising training speed or performance.\nHypothesis: The authors hypothesize that the standard transformer block can be significantly simplified by removing several components without any loss in per-iteration training speed or performance. These components might include skip connections, value or projection parameters, sequential sub-blocks, and normalization layers.\nMethod: The authors propose and test simpler transformer block designs based on signal propagation theory and empirical studies. They suggest various modifications: removing skip connections, setting value and projection parameters to identities, employing shaped attention, and using parallel block structures. These changes are evaluated through experiments on autoregressive decoder-only and BERT encoder-only models. The study largely uses a 18-block 768-width model on the CodeParrot dataset, applying standard setups from existing literature for baselines. For BERT experiments, the authors build on the Crammed BERT setup, using the Pile dataset for pre-training and GLUE for fine-tuning, with a masked language model pretraining objective.\nConclusion: The authors successfully demonstrate that transformer blocks can be simplified significantly. By removing several components like skip connections, value, and projection parameters, and using simplified attention strategies, they achieve models with 16% faster training throughput and 15% fewer parameters while maintaining the performance of standard transformers. This work suggests that streamlining transformer blocks could lead to better resource efficiency in training and inference without sacrificing model capability.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"- As the work mentions, this is the first work which has simplified the transformer architecture with training throughput gains. Previous works which have tried to simplify transformer architectures have led to increase in training speeds.\\n- The paper is overall well written. The authors discuss each design choice in detail, which helps in understanding the motivation behind the simplifications proposed.\\n- The authors have appropriately discussed the limitations of their work.\", \"Weaknesses\": \"- One weakness is that the results are limited to smaller models of size ~100M. It is not clear if the results would scale to bigger models. \\n- Moreover, the authors only consider GLUE benchmark for downstream evaluation. It would be nice to include a more comprehensive evaluation across different task categories like classification, generation or reasoning.\", \"Questions\": \"- The authors mention that there is a Pre-LN and a Post-LN block and the Pre-LN work is more popular because Post-LN suffers from training instability. Is this phenomenon observed in the signal propagation literature only or all the current LM works also use this Pre-LN variant?\\n- The authors mention \u201cour simplified transformers match the per-update training speed and performance of standard transformers, while enjoying 15% faster training throughput\u201d. If the parameters are being reduced, why isn\u2019t the per update speed faster? And, why is the throughput faster?\\n- On page 3, the authors mention skipless transformers are slower than those with skip connections. Why is that?\\n- Are there any settings (data or tasks) where this simplified transformer does not work as well compared to the standard transformer?\\n- What are the limitations of the proposed simplifications in terms of model expressiveness and capability?\\n- What is the size of the model which has 18 blocks and 768 hidden dimensions?\\n- In section 4,1 what is the motivation from going from SkipInit to ShapedAttention? Another question is since this work is adding identity addition to value and projection matrices, in what way this is simpler than the residual connection?\"}"}]}
{"paper_id": "koYsgfEwCQ", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Unsupervised learning of object-centric representations in dynamic visual scenes is important for understanding spatial and temporal information. Most existing approaches decompose 2D images, often leading to issues like the multi-view mismatch of 2D object segmentation due to the ignorance of the 3D nature of scenes.\nGap: The problem addressed in the paper is the unsupervised 3D dynamic scene decomposition to ensure 3D-consistent scene understanding and motivate downstream tasks such as scene editing.\nHypothesis: DynaVol, a 3D scene generative model using object-centric voxelization, can capture the 3D nature of scenes and outperform existing methods in unsupervised dynamic scene decomposition. This approach will allow not only novel view synthesis but also scene editing.\nMethod: The method involves object-centric voxelization within a differentiable volume rendering framework. DynaVol contains three network components: a dynamics module for voxel features, a volume slot attention module for refining object-level global features, and a compositional neural radiance field for view synthesis. The training involves three stages: a 3D voxel warmup stage, a 3D-to-4D voxel expansion stage, and a 4D voxel optimization stage. The model uses a cycle-consistency loss and volumetric slot attention for learning representations.\nConclusion: DynaVol outperforms existing 2D methods like SAVi and 3D methods like uORF in unsupervised dynamic scene decomposition. It can handle occlusion issues better and enables dynamic scene editing, allowing manipulation of geometric shapes and motion trajectories.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"I like this paper. It is well written, it is interesting, the method seems sensible (thought I have a few questions), and the results look good.\", \"Weaknesses\": \"I have a variety of fairly low-level questions, which I hope the authors can address. \\n\\nOverall I think the method would be easier to understand if the stages were presented in a more central way, instead of at the very end. After reading everything I am still a bit unclear on the meaning of an \\\"episode\\\". \\n\\nSection 2 emphasizes that \\\"we also consider scenarios where only one camera pose is available at the first timestamp\\\". What does it mean for only one camera pose to be available? Camera poses are relative to something. A single pose might as well be the identity matrix. \\n\\nThe beginning of Section 3.1 says the method will be trained \\\"without any further supervision\\\", without defining any initial supervision. So I suppose the question is: further to what?\\n\\nSection 3.1 introduces the number of objects N, as \\\"the assumed number of objects\\\". Is this manually chosen? Later on there is a connected components step, so I wonder if connected components could choose N instead.\\n\\n\\nIt is unclear to me what exactly is happening in the warmup stage vs. the dynamics stage. From the names I might guess that the warmup stage does not involve any dynamics, but this is wrong, because it uses all timesteps and it also trains the forward and backward dynamics modules. The section also says \\\"To obtain the dynamics information, we train an additional module f\u2032\u03be\\\" -- but dynamics information is already obtained by f_\\\\psi, so there is some mixup here, unless the authors do not see f_\\\\psi as obtaining dynamics information. Finally, it's unclear how exactly the connected components and the \\\"feature graph\\\" connect with the rest of the method.\\n\\nL_point seems like it is never defined, and only a citation is given. It would be great to describe in English what is happening there.\\n\\nSection 3.3 seems to complain that prior compositional nerfs used an MLP to learn a mapping from position+direction+feature to color+density, and then proposes to do the exact same thing. Have I missed something here? \\n\\nOverall I like the paper and I hope it can be cleaned up. The video results show that the segmentations are a little messy, and it seems like only 2 real videos are used, but still I think the paper is interesting and useful to build on.\", \"Questions\": \"This is more of a personal curiosity, but: instead of learning two deformation networks (one forward, one backward) and then asking them to be cycle-consistent, would it work to learn an invertible mapping here instead? (Like with Real NVP.)\"}"}]}
{"paper_id": "kaAtQwhnM2", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Energy-based models (EBMs) are valuable for probabilistic modeling because of their flexibility in different domains. They are commonly used with continuous data using methods like contrastive divergence, which involves MCMC sampling. Training EBMs in discrete spaces is challenging due to the intractability of normalization and sampling difficulties.\nGap: Training EBMs on discrete data is problematic because existing approaches, like contrastive divergence, rely on MCMC sampling, which lacks theoretical guarantees and results in biased energy landscape estimates.\nHypothesis: The paper proposes Energy Discrepancy as a method to train discrete EBMs, which eliminates the need for complex sampling methods like MCMC by relying on energy function evaluations at data points and their perturbations with theoretical guarantees.\nMethod: The authors propose using Energy Discrepancy, which involves perturbations based on Bernoulli noise, deterministic transforms, and neighborhood structures, requiring only the evaluation of the energy function at perturbed data points. The method uses importance sampling with uninformed and gradient-informed proposals for estimating energy discrepancies, improving on contrastive sample constructions without demanding sampling.\nConclusion: The proposed method effectively trains EBMs across various discrete tasks without MCMC, achieving competitive results in Ising models, discrete density estimation, and discrete image modeling, while allowing for parallel evaluations, reducing computational demands, and providing theoretical consistency.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 4, \"Contribution\": 4, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"Very thorough ablations are presented in the appendix, investigating the effects of a number of parameters to configure their method.\", \"Weaknesses\": \"The results on discrete image modelling are not very strong, either in terms of sample quality or in terms of estimated NLL.\", \"Questions\": \"I\u2019m curious about the wall-clock speed of training. Your approach admits better parallelisation than existing techniques since it relies on importance sampling rather than MCMC. Do you have any wall-clock timing experiments investigating this? For example, how does the wall-clock time compare of your approach vs. CD-10? \\n\\nMust the GwG sampler be tuned differently when training with ED vs. GwG? i.e. do you find that sampling is less efficient at test-time when a model is trained with ED?\"}"}]}
{"paper_id": "ODSgo2m8aE", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Relational learning, specifically using Graph Neural Networks (GNNs), has emerged as a powerful method for analyzing graph data across various domains such as social media and drug discovery. GNNs have shown great promise due to their ability to perform deep learning operations on relational data, particularly using architectures like Graph Convolutional Networks (GCNs). However, as GNNs are often trained on data that may contain inherent biases, it poses challenges in ensuring fairness and stability in their outputs.\nGap: The gap identified is the lack of research addressing how to stabilize the outputs of GNNs when the graph data contains implicit biases. Existing studies have not applied Lipschitz bounds as a method to control and stabilize the output perturbations of GNNs due to such biases, even though Lipschitz bounds are known to be effective in stabilizing outputs in Euclidean neural networks.\nHypothesis: The hypothesis is that by formulating a Lipschitz bound specifically for GNNs, it is possible to constrain the output perturbations caused by input biases in graph data, thus enhancing fairness during the training of GNN models.\nMethod: The authors propose a method to apply Lipschitz bounds to GNNs to constrain biases and stabilize outputs. This involves defining Lipschitz bounds for GNNs and using the Jacobian matrix to efficiently calculate these bounds, facilitating practical implementation during training. The method is validated through theoretical analysis and empirical experimentation on node classification and link prediction tasks using real-world datasets. The key implementation is through a plug-and-play approach called JacoLip that integrates with existing GNN training pipelines in PyTorch, leveraging automatic differentiation for efficient computation of these bounds.\nConclusion: The study concludes that incorporating Lipschitz bounds in GNN training can effectively limit biases and stabilize model outputs, thus enhancing fairness without significant computational overhead. The proposed method, JacoLip, demonstrates superior performance in enhancing both the fairness and stability of GNN's predictions, validating the hypothesis.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 1, \"Strengths\": \"- Estimate the Lipschitz constant of GNNs via Jacobian matrix\\n- Comprehensive experiments on GNNs for the bias\", \"Weaknesses\": \"Since I\u2019m not familiar with GNN and fairness, it appears difficult to evaluate the contribution. \\n\\nFor the estimation of the Lipchitz constant, the approach is based on the Jacobian matrix, which is loose. In this community, there are several advanced and promising approaches for the Lipschitz constant estimation on DNNs. So I\u2019m wondering how this can be extended to the GNN setting, and what\u2019s the difficulty issue behind this?\", \"Questions\": \"See the above.\"}"}]}
{"paper_id": "JGTC6WgO4T", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Unsupervised Domain Adaptation (UDA) is a technique aimed at training models using an unlabeled target domain by leveraging knowledge from a labeled source domain. UDA traditionally requires access to source data and models, which raises privacy and security concerns. Black-Box Domain Adaptation (BDA) was introduced to alleviate these constraints by utilizing pseudo labels derived from a single source prediction for self-training of the target model.\nGap: Existing methods in BDA primarily focus on a single source model, assuming adequate correlation between the source and target domains. However, in real-world scenarios, multiple source domains might be available, facilitating the generation of pseudo labels, yet these multi-source settings have been largely overlooked in BDA research.\nHypothesis: The authors propose that a novel framework, which efficiently refines pseudo labels by considering the statistical relationships between multiple source domains and the target domain, can fundamentally enhance multi-source black-box domain adaptation (MSBDA).\nMethod: The paper introduces a Label Space-Induced Pseudo Label Refinement (LPR) framework for MSBDA, featuring a Pseudo-label Refinery Network (PRN). This framework learns to relate each source conditioned by the target based on source predictions. The PRN is designed to improve pseudo labels' quality by incorporating prediction concentration within label spaces and reducing the distance between distinct label spaces. The approach is supported theoretically and empirically validated on four benchmark datasets, demonstrating competitive performance.\nConclusion: The proposed MSBDA framework using LPR and PRN demonstrates competitive performance compared to state-of-the-art domain adaptation methods under various domain adaptation settings. It efficiently utilizes multi-source information to refine pseudo labels, thereby facilitating effective target domain adaptation without access to source data or models.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"1. The studied problem is interesting and significant for distilling black-box models into user-accessible models. \\n\\n2. The authors provide thorough experimental details for reproducing this paper.\", \"Weaknesses\": \"1. The major concern is that the presentation is **terrible**. It's hard to understand the motivation and the details of this work. In fact, I don't think this can be easily fixed during the rebuttal phase.\\n\\n2. First, the author mentioned that existing BDA methods suffer from 'decrease empirical errors e.g. through de-noising of source predictions and label smoothing when yT is not accessible'. But, in the modified triangle inequality, there is still a term of p_c, whose definition requires the ground truth. \\n\\n3. I don't quite understand how p_c can be properly obtained. It seems this paper attempts to obtain p_c via the concentration of one source's predictions to those of other sources. What if there is only one best source while the other sources are all terrible? Also, Section 3.3 is confusing. Why the label space should be divided?\\n\\n4. The proposed label refine network also confused me. Is the network architecture derived according to the theorem? I don't think such a complex network in the label space can improve the performance.\\n\\n5. The final prediction performance improvement is marginal. Also, a recent SOTA paper is missing - https://openreview.net/forum?id=hVrXUps3LFA\", \"Questions\": \"See above.\"}"}]}
{"paper_id": "dyG2oLJYyX", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Neural Architecture Search (NAS) automates the design of neural architectures, reducing manual trial and error but often incurs high search costs due to extensive training requirements. Traditional NAS involves sampling a vast space of architectures and evaluating them, usually demanding significant computational resources. Previously, parameterized predictors were used to estimate an architecture's performance without full training, but this led to inefficiencies such as passive architecture filtering.\nGap: Existing NAS methods are inefficient, focusing on time-consuming sampling and training of numerous architectures that may not align with task-specific objectives. They largely rely on evaluating architectures post-generation, which results in wasted computational efforts on irrelevant architectures.\nHypothesis: By shifting from the traditional NAS approach to a conditional Neural Architecture Generation (NAG) framework using diffusion models, architectures can be generated with properties aligning more closely with specific tasks, guided by predictors, thereby increasing efficiency and performance.\nMethod: The proposed method, DiffusionNAG, employs diffusion models to generate neural architectures by treating them as directed graphs. The approach integrates predictors to guide the generation towards architectures satisfying specific conditions, like better performance or robustness. DiffusionNAG involves conditioning the generative process with predictors, allowing architectures to be generated in regions of the search space that are more likely to meet desired properties. This method was tested through experiments in Transferable NAS and Bayesian Optimization NAS scenarios, demonstrating significant performance improvements over baseline NAS techniques. The experimental setup also involved tasks on MobileNetV3 search space and NAS-Bench-201, leveraging dataset-aware predictors for validation.\nConclusion: DiffusionNAG demonstrates a paradigm shift from conventional NAS methods by efficiently generating task-specific architectures using diffusion models under the guidance of property predictors. This method significantly outperforms existing approaches in efficiency and quality, notably achieving substantial speedups in large search spaces and enhancing BO-based NAS processes by limiting early low-quality sample generation.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"This paper is well-motivated. Previous work only uses the diffusion model to model undirected graphs for neural architecture generation. Using the diffusion model for modeling the directed graph is needed. Moreover, classifier guidance is well suited for the Transferable NAS and substitute step 3) in BO-based NAS approaches, where the dataset and the pre-trained classifier are given as the setup.\", \"Weaknesses\": \"1.\\\"Whether diffusion model for neural architecture generation is more efficient than Mutation + Random\\\" is a good question. It is important to ask \\\"when they are better\\\".\\n\\n* The strength of the predictor guidance is an important hyperparameter. In this paper, a good performance can be achieved by cross-validation. Could the author provide some ablation studies of how this hyperparameter affects the performance? How could we effectively define the search range for cross-validation?\\n\\n* Will using the diffusion model for neural architecture generation have some model collapse phenomenon? Does the Diffusion model only generate some similar architecture, and achieve the better \\\"worse case accuracy\\\" by sacrificing the diversity? If that is the case, any explore and exploit framework for it?\", \"Questions\": \"Please see the Weaknesses part.\"}"}]}
{"paper_id": "rtzUW1FU3H", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large language models (LLMs) have shown significant promise in various NLP tasks, but they are limited by the fixed context window in the transformer architecture, which hampers them from effectively processing long input sequences. While many efforts aim to extend LLMs' context windows, assessing their real abilities to understand long contexts remains a challenge, necessitating high-quality benchmark datasets.\nGap: Existing benchmarks for assessing LLMs on long-text tasks suffer from several issues: shorter text lengths that do not meet modern models' capabilities, outdated documents included in training datasets, and an overemphasis on short dependency tasks. Real-world applications demand evaluation of long dependency tasks, which require integrative comprehension of information spanning entire documents.\nHypothesis: The paper posits that a high-quality benchmark, specifically designed and annotated to evaluate long-context understanding abilities of LLMs, can address the deficiencies of current datasets and aid in effectively assessing and advancing LLM capability in long-context comprehension.\nMethod: The authors introduce 'LooGLE,' a benchmark dataset comprising 145 contemporary documents (all post-2022) from diverse domains, each containing over 24k tokens paired with newly generated questions. LooGLE includes both short and long dependency tasks. For long dependency tasks, 1.1k QA pairs were manually created, cross-validated, and constructed to span the entire documents. The dataset was used to evaluate eight state-of-the-art LLMs, exploring how well they perform on long-context tasks relative to short-dependency tasks.\nConclusion: The evaluation demonstrated that commercial models surpass open-source models. LLMs perform adequately on short-dependency tasks but struggle with long-dependency ones. The introduction of retrieval-based methods improved performance, yet advancements in transformer architectures have not resolved long-context understanding challenges. LooGLE serves as an effective benchmark for assessing LLM long-context capabilities and informs future model development.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"- A curated dataset, thoughtfully designed with great efforts to prevent data leakage and ensure long dependency.\\n- Great efforts in assessing current state-of-the-art LLMs' long dependency capabilities such as information retrieval, reading comprehension and reasoning, computation and timeline reordering.\", \"Weaknesses\": \"- see questions.\\n- the paper is sprinkled with typos (refer to questions for a few).\", \"Questions\": \"- Is the benchmark english-only ? If so, this needs to be mentioned.\\n- How will the dataset be released ? For instance ZeroSCROLLS only released the inputs and evaluate through a system of leaderboard, will LooGLE be released the same way ? I'm concerned that revealing input/gold outputs pairs would lead to data leakage for future models.\\n- Concerning data collection, were the sourced documents (after 2022) subjected to any machine-generated verification ? I am concerned that ChatGPT-like texts might compromise the fairness of the evaluation.\\n- Are the open-source models instruction-tuned ? Commercial closed source models like GPT3.5 rely on RLHF or some instruction tuning techniques that enable them to better follow instructions. If the considered open-source baselines are not instruction-tuned, the comparison might be unfair since the prompt used for evaluation is the same and is instruction-based.\\n- Is it fair to have GPT4 both as baseline and evaluator ? I am also concerned about the creation of the dataset of short QAs (generated by GPT3.5). Is it fair to evaluate a model that was used to create the dataset ?\\n- In section 4.2, you mention human evaluation (3) but I cannot find any human evaluation both in the paper and the appendix.\\n- Section 4.3.2 is confusing, the experiments are based on the recent work from Liu et al. 2023, that accessing information in the middle of the document is more challenging for LLMs. If I understood correctly, you suggest concatenating the head and the tail of the inputs and give it to the LLMs as input. This would mean discarding the whole \\\"middle\\\" and leaving the beginning and the end. This puts an immediate limitation on information that can be retrieved by the LLMs. Also did you only use the same model (GPT4-32k) but with various context input length or different variants of GPT4 ? What is the last entry in Table 5 ? (GPT4). For long summarization, arxiv abstracts are highly biased towards the beginning of the article so it is expected that increasing context would result in a higher divergence between the generated summary (which will contain more and more details) and the (gold) abstract.\\n \\n**Typo**\\n- page 2: \\\"Mannually designed both short and long dependency tasks\\\"\\n- page 3: \\\"COmputation.\\\"\\n- page 7: \\\"Retrival\\\"\\n- page 8: \\\"GPT4 evalution\\\"\\n- table 4: \\\"Performence\\\"\\n- appendix page 3: \\\"Dispcrepancy\\\"\\n- not really typos but it is uninformative to report results of the order of \\\"e-300\\\" since at this point there is nothing to really compare.\"}"}]}
{"paper_id": "09xFexjhqE", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Fine-tuning pre-trained model feature extractors is an effective strategy to achieve higher performance in downstream tasks, evidenced by advances such as GPT-3 achieving high GLUE benchmark scores. Robust Fine-Tuning (RFT) specifically aims at enhancing adversarial robustness, crucial for applications in safety-critical areas like medicine and autonomous vehicles.\nGap: Current RFT methods face significant issues with optimizing both adversarial and natural objectives through the same feature extractor, leading to divergent gradient directions. This divergence results in instability and high sensitivity to hyperparameters, limiting the effectiveness of RFT in achieving adversarial robustness.\nHypothesis: Introducing a low-rank branch to separate the optimization of natural and adversarial objectives can address the issue of gradient divergence in RFT, leading to more stable optimization and enhanced adversarial robustness without the need for extensive hyperparameter tuning.\nMethod: The proposed method, AutoLoRa, uses a low-rank (LoRa) branch to partition RFT into two pathways: the natural objectives are optimized via the LoRa branch, while the adversarial objectives are optimized using the feature extractor. The paper also introduces heuristic strategies for automatically scheduling learning rates and loss term scalars to enhance convergence and reduce hyperparameter sensitivity. Extensive experiments using ResNet models and various datasets demonstrated the effectiveness of this approach.\nConclusion: AutoLoRa effectively disentangles adversarial and natural optimization objectives in RFT, achieving state-of-the-art adversarial robustness across various tasks while alleviating sensitivity to hyperparameters. This approach provides a practical, automated solution for converting pre-trained feature extractors to adversarially robust models, validated by empirical results showing improvements over existing methods.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"**Strength:**\\n\\n-   The motivation that optimizing both adversarial and natural objectives through feature extractor yields divergent gradient directions make sense. The proposed disentangling of the training objective by introducing the LoRA branch is consistent with the motivation.\\n-   Extensive empirical evaluation (including the P-test) demonstrates the improvements in Robust Fine-Tuning on various tasks.\", \"Weaknesses\": \"**Weakness:**\\n\\n-   The reason for automating scheduling hyper-parameters is not well illustrated, and the ablation study in Table 5 can not show its superiority, especially for the *RA* metric.\\n-   In Formula 7, the constant factor $6$ is not well-explained, and it could be considered as a hyper-parameter with further ablation study.\\n-   Table 4 is confusing. Specifically, when the adversarial budget is set to $8$ which is the default configuration, the resulting metric is supposed to be that in Table 2. However, this is not true.\\n-   Typo in Section 5.2 Ablation Study, the end of adversarial budgets paragraph says \\\"consistently achieves consistently\\\".\\n-   Diverse network backbone architectures are encouraged to be considered beyond ResNet.\", \"Questions\": \"Refer to the weakness section.\"}"}]}
{"paper_id": "B21c9hT1D7", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: In the field of statistics, robust regression is a classical topic that deals with the estimation of vector parameters in the presence of data contamination. Traditionally, the Huber loss has been used as it provides robustness against outliers while maintaining convexity. However, most studies have been conducted under assumptions of sub-Gaussian features and bounded noise variance, which may not hold in high-dimensional settings with heavy-tailed data.\nGap: Existing work predominantly analyzes robust regression assuming sub-Gaussian features and bounded noise variance, leaving a gap in understanding the behavior of robust regression techniques when covariates and noise exhibit heavy-tailed distributions, which lack finite second moments.\nHypothesis: The authors propose that, in high-dimensional regimes with heavy-tailed data, traditional assumptions about the optimality of regression techniques (like Huber loss) may not hold, and further regularization is required to achieve optimal performance.\nMethod: The paper employs high-dimensional asymptotic analysis via the replica method to characterize the performance of robust M-estimators, particularly focusing on Huber regression, under heavy-tailed covariate and noise distributions. The authors explore various loss functions and covariate distributions to derive theoretical characterizations of error decay rates and optimal estimators. Experiments are conducted to observe the behavior of these estimators under different contamination levels and to compare them against Bayesian optimal performance.\nConclusion: The study finds that in high-dimensional settings, robust regression techniques like the Huber loss, when unregularized, are suboptimal amidst heavy-tailed data, underscoring the necessity of additional regularization. The error decay rates are affected by the presence of heavy tails, and different convergence behaviors are observed depending on the distribution of the covariates and noise. The analyses generalize to a broader family of models and data distributions, hinting at complex interactions in mixture models.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1- The motivation to study the asymptotic performance of robust classifiers in the realistic regime, where the dimension is of the same order as the sample size, is interesting. \\n\\n2- The results and intuitions derived seem interesting and new, and the theoretical and empirical predictions are in agreement with the experiments carried out.\\n\\n3- The generality of the results, which can be applied to broader distributions such as Gaussian mixture model, is also an advantage, as is the independence of the formulae from any function used for robust regression.\", \"Weaknesses\": \"1- The presentation could be improved, particularly the ratings. Given the wide range of notations, a section explaining the notational conventions would have been ideal, as would the rigour of defining all the notations and pointing them out. Although there is a section discussing the results, it would have been interesting to discuss results 2.4 and 2.6 a little better by showing particular cases where the solution is closed (such as ridge regression in a contaminated or uncontaminated case), and to derive the major changes in the theoretical formulae progressively.\\n\\n2- It is a pity that the asymptotic performance depends on the true regression parameter, which limits its practical use (for example on real data) for optimizing the regularization parameters and loss in particular. The authors could have addressed the issue of estimation.\\n\\n3- The results derived by the authors seem non-rigorous, although several attempts in the literature have made them fairly rigorous. It would be interesting to discuss the reasons why it is not possible to derive rigorous results and, above all, to explain why it is not possible to derive rigorous results. It would be interesting to discuss the reasons why it is not possible to derive rigorous results and above all to show to what extent the theoretical contribution of the paper is different and challenging from previous works.\", \"Questions\": \"1- The paper mentions that the contamination model applies to both the covariates and the responses. However, in the definition of the model there is more mention of contamination on the covariates, which obviously indirectly induces contamination on the responses. But written this way, we would expect additional contamination on the regression noise.\\n\\n2- Given the complexity of the expressions, it is important to simplify the notations as much as possible and to avoid defining unnecessary quantities and also to clearly define those which are used. Unless I'm mistaken, this is not always the case. For example in the result 2.4,  $\\\\xi \\\\sim \\\\mathcal{N}(0, I_d)$ and $z$ seem to be defined but never used. Likewise, the integration variables $\\\\eta$ and $\\\\tau$ are not properly defined. The notation $\\\\partial_w$ and $\\\\partial_\\\\mu$ are not clear since we do not know what $w$ and $\\\\mu$ refer to.\\n\\n3- The results depend on the real parameter $\\\\beta_\\\\star$, which limits the application of the bound on real data. Can the authors comment on when to estimate asymptotic performance to carry out, for example, model selection, ...\\n\\n\\n4- Section 4 could have been reorganized differently to provide more detailed explanation and experiments. In substance the result is, with the exception of one or two notations, the same as that of the main result. It is therefore in my opinion unnecessary to present it again. It can be put in an appendix by highlighting in the main paper what are the differences with the main result and deriving the main complementary intuitions.\\n\\n5- The lack of rigor of the results still remains one of the weaknesses but it is very little discussed. What else prevents you from having rigorous results? What are the blocking passages?\\n\\nAfter rebuttal, I read the answer of the authors. The authors promise to enhance the readability of the paper by providing notations section, by illustrating and discussion specific particular cases. I think this will allow to reach a wider audience. But I still think there are more to do in this sense. I will advise authors to progressively help the reader to grasp all the cumbersome notation by explaining before the theorem each main variables and giving intuitive explanation. How they behave in the asymptotic case (p fixed and n large)? What are the effects of the large dimensional corrections, and to particularly discuss notations that are specific to the heavy tailed analysis and that don't appear in the non heavy tailed analysis by raising some limiting cases. I think the problem of estimation of the signal to noise ratio limits the practical application of the paper and even though the authors claim it s a standard problem for regression problem, I think it is important to make the nice theoretical analysis practical even by providing some guidelines from the theory or from the analysis.\"}"}]}
{"paper_id": "4y3GDTFv70", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large language models (LLMs) have shown significant abilities in natural language processing tasks, such as language understanding, few-shot in-context learning, and instruction fine-tuning. These capabilities are often referred to as emergent abilities and are observed as LLMs increase in model size and data. Prior theories have linked these abilities to the properties of data distributions or latent concepts in language.\nGap: Despite observations of emergent abilities in LLMs, there is not a comprehensive theoretical framework explaining why and how these abilities develop, particularly given that LLMs are trained primarily through unsupervised learning to predict next tokens without explicit semantic training.\nHypothesis: The paper hypothesizes that the emergent abilities of LLMs can be attributed to Bayesian inference on sparse joint distributions of languages, distinguishing between unambiguous and \u03b5-ambiguous languages. LLMs effectively perform Bayesian inference, examining sparse structures in joint distributions through their role as universal density approximators of marginal distributions.\nMethod: The study proposes a latent space theory for emergent abilities in LLMs. It defines languages as either unambiguous or \u03b5-ambiguous, based on the ability to infer intentions from messages with varying degrees of precise unambiguity. The authors use a stochastic process for language generation involving the sampling of intentions and conditioning message generation on these intentions. They argue that LLMs trained with maximum likelihood estimation on large data sets approximate the marginal distribution of languages accurately, thereby supporting Bayesian inference for understanding and generating text responses. Simulation experiments using synthetic data (doubly-embedded Markov chains) provide validation for this theoretical framework.\nConclusion: The paper concludes that LLMs' emergent abilities, including language understanding and in-context learning, arise from their implicit ability to perform Bayesian inference on sparse joint distributions of languages, supported by LLMs' capacity to approximate marginal distributions. The paper also suggests a scaling behavior for these abilities, asserting that model size and data breadth enhance the realization of these emergent qualities.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 1, \"Confidence\": 4, \"Strengths\": \"The problem itself is very important.\", \"Weaknesses\": \"I agree that the emergent behaviors of LLMs need a thorough scientific\\ninvestigation, but this paper says almost nothing about these emergent\\nabilities. What is important here is that LLM seem to solve a new task that is\\nnot contained in the training data: that is called emergent. However, just \\nfinding a latent \\\"intention\\\" (quite loosely defined in this paper) cannot\\nexplain this behavior, because the new emergent intention just doesn't exist\\nso far. Therefore, in spite of elementary mathematical discussions in Section\\n4, the paragraph \\\"In other words, ...\\\" virtually adds nothing about the\\nunderstanding of the behaviors of LLMs.\\nConsidering other possible explanations, such as recently developed by Arora\\net al. (2023), the arguments of this paper is too trivial to be useful in\\npractice.\", \"Questions\": \"Nothing.\"}"}]}
{"paper_id": "RmQAKu1wCe", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Spiking Neural Networks (SNNs) are inspired by biological neural mechanisms and are valued for their energy efficiency, particularly when implemented on neuromorphic hardware. Traditional methods for training SNNs often involve surrogate gradients to handle the non-differentiable spiking behavior, which allows them to function similarly to Recurrent Neural Networks (RNNs). However, these methods are typically optimized for a specific time step, necessitating separate and computationally expensive fine-tuning for other time steps.\nGap: Current training methodologies only optimize SNNs for specific time steps, which leads to computational inefficiency and the need for fine-tuning each time a new time step is introduced. This lack of temporal flexibility limits the generalization capabilities of SNNs across different time steps.\nHypothesis: The authors propose that using a mixed time step training (MTT) method will create a temporally flexible SNN (TFSNN) that can generalize across different time steps without requiring fine-tuning. This method could improve performance, efficiency, and adaptability of SNNs.\nMethod: The proposed method involves mixed time step training (MTT), where different time steps are randomly assigned to various stages of SNNs during training. This creates temporal flexibility by training a single TFSNN capable of performing at any time step without additional fine-tuning. The method includes developing inter-stage communication protocols and testing the model on various datasets. The approach was validated experimentally using static datasets like CIFAR10, CIFAR100, and ImageNet as well as event-based datasets such as CIFAR10-DVS and N-Caltech101. The paper also explores TFSNN's compatibility with dynamic inference time steps and its potential for deployment on asynchronous neuromorphic chips.\nConclusion: The TFSNN achieved superior accuracy compared to traditional approaches, such as a 96.84% accuracy on CIFAR10, 81.98% on CIFAR100, and 68.34% on ImageNet with time step T = 6. It showed robust generalization across different time steps and proved more efficient in computation by eliminating fine-tuning. The work demonstrates improved generalization, flexibility, and potential reduction in energy consumption, positioning the TFSNN as a suitable candidate for advanced neuromorphic applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"The MTT performs a wider search along the timestep setting by sampling different. Compared to naive NMT, MTT urges each layer of SNNs to learn feature that is weakly correlated to input length from the previous layer. Such a strong regularization causes an issue in assessing mean and variance within BN layers but is immediately resolved by calibration. Overall, I believe MTT marks a significant step forward in the right direction.\", \"Weaknesses\": \"However, all datasets used in MTT are directly retrieved or derived from static images (CIFAR10-DVS & CIFAR10, N-Caltech101 & Caltech101). I'm worried that these datasets contain essentially time-invariant features along the time dimension. Even for CIFAR10-DVS or N-Caltech101, only simple movement of static images is recorded using the event cameras. The authors should validate their methods on those datasets with affluent temporal dynamics, like from DVS-Gesture to audio such as GSC (Google SpeechCommands) or SHD (Spiking Heidelberg Dataset) to strengthen their results.\\n\\nThe former concern raises the other issue that, mixing different timesteps is partially empirically based on the finding that the temporal gradients resemble each other when timestep stretches or shrinks, and we expect the training will be stable to some extent since gradients are relatively similar. If the input has invariant length and a different amount of temporal information in essence, will the MTT work as it is now?\", \"Questions\": \"Could the authors demonstrate results on datasets with rich temporal information? This could bring some real challenges to the TFSNN.\"}"}]}
{"paper_id": "tVMPfEGT2w", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: In standard reinforcement learning (RL), agents learn to maximize observed numerical reward signals. However, designing appropriate numerical rewards is challenging and can impact the effectiveness of RL algorithms. Preference-based RL (PbRL), where agents rely on human feedback in the form of preferences between trajectories, has emerged as a promising alternative. This area has gained attention in applications like games and robot learning. Offline RL, which relies on pre-collected data without active interaction, is relevant to PbRL due to the high cost and latency associated with obtaining human feedback.\nGap: There are few algorithms specifically designed for offline PbRL with robust statistical guarantees. Existing methods often assume linear models and known transition dynamics, which limits their applicability in scenarios with unknown transitions and general function classes.\nHypothesis: The paper proposes that it is possible to develop an algorithm for offline PbRL with general function approximation and unknown transition dynamics, providing competitive performance guarantees under partial coverage conditions.\nMethod: The proposed algorithm involves two main steps: estimating implicit rewards using Maximum Likelihood Estimation (MLE) with general function approximation from offline data, and solving a distributionally robust planning problem over a confidence set around the MLE. The method introduces a single-policy concentrability coefficient to measure the coverage of a policy and establishes theoretical guarantees for learning target policies. The approach is extended to handle action-based comparisons and unknown transition dynamics.\nConclusion: The paper presents the first provable algorithm for trajectory-wise PbRL with general function approximation under partial coverage. It establishes lower bounds to highlight the distinction between offline PbRL and standard RL with explicit rewards. The algorithm remains effective even with unknown transitions and action-based feedback, maintaining strong guarantees.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 4, \"Rating\": 8, \"Confidence\": 3, \"Strengths\": \"The paper is clearly written.\\nThe paper provides several strong and novel theoretically results on preference-based RL in offline setting. Some of these results also generalize the results of Zhu et al for linear function approximators to general function approximators.\", \"Weaknesses\": \"Although the main contribution is theoretical, it would be nice to see an empirical case study or two  where the given algorithm works as expected.\", \"Questions\": \"No questions.\"}"}]}
{"paper_id": "aiPcdCFmYy", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Distributional reinforcement learning (RL) enhances the classical RL approach by estimating the full distribution of returns, rather than just expectations. This approach has led to improvements in a wide range of applications, including risk-sensitive control and policy exploration. Key methods in distributional RL have employed techniques like quantile regression and maximum mean discrepancy (MMD) to represent and compare return distributions, each with their own strengths and computational challenges.\nGap: Current distributional RL methods often face challenges with the representation of return distributions and the computational inefficiencies of existing divergence measures. Specifically, quantile regression can struggle with high-dimensionality and non-monotonicity, while MMD-based approaches may overlook finer distribution characteristics.\nHypothesis: The authors propose that utilizing Sinkhorn divergence, which interpolates between Wasserstein distance and MMD, provides a computationally efficient and geometrically aware method for measuring discrepancies between return distributions in RL. This approach is expected to offer superior performance over existing techniques by capturing richer data geometry and enabling flexible sample-based distribution representation.\nMethod: The paper introduces the Sinkhorn distributional RL (SinkhornDRL) algorithm, which represents return distributions through unrestricted samples and minimizes their discrepancy using Sinkhorn divergence. SinkhornDRL is theoretically grounded with established convergence properties in tabular cases and offers a new form of regularized MMD. Experimentally, the algorithm is tested across 55 Atari games, showing its empirical effectiveness compared to existing distributional RL methods. The authors also conduct a sensitivity analysis to evaluate the impact of different parameters on the performance of SinkhornDRL.\nConclusion: SinkhornDRL consistently achieves competitive performance compared to state-of-the-art distributional RL methods. Theoretical and empirical results suggest that Sinkhorn divergence effectively balances computational efficiency with the ability to capture complex data geometry. This work not only contributes a novel algorithm to the distributional RL field but also enhances understanding of the relationship between different divergence measures, paving the way for future RL advancements.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"The propsed sinkhorn divergence based Bellman update loss includes the regularization loss which is the mutual information of a given distribution and the target distribution.\\n\\nThis makes the above two distributions statistically independent, and the reguarlizaiton loss with the coefficeint $\\\\epsilon$ helps to increase the interquantile mean performance of atari-55.\", \"Weaknesses\": \"The main theoretical contribution can be seen as a simple combination of the architecture of MMD DRL and Sinkorn divergence. The additional KL divergence term (mutual information) in the proposed loss is a key factor to improve the DRL algorithms except for two corner cases $\\\\epsilon=0,\\\\infty$. However, the role of this proposed element is not well explained in the main text. The detailed comments are as follows. The main strength statement in this paper is that Sinkhorn divergence can be useful in the complex environments with high dimensional action space. This discussion is not rigorous, because I cannot find any explanation why the mutual information between two return distribution and the dimension of action space is related.\\n\\nI also have the following minor concerns.\\n\\n1. It is hard to get an insight by introducing the Sinkhorn divergence. For example, the paper of MMD DRL provides the figure that explains the propsed scheme can estimate the high order moments better than the existing methods.\\n2. The main ablation study (sensitivity anlysis) is provided without any detailed discussion. The authors states taht the property of loss changes as the value of $\\\\epsilon$ changes in the main theorem, but the empricial analysis and discussion is not provided well. \\n3. The experiment protocol in this paper is not standard. Previous works almost conducted in 200M iterations or the authors could have chosen the protocol of the paper, DRL at the edge of statistical precipice to validate the performance.\\n4. In table 2 and Figure 2, the propsed method outperforms in terms of mean score but not in median score than baselines, and the proposed method show remarkable performance in venture and sequest. This means that the proposed method is not a generally better algorithm but a specialist for Venture and Sequest. In my point of view, the authors should have stated why the propsee method is better in such envs.\\n5. There exists more projection method for distributional RL such as EDRL [A], but the provided baselines is not enough.\\n\\n\\n\\n[A] Rowland, Mark, et al. \\\"Statistics and samples in distributional reinforcement learning.\\\"\u00a0International Conference on Machine Learning. PMLR, 2019.\", \"Questions\": \"I mentioned my main concern in the weakness part. Please refer the weakness section.\\n\\nIn addition, it is hard to understand that reducing the mutual information helps to build a better projection operator.\\nCan the authors provide detailed explanation why the mutual information between the return distribution and target distribution is important?\\nIn my point of view, the unresticted statistics can be constructed better, because its deterministic samples can be more uncorrelated by reducing the mutual information.\\nIf it is true, the main effect seems related to the sampling scheme, not the discrepency between two ground-truth return distribution.\"}"}]}
{"paper_id": "cHy00K3Och", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Modern datasets and deep learning models are growing in size and complexity, which results in extensive use of computational resources and increasing training time and effort. Additionally, this rise contributes to a larger carbon footprint for training models. There is a need for methods to reduce these computational requirements while maintaining model performance.\nGap: The main gap addressed in the paper is the need for a simple and effective method to select representative subsets of data (coresets) that can reduce the computational requirements for training deep learning models, particularly during the early stages, without compromising the model's generalization performance.\nHypothesis: The authors propose that a coreset can be effectively identified using the similarity of loss gradients. By focusing on samples whose gradients are similar to others, these samples can approximate the training set's gradients and thus represent the dataset adequately. This method would provide an efficient and effective means of coreset selection that could outperform existing methods.\nMethod: The proposed method, GradSimCore, identifies representative samples through gradient similarity. The method evaluates the gradients produced by data samples and selects those with the strongest gradient similarity to others as part of the coreset. Experiments were conducted using popular datasets, including CIFAR10, CIFAR100, and ImageNet-1K, with various CNN architectures to demonstrate efficacy. The approach employs several computational efficiency techniques, such as focusing on the last layer's gradients and using a radius-based nearest neighbor search.\nConclusion: GradSimCore is a novel, intuitive coreset selection method that achieves state-of-the-art accuracy on several benchmark datasets while being efficient in computational time. It provides a simple yet effective baseline for future coreset selection methods with a promise of reduced computational costs and carbon footprint in training deep learning models.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 1, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"This article is that it provides links to the code\", \"Weaknesses\": \"See the summary.\", \"Questions\": \"No question! I suggest the author should not resubmit a manuscript like this again.\"}"}]}
{"paper_id": "2J25Vi9W8I", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Ordinal regression, also known as ordinal classification, is concerned with predicting labels with a natural order, such as age estimation and historical image dating. It is a problem of growing interest and has been approached using regression-based, classification-based, and ranking-based methods, each with its strengths and limitations. Pre-trained vision-language models (VLMs), like CLIP, have shown improvement in many vision tasks, but their applicability to ordinal regression tasks remains inadequately explored.\nGap: The paper identifies insufficient exploration of vision-language models, specifically CLIP, in the field of ordinal regression. Existing techniques often suffer from limited performance due to insufficient numerical captions and ineffective training objectives when applying CLIP directly to these tasks.\nHypothesis: The authors propose that by integrating a coarse-to-fine learning paradigm with language prior information, CLIP can be effectively adapted for ordinal regression tasks ensuring both semantic and ordinal alignment, thus significantly enhancing the performance over existing methods.\nMethod: The RegCLIP framework involves a two-level coarse-to-fine learning approach. Initially, a stagewise process is utilized, where intermediate classification is performed followed by refined predictions. Coarse semantic labels generated by large language models serve as these intermediate classes. This approach is further refined using a novel fine-grained cross-modal ranking-based loss that updates these labels to ensure semantic and ordinal alignment. The method is validated against three tasks: historical image dating, image aesthetics assessment, and age estimation, showing significant performance improvements over state-of-the-art baseline methods.\nConclusion: RegCLIP introduces a novel framework leveraging coarse-to-fine methods and language models for efficient ordinal regression. The framework outperforms existing methods significantly in accuracy and mean absolute error (MAE) in various tasks under different settings, demonstrating its effectiveness and potential impact in enhancing CLIP's performance in ordinal regression domains.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 5, \"Strengths\": \"- The paper is easy to follow and well organized.\\n- The proposed algorithm is simple but effective way to utilize CLIP for ordinal regression tasks.\\n- The proposed algorithm shows good results on various datasets.\", \"Weaknesses\": \"1. Even though the authors mentioned that they will release the codes, I think there are some missing details which are important for reproducibility and further researches.\\n\\n     - Section 3.5: Exactly, what is the revised FCRC loss? \\n     - How the fine-grained labels are predicted?\\n     - What is batch size M?\\n     - In Eq(2), what is the default value for $\\\\beta$?\\n     - In Eq(2), how the scaled ranking differences are normalized?\\n     - What are language templates for historical image classification and aesthetic quality assessments?\\n\\n\\n2. It would be good to have comparison of feature space visualizations for ablated methods. I think it can provide insights and intuitive understanding of each design choice for readers.  \\n\\n\\n3. MORPH II dataset has many different evaluation protocols. However, the results for only one setting are provided in the paper. So, it may be more useful to have more comparisons on MORPH II dataset. \\n\\n\\n4. There are some other popular datasets for age estimation (e.g. CACD, UTK, CLAP2015, Adience, \u2026). So, it would be better if the results on those datasets are compared in the paper.\", \"Questions\": \"The proposed algorithm is simple but makes sense for me. However, it omits some important details in the paper and does not contain many analysis of the proposed algorithm, as I mentioned in the weakness section. (Please see the weakness section for other concerns.) Nevertheless, in overall, I'm positive about the acceptance of this paper, because it outperforms all conventional methods with meaningful performance gaps and also authors described that the source codes will be released for algorithm details.\"}"}]}
{"paper_id": "3K3aWRpRNq", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The field of 3D Structure-Based Drug Design (SBDD) utilizes the three-dimensional structures of target protein pockets to generate molecular geometric graphs. Geometric Diffusion Models (GDM) are considered state-of-the-art in this domain due to their ability to generate geometric graphs effectively. Recent advancements in deep learning have spurred the use of generative models that integrate protein structure information for informed molecular generation, which can accelerate drug discovery.\nGap: While GDMs are powerful in generating molecular structures, they often neglect crucial inter-molecular interactions like Van der Waals forces and hydrogen bonding due to their data-driven nature. This oversight can lead to atomic clashes where generated molecules are too close to protein structures, violating physical constraints and potentially resulting in chemically invalid products.\nHypothesis: The authors propose that by introducing a novel constrained sampling process that integrates non-convex constraints into the Langevin Dynamics of GDMs, they can prevent atomic clashes during molecular generation without altering the training process of the GDMs.\nMethod: The method involves incorporating a non-convex constraint within the Langevin Dynamics of GDM using proximal regularization techniques. This is done by defining a constraint function that prevents molecular atoms from approaching too closely to the protein surface (less than 2 \u00c5), which is then incorporated into the optimization process as a proximal regularization problem. The sampling process is modified using a Proximal Inexact Langevin Algorithm (PILA) to ensure molecular coordinates adhere to these constraints. The effectiveness of the method was empirically validated using datasets like CrossDock2020, with a focus on reducing atomic clash ratios without training modifications.\nConclusion: The proposed constrained sampling method significantly reduces atomic clashes in generated molecular structures compared to the original GDM processes. This approach enhances the stability of the drug design process while preserving the original generative model's performance, effectively addressing the shortcomings of existing diffusion models in generating clash-free molecular structures.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The problem formulation and presentation are clear. \\n2. The method is rationale and theoretical analysis is provided.\\n3. The proposed method improves the connectivity, mean CR, and stability of generated ligand molecules.\", \"Weaknesses\": \"1. This method is not novel to some extent, which is some kind of similar to the validity guidance proposed in DecompDiff.  \\n2. The experimental results on molecular properties of generated ligands are not satisfying. The improvement on Vina score is only marginal. It is notable that vina score is one of the most metrics for structure-based drug design and strongly related to intermolecular spatial interaction between pockets and generated ligands.\", \"Questions\": \"Can the author provide more comparison between the validity guidance proposed in DecompDiff and proximal sampling proposed in this paper? Can the vina score be further improved?\"}"}]}
{"paper_id": "EEqEyyYUwc", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Diffusion models are powerful generative models that can convert noise into complex data distributions. Conditional generation using diffusion models involves using additional inputs like class labels or text to guide the model. Traditional approaches include Classifier Guidance, which uses pretrained classifiers for guidance, and Classifier-Free Guidance, which introduces the condition during training. Both methods rely on a weighting parameter to balance fidelity and condition adherence.\nGap: Existing methods use a constant weighting parameter during diffusion guidance, which can result in conflicts between image quality and adherence to conditions, especially in early stages with high noise. There is a lack of systematic investigation into how guidance behavior varies across different timesteps in diffusion processes.\nHypothesis: The paper hypothesizes that conflicts in generation quality and condition adherence arise due to conflicting gradients from classifiers at different timesteps, especially early on. A timestep-dependent weighting scheme can improve the balance between fidelity, diversity, and condition adherence in diffusion guidance.\nMethod: The authors propose Progressive-Guidance, a timestep-dependent weighting scheme for guidance in diffusion models. They introduce two approaches: a heuristic-based method using linear and cosine functions, and a gradient-norm-based method that adjusts weighting based on the condition's gradient norm with respect to the timestep. These methods can be implemented without retraining models and are tested across class-conditioned image, text-to-image, and text-to-motion generation tasks.\nConclusion: The proposed Progressive-Guidance scheme improves the performance of diffusion models on several benchmark tasks by achieving better trade-offs between fidelity and condition adherence. The methods outperform static guidance in generating more diverse and higher quality outputs. The approach requires minimal implementation effort and no retraining, making it applicable to various diffusion model applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"1.\\tThe empirical analysis in section 3 is interesting and reasonable.\\n2.\\tThe motivation of this paper is clear.\", \"Weaknesses\": \"1.\\tThe main idea of the proposed methods is to attenuate the influence of guidance in the initial stages. However, based on my experiences, with small guidance weight in the initial stages, the structural integrity and layout of the generated images by diffusion models would be disrupted, particularly for some complex structures. The authors should provide more evaluations of the samples with complex structures, such as instances with human bodies or longer prompts.  Actually, I believe that the trade-off between sample fidelity and conditional adherence still exists, even with the proposed methods.\\n2.\\tI have read the appendix carefully. It is weird to find that the proposed methods cannot outperform the baselines in terms of CLIP-Score with a small guidance weight w.  If I focus on the experimental results with w<10, which is actually used in practice, what are the corresponding trade-off curves in Figure 5?  \\n3.\\tThe paper does not conduct extensive experiments on different model architectures, and samplers to validate the robustness and generality of their method. \\na)\\tModels: some more powerful diffusion models should be involved for comparisons, e.g., SD XL[1], and DeepFloyd IF[2], which can both be accessed publicly. Based on my understanding, those models can provide a more accurate estimation for classifications at the initial stage of the generation. As a result, the performance improvement of the proposed methods may decrease in those models. \\nb)\\tSamplers: DDIM sampler is the main selection for this paper to sample images during generation. More samplers, such as DPMSlover[3], should also be considered.  \\nc)\\tHuman-level metric: It is well-known that some quantitative metrics like FID, and IS, may be problematic in some cases. Human evaluation should be involved to provide more clear comparisons. \\n[1] https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0\\n[2] https://huggingface.co/DeepFloyd/IF-I-XL-v1.0\\n[3] Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps\", \"Questions\": \"1.\\tWhat are the corresponding trade-off curves in Figure 5 if we focus on the cases with guidance weight w<10.0?\\n2.\\tProvide more experimental analysis mentioned above to validate the proposed methods.\"}"}]}
{"paper_id": "GtnNhtuVrc", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Supervised learning is essential in modern computer vision but requires extensive labeled data, which is costly. Semi-supervised learning (SSL) combines labeled and unlabeled data to address this challenge, particularly useful in tasks like semantic segmentation where manual labeling is expensive. Pseudo-labeling is a popular SSL approach where labels are dynamically assigned based on model predictions but suffers from confirmation bias and erroneous predictions.\nGap: Current pseudo-labeling methods suffer from confirmation bias and rely heavily on filtering low-confidence predictions, which can lead to overfitting and inefficient learning when labeled data is sparse.\nHypothesis: Incorporating spatial contextual information in confidence refinement can improve pseudo-labeling in semi-supervised semantic segmentation. By considering neighboring pixels collectively, it is possible to increase the availability of pseudo-labels without a loss in accuracy.\nMethod: The authors propose S4MC, a novel confidence refinement scheme. It employs a teacher-student paradigm with event-union probability for grouping neighboring pixels and evaluating their pseudo-labels collectively. The method includes pseudo-label refinement, dynamic partition adjustment for thresholding, and considers spatial context to refine pseudo-label creation. It uses a teacher-student setup where the teacher model is updated via an exponential moving average of student weights. The approach was experimentally validated on datasets like PASCAL VOC 12, Cityscapes, and MS COCO, where it showed improvements in mIoU over existing methods.\nConclusion: S4MC leverages marginal contextual information to enhance pseudo-labels in semi-supervised semantic segmentation, outperforming existing methods in benchmarks and increasing data usage while maintaining label quality. The method shows potential for reducing annotation costs but may not generalize to non-spatially coherent tasks without modification.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"Originality: The approach offers some degree of novelty - filtering low-confidence predictions by using the context around the pixel, rather than the pixel in isolation (current sota approaches). The contribution is relevant to an actual problem, it increases the use of unlabeled data.\\nQuality: The method is sound and thoroughly explained. Experiments prove the effectiveness of the approach when applied on top of state-of-the-art methods with a negligible added computational cost. \\nClarity: The paper is an interesting read, well-structured, well-detailed, and very easy to understand (fairly enjoyed reading it). \\nSignificance: The results offer marginal improvements only in some scenarios, compared to state-of-the-art methods.\", \"Weaknesses\": \"* I would suggest changing the main figure of the paper (the elements within the figure are way too small and hard to follow).\\n* The contribution is not groundbreaking. The +1.29 mIoU gain on PASCAL VOC 2012 and +1.01 mIoU improvement on Cityscapes declared at the beginning of the paper are not backed up by the numbers in the table (Table 1 and Table 3). The numbers in the tables show that the method is not robust enough to offer a consistent improvement in all tested scenarios.  \\n* The biggest weakness of the paper is Section 4.3 (the ablation studies) and Tables 4 & 5. The text states that the experiments were conducted using the CutMix-Seg framework, but I could not find the numbers in the previous tables. Also in Table 5, the caption states that the numbers are for FixMatch. The text, the numbers, and the tables do not correspond, this part needs further clarification (or another check) because it confuses me the most. \\n* Low range of datasets, more experiments that include more varied and challenging scenarios to better understand the method's limitations.\", \"Questions\": \"* There are no insights as to why the best window for the used contextual information is in a 3x3 range - this actually suggests that the context is not used properly, or what is actually causing this degradation in performance when more neighboring pixels are used?\"}"}]}
{"paper_id": "di52zR8xgf", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Generative models have witnessed significant advancements across multiple data domains, especially text-to-image synthesis with the development of models like Stable Diffusion, which has been foundational for applications like image personalization, controllable editing, and others. However, these developments are often outpaced by black-box models whose opacity raises challenges for reproducibility and validation.\nGap: Despite the widespread adoption of models like Stable Diffusion, they are often not competitive with state-of-the-art but opaque black-box models. There is a need for an open model that can offer competitive performance with these state-of-the-art models, providing a transparent alternative for the community.\nHypothesis: Enhancing the Stable Diffusion model with a larger architecture, novel conditioning schemes, and a refinement stage can significantly improve its performance, making it competitive with state-of-the-art black-box models while maintaining openness and transparency.\nMethod: The paper introduces Stable Diffusion XL (SDXL), which includes a three times larger UNet backbone than previous versions, enhancements in the form of multiple attention blocks, and a second text encoder. Novel conditioning schemes, such as size and crop micro-conditioning, are introduced to improve training on varying aspect ratios. A separate refinement model is used for improving the visual fidelity post-processing of generated images while maintaining cohesion with the original model\u2019s architecture. SDXL was trained on a diverse dataset including multiple aspect ratios with an advanced autoencoder to enhance semantic composition and high-frequency details.\nConclusion: SDXL significantly outperforms previous versions of Stable Diffusion and is competitive with state-of-the-art black-box models like Midjourney. It offers an open model alternative, enhancing reproducibility and transparency. The model shows improvements in image quality and adherence to text prompts, although it still suggests future work in areas like single-stage generation and incorporation of additional text synthesis improvements.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 3, \"Strengths\": \"1. Very sound elaboration and experiments. The proposed improvement is reasonable and effective.\\n2. The method is open sourced.\", \"Weaknesses\": \"1. Lacks some comparison with SoTA pixel-space diffusion model such as Simple Diffusion from Google.\\n2.  Also, SDXL still relied on two-stage training where a high-quality encoder is required. I wonder whether the two stage can be combined and lead to further performance boost?\", \"Questions\": \"1. As discussed in your future work section, can SD be combined into single-stage training and further boost the performance?\\n2. Can similar training pipeline introduced in this paper be applied to 3D diffusion training, and what are the challenges?\"}"}]}
{"paper_id": "ZKnbIZefER", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Availability attacks aim to prevent unauthorized use of private data and commercial datasets by adding imperceptible noise to make the data unlearnable, thus hindering the training of usable models. Contrastive learning, which has gained performance comparable to supervised learning, can bypass these attacks when traditional supervised learning fails.\nGap: Existing availability attacks fail to achieve contrastive unlearnability, allowing malicious data collectors to use contrastive learning to effectively leverage poisoned data, undermining data protection.\nHypothesis: Employing stronger data augmentations in supervised poisoning generation can create shortcuts for contrastive learning, improving the worst-case unlearnability across multiple supervised and contrastive learning algorithms.\nMethod: The authors propose two attacks: Augmented Unlearnable Example (AUE) and Augmented Adversarial Poisoning (AAP). These attacks use enhanced data augmentations during the noise generation process to mimic the behavior of contrastive learning models. This involves optimizing a supervised loss under stronger augmentations to indirectly minimize the contrastive loss, thus creating more effective noise that undermines both supervised and contrastive learning.\nConclusion: The proposed AUE and AAP attacks improve the worst-case unlearnability across multiple supervised and contrastive algorithms compared to existing methods. These attacks show increased effectiveness in diminishing the impact of contrastive learning on protected data, verified through experiments on datasets like CIFAR-10 and ImageNet-100. The approach highlights the necessity of considering both supervised and contrastive pathways in designing data poisoning attacks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"- The authors quantitatively measure the GAP between the attack effectiveness of supervised learning and contrastive learning methods using two contrastive learning metrics.\\n- By incorporating the data augmentation techniques of contrastive learning into supervised learning, the authors develop an availability attack method that is effective under both supervised learning and contrastive learning training frameworks.\", \"Weaknesses\": \"- The paper is difficult to read and the organization of the content is not very clear.\\n- The Cross-Entropy (CE) loss and InfoNCE loss may be essentially similar, and using these two losses to reflect the relationship between the two tasks is not particularly convincing.\\n- Although the paper emphasizes sampling in data augmentation as being introduced from contrastive learning, it bears a resemblance to the Expectation Over Transformation (EOT) used in the reference paper on REM. EOT also involves sampling from data augmentation, and this technique is commonly used in adversarial settings.\\n- The assumptions made in the theoretical analysis employ a simple linear network, which presents a significant discrepancy from practical settings.\\n- The resolution of the Tiny-ImageNet dataset is not down-sampled, and the Mini-ImageNet dataset has a limited number of samples per class.\", \"Questions\": \"- Does the value of the GAP affect accuracy? According to the paper, GAP reflects the difference between clean and poison distributions. Logically, the greater the difference, the better the protection effect should be. However, the results in Table 1 do not seem to support this notion.\\n- Why does the loss of Alignment in Figure 3(c) first decrease and then increase?\"}"}]}
{"paper_id": "sDmjlpphdB", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large Language Models (LLMs) have demonstrated a strong capacity to generalize across novel tasks when guided by effective language prompts and in-context learning demonstrations. Prompts, which comprise instructions for tasks, are critical to the models' ability to perform complex tasks, as evidenced by earlier methods that tried to automate prompt design.\nGap: Previous methods in prompt optimization constrained the search to demo-free instructions, limiting their ability. A single instruction may not suitably cover the diverse problem space of complex tasks, given that it lacks the context provided by demonstrations.\nHypothesis: The paper proposes that by adopting a Mixture-of-Experts approach, where the problem space for prompts is divided into homogeneous regions with specialized experts, and by extending prompts to include both instructions and demos, the performance of automatic prompting can be significantly enhanced.\nMethod: The paper introduces Mixture-of-Prompts (MoP), which divides the problem space into regions managed by different experts\u2014each with a customized prompt containing specific instructions and demos. The methodology uses a two-phase process: firstly, demos are assigned to clusters based on semantic similarity using kernel regression principles; secondly, instructions are allocated through a region-based joint search to complement these demo clusters. Experiments were conducted on benchmark NLP tasks using datasets formatted for these tasks to evaluate MoP's performance over competing techniques.\nConclusion: The MoP framework outperformed previous prompt optimization methods on benchmark NLP tasks, with improvements up to 43%. The framework successfully showed that splitting tasks into smaller, similar problem spaces dedicated to specialized experts with tailored prompts can significantly improve task performance.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"S1: I like their presentation and language writing, which is clear.\\nS2: they proposed two-step search algorithm, which leverages semantical similar- ity for demo assignment and routing function and region-based joint search for instruction assignment, achieves significant performance gains on the APE Benchmark.\", \"Weaknesses\": \"W1: The idea of grouping training demos into homogeneous clusters, with each cluster corresponding to a specific expert, is disputable. This method actually assumes that all the potential demos or test queries can be all decomposed by these groups. This causes several problems: (1) how to make sure the given training demos are representative enough to cover all the cases? (2) how to make sure your clustering method is effective in making each cluster homogeneous? (3) how to make sure your generated groups are qualified to support better performance? (4) If each group corresponds to one expert, how to set an optimal number of experts for different clustering methods? How to make sure these experts are representative enough, how to achieve the balance between expert number and efficiency?   \\n\\nHere is a very simple solution to all the problems raised by W1: ignore/remove the clustering component, and treat all the demos as normal instances that can be represented by a K-dimensional latent space where each dimension corresponds to one expert. I would like to see the feedback from the authors.\\n\\n\\nW2: can I assume that the proposed INSTRUCTION ASSIGNMENT is just the combination of Independent Search and Joint Search? I thank the authors for their insights but it is too straightforward, and there exist more important problems that should be considered. For example, since each expert corresponds to one sub-instruction, there is a pressing need to make sure these instructions are heterogeneous. Achieving this goal is not easy, for example, maybe we should maximize KL divergence among pairs of sub-instructions. I didn\u2019t find such effort in this paper.\\n\\nW3: Overall, I thank the authors for their hard work in solving an interesting problem. But I am not surprised about their novelties.\", \"Questions\": \"Q1: In Equation (3), if we let $C=1$, can I treat this equation as this: we use one region with the entire training demos to search a single instruction? So the insight of your Mixture-of-Expert for prompt optimization is: you think searching one single instruction is hard (no matter whether we have training demos), and you reduce the difficulties by splitting this problem into several sub-problems (searching the combination of multiple sub-instructions with demos)\\n\\nIf my understanding is correct, I think the following sentence in your paper is over-claimed: \\\"a limitation of existing auto-prompting methods is that they solely focus on searching for an optimal demo-free instruction\\\" (below equation 2). It seems that the above sentence should be replaced with something like: \\\"a limitation of existing auto-prompting methods is that searching an optimal instruction is usually very hard\\\"\\n\\nQ2: W1\\n\\nQ3: W3\"}"}]}
{"paper_id": "28L2FCtMWq", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Recent advancements in video editing have achieved success in single-attribute editing and style transfer using text-to-video (T2V) models. These models either require training on text-video datasets or use training-free methods. However, complex multi-attribute editing remains challenging, often resulting in issues like omitted edits, incorrect modifications, and poor consistency.\nGap: Current video editing techniques struggle with multi-attribute editing, often failing to make all intended changes, applying edits to the wrong elements, or altering regions that should remain unchanged. This indicates a lack of precision and consistency in handling multiple attributes simultaneously.\nHypothesis: The authors propose that integrating grounding information through a novel framework can achieve temporally consistent multi-attribute video editing in a training-free manner, thus overcoming the shortcomings of existing methods.\nMethod: The proposed method, Ground-A-Video, employs cross-frame gated attention combined with modulated cross-attention and optical flow guided inverted latents smoothing. This approach incorporates grounding information\u2014bounding box coordinates and textual descriptions\u2014into the latent representations to ensure precise and temporally consistent edits across video frames without additional training. The framework utilizes Stable Diffusion and ControlNet for structure preservation and employs DDIM inversion for video frames. Groundings are used to guide spatially-continuous and discrete editing, with experiments demonstrating superior performance in edit-accuracy and frame consistency compared to baseline methods.\nConclusion: Ground-A-Video effectively addresses the limitations of current video editing techniques, achieving superior accuracy and temporal consistency in multi-attribute video editing scenarios through a zero-shot approach. The method demonstrates improved performance over existing solutions by incorporating grounding information to guide edits in a structured and consistent manner.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"S1. This paper presents the \ufb01rst training-free grounding-driven video editing framework, which is relatively innovative.\\n\\nS2. The proposed method and experimental results are consistent with their motivation and e\ufb00ectively solve the problem of multi-attribute video editing in complex scenes.\\n\\nS3. The method is clearly stated, and the details are comprehensive.\", \"Weaknesses\": \"W1. Since the introduced depth map and optical \ufb02ow map both re\ufb02ect pixel-level structural information, will this cause the structure before and after editing to be too consistent, so that if the foreground is replaced by an object with inconsistent structure, the editing result will be poor and lack of \ufb02exibility? e.g. replace the \u201crabbit\u201d in the phrase \\\"A rabbit is eating a watermelon on the table\\\" with an animal without long ears.\\n\\nW2. For similar reasons, this may also limit the editing method to the task of adding or deleting objects.\", \"Questions\": \"See weaknesses above.\"}"}]}
{"paper_id": "CSpWgKo0ID", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large Language Models (LLMs) have emerged as powerful tools in text generation, demonstrating the capability to perform tasks that correlate with human cognitive skills, like reasoning and analogical thinking. As LLMs become more integrated into society and applications, understanding their behavior in interactive social contexts, particularly in terms of cooperation and coordination, is of significant importance. Traditional and behavioral game theory provides a structured method to explore these interactions, focusing on strategies beyond purely rational choices.\nGap: There is a limited understanding of how LLMs behave in social interactive settings, particularly when it involves repeated interactions with other agents. Previous studies have not thoroughly evaluated LLMs in repeated game settings to determine their strategic behavior in terms of cooperation and coordination, which is necessary to gauge their potential efficacy and trustworthiness in future applications.\nHypothesis: The study posits that behavioral game theory, when applied to LLMs, can uncover predictable behavioral patterns, especially concerning cooperation and coordination strategies. It is hypothesized that LLMs can perform optimally in self-interest-oriented games but might struggle in coordination games, revealing consistency or flaws in their interactive strategies.\nMethod: The researchers conducted experiments using LLMs, including GPT-4, in finitely repeated two-player, two-strategy games from different game families like the iterated Prisoner's Dilemma and the Battle of the Sexes. They used behavioral prompts to simulate interactions between LLMs and human-like strategies. Through a series of 144 different 2x2 game scenarios, they observed LLM performances which involved evaluating interactions across 10 rounds. Additionally, robustness checks were conducted by varying prompts and payoff matrices, and also using prompting techniques to improve LLM behavior.\nConclusion: The study finds that LLMs, such as GPT-4, generally excel in games that prioritize self-interest but perform less optimally in coordination-requiring games. GPT-4 is characterized by unforgiving behavior in competitive settings, like the Prisoner's Dilemma, and poor coordination in games like the Battle of the Sexes. Improving GPT-4's understanding and behavior through predicting others' actions and providing context about player mistakes showed potential for enhancing its coordination capabilities. This research lays groundwork for developing a behavioral game theory applicable to LLMs and highlights challenges in aligning LLM decisions closer to human social interaction ideals.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"The problem setup are well-motivated: using game to simulate LLMs behaviors and as a tool to assess their intelligence level is definitely a very promising direction. The presentation is clear.\", \"Weaknesses\": \"The way that it construct a strategy from an LLM is questionable. The conclusion of the evaluation does not provide too much insights to me. Please see the questions section below.\", \"Questions\": \"While I like the idea of using games to assess and evaluate the intelligence of LLMs, I am concerned about the following points.\\n\\n1. The way that you construct a strategy from LLMs. Equation (2) is not entirely clear to me. If I understand correctly, this approach always determinstically pick the action with maximum probability output by an LLM? What would be its difference between just sample according to the raw probability? There also could be a variety of other ways to construct a strategy. Actually the results in your coordination game study suggest one: first predict the opponent's next action, and then play a best response against that. This type of LLM strategy has been found impressive in repeated rock-paper-scissors [1]. However, no justification for your approach and comparison can be found in the paper.\\n\\n2. The evaluation methodologies in the experimental section. These include:\\n\\n      i. The scores in Table 1. I am not entirely following the definition \\\"if both players had played such that the player we are analyzing would have gained the maximum possible outcomes on every round\\\". Does it mean the maximum payoff of the current player in the game matrix? If it is then I think it is a really bad metric as it optimistically assumes the opponent always help you, which almost never happen. A more reasonable metric could be regret-based notions. For example, the maximum deviation you can gain assuming your opponent's strategy is fixed.\\n\\n     ii Also, I am not sure how is the scores against different types opponents are aggregated into one score for each entry of Table 1. Is it an average over the scores against all opponents? If it is, then probably it also need justification.\\n\\n    iii For the results in the prisoner's dilliemma section. First I think an important baseline strategy, tit-for-tat is missing. And according to the head-to-head payoff matrix, looks like GPT-4, text-davinci-003, claude and defect are all pretty strong? Which one is the more intelligent one according to this result? Also is the right-hand figure of Figure 3 depict only one trajectory? How many trajectories do you run for each pair of strategies?\\n\\n    iv. In your opinions, why different LLMs behaviors qualitively different?\\n\\n[1] \\\"Population-based Evaluation in Repeated Rock-Paper-Scissors as a Benchmark for Multiagent Reinforcement Learning\\\", Lanctot et. al. TMLR\"}"}]}
{"paper_id": "Aq35gl2c1k", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Critical learning periods are well-established phenomena in both biological systems, where temporary sensory deficits early in development can permanently affect behavior, and in artificial networks. Research has shown that these periods are tied to neural plasticity in biology. In artificial deep neural networks, analogous phenomena have been observed, raising questions about whether critical periods are inherent to the learning process of any flexible learning system, rather than being purely biologically driven.\nGap: While critical periods have been observed in artificial systems, the underlying causes are not well understood, particularly whether these periods depend on specific architectural or optimization details. There is a need to isolate the fundamental factors that cause critical periods to emerge in deep networks to better understand their generality across biological and artificial systems.\nHypothesis: The authors propose that critical learning periods are a fundamental feature of deep learning systems, driven by the depth of networks and data distribution structure rather than specific architectural or biochemical components. They suggest that these properties promote competitive dynamics between pathways, leading to critical periods.\nMethod: The study uses deep linear networks which lack the complexity of nonlinear activations to analytically explore the emergence of critical learning periods. The approach is two-pronged: (1) investigating the role of competition in a multi-pathway linear network model and (2) analyzing generalization in matrix completion tasks. Simulations and differential equation models reveal how depth and initial learning phases influence feature learning and generalization in networks. Experiments include analyses of singular value trajectories under various scenarios, such as temporary deprivations or pre-training relays on different tasks.\nConclusion: The analysis shows that critical periods arise from fundamental aspects such as network depth and the underlying data structures. These factors independently drive competitive learning dynamics and affect generalization for transfer learning. The simplicity of the linear models demonstrates that these phenomena are not reliant on complex neural architectures, supporting the idea that critical periods are inherent to deep learning processes, providing insights into both artificial and biological learning systems.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 10, \"Confidence\": 4, \"Strengths\": \"Well written, interesting avenue of research. Results well explained early in the text, with clear examples and clear explanations of experimental settings and technical details. Particularly nice clarity given the cross-disciplinary nature of the work.\\nRelated work is well written, reasonably thorough (to my knowledge), and well-explained.\\nNice phase portraits.\\nCaptions are mostly well-explained and self contained! (I don't know why this is so rare, but good job)\", \"Weaknesses\": \"It's a bit of a tradeoff with having a clear hypothesis, but I find the case oversimplified/misframed\u00a0 in the beginning and conclusion. Sure the literal biochemical explanation doesn't hold in an artificial setting, but analogies of this (reduced plasticity) could well occur. I don't think these are 'competing' hypotheses as they are framed; plasticity, imperfect optimization, etc. are *mechanisms* by which critical periods could arise in both artificial and biological systems; it doesn't tell us the reason\u00a0\u00a0(inherent to learning, quirks of substrate, something else, etc.). In the conclusion \\\"natural information processing constraints\\\" are contrasted with biochemical processes like plasticity as an alternative hypothesis, but what is (decreased) plasticity buyt a natural information processing constraint? Demonstrating presence in artificial systems doesn't estabilsh it as a general feature of learning, except insofar as we've defined learning narrowly to be done by biological and ANNs (does naive bayes learn? does it exhibit critical periods?). It becomes sort of a circular argument about what does \\\"learning\\\" even mean. This commonality is actually supported by your experiments in Fig 3 recreating Guillery's. Anyway, I think the key sentences/claims (e.g. last of the abs) and results stand and are extremely interesting,\u00a0 I would just like to see this framed around mechanisms or instantiations of information processing constraints, or something, rather than competing hypotheses. and maybe acknowledge\u00a0more of the ambiguity of what \\\"learning systems in general\\\" means/ (without compromising clarity and narrative, which I believe are strong points of the paper).\\n\\nTerms used in equations are not explained when introduced\u00a0 in 3.1 -- I know it's from a citation and you might be short on space, but you will lose a lot of readers here and it's a shame not to continue the clear prose you've had thus far.\\n\\nAround eqn 9, connection of matric completion to what you're trying to do is not clear -- be explicit about how imputing missing values is the same as / allows us to specify\u00a0 task relationships with flexibility.\u00a0\\n\\nWhile the relationship of experiments to showing dependence on depth is very clear and well-explained, the insights and relationship of experiments to data distribution is much less clear.\", \"Questions\": \"See \\\"weaknesses\\\" for some less concrete suggestions (feel free to tell me what you plan to write if you address them and I'm happy to provide feedback!)\\n\\nThe main other thing I'd like to see is connections made/discussed with nonlinear networks -- what should we expect to change vs. hold? maybe according to the sources you cite (e.g. Saxe), or ideally, a suite of experiments with e.g. MLPs of increasing depth. \\nAnother specific link could be made in Fig. 6 to scaling laws.\\n\\nsmall things:\\n\u00a0- first sentence shouldnt repeat abstract\\n\u00a0- 'competition between pathways affect' -> affects\\n- inconsistent use of \\\"artificial neural network\\\", \\\"deep net\\\", \\\"neural net\\\", et al. For me it's fine, but could be confusing for someone outside the field; better to be consistent\u00a0\\n- title for plot should be removed (conflicts/causes potential confusion vs caption and X axis label, nonstandard for technical works to have an embedded title)\\n\u00a0- remind us what \\\"Path Singular Value\\\" is in the caption/change caption e.g .singular value dimensions (confusing that its referred to as 1-5 but labelled 0-4 on plot and called something different). All this stuff explaining the axis and basics of the plots\u00a0should go first, before the interpretation.\u00a0- I didn't understand the comment \\\"and do not affect performance on the final gask\\\" in fig $. Does it mean they have equal performance?\"}"}]}
{"paper_id": "H9DYMIpz9c", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper addresses the importance of data quality and efficiency for training machine learning models, particularly focusing on autoregressive tasks like language modeling and sequential recommendation. Traditional approaches to handling large datasets for machine learning involve data downsampling or valuation to reduce costs, but these methods can be inefficient or intractable. Data distillation, which synthesizes data summaries for training models to a similar quality as training on full datasets, has been focused mostly on low-resolution data like images, whereas autoregressive tasks present unique challenges due to their discrete nature and large vocabulary spaces.\nGap: There is a lack of efficient data distillation techniques specifically tailored for autoregressive tasks, which involve a structured left-to-right causality of input and output sequences. Traditional data distillation methods are not easily applicable to autoregressive tasks due to computational inefficiencies and the inability to handle large discrete token spaces.\nHypothesis: The authors propose that their approach, FARZI, can effectively summarize large autoregressive datasets into smaller, synthetic sequences while maintaining or improving model performance, thereby reducing the cost of training large models.\nMethod: The FARZI method performs memory-efficient data distillation by using reverse-mode differentiation of the Adam optimizer with Hessian-Vector Products, and factorizes the high-dimensional event space into a latent space. This latent representation is intended to promote regularization implicitly. FARZI was tested on sequential recommendation and language modeling tasks, achieving 98-120% of the full-data performance utilizing only 0.1% of the original dataset size. The approach involves generating FARZI DATA as 3-D tensors of sequences of token distributions and employing a meta-learning framework to optimize the data summary effectively.\nConclusion: FARZI demonstrates that training state-of-the-art models on compact synthetic data derived from FARZI DATA is both computationally efficient and effective, achieving high performance with significantly reduced data inputs. This opens up possibilities for designing future large autoregressive models that are more resource-efficient and capable of scaling with less environmental and financial cost.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"- Originality and Significance: The latent parametrization that makes FARZI optimization friendly, and the proposed trick that enables reverse mode differentiation of Adam such that its memory complexity is independent of the number of inner loop steps are great contributions and of practical value.\\n- Quality and Clarity: The paper is well written with extensive experiments whose details and evaluations are that are clearly described. The results are impressive. The method is able to achieve better performance on downstream tasks compared with using the full dataset.\", \"Weaknesses\": \"- It is not clear whether this method will be practical and scale for larger language models and larger datasets. It would be great if the authors can elaborate on this.\\n- There is not a clear analysis of the total time gains of this method in comparison with training from scratch. Providing some values would make the case for this method more compelling.\", \"Questions\": \"Listed in weakness section.\"}"}]}
{"paper_id": "0b328CMwn1", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses visual prompting (VP), a method developed to repurpose large pre-trained models for downstream vision tasks without altering the model parameters. VP introduces a universal perturbation into the input data to facilitate task-specific tuning. It is positioned among parameter-efficient fine-tuning (PEFT) techniques, which aim to reduce the computational cost and parameters required to adapt pretrained models for new tasks.\nGap: Although VP provides a novel way to use pre-trained models, there is a performance gap between VP and conventional fine-tuning methods. This gap reveals an unexplored area in both theory and practice that requires understanding and advancement to improve the performance and efficiency of VP beyond its current limitations.\nHypothesis: The authors propose that activation prompts (AP), which extend VP by introducing perturbations at the intermediate activation map layers instead of solely at the input layer, can enhance the performance and efficiency of VP. AP connects naturally to normalization tuning and can potentially outperform VP and other PEFT methods.\nMethod: The authors introduce AP, which applies task-specific perturbations to activation maps at intermediate layers of pre-trained models. This approach is analyzed theoretically in terms of its connection to normalization tuning techniques like batch normalization in CNNs and layer normalization in ViTs. The paper conducts extensive experiments over 29 datasets and compares AP with existing VP and PEFT techniques, analyzing the performance across various datasets and architectures. The experiments explore how AP affects efficiency metrics such as memory usage, time consumption, and accuracy.\nConclusion: The study concludes that AP surpasses VP in accuracy and efficiency and that the choice of the layer depth for AP implementation significantly affects performance in CNNs and ViTs. This new prompting strategy demonstrates reduced memory, time requirements, and improved accuracy compared to state-of-the-art PEFT methods, supporting its potential to close the performance gap previously exhibited by VP.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"1. The study of applying AP to different layers provides interest insights related to model interworks such as attention distance.\\n2. Experimental results are presented with many datasets, settings, and compared with various benchmarks.\", \"Weaknesses\": \"1. Novelty given extensive studies in the VPT paper. The VPT paper studies various different ways of applying the prompt. For example, in Figure 5, they studied the option of adding tokens instead of prepending tokens and observed slightly worse results, which is almost verified in Table 4 of this paper. In Fig. 14 of VPT, they also studied adding prompts in different depths of the network. These previous work almost makes this paper a special case in the search space covered by VPT.\\n2. Fair comparison with existing works such as VPT. The main results are reported all in different settings (e.g. different backbone, patch size, pretraining, etc.) that prevent direct comparison with numbers reported in the VPT paper, although this work outperforms basic baselines such as VP and norm-tune.\\n3. Soundness of the parameter counts in tables. For example, in Table 2 with ViT-L and CIFAR-10, how does LINEAR-PROBE have 1M parameters instead of 10K (1024 * 10)? In addition, if other methods (e.g. norm-tune and AP) also train a linear layer on top of the features, then this should be made clear as well. The current reported counts are significantly higher than expected (also see table 5 of VPT).\\n4. Limited gain of efficiency-accuracy trade-off compared with other methods in Table 4 such as LORA etc. The main advantage of AP is the lower cost but other methods are configured to be more expensive than they should be. e.g. this paper experiments with VPT-DEEP with 50 tokens, but (1) VPT also presents VPT-shallow as a more efficient option, (2) VPT by default searches over a set of prompt lengths and in their table 13, a much smaller average token length is needed, e.g. 10 or 1, (3) VPT applied to a deep ViT-L will put VPT in a disadvantage but this could be easily fixed by prompting a subset of layers. As a result, it is not clear where this paper stands in the extensive ablations of VPT.\", \"Questions\": \"As suggested in the weakness section, numbers directly comparable with those reported in VPT paper will facilitate fair comparison. Please see the weakness section for more details.\"}"}]}
{"paper_id": "am7BPV3Cwo", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Out-of-distribution (OOD) detection is crucial for neural networks to avoid unreliable predictions by identifying and rejecting unknown samples during deployment. Traditional OOD detection methods are usually evaluated with category-balanced in-distribution (ID) data, but in real-world applications, data often show imbalanced distribution which affects the performance of such detectors.\nGap: Existing OOD detectors struggle to maintain performance on imbalanced ID data, often misidentifying tail class ID samples as OOD and mistaking OOD samples as head class ID samples. This is due to the inherent bias in data distribution, which has not been thoroughly addressed by current methods.\nHypothesis: The authors propose that the performance gap between balanced and imbalanced OOD detection is due to a class-aware bias. By understanding this bias, it is possible to improve OOD detection methods on imbalanced data distributions through a combination of post-hoc normalization and training-time regularization techniques.\nMethod: The authors introduce a statistical framework, ImOOD, which extends OOD detection to imbalanced data distributions by using post-hoc normalization and training-time regularization. Post-hoc normalization adjusts detection probabilities using ID classification information, while training-time regularization involves a loss function that encourages the separation of tail ID classes and OOD samples and prevents OOD samples from being classified as head ID classes. This method is applied and tested on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT benchmarks, showing significant improvements over state-of-the-art methods.\nConclusion: The ImOOD framework was shown to significantly enhance OOD detection performance on imbalanced datasets by consistently outperforming existing methods on benchmarks. By providing a unified perspective on handling class-aware biases, the method effectively reduces errors associated with imbalanced data, offering a robust solution for practical OOD detection challenges.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"- The statistical analysis performed in the paper is sound and written in a manner that is easy to follow. The authors include a helpful intuitive understanding of the bias term that connects it back to the empirical observations about the behavior of OOD detection models under the class-imbalanced setting.\\n- ImOOD is well-motivated and theoretically-grounded on the the above analysis. The general thinking behind the method is sound, and for the most part, it\u2019s described clearly enough to understand and re-implement the design.\", \"Weaknesses\": \"- I believe the major drawback of this paper lies in the limited scope of the studied problem. It appears that the authors assume that the class-imbalance problem only exists on the original ID dataset. I am not too convinced with the practicality of the proposed scenario; it\u2019s hard to imagine in what deployment scenarios only the ID dataset will suffer from this problem, while the OOD dataset(s) used for training/testing is void of it. Wouldn\u2019t it be more natural to assume that the ID training dataset can be refined in advance to make it class-balanced, while the OOD data the detector must be able to identify arise in various forms, and thus is more likely to be class-imbalanced?\\n    - What if the auxiliary OOD dataset used for training and/or the target OOD dataset exhibits class-imbalance as well? I think it is important to study various combinations of class-balanced and -imbalanced ID/auxiliary OOD/target OOD datasets. Does the analysis performed in the paper extend to and hold in such settings? Can ImOOD still outperform other baselines?\\n- Please correct me if I am wrong, but it appears that the verification of post-hoc normalization (minus the train-time regularization) on larger datasets appears to be missing. Even on CIFAR10-LT, the improvement from post-hoc normalization seems marginal, but this concern could be alleviated, as long as the improvement is consistent across various datasets. If post-hoc normalization is not quite as effective on datasets, it would signify that ImOOD is heavily reliant on the train-time regularization.\\n- More details on how the label distribution is learned (during test-time regularization) would be appreciated. Also, do you use the same learned label distribution when performing post-hoc normalization? Or is it used only for training-time regularization? If you discard it after training, one would have to know the label distribution anyways for post-hoc normalization, so do we really need to use the learned distribution in the first place?\\n- The empirical validation is limited to one target OOD dataset per InD dataset. Validation on more challenging OOD datasets (e.g., near OOD data, data with spurious correlation) would be helpful to gauge the effectiveness of the proposed method.\", \"Questions\": \"Please refer to the Weaknesses section.\"}"}]}
{"paper_id": "fXa9rO75dB", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Classical visual odometry methods, which use image depth and ego-motion prediction, have been accurate in favorable conditions but often fail in challenging, textureless, or dynamic environments. Deep visual odometry using deep learning has emerged as a promising alternative, although traditional approaches, typically requiring extensive labeled data, can be cumbersome. Recent methods like Zhou et al. (2017) demonstrate self-supervised learning but often rely on monocular vision, which faces difficulties without reference scales and in dynamic environments.\nGap: The existing methods for deep visual odometry either focus on monocular systems or fail to adequately integrate depth and ego-motion predictions from stereo cameras holistically. Also, the robustness of these methods in dynamic environments remains an issue, as they often rely on the assumption of a static world.\nHypothesis: The paper hypothesizes that integrating optical flow predictions with stereo camera data using a deep neural network can offer better accuracy in visual odometry. By imposing a 3D-geometric constraint to enforce realistic scene structures, more robust depth and ego-motion predictions can be achieved.\nMethod: The proposed self-supervised framework leverages stereo cameras and a neural network to predict optical flow, from which depth and ego-motion are derived. Key components include: 1) A FlowNet for optical flow prediction, 2) Depth estimation using stereo disparities, 3) A 3D-geometric constraint in the loss function to ensure realistic movement predictions of static and moving objects, and 4) A PoseNet for estimating ego-motion from optical flow between consecutive frames. The methodology involves training on the KITTI dataset with photometric consistency and embedding stability facilitated by features processed through a variational autoencoder.\nConclusion: The framework demonstrates state-of-the-art results on the KITTI dataset, showing higher accuracy in depth prediction compared to monocular-based methods and comparable visual odometry performance with existing stereo approaches. The results underscore the value of stereo cameras in overcoming the limitations imposed by monocular hardware in deep visual odometry tasks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 1, \"Contribution\": 1, \"Rating\": 1, \"Confidence\": 5, \"Strengths\": \"The proposed method is slightly different from previous methods, even though the use of optical flow and stereo cameras in this field has not been new.\\n\\nThe depth accuracy of the proposed method is slightly better than previous methods that are mentioned in the paper.\", \"Weaknesses\": \"The paper looks incomplete. I hope that the authors can significantly revise the paper and add more content for the next submission.\\n\\nThe novelty of the proposed method is very limited. Please reconsider this problem and better claim the contribution.\\n\\nThe evaluation results show that the proposed method is worse than previous methods in both visual odometry and optical flow estimation.\", \"Questions\": \"NA\"}"}]}
{"paper_id": "Qufas3h0mY", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Training large neural networks involves challenges such as backward locking, which results from sequential dependency where the chain rule restricts each GPU's layer to await gradient information from other GPUs. This bottleneck limits the potential speedup from parallel processing and hinders efficient training of deep neural networks.\nGap: Current methods like backpropagation (BP) are inefficient for deep neural networks due to backward locking, preventing effective model parallelism. This sequential nature of BP's gradient flow creates significant delays and inefficiencies when deploying neural networks across multiple GPUs.\nHypothesis: The authors propose Supervised Contrastive Parallel Learning (SCPL) as a solution to enable model parallelism, which can decouple the end-to-end backpropagation into multiple shorter flows, allowing simultaneous gradient computations across different layers and thereby overcoming the backward locking issue while enhancing training efficiency.\nMethod: SCPL leverages supervised contrastive learning to decouple the gradient flow in deep neural networks. It assigns local objectives to network components, allowing parameter gradients to be computed separately on different GPUs. The method was tested on various datasets like CIFAR-10, CIFAR-100, Tiny-ImageNet, IMDB, and AG\u2019s news, using architectures such as ConvNet, VGG, ResNet, LSTM, and Transformer. The approach enables pipeline-like parallelism and, hence, model parallelism, potentially speeding up training and maintaining high accuracy.\nConclusion: SCPL significantly reduces training time while maintaining or even increasing accuracy compared to traditional BP, Early Exit, and Associated Learning methods. It addresses long gradient flow issues in deep networks, presenting a flexible alternative to state-of-the-art methods without needing additional architectural changes.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 2, \"Strengths\": \"1. The paper provides a thorough explanation, making the proposed method reproducible.\\n2. Excellent visualization techniques are employed to illustrate the algorithms, facilitating understanding.\\n3. Merging local gradient computations with self-supervised learning stands out as a novel approach in this domain.\", \"Weaknesses\": \"1. The description surrounding gradient length appears overly assertive. It might be more accurate to present it as $O(H/N)$, relative to the total layer count $H$ and the number of local losses $N$ employed. This observation can be further corroborated by the authors' implementation details.\\n2. The paper does not delve into the influence of $N$\u2014specifically, the count of local losses used and their respective placements\u2014on the system's overall performance. Investigating this could provide more comprehensive insights.\\n3. Although the paper introduces a synchronized pipeline parallel algorithm, it seems the \\\"bubble\\\" issue persists in the suggested method. For a rounded perspective, it would be beneficial if comparisons concerning accuracy with asynchronous pipeline parallel algorithms (like PipeDream) were provided. This would further highlight SCPL's potential advantages over asynchronous counterparts.\", \"Questions\": \"1. Is there a mistake in in paragraph 1 in Section 2.2, which should be 'the author of AL' ?\\n2. Also see weaknesses part.\"}"}]}
{"paper_id": "SL7djdVpde", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Variational Quantum Algorithms (VQAs) have gained attention during the Noisy Intermediate-Scale Quantum (NISQ) era for their potential to provide quantum advantage. A key challenge is implementing symmetries as hard constraints within VQAs, as typical VQAs were not initially designed to accommodate such constraints.\nGap: Current VQAs, like the Quantum Approximate Optimization Algorithm (QAOA) and methods for ground state energy estimation, often fail to natively handle hard symmetries and constraints, which limits their applicability in solving certain quantum chemistry and optimization problems.\nHypothesis: The hypothesis is that the Hamming Weight (HW) preserving ansatz can be utilized to enforce hard constraints in VQAs more effectively than existing methods, and can be theoretically linked to various physical symmetries and constraints, thus expanding its applicability beyond current use cases.\nMethod: The paper explores HW preserving ansatz, detailing its application in symmetry-preserving VQAs and leveraging quantum optimal control and overparameterization theories. This includes testing on unitary approximation, ground state energy estimation, and feature selection tasks using different HW preserving gates, particularly focusing on the Reconfigurable Beam Splitter (RBS) gate. Experimental setups involve verifying theoretical findings on constrained VQA problems.\nConclusion: The HW preserving ansatz shows superior performance in applying hard constraints within VQAs. The RBS gate, alongside its tested variations, achieves better outcomes on constrained VQA problems like ground state energy estimation and feature selection, indicating efficiency and supremacy over alternatives like the Hardware Efficient Ansatz (HEA) and other soft constraint methods.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"- This paper nicely summarizes and makes clear the idea of using Hamming weight (or in physics terminology, excitation) preserving ansatz for variational quantum algorithms. The numerical results verify the accuracy of the theory, which characterizes the expressibility of these ansatz.\\n- The experiments are nice, showing a modest improvement over existing methods.\", \"Weaknesses\": \"- Typos: \\\"relative\\\" should be \\\"relatively\\\" throughout\\n- Typo: In Figure 2, \\\"Haar measurement\\\" should be \\\"Haar measure\\\"\\n- Typo: \\\"Hartree-Fork\\\"\\n- Based on common gate sets of real machines, I feel the BS gate is quite artificial. It might make more sense to use parameterized $XX$ rotations with single-qubit $Z$ rotations. If the intention is to perform VQAs on near-term machines, then one might want to use more native instructions than the decompositions shown in Fig 5 and Fig 6.\\n- Related to the above point, the decomposition of the BS gate is addressed as a possible limitation. I am pretty sure a better decomposition is possible using $XX$, $YY$ and $ZZ$ rotations.\\n- I think this idea of preserving the Hamming weight is not very new, and the theoretical results are not very surprising to me. For this reason, I feel the contribution of this work is not so large, although the proposed ansatz appear to perform better than existing ones.\", \"Questions\": \"- In Equation 8, should the maximization be over $M$ such that $M\\\\geq M_c$?\\n- Where does the construction for the BS gate come from?\\n- Throughout this paper, it is stated that this is a \\\"revisiting\\\" of the Hamming weight preserving ansatz. What is the status of the previous work using these ansatz, and what are the specific improvements made in this paper compared to previous proposals?\"}"}]}
{"paper_id": "Yp01vcQSNl", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Directed graphs are crucial for representing various real-world relationships such as social networks and paper citations. Traditionally, graph neural networks (GNNs) have been applied to these datasets to learn node and edge encodings. However, conventional graph transformers, which can potentially outperform GNNs due to their flexible attention mechanism, often fail to incorporate edge directionality effectively. Existing methods frequently rely on the static adjacency matrix or local computations, which do not fully exploit the potential of dynamic attentional relations that can be developed with transformers.\nGap: Current graph transformer architectures largely ignore or inadequately handle edge directionality, undermining their applicability to directed graphs. The core limitation is their reliance on structural assumptions like static adjacency matrices, which restrict their ability to leverage edge directions flexibly in learning tasks.\nHypothesis: The main hypothesis is that a graph transformer architecture that explicitly accounts for edge directionality using dual encodings and dynamic attention mechanisms can significantly enhance performance on learning tasks for directed graphs.\nMethod: The proposed method, Directed Graph Transformer (DiGT), introduces dual encodings representing each node's source and target role in directed edges. These are learned dynamically using a multi-head directional attention mechanism without relying on explicit graph structure. The DiGT leverages a novel attention matrix interpretable as a latent adjacency matrix, incorporating edge channels as a bias and focusing on k-hop neighborhood localization. DiGT also includes techniques to mask attention matrices based on k-hops and combines multiple heads to achieve final predictions. Additionally, the paper explores alternative ways to incorporate directionality into existing graph transformer architectures.\nConclusion: DiGT outperforms state-of-the-art models in graph learning when directionality is a key component, demonstrating that capturing the directional aspect of edges is crucial for certain directed graph datasets. The experiments on new directed graph datasets designed specifically where edge directionality is important further validate the effectiveness and promise of DiGT.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"Strong Points:\\n- Dual node encodings in DiGT elegantly capture directionality throughout the model.\\n- Directional attention neatly exploits query/key asymmetry for modeling direction.\\n- DiGT significantly outperforms baselines on directional graphs.\\n- Quantifies dataset directionality via entropy measure.\\n- New directional graph datasets enable better evaluation, although it may be biased in the context of this work.\", \"Weaknesses\": \"Weaknesses and\", \"Questions\": \"included together with weaknesses\"}"}]}
{"paper_id": "f4HohsyNEk", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Recent advancements in neural field representations, particularly those using volumetric rendering, have enabled impressive scene reconstructions with photorealistic quality but suffer from slow rendering and limited support for standard 3D applications. Volumetric methods generate meshes with poor topology, while differentiable rasterization methods produce higher quality meshes but struggle with initialization sensitivity. These challenges hinder efficient rendering and integration with existing graphics pipelines.\nGap: The problem with existing methods is the trade-off between mesh quality and rendering speed. Volumetric rendering provides detailed geometry but is slow and not easily compatible with standard 3D processing pipelines, while differentiable rasterization methods produce higher-quality meshes but suffer from poor initialization, making them less reliable.\nHypothesis: The authors propose a method, NeuManifold, that leverages neural volumetric rendering for initial geometry generation and refines these using differentiable rasterization. This approach can generate watertight manifold meshes that combine the strengths of volumetric and rasterization methods to produce high-quality meshes efficiently for rendering and use in various 3D applications.\nMethod: The proposed method consists of two main stages: 1) Initialization using neural volumetric fields to create an initial mesh structure, 2) Refinement through differentiable rasterization that utilizes a newly introduced Differentiable Marching Cubes (DiffMC) algorithm to enhance mesh quality and eliminate artifacts. This integrated approach improves rendering speed significantly, maintaining compatibility with 3D graphics applications.\nConclusion: The NeuManifold method delivers high-quality watertight manifold meshes that improve rendering speed by an order of magnitude compared to previous methods, while preserving detailed geometric quality. The meshes generated are compatible with modern rendering pipelines and suitable for downstream applications, including simulations. Despite potential challenges with specular surfaces, the method marks a significant advancement in efficient, high-quality 3D reconstruction.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"The writing quality is generally good, and well-articulated.  The paper provides thorough evaluation on multiple datasets, and comparisons to multiple baseline methods.  The end-to-end differentiability of the system allows refinement of the initial geometry based on a rendering loss, which improves the quality of the reconstruction result.  The implementation and engineering, such as deploying the optimized model in GLSL shaders, is notable.\", \"Weaknesses\": \"While the paper shows favorable results in terms of quality of reconstruction, it seems only about on par or marginally better than methods that it has been compared to.  It mostly connects components from existing works (TensoRF, nvdiffrast, Marching Cubes algorithm) into a well-engineered end-to-end pipeline, but I question whether the specific technical contributions of the paper are significant enough to stand on their own.\\n\\nThe paper claims that it is the first differentiable Marching Cubes implementation, yet it's not clear whether this is indeed the case (see for example: Deep Marching Cubes, MeshSDF, Neural Marching Cubes, Flexicubes).  Furthermore, the differentiable formulation of MC presented in the paper is compared only to DMTet, but not to any other methods that might be even closer in spirit (i.e. those mentioned in the previous remark).  Therefore, it is hard to evaluate the level of contribution.\", \"Questions\": \"- The paper strongly emphasizes the watertight manifoldness of the resulting meshes, but does not go into much detail about the specific technical contribution of this paper that results in this property.  Could you clarify whether this is indeed a contribution of this paper, or is it inherited from the other methods that the paper builds off of?\\n\\n- On page 4, it is stated that during the second, mesh optimization step, the *topology* is also optimized.  Could you elaborate/clarify what you mean by this exactly?\\n\\n- On page 5: \\\"given that most real-world objects tend to be axis-aligned\\\" -- could you briefly explain why this is a reasonable assertion to make?\\n\\n- SDF-based reconstruction methods (e.g. NeuS) claim that the raw density field is optimized only to satisfy the rendering objective (and is also biased away from the true surface), and therefore are not ideal for surface reconstruction.  In this paper, you argue that this is unnecessary and that using the density field directly is a better choice.  Could you elaborate on this choice?\"}"}]}
{"paper_id": "o4CLLlIaaH", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Neural Radiance Fields (NeRF) have seen significant success in 3D scene representation and novel view synthesis, mainly using image-based neural rendering techniques combined with multiview stereo (MVS) methods. Recent advancements have aimed at making NeRF generalize without per-scene optimization, integrating approaches like PixelNeRF, IBRNet, and MVSNeRF.\nGap: Existing generic NeRFs face issues with feature inconsistencies from occlusions, distortions due to standalone sampled point processes, and degradations when source and target views are not aligned well. Image-based methods are limited in their capacity to interact and modify scenes.\nHypothesis: The authors propose shifting from an image-based to a point-based rendering approach for generalizable neural radiance fields to solve the issues of occlusion, poor geometric handling, and lack of interactivity in representation.\nMethod: The paper introduces the Generalizable neural Point Field (GPF), which includes several innovations: visibility-oriented feature fetching based on geometric priors; a nonuniform log sampling strategy to enhance rendering speed and quality; and a feature-augmented learnable kernel for effective local feature aggregation. The method is experimentally validated on NeRF synthetic, DTU, and BlendedMVS datasets in both generalization and finetuning settings.\nConclusion: The proposed GPF outperforms traditional image-based approaches in terms of rendering quality, geometry correctness, and view consistency. It shows potential in enabling scene interaction and achieves significant enhancement over benchmarks in both generalization and finetuning scenarios, thus supporting the validity of point-based representation for generalized neural radiance fields.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"- Originality: The Generalizable Neural Point Field (GPF) is a fresh perspective in NeRF research, emphasizing point-based over image-based rendering.\\n- Quality: The proposed methods exhibit high-quality research and innovation, from the nonuniform log sampling strategy to the feature-augmented learnable kernel.\\n- Clarity: The paper's overall structure is clear, presenting a logical flow of ideas and discussions.\\n- Significance: If the claims are validated further, this research could serve as a benchmark in the NeRF domain.\", \"Weaknesses\": \"- Lack of Detailed Explanations: Some sections, especially the technical components, could use more in-depth explanations or visual aids.\\n- Dependency on Other Technologies: The initial dependency on PatchmatchMVS might limit the paper's approach from being a standalone solution.\\n- Limited Experimentation: Testing on only three datasets might not showcase the full potential or limitations of the method.\\n- Complexity: The approach's intricate nature might pose scalability or efficiency challenges that haven't been addressed comprehensively.\", \"Questions\": \"- Could the authors expand on the visibility-oriented feature fetching, possibly with diagrams, for better clarity? Figure (b) alone is not clear enough.\\n- Given the reliance on PatchmatchMVS for the initial point scaffold, how does this affect the scalability or deployment of GPF in diverse scenarios? The authors also mention in the limitation section that they want to propose a NeRF-based initialization module that can be trained from scratch. Please comment on how you plan to achieve this.\\n- Can the authors comment on potential efficiency challenges due to the method's complexity? For example, compare the rendering speed with E-NeRF.\"}"}]}
{"paper_id": "RlfD5cE1ep", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Self-supervised representation learning, particularly contrastive learning, utilizes both positive views generated via data augmentation and negative examples to align representations. Non-contrastive methods like BYOL (Bootstrap Your Own Latent) and SimSiam offer computational benefits by eliminating negative examples. Despite initial assumptions that representations might collapse without negative samples, prior work has shown non-collapse is possible with strong data augmentation balancing regularization.\nGap: Existing analyses of non-contrastive dynamics, such as those using L2 loss, do not incorporate feature normalization, which is commonly employed in practice. This oversight may lead to collapse when regularization is too strong, contradicting the observed stability in practical settings that use normalization.\nHypothesis: Feature normalization significantly influences the dynamics of non-contrastive learning, preventing collapse even under strong regularization by inducing higher-order dynamics that stabilize the representation space.\nMethod: The paper extends previous L2-loss-based dynamic analyses by incorporating cosine loss, which accounts for feature normalization. Using a high-dimensional limit, the research models the learning dynamics with synthetic data. It presents eigenmode dynamics analysis, demonstrating that cosine loss induces sixth-order dynamics, contrasting with the third-order system from L2 loss approaches. Numerical simulations are conducted to verify these dynamics using the CIFAR-10 dataset and SimSiam framework with ResNet-18.\nConclusion: Feature normalization plays a pivotal role in stabilizing non-contrastive learning dynamics, counteracting potential collapse even with strong regularization. This is attributed to the emergence of stable equilibriums due to the complex dynamics induced by the cosine loss. The findings suggest a theoretical expansion towards understanding practical mechanisms that support the robust performance of self-supervised non-contrastive learning systems.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"This article presents an improvement other than the theoretical framework of non-contrastive learning using solely the Euclidian loss.\\nThe paper is well-written and easy to follow. The assumptions taken are relatively well justified and allow for an interesting analysis.\", \"Weaknesses\": \"**Previous literature** There have been recent contributions to the literature of non-contrastive learning which do take into account the cosine loss, and which are not referenced in this article. In particular, Halvagal et al., Implicit variance regularization in non-contrastive SSL, 2023. The eigenmode dynamics seem extremely similar (after some changes in the notation) and it seems extremely important to me that the authors compare themselves to this article. The authors also do not seem to have a similar conclusion on the implicit variance regularization that Halvagal et al. focus on.\\n\\n**Regimes** The three regimes found in Section 5.2. seem to have been found solely by categorizing the regimes experimentally shown in Figure 2 while reading, which seems like a weak justification for those regimes. A clearer analysis of the equilibrium points at least in the Appendix seems necessary. \\n\\nThe authors claim that as the norms decrease, the regimes fall to the stable one. However, in the stable regime, the norms will increase as the eigenmodes increase to the saddle point $p^+$. Is there a risk of the acute and stable regimes alternating between each other? \\n\\n**Experiments** Numerical experiments on the SimSiam model remain on linear networks in Section 5.4. A similar Figure to Figure 6 for a real network such as ResNet (maybe only focusing on the linear projection head) would help confirm the theoretical findings in the linear case. Otherwise, the link with a real SimSiam network remains relatively limited, except for the weight decay argument.\\n\\n**Figure 6c** I also find Figure 6.c. hard to read. Are the intervals the theoretical intervals using the values of the norms? In this case, what is the theoretical value of the saddle point? Having the values of a single eigenmode gives very little information on the values of the spectrum of $W$. Do all the values stay relatively constant like here?\", \"Questions\": \"The notion of Thermodynamic limit is novel to me in optimization and needs to be further explained. How is it different from the Neural Tangent Kernel regime, is it the constant ratio $\\\\alpha$?\\n\\nDo the authors have more intuition on the role of the exponential moving average in BYOL with their new findings?\\n\\nSmall remarks:\\n* Intro: \\\"Folklore says\\\" is a somewhat strange way to quote an article.\\n* Sec 4. Assumption 2. $\u03a3 = I $ seems superfluous to add after $D = N(0, I)$.\\n* After Lemma 3: $\\\\Phi x'$ is not defined. \\n* Equation (5): $\\\\hat H$ is not defined.\\n* After Equation (9): \\\"unite learning rate\\\"\"}"}]}
{"paper_id": "hRos9WldRK", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Deep learning in computer vision has made significant advancements, with its success largely attributed to large, high-quality datasets. However, label noise, prevalent in real-world datasets due to factors like annotation errors, hinders model performance. Existing methods for learning with noisy labels primarily involve loss correction by estimating noise and adjusting loss functions, which often require assumptions or can lead to neglecting noisy yet informative data.\nGap: Traditional methods struggle to handle high levels of noise effectively and often depend on assumptions about the noise model or require a clean validation set, which might limit their applicability and effectiveness, especially when relying on assumptions or potentially discarding valuable noisy data.\nHypothesis: The authors propose that a joint reweighting mechanism, which dynamically adjusts the importance between real and pseudo-labels as well as between samples, can enhance model robustness against label noise without depending on explicit assumptions about noise or a clean validation set.\nMethod: The proposed Learning to Bootstrap (L2B) method exploits a meta-learning framework, adjusting the weights of instances and labels using a validation set feedback. The model adjusts weights in training using its own predictions while minimizing loss influenced both by real and pseudo-labels. The method is validated on datasets like CIFAR-10, CIFAR-100, ISIC2019, and Clothing 1M through empirical testing and is shown to integrate well with existing noisy label learning techniques.\nConclusion: The method delivers robust results against label noise across several datasets, demonstrating improved accuracy over baseline approaches. It eliminates the dependency on a clean validation set, leverages pseudo-labels more effectively, and generalizes well to other tasks like segmentation, suggesting its potential as a practical complement to existing noisy label learning methods.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 2, \"Strengths\": \"- The authors perform extensive experimentation on synthetic datasets, one dataset with real label noise, and compare to a large number of methods. \\n- The convergence analysis is a great contribution to the paper. \\n- The paper is well-written. Before getting to the experiments section, I was curious about whether or not the means of simplex projection for \\\\alpha and \\\\beta mattered in the method, since the authors perform an approximation. The authors seem to have pre-empted this potential question of mine, as they include an ablation that evaluates using the softmax of \\\\alpha and \\\\beta.\", \"Weaknesses\": \"While the authors evaluate on one dataset with real label noise, Clothing-1M, there are a variety of other real-world settings where coping with (potentially systematic) label noise is important. It would be great to include additional real-world settings that may have systematic label noise. For example, label noise is a problem that is often inherent to the family of programmatic weak supervision techniques.\", \"Questions\": \"- Given Euqation 9, it's unclear to me if \\\\alpha and \\\\beta actually sum up to 1 in the final algorithm. Can the authors clarify this? In earlier discussion of the algorithm, it seems as though allowing \\\\alpha and \\\\beta to not sum to 1 is crucial for its final performance. \\n- The ablation studies on \\\\alpha and \\\\beta are interesting and a useful contribution to the paper. While I imagine that performing an exact simplex projection using one of the algorithms for doing so would be slow (mainly because it must be done separately for each sample), have the authors tried other methods besides the softmax or the sigmoid? \\n- In the current formulation that does not use the sigmoid or softmax, parameters are allowed to be zero. Is it common for either or both of them to be zero? Could this by why projecting to the interior of the simplex via softmax or sigmoid improves the stability of training? \\n- It is not clear to me whether or not a validation set is required. It is mentioned in several places in the paper that a validation set is not required, however, in 3.1, \\\"In this work, we also assume that there is a small unbiased and clean validation set.\\\" Can the authors clarify this point?\"}"}]}
{"paper_id": "BpAxeHmQ94", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Recent advances have shown that deep learning models are highly vulnerable to adversarial examples, leading to the development of defensive mechanisms divided into empirical defenses that lack robustness guarantees and certifiable methods that offer robustness certificates. Certifiable methods focus on model robustness within certain perturbation norms. There is also an increasing interest in cost-sensitive learning, where different adversarial misclassifications incur different costs, which is vital when certain misclassifications have more significant real-world consequences.\nGap: Existing methods for adversarially robust classification either do not provide robustness certification or face scalability issues. While many methods focus on improving overall model robustness, they often disregard the varying costs of different adversarial misclassifications. There is a need for scalable, certifiable methods that also account for these cost sensitivities.\nHypothesis: Randomized smoothing, a scalable robustness certification framework, can be adapted to certify cost-sensitive robustness. By introducing a cost-sensitive certified radius, it is possible to provide tight robustness guarantees customized for any given cost matrix.\nMethod: The paper introduces a cost-sensitive certified radius, an adaptation of the traditional randomized smoothing certification, to account for class-specific misclassification costs. This involves extending the standard randomized smoothing framework to include cost-sensitive considerations, optimizing smoothed classifiers through fine-grained radius optimization schemes, and training with a focus on different data subgroups. The method involves using Monte Carlo sampling to estimate robustness bounds and aims to balance the trade-off between cost-sensitive robustness and overall accuracy. Experiments on image benchmarks and a real-world medical dataset demonstrate the approach.\nConclusion: The proposed method significantly improves certified cost-sensitive robustness without compromising overall accuracy, as demonstrated through experiments. It outperforms baseline methods, offering a better trade-off between robustness and accuracy while providing tight robustness guarantees for various cost matrices. The study opens new opportunities for developing certifiably robust models that align better with cost-sensitive real-world applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"The paper studies provable robustness in a setting that has largely remained unexplored. The paper is well-written and easy to follow.\", \"Weaknesses\": \"1. Binary Cost Matrix: A 0/1 cost matrix may not be sufficient to capture real-world cost-sensitive ML tasks. The cost of misclassification would rarely be zero. Take the example mentioned in the second paragraph of the introduction. While misclassifying a benign tumor as malignant is less detrimental than the reverse, the cost of such a misclassification will not be zero. If so, one could simply label all tumors as malignant and achieve an overall classification cost of zero. However, such a classifier would not provide us with any valuable information.\\n\\n   It would be more impactful to design robustness certificates for a general cost matrix where the entries need not be 0/1. It might be possible to certify the expected misclassification cost by using the distribution of the cost under the smoothing noise. The following work on certifying the expected confidence of a neural network could be adapted for a general cost matrix:\\n\\n   Certifying Confidence via Randomized Smoothing, Kumar et al., NeurIPS 2020.\\n\\n2. Novelty: The robustness certificate designed in this work is a straightforward adaptation of Cohen et al.'s certificate [1]. The certificate in [1] takes the difference between two terms where the second term depends on p_b, the probability of the second most likely class. This paper redefines p_b as the probability of the most likely class in the set of class labels with cost 1.\\n\\n   [1] Certified Adversarial Robustness via Randomized Smoothing, Cohen et al., ICML 2019.\\n\\n3. Sample Complexity: The number of samples required for computing the proposed certificate is higher than that of the baseline certificate from [1]. It depends on the number of classes with cost 1. While this might be manageable for the small number of classes considered in the experiments (<= 10), scaling to a large number of classes, such as in ImageNet (1000 classes), would be difficult.\", \"Questions\": \"1. What would be a practical application where a binary cost matrix would be sufficient? In most scenarios, the misclassification cost would take a range of different values.\\n\\n2. During inference, when the ground truth is unknown, how do we find the classes with cost 1? This set depends on the correct class label, which is not known during inference.\"}"}]}
{"paper_id": "DO2WFXU1Be", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Physics-Informed Neural Networks (PINNs) have gained traction in numerically solving partial differential equations (PDEs) due to their ability to embed physical laws within the neural network framework. Traditional numerical methods, like the finite element or pseudo-spectral methods, are computationally costly, especially in high dimensions. PINNs, using multilayer perceptrons (MLP), have achieved success in various applications but struggle with high-frequency or multiscale features and typically neglect temporal dependencies.\nGap: Conventional PINNs rely on MLPs which often fail to model temporal dependencies in physical systems properly. This shortcoming results in inaccurate global solutions, as PINNs cannot effectively propagate initial condition constraints and accurately capture dynamic PDE solutions. Current approaches to improve PINNs, such as data interpolation and training scheme variations, often involve high computational costs and still fail to handle specific PINN failure modes.\nHypothesis: The authors propose that a Transformer-based approach, called PINNsFormer, which utilizes multi-head attention mechanisms, can better capture temporal dependencies in PDE solutions. By transforming point-wise inputs into pseudo sequences and employing a sequential loss, this framework aims to improve the generalization and accuracy of solutions obtained by PINNs.\nMethod: The methodology involves designing a new framework, PINNsFormer, which applies a Transformer architecture to PINNs. Key components include a Pseudo Sequence Generator for transforming inputs into time series, a Spatio-Temporal Mixer for data embedding, and an Encoder-Decoder architecture to model temporal dependencies using self-attention. Additionally, a novel activation function, Wavelet, is introduced to anticipate Fourier decomposition, enhancing the approximating power of the network. The model is trained using a modified loss function, tailored for sequential data, which allows PINNsFormer to handle boundary and initial conditions appropriately.\nConclusion: Empirical evaluations demonstrate that PINNsFormer outperforms traditional PINNs and their variants in generalizing solutions to complex PDEs, mitigating known failure modes, and handling high-dimensional problems. The framework's flexibility also allows it to integrate with other learning schemes, further boosting performance. The findings suggest that focusing on capturing temporal dependencies significantly improves the accuracy and robustness of PDE solutions obtained using PINNs.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 4, \"Contribution\": 4, \"Rating\": 8, \"Confidence\": 5, \"Strengths\": \"Impressive results.  Clear representation.\", \"Weaknesses\": \"The system is using a time discretised set of function values to predict the next time step. This reminds me of the finite difference method, In this case the stencil is 100 elements long, so the optimum stencil would have a very high order in accuracy. \\n\\nThe reason why very large stencils are not used is that these bear a computational cost, and the same happens in using attention, although most of the computing is parallel.\\n\\nNow the manuscript does not provide a baseline using a normal, discrete PDE solver, of course,  on the par with the  computational load that the attention mechanism is requiring . Of course, having an analytical, although complicated, solution for a problem has its advantages compared to a set of discrete nodal values.\\n\\nI would like that the authors would address this in their submission for better rating.\", \"Questions\": \"See the weakness part.\"}"}]}
{"paper_id": "EjIKerYk1O", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Digital towers are emerging as a cost-effective alternative to physical air traffic control towers, especially beneficial for small and medium-sized airports. They offer video-sensor-based surveillance, which enhances safety and efficiency when integrated with computer vision technologies. Currently, most research focuses on ground-based aircraft tracking at airports, whereas tracking aircraft during their final approach requires development. Existing distance measurement techniques using monoscopic or stereoscopic views face challenges concerning accuracy and calibration, especially over large distances.\nGap: There is a notable gap in research concerning the estimation of distance-to-touchdown (DTD) for aircraft approaching runways, a crucial parameter for final approach and departure sequencing. Current techniques such as monoscopic and stereoscopic distance estimation are ineffective for achieving the required accuracy over distances of up to 10 nautical miles.\nHypothesis: To accurately estimate the distance-to-touchdown of approaching aircraft, a multi-view video feed approach can overcome the limitations of monoscopic and stereoscopic methods. Integrating advanced object detection models with auxiliary regression and auto-calibration can provide real-time and accurate DTD estimates.\nMethod: The authors propose a real-time deep learning approach utilizing multi-view video feeds from digital towers. The approach uses Yolov7 for real-time aircraft detection and combines it with auxiliary regression auto-calibration to handle diverse camera viewpoints. The final features extracted from camera views are processed using LSTM models to ensure precise distance estimations. The approach is adaptable to various camera configurations without requiring retraining. The model was evaluated using simulated and real video data from Singapore Changi Airport, demonstrating low predictive errors and successful performance under different meteorological conditions. Data collection involved generating high-fidelity simulated data for training. A TensorRT engine was used to enhance processing efficiency, achieving high frame rates needed for real-time applications.\nConclusion: The proposed method effectively estimates aircraft distance-to-touchdown using multi-camera inputs, achieving high accuracy (Mean Absolute Percentage Error of 0.18% with simulated data up to 10 nautical miles, and 0.33% with real data under cloudy conditions up to 7 nautical miles). It outperforms monoscopic methods and demonstrates robustness across various lighting and weather conditions, thus enhancing runway controllers' ability to manage sequencing and spacing, improving overall airport operations.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"1. Distance estimation is a challenging task.  \\n2. This paper is easy to understand.\", \"Weaknesses\": \"1. The model design is not novel, which has limited technical learning.   \\n2. The dataset is not available. Then it cannot be a part of contribution.  \\n3. Calibaration network is a little strange. Why it can align two views without knowing the related position for the two cameras? If the two camera's position is changed, can this model still work?   \\n4. The number of views in the dataset is only 2.  The statement of \\\"multi-view\\\" is unsoundness. Author should increase the view number.  \\n5. The paper writing should be further improved. Besides, figure in the manuscript should be the vector figure (Most figures are blur).\", \"Questions\": \"Refer to the weakness.\"}"}]}
{"paper_id": "6vtGG0WMne", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Deep learning models often struggle with imbalanced datasets where the majority class dominates the data. Traditional methods aim to balance models to improve minority class performance by re-sampling or re-weighting. However, these methods usually focus on scenarios where training data is unbalanced, but the test set is balanced.\nGap: Existing balanced optimization methods cannot effectively handle real-world situations where the test data is also imbalanced, and different applications may require different optimizations between classes. It's challenging to adjust the model imbalance efficiently to meet specific application requirements.\nHypothesis: The authors propose that models should not always be balanced to be optimal for all applications. Instead, they should be optimized towards a user-specified imbalance state based on application metrics, leading to better performance in real-world settings.\nMethod: The authors propose a bias adjustment (BA) method that optimizes the model's class imbalance based on application-specific metrics. They define a Model Imbalance State (MIS) using class probability distributions and adjust the model's output layer bias to achieve the desired imbalance. The BA method uses KL divergence to align the MIS with a target distribution, making it efficient as it only modifies the model's final layer parameters. A new training strategy is introduced that uses BA during traditional training to find optimal representation parameters.\nConclusion: The proposed method effectively regulates model imbalance according to user-specified metrics, improving performance across various applications. Extensive experiments demonstrate that this approach offers significant improvements in accuracy, F1 score, and G-means compared to state-of-the-art methods. The BA method is found to be effective for different tasks, illustrating its broad applicability and efficiency.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 4, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"The proposed method is straightforward. Evaluations are however, fairly extensive, and are done across a variety of data types - tabular, image and text. A per-measure comparison is also provided. This kind of papers are persuasive with broad-based evaluations, usually on even more datasets.\", \"Weaknesses\": \"The hypothesis of just the bias on the softmax layer meant for classification making the classification swing by these amounts us astounding. In fact, a study on the values of the bias versus predictive performance is required in my opinion before bias adjustment is presented.\", \"Questions\": \"At 500:1 imbalance ratios, multiple benchmark methods show numbers decimated to zero. A previous study, MMM, by Mirza et al. '21 appears to suggest that even at such a ratio, classical resampling and even baselines report an above zero performance. Could you reason about the disparity?\"}"}]}
{"paper_id": "FJjHQS2DyE", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper addresses unsupervised domain adaptation (UDA), which is a framework where a learning model is trained using labeled samples on a source domain and unlabeled ones on a target domain. Traditional approaches have focused on reducing feature distribution divergence based on the covariate shift assumption, which often fails under label shift conditions.\nGap: Most existing UDA methods achieve suboptimal performance under label distribution shift because they are based on the covariate shift assumption, which assumes that the feature distribution changes, not the label distribution. Label shift, where the marginal label distribution varies across domains, is a more realistic assumption in various settings, such as healthcare, but is underexplored.\nHypothesis: The authors propose a novel Conditional Adversarial Support Alignment (CASA) framework, which aims to minimize the conditional symmetric support divergence between source and target domains to achieve more discriminative feature representations for classification, particularly addressing label distribution shifts.\nMethod: CASA involves aligning the supports of conditional feature distributions between the source and target domains and proving a new theoretical target risk bound. A complete training process is presented that includes minimizing the source risk, target conditional entropy, and Lipschitz continuity of classifiers, and employing pseudo-labels to approximate support divergence reduction. The method is empirically validated on benchmark datasets like USPS to MNIST, STL to CIFAR, and VisDA-2017, with CASA outperforming state-of-the-art methods in various label shift scenarios.\nConclusion: CASA provides a more effective approach to UDA under label shift by aligning conditional feature supports, resulting in improved classification performance. The paper's theoretical contributions justify the benefits of this alignment over existing marginal support approaches, supporting its empirical success across benchmark tasks. Future work could extend CASA to more complex settings like domain generalization and universal domain adaptation.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"This paper proposed conditional adversarial support alignment (CASA) to minimize the conditional symmetric support divergence between the source\u2019s and target domain\u2019s feature representation distributions. Generally, the paper is well-written and easy to follow. They evaluate the model on several benchmarks from various types of results.\", \"Weaknesses\": \"The model's novelty is incremental over multiple loss functions. The alignment loss in Eq(10) is more like pair-wise alignment loss, which has been explored before for cross-domain graph alignment. It is hard to verify the novelty.\\n\\nFrom the experiments, they show the improvements when \\\\alpha decreases. However, there is no insight why this happens. It is better to discuss the intuition and data used behind. It needs more visualization to demonstrate the improvement.\", \"Questions\": \"The novelty clarification.\\nThe performance analysis.\"}"}]}
{"paper_id": "Zw8YxUWL4R", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Text-to-image generation models have significantly advanced image synthesis, allowing the creation of diverse images from textual descriptions. These models often use diffusion processes that involve multiple cross-attention layers in a U-Net architecture, conditioning on a single text prompt embedding across all layers.\nGap: Current text-to-image models typically condition image generation on a single text prompt embedding, which limits the expressiveness and control over specific aspects of the synthesized image's structure and appearance.\nHypothesis: The introduction of an Extended Textual Conditioning space, referred to as P+, that uses multiple per-layer tokens for conditioning in the cross-attention layers of a text-to-image diffusion model can provide greater control and expressiveness in the synthesis process.\nMethod: The authors propose a new conditioning space, P+, where each cross-attention layer in the U-Net is conditioned on its specific textual token. They also introduce Extended Textual Inversion (XTI), which extends Textual Inversion by using a set of per-layer tokens to represent a subject. The authors conducted a series of experiments using a publicly available Stable Diffusion model, analyzing cross-attention mechanisms and evaluating XTI against traditional Textual Inversion (TI) and DreamBooth methods.\nConclusion: The P+ space was found to be more expressive and provided better control over the synthesized images by disentangling shape and appearance in different layers. XTI showed improved reconstruction quality and editability, converging faster than TI. The method also enabled novel results in object-style mixing, suggesting potential for broad applications in personalized and creative image synthesis.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"- The paper is well written and easy to follow, with sufficient literature references and reasonable and intuitive design in terms of extended controlling space.\\n\\n- The findings illustrated in Fig. 3 in terms of per-layer prompting is interesting.\", \"Weaknesses\": \"- The XTI section is a bit confusing, the proposed XTI reconstruction loss (also, this equation has no numbering which makes it difficult to refer to) seems to be a stepwise loss, which means the extended space is constructed/optimized at every diffusion step of T2I models?\\n\\n- Following the first point, while this operation is intuitive as many existing editing methods do follow the step-wise iterative paradigm, it is worth doing some ablations/analytical experiments on this particular operation, as recent works [a] have shown that single-step operation is sufficient to achieve semantic modification for diffusion models with more theoretical justifications. \\n\\n- Following the previous two points, for the simple experiments illustrated in Fig.3, does the textual conditioning are introduced at every diffusion step? \\n\\n- While the work seems to be technically solid and shows interesting results in downstream applications, most of the findings are empirical-driven, and I find it relatively difficult to interpret the proposed findings from theoretical perspectives. Do the authors have any insights on potential rationales for the layer-wise differences in U-Net on the denoising results? \\n\\n[a] Boundary Guided Learning-Free Semantic Control with Diffusion Models, NeurIPS 2023.\", \"Questions\": \"Please see the weaknesses for details.\\n\\nOverall, I feel the step-wise operation is worth further investigating. To be more specific, my concern is that the statement/the proposed empirical finding \u201cthe coarse inner layers of U-Net affecting the shape of the generated image, and the outer layers affecting the style and appearance\u201d may be inaccurate, which is more likely to be a fact of diffusion steps rather than the U-Net architecture.\"}"}]}
{"paper_id": "s6bKLlF4Pe", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Reinforcement learning (RL) aims to train agents to perform tasks in environments by learning optimal policies that maximize rewards. Deep RL uses deep neural networks (DNNs) but is resource-intensive. Transfer learning allows agents to leverage knowledge from previous tasks to improve performance on new tasks. Previous work introduced successor features (SFs) as a means to disentangle environment dynamics from task-specific reward functions, potentially improving efficiency. However, theoretical foundations for SFs combined with deep RL remain underdeveloped, especially in the context of deep Q-networks.\nGap: The theoretical understanding of successor features (SFs) combined with deep learning models, specifically when using deep Q-networks (DQN) with SF and generalized policy improvement (GPI), is lacking. While empirical successes have been noted, formal convergence analysis and generalization guarantees are sparse, particularly in function approximation settings like DNNs.\nHypothesis: The authors propose that employing a successor feature framework with deep Q-networks (SF-DQN) and a generalized policy improvement operator (GPI) provides provable convergence and generalization guarantees, leading to improved transfer learning performance in RL compared to traditional methods like DQNs.\nMethod: The study involves developing a theoretic framework to analyze the convergence and generalization of SF-DQNs with GPI. This involves decomposing the Q-function into task-specific reward mapping and successor features using deep neural networks for approximation. The authors derive theories showing convergence rate and generalization, supported by numerical experiments on synthetic and real-world RL tasks. The theoretical analysis includes characterizing the distance between estimated and optimal Q-functions and comparing performance with traditional DQNs.\nConclusion: The authors conclude that SF-DQN with GPI significantly improves the convergence rate and transfer learning performance compared to traditional non-representation learning methods like DQNs. Theoretical results show SF-DQN converges with generalization guarantees, offering superior estimation accuracy for optimal Q-functions across related tasks. Numerical experiments corroborate these theoretical findings.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 1, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"Theoretical analysis of SF when combined with function approximation is quite interesting and can be insightful.\", \"Weaknesses\": \"- The first half of the paper is confusing as the authors seem to conflate optimal value/policy at the task level with some generic $Q^*$ and $\\\\pi^*$. It was hard to follow and figure out what they mean by the notations and/or possible notational mistake. \\n\\n- As an important point, note that the GPI theorem induces improvement for the case of several policies but **the same** reward (i.e., only one task). If there are multiple rewards, then maximum of their corresponding value functions is simply meaningless and does not represent any physical concept regarding a new reward function (see the comments bellow). [This is OK when using SF as the $w_i$ corresponds to the current task and only $\\\\psi_j$ comes from some previous task.]\\n\\n- There were a number of other typos (a proof read aside from the points mentioned above is strongly suggested).\", \"Questions\": \"- If you have multiple tasks, then what do you mean by $Q^*$ and $\\\\pi^*$? In fact, both $\\\\pi^*$ and $Q^*$ are not well-defined unless the reference for optimality is specified. For instance, you may define a total reward as the summation of all the task-level rewards, whose $\\\\pi^*$ and $Q^*$ would be different from the case where a total reward is a *weighted* sum of task-based rewards or some non-linear function of them. \\n\\n- Eq (13) is not a policy improvement operator (PI is to take an argmax of the current value to induce a new policy). \\n\\n- Last three lines of page 4 --> First, your reference to $Q^*$ is senseless. More importantly, this setup has nothing to do with your settings. Maximum of several $Q$ functions corresponding to various rewards may not even be a valid $Q$ function for a different reward signal. One can easily construct an MDP with say three tasks and two actions, where in a certain state, $a_1$ is the optimal action for task 3, while $\\\\max_{i\\\\in \\\\{ 1,2 \\\\} } Q_{i}$ give you action $a_2$. Indeed it is theoretically possible that $\\\\max_{i\\\\in \\\\{1,2\\\\}} Q_{i}$ gives you suboptimal action in *all* states. \\n\\n- Table 1 --> what is $\\\\phi_{i}(\\\\Theta^{*}_{i})$ ? $\\\\phi$ is supposed to be the feature vector. \\n\\n- Assumption 3 is unclear. Note that $\\\\pi(a|s) \\\\in [0,1]$, Hence, for sufficiently large $C$ this assumption is always true! Am I missing something?\\n\\n- Is (20) a s fair assumption? For most cases, $K$ would be a large number, so it looks like that the initial approximation of $\\\\psi_1$ must be already very good! Perhaps a better presentation could be that there has to be some pre-training for $\\\\psi_1$ before using this algorithm?\"}"}]}
{"paper_id": "hH5HK4hsLY", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Fingerprint recognition is a widely used biometric identification method owing to its uniqueness and permanence. It is essential in law enforcement, personal identification, and secure authentication. Traditional fingerprint recognition methods often use minutiae matching, which can be sensitive to noise and partial prints. More recently, convolutional neural networks (CNNs) have been employed to handle these challenges better, enhancing the accuracy and scalability of fingerprint recognition systems. Self-supervised learning has also emerged as a powerful approach for learning representations without requiring labeled data.\nGap: The main problem addressed in this paper is the challenge of deriving robust fingerprint representations that can improve verification performance, especially from degraded images, without relying heavily on enhanced sample data. Existing methods, particularly those based on minutiae matching, face limitations due to noise sensitivity and difficulties handling partial or distorted fingerprints.\nHypothesis: The authors propose that leveraging a U-Net-based fingerprint enhancement approach for pre-training can help obtain robust fingerprint representations in a self-supervised setting, which can significantly improve fingerprint verification accuracy, especially when dealing with degraded images.\nMethod: The proposed method involves a two-stage framework leveraging U-Net architecture. The first stage is self-supervised pre-training of a U-Net encoder on a fingerprint enhancement task, aiming to learn meaningful features from degraded fingerprints. The second stage involves adding a multi-layer perceptron (MLP) for fingerprint verification, using the encodings from the pre-trained encoder. This involves freezing the encoder's weights and training the MLP for fingerprint verification using labeled data. The approach is compared against other self-supervised methods using datasets like SFinGe, FVC, and NIST SD-302, with augmentation techniques like rotation and color jitter used during training.\nConclusion: The proposed enhancement-driven pre-training method outperformed existing self-supervised techniques in learning stable and effective fingerprint representations for verification tasks. The results suggest that it is possible to derive meaningful fingerprint representations from degraded images without relying on enhanced samples. However, the method showed better performance on synthetic datasets than on real-world data, indicating potential biases and the challenges of real-world application.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"The main strength of the paper is its set of experimental results. They indicate that the proposed approach provides better accuracies than other comparable approaches, such as SimCLR, SimSiam, MoCo and BYOL.\\n\\nAnother strength of the paper is that it is easy to understand and the quality of the language is good.\", \"Weaknesses\": \"This manuscript has following high-level weaknesses that need to be addressed in any revision.\\n\\n1. Fingerprint verification accuracy on FVC test dataset is worse than on SFinGe test dataset. While I agree with the authors' explanation that it \\\"might be due to underrepresentation of FVC data in the training sets, leading to potential bias, and also due to inherent complexities in real-life fingerprint data, which are harder to handle\\\", these challenges are faced by all learning-based fingerprint recognition approaches and solutions that don't perform well on real data will be less attractive.\\n\\n2. Perhaps this is a weakness for deep learning based fingerprint recognition systems, but given that there are many real-world fingerprint verification systems based on minutiae matching, there needs to be a stronger case made for how the proposed deep learning approach is expected to have an impact in the real world.\\n\\n3. It is not sufficiently explained what \\\"self supervision\\\" means in this paper. My understanding is that both degraded and enhanced fingerprints are used for training. Aren't the enhanced fingerprints akin to having ground truth and if so isn't that training supervised? Not enough details are provided to understand this correctly.\\n\\n4. The paper needs to provide more mathematical details of the proposed approach. For example, more details should be provided about the fingerprint enhancement procedure and results.\\n\\n5. It is not clear what attributes of fingerprints are being taken advantage of, in the proposed approach. Are fingerprint geometric structures or minutiae being used somehow in the training? Overall, this paper is lacking in technical details.\", \"Questions\": \"1. Please be more clear about what self-supervision refers to.\\n\\n2. Provide more details about the fingerprint enhancement process. How is the enhancement quality evaluated?\\n\\n3. In Fig. 3, I assume that degraded images are on top and enhanced ones on the bottom  --- if this is correct, please state that in that figure caption.  It appears that the fingerprint degradation is artificial and not based on real-world challenges. If that is correct, could it be the reason for the poorer performance on FVC datasets?\\n\\n4. I am a little confused about how the accuracy numbers on the entire dataset are computed. In Table 2, it is stated that the accuracy of the proposed method on the entire data is 0.973. But if the impostor accuracy is 0.982 and the genuine accuracy is 0.886 and the data split is 1 genuine pair for every 3 impostor pairs, shouldn't the overall accuracy be (3*0.982 + 1*0.886)/4 = 0.958, not 0.973?\"}"}]}
{"paper_id": "lsvlvWB9vz", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: In distributed training of large models, a major challenge is the communication overhead which limits scalability. To address this, lossy communication compression methods, including error compensation (EC), have been developed. These methods are widely used in data homogeneous settings but are less understood in heterogeneous environments common in federated learning.\nGap: Existing approaches assume unrealistic conditions such as bounded data heterogeneity and large batches. Moreover, handling arbitrary compressors with existing methods has not achieved linear speedup in training time when increasing the number of devices.\nHypothesis: The authors propose that a new mechanism, EControl, can regulate error compensation by controlling the feedback signal's strength, allowing for stable and efficient convergence in heterogeneous data settings without making strong assumptions about data distribution or requiring impractical batch sizes.\nMethod: EControl adjusts the strength of error feedback and fuses error and gradient update steps, allowing it to efficiently balance error compensation. It operates under strongly convex, general convex, and nonconvex settings. The paper provides convergence theorems for these regimes and presents empirical evaluations showing linear speedup and robustness to data heterogeneity.\nConclusion: EControl offers provably fast convergence in various problem settings and effectively overcomes the limitations of existing EC methods, making it suitable for distributed optimization tasks in real-world deep learning applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"New error compensation algorithm: EControl with theoritical gaurantees for strongly convex, general convex, and nonconvex functions\\nEmpirical experiments to support the theoretical gaurantees and assess the efficacy of EControl.\", \"Weaknesses\": \"see the section below (Questions)\", \"Questions\": \"-In step 6 of EC-Ideal you don't have the extra parameter \\\\eta that you have in EControl, can you explain why for EC-Ideal this is not needed. Will EC-Ideal have the same convergence guarantees if you add \\\\eta in step 6, with the same values as for EControl?\\n-Bounded variance: This assumption contradicts strong convexity. I understand it is in several works in the literature. Take for instance F(x,y,z) = zx^2 + (1-z)y^2, z follows Bernoulli (1/2), then E_z(norm(\\\\nabla F(x,y,z) - \\\\nabla E_z(F(x,y,z)))^2) = (x-y)^2 and this is not bounded for all x and y in R.\\n-You mention in page 7 that \\\"The asymptotic complexity of EControl in this regime with stochastic gradient is tight and cannot be improved,\\\"  I understand for the depence on \\\\epsilon, however can you tell why the dependence on other parameters can not be improved.\\n-In the experiments, how you relate \\\\zeta to the noniddness, why for instance zeta equal to zero is iid and zeta >0 is non idd.\\n-Did you try \\\\eta= delta/400 suggested by the theory in the experiments? why you need to fine tune it, since it is fixed by theory. Is only fixed \\\\eta = \\\\delta/400 that works in theory or many other values can work, this is not clear in the paper.\\n-Did you test with other compressors?\"}"}]}
{"paper_id": "5JWAOLBxwp", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: 3D vision tasks, such as shape reconstruction, typically require inputs to be in a fixed canonical rotation, which limits their usage. Equivariant neural networks developed for 3D input, like Vector Neurons (VN), are capable of handling rotations but are limited by their reliance on three-dimensional feature representations, which are insufficient for capturing detailed 3D data.\nGap: Current SO(3)-equivariant networks, like Vector Neuron, struggle with capturing high-frequency details in 3D data due to their limited three-dimensional feature representation. The problem is that these networks cannot adequately map a 3D point to a high-dimensional feature space that acknowledges various frequencies present in 3D shapes.\nHypothesis: Introducing a new equivariant feature representation that maps 3D points into a high-dimensional feature space can improve the performance of equivariant networks like Vector Neurons by capturing multiple frequency details in 3D data.\nMethod: The authors propose a frequency-based equivariant feature representation (FER) that maps a 3D point to a higher-dimensional space. This involves constructing a mapping D from SO(3) to SO(n) such that the feature representation \u03c8 consists of sinusoids. The algorithm ensures rotation equivariance by representing orientation changes in a high-dimensional space, and is implemented in standard point processing networks like PointNet and DGCNN. The method involves proving mathematical conditions for D and integrating the representation into existing VN-based networks. Experiments cover shape classification, segmentation, normal estimation, point completion, and shape compression using datasets like ShapeNet and ModelNet40.\nConclusion: The proposed FER method enhances the ability of VN and VN-based networks to capture fine details by enabling them to process features with multiple frequencies. It demonstrates higher performance than traditional methods in various 3D vision tasks, addressing the limitations of prior SO(3)-equivariant models. Empirical results show that FER-equipped models outperform their counterparts in capturing high-frequency details, notably improving tasks like shape reconstruction and compression.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 2, \"Strengths\": \"*Originality:* The proposed approach appears novel. Although other approaches have used a Fourier like representation to represent 3D objects [1,2,3], this approach differs from those in that 1) the representation is intentionally high-dimensional, 2) only a subspace of the high-dimensional space is considered.\\n\\n*Quality:* In the experiments, proposed method performs on-par with or better than most baselines.\\n\\n*Clarity:* (see weakness)\\n\\n*Significance:* Designing expressive equivariant representations is a challenging and important problem in the field.\\n\\n\\n[1] Esteves, Carlos, et al. \\\"Learning so (3) equivariant representations with spherical cnns.\\\" Proceedings of the European Conference on Computer Vision (ECCV). 2018.\\n\\n[2] Cohen, Taco S., et al. \\\"Spherical cnns.\\\" arXiv preprint arXiv:1801.10130 (2018).\\n\\n[3] Kondor, Risi, Zhen Lin, and Shubhendu Trivedi. \\\"Clebsch\u2013gordan nets: a fully fourier space spherical convolutional neural network.\\\" Advances in Neural Information Processing Systems 31 (2018).\", \"Weaknesses\": \"*Quality:* \\n* The authors present a very critical view of methods that use spherical harmonics which I think is not only inappropriate but also incorrect. Spherical harmonics can be seen analogous to the Fourier basis which have been applied in signal processing problems for centuries, and the authors rely on for intuition building. Also, the authors state that spherical harmonics come from quantum mechanics, but the use of spherical harmonics predates the \\\"discovery\\\" of quantum mechanics by more than a century. \\n* There does not appear to be any discussion of the impact of choosing a higher dimensional input representation space. How does the choice of $n$ impact performance? What is the additional computational cost?\\n* The notation is inconsistent (see questions)\\n* Some proofs are missing (see questions)\\n\\n*Clarity:* I found the presentation to be difficult to follow. The paper uses advanced math concepts such as the Lie bracket without motivating or even naming them.\", \"Questions\": \"*Questions:*\\n- What is meant by \u201c$R^z(\\\\hat{u})$ is a rotation matrix defining the orientation measured from the $z$-axis to $\\\\hat{u}$.\u201d Is it the 2D rotation about the axis $z \\\\times \\\\hat{u}$ that aligns the $z$-axis and $\\\\hat{u}$?\\n- It seems like the map is from SO(3) to a subspace of SO(n) is that right\\n- Is the axis arbitrarily selected?\\n- Createsearchspace from Algo 2 is not described in the text, how does this work?\\n\\n*Possible typos:*\\n- \u201cTheorem 1. theoremIf\u201d \u2192 \u201cTheorem 1. If\u201d \\n- Are $\\\\hat{u}$ and $\\\\overrightarrow{u}$ used interchangeably in section 3? It would be clearer if the notation were consistent\\n- \u201cintuitively, just like F1, F2 and F3 represent angles\u201d \u2192 \u201cintuitively, just like F1, F2 and F3 represent axes of rotation\u201d\\n- Should \u201caxes \u03c8(\u02c6x), \u03c8(\u02c6y), and \u03c8(\u02c6z)\u201d be \u201caxes \u03c8(F_1), \u03c8(F_2), and \u03c8(F_3)\u201d? If not what does $\\\\hat{.}$ mean here?\\n- \u201c effective-yet-rotation-equivariant\u201d \u2192  expressive yet rotation equivariant\\n- In section D, readers are referred to the proof of Thm 1 for the proof of Prop 1, but the proof of Thm 1 does not prove Prop 1.\"}"}]}
{"paper_id": "4fH5ELoWFu", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Neural networks are prevalent in critical systems such as self-driving cars and face recognition. However, these systems are susceptible to adversarial examples, which are inputs that are subtly modified to mislead the model into incorrect predictions. To counter this, methods like adversarial training (which enhances empirical robustness but lacks certification) and certified training (which provides robustness certification but reduces accuracy) have been developed. Randomized smoothing is another method that provides certain robustness certifications but also sacrifices accuracy.\nGap: The existing challenge is to develop neural network models that are both certifiably robust and maintain high accuracy. Current approaches either compromise on the robustness certification (adversarial training) or sacrifice significant model accuracy (certified training). Randomized smoothing introduces noise, impacting accuracy as well. Therefore, a solution that concurrently achieves high accuracy and certified robustness without these drawbacks remains an open problem.\nHypothesis: The authors propose that achieving both high accuracy and certified probabilistic robustness is feasible. They suggest that deterministic robustness, which demands consistent predictions around an input, is restrictive and unnecessary. Instead, probabilistic robustness, where a model makes the same prediction most of the time within a certain vicinity, could be a practical and sufficient objective.\nMethod: The approach consists of two main components: 1) A novel training method that minimizes variance in predictions within a defined vicinity of an input, aimed at enhancing probabilistic robustness without overfitting to specific attack types. This is combined with empirical risk minimization to balance accuracy and robustness. 2) An inference method that certifies the probabilistic robustness of a model's predictions based on statistical testing. During inference, the method uses majority voting over samples from an input's vicinity to certify the prediction\u2019s robustness against adversarial attacks. The effectiveness of this method is demonstrated on multiple datasets with comparisons to state-of-the-art methods.\nConclusion: The proposed method achieves a high level of certified probabilistic robustness without sacrificing accuracy. It outperforms existing adversarial and certified training methods in both certification rate and accuracy across various datasets and perturbations. The method is also shown to defend effectively against adversarial attacks, providing a significant improvement over baseline methods in terms of defence success rates. Thus, the method successfully bridges the gap between robustness and accuracy for neural network models.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"There are a few main concerns with the paper (see Questions below) that need to be addressed. They may be based on a misunderstanding of the approach, and so appropriate answers from the authors would lead me to raise my score.\", \"Weaknesses\": \"My main concerns are raised in the Questions section.\\n\\nBut some minor details:\\n- in section 2 it was stated initially that h outputed a label. And yet later on in the same paragraph this was changed to logits. Precision/consistency is required\\n- writing G_x = arg max h(x) for example needs to make it very clear what the argument is they're searching over (vector index of logit output here)\\n- It is claimed in the introduction that randomised smoothing suffers from \\\"significant accuracy loss\\\". This is a broad statement to make as for example one can vary the variance of the added noise in order to vary the certified robust radius achieved (and also the impact on accuracy)\\n- In equation (2) it is not clear what the random variable (more importantly its distribution) is for the inner Probability\\n- near end of section 2 ... \\\"solution expects efficiency\\\" should be \\\"solution exhibits efficiency\\\"?\\n- in (1) it seems to be defining an adversarial example as a pair of points x1 and x2. Yet later on single points are referred to as adversarial examples. The definition (1) is a bit misleading\", \"Questions\": \"- In (3), conventionally I would have thought it should be G_x not G_t in the loss. i.e. you are trying to maximise the loss for the target label at x as t varies in the disk around x (also it is not made clear in the max specification that t is the free variable here)\\nThe same problem occurs in the discussion below equation (5). The text talks of computing the loss against the label of x and yet the expression in the bracket \\\"(i.e. E ....)\\\" has G_t used as the label\\n\\n- The distribution of t is not clear (in order to evaluate the expectation of the loss mean and variance). Is it uniform in the disk around x?\\n\\n- My main question is more fundamental. Firstly, it is not clear what classifier's output is being certified. Is it h or h^\\\\dagger\\n\\n(i) If h, then it is not sufficient to show that adversarial examples are sparse in the disk of radius epsilon around x. This was implied in the statement \\\"to establish certified probabilistic robustness, we must show that there is a guaranteed upper bound on the probability of adversarial example\\\".  But it doesn't seem to be sufficient. e.g. if there is one adversarial point x' in the disk around x and an attacker has some algorithm that allows them to find it, then the point x' is adversarial, regardless of the fact that such points are very rare.\\n\\n(ii) If h^\\\\dagger (which seems more likely what is meant), then the intent is to say that the majority class value is the classification at x, certified on the disk around x if the other classes are (statistically) sufficiently small. [Separate note - I believe there may a difference between equation (6) setting h as the expected logits value and then doing argmax for the class on that, and algorithm 2 which does the argmax at each sample point and then counts the occurrences of each class and takes the max. This is a point which is discussed in the certified robustness literature]\\nIf the claim that the vast majority of the points being of say class 1 in the disk is evidence that the classifier h\\\\dagger is always 1 on the disk, this seems wrong. If evaluating h^\\\\dagger at another point in the disk, we know nothing about the values outside the current disk. Hence the value of h^\\\\dagger at a point where say its disk has less than half its area in common with the disk around x is unknowable. Approaches to randomised smoothing are able to make assertions about the values across a disk (of a radius which depends on the logit differences at x), given their long-tailed noise distributions.\\n\\nPossibly this is a misunderstanding of what exactly is being claimed as being the certified robust classifier and what that property preciesly means. I look forward to clarifications.\"}"}]}
{"paper_id": "1NHgmKqOzZ", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Dataset distillation aims to minimize the time and memory needed for training deep networks by creating small sets of synthetic images that replicate the performance of large datasets. This is particularly useful for reducing the computational costs associated with training on large datasets and is relevant to fields like continuous learning, neural architecture search, and privacy-preserving machine learning.\nGap: Current dataset distillation methods fall short in performance compared to training on original datasets. They primarily use a single synthetic subset to match early training dynamics, which doesn't capture the entire training process, especially the later stages where neural network training dynamics change significantly.\nHypothesis: The authors propose that instead of using a single synthetic subset, multiple subsets should be used, each capturing different stages of the training dynamics. This approach can yield better generalization performance by capturing the full range of training dynamics in deep networks.\nMethod: The authors introduce Progressive Dataset Distillation (PDD), a pipeline that generates multiple small sets of synthetic images, each conditioned on the previous sets. Each set captures different stages of training dynamics, and the model is cumulatively trained on these subsets. PDD does not require additional training time compared to conventional methods.\nConclusion: PDD improves performance of existing dataset distillation methods by up to 4.3%. It also enables the generation of larger synthetic datasets, significantly closing the performance gap with training on full datasets. The progressive approach of capturing training dynamics in stages allows for better generalization, and PDD achieves near full dataset accuracy with only a fraction of the data size.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 5, \"Strengths\": \"The idea of progressive dataset distillation is interesting. This can 1) capture the training dynamic of neural networks better as demonstrated by authors, 2) reduce the complexity of training the whole synthetic dataset together, and 3) serve as a strong method for slimmable dataset condensation [a].\\n\\n[a] Slimmable Dataset Condensation, CVPR 2023.\", \"Weaknesses\": \"1. Many places are unclear.\\n    * Fig. 1 needs some explanations. How do the results come in detail, like what's the IPC of each stage, and how to conduct multi-stage training? Although some of these questions are answered in the following parts, the writing is not coherent.\\n    * The networks for the next stage come from the training results with previous batches of synthetic data. In IDC, the networks are periodically re-initialized randomly. In MTT, the networks come from checkpoints of training with original datasets. We have to conduct some modifications to these baselines before using PDD. These operations are unclear.\\n2. A comment: this method makes an assumption on downstream training using synthetic datasets: models must be trained in a multi-stage way using the provided multi-stage synthetic data, which would introduce a lot of hyperparameters, especially in the cross-architecture setting and make the dataset distillation less elegant. Given that the performance gain is not significant in most cases, the practical value of the proposed method is somewhat limited.\\n3. Through the results in Tab. 6, the effect of discarding easy-to-learn examples at later stages is not significant. More evidence is necessary to demonstrate the effectiveness.\", \"Questions\": \"Please refer to the Weaknesses part. I would also like the authors to discuss more benefits of PDD as mentioned in the 1st point of Strengths in the camera-ready version or in the next revision cycle.\"}"}]}
{"paper_id": "3s4fZTr1ce", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Reinforcement learning (RL) is effective in decision-making but struggles with sparse-reward environments. Large Language Models (LLMs) potentially enhance RL by providing policy guidance, leveraging their pre-trained knowledge. Prior work involved fine-tuning LLMs for better task alignment, but faced challenges such as inaccessible model weights and computational demands.\nGap: Despite LLMs' extensive knowledge, they often struggle to understand specific downstream tasks, reducing their efficacy in assisting RL agents. Fine-tuning LLMs presents practical challenges and can harm generalization, complicating deployment across diverse environments.\nHypothesis: Incorporating an adapter model within the RL framework can optimize the interaction between RL agents and LLMs without modifying LLM weights, enhancing their ability to provide useful guidance in complex environments.\nMethod: The RLAdapter framework introduces a lightweight adapter model that fine-tunes its understanding based on feedback from RL agents. It transforms LLM guidance into more applicable insights for the agent, using a partially observable Markov decision process and leveraging a lightweight language model. Evaluations were conducted in the Crafter environment with 22 tasks, comparing against state-of-the-art baselines.\nConclusion: RLAdapter significantly improves RL performance compared to baselines, showcasing enhanced common-sense behaviors and decision-making capabilities in agents. The framework allows for effective leveraging of LLMs without modification, presenting a promising avenue for further research. However, complex tasks still pose challenges.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"- Novelty and Clever Design: The introduction of an adapter model to bridge LLMs and RL algorithms is a novel idea. The adapter model helps in effectively summarizing and communicating the essential details from the RL agents to the LLMs.\\n- Empirical Evaluation: The empirical evaluation of RLAdapter in the Crafter environment and its comparison with several baselines demonstrate its effectiveness. The RLAdapter not only showed improved performance over state-of-the-art baselines but also exhibited agent behaviors grounded in common sense.\", \"Weaknesses\": \"- Understanding Score: The paper uses an understanding score calculated based on the semantic similarity between a trajectory and the LLM\u2019s sub-goals. However, it seems that this aspect is understudied in the paper. A more in-depth exploration and justification of the understanding score, possibly discussing its limitations, would have strengthened the paper. It\u2019s unclear whether it indeed does what the authors intended.\\n- It\u2019s unclear whether SFT is needed or even helpful. Perhaps few-shot-learning would be enough.\\n- In the ablation study, when removing the RLAdapter model, I believe the authors should have used the same prompts with the LLM. I was left thinking that perhaps the improved performance comes from a form of self-critique of the goal proposer before writing the next goal. The same could be achieved with prompting GPT.\\n- SPRING is mentioned quite few times, so I'd expect it to be better mentioned in \\\"related work\\\". In particular, examples of tight coupling with the environment.\", \"Questions\": \"- Analysis of Adapter Model's Capability: Does the adapter model have the capability of generating more general goals apart from making the LLM give more detailed goals? An exploration of this aspect would provide a more comprehensive view of the adapter model's functionalities.\\n  * If the adapter model only makes the LLM give more detailed sub-goals, we could simply prompt the LLM to analyze whether the last sub goal seems to be achieved and give more detailed sub-goals if not.\\n- Similar to the point above, it seems like the adapter model could be replaced by the LLM itself? Just querying it twice, one for analysis and another for sub-goal.\\n- Detailed Analysis: A more detailed analysis, such as an ablation study, on the understanding score, and how it influences the model's performance, could be beneficial.\\n- [Minor] The use of training steps in Table 1 as a metric might not be the most relevant. Consideration of other metrics like FLOPs could provide a more standardized comparison across different models. I understand this is probably not feasible given that OpenAI inference FLOPs are not public.\"}"}]}
{"paper_id": "WYsLU5TEEo", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Neural Networks (NNs) are widely used for binary classification in various fields like medical diagnosis and defect detection due to their precision and efficiency. However, they are difficult to interpret and vulnerable to adversarial attacks. Existing efforts to improve interpretability include generating counterfactual examples and techniques like Grad-CAM, but these often lack precision. Recent research explores GANs for counterfactuals to enhance interpretability. On the robustness front, projects enhance models by adversarial training methods such as PGD. Most current methods handle interpretability or robustness separately, without integrating both.\nGap: Existing strategies primarily address either improving interpretability or enhancing robustness of neural image classifiers, but not both simultaneously. Post-hoc interpretability approaches do not alter the classifier during the counterfactual training process, possibly missing opportunities to increase robustness.\nHypothesis: The authors propose that a unified framework using image-to-image translation GANs can generate counterfactual samples which both highlight salient regions for interpretability and act as adversarial samples to enhance dataset robustness.\nMethod: The paper introduces a framework combining a classifier and discriminator within a single model to produce and identify counterfactuals as 'fake'. The generator performs image-to-image translation to create samples attributed to opposite classes, augmenting robustness and interpretability. They tested on two datasets: a CASC IFW database for fruit defects and a Concrete Crack Segmentation Dataset, measuring performance against a semantic segmentation task and resilience to PGD attacks. Key components include specialized architectures for the GAN components, cycle-consistent loss term, and gradient update strategies to improve GAN stability and performance.\nConclusion: The framework effectively combines interpretability and adversarial robustness, achieving competitive IoU scores for saliency maps compared to traditional methods and resisting adversarial attacks better than conventional classifiers. The discriminator's output could further serve as a measure of prediction uncertainty, demonstrating both methodological and practical advances in image classification tasks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"- **Noteworthy Contribution**: The paper introduces a novel framework that has the potential to address the challenges of achieving robust and interpretable classifiers. This contribution is particularly relevant in the context of existing classifiers that often struggle with the trade-off between robustness and interpretability.\\n\\n- **Varied Evaluation**: The paper's assesses the proposed method using different model architectures, including convolutional networks and transformer models. This varied evaluation demonstrates the adaptability of the approach across different scenarios and model types, highlighting its potential for broader applicability.\", \"Weaknesses\": \"- **Limited to Binary Tasks**: A major limitation of the paper is that it only addresses binary classification tasks. It would be interesting to expand its applicability to multiclass problems to demonstrate broader utility, as mentioned in the discussion section.\\n\\n- **Single Seed Experiments**: The experiments in the paper are limited to training on a single seed, making it difficult to assess the significance of performance differences and the true impact of the proposed cycle consistency loss on convergence. Multiple seed experiments would provide a more robust evaluation.\\n\\n- **Experiment Clarity**: The presentation of experiments can be confusing and should be more detailed. For instance, the \\\"Hybrid D\\\" model is never introduced in the paper. The explanation of the computation of performance when using D is also presented *after* showing results. The description of Table 2 is also unclear, making it challenging for readers to understand the methodology and the comparison.\\n\\n- **Misleading Introduction**: The paper introduces the approach as \\\"combining classifier and discriminator in a single model\\\" (in the abstract), which is incorrect since the generator and discriminator are fundamentally different.\\n\\n- **Lack of Comparative Analysis**: The paper lacks a comparison with other counterfactual approaches, which could provide insights into the quality of the counterfactuals produced and help position the proposed method within the broader context of counterfactual research.\", \"Questions\": \"- It could be interesting to have a rule of thumb in which model to use, G or D ? Both seems to be strong for classification, but do they have their own advantages ?\\n- Can we do more than 1 cycle in the \\\"cycle consistency loss\\\" ? The loss is described with $c \\\\geq 1$, but experiments only show $c=1$ or $c=0$.\\n- I'm not sure about the conclusion of section 4.5 on the robustness of the classifiers. From Figure 3, we can see that the models trained with the proposed approaches both have *lower* F1 scores than the baselines when increasing the perturbation size. I assumed that the labels of the approaches are inverted in the plot. Can the authors clarify this ? Otherwise, the conclusion on robustness would be completely different.\"}"}]}
{"paper_id": "gwbQ2YwLhD", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Structure learning aims to uncover the underlying graphical structure from data, often represented by a Directed Acyclic Graph (DAG). DAGs are prevalent in applications like Bayesian Networks and causal research. Various methods exist to learn DAGs from data, with many relying on optimizing least square losses or log-likelihood losses. Previous research has indicated that these optimization processes can be influenced by the scale of data, particularly noted in low-dimensional settings.\nGap: While it is known that least square losses and performance of structure learning algorithms can be influenced by data scale in low-dimensional, linear settings, there is a lack of understanding and exploration of these issues in higher-dimensional and non-linear cases.\nHypothesis: The authors hypothesize that scale affects performance of structure learners, and that least square losses and log-likelihood losses can be minimal for incorrect DAGs in higher dimensions and non-linear cases. They aim to generalize previous findings to d-dimensional settings and show the scale impact on various loss functions used in structure learning.\nMethod: The authors provide theoretical proofs and conduct experiments to show conditions under which MMSE and other loss functions become minimal for incorrect DAGs. They utilize synthetic data and real-world datasets to test their theoretical findings across different graph structures and dependency types (linear and non-linear). The structure learning algorithms explored include NT, DAG-GNN, GraN-DAG, GES, and others optimized on MMSE, BIC, and ELBO losses.\nConclusion: The paper concludes that the scale of the variables significantly affects the prediction performance of structure learning algorithms. Specifically, MMSE and log-likelihood losses become minimal for incorrect DAGs due to scale, influencing both linear and non-linear relationships. The study emphasizes the need for caution when interpreting results from scale-sensitive losses and advocates for normalization and exclusion of free variances in loss calculations to improve robustness.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"the authors provide conditions under which using square-based losses will lead a wrong DAG result in structure learning. The conclusions drawn in the paper are reasonable.\", \"Weaknesses\": \"I think that the findings in the paper aren't unexpected. Learning causal structures should go beyond just predictive effects (MMSE). I don't see the paper as particularly significant.\", \"Questions\": \"1.\\tIn this article, MMSE seems to be used for prediction, rather than for learning causal graphs. The definition of MMSE is the sum of squares of predicted residuals, which characterizes the statistical correlation of variables rather than causal relationships. It is an obvious conclusion that only using statistical correlation for causal graph learning will lead to errors.\\n2.\\tIn structure learning algorithms, the score function usually contains penalty terms, and the variable that needs to be optimized in the score function is the adjacency matrix W, rather than data X. In addition, the adjacency matrix W will also be included in the penalty term. So, why does the score function directly degenerate into a square based loss function? Or, why is optimizing the score function equivalent to optimizing MMSE?\\n3.\\tIn practical applications, most of the score functions we choose are not affected by data normalization, just like the SRL function mentioned in this article. Thus, this article does not have a clear critical objective, or rather, the critical target seems to be rare in practical applications, which greatly reduces the significance of this study.\"}"}]}
{"paper_id": "yIEKq72cTE", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Federated Learning (FL) has become an effective framework for distributed machine learning, allowing multiple clients to collaboratively train a model while keeping their data decentralized. It has applications in various fields such as healthcare, personalized services, and more. However, FL's decentralized nature makes it susceptible to adversarial attacks, particularly data poisoning attacks, where malicious clients inject harmful data to manipulate the learning process.\nGap: Existing defense mechanisms for FL against data poisoning attacks have significant limitations, including a trade-off between precision and robustness, assumptions on i.i.d. data distributions, and strongly convex optimization problems. These approaches often fail in non-i.i.d. data scenarios, which do not align with real-world applications. There is a need for an effective method to mitigate such attacks without these stringent assumptions.\nHypothesis: The hypothesis is that a dynamic weighted aggregation method using Optimal Transport (OT) principles can effectively combat data poisoning attacks in Federated Learning by distinguishing and discarding malicious updates based on validation loss differences at the server, thereby improving resilience against such attacks.\nMethod: The proposed method, Federated Learning Optimal Transport (FLOT), introduces a dynamic weighting scheme and a loss function-based rejection mechanism to suppress malicious updates. It uses the Wasserstein barycentric technique to aggregate updates from client devices into a global model. The approach involves rejecting updates with significant loss deviations and applying OT to compute a barycenter for robust aggregation. Experiments were conducted on four datasets: GTSRB, KBTS, CIFAR10, and EMNIST, under various attack scenarios. Theoretical proofs for the method's Byzantine resilience and convergence were provided, and the approach was compared against several baseline methods.\nConclusion: FLOT demonstrates superior performance compared to existing methods in mitigating data poisoning attacks across different datasets and attack scenarios. It maintains high accuracy and scalability while effectively discarding malicious updates, evidenced by low validation loss dispersion in benign clients. The method shows significant improvements in time complexity and is robust under non-i.i.d. data conditions, highlighting its practical application value for secure FL.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. This paper proposes an optimal transport-based dynamic weighted federated aggregation method designed to mitigate untargeted data poisoning attacks within the FL framework. \\n2. The complexity of the proposed defense method has a substantial improvement compared to Krum.\\n3. The theoretical convergence results are provided.\\n4. Eextensive experimental results demonstrate that the proposed method consistently outperform other methods, including Byzantine robust aggregation rules, in terms of classification performance under both single-client and 50% Byzantine worker scenarios.\", \"Weaknesses\": \"1. The proposed method relies on validation data at the server. This validation data will affect the performance of the proposed defense method much. If the server cannot obtain the training data, the performance of the proposed method should be shown. \\n2. Some advanced defense methods are missing in the introduction and experiments, e.g., [1] and [2]. \\n3. The datasets used in this paper are too simple and limited to image classification tasks. Some real-world datasets, e.g., imagenet and cifar100, and other tasks should be considered, e.g., sentiment classification and multi-modal tasks.\\n4. The federated self-supervised learning framework should be considered in the experiments.\\n5. The convergence analysis is not meaningful, due to some impractical assumptions, e.g., Polyak-\u0141ojasiewicz (PL) inequality.\\n6. The attacker setting in this paper is also simple. There are some advanced model poisoning attack methods, e.g., [3] and etc. \\n\\nThe references mentioned are as follows:\\n[1] Xiaoyu Cao, Jinyuan Jia, Zaixi Zhang, and Neil Zhenqiang Gong. \\\"FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information,\\\" In IEEE Symposium on Security and Privacy, 2023.\\n[2] Sungwon Park, Sungwon Han, Fangzhao Wu, Sundong Kim, Bin Zhu, Xing Xie, and Meeyoung Cha. \\\"FedDefender: Client-Side Attack-Tolerant Federated Learning,\\\" In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2023.\\n[3] V Shejwalkar, A Houmansadr, \\\"Manipulating the byzantine: Optimizing model poisoning attacks and defenses for federated learning, \\\" In NDSS, 2021.\", \"Questions\": \"1. Please show the performance of the proposed method when the server cannot obtain the training data and enough training data. \\n2. Please add some advanced defense methods in the experiments, e.g., [1] and [2]. \\n3. The datasets used in this paper are too simple and limited to image classification tasks. Some real-world datasets, e.g., imagenet and cifar100, and other tasks should be considered, e.g., sentiment classification and multi-modal tasks.\\n4. The federated self-supervised learning framework should be considered in the experiments.\\n5. The convergence analysis is not meaningful, due to some impractical assumptions, e.g., Polyak-\u0141ojasiewicz (PL) inequality.\\n6. The attacker setting in this paper is also simple. There are some advanced model poisoning attack methods, e.g., [3] and etc.\"}"}]}
{"paper_id": "4u0ruVk749", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Estimating the Individual Treatment Effect (ITE) from observational data is crucial in various fields like healthcare and recommendation systems. Traditional methods often rely on the Ignorability assumption, implying no unobserved confounders exist, which is unrealistic. Generative models like VAEs and GANs have been used to simulate confounders but have limitations in model flexibility and stability.\nGap: Current methods primarily assume the absence of unobserved confounders, making the Ignorability assumption impractical in many real-world scenarios. Additionally, existing generative models for confounder simulation have stability and flexibility issues.\nHypothesis: The authors propose using a diffusion model to capture unobserved latent spaces, allowing for the estimation of causal effects under more realistic assumptions than prior models. This approach should improve ITE estimation by reconstructing meaningful unobserved confounders using a diffusion process conditioned on prior knowledge.\nMethod: The proposed method involves a two-step diffusion process to model unobserved confounders. A forward diffusion process converts observed confounders into a noise distribution while preserving useful information. A reverse process, parameterized as a Markov chain with deep neural network-estimated transitions, reconstructs the unobserved confounders. The model's performance is optimized using a variational lower bound and evaluated on both synthetic and benchmark datasets. Experimental comparisons are made with 12 state-of-the-art models using metrics like \u221a\u03f5PEHE and \u03f5ATE.\nConclusion: The diffusion model-based approach outperformed existing methods in estimating ITE by effectively handling unobserved confounders. This paper demonstrates that capturing latent confounders improves causal inference tasks, suggesting a promising direction for utilizing diffusion models in complex causal inference scenarios.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 1, \"Contribution\": 1, \"Rating\": 1, \"Confidence\": 4, \"Strengths\": \"Comprehensive numerical results are provided to support the proposed method\", \"Weaknesses\": \"1. The writing is extremely poor, making the reviewer wondering whether the draft was proofread before submission:\\n\\n- $\\\\epsilon_{PEHE}$ and $\\\\epsilon_{ATE}$ undefined before used in abstract\\n- in 2nd paragraph: \\\"(e.g.,medicine)\\\" --- missing space --- and \\\"(e.g., demographic characteristics )\\\" --- one extra space before \\\")\\\".\\n- in 2nd paragraph: \\\"DR\\\", which the reviewer assumed to be \\\"doubly robust\\\", is again used before definition\\n\\nIt is not the reviewer's responsibility to proofread the paper and it is highly recommended that a careful revision is needed before resubmission.\\n\\n2. Additionally, the review of existing work and motivation are unclear from the introduction:\\n\\n\\\"...and accordingly to infer the ITE in an unbiased settings...\\\"\\nWhat is an \\\"unbiased setting\\\"? Does it mean the estimator is unbiased?\\n\\n\\\"VAEs make substantially weaker assumptions in generating the structure of the hidden confounders Louizos et al. (2017), which could restrict the model\u2019s flexibility.\\\"\\nWhy a weaker assumption leads to more restrictive method?\\n\\n\\\"For examples, GAN-based methods could be unstable in modeling ITE due to the adversarial losses. VAEs make substantially weaker assumptions in generating the structure of the hidden confounders Louizos et al. (2017), which could restrict the model\u2019s flexibility. In order to address these challenges, in this paper, we propose to generate the unobserved confounders using diffusion model.\\\"\\n\\nBased on the above review of existing work, it is very, very unclear why the proposed diffusion method can handle the issues (and not to mention that those issues are not stated clearly at all).\", \"Questions\": \"See clarity questions above.\\n\\nMoreover, in table 1:\\n\\nHow is the hyperparameter selected? what is the criterion? Indeed, there is recent work (this year) showing that the criterion for hyperparameter selection is even more important than the model itself. Please look at:\\n\\nMachlanski, Damian, Spyridon Samothrakis, and Paul Clarke. \\\"Hyperparameter tuning and model evaluation in causal effect estimation.\\\" arXiv preprint arXiv:2303.01412 (2023).\\n\\nCurth, Alicia, and Mihaela van der Schaar. \\\"In search of insights, not magic bullets: Towards demystification of the model selection dilemma in heterogeneous treatment effect estimation.\\\" arXiv preprint arXiv:2302.02923 (2023).\\n\\nWei, Song, et al. \\\"Detecting Electricity Service Equity Issues with Transfer Counterfactual Learning on Large-Scale Outage Datasets.\\\" arXiv preprint arXiv:2310.03258 (2023).\\n\\nIf there were cross validation involved, please specify what error table 1 is reporting --- the current table cannot be compared to existing results in literature, e.g., table 1 in Dagonnet.\"}"}]}
{"paper_id": "4kLVvIh8cp", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Offline RL, also known as batch RL, involves learning an optimal policy from pre-collected data without real-time interaction with the environment. Recent advancements have seen offline RL utilizing deep neural networks and achieving high performance in domains like gaming, robotics, and control systems, typically using linear function approximation. However, these models can struggle with large state-action spaces encountered in real-world applications, prompting interest in non-linear function approximations.\nGap: While offline RL with linear function approximation has been extensively studied and optimal results have been achieved, there is a lack of algorithms for non-linear function approximation that offer instance-dependent regret guarantees. Solving this could enable more effective algorithms in complex function class scenarios.\nHypothesis: The authors propose that a pessimism-based algorithm for non-linear function approximation could offer statistically optimal performance in offline RL by providing instance-dependent regret guarantees.\nMethod: The authors introduce an algorithm called Pessimistic Nonlinear Least-Square Value Iteration (PNLSVI). It includes three components: variance-based weighted regression, variance estimation, and a planning phase using pessimistic value iteration. The algorithm calculates a D2-divergence to quantify uncertainty and uses it to provide a tight regret bound. It is designed to be oracle-efficient, needing efficient regression and bonus oracles. Empirical variance estimation and regression reduction are conducted alongside a regular ridge regression to manage uncertainties about the function class without needing all errors or an entire policy space covered.\nConclusion: The PNLSVI algorithm is the first to offer statistically optimal instance-dependent regret bounds for non-linear function approximation. It generalizes pessimism-based linear approaches, provides a tight dependency on function class complexity, and achieves minimax optimality in linear approximation contexts, confirming its effectiveness and efficiency compared to existing methods.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"- The paper proposes an algorithm for offline RL with non-linear function approximation that has instance-dependent regret guarantees, which is new in literature.\\n- The author extend techniques used for linear MDPs (including variance-weighted ridge regression and reference-advantage decomposition) to nonlinear function classes.\\n- The paper is generally well-written and clear.\", \"Weaknesses\": \"- This paper poses a very strong converage assumption (Assumption 3.5), which may not be realistic in practice. In contrast, most pessimism-based offline RL papers that I'm aware of adopt weaker partial coverage assumptions. I wonder how valuable it is to prove an instance-dependent bound when we have to impose a uniform coverage condition.\\n- The lower bound cited by the paper only works for linear cases. The authors do not provide a matching lower bound to showcase the optimality of the proposed algorithm with general function approximations.\\n- The authors do not provide any numerical experiments in the paper. In my opinion, adding numerical experiments can better showcase the benefits of the proposed algorithm compared to other offline RL algorithms with general function approximation.\", \"Questions\": \"- Which part of the proof requires uniform data coverage while partial data coverage does not work?\\n- This paper considers general function approximation, yet I'm unclear what types of function classes & instance structures will satisfy the conditions listed in the paper. Can you give some concrete examples beyond linear MDPs?\\n- Can you provide a lower bound for general function approximations?\"}"}]}
{"paper_id": "WZfatbNdLV", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Alternative splicing (AS) of pre-mRNA is a crucial, complex process influencing transcript diversity and phenotype, including disease. It results in multiple mRNA isoforms from one gene. Significant efforts have been made to predict or manipulate AS, crucial for uncovering novel regulatory mechanisms or designing therapeutic targets. Historically, splicing code models used manually curated features or deep learning, but limitations remain in accuracy and adaptability.\nGap: Despite advancements, accurately predicting and designing RNA splicing outcomes in a tissue-specific manner remains a challenging task, particularly due to the complexity and variability of splicing mechanisms across different cellular conditions and datasets.\nHypothesis: Using generative model architectures, especially multiple transformers combined with side information as in the TrASPr model, may more accurately predict and manipulate RNA splicing specific to conditions and tissues, outperforming current models and identifying regulatory features.\nMethod: The authors introduce the TrASPr model, using transformers for predicting tissue-specific RNA splicing. The model is pretrained on human RNA splice sites using a 6-mer tokenization strategy and fine-tuned with datasets from human and mouse tissues using MAJIQ for splicing quantification. TrASPr serves as an oracle for a Bayesian Optimization (BO) algorithm designed for RNA splicing outcome design. Evaluations involved comparisons with models like Pangolin on test datasets for PSI and dPSI prediction accuracy, and regulatory elements were tested using ENCODE RBP knockout data.\nConclusion: TrASPr significantly outperforms state-of-the-art models in predicting splice site inclusion and tissue-specific splicing changes, suggests biologically plausible regulatory mechanisms, and holds potential for generating accurate RNA sequence designs. The Bayesian Optimization framework (BOS) leveraging TrASPr can effectively propose sequence alterations for specific splicing outcomes, indicating its utility in experimental and therapeutic applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 5, \"Strengths\": \"While alternative splicing modeling has been extensively studied, systematic methods for designing/altering sequences are relatively new, and the authors' Bayesian optimization method appears promising.  RBP knockdown analysis is also interesting. The authors have also undertaken a significant amount of data processing and batch correction to facilitate model training and validation.\", \"Weaknesses\": \"This manuscript attempts to address two major problems but, unfortunately, does not succeed in either case. Here are the main issues:\\n\\n1. The authors focus on differential splicing and use splicing profiles across multiple tissues and species to train their model. However, they do not present any immediate benefits. For instance, they do not provide information on how much SpliceAI (Jaganathan et al., 2019) loses in performance, or how much Traspr gains, by having a universal (tissue-agnostic) model for splicing. In Section 4.1, the authors themselves acknowledge that \\\"[...] predictions for tissue-specific splicing changes were not very accurate, and we, therefore, did not include them here.\\\"\\n\\n2. The manuscript lacks convincing performance comparisons. It is challenging to believe that Traspr outperforms Pangolin (and subsequently SpliceAI) by such a significant margin (Spearman correlation 0.81 vs. 0.17). SpliceAI and other methods have undergone extensive testing by independent researchers. Furthermore, the approach for calculating cassette exon inclusion in Pangolin, as described in the manuscript, may not be optimal, as it ignores the upstream and downstream exon junctions. A more appropriate method can be found in the Methods section of the SpliceAI paper.\\n\\n3. Traspr appears to excel at predicting constitutive and non-constitutive exons but performs poorly for exons that fall in between (the truly alternatively spliced exons). It might provide more insight if the tested exons were categorized accordingly (e.g., see Fig. 1C in the SpliceAI paper).\\n\\n4. The manuscript lacks a comparison with the AE+MLP model, which is trained only on mouse data and is not considered state of the art.\\n\\n5. The authors only examine Daam1 mutations published in (Barash et al., 2010), while numerous datasets have been published since then, e.g., in (Xiong et al. 2015) and (Jaganathan et al., 2019), among many others.\\n\\n6. When evaluating the Bayesian optimization (BO) method for sequence design, other datasets could be considered, such as the SMA dataset in (Xiong et al. 2015).\\n\\nMinor Comments:\\n- Not all figure panels are labeled, and some figure references are incorrect, e.g., Fig. 1b in subsection 3.1.1.\\n- Some tables have two values in some cells, and it is not clear what these values represent, e.g., in Tables 1 and 2.\\n- Fig. 1 is too small and challenging to read.\", \"Questions\": \"I find the sequence design aspect of this manuscript interesting and promising. I have a couple of questions on that:\\n\\n1. Is the Transformer model truly necessary here? Could the VAE be used to obtain representations and then be fed into the MLP? Essentially, could a single splice junction model be trained instead of a Transformer and a VAE?\\n2. On the same topic, it would be beneficial to understand how a model like SpliceAI, followed by an MLP, would perform as the oracle. I'm not convinced that the Transformer-based splice site detection (with a 400bp context) outperforms SpliceAI with a 10kb context.\\n3. Assuming that a tissue-specific splicing model works (which needs to be demonstrated), can the BO method be adapted to modify AS only in a specific tissue, such as the brain?\"}"}]}
{"paper_id": "LndMyiBl3n", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Graph Neural Networks (GNNs) have become popular across various applications. Despite their success, GNNs are vulnerable to adversarial attacks that involve structural modifications, i.e., adding or deleting edges. This vulnerability has led to the development of different types of attacks, categorized based on the attacker's knowledge (white-box, grey-box, black-box). The most strict setting is the restricted black-box attack (RBA), where attackers know only the node features and graph structure, not labels or model parameters.\nGap: Most previous studies on graph adversarial attacks assume conditions like low-rank node embeddings or graph homophily, which limits their applicability, especially for heterophilic graphs. Heterophilic settings, where nodes of different classes are more likely connected, pose a unique challenge for existing models since they often rely on the homophily assumption.\nHypothesis: The authors propose that evaluating the quality of graphs using a modified silhouette score (MSS) can effectively guide attacks under the RBA setting. MSS aims to serve as a more generalizable approach across graphs with varying levels of homophily and heterophily.\nMethod: The authors introduce SheAttack, guided by a Modified Silhouette Score (MSS) that evaluates the quality of node embeddings within graphs. MSS is calculated based on intra-class and inter-class distances, generalized by replacing the minimal inter-class distance with an average, thus widening the objective's scope. SheAttack employs Greedy Randomized Block Coordinate Descent to optimize edge perturbations guided by MSS, providing scalable computations and effectiveness across different graph settings without prior label or model knowledge.\nConclusion: SheAttack demonstrates strong performance in degrading GNNs across homophilic and heterophilic graphs, comparable to white-box attacks without relying on additional split information. It is efficient and scalable, achieving success without prior knowledge of node labels or victim models, and adapts to both poisoned and evasion settings effectively.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The studied restricted black-box attack is a practical scenario and an important problem.\\n2. The motivation of this paper is clear and the idea of introducing the Silhouette Score to measure the quality of a graph is interesting.\\n3. Extensive experiments are conducted.\", \"Weaknesses\": \"1. The proposed SheAttack depends on various hyper-parameters, which may significantly affect the performance of the proposed method and lack detailed theoretical support or empirical analysis. For example, the number of clusters $k$, the propagation layer/time on the adjacency matrix, and the $\\\\lambda$ that balance the Shift Loss and Silhouette Score-based Loss. \\n2. The proposed method does not seem to be consistently effective in all scenarios in Tables 1&3.\\n3. The writing of this paper needs to be further improved. For example, the citation format of references in Introduction and Preliminaries seems strange. 'Aggreation function' should be 'Aggregation function\u2019. The definition of poison attacks and evasion attacks on page 3 is confusing.\", \"Questions\": \"1. In Figure 3, why do GRBCD and RandAttack show different Modified Silhouette Score at epoch 0?\\n2. How would the proposed method perform when the victim model is not a two-layer GNN? In other words, when the propagation layers/times in the proposed method and victim model are not the same, would the proposed method still be effective?\"}"}]}
{"paper_id": "mFTPRV5hYw", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Location-based services (LBS) have become increasingly popular due to their ability to provide personalized assistance, such as traffic management and point-of-interest (POI) recommendations, by leveraging large volumes of human mobility data. However, the privacy of such mobility data is a growing concern, as it can potentially reveal sensitive user information, such as their home/work locations.\nGap: Existing research on mobility data privacy primarily focuses on protecting data during transmission or before release, often neglecting the evaluation of privacy risks associated with machine learning (ML) models that use this data, particularly POI recommendation models.\nHypothesis: The authors hypothesize that POI recommendation models, like other ML models, are vulnerable to specific privacy attacks, and evaluating these vulnerabilities requires specialized attack techniques to assess the risk of privacy leakage in mobility data.\nMethod: The authors developed a privacy attack suite specifically for POI recommendation models trained on mobility data. This suite includes data extraction and membership inference attacks. Data extraction attacks involve extracting common locations and full user trajectories, while membership inference attacks determine whether certain locations or trajectories were part of the training data. They tested these attacks on three POI recommendation models using two real-world mobility datasets. The research also included evaluations of various defense mechanisms against these attacks, such as differential privacy and other standard machine learning techniques.\nConclusion: The study demonstrates that current POI recommendation models are susceptible to the proposed privacy attacks, as evidenced by the high success rate of these attacks in experiments. It also reveals that existing defense mechanisms are not fully effective at mitigating these privacy risks without substantial utility drop, highlighting the need for more targeted defenses to protect sensitive information in mobility data.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 2, \"doi\": 10, \"Strengths\": \"1. The research problem is important.\\n2. The paper analyzes the factors in the data that affect the attack performance.\\n3. The proposed attacks for data extraction and membership inference attacks are simple yet effective.\", \"Weaknesses\": \"1. The appropriate baselines are missing. Can the existing data extraction or membership inference attacks be applied to the POI recommendation models, e.g., [1]?\\n2. There is no qualitative result analysis on the data extraction attacks. It would be better if the authors could conduct these analyses on the data extraction attacks.\\n3. The threat models assume that the adversaries are capable of accessing the confidence scores, which makes them impractical. In practice, the model owner only releases the final result to the users. In this case, whether the proposed attacks are still effective is unknown.\\n\\n\\n\\n\\n[1] R. Shokri, M. Stronati, C. Song and V. Shmatikov, \\\"Membership Inference Attacks Against Machine Learning Models,\\\" 2017 IEEE Symposium on Security and Privacy (SP), San Jose, CA, USA, 2017, pp. 3-18, doi: 10.1109/SP.2017.41.\", \"Questions\": \"N/A\"}"}]}
{"paper_id": "x4OPJ7lHVU", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: In-context learning (ICL) enables large language models (LLMs) to adapt dynamically based on contextual exemplars, improving accuracy and relevance without fine-tuning, which is increasingly valued for integrating private data sources.\nGap: ICL responses can inadvertently leak sensitive private information contained in in-context exemplars, creating a privacy risk similar to traditional fine-tuning methods.\nHypothesis: The authors propose Differentially Private In-context Learning (DP-ICL), a paradigm that uses a noisy consensus mechanism among an ensemble of LLM's responses based on disjoint exemplar sets to generate differentially private responses.\nMethod: DP-ICL involves partitioning exemplar data into subsets, generating exemplar-query pairs, and processing these pairs through LLMs. The outputs are aggregated using differentially private mechanisms such as the Report-Noisy-Max with Gaussian noise for text classification, and embedding or keyword space aggregation for language generation tasks. Evaluation was conducted on text classification benchmarks (SST-2, Amazon, AGNews, TREC) and language generation tasks (DocVQA, SAMsum), showing a balanced trade-off between privacy and utility.\nConclusion: DP-ICL achieves a strong utility-privacy tradeoff, providing privacy guarantees while maintaining competitive performance compared to non-private methods. It paves the way for trustworthy and privacy-preserving applications of LLMs in sensitive domains.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 4, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"- This paper is well-written and well-organized. \\n- The proposed paradigm is novel and is a novel paradigm for privatizing ICL tasks.\\n- The authors provide a clear and concise explanation of the DP-ICL paradigm. The figures and captions are easy to understand.\\n- Experiments on 4 benchmark datasets are extensive and sufficiently demonstrated the promise of the approach towards trustworthy usage of large language models.\", \"Weaknesses\": \"- Limitations can be discussed in more details. For example, \u201ca better-trained embedding-to-text model may yield further improvements in the model performances.\u201d Then what can be the potential candidate models here and why did the authors not adopt these models?\", \"Questions\": \"- How do you see the DP-ICL paradigm being applied in real-world scenarios, and what are some of the challenges that would need to be addressed?\\n- What are some of the limitations of the DP-ICL paradigm, and how might they be addressed in future research?\"}"}]}
{"paper_id": "6CZ50WgfCG", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Reinforcement Learning (RL) heavily relies on dense reward functions, often requiring significant human expertise to craft. Sparse rewards, while easier to obtain, pose challenges for RL due to exploration difficulties.\nGap: Existing methods for reward learning, such as Inverse RL and Adversarial Imitation Learning, do not focus on reward reuse for new tasks. These methods often result in rewards that cannot be effectively transferred to different tasks due to diminishing differences between agent and demonstration trajectories over time.\nHypothesis: The authors propose that it is possible to learn dense rewards for multi-stage tasks in a data-driven manner that are reusable across tasks with similar success conditions, reducing the human effort required to engineer rewards.\nMethod: The authors introduce DrS (Dense reward learning from Stages), which learns reusable dense rewards by leveraging the stage structures of tasks. The approach involves training discriminators to classify success and failure trajectories based on binary sparse rewards and stage indicators, with the discriminators providing dense rewards. The method is evaluated on the ManiSkill benchmark, across three task families: Pick-and-Place, Turn Faucet, and Open Cabinet Door, involving over 1000 task variants.\nConclusion: DrS effectively learns high-quality dense rewards that are reusable in unseen tasks, improving the performance and sample efficiency of RL algorithms compared to sparse rewards. The learned rewards achieve comparable results to human-engineered rewards in some tasks and substantially reduce the human effort needed in reward engineering. The method is robust to various stage configurations and has shown compatibility with different modalities of reward input.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"- This paper makes a contribution towards automating reward design, which is of paramount importance in the field of RL. Having access to dense rewards takes the burden off of exploration, which in turn reduces the number of samples required to solve a task.\\n- The method is a niche application of contrastive discriminator learning, which is well-established in the literature.\", \"Weaknesses\": \"- The method requires success and failure trajectories for each stage in the training data, which can be expensive to collect. \\n- The scope of the method is limited to a family of tasks that can be divided into stages. This prevents it from being applied to other tasks such as locomotion. It also means the method is less general compared to LLM-based rewards with external knowledge [1, 2].\\n- Similarly, the need for stage indicators prevents the method from scaling to real-world problems, which would arguably benefit more from automated reward design than simulated domains.\\n- The experiments do not demonstrate new skills that are unachievable by human-designed rewards.\\n\\nReferences:\\n1. Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, and Tao Yu. Text2reward: Automated dense reward function generation for reinforcement learning. arXiv preprint, 2023.\\n2. Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi \u201cJim\u201d Fan, Anima Anandkumar. Eureka: Human-Level Reward Design via Coding Large Language Models. arXiv preprint, 2023.\", \"Questions\": \"- Does the method work with pixel observations?\\n- If the method only works with states, does generalization come as a result of state abstraction or the method itself?\\n- Do the learned dense reward functions resemble hand-designed rewards?\"}"}]}
{"paper_id": "DOerIFfUbs", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Contrastive pre-training on image-text pairs, such as CLIP, has become the standard for learning multi-modal visual-language representations. CLIP allows for applications like zero-shot image classification and retrieval. However, it requires extensive computational resources to train from scratch on noisy web datasets. Meanwhile, Masked Image Modeling (MIM) offers an efficient alternative for pre-training single-modal representations but lacks the multi-modal capability without additional steps.\nGap: There is a need to enhance the efficiency and effectiveness of training methods like CLIP while avoiding the drawbacks of MIM's training-finetuning inconsistencies and significant computational demands.\nHypothesis: The authors propose that Unmasked Token Alignment (UTA) can efficiently enhance CLIP's vision-language representations by training a Vision Transformer (ViT) to align unmasked visual tokens with a frozen CLIP vision encoder, effectively aligning it with the CLIP text encoder without the need for [MASK] tokens.\nMethod: UTA trains a Vision Transformer (ViT) without using [MASK] tokens by aligning unmasked visual tokens with a frozen CLIP vision encoder. The process involves masking a portion of image tokens and using the unmasked ones to align with CLIP features, maximizing cosine similarity for alignment. The ViT model can then be applied in zero-shot settings. UTA maintains training-finetuning consistency by only processing unmasked tokens, reducing training FLOPs by up to 50%. Experiments were conducted on ImageNet-21K and other datasets to assess performance in various zero-shot and fine-tuned tasks.\nConclusion: UTA effectively enhances the alignment of vision-language representations by leveraging pre-trained CLIP models, achieving strong zero-shot evaluation performance and outperforming existing methods on various benchmarks. It is training-efficient and maintains training-finetuning consistency, but still benefits from further fine-tuning on large datasets.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"-It is a good idea to perform unmask token alignment with the pretrained CLIP model due to its efficiency. \\n\\n-The experiments are quite extensive including many downstream tasks. The method also achieves the SOTA on most of the datasets.\", \"Weaknesses\": \"-Motivation of reducing pretraining costs is not convincing. In particular, the abstract claims that \\\"training CLIP from scratch on noisy web-scale datasets is computationally demanding\\\". Although this is true, this paper does not solve this issue at all because it still relies on a pretrained CLIP model at the first place.\\n\\n-The performance gains seem to come from the contrastive finetuning rather than the proposed unmasked alignment pretraining. In Table 1, comparing with the CLIP teacher i.e., EVA-CLIP, there is always a performance drop for UTA without finetuning. This is concerning because this paper claims the unmasked alignment pretraining as one of the main contributions. \\n\\n-One important baseline is missing. Since finetuning is very effective for zero-shot image classification, this paper should also compare with the CLIP teacher i.e., EVA-CLIP,  that is also finetuned on the same dataset. \\n\\n-Improving the VIT efficiency by dropping masked tokens has been done in [A].  This paper fails to cite this important reference paper and claims it to be something new. \\n\\n[A] Li et al., Scaling Language-Image Pre-training via Masking. CVPR 2023\\n\\n-The improvement in Table 4 & 5 (detection and segmentation) is marginal (< 1%). This is also concerning. The method seems to be limited to image-level prediction tasks.\", \"Questions\": \"The authors are highly encouraged to address my questions mentioned in the weakness. In addition, I have the following questions. \\n\\n-Is the CLIP teacher-model always the giant EVA-CLIP? \\n\\n-It would be good to provide the CLIP teach results in Table 3, 4, & 5.\\n\\n-This paper says that it is following previous works to perform the second-stage contrastive finetuning without proving the reference. Please provide the reference. \\n\\n-Is finetuning helpful in Table 4 & 5?\\n\\n-Is masking strategy applied in the finetuning stage?\"}"}]}
{"paper_id": "ISrxxvXJQO", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: In recent years, deep learning has become a dominant force in computer vision, excelling in tasks like image recognition, object detection, segmentation, and image generation. While these achievements are impressive, the community has not fully explored shared mathematical properties underlying images. This research seeks to understand images through mathematical invariance, offering insights into their representation.\nGap: The fundamental mathematical properties shared by images remain largely unexplored. Current methods have not adequately addressed this question, focusing more on performance metrics rather than underlying principles.\nHypothesis: The authors propose that images can be effectively approximated using a set of hidden one-way wave equations, referred to as the 'hidden waves phenomenon.' These equations have latent, learnable speeds that are shared across images, introducing a new mathematical perspective for understanding image data.\nMethod: The method involves leveraging a visual encoder, such as a convolutional neural network, to compute unique initial conditions for images, which correspond to solutions of one-way wave equations. The solutions manifest as collections of special solutions that demonstrate first-order autoregressive properties with shared coefficient matrices. The authors implemented this methodology using ImageNet data, achieving notable reconstruction performance by relaxing local constraints and applying linear autoregression models, termed FINOLA, to approximate initial conditions.\nConclusion: The study reveals a fundamental mathematical property where images can be represented within a compact space defined by one-way wave equations with hidden, learnable speeds. This contributes a novel understanding of shared invariances in image data, facilitating reconstruction with minimal loss of fidelity. The hidden waves phenomenon illuminates invariant properties across diverse images, potentially impacting future image analysis and representation techniques.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"- Through mathematical enhancements and experiments, they develop a deep understanding of FINOLA, which is a general wave representation of an image.\\n- The section comparing \\\"hidden wave representation\\\" with image autoregression or transformations is well-written and provides clear insights into this encoding.\\n- They conducted numerous experiments to validate the improvements they implemented.\", \"Weaknesses\": \"- Experimental design\\n\\nDrawing conclusions from the current results is uncertain due to the limited number of data points and the lack of significant differences. For instance, the interpretation of Table 3 remains unclear. Should we consider choosing initial condition positions different from the center? Overall, experiments were conducted from various perspectives, they do not yield meaningful insights or conclusions.\\n\\n- The phenomenon is undoubtedly intriguing and interesting, it remains unclear how this method can be applied to future research or practical applications. The paper would be improved by including discussions on potential future perspectives.\", \"Questions\": \"- Were all images resized to the same aspect ratio?  The aspect ratio can impact wave speed, and it may be more natural to preserve the aspect ratio from the original images.\\n- Conventional image compression methods like DCT transform the image into a linear composition of orthogonal basis. This is simple and provides a straightforward understanding of each coefficient. The paper effectively describes the differences, but what are the main strengths and weaknesses of this method?\\n- Is the selection of hyperparameters for training this method not very sensitive? How long does the training process take?\\n\\nMinor comments:\\n- Use ` for opening quotation mark in latex. Currently ' ... '.\\n- There are some missing brackets for citations. For example, ... known as FINOLA Chen et al. (2023).\"}"}]}
{"paper_id": "jFox1iMWUa", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Causal inference is essential in many fields such as medical decision-making, advertising evaluation, and voucher distribution, where it aims to estimate the causal effect of treatments on outcomes using observational data. Traditional approaches struggle with bias due to confounding factors and often only rank samples by uplift rather than precisely estimating treatment effects, especially when treatments are continuous.\nGap: Existing methods for causal inference often fail to adequately model the response performance while handling continuous treatments and mitigating confounding bias. Most previous work has focused on categorical or binary treatments and has not sufficiently addressed continuous treatments or provided unbiased uplift estimation in the presence of confounders.\nHypothesis: An end-to-end multi-task deep neural network can effectively capture the relationship between treatment propensity and treatment effect in scenarios with continuous treatments, thereby providing better uplift modeling by balancing estimation of response performance and individual treatment effect.\nMethod: The authors propose a three-headed neural network structure comprising logistic, monotonicity, and Diffloss networks to estimate continuous treatment effects while addressing confounder bias. The approach involves embedding treatments and features into networks to predict outcomes based on mutually independent factors. This architecture is tested using semi-synthetic and real-world datasets, and its effectiveness is further evaluated by implementing it in an online voucher distribution system on a major ride-hailing platform.\nConclusion: The proposed method manages to balance response estimation and uplift ranking, outperforming benchmarks in terms of estimation accuracy for individual treatment effects (ITE) and uplift. Online testing in a real-world application showed improved market scale and better return on investment (ROI) compared to existing models.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 1, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"Continuous treatment effect estimation is a significant problem in causal inference.\", \"Weaknesses\": \"- **The statement \\\"The following mild assumptions are made (Holland (1986); Kennedy et al. (2017))\\\" does not correspond to reality. The Monotonicity Assumption is not mentioned in (Holland (1986); Kennedy et al. (2017)), and the Consistency Assumption also differs from the one proposed by Kennedy et al. (2017). Furthermore, there are many other statements in this paper that do not match reality.**\\n\\n- The writing needs significant improvement. The survey on relevant literature is not comprehensive. \\n\\n- The statement \\\"Traditional methods focus on the ordering of the problem. Little attention has been paid to the response performance, neither on the evaluation metric, nor the modeling\\\" is confusing. In both binary and continuous settings, researchers have recently proposed numerous estimators to predict CATE or ITE, such as Johansson et al. (2016), Shalit et al. (2017), Bica et al. (2019; 2020), Schwab et al. (2020) and Nie et al. (2021). Recent deep methods typically use PEHE or MSE on CATE to evaluate the response performance, and none of them focus on the ordering of the problem. This statement contradicts my understanding.\\n\\n- The statement \\\"Not to mention that most of the works limit to categorical treatment\\\" is incorrect. Recently, many machine learning IV algorithms have been proposed to identify CATE and are not limited to categorical treatment, such as NPIV, DeepIV, KernelIV, DualIV, FastIV, DFIV, CBIV, AGMM, and DeepGMM.  **The survey on relevant literature is not comprehensive.** \\n\\n- The statement \\\"Most works in causal inference area focus on the binary or discrete treatment case. The continuous treatment case is less studied\\\" is incorrect. Many studies have been conducted on estimating continuous treatment effects, particularly in predicting average dose-response function (ADRF) or average dose-response curve (ADRC) to identify causal effects. Some notable works in this area include (Robins et al., 2000), (Hirano & Imbens, 2004), (Imai et al., 2004), (Flores et al., 2007), (Flores et al., 2012), (Kennedy et al., 2017), (Wilson et al., 2018), (Kallus et al., 2019), (Colangelo et al., 2020), (Sharma et al., 2020), (Klosin, 2021), (Linton & Zhang, 2021), (Zhang et al., 2022), and many others. **The survey on relevant literature is not comprehensive.** \\n\\n- That statement \\\"Conditional average treatment effect (CATE) is known as causal inference\\\" is incorrect. Conditional average treatment effect (CATE) is not synonymous with causal inference. CATE refers to the average effect of a treatment (intervention) on an outcome under a specific condition. Causal inference, on the other hand, involves analyzing causal relationships to determine the effect of a treatment (intervention) on an outcome of interest. CATE is one method used in causal inference, but they are not identical concepts.\\n\\n- **Algorithm**: The feature X is a common cause of treatment T and outcome Y, which could potentially confound the causal effect of T on Y. Although this paper uses lossdiff to make embedding b independent of treatment T, the embedding factor w may remain confounders of the causal effect of T on Y. How to address the confounding bias from w?\\n\\n- **Experiments**: Due to the incomplete literature review, the paper lacks a comprehensive evaluation and comparison with existing methods, such as GPS (Imbens, 2000; Fong, 2018), DRNet (Schwab et al., 2020), SCIGAN (Bica et al., 2020), VCNet (Nie et al., 2021), making it difficult to assess the practical contribution of the proposed approach.\", \"Questions\": \"See Above.\"}"}]}
{"paper_id": "39cPKijBed", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Diffusion models have made significant advancements, becoming a dominant framework in generative modeling tasks such as high-fidelity image generation, image-to-image translation, and counterfactual generation. These models are typically trained on large datasets, which often contain biases that can affect the quality and fidelity of generated samples. Dataset bias in generative models can perpetuate or even amplify the biases present in the training data, leading to a distribution that does not accurately reflect the intended or unbiased distribution of target outputs.\nGap: The generative outputs of diffusion models suffer from biases present in the datasets they are trained on. Traditional methods to mitigate this include using techniques like importance reweighting, but these approaches encounter issues such as the imprecision of density ratio estimation, which leads to the propagation of errors during model training. There's a need for a more precise method to estimate the density ratio and mitigate these biases effectively.\nHypothesis: The authors propose a novel method utilizing time-dependent importance reweighting for training diffusion models that can effectively mitigate dataset bias. They hypothesize that by using a time-dependent density ratio both for reweighting and score correction, they can achieve a tractable and more precise form of the objective function, which allows the trained model to converge towards an unbiased data distribution.\nMethod: The authors introduce Time-dependent Importance reWeighting (TIW) for diffusion models. This approach involves estimating a time-dependent density ratio between the biased and unbiased distributions using a time-dependent discriminator. They use this density ratio for both weighting and score correction in the model training process. The proposed method uses a stochastic differential equation framework to learn the time-dependent changes in the data distribution, allowing for the mitigation of bias. The method's performance is validated through experiments on datasets like CIFAR-10, CIFAR-100, FFHQ, and CelebA against baselines like time-independent reweighting, demonstrating superior performance in mitigating bias with various dataset settings.\nConclusion: The proposed time-dependent importance reweighting method effectively reduces bias in diffusion model training, outperforming existing baselines. The theoretical and experimental results show that the method converges to an unbiased distribution more reliably, addressing the corruption of generated outputs due to dataset bias. The approach is substantiated by results on diverse datasets with varying bias intensities, highlighting its practical applicability and significance in enhancing the generative quality of diffusion models.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"1. The paper builds on successful recent advances in diffusion models, which excel in high-fidelity image generation within generative learning frameworks. Thus, the proposed TIW method shows promise for diverse applications, including text-to-image generation, image-to-image translation, and video generation.\\n2. The paper addresses the understudied issue of dataset bias in generative modeling, introducing a method to enhance the fairness and reliability of ML systems.\\n3.The paper establishes a theoretical equivalence between the proposed method and existing score-matching objectives from unbiased distributions, which could provide a strong foundation for the validity of their approach.\", \"Weaknesses\": \"1. The paper builds upon a large body of previous work, which while showing the paper\u2019s relevance, also means that any limitations or issues in those previous works could potentially affect the validity and effectiveness of TIW. The actually implementation process is similar to previous work on GAN.\\n2. The paper shows that it mitigates the bias in the dataset, but I am not sure whether that method would influence the overall generation quality. It is important to see detailed qualitative and quantitative analysis.\", \"Questions\": \"N/A\"}"}]}
{"paper_id": "9qtswuW5ux", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Combinatorial optimization (CO) is a crucial area in computer science characterized by selecting combinations of elements to minimize costs within problem constraints. These problems are often NP-hard, making exact solutions impractical for large instances. The rise of Graph Neural Networks (GNNs) offers promising alternatives for solving CO problems due to their ability to learn patterns within graph data. The Quadratic Unconstrained Binary Optimization (QUBO) problem formulation is a valuable approach, as it can represent numerous CO problems, including Max-Cut and graph coloring, making algorithms addressing QUBO solutions especially significant.\nGap: Although there have been various approaches leveraging GNNs for CO problems, including supervised methods requiring labeled training instances and reinforcement learning with scalability challenges, the use of unsupervised GNNs leveraging QUBO as a loss function has been debated regarding their solution accuracy and efficiency compared to classical algorithms.\nHypothesis: The paper proposes that a novel unsupervised QUBO-based Graph Neural Network with a Recurrent Feature (QRF-GNN) will improve performance for solving combinatorial optimization problems by using recurrent features and parallel convolutional layers, leading to results competitive with state-of-the-art heuristics.\nMethod: The proposed method, QRF-GNN, employs a GNN architecture with a recurrent feature that uses previous predictions to improve current predictions in an unsupervised learning setup. The architecture involves parallel convolutional layers implementing different aggregation functions, alongside using artificial node features. It optimizes a loss function based on QUBO relaxation. The method was tested on benchmark datasets for maximum cut and graph coloring problems, using various performance metrics and comparisons against existing GNN and heuristic methods.\nConclusion: QRF-GNN demonstrates superior performance compared to existing unsupervised GNN approaches and is competitive with state-of-the-art heuristics, particularly in terms of solution accuracy and computational efficiency for large graphs. This suggests that unsupervised learning with QUBO loss functions and recurrent features can provide viable solutions for CO problems, bridging a gap in current methodologies.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. Using Neural Networks to solve combinatorial optimization problems is an interesting topic.\\n2. The proposed QRF-GNN can outperform existing GNNs in solving Max-Cut and graph coloring problems.\\n3. The paper is well-written and easy to follow.\", \"Weaknesses\": \"1. The paper's novelty appears constrained, utilizing a GNN with QUBO's continuous relaxation as a loss function for combinatorial optimization. Clarification on how this differs from prior work would be beneficial.\\n\\n2. The motivation behind the specific design of the GNN, particularly the use of two different convolutional layers (SAGEConv Pool and SAGEConv mean) in the first layer, is not explained. A rationale for these choices should be provided.\\n\\n3. An ablation study is needed to substantiate the proposed GNN's superiority over existing methods. Without this, the advantage of the proposed model remains unclear.\", \"Questions\": \"Please refer to the weaknesses.\"}"}]}
{"paper_id": "gqzknNJShg", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Resource-constrained vision tasks, like image classification on low-end devices, face challenges due to limited computational resources and restricted access to large training samples. Prior research has used data augmentation to learn lightweight models with fewer samples, but this often requires scenario-specific optimization and does not fully exploit frequency components accessible via Fourier analysis.\nGap: Conventional data augmentation methods for resource-constrained environments require optimization for specific scenarios or do not exploit frequency components from Fourier analysis, leading to models that may not learn discriminative high-frequency features effectively.\nHypothesis: The hypothesis is that by using a frequency-based image encoding method (FourierAugment), lightweight models can learn richer features from limited data by explicitly providing information from various frequency bands.\nMethod: FourierAugment is a frequency-based image encoding method that uses discrete Fourier transformation (DFT) to separate frequency components of an image. It provides these components to a lightweight model, aiming to learn both low- and high-frequency information in balance. The method does not complicate model architecture or increase computation and involves transforming images into frequency domain and back for feature extraction.\nConclusion: FourierAugment significantly enhances the performance of lightweight models under resource constraints by promoting balanced learning of frequency components. It achieves state-of-the-art performance in multiple benchmarks, is easily applicable to existing models, and supports richer feature learning without architectural complexity or computational increase.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"1. The introduction of FourierAugment as a frequency-based image encoding method represents a novel and innovative approach in addressing resource-constrained vision tasks. FourierAugment's unique utilization of frequency components sets it apart from conventional data augmentation methods.\\n2. The paper's empirical study is thorough and well-designed, ensuring the validity and reliability of the results. Extensive experiments on multiple datasets and resource-constrained vision tasks provide a strong foundation for the paper's claims.\\n3. The paper is well-structured, with a clear presentation of the problem statement, method development, empirical study, and results.\", \"Weaknesses\": \"The paper could benefit from a more comprehensive theoretical background and motivation section. It's important to provide a clear foundation for why FourierAugment was developed and the specific theoretical underpinnings. A deeper exploration of the relationship between frequency components and lightweight model learning could enhance the paper's overall coherence.\", \"Questions\": \"1. The theoretical underpinnings and motivations for proposing FourierAugment, including the specific theoretical underpinnings that guided its development, would add to the coherence of the paper if they were added in full.\\n2. The n value is chosen empirically? Is there a definite basis for setting 2 or 3?\\n3. How does the method proposed in this paper compare to the comparative baseline AugMix, RandAugment and Deep AutoAugment in terms of computational efficiency and generalizability to different visual tasks?\\n4. Does adding the image enhancement proposed in this paper to the image classification and FSCIL tasks significantly extend the processing time of the tasks?\"}"}]}
{"paper_id": "XbLffB0T2z", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Availability data poisoning attacks involve introducing small, imperceptible changes to training data to degrade the test accuracy of machine learning models. This type of attack is particularly relevant given the prevalence of untrusted data sources used in training models and the threat they pose to model robustness.\nGap: Existing poisoning strategies typically assume that the adversary and the victim use the same learning algorithm, limiting their transferability across different learning paradigms. The effectiveness of such attacks decreases significantly when the victim employs a different training method than what was anticipated by the adversary.\nHypothesis: The authors propose that by generating high-frequency poisoning perturbations using both supervised and unsupervised contrastive learning paradigms, the transferability of availability poisoning attacks can be greatly enhanced across different learning methods and paradigms.\nMethod: The authors introduce Transferable Poisoning, an approach that iteratively uses gradient information from models trained via supervised and unsupervised contrastive learning to optimize high-frequency poisoning perturbations. The method involves alternating between supervised targets with a pretrained model and unsupervised contrastive learning to generate perturbations. The experiments utilize benchmark image datasets like CIFAR-10, CIFAR-100, and TinyImageNet, with various learning algorithms to test transferability.\nConclusion: Transferable Poisoning achieves significantly improved transferability of poisoning perturbations across different learning paradigms, reducing victim model test accuracy by over 27% compared to existing methods. The use of high-frequency perturbations ensures efficacy regardless of the training paradigm, demonstrating a more severe threat of availability attacks than previously thought.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The motivation is convincing that availability poisoning attack should be effective for different learning paradigms.\\n2. The proposed algorithm improves the transferability of poisoning attack across evaluation algorithms including SL, CL, Semi-Supervised Learning on CIFAR-10, CIFAR-100 and Tiny-ImageNet.\\n3. This paper provides plenty of investigation on alternative methods including HF, CC, SCP, and WTP.\", \"Weaknesses\": \"1. The authors should check their code regarding the CL and SupCL evaluation for poisoning attacks, especially how transforms (augmentations) are performed when preparing the poisoned dataset. It seems that the data augmentation for each image is fixed before the CL (SupCL) training starts in their code. For example, in this case, each positive pair consists of two fixed augmented images. However, the CL (SupCL) algorithm applies different data augmentations each time it reads an image. Once this part of code is not properly configured, the reliability of the algorithm is questionable. For these reasons, I have significant concerns about the CL and SupCL evaluation results reported in this paper.\\n2. The novelty of the proposed method is limited. The core of the TP algorithm is a just combination of two existing algorithms, i.e. iteratively updating poisons using TAP and CP to inherit their advantages.\\n3. The paper seems inaccurately describe the existing work. \u201cTargeted Adversarial Poisoning (Fowl et al., 2021) can produce poisoning perturbations with high-frequency features that are not linear separable, ...\u201d However, as discussed in Table 1 of [a], TAP perturbations are linear separable.\\n4. The main machanism of TP is kind of vague. This paper claims that \u201cwe propose Transferable Poisoning (TP) to generate poisoning perturbations with shared high-frequency characteristics that can be transferred across different learning paradigms (Section 3).\u201d It is unclear why high-frequency perturbation should be a goal for generating transferable poisoning attacks. The proposed algorithm even does not explicitly optimize the frequency of perturbations. In Figure 4, all four attacks are high-frequency but three of them, namely CC, SCP, and WTP have poor transferability. Does it contradict the claim?\\n5. Besides the first concern about CL evaluation, I have also concerns about the comparison of attack performance.\\n(1)\\tTable 1,2,4 do not consider BYOL that was used in the paper of CP. What is the attack performance of TP against BYOL on CIFAR-10 and CIFAR-100?\\n(2)\\tIn Table 1, is SimCLR used for CP and TUE perturbation generation? However, SimCLR is not surely the best generation algorithm for transferability of CP and TUE. To be a fair comparison, it is helpful to consider CP and TUE attacks based on other CL algorithms as well as class-wise CP attacks which are effective for SL.\\n(3)\\tIt would be better to compare your TP with existing powerful poisoning methods (UE, AP, CP and TUE) on larger datasets like CIFAR-100 and TinyImageNet.\\n6. As the algorithm involves contrastive training, it possibly costs too much time to generate TP perturbations, especially on Tiny-ImageNet and ImageNet. Can you provide time consuming comparison of your TP with existing transferable methods like AP, (class-wise) CP, and TUE? Considering availability poisoning attacks are a sort of data protection means, low efficiency of generation might hinder the applications in real world scenarios.\\n\\n[a] Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Availability attacks create shortcuts.\\n\\nClarity, Quality, Novelty And Reproducibility:\\nI hope the author would make more clarificaitons about the main machecism of TP and descriptions about existing work. The novelty of proposed algorithm is hindered by the combination of two existing algorithms. For reproducibility, the first concern about CL and SupCL evaluation is vital.\", \"Questions\": \"See Weakness.\"}"}]}
{"paper_id": "9lvyCHhQix", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Continual Learning (CL) involves the ability of models to learn continuously from non-stationary data distributions, often addressing the issue of catastrophic forgetting, where new tasks negatively impact previously learned ones. Previous CL approaches focus primarily on avoiding forgetting but often do not balance parameter efficiency and capacity saturation. Parameter efficiency ensures the effective use of parameters without overfitting, while capacity saturation can occur when a model's ability to generalize is compromised as task diversity increases.\nGap: Existing approaches to continual learning struggle with the trade-off between parameter efficiency and capacity saturation, which is particularly problematic in long-term task-incremental settings with storage constraints. The challenge is maintaining parameter efficiency while handling capacity saturation for evolving tasks, which current methods fail to adequately address.\nHypothesis: The paper hypothesizes that a dynamic approach to expanding a model's weight space can effectively balance parameter efficiency and capacity saturation in continual learning. By introducing a knowledge hypergraph structure that captures latent knowledge across tasks, it is possible to measure task diversity and estimate necessary capacity increases for new tasks, ensuring efficient parameter usage and avoiding capacity saturation.\nMethod: The authors propose a novel CL approach called Continual Knowledge HyperGraph Learning (HyperGKL). This method introduces a dynamic knowledge hypergraph structure, which identifies and leverages latent knowledge across tasks to measure task diversity and manage capacity increases for new tasks. The hypergraph connects shared skills across tasks and estimates the required capacity increment. The method uses a hypernetwork for dynamic skill-specific subnetwork generation to ensure constant parameter numbers over time. It involves decomposing tasks into skills, constructing a dynamic hypergraph, and using constraints and parameter generators to manage adaptation and efficiency. The approach is empirically validated on several benchmark CL datasets, comparing it with existing methods for performance and resource utilization.\nConclusion: The proposed HyperGKL method effectively addresses the balance between parameter efficiency and capacity saturation in continual learning, achieving state-of-the-art results on several benchmark datasets. It maintains low parameter counts through a novel hypergraph structure and a hypernetwork that dynamically manages model complexity, demonstrating its robustness in long-term, memory-constrained CL scenarios.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"- The trade-off between capacity saturation and parameter efficiency is an interesting point that is not discussed frequently\\n- The idea of modeling a task diversity using hyper graphs seem novel\", \"Weaknesses\": \"- The setting in which capacity saturation and parameter efficiency really matters is, as mentioned in the manuscript, when CL methods are deployed on edge devices with little memory. This is the main reason why replay-based CL methods are seldom compared to in the submission but I disagree. If using a replay buffer is too costly, what's to say that expanding the model size (however efficiently that might be) is not too costly? Replay based CL methods can be understood as having the best parameter efficiency (as the model size does not grow) and tackling the capacity saturation issue with past task samples in the replay buffer. I think more comparisons both in terms of concepts and experiments should be done against replay based CL methods to really justify the proposed trade-off in the paper.\", \"Questions\": \"Please see the weaknesses.\"}"}]}
{"paper_id": "Bvrc6kobWd", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Self-supervised learning (SSL), particularly contrastive learning, is a widely used framework that learns instance-level relationships using a similarity function, often cosine similarity, and cross-entropy. In face and speaker recognition, margins have been effectively utilized to enhance recognition performance by increasing inter-class distance and decreasing intra-class distance. While some attempts have been made to incorporate margins into contrastive learning, their effects have been primarily explained through decision-boundary analyses from face recognition tasks, without delving deeply into the gradient-level understanding.\nGap: The role of margins in contrastive learning, particularly through their effect on gradients, has not been well-explained or explored. Existing explanations are largely borrowed from face recognition and rely on decision-boundary-based approaches, which are not applicable to the unlabeled nature of contrastive learning tasks.\nHypothesis: The authors propose that analyzing the effect of margins on the gradients of contrastive learning can provide a new understanding that is not reliant on decision-boundaries and can reveal elemental effects that contribute to representation learning. This gradient perspective might offer directions for enhancing contrastive learning performance.\nMethod: The authors conducted a gradient analysis of the generalized contrastive learning loss, incorporating margins, to identify how these margins affect gradients. They specifically analyzed the gradient levels affected by margins, such as emphasizing positive samples, scaling gradients by angular margins, and scaling based on the ratio of summed logits. Experimental results using datasets such as CIFAR-10, CIFAR-100, STL-10, and TinyImageNet, evaluated the impact of these factors on generalization performance.\nConclusion: The study concludes that emphasizing positive samples and scaling gradients based on positive sample angles and logits are crucial for improving the generalization of contrastive learning across both seen and unseen datasets. Other factors provide marginal improvements. The gradient-based approach to understanding margins could lead to better exploitation of margins and improvements in contrastive learning frameworks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 1, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"* The approach is novel\\n* The authors ran several experiments with good ablation.\", \"Weaknesses\": \"* Shallow theoretical depth\\n* Hard to get some insight on what Fig.1,2 and the theoretical results in Sec.4 imply \\n* The authors claim that 4 effects of introducing the margin term are empirically are covered in Sec.5. First, it was hard for me to get those 4 messages from both theory (in Sec.4) and in experiments (in Sec.5). Let us say 4 effects are observed. Does it explain why introducing the margin is beneficial for contrastive learning? \\n* Probably many of these issues are from the quality of writing.\", \"Questions\": \"* In equation (3), it is unclear how the \\\\beta term in the denominator (inside the log) is coming out to the linear form in the righthand side. Probably have typo somewhere? When \\\\beta=0 (i.e., when only the positive samples are considered), the denominator inside the log becomes 0, which is also weird.\"}"}]}
{"paper_id": "YxvmODVWny", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Natural language and images are typically used for goal specification in goal-conditioned imitation learning, but each has limitations. Language can be ambiguous, and images can be over-specified. The paper explores hand-drawn sketches as a new modality, which is easy to create like language but more spatially aware like images.\nGap: The gap identified is the lack of a goal representation that balances the intuitive, easy-to-provide nature of language with the spatial precision of images, all while avoiding the overspecification issue inherent in images and the ambiguity in natural language for goal-conditioned imitation learning.\nHypothesis: Hand-drawn sketches can serve as an effective goal representation in visual imitation learning by providing a balance between abstraction and spatial specificity, leading to better task performance, especially in the presence of ambiguities or visual distractors.\nMethod: The approach involves developing RT-Sketch, a policy that uses hand-drawn sketches for manipulation tasks. The model is trained on a dataset of 80K trajectories paired with synthetically generated sketches. An image-to-sketch translation network is used to generate goal sketches from images which are then used to train the policy using imitation learning. Experimentation is conducted across six manipulation tasks with varying scene complexities on real robots.\nConclusion: RT-Sketch is shown to perform on par with language and image-conditioned policies in straightforward settings while demonstrating greater robustness to language ambiguity and visual distractions. The model also adapts well to sketches with varying details, from simple lines to colored drawings, indicating its flexibility and potential practical advantages in real-world applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"- The direction the authors are going with this work, thinking about different ways of specifying a goal, will be useful for improving accessibility and has intellectual merits. \\n- The authors take steps to perform surveys to study human preference in specifying goals rather than just quantitative analysis through robot performance.\\n- Creation of the RT-Sketch dataset and the Image-To-Sketch model have technical merits.\\n- The limitations and failure modes are honest from a methodology perspective and are appreciated.\\n- Overall this paper is well written, Figure 2 does a good job of demonstrating the architecture of RT-Sketch and Figure 3 does a great job of conveying the survey results.\", \"Weaknesses\": \"The main concerns with this manuscript come from two sources. The first is with respect to the motivation of using sketches to specify a goal and the second is concerning the quantitative robotics results.\\n## Motivation\\nOverall the reviewer is not convinced that the examples and arguments given motivate the superiority of using a sketch as a goal over using natural language.\\n- Regarding the granularity argument that the authors use in the introduction including the examples of \\\"put utensils, ..., on the table\\\" and \\\"put the fork 2cm to the right...\\\". Doesn't this demonstrate the flexibility that language has as a goal? Even if a human had to communicate the placement of utensils on a table, this would still seem easier than drawing a corresponding representation for an entire table.\\n- While language can be ambiguous, so can sketches. As an example, if a sketch was given with an empty table, am I telling the agent that I want it to throw out the garbage on the table or am I telling it to ignore the garbage? It would seem like the desirable solution would be to create more intelligent agents to create reasonable solutions or ask for clarifications when given an ambiguous problem.\\n\\n## Robotics Results\\n- The metrics, \\\"Spatial Precision\\\" and \\\"Failure Occurrence\\\" are not carefully defined or motivated. Failure occurrence is not defined at all. The spatial precision metric is at best defined as \\\"the distance (in pixels) between object centroids in achieved and ground truth goal states, using manual keypoint annotation\\\". However, how these centroids are obtained and why manual keypoint annotation is necessary over using an off-the-shelf image classifier should have been mentioned. \\n- It is not obvious how big of a difference the errors are when looking at the RMSE in pixels. Could the errors when finding the centroid and the manual keypoint annotations be an issue in measuring this? Can a visualization be created to show this?\\n- Typically bolded numbers in a column/row represent the method with best performance. However, in the column for failure occurrences, RT-Goal-Image is bolded despite having the highest failure occurrence.\\n- The meaning of the shading of the cells of Table 1 adds a lot of confusion and should have been defined in the caption to improve readability. This confusion comes because, in one portion of the table, darker gray colors represent lower centroid distance and in another portion it represents the frequency of failures. This frequency of failure metric is also not well defined, how is this different than failure occurrence?\", \"Questions\": \"Beyond the concerns and questions given in the weakness section.\\n\\n- How many people took the survey? How was this survey conducted?\"}"}]}
{"paper_id": "8Cc6qOPvFo", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The task of text-driven image editing involves translating a source image into a target domain while preserving the original image's structure or background. Traditional methods using Generative Adversarial Networks (GANs) have been effective, with approaches like StyleCLIP leveraging pretrained models for enhanced image translation. Recently, diffusion models have gained traction, offering promising results for text-image generation tasks by leveraging pretrained diffusion models such as Stable Diffusion.\nGap: Existing diffusion model-based methods for text-driven image editing often require significant computational resources and fine-tuning, which is computationally intensive. Furthermore, naive application of diffusion models can degrade the structural integrity of the source image during translation.\nHypothesis: The authors propose that a training-free, cycle-consistency-driven metric learning approach can effectively leverage pretrained diffusion models for text-driven image-to-image translation, preserving the original image's structure and background without the need for extensive computational resources.\nMethod: The method involves revising the naive forward and backward processes of diffusion models. Instead of fine-tuning, the authors propose two guidance terms during the translation process: a distance objective based on triplet loss to encourage alignment with target prompts and maintain source image structure, and a cycle-consistency objective that iterates between optimizing source and target latents. The approach employs the DDIM framework and utilizes guidance terms that integrate CLIP's text-image similarity scores and stable diffusion's latent space navigation. The experiments are conducted using Stable Diffusion on various tasks with publicly available datasets, avoiding additional fine-tuning.\nConclusion: The proposed method achieves superior performance in text-driven image-to-image translation tasks, effectively preserving the source image's structure and improving alignment with target text prompts. The cycle-consistency objective further enhances image preservation, demonstrating its effectiveness across various experimental settings.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"- **Simplicity & Effectiveness**: The proposed method is both straightforward and seemingly efficacious, as evidenced by the results presented in the paper.\", \"Weaknesses\": \"- **Evaluation Methods**: The evaluation could be more robust. The prompts used for evaluation are closely related to the original noun, reducing diversity and potentially biasing results.\\n- **Aspect Ratio Concerns**: The samples used for evaluation have been altered from their original aspect ratios. This could inadvertently disadvantage competing methods.\\n- **Comparison Choices**: The results from prompt-to-prompt evaluations seem to perform well on generated images rather than inverted real ones. The absence of a comparison with Null-text-inversion, which might be a more apt benchmark, raises questions.\\n- **Efficiency Metrics**: The paper would be more informative with a runtime efficiency comparison against other methods.\", \"Questions\": \"None\"}"}]}
{"paper_id": "Fq8tKtjACC", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The training of large artificial neural networks, especially with the advent of Transformer architectures, has seen significant progress, where scaling laws have shown that performance typically improves with increased compute and model size. Recent work has explored alternative approaches focusing on improving the quality of training data to enhance model performance, potentially matching larger models while reducing dataset size and computational cost.\nGap: Current approaches for training large language models for code generation heavily rely on large-scale data and computational resources. However, there is a need to explore if high-quality data can allow achieving state-of-the-art performance with smaller models and resources.\nHypothesis: The authors propose that using 'textbook quality' high-quality data in training large language models can significantly improve performance, matching or surpassing larger models while reducing the required data size and computational resources.\nMethod: The authors introduce phi-1, a Transformer-based model with 1.3B parameters, trained on 7B tokens of high-quality data. This includes 6B tokens from filtered web sources and 1B synthetically generated tokens using GPT-3.5. The initial phase involves pretraining on a code-language dataset and further finetuning on a set of synthetic coding exercises. The training was executed on 8 A100 GPUs for 4 days, focusing on tasks like writing Python functions from docstrings, evaluated using the HumanEval benchmark. Key distinctions include data filtering using a classifier trained on GPT-4 annotations and generating synthetic datasets designed to enhance diversity and instructiveness.\nConclusion: Despite its smaller size, phi-1 achieves 50.6% pass@1 accuracy on HumanEval and 55.5% on the MBPP benchmark, outperforming several larger models. The findings show that leveraging high-quality data can break the existing scale-performance relationship, achieving high performance with smaller models. However, there are limitations, such as the model's specialization in Python and sensitivity to prompt variations, but these can potentially be addressed with further refinements in data and methods.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 4, \"Rating\": 8, \"Confidence\": 5, \"Strengths\": \"This paper proposes a relatively high efficiency approach to training LLMs. It is impressive to get such results from a model trained on 8 A100s in 4 days. This makes this work approachable from academic research labs, which takes a step towards reversing the trend of pursuing ever larger datasets with larger models and computational requirements. This also has implications for energy efficiency and sustainability.\\n\\nThe paper itself is well-written and quite clear.\\n\\nThe authors have commited to the release of the \\\"model for usage and evaluation by the broader community\\\". This is important for open science and a big strength.\", \"Weaknesses\": \"Key details on the data generation process are not shared for \\\"proprietory reasons\\\", yet this is central to the paper, its proposition and its results. \\n\\nThe lack of a broader impacts section weakens this paper.\\n\\nIf this weakness is addressed, I am happy to improve my score.\", \"Questions\": \"Would you argue that the structure and compositionality of programming code is critical for achieving such high levels of performance from small models and datasets. I wonder if a takeway is to always seed training from well structured and commented code? If so, would this translate to other programming/instruction like language tasks? e.g. recipes, etc..\\n\\nDo you think that the code excercises factor into improving performance by encouraging the model to self-critic whatever it generates, given that it seems to append excercises to generations?\\n\\nWhere do you think this approach is most likely to fail? With what kinds of tasks/data?\\n\\nDoes phi-1 have ICL abilities? And are there other emergent capabilities that can be forseen and easily tested?\\n\\n\\nUpdate: Authors have addressed my questions.\"}"}]}
{"paper_id": "YkRwadXWHd", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Sign language recognition, especially continuous sign language recognition (CSLR), is important for facilitating communication within the hearing-impaired community. CSLR is more challenging than isolated sign language recognition (ISLR) since it involves the continuous transcription of sign sequences without clearly defined boundaries. Current methods predominantly rely on video data, but struggle with dynamic hand movements, high data dimensionality, and computational costs.\nGap: Existing CSLR methods typically use dual-stream networks involving video and one other modality such as keypoints or optical flow. These approaches do not take advantage of integrating all three modalities\u2014video, keypoints, and optical flow\u2014together, and they also face significant computational burdens.\nHypothesis: The authors propose that a multi-modal framework combining video, keypoints, and optical flow, along with a hierarchical knowledge distillation framework, can enhance CSLR performance by extracting more representative features while reducing computational costs.\nMethod: The authors propose a multi-modal feature extractor that integrates video, keypoints, and optical flow using several fusion techniques like MLP-based, attention-based, and convolution-based methods. They introduce a hierarchical knowledge distillation (HKD) framework that transfers knowledge from the multi-modal teacher model to a single-modal student model to reduce computational load. They evaluate their approach using datasets like Phoenix-2014, Phoenix-2014T, and CSL-Daily, and use CTC loss for training.\nConclusion: The proposed SignKD approach achieves state-of-the-art performance on the Phoenix-2014, Phoenix-2014T, and CSL-Daily datasets. It demonstrates significant improvements in WER compared to previous methods, both in multi-modal and single-modal settings, validating the effectiveness of the proposed multi-modal integration and knowledge distillation framework.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"- Transferring multi-modal knowledge learned by multiple streams to a single-modality network via knowledge distillation seems to be an innovative and practical idea in the CSLR community. The advantage of this method is that while enjoying multi-modal knowledge from the teacher model, the student model itself does not need to take additional modality inputs such as optical flow and pose, where such extra modalities actually incur computational overhead during inference time.\\n\\n- The presentation for the effect of each proposed component is quite clear from both teacher-side and student-side. For the teacher side, improvements are gradually observed when more modalities are combined, and more fusion layers are applied (Table 3). From the student-side, similar to the teacher model, the performance increases when the knowledge distillation loss terms are applied in feature levels and the output-level (Table 4b). \\n\\n- Compared to the provided previous CSLR methods, the proposed scheme achieves strong results in both cases of multi-modal network and single-modal network in some standard CSLR benchmarks.\", \"Weaknesses\": \"[Major]\\n- Although the overall flow of the technical design is plausible, the proposed SignKD framework combines two existing components well-established in literature (i.e., multi-modal fusion layer and feature-level knowledge distillation), which is not an astounding result and can weaken the technical novelty of the work. As submitted to a top-tier ML conference, further theoretical or intuitive insight on the architectural design would be expected.\\n\\n- The claim for the state-of-the-art performance in CSLR tasks needs to be reconsidered. Specifically for the video-only methods, is the video-based backbone S3D pretrained on video data, or initialized from scratch? While the most performant baseline SEN (Hu et al., 2023c) is based on the architecture with frame-level 2D CNN followed by LSTM, but the proposed work adopts a strong video backbone. Such architectural differences and the following advantages should be acknowledged when comparing performances. Can the proposed work still achieve superior performances even when built upon such frame-level architecture? In addition, for the multi-modal-based experiments, comparison with baseline methods in the same modality condition is missing. For example, how much the performance drop would be observed when the same kind of modalities with baseline methods (i.e., video+pose) are used for training the final multi-modal teacher model?\\n\\n[Minor]\\n- For the additional techniques explained in Appendix C. in detail, missing citation for the sign pyramid network [Chen et al., 2022]. In addition, frame-level supervision has been also previously explored [Min et al., 2021, Hao et al., 2021, Jang et al., 2023]. Further discussions on advantages or differences of the proposed work compared to previous works would be more helpful for potential readers.\\n\\n- Computational aspects for the proposed model are missing. Batch size during training is not provided. In addition, as the training cost for a CSLR model is highly demanding, it would be beneficial to share total training hours and required VRAM memory. Moreover, how many frames the final student model can process during inference (i.e., FPS)? Considering real-world deployment, reporting such quantities and comparing with existing CSLR methods is suggested. \\n\\n- For the ablation results from Table 4(b), one additional row corresponding to the result where only CTC and O are marked is expected. This can be compared with the item where all the feature-level distillation losses are applied (the last row). \\n\\n- Related to the reproducibility, it is unclear whether the codes for training and inference would be released. If there is no plan to open the codes, could you provide a brief reason?\", \"Questions\": \"[Questions, discussions and suggestions]\\n\\n- Considering current standard CSLR benchmarks such as PHOENIX-2014, in my opinion, it would be relatively easy for a model to capture motion information of a signer since it has uniform background and the only component with motion is caused by a human. Related to this, it would be interesting to see the stress-testing result for the already trained model on challenging environments [Jang et al., 2022].   \\n\\n- Presenting sample-level qualitative visualizations such as feature activation map for a signer or gloss-level predictions on some video sequences is suggested, as usually done in CSLR literature. It would be valuable to explore whether such visualizations show different characteristics depending on the single-modal (student) network and multi-modal (teacher) network. \\n\\n- The proposed model includes a lot of loss terms with each weighting factor. Are those weight values sensitive to different datasets? \\n\\n- Inconsistent notations for Table. (i.e., \u2018table 3\u2019 in the first line of section 4.3).\\n\\n- Typo: \u2018is feed\u2019 in the first line of page 4 below the figure. \\n\\n- What is the meaning of a \u2018linear transitional layer\u2019? Is it a standard term?\\n\\n---\\n\\n[Reason for the score recommendation]\\n\\nThis paper has distinctive merit as stated in the Strengths section. However, it is quite unsurprising that the combination existing techniques such as multi-modal fusion and knowledge distillation mechanism can produce strong performance. In addition, it is not sufficient to claim as SOTA at this version. It needs to be more rigorous to compare with previous baselines beyond considering just error rates. I would be more inclined to the positive side when those two main issues are addressed during the rebuttal period. \\n\\n---\\nReferences\\n\\n[Chen et al., 2022] Two-Stream Network for Sign Language Recognition and Translation, NeurIPS 2022.\\n\\n[Min et al., 2021] Visual Alignment Constraint for Continuous Sign Language Recognition, ICCV 2021.\\n\\n[Hao et al., 2021] Self-Mutual Distillation Learning for Continuous Sign Language Recognition, ICCV 2021.\\n\\n[Jang et al., 2023] Self-Sufficient Framework for Continuous Sign Language Recognition, ICASSP 2023.\\n\\n[Jang et al., 2022] Signing Outside the Studio: Benchmarking Background Robustness for Continuous Sign Language Recognition, BMVC 2022.\"}"}]}
{"paper_id": "8Wuvhh0LYW", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large language models (LLMs) such as GPT-4 and LLaMA have shown remarkable performance across various natural language processing tasks, and are considered precursors to artificial general intelligence. However, their deployment in real-world applications is severely limited by high memory and computation requirements. Recent research on quantization methods, notably post-training quantization (PTQ), has aimed to reduce these resource demands.\nGap: Existing quantization methods in LLMs face significant performance degradation when applied to extremely low-bit quantization settings such as W4A4 or W2A16. These methods often rely on handcrafted quantization parameters, which can result in lower performance. Quantization-aware training (QAT), although more precise, incurs high computational costs, making it impractical for LLMs.\nHypothesis: The authors propose that an improved quantization technique called Omnidirectionally Calibrated Quantization (OmniQuant) can achieve higher performance across diverse quantization settings while maintaining the efficiency of PTQ. By integrating gradient optimization via Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET), the method can learn optimal quantization parameters, overcoming limitations of traditional PTQ approaches.\nMethod: OmniQuant introduces a new quantization technique for LLMs, utilizing a differentiable, block-wise error minimization framework. The method features two key components: Learnable Weight Clipping (LWC), which optimizes clipping thresholds for weight quantization, and Learnable Equivalent Transformation (LET), which shifts the quantization burden from activations to weights. This approach allows for efficient optimization over both weight-only and weight-activation quantization settings. Experiments were conducted on various models including the LLaMA family, with tests of differing bit-width configurations to validate performance.\nConclusion: OmniQuant successfully addresses the challenges of low-bit quantization in LLMs, demonstrating superior performance over traditional PTQ methods across a range of quantization configurations. It offers improvements in memory efficiency and inference speed on actual hardware setups. The research suggests that OmniQuant makes the quantization process more adaptable and efficient, potentially paving the way for more practical deployment of large language models.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 5, \"Strengths\": \"The biggest contribution of this work is the learnable weight clipping (LWC). \\n\\n1. When compared to AWQ [1] which only optimize the scale, LWC uses the same objective function (minimize output distortion) but also handles clipping.\\n\\n2. When compared to PACT, which only applies to activation with positive values, LWC is more general and applies to weight, activation, and negative values.\\n\\n3. When compared to LSQ, which is a combination of multiple optimization goals (step-size, gradient scaling, and clipping), LWC is straightforward to implement and speedy to train.\\n\\n[1] Lin, J., Tang, J., Tang, H., Yang, S., Dang, X. and Han, S., 2023. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. arXiv preprint arXiv:2306.00978.\", \"Weaknesses\": \"1. While I highlight the advantage of LWC over AWQ, PACK, and LSQ, this evaluation for PACK and LSQ is lacking.\\n\\n2. Authors should present Figure 1 (a) in a quantitative way. It lacks actual numbers for cost of time and performance.\\n\\n3. Learnable Equivalent Transformation (LET) isn't novel and can be seen in common quantization kernels such as LLM.int8. I suggest authors to elaborate more on comparison with PACK and LSQ instead of LET.\\n\\n4. The latency benchmark only contain fp16 and OmniQuant. Strong baseline such as GPTQ is lacking.\", \"Questions\": \"1. How does OmniQuant runtime efficiency compare to GPTQ and AWQ?\\n\\n2. What is the performance when compared to PACK and LSQ?\"}"}]}
{"paper_id": "R3CDj2DLln", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The study focuses on multimodal physical scene understanding where an embodied agent needs to discern the properties, direction, and distance of an impact sound source to locate fallen objects. Previous work, such as Neural Acoustic Fields (NAFs), could capture sound propagation in a single scene but lacked generalizability across multiple scenes due to overfitting. Acoustic rendering and sound-based scene understanding are critical for applications in navigation, particularly under multimodal scenarios that integrate both auditory and visual inputs.\nGap: Previous methods, specifically feed-forward neural networks used for sound source inference, failed to generalize well across different environments and struggled with poor domain adaptation. NAFs, while capturing acoustics effectively within single scenes, were not effective for analyzing sound across multiple environments.\nHypothesis: By learning an inverse model of acoustic formation, termed as the Disentangled Acoustic Field (DAF), the embodied agent could construct a spatial uncertainty map to improve the localization and understanding of fallen objects through sound.\nMethod: The method involves creating a disentangled acoustic field model that captures the generation and propagation of sounds. The DAF approach includes an encoder to process short-time Fourier transform (STFT) into latent factors and a generator to produce the power spectral density (PSD) representing the emitted sound characteristics. The analysis-by-synthesis framework enables sound properties inference by decomposing latent space. The system uses the Testament Dataset with experiments conducted in the TDW simulation platform for audio-visual scene understanding tasks. During tests, agents use sound to form hypotheses about object characteristics and location, refining them with visual data.\nConclusion: The DAF framework significantly advances the success rate for localizing fallen objects in multimodal environments, outperforming previous models by providing both improved generalization across different scenes and enhanced interpretability of sound data. The framework facilitates effective navigation plans and can generalize to novel scene types, indicating its robustness in complex audio-visual environments.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"1. The proposed architecture is interesting. If we can obtain lots of pairs of audio signals and object properties, the proposed architecture might be one of the best choices for simultaneously analyzing and synthesizing audio scenes.\\n\\n2. Based on the experimental evaluations, the proposed method reasonably work well for several tasks.\", \"Weaknesses\": \"1. The task of this paper is closely related to sound source localization. If the authors justify the effectiveness of the proposed method in terms of position errors of sound sources, experimental comparisons with several previous methods for this task would be mandatory. As presented in [Grumiaux+ JASA 2021 https://arxiv.org/abs/2109.03465], lots of previous methods have already been proposed for this purpose. I understand that many of those previous methods typically require multi-channel audio inputs and the proposed method employs a single-channel or binaural audio clip. However, you will find several techniques that work well only with single-channel audio, since single-channel sound source localization has been one of the typical tasks in audio signal processing before the deep learning era.\\n\\n2. If we want to simply predict acoustic scenes from audio signals, we do not have to introduce decoders that generate audio signals from object properties. In this sense, the ablation study that compares the full proposed method and the one without decoders should be presented.\\n\\n3. If the main objective of this paper is a proposal of a novel construction of neural acoustic fields, the proposed method should be compared with the original NAFs in the task of audio synthesis.\", \"Questions\": \"Please check the above Weakness section.\"}"}]}
{"paper_id": "zUHgYRRAWl", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Variational autoencoders (VAE) have become popular in drug discovery for generating molecules, leveraging large-scale datasets like ChEMBL and ZINC. However, they struggle with poor manifold recovery in complex data settings\u2014a challenge known in high-dimensional data spaces.\nGap: Current VAE approaches for molecule generation suffer from poor manifold recovery, particularly when data is embedded on a low-dimensional manifold within a high-dimensional space. This issue leads to inferior property matching with the target dataset.\nHypothesis: A multi-stage VAE approach can improve manifold recovery over traditional single-stage VAEs by creating better latent variable representations, enhancing the quality of generated molecules without using property predictors in the training process.\nMethod: The authors propose a multi-stage VAE method where subsequent VAEs are trained on the latent representations of the prior stage. The approach was tested on a synthetic dataset and the ChEMBL dataset for molecule generation, along with fine-tuning on smaller datasets targeting specific protein activities. The study compares the multi-stage VAE against one-stage models and other methods that integrate property predictors.\nConclusion: The multi-stage VAE demonstrates improved manifold recovery, generating molecules with properties more similar to target datasets without relying on property predictors. It performs effectively against models using learned property metrics, offering an objective-agnostic improvement method.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. Correct posing of a problem, with correct references to related work.\\n2. The authors undertake the issue of finding better low-dimension space immersion in high-dimensional space. On the other hand, it is not proved that their solution is the right one.\", \"Weaknesses\": \"1. The authors note that typically, high-dimensional data can be projected onto a low-dimensional space immersed in a high-dimensional space. To demonstrate this, they conduct a very simple experiment on synthetic data. This experiment does not show much, only the authors claim that for one of the later layers a space with the correct dimensionality for the data is obtained. This experiment is clearly spurious and can be removed.\\n2. The authors propose to build a VAE model with many hidden layers iteratively. In each step, developing the projection of the most recently obtained hidden layer into a new VAE model. The obtained model would get the variances on the diagonal reach the values {0, 1}, which would show the selection/reduction of individual dimensions projection. \\nIt seems that the analogy with synthetic experiment is insufficient for a direct application of this approach.\\n3. Several models with randomly selected numbers of hidden layers are used in the experiments.\\n4. In the comparisons, each model has a different dimensionality of hidden layers for the same data.\\n5. There is no bibliography in the work. I suspect that this is a simple editorial error (references in the work are marked as links), but such an error in a work for a major conference is unacceptable. Similarly, there is no appendix to which there is a reference in the main text.\", \"Questions\": \"1. Is the synthetic experiment really needed? I would remove it if I were you. In place of it, I would give some better mathematical discussion.\\n2. Is the analogy between the scheme presented in the synthetic data experiment to the problem of molecule generation really valid? Could you, please, better justify the analogy?\\n3. Why just that number of layers are used in the models? Why are there 3 layers for HGNN and MoLeR? The \\\"MoLeR + prop\\\" model is completely unclear? Why does the RNN model only have two layers? The LatentGAN model seems to have only one hidden layer, right? Or maybe some of the nodels are for comparison? This is not clear from the text of the work.\\n4. If one of the goals was to show that multi-layer models, along with variance optimization, allow for better matching of the implicit dimensions, does using different dimensions allow this to be shown for different models? There is no description of the individual statistics used in the work.\\n5. Was variance optimization used for all models? This is not clear from the text.\\n6. The standard in the editorial composition of scientific works is to place descriptions of tables above them, not below.\"}"}]}
{"paper_id": "OCx7dp58H1", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Transformer models have achieved significant success in multiple domains, such as vision and natural language processing. However, without large datasets, their performance can saturate quickly when increasing model depth due to supposed oversmoothing, where the representations of features converge to the same vector as layers increase.\nGap: Recent work suggests Transformers are inherently low-pass filters leading to oversmoothing. This oversmoothing can cause performance degradation at increased depths. The problem is finding out how successful Transformers overcome this theoretical limitation without changing architectures or adding hyperparameters.\nHypothesis: Transformers are not inherently low-pass filters as previously assumed. Instead, whether they oversmooth depends on the eigenspectrum of their update equations. By understanding and controlling these eigenspectrums, oversmoothing can be avoided.\nMethod: The analysis was conducted by characterizing the eigenspectrum of attention and weight matrices in Transformers and examining their impact on oversmoothing. The study extends the understanding of oversmoothing through filtering operations and explores rank collapse. Additionally, it proposes a reparameterization method that adjusts the weights of update equations to prevent oversmoothing. The empirical evaluation involved testing different models, including a reparameterized version, across various tasks with datasets like CIFAR100 and ImageNet, measuring performance under different training data conditions and corruption levels.\nConclusion: The paper concludes that Transformers are not inherently limited to oversmoothing, and much of their successful application already exhibits properties that avoid this issue. The proposed reparameterization strategy effectively prevents oversmoothing without new architectures or hyperparameters, enhancing performance in scenarios with limited or corrupted data.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"The paper is easy to follow with necessary details given to understand the theory. It extends the available theory and proposes a new perspective on reparameterization. Experimental results support the proposed theory and preventive measures on oversmoothing.\", \"Weaknesses\": \"While the theory extends previous work in some aspects, it is not clear if this difference is significant. Other preventive measures such as AttnScale/FeatScale from Wang et al. needs comparison and more discussion. Several details are also missing from the paper.\\n\\n1. Wang et al also examines residual blocks and FFN in addition to the self-attention layer. While they mention the inevitability of high frequency components to be suppressed, their analysis suggest that using residual connections might prevent it from collapsing to zero. In fact, Eq (6) in Theorem-3 suggests that it is possible to prevent decaying of high-frequency components by constraining the $||W_V||_2$ properly so that it is a non-contractive mapping. I think a more detailed comparison to Wang et al is needed to highlight how your analysis differs from low-pass filtering aspect and how your preventive measures are different from constraining $||W_V||_2$ or AttnScale/FeatScale that Wang et al applies.\\n\\n2. While Figure-2 shows that asymmetry degrades with more epochs, is 0.7 small enough to suggest symmetry, given that it starts at 0.95? What about outliers in the off-diagonal entries? Additionally, can you connect it to the condition number?\\n\\n3. Can you clarify more on how do you update $\\\\Theta$ in $QR(\\\\Theta)=[V_H, R]$? Do you do QR decomposition after every gradient step or do you use $\\\\Theta R^{-1}=V_H$ where you backpropagate gradients directly to $\\\\Theta$?\\n\\n4. How does the HFC/LFC evolve in training steps? \\n\\n5. At the end of Section 4, page 6, why do you have $V_H^{-1}$ after clip?\\n\\n6. The same page, last paragraph, I think it should be $diag(\\\\Lambda_H)=\\\\Psi^2$ -- no negative sign.\\n\\n7. Please define Q in the main statement of Theorem-2.\\n\\n8. Page 6, \\\"||\\\" is missing in definition of asymmetry.\\n\\n9. Please describe the metrics for Table-2 and Table-3.\", \"Questions\": \"Please see above for more details.\\n\\n1. Can you provide a more detailed comparison with Wang et al? Including more baselines.\\n\\n2. How should I interpret the asymmetry metric? How does it relate to condition number?\\n\\n3. Can you give more details on updating QR decomposition?\\n\\n4. How does HFC/LFC evolve during training?\"}"}]}
{"paper_id": "LxruQOI93v", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Neural networks are believed to fit training sets with at least as many samples as they have parameters, a concept linked to overparameterized and underparameterized models. Prior studies and theories, like approximation theories and generalization theories, provide insights into the capacity and generalization abilities of neural networks, often discussing their parameter count, expressiveness, and capacity to fit data.\nGap: While theoretical perspectives suggest that overparameterized models can fit large datasets, practical observations reveal limitations often imposed by the training process, optimizer, and architecture. The exact parameterization impacts the loss surface and limits the solutions accessible via common training procedures, indicating a gap between theoretical capacities and practical outcomes.\nHypothesis: The paper hypothesizes that, contrary to the general belief that neural networks can fit as many samples as they have parameters, practical constraints limit their ability to fit data due to training methods, optimizers, and architectural choices, leading to findings that models achieve fit with significantly fewer samples than parameters.\nMethod: The authors introduce a metric called Empirical Data Capacity (EDC) to quantify how many samples a model can fit in practice. They examine the effects of various model architectures (MLPs, CNNs, ViTs), optimizers (SGD, Adam, etc.), and datasets (MNIST, CIFAR, ImageNet) on EDC. The paper describes experiments varying architecture parameters (like depth and width), using randomly and semantically labeled data, and employing different regularization and compression techniques to observe their impact on EDC.\nConclusion: The study concludes that common training setups typically find suboptimal minima, leading to results where neural networks fit far fewer samples than they have parameters. CNNs demonstrate more parameter efficiency than MLPs and ViTs, even on randomly labeled data, indicating their architectural influence. The results suggest that optimization strategies and regularizers impact data fitting capacity, and that architecture choices, like those employing ReLU, can improve capacity beyond original intentions. The work highlights the inefficiency in parameter usage and the potential for improvement through smarter architectural and training strategies.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 1, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"- The paper is well-written overall and easy to follow. The breadth of the analysis is commendable. The paper additionally presents many interesting results.\", \"Weaknesses\": \"- First, the notion of EDC is identical to the Effective Model Complexity (Nakkiran, et al. 2021), which the authors cite but do not really compare with. Both correspond to the largest subset of training data that is interpolated by the model. However, the authors exercise caution when using it (e.g. by checking the norm of the gradients and the flatness of the loss curve, etc) to make sure that the models are not undertrained. I would recommend that the authors refrain from claiming novelty in the notion of EDC and simply center their contribution around the empirical investigation of the Effective Model Complexity, instead.\\n\\n- The second primary weakness in my opinion is the lack of clarity about the experimental details in a few places. \\n  * When comparing different architectures such as CNNs and ViTs, how did the authors ensure that the architectures are compared fairly? For example, there is no mention of the number of parameters used in each architecture. Even if those architectures share the same parameter count, the shape of the architecture can have a big impact as the authors themselves study for CNNs. It's not clear if one can conclude, for example, that CNNs are more parameter efficient than ViTs. There aren't enough details in the paper for the reader.\\n  * The authors mention repeatedly that ReLU improves EDC even though they \\\"were introduced to neural networks to prevent vanishing and exploding gradients.\\\" I do not see the discrepancy here. ReLU were introduced to prevent training instability issues so I would expect ReLU to improve training and improve EDC as a result. It is unclear why the authors highlight that the impact of ReLU on EDC is surprising. \\n  * When the authors compare different datasets, like CIFAR10/100, MNIST, and ImageNet, they use these results to argue for the impact of the input distribution. However, those datasets have different number of classes. One way to fix that issue is to fix the number of random labels in all datasets. There is no discussion of this in the paper and it seems that the authors kept the label space unchanged. The problem is that it is not clear (again) when the input distribution matters much when accounting for the differences in the number of classes.\\n  * In many figures, the authors average log(EDC). Is it reasonable to average them? Wouldn't the average EDC be dominated by what happens to large models? Perhaps it would be better to report the raw results without averaging.\\n \\n- Some minor comments: \\n  * The author should provide precise definitions of some of the terms they use. For example, what is \\\"parameter-efficient\\\"? My understanding is that it is EDC divided by the parameter count. Is this the case?\\n  * The figures are not presented in the same order they appear in the main text. \\n  * Figure 2(b) is very difficult to understand. Are the authors here scaling a single dimension at a time using EfficientNet scaling recipe and reporting the average results?\\n  * Page 6: typo in \\\"we three neural network\\\"\\n  * It would be interesting to see of the scaling laws derived for ViT [1] would also increase EDC, similar to the analysis that was done on EfficientNet. \\n\\n[1] Alabdulmohsin, Ibrahim, et al. \\\"Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design.\\\" NeurIPS, 2023.\", \"Questions\": \"- Can you please clarify how the comparison across different architectures was done? Did you ensure that all architectures have the same size? How were the shapes selected? Are they based on some standard shapes, such as those used in the original ViT and EfficientNet papers? What about MLP?\\n- When using random labels, did you fix the label space in all datasets (e.g. binary classification) or was the same label space used in each dataset (e.g. 10 classes in CIFAR10 but 100 classes in CIFAR100)?\\n- Can you please clarify Figure 2(b)?\"}"}]}
{"paper_id": "svSWP21tdp", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper addresses the issue of model-induced distribution shifts (MIDS), which occur when the outputs of models in one generation contaminate the training sets for the next generation, leading to effects like performative prediction, unfairness feedback loops, and model collapse. Historical examples, like the redlining maps of the Home Owner Loan Corporation, illustrate how policy decisions can create feedback loops that perpetuate discrimination. In machine learning, such loops can entrench bias, leading to data ecosystems that are increasingly biased against minoritized groups as synthetic data pollutes new training data.\nGap: Current research lacks a unified framework for understanding how MIDS contribute to long-term unfairness in machine learning models. There is also a need for systematic approaches to mitigate these effects and improve fairness in models trained on synthetic data across generations.\nHypothesis: The authors propose that recognizing MIDS as a distinct category of distribution shift can help researchers and practitioners better understand and mitigate the adverse impacts of model training on fairness. They hypothesize that by employing algorithmic reparation as an intentional MIDS, it is possible to redress historic discrimination and improve fairness outcomes for minoritized groups subjected to other forms of MIDS.\nMethod: The authors introduce a taxonomy of MIDS and explore two experimental settings: sequential classifiers and sequential generators coupled with sequential classifiers. These settings allow them to analyze the impact of MIDS on model performance and fairness over multiple generations. Key metrics used include accuracy, demographic parity, and equalized odds differences. They also propose algorithmic reparation, which employs sampling strategies to counteract bias and promote fairness, evaluated through simulation on datasets like CelebA, ColoredMNIST, and ColoredSVHN.\nConclusion: The study finds that MIDS can lead to significant performance degradation and increased unfairness, particularly for minoritized groups. Algorithmic reparation, when employed strategically, can mitigate some of these adverse effects, though it may lead to trade-offs with model accuracy. The paper concludes with a call for more attention to data curation and the implications of synthetic data, advocating for approaches that emphasize fairness and equity in the face of evolving data ecosystems.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"- The paper addresses the important issue of studying the fairness effects of repeated training in a data ecosystem. \\n- The paper presents a clear model of sequentially training classifiers and generators that can be used to simultaneously model various effects like feedback loops, performative effects and model collapse.\", \"Weaknesses\": \"- The paper does not propose a mathematical model for the proposed MIDS scheme. Although various effects of retraining are shown through experiments, there is no theoretical investigation on the root causes of these effects, or the efficacy of the presented Algorithmic Reparation (AR) scheme.\\n- The experimental results with the proposed AR scheme based on resampling are not very convincing. For example, the decrease in the equalized odds difference with reparation in Figure 4 is not monotonic. The gap closes for a few generations and then seems to rise up again. The plots on fairness metrics in Figure 5 are even less convincing.\", \"Questions\": \"- What is the reason for the decrease in the equalized odds difference with reparation in Figure 4 not being monotonic?\\n- The paper claims that \\\"Generator-side AR improves fairness and minoritized representation\\\", but plots on fairness metrics in Figure 5 do not corroborate this claim. Please explain.\\n- I do not understand the claim, \\\"Performative prediction on model collapse leads to higher utility\\\". Please explain what you mean by this.\"}"}]}
{"paper_id": "qup9xD8mW4", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Dataset distillation involves synthesizing a small number of data points that can effectively replace large datasets for model training, and has shown success in supervised learning tasks. However, its adaptation to reinforcement learning (RL) has been limited due to the absence of a fixed dataset in RL settings.\nGap: Most existing dataset distillation methods cannot be directly used in reinforcement learning because there is no fixed dataset to sample from, which limits their application in RL tasks.\nHypothesis: The paper hypothesizes that it is possible to discover and condense the information required for training an expert policy in reinforcement learning into a small synthetic dataset of state-action pairs without access to prior expert data, thus enabling the concept of 'behaviour distillation'.\nMethod: The authors introduce HaDES (Hallucinating Datasets with Evolution Strategies), a novel method for behaviour distillation. HaDES uses a meta-evolutionary approach, applying evolutionary strategies (ES) in the outer loop to update a synthetic dataset, while employing supervised learning techniques in the inner loop to train policies on these datasets. By utilizing this bi-level optimization process, HaDES is able to distill RL information into as few as four state-action pairs that are effective across various tasks and architectures. The method was tested extensively across multiple environments, including continuous control tasks in the Brax suite and discrete tasks in MinAtar.\nConclusion: The study concludes that HaDES effectively distills RL tasks into small datasets, enabling efficient training of policies without exploration. It also shows that these datasets generalize well to a variety of architectures and hyperparameters and can be applied to multi-task learning without additional environment interaction, thus expanding the scope and utility of distilled datasets in RL. Moreover, HaDES also achieves competitive performance in traditional dataset distillation for supervised learning tasks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"+ The introduction of behaviour distillation can enrich the literature and direction of Dataset Distillation. Different from standard DD, behaviour distillation does not require access to the expert datasets. This is quite close to a standard /basic RL setting and could be a good starting point.\\n\\n+ The author's writing is easy to follow and pretty clear on the technical details\\n\\n+ The experimental section show promising results using the proposed algorithm HADES.\", \"Weaknesses\": \"- Although it is interesting, the proposed behaviour distillation seems to not have a clear motivation on why it can be useful (what's the motivation for proposing this problem and it's potential application, besides DD hasn't been applied to RL), and why not directly formulating the problem on expert dataset. Distilling directly from scratch can make the problem a lot harder.\\n- It would be great if the authors can discuss the behaviour difference of a standard RL algorithm and a synthetic data-driven RL algorithm (HADES). What's the potential benefits?\\n- One missing citation on BPTT [2]. It would be nice to add some discussion on optimization algorithm in DD, for example BPTT (the original one[1] and momentum-based[2]).\\n\\nOverall the paper is quite interesting. Looking forward to authors' response on the above questions.\\n\\n[1] Dataset Distillation\\n\\n[2] Remember the Past: Distilling Datasets into Addressable Memories for Neural Networks\", \"Questions\": \"See above.\"}"}]}
{"paper_id": "70A6oo3Il2", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Training deep neural networks towards zero training loss can lead to overfitting, as these networks often memorize entire training sets. Strategies to reduce overfitting include various regularization techniques. Recently, targeting a non-zero training loss threshold, termed as flood level, has been shown to improve generalization. However, existing flood regularization methods, such as the original Flood and iFlood, apply a constant flood level to all samples, assuming uniform sample difficulty.\nGap: Existing flood regularization methods treat all training samples as having similar difficulty by applying a constant flood level across the board. This does not account for samples with varying levels of uncertainty, noise, or difficulty, potentially limiting the effectiveness of the regularization.\nHypothesis: A flood regularization method that adapts the flood level based on individual sample difficulty can improve the generalization performance of models by preventing overfitting on noisy or hard-to-fit samples while allowing easier samples to converge towards lower training loss.\nMethod: The AdaFlood method involves using an auxiliary network to determine an adaptive flood level for each training sample based on its difficulty. This auxiliary network is trained on a subset of the training data, with a correction function applied to adjust predictions that might overestimate the loss of noisy samples. The adaptive flood levels are pre-computed before training the main network and can be shared across different runs. The method was tested on datasets with various input modalities, utilizing different neural network models and regularization techniques. Experimental comparisons were made against other flood regularizers on tasks including image and text classification, regression, and probability density estimation for asynchronous event sequences.\nConclusion: AdaFlood generally outperforms previous flood regularizers across a variety of tasks and is particularly robust to noise in the data. By adapting flood levels per sample difficulty, AdaFlood improves model generalization performance and calibration, demonstrating its versatility and effectiveness across different data modalities.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"-The proposed method is reasonable. Different samples has different difficulty. It is reasonable to use different loss.\\n-The paper is well-written and easy to follow.\", \"Weaknesses\": \"-This paper lacks experiments on large-scale dataset, such as ImageNet. CIFAR and SVHN are small datasets, which are very different from the real sense.\\n\\n-Does the adaptive strategy bring extra training cost? More detailed discussion is required.\", \"Questions\": \"See the weakness.\"}"}]}
{"paper_id": "ZULjcYLWKe", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Offline reinforcement learning (RL) is aimed at learning from offline datasets without interacting with the environment, addressing the challenges of high costs and risks associated with exploration in online settings. However, a key challenge in offline RL is ensuring robustness against perturbations in state observations, which can arise from sensor errors or adversarial attacks. Current approaches to robust RL often involve adversarial training, which is not directly applicable in the offline setting where interactions with the environment are not possible.\nGap: The current solutions for robustness in offline RL struggle with handling observation perturbations effectively without becoming too conservative. Specifically, existing methods may suffer from performance degradation when noise scale increases, particularly in complex environments with high-dimensional state and action spaces. Ensuring robustness against state perturbations without the ability to interact and adjust during training remains a significant challenge in offline RL.\nHypothesis: The hypothesis is that a framework using diffusion models, specifically a Diffusion Model-Based Predictor (DMBP), can effectively recover true states under perturbed observations by employing conditional diffusion models. This approach is expected to enhance the robustness of state-based offline RL strategies against various types of noise and perturbations by optimizing a non-Markovian training objective.\nMethod: The proposed method, DMBP, leverages conditional diffusion models to denoise observed states, improving the robustness of offline RL policies. Unlike classical diffusion models, DMBP employs a non-Markovian training objective that minimizes the entropy of denoised states over RL trajectories, thus mitigating error accumulation. The system is implemented with a Unet-MLP neural network structure to aid noise prediction, enhancing the representation of trajectory information. For experimental validation, extensive tests are conducted on standard benchmark problems from the D4RL dataset, examining both robustness against added noise and handling of incomplete state observations. The implementation is available in an open-source repository.\nConclusion: The experiments demonstrate that DMBP significantly improves the robustness of offline RL algorithms against a variety of perturbations, including random and adversarial noise. It also effectively addresses issues related to incomplete state observations, outperforming existing offline RL methods. The findings suggest that diffusion models can serve as viable noise reduction tools, rather than purely generative models, in practical offline RL applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"1. DMBP strengthens the resilience of existing offline RL algorithms, allowing them to handle different scales of random noises and adversarial attacks effectively.\\n2. Unlike traditional approaches, DMBP leverages diffusion models primarily for noise reduction, rather than as generation models. This innovative use helps in better state prediction and recovery against observation perturbations.\\n3. The framework introduces a non-Markovian loss function, specifically designed to prevent the accumulation of estimation errors over the RL trajectory.\\n4. DMBP offers a more balanced approach than methods that train robust policies against worst-case disturbances. This ensures that policies do not become overly conservative, which can hinder performance in certain scenarios.\\n5. DMBP's inherent ability, derived from the properties of diffusion models, allows it to effectively manage situations with incomplete state observations. This is particularly valuable in real-world applications, such as when robots operate with compromised sensors.\", \"Weaknesses\": \"1. The paper has the assumption that perturbation on state space follows a Gaussian distribution. However, in practical settings, such perturbations might be biased and skewed. For example, a water drop on a camera might lead to distortion of captured images. Therefore, it might be better if the authors could elaborate on the assumption of the perturbations and how they are produced. \\n2. Introducing a diffusion model for noise reduction could increase the complexity of the model, which may lead to longer training time and resource-intensive computations. The authors might want to include more details regarding training time and computational resources.\\n3. Scaling to very large state spaces or handling very noisy environments might pose challenges, especially when using diffusion models. It would be better for the authors to consider testing on tasks with larger state spaces such as the humanoid. \\n4. Eq.(2) is wrong as the recovered state should not be a probability.\", \"Questions\": \"Same as the weakness.\"}"}]}
{"paper_id": "v5Bb7F1Wkf", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Incremental Learning (IL) is crucial in dynamically evolving applications such as autonomous robots, where models are required to learn new tasks sequentially without accessing data from completed tasks. Traditionally, IL faces the challenge of balancing plasticity (learning new tasks) with stability (retaining old-task knowledge). Existing methods have attempted to address this through approaches like memory-based, regularization-based, and parameter-isolation techniques. Memory-based methods store data to prevent forgetting, regularization-based techniques penalize changes to maintain stability, and parameter-isolation approaches allocate specific model parts to tasks, but each comes with trade-offs.\nGap: The challenge in exemplar-free IL, where no old-task data can be retained, lies in achieving a balance between plasticity and stability without excessive memory consumption. Existing methods either use significant memory or tend to compromise either stability or plasticity.\nHypothesis: The authors propose that averaging model parameters from independently learned models for each task can preserve knowledge across tasks effectively. Based on this, they suggest a dual-learner framework with cumulative parameter averaging to manage knowledge transfer between a plastic learner (for new tasks) and a stable learner (for accumulated knowledge from all tasks).\nMethod: The Dual-Learner framework with Cumulative Parameter Averaging (DLCPA) involves a cyclic method where: (i) a plastic learner is optimized for new tasks using both self-supervised and supervised loss functions to enhance feature extraction robustness, (ii) a stable learner receives cumulative average updates from the plastic learner to maintain generalization across tasks, and (iii) task-specific classifiers are adjusted to align with the stable learner. The approach was tested on CIFAR-100 and Tiny-ImageNet datasets, where DLCPA outperformed state-of-the-art exemplar-free IL methods in both Task-IL and Class-IL settings.\nConclusion: The proposed DLCPA framework significantly enhances the balance between plasticity and stability in exemplar-free IL scenarios. By using cumulative parameter averaging, it effectively consolidates and transfers knowledge across tasks, achieving competitive results while maintaining memory efficiency. However, the method assumes clear task boundaries and equal task importance, and future work might address these limitations to expand DLCPA's applicability.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"## Presentation\\n- The paper is clearly written for the most part.\\n\\n## Contribution\\n- The core idea behind the presented method is simple.\\n- The method seems to work well on Tiny-imagenet\\n- The analysis of how the proposed stable-plastic architecture can help is nice and simple.\", \"Weaknesses\": \"## Presentation\\n- The \\\"Analysis\\\" section contains the problem definition and explanation of the baselines, which are background information. It would be better to split this information into a clear background section.\\n- The task settings \\\"Task-IL\\\" and \\\"Class-IL\\\" are never defined in the paper.\\n- The paper assumes that storing previous models or datasets is so costly that it is a big disadvantage to the memory-based methods. However, from a lay perspective, it would seem that storage is very cheap. I suggest the authors include more motivating discussion about why the memory-based IL methods have a disadvantage.\\n\\n## Contribution\\n- The self-supervised learning (SSL) objective is introduced without appropriate analysis and baselines.\\n    - From the ablation studies, we see that the SSL objective is a crucial component of the proposed method. However, I don't see how SSL is specific to the proposed method. Without applying SSL objectives to at least some of the most competitive baselines, we don't know how much of the improvement from the proposed method is actually due to the stable-plastic learner architecture.\\n    - Three different SSL losses are proposed to be used with the method. The best results in each of the datasets are split between each of the SSL objectives. On some datasets, choosing a different SSL objective would lead to the proposed method no longer beating the baselines. This seems like a weakness of the method, in that you have to train three versions of it in order to pick which one you want to deploy.\", \"Questions\": \"- Which SSL objective should a user choose?\\n- To which baselines is the SSL objective applicable?\\n    - How well would those baselines do with the SSL objective?\\n- Is there a practical task, where one of the methods considered in this paper would work well, but storing all of the data would become prohibitively expensive?\"}"}]}
{"paper_id": "eADuHv62J5", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Long-tailed data distributions, where a few head categories have abundant data and many tail categories have scant data, are common in domains such as e-commerce, finance, biomedical science, and cybersecurity. Machine learning models typically perform well on head categories but poorly on tail categories.\nGap: Despite numerous studies to address long-tailed learning, challenges remain in characterizing the extent of long-tailedness, understanding the performance across different tasks and domains, and addressing cross-domain applications.\nHypothesis: The authors propose that a comprehensive benchmarking framework, considering the characterization of long-tailedness, data complexity across domains, and task heterogeneity, could provide a systematic view for evaluating long-tailed learning approaches.\nMethod: The paper introduces HeroLT, a comprehensive long-tailed learning benchmark integrating 15 state-of-the-art algorithms evaluated on 16 real-world datasets across five tasks (classification, multi-label text classification, image classification, instance segmentation, and node classification) in three domains (natural language processing, computer vision, and graph mining). HeroLT uses six evaluation metrics and conducts 304 experiments to offer a thorough performance evaluation. Novel aspects include introducing the Pareto-LT Ratio to measure dataset long-tailedness, providing a taxonomy of data complexity, and addressing heterogeneous tasks.\nConclusion: HeroLT's extensive benchmarking reveals that no single algorithm outperforms others across all tasks and domains, emphasizing the importance of scenario-based algorithm selection. The study also underscores the significance of characterizing long-tailedness and suggests future directions such as better theoretical analysis tools, cross-domain algorithm development, and addressing under-explored applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The long tail problem is a practical and valuable research topic.\\n2. Benchmark contains different data types and tasks.\", \"Weaknesses\": \"1. Most of the datasets and tasks seem to have been well constructed and the paper seems to merely concatenate them together without much additional integration.\\n2. The methods discussed in the paper appear to be somewhat dated. For instance, the image classification data that is prominently featured in Table 2 is based on methodologies that were largely developed before 2020. Considering that it is already near the end of 2023, this renders the insights provided by the paper less credible.\\n3. Referring to section C.1 of the paper, there is a misalignment in the model architectures and the number of epochs used when comparing different methods, which calls into question the article's claim of a 'fair evaluation'.\\n4. According to the four-point analysis in the upper half of the third page, the Pareto-LT Ratio still requires integration with the Gini coefficient and IF to describe the dataset. The former primarily represents the number of categories, while the latter two indicate data imbalance. So why not directly use the number of categories as a descriptor? Additionally, I do not find comprehensive experimental and theoretical evidences to support this four-point analysis.\", \"Questions\": \"See above.\"}"}]}
{"paper_id": "ulMXGO1fdH", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Spiking neural networks (SNNs) hold potential for energy-efficient machine learning by processing streaming data. Training SNNs on temporal sequences remains challenging, primarily done with backpropagation through time (BPTT), which is unsuitable for online learning. Alternatives like Real-Time Recurrent Learning (RTRL) provide exact gradients suitable for online learning but incur high computational costs, making them impractical for larger networks.\nGap: Current methods for approximating real-time gradient calculations, such as Online Spatio-Temporal Learning (OSTL) and Online Training Through Time (OTTT), restrict scaling and learning performance due to simplified assumptions about temporal dynamics, especially in deep networks. These methods often ignore the impact of previous neuronal outputs on downstream neurons, which leads to degraded learning performance in temporal sequence tasks.\nHypothesis: The authors propose that a novel approximation method, Online Training with Postsynaptic Estimates (OTPE), can improve online learning of feed-forward SNNs by capturing temporal effects on subsequent layers' activities that current approximations miss. This method would potentially provide better gradient alignment with exact calculations and enhance performance with minimal overhead.\nMethod: The authors developed OTPE, an algorithm for training feed-forward SNNs, which maintains a running weighted sum of the synaptic influence of past spikes, approximating the temporal effects in RTRL but with improved scalability. The algorithm applies a local layer-based derivative computation without surrogate derivatives when capturing post-synaptic temporal dynamics. They evaluated the learning performance on various datasets, including synthetic and realistic spiking datasets like Spiking Heidelberg Digits (SHD), and compared it against established algorithms such as BPTT, OTTT, and OSTL.\nConclusion: OTPE proved to have superior directional alignment with exact gradients and higher average performance in tasks requiring temporal credit assignment, such as the SHD. It demonstrated scalability and outperformed existing approximation methods like OTTT and OSTL in multi-layer configurations. This suggested that OTPE effectively captures temporal dependencies required for efficient online learning in SNNs.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"1. This paper considers facilitating online learning in SNNs, which is important for biological plausibility and neuromorphic hardware.\\n\\n2. Experiments demonstrate large improvements on gradient angle alignment and better performance than existing methods.\", \"Weaknesses\": \"1. The presentation can be improved. It is not quite clear what is the major component in the methodology that leads to the large improvement over existing methods. From Section 3, OTPE is very similar to OSTL and there is no explicit description of the key difference. From Section 3.1, AOTPE differs from OTTT only in that AOTPE uses a running weighted average of surrogate derivatives. Why this small change can almost double the cosine similarity given the similar time and space complexity? Is there more formal theoretical justification? From the derivation, OTPE also makes many approximations, why these approximations have less effect than OSTL/OTTT? Besides, what does the term \u201cpostsynaptic estimates\u201d mean?\\n\\n2. Scalability is mentioned multiple times in the paper. Are there more large-scale (datasets, network, etc.) results?\\n\\n3. In the abstract, \u201crate-based and time-based encoding\u201d is mentioned. However, there is no corresponding part in the paper.\", \"Questions\": \"See weakness.\"}"}]}
{"paper_id": "lEkFq4RUCX", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: 3D point clouds, used in computer vision, 3D modeling, and robotics, represent geometric shapes of objects in an unstructured form. Traditional methods for measuring differences between 3D point clouds, like Earth Mover's Distance (EMD) and Chamfer Distance (CD), focus on point-wise correspondence. These methods are often memory and computationally heavy (EMD) or prone to reaching local minima (CD).\nGap: Current metrics for assessing the dissimilarity between 3D point clouds focus on point-wise differences and are often inefficient or ineffective, particularly when different sampling might represent the same surface. These metrics miss evaluating the geometric surface differences and are computationally intensive or lack robustness in practical scenarios.\nHypothesis: The authors propose a new metric, Directional Distance Field (DDF), which measures the difference between the underlying 3D surfaces of point clouds through directional distances from reference points. This metric is expected to be more efficient and accurate for tasks involving 3D point cloud processing, like shape reconstruction, registration, and scene flow estimation.\nMethod: The DDF metric involves generating reference points that induce local surface geometry of 3D point clouds. Each reference point calculates its directional distance to both point clouds. The difference in these distances characterizes the geometric difference. The final DDF is the average of these differences across all reference points. This method transforms and evaluates multiple 3D tasks - including shape reconstruction using normalized point clouds from ShapeNet, rigid registration with the Human dataset, scene flow estimation with Flyingthings3D, and feature representation using auto-encoding.\nConclusion: The proposed DDF metric significantly improves accuracy in analyzing 3D point clouds across various tasks, offering a more memory-efficient and computationally effective solution compared to existing metrics like EMD and CD. By focusing on underlying surface differences rather than point-wise correspondences, DDF enhances the potential of 3D point cloud processing frameworks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 3, \"Strengths\": \"1. It is reasonable to utilize the surface where the point clouds should be to compute the distance of two 3D point clouds.\\n2.The experiments show the proposed method improves the performance of all the downstream tasks.\\n3. The implementation of methods of all the downstream tasks are explained in detail.\", \"Weaknesses\": \"The generation of reference points are not explained very clearly. The reviewer is confused by the shared identical weight operation and the reference point generation process.\", \"Questions\": \"The reference points are generated from one of the two point clouds, and the weights in Equ.1 are computed for each kNN points and reference points. The reviewer wants to ask how to share the weights in g(q_m, P1) and g(q_m, P2)?\"}"}]}
{"paper_id": "g9diuvxN6D", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Instruction fine-tuning has emerged as a promising approach to improving zero-shot capabilities of LLMs, enhancing the performance of smaller models to be competitive with larger counterparts. Prior work highlights the potential of instruction tuning and its success across various benchmarks, but issues regarding LLMs understanding and robustness to prompt phrasings remain under-explored.\nGap: There is a lack of comprehensive analysis on the robustness of instruction-tuned LLMs to variations in instruction phrasings, especially in zero-shot contexts. It is essential to determine if models are overly sensitive to unobserved instructions, which could limit practical utility.\nHypothesis: Instruction-tuned models are not robust to variations in instruction phrasings, leading to performance degradation with unobserved instructions. Introducing a method to align the representations of semantically equivalent instructions could improve robustness.\nMethod: The authors collected 319 novel instructions from NLP practitioners for over 80 tasks and evaluated LLMs against unseen instructions. They introduced 'soft prompt' embedding parameters that optimize the similarity between representations of equivalent instructions. Experiments were conducted using large benchmarks like MMLU and BIG-BENCH with various instruction-tuned models (Flan-T5, Alpaca, T0).\nConclusion: The study found that LLMs show significant performance variability when using unobserved instructions. The proposed method of aligning instruction representations improved robustness, indicating potential for addressing the sensitivity of instruction-tuned models to phrasing variations.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"- The experiments are clear and well-thought-out, and the results are solid and interesting, affecting many current popular models (Llama finetunes), and holding for different model families.\\n- The observed correlations between performance and instruction similarity (in representation space) are interesting (even if the correlation is somewhat weak).\\n- The proposed fix method is well-motivated, and the results seem reasonable, especially for the Alpaca-7B model.\\n- The robustness of models to different prompts is an important and interesting area of study, especially considering current difficulties with LM evaluations.\", \"Weaknesses\": \"- The novelty of the robustness experiments is somewhat limited, as there is prior work that has observed variance based on prompt formatting (discussed in the work itself). Considering this work seems to examine more modern models such as Alpaca, and considers a different set of tasks, I think this is a minor weakness - the work does add a number of interesting insights, especially around representation distance.\\n- The claim that models perform worse at novel phrasings of tasks seems a bit strong compared to the results: examining Table 3, the variance of performance on unobserved phrasings often is greater than the difference in average performance, suggesting that the difference in performance may not be significant (although the large increase in variance is certainly not desirable and interesting to see).\\n- While the proposed method seems strong, examining the results in Appendix B.3 seems to show that in many cases, prompt-tuning alone performs within one standard deviation of the PT+KL results (e.g., for BBL unobserved and average). As such, the proposed method might not be significantly different from just prompt tuning.\\n\\nOverall, I think this paper is okay, and the robustness results are interesting and useful to see, along with the representation alignment results. However, the novelty of the robustness experiments is a bit limited, and it\u2019s not clear the proposed method for improving this is significantly better than baselines.\", \"Questions\": \"- How does your proposed method compare to introducing the model paraphrases during initial instruction finetuning? Training on the paraphrased versions alone after the instruction tuning may cause forgetting in an undesirable way.\\n- Is the degradation of performance on unobserved prompts statistically significant? The variance seems very high.\\n- Doing prompt tuning alone seems to often perform very similarly to PT+KL, and within the reported standard error in Appendix B.3. Do you have statistical tests for significance for your results?\\n- It would be interesting to see the results in Table 7 broken down into the BBL QA/BC/MC categories shown in Table 6 to see how much the difference in closest distance affects the performance of these different categories.\"}"}]}
{"paper_id": "B9klVS7Ddk", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large Language Models (LLMs) have achieved significant advances in NLP, computer vision, and other areas but are limited by their extensive computational and memory requirements. Recent efforts to mitigate these limitations focus on network pruning and weight quantization, achieving up to 60% sparsity and reducing bit-width significantly with negligible impact on performance metrics like perplexity.\nGap: There is an over-reliance on perplexity as the primary metric for evaluating the performance of compressed LLMs, which fails to capture subtle but important changes in capabilities. There is a need for a more comprehensive evaluation protocol to accurately assess the abilities of compressed LLMs, especially in knowledge-intensive tasks.\nHypothesis: Current state-of-the-art (SoTA) compression methods, which rely heavily on perplexity for evaluation, may not effectively preserve the full capabilities of LLMs. More sophisticated and realistic benchmarks are needed to reveal the true performance impacts of these methods.\nMethod: The authors introduce the Knowledge-Intensive Compressed LLM BenchmarK (LLM-KICK), a suite of tasks designed to provide a holistic evaluation of compressed LLMs' abilities across language understanding, reasoning, generation, in-context retrieval, and summarization. They evaluate pruning and quantization methods using widely recognized LLMs like Vicuna and provide empirical results on performance degradation under different compression techniques across various tasks. The experiments include different sparsity patterns, such as unstructured and structured N:M sparsity, using open-source datasets.\nConclusion: LLM-KICK reveals several critical insights: current SoTA pruning methods cause significant performance degradation in knowledge-intensive tasks even at modest sparsity ratios; quantization methods maintain performance better than pruning; and perplexity-based evaluations overlook these impacts. The benchmark suggests that even highly compressed LLMs can be robust in in-context retrieval tasks but suffer greatly in understanding and generating enriched, factually correct content. It highlights the need for developing more nuanced compression evaluation metrics and methodologies.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"1. Compression of LLMs is very timely and important.\\n\\n2. The paper reveals new and yet widely unknown gaps in compressed LLMs in comparison to their uncompressed counterparts. \\n\\n3. The paper shows that compressed models may offer better performance in some tasks (e.g., In-Context Text Summarization) than others (e.g., Factoid-based Question Answering)\\n\\n4. The authors plan to release their code which may be help in the development of future compression techniques.\", \"Weaknesses\": \"1. It would make the conclusions more robust and convincing if the evaluations use more than a single family of LLMs (i.e., Vicuna).  Why not repeat these experiments with, e.g., Llama 2 and Falcon?\\n\\n2. Regarding the observation that even 8-bit quantization has evident gaps with respect to uncompressed models, have the authors considered evaluating LLM.int8()?  (https://arxiv.org/pdf/2208.07339.pdf) \\n\\n3. It would help the reader to have a table summarizing all the tasks' performance over the different architectures, compression techniques and the their resulting perplexity.\", \"Questions\": \"See weakness 1 and 2. Also, does the authors have insights regarding why, in the paper's evaluation, quantization works better than pruning?\"}"}]}
{"paper_id": "HXZK1Z8tHa", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Image restoration is a classical inverse problem aiming to reproduce high-quality images from degraded inputs. CNN-based methods initially dominated this field, but limitations like excessive inductive bias and limited perceptual fields led to the adoption of Transformer-based methods. Transformers are advantageous for handling long-range dependencies, but they pose challenges such as high computational overhead when dealing with high-resolution images.\nGap: The primary issues with existing Transformer-based image restoration methods are their long inference times and complex training processes. The trade-offs between model latency and trainability remain unresolved, as convolutional modules can improve trainability but not reduce latency, while sparsification can decrease latency but complicates optimization.\nHypothesis: The authors propose that sharing attention maps among neighboring blocks, coupled with residual connections on the 'Value' of self-attention, can achieve a balance between latency reduction and improved trainability, leading to a more efficient and easier-to-train Transformer model for image restoration tasks.\nMethod: The proposed method, ShareFormer, introduces Shared Portion Stripe Attention (SPSA) to share attention maps between neighboring blocks, reducing redundancy and computational overhead. Residual connections are added to the 'Value' in self-attention to enhance optimization and introduce a local inductive bias. The model architecture is built with Share Attention Blocks (SAB) and combined shared attention unit (CSAU), ensuring efficiency and performance across various tasks like image super-resolution, denoising, and JPEG compression artifact reduction. The architecture is evaluated using large datasets and compared against state-of-the-art methods concerning latency, flops, and visual quality improvements.\nConclusion: ShareFormer effectively reduces network latency and improves trainability compared to existing Transformer-based methods, achieving state-of-the-art performance across multiple benchmarks. The developed model provides an optimal balance between speed and accuracy, demonstrating the potential for further applications in other image restoration tasks and possibly extending to high-level vision problems.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 5, \"Strengths\": \"[S1] The references and related works cited by the authors are very recent and relevant.\\n\\n[S2] The Lesion study presented in Sec.4.1 is comprehensive for demonstrating the proposed shared attention is highly related to ensemble behavior of the sequentially placed attention mechanism. Deleting or permutating some self-attention layers could smoothly increase the reconstruction loss. This can be the evidence that the shared attention is not the ensembles of shallow networks.\\n\\n[S3] The performances of ShareFormer for large SR and lightweight SR are notably enhanced, compared to recent SOTA methods. These results were achieved with smaller model size, fewer computations, and faster speed than the others. (But an unfair issue remain. See question Q4.)\", \"Weaknesses\": \"[W1] The presentation of text and figures in this paper should become clearer and more understandable. See Q1, Q2, and Q3.\\n\\n[W2] The paper lacks some ablation studies proving the importance of the proposed components, such as CSAU. The authors should study architecture variants if they hope to clarify that CSAU comes not from insufficient considerations but from careful construction. And the efficiency and effectiveness differences between the original GDFN and the enhanced CSAU must be provided.\\n\\n[W3] Most importantly, the core parts of ShareFormer, shared attention, seem not novel. In ELAN, one of the sota methods, shared attention map mechanisms have been already proposed. Moreover, it has been already shown how sharing attention maps in more than one layers can impact on the performance and efficiency of the model (related to your Tab.8).\\n\\n[W4] A potential unfair issue is observed with respect to SR. See Q4.\\n\\n[W5] Despite the improved inference speed, the performance gains of grayscale denoising, color denoising, and JPEG CAR are not significant.\", \"Questions\": \"[Q1] Where is the exact part that the shared attention is operated? Eq.(4), (5) apparently reveals that this operates in $(l-1)$-th and $l$-th SPSA blocks. However, Fig.2 illustrates attention map of SPSA is shared to CSAU, while Fig.3 depicts sharing attention map appears after residual connection on $\\\\textit{value}$. The reviewer thinks that the explanation of Eq.(4), (5) is the correct case the authors intended, while the phrase, \\\"**sharing attention map**\\\", in Fig.2 and Fig.3 is confusing. If it\u2019s right, \u201csharing\u201d can be omitted. Or not, please let me know what your first intention with respect to the exact parts of sharing attention map is.\\n\\n[Q2] This question is related to Sec.4.2. From ConViT, how can you draw the conclusion that the concentrated ERF implies an amplified locality bias of the network? The reviewer thinks that it is not sufficient for the authors to claim that this fact shows the locality bias of the residually connected $\\\\textit{value}$ in ShareFormer. I cannot find the acceptable evidence from ConViT paper. Don\u2019t you have any other evidence justifying your claim, such as visualization?\\n\\n[Q3] In Appendix D, why did you compare the attention maps of layers 1 and 3? ShareFormer shares attention map in layer-order-number pairs (0, 1), (2, 3), \u2026, (2n-2, 2n-1), respectively. Thus, comparison of attention maps in \u201clayers 0 and 1\u201d or \u201clayers 2 and 3\u201d is more compelling to demonstrate attention map redundancy. Additionally, I recommend the authors to compare attention redundancies of the cases where the shared attention is \u201cemployed\u201d and \u201cnot employed\u201d in the proposed ShareFormer, instead of SwinIR cases.\\n\\n[Q4] Did you apply the Progressive Learning to large and lightweight SR tasks with training patch size of from 128 to 384? However, the comparative methods, SwinIR, ELAN, and DLGSA, used smaller patch size, such as 48x48 and 64x64. I am concerned that this leads to a potential unfairness issue.\\n\\n\\n\\n**Minor issue (not necessary to be mentioned in author rebuttal, if the authors struggle to a limit on the number of characters of rebuttal.)**\\n\\n(1) Fig.2 omits (a) and (b) mark, while the caption uses (a) and (b).\\n\\n(2) In the last to second sentence of Tab.1 caption, Restormer never used window attention. Moreover, you should mention what is MHDA, which seems a typo. (In Restormer, MDTA was used, and MHDA was not shown in SwinIR.)\"}"}]}
{"paper_id": "o6AN2ZoNw2", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper focusses on the task of open-vocabulary semantic segmentation, a field that assigns semantic labels to each pixel in an image utilizing arbitrary, open-vocabulary texts. Recent advancements in object detection and segmentation have been driven by frameworks like Mask R-CNN and DETR, but an enabling framework for open-vocabulary segmentation has been relatively underexplored.\nGap: Prior techniques often rely on annotated masks or large-scale pre-trained models for training, leading to complexity and potential overfitting to known categories. These techniques generally do not address the direct alignment of pixel-level features with language, which is necessary for true open-vocabulary segmentation without reliance on fixed category sets.\nHypothesis: The authors propose that a simple framework leveraging pseudo-mask generation combined with language supervision, without the need for annotated masks, can effectively train a MaskFormer model for open-vocabulary semantic segmentation, achieving state-of-the-art results compared to more complex models.\nMethod: P-Seg is built upon the MaskFormer architecture without requiring annotated image masks for training. Instead, it utilizes a pseudo-mask generator to produce artificial masks from image tokens derived using a self-supervised ViT. Semantic supervision is provided by associating language features, derived from noisy image-text datasets, with the mask embeddings. Training involves an image-text contrastive loss to align features and includes a self-training enhancement step leveraging model predictions to further refine predictions. The method is evaluated against benchmarks (Pascal VOC, Pascal Context, COCO), demonstrating scalability and improved results with increased data.\nConclusion: P-Seg demonstrates that open-vocabulary semantic segmentation can be achieved via simple, yet robust, pixel-level and language alignment without the need for extensive supervised segmentation data or large pre-trained models. It achieves state-of-the-art results, highlighting its effectiveness and scalability, and serves as a competitive alternative to prior methods that require more resource-intensive setups.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The presented method achieves competitive performance while training models from scratch (with the exception of using DINO for pseudo-supervision).\\n2. The paper investigates performance scaling with increasing dataset size, showing improvements. \\n3. The writing is clear, and the authors promise to release code and checkpoints soon.\\n4. The method obtains competitive results.\", \"Weaknesses\": \"The central weaknesses of this paper include the limited novelty of the proposal and the need for appropriate positioning and comparison within existing literature, which require further clarification of some claims.\\n\\n### Novelty\\n1. The proposed architecture closely follows the proposed scheme of ZegFormer [A] with minor modifications, such as not using a pre-trained language encoder and not using an additional image encoder during inference. The proposed method is not compared to ZegFormer, and the differences are not be discussed.\\n2. The scheme of pseudo-mask labelling self-training is similarly widely employed; most recently, CutLER [B] proposes to train an unsupervised segmenter using masks obtained from DINO clustering. \\n3. The use of pseudo-mask generation in the context of OV segmentation has also been explored in DiffuMask [C], which might require further discussion. \\n\\n### Comparisons and Claims\\n4. The paper claims the proposed method P-Seg achieves SOTA results, but the comparative baselines could be more recent. Critically, methods such as OVSegmentor [D], TCL [E] and SegCLIP [F] are not included in comparisons. The lack of such baselines is critical because, based on reported results in the paper, P-Seg would not be SOTA on Pascal VOC (OVSegmentor, TCL) and Context (TCL) and COCO (TCL), which contradicts the bolded claim in the introduction. Additionally, OVSegmentor is trained on smaller datasets and similarly does not use CLIP or ALIGN models, relying instead on DINO (like P-Seg), which offers a closely matched baseline.\\n5. Additionally, the paper compares on the COCO dataset with 80 classes and backgrounds, which matches other papers' descriptions of the COCO-Object dataset. The dataset description might need some additional clarification. CLIPpy results are reported much lower and match those of full COCO (133 classes) rather than COCO-Object (80 classes) from the paper, which might need correcting.\\n6. It might also be more reasonable to include more recent _mask+text_ methods such as OVSeg [G] or ODISE [H] in the comparison to provide a more complete picture of the models and performance attained in the field.\\n6. It should also be clarified in what sense are the methods \\\"zero-shot\\\"? Both P-Seg and P-Seg+ likely observe instance images of the target class during training. This is because the word token embeddings of the corresponding classes in the language model were trained, so training datasets should have contained target classes. P-Seg+ is trained on the training sets of the evaluation datasets, which include classes in question by design. Perhaps the paper uses more of a CLIP-like interpretation of \\\"zero-shot\\\", where the model is not trained on the target classes but on a large dataset of images and text that likely _contained_ such instances. This should be clarified. In any case, P-Seg+ should not be classed as zero-shot since it is trained on the train spits of evaluation datasets. As an alternative, observed/unseen class comparisons could be made like in other zero-shot semantic segmentation papers (e.g. [C]).\\n\\n### Experimental Evaluation\\n7. The paper lacks ablations that confirm the design choices of the method. The ablation presented centres around showing that the method improves with a larger amount of data. However, this confirms the general trend of deep learning methods and is not, arguably,  very surprising. The question of what makes the method work needs to be addressed, as they are critical to understanding and building on top of this work.\\n    - Why use simple K-Means on top of DINO features? Prior methods for extracting masks using DINO features have been developed, such as LOST [I], TokenCut [J], and MaskCut [B], which have achieved state-of-the-art results in unsupervised segmentation with mere modification of the clustering procedure. These also offer better points of comparison to the pseudo-mask extraction method. \\n    - Is setting K=8 critical? This needs to be discussed. How sensitive is the method to this choice? Nouns are extracted from the caption. Why not use the full caption? Why not set K to some function of the number of nouns in the caption?\\n    - Why are language features compared to predicted mask features? An alternative would be to compare the average features used to obtain dense masks. Is this choice critical?\\n    - Why is P-Seg+ using a different architecture and different, now pre-trained, backbone? This avoids the opportunity to initialize from large-scale training of the P-Seg. Is the large-scale VL pre-training detrimental in this case?\\n\\n### Presentation and Clarity\\n8. The paper also needs some crucial details.\\n    - The evaluation protocol for including background could be clearer. How is background class determined in P-Seg/P-Seg+? Is textual prompt used for background? What are textual prompts used for the categories? Prior works rely on templates or prompt expansion to obtain textual prompts for classes. Such details should be included.\\n    - The procedure for self-training should be discussed in detail. In particular, from the description in Sec. 3.3.4, the new model is trained using predictions of the current model, which are described as pixel-dense masks and class labels (Section 3.1, last paragraph). **This makes P-Seg+ a standard closed-set segmentation method, which is not entirely comparable to other methods used in the paper.** If this is the case, then it should be clarified.\\n9. Presentation of qualitative results should clearly indicate that **P-Seg results are obtained using a CRF**, as it is only mentioned in Appendix C and not when such comparisons are discussed. A comparison could be made without CRF to show substantial differences, as other works might similarly benefit from CRF, and CRF might obscure key differences in results.\\n10. It also needs to be clarified why specific models in Tables 3 and 4 have multiple entries with different results for the same dataset. How do these models differ? Are these different runs of the same model? This should be clarified.\\n\\n---\\n#### References\\n- [A] \\\"Decoupling Zero-Shot Semantic Segmentation\\\" Ding et al. CVPR 2022\\n- [B] \\\"Cut and learn for unsupervised object detection and instance segmentation\\\" Wang et al. CVPR 2023\\n- [C] \\\"DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models\\\" Wu et al. ICCV 2023\\n- [D] \\\"Learning Open-Vocabulary Semantic Segmentation Models From Natural Language Supervision\\\" Xu et al. CVPR 2023\\n- [E] \\\"Learning to Generate Text-grounded Mask for Open-world Semantic Segmentation from Only Image-Text Pairs\\\" Cha et al. CVPR 2023\\n- [F] \\\"Segclip: Patch aggregation with learnable centers for open-vocabulary semantic segmentation\\\" Lou et al. ICML 2023\\n- [G] \\\"Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP\\\" Liang et al. CVPR 2023\\n- [H] \\\"Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models\\\" Xu et al. CVPR 2023\\n- [I] \\\"Localizing Objects with Self-Supervised Transformers and no Labels\\\" Simeoni et al. BMVC 2021\\n- [J] \\\"Tokencut: Segmenting objects in images and videos with self-supervised transformer and normalized cut\\\" Wang et al. CVPR 2022\", \"Questions\": \"The key questions have been presented alongside the weaknesses. The following are additional questions that could be considered, though they are very minor. \\n\\n- Why were the models not pre-trained? Using a pre-trained backbone for a segmentation network is a common practice. It is also not clear why the language model needs training from scratch. It seems that using a language model pre-trained on a large corpus would be beneficial.\\n- The emphasis seems to be placed on processing time for pseudo-mask generation. However, the method is trained using 30 epochs, which would likely take a lot longer than any pre-calculation of pseudo-masks (single epoch). It is not clear why processing time is so critical.\\n- The text says that N binary masks are predicted. How are these masks converted to C-way dense masks? How are possible overlaps handled?\"}"}]}
{"paper_id": "sKPzAXoylB", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Continual learning poses significant challenges for deep representation learning models, such as catastrophic forgetting and loss of plasticity. Catastrophic forgetting refers to the loss of previously acquired knowledge when new information is learned, and loss of plasticity is the diminishing ability of models to adapt to new information over time. Existing approaches often address these issues separately, mainly focusing on environments where task boundaries are known, and data can be stored and replayed, which is not feasible in streaming learning settings.\nGap: There is a lack of approaches that can simultaneously address catastrophic forgetting and loss of plasticity, particularly in streaming learning environments without relying on known task boundaries, replay buffers, or pretraining. This gap poses a challenge for real-world applications where non-stationary data streams must be processed online.\nHypothesis: The authors propose that by using Utility-based Perturbed Gradient Descent (UPGD), which incorporates perturbations and utility-based gradient updates, it is possible to address both catastrophic forgetting and loss of plasticity simultaneously. The approach optimizes gradient updates by adjusting them based on the importance of neural network units, thus protecting useful units and rejuvenating less useful ones.\nMethod: The proposed method, UPGD, combines gradient updates with utility-based perturbations. It determines the utility of network weights using a Taylor series approximation of the loss function, distinguishing between useful and less useful weights. Smaller updates are made to protect useful weights (which are likely to contribute to catastrophic forgetting if altered), while larger perturbations are introduced to less useful weights to rejuvenate plasticity. The effectiveness of UPGD was assessed using non-stationary streaming problems based on datasets like MNIST, EMNIST, CIFAR-10, and ImageNet. The model was further tested in reinforcement learning tasks using PPO to confirm its applicability to policy collapse issues.\nConclusion: Utility-based Perturbed Gradient Descent effectively mitigates both catastrophic forgetting and loss of plasticity in continual learning environments. The method demonstrates improved performance over traditional approaches that focus on only one of these challenges. By leveraging a utility-based approach, UPGD can dynamically adjust to the importance of learned representations, maintain plasticity, and protect against forgetting in both representation learning and reinforcement learning tasks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"**Clear Structure and Writing:** The paper benefits from a clear structure and concise writing style.\\n\\n**Addressing a Complex Issue:** The authors tackle an underexplored yet challenging problem, and I appreciate their efforts to address online continual learning.\\n\\n**Mathematical Foundation:** The definition of utility introduced in the paper is based on simple and sound mathematical derivations. \\n\\n**New metric:** The introduction of a new plasticity metric is a nice contribution to the relatively uncharted territory of streaming learning.\", \"Weaknesses\": \"**Unscaled perturbations:** My main point of issue is the reasoning behind the perturbations in the update rule. The authors claim that by adding the perturbation we are making the unimportant weights more plastic however I am not really convinced by this explanation I believe it requires elaboration both in the rebuttal and in the paper. \\n\\nAnother related issue with the proposed perturbation is the fact that all of them are getting drawn from the same standard normal distribution. This design choice is strange to me since the parameters of a neural network usually differ in magnitude from layer to layer. By adding an unscaled random perturbation to all of the weights we are ignoring this scale difference which I believe is sub-optimal. I know that in the unprotected version, they are getting weighted by different values but this particular scaling is more correlating with changes in the loss value rather than the parameter magnitudes.\\n\\nHighly relevant to the above issue, I believe it is also necessary to have an additional ablation study, investigating the role of having and not having the perturbations in the update rule. I also want to disentangle the effect of weight decay. The only time that UG is added in the ablation is in the presence of WD. More specifically I am curious about the following scenarios in Figure 8: \\n\\n* Added ablations:\\n    + SGD + UG + WP + WD (present in the paper)\\n    + SGD + UG + WP \\n    + SGD + UG + WD\\n    + SGD + UG\\n\\n    \\n**Including more diverse experiments:** Moreover, in the experiments section I believe the authors need to include more diverse experiments. All of the streaming tasks are permutations of the same task. Whether in the label or in the input space. It is not as obvious as the authors' claim that after the permutation of the input space the previously learned representations are not relevant anymore (end of page 6). In the input-permuted scenario, only the first layer needs to have significant change. This is especially true for the label-permuted tasks as the network does a good job of clustering the data up to the final FC layer. I encourage the authors to use the Cifar100 superclass dataset (or any similar sequence of tasks that does not simply rely on the permutation).\\n\\n**Visualization:** Finally, I believe the visualization needs several improvements: the legends on the plot are very hard to read (Fig 2, 3, 4, 5). Some colors are similar to each other and the width of the lines in the legends is too thin. (Especially in figure 4). In Figure 7, some numbers in dark blue cells are almost impossible to read.\", \"Questions\": \"**Q1:** Have the authors tried to use an scaled version of the perturbation that takes the magnitude of the parameters into account? (Other than the unprotected version). Also I would appreciate the if you could elaborate on the effect of perturbations.\\n\\n**Q2:** Could you also explain about the average online accuracy? it is stated that \\\"The average online accuracy is the percentage of correct predictions within each task.\\\" I cannot see the average part here. Is it calculating the accuracy on each task separately then averaging over the number of tasks?\"}"}]}
{"paper_id": "qrGjFJVl3m", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large Language Models (LLMs) have achieved remarkable capabilities in text generation and comprehension but are limited to dealing with pure text, lacking the ability to process other modalities like images and speech. Recently, there has been interest in Large Vision-Language Models (LVLMs) which aim to incorporate visual perception into language models to solve real-world problems involving both text and images. However, many open-source LVLMs suffer from inadequate training and optimization compared to proprietary models. Fine-grained visual understanding is essential for LVLMs, yet most existing models only offer coarse-grained understanding or lack capabilities such as object grounding and text reading.\nGap: Open-source LVLMs currently lag behind proprietary models in terms of training and optimization, limiting their application. Fine-grained visual understanding, including tasks like object grounding and text reading, remains challenging, and most models only achieve coarse-grained perception. Additionally, multilingual support in LVLMs is underexplored.\nHypothesis: The authors hypothesize that by endowing the Qwen-LM with visual capabilities through a meticulous design, it is possible to develop a series of vision-language models (Qwen-VL) that achieve superior performance on visual-language benchmarks, particularly in fine-grained visual understanding and multilingual support. These models could set new records for generalist models on various tasks and improve the state of the art in vision-language dialogs.\nMethod: The Qwen-VL series is built on the Qwen-7B language model foundation and enhanced with visual capabilities through a new visual receptor, input-output interface, a three-stage training pipeline, and a multilingual multimodal cleaned corpus. The network architecture consists of a large language model, a visual encoder (using Vision Transformer), and a position-aware vision-language adapter to compress image features efficiently. The training process consists of three stages: large-scale pre-training on weakly labeled image-text pairs, multi-task pre-training on fine-grained annotated data across several tasks (e.g., VQA, grounding), and supervised fine-tuning for dialog capabilities. The models are trained on a multilingual corpus to support multiple languages and include fine-grained visual tasks for enhanced comprehension. Evaluation is extensive, covering benchmarks in image captioning, general and text-oriented VQA, and referring expression comprehension, and measuring effectiveness in real-world scenarios through dialog benchmarks like TouchStone.\nConclusion: The Qwen-VL series, including Qwen-VL-Chat, significantly outperforms previous models across a range of visual-language tasks and languages, showing superior capability in multilingual dialog, fine-grained understanding, and grounding. These open-source models set new benchmarks and represent a significant step forward for LVLMs. The comprehensive training on diverse datasets allows Qwen-VL models to excel in image captions, VQA, and real-world applications, thereby providing a strong foundation for future research. The authors plan to further enhance Qwen-VL by integrating additional modalities and increasing the model scale.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"- Open sourcing the model is going to help the research community\\n- The model shows decent multimodal capability, especially in terms of bounding box reasoning\", \"Weaknesses\": \"My concerns are regarding the scientific and technical contributions from this paper.\\n- The claim in the related work section, \\\"Despite achieving significant progress, previous vision-language models still have several limitations such as poor robustness in instruction following, limited generalization capabilities in unseen tasks, and a lack of in-context abilities.\\\" lacks justification. For example many models are not instruction-tuned (yet). That does not mean they have a fundamental difficulty in instruction following.\\n- There is limited innovation on the model architecture and training recipe. For example the use of interleaved data and multi-stage, multi-resolution training has been proposed in previous works. Also there is limited novelty in showing that supervised finetuning with interleaved chat data can lead to chatting capability.\\n- The ablation study is not written clearly (also see questions below)\", \"Questions\": \"In the ablation study of Figure 7, which stage is that, lower-res or higher-res? If it is lower-res (224) stage it makes sense to use 256 tokens as the native number of patches is just (224/14)^2 = 256. If it is the higher-res (448) stage, then it is counterintuitive that using more tokens, i.e., 400, with more degrees of freedom, will lead to worse performance.\"}"}]}
{"paper_id": "MSe8YFbhUE", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Visual reinforcement learning (RL) has been applied to continuous control tasks using high-dimensional pixel inputs. There have been advances in areas like data augmentation, self-supervised representation learning, and high update-to-data ratios to improve sample efficiency of RL agents.\nGap: Existing visual RL methods often result in agents being motorically inactive in early training phases, preventing effective exploration. This inactivity is correlated with inactive neurons in the policy networks, limiting the agent's ability to learn efficiently.\nHypothesis: The decline in dormant ratio\u2014a measure of inactive neurons in a neural network\u2014acts as an early indicator of successful skill acquisition in RL agents, even before rewards increase. By minimizing this dormant ratio, the exploration-exploitation trade-off can be better managed, resulting in improved performance.\nMethod: The proposed DrM method includes three main components: (1) Periodic perturbation of neural network weights guided by the dormant ratio, using soft resets for refreshing weights when the dormant ratio is high; (2) A self-adaptive exploration noise schedule that adjusts noise based on the dormant ratio; (3) An exploitation mechanism that prioritizes actions based on a lower dormant ratio, allowing the agent to emphasize exploitation as it becomes more capable. Extensive experiments were conducted across various domains such as DeepMind Control Suite, MetaWorld, and Adroit, comparing DrM against state-of-the-art baselines like DrQ-v2, ALIX, and TACO.\nConclusion: DrM demonstrated significant improvements in sample efficiency, asymptotic performance, and robustness across tested environments. It was notably successful in solving previously unsolved tasks in Dog, Manipulator, and demonstration-free dexterous hand manipulation tasks in Adroit based solely on pixel observations.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"The greatest strength of the paper is the significance of the results. DrM is the clear winner on *all* of the tasks reported on in the paper, and in some domains (particularly Adroit) it achieves a step change improvement. I like that the authors have considered three separate domains (DeepMind Control Suite, MetaWorld and Adroit), with multiple tasks from each.\\n\\nThe main idea of treating the dormant ratio as a progress proxy, and hence using it as the basis for adaptive hyperparameter scheduling, makes intuitive sense. While I have some minor qualms about the writing (see below), the paper was clear and easy-to-read overall. Lastly, the coverage of the huge amount of related work from the last 2-3 years is about as thorough as possible.\", \"Weaknesses\": \"My biggests concerns with the paper are its limited novelty a lack of proper ablations.\\n\\nNovelty:\\n\\nNone of the three mechanisms introduced by DrM are novel in-and-of themselves.\\n- As noted in the paper, \\\"dormant-ratio-guided perturbation\\\" follows the previous work of (D'Oro et al., 2023; Ash & Adams, 2020).\\n- Similarly, \\\"dormant-ratio-guided exploitation\\\" is based on the work of Ji et al. (2023).\\n- The idea of scheduling the exploration rate goes back a very long way, pre-dating deep RL. Some recent approaches, e.g., Agent57 (Badia et al.), also use a form of adaptive exploration.\\n\\nWhat *is* new is the idea of using the dormant ratio to schedule the hyperparameters of these methods. I don't disagree that this is novel; however, it's rather limited novelty in my opinion.\\n\\nAblations:\\n\\nWhile Figure 10 contains some ablations, the three mechanisms are each ablated in their entirety. I'd prefer it if only the adaptive scheduling component were ablated for each method, since this is the novel part. In other words, for a fair comparison, DrM ought to be compared against DrQ + perturbation resets (with fixed $\\\\alpha$) + \\\"Blended Exploration and Exploitation operator\\\" (with fixed $\\\\lambda$).\\n\\nMoreover, the ablations only consider one domain (Adroit) and the results are aggregated across all Adroit tasks. I'd prefer to see more ablations in greater granularity. For me, they're the most important part of the experiments, since without them it's very hard to understand *why* the improvements work. (The argument that DrM lowers the dormant ratio and hence improves performance seems very \\\"chicken-egg\\\" to me; it's unclear which is the cause and which is the effect. Only the dormant-ratio-guided perturbations seem to directly target the dormant ratio.)\\n\\nMinor things:\\n- Section 3.1 is too unscientific, e.g., the claim \\\"This suggests that the dormant ratio acts as an intrinsic metric, influenced more by the diversity and relevance of the agent's behaviors than by its received rewards\\\". This is all based on one example, and the claim that RL agents learn relevant behaviours before their returns improve sounds dubious to me. Another too-bold claim is: \\\"As the dormant ratio captures the intrinsic characteristics of an agent's policy network, all the hyperparameters in this approach are robust to tasks and domains.\\\" This is never established, and in fact Table 1 in the Appendix suggests that some parameters of DrM require domain-specific tuning.\\n- The claim in the abstract that DrM has \\\"no broken seeds (76 seeds in total)\\\" is a little vague (what constitutes \\\"broken\\\"?) and it's never really returned to in the main body of the paper.\", \"Questions\": \"- In the \\\"dormant-ratio-guided exploitation\\\" section, it's mentioned that \\\"value underestimation occurs as the agent starts to acquire skills, given that $\\\\pi$ is sub-optimal\\\". I don't quite follow this. As the agent starts to improve, wouldn't we expect $\\\\pi$ to become less sub-optimal?\"}"}]}
{"paper_id": "o1TKGCrSL7", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I\u2019m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations\u2014where domain predictability is removed and task predictability is emphasized\u2014is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive \u2014 It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}"}]}
{"paper_id": "cZo6pDtDZr", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Collision probability is a significant measure of how widely a distribution's probability mass is dispersed. It is widely used in fields like ecology (Simpson index), economics (Herfindahl-Hirschman index), and statistical mechanics. Unlike Shannon entropy, collision probability is easier to estimate as its sample complexity is independent of the support size.\nGap: Previous work on locally differentially private estimation of collision probability had a sample complexity that worsened as the privacy parameter tightened. Additionally, no sequential testing algorithms specifically for collision probability, rather than batch testing, existed prior to this study.\nHypothesis: The authors propose that it is possible to develop a locally differentially private (LDP) algorithm that estimates collision probability more sample-efficiently than previous methods and one that supports sequential testing under LDP without needing a known epsilon.\nMethod: The authors introduce two novel algorithms: (1) A private estimation algorithm using a distributed user-server protocol that computes estimates by hashing and bias correction, overcoming sample complexity issues with a thoughtful distribution of computational steps across users and the server. (2) A sequential testing algorithm that draws samples iteratively and applies threshold checks on collision frequency observations, allowing it to adapt to unknown difficulty parameters and efficiently reject hypotheses.\nConclusion: The private estimation algorithm achieves near-optimal sample complexity in terms of achieving better error bounds, roughly independent of the support size, while using significantly fewer samples than prior methods. The sequential testing algorithm efficiently determines hypothesis distinctions without pre-specified sample limits, showcasing reduced sample needs as compared to batch methods, making them suitable even when epsilon is unknown.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The writing is clear and intuitive. It was easy to follow the technical arguments, and I believe they are correct (did not look at some of the full proofs though).\\n2. Experimentally, the mechanism proposed here seems to outperform the existing approach from Bravo-Hermsdorff et al., and slightly outperform the open source implementations for locally private heavy hitters.\", \"Weaknesses\": \"1. Poor contextualization of the results in comparison to previous work. Why focus on additive approximation? What are precisely the best known results?\\n2. Limited novelty, both results are simple and do not introduce significant new tools.\", \"Questions\": \"Disclaimer: I have reviewed this paper in the past and one request I had still remains. Please contextualize your paper better against the problem of locally private heavy hitters. It seems that your problem is in some ways easier and perhaps less interesting than heavy hitters. Does a separation between collision probability and heavy hitters follow from existing results in the literature (or from your existing results)? can you show that your problem is provably easier in terms of sample complexity?\"}"}]}
{"paper_id": "1PaDPHDhwe", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Machine learning models often perform well on average but struggle with minority groups due to spurious correlations and dataset biases. Group distributionally robust optimization (GDRO) has been introduced to mitigate algorithmic biases, but it typically improves robust accuracy at the cost of average accuracy.\nGap: Existing approaches to improve robust accuracy often sacrifice average accuracy, and there has been little exploration of methods that consider both metrics comprehensively. Current methods have limited ability to evaluate performance across the trade-off between robust and average accuracy.\nHypothesis: The authors propose that a simple post-processing technique called class-specific scaling can effectively manage the trade-off between robust and average accuracies by optimizing both metrics simultaneously, without requiring additional training.\nMethod: The method introduces a class-specific scaling strategy that adjusts prediction scores of pre-trained models to identify Pareto optimal points on the trade-off curve between robust and average accuracies. They propose an instance-wise adaptive scaling method, which uses clustering to further optimize scaling factors for individual test examples. A novel evaluation metric, robust coverage, measures the completeness of debiasing algorithms by capturing the trade-off.\nConclusion: The proposed scaling methods improve both robust and average accuracies across various datasets, demonstrating that a post-processing approach can be effective without additional training. The approach allows for a comprehensive evaluation and understanding of debiasing algorithms by revealing the trade-offs they entail.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 1, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"The paper tackles an important problem. It has a detailed discussion of related works. Performance improvements are consistent.\", \"Weaknesses\": \"1. The proposed method is too trivial. It simply searches a classwise scaling factor on the validation set. There is also no insight provided, e.g., why does the proposed classwise scaling bring improvements in the target metric? How does the optimal scaling factor differ for various metrics?\\n\\n2. The comparison with baselines is potentially unfair. Since the scaling factors are found by greedy search on the validation set, they can be regarded as trainable parameters. Hence the proposed method effectively leverages the validation set as additional training data. Hence direct comparison with baselines is unfair.\\n\\n3. The paper is poorly written. For example, in the first paragraph of the introduction, the concept of \\\"group\\\" and \\\"minority group\\\" is discussed without proper introduction. The claim that \\\"there has been little exploration for a comprehensive evaluation that considers both metrics jointly\\\" is not convincing, as there are many works studying the tradeoff between in-distribution and out-of-distribution accuracy [1*]. Despite the simplicity of the method, it is poorly explained, e.g., what is the detailed procedure of greedy search? Is it by gradient descent or brute-force trial from a pre-determined list? Section 3.3 is especially confusing: what are the cluster centroids used for? How to perform step 2? How to determine the cluster membership?\\n\\n[1*] Calibrated ensembles can mitigate accuracy tradeoffs under distribution shift. UAI 2022.\\n\\nOther suggestions & questions\\n\\n1. It is recommended to include the dataset statistics (train/val/test set), especially because the proposed method leverages the val set.\\n\\n2. Beyond numerical results, it is more interesting to see some qualitative examples of how the proposed approach improves baselines.\", \"Questions\": \"See weaknesses.\"}"}]}
{"paper_id": "aG3EARrrd1", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the significance of online learning, particularly in areas with large dynamic datasets such as real-time data analysis, online advertising, and financial modeling. Online regression, an area of focus, involves transforming complex tasks into simpler sub-tasks often using RBF methods due to their dimensional independence and convergence properties. Traditional RBF approaches face challenges like determining neuron size and network growth which often lead to inefficient networks or poor generalization performance. Approaches like RAN and GAP-RBF partially address these but separate structure and parameter optimization and often involve complex growth and pruning strategies. There is a growing interest in hybrid models integrating tree structures and RBF networks to enhance capacity and simplicity.\nGap: Existing RBF-based online learning methods either lack efficiency in handling large, streaming datasets or have challenges balancing network growth and generalization capability. While recent hybrid approaches attempt to address these, they often separate structure determination from parameter optimization, leading to potential inefficiencies. Additionally, the limitations of RBF networks in fitting complex target functions highlight a need for a model that integrates tree-based structures for better adaptability.\nHypothesis: The authors propose the Online Sparse Residual Tree (OSRT), a model integrating tree decomposition and adaptive radial basis function exploration, as a more efficient and accurate method for handling streaming multivariate scattered data. They suggest that OSRT can dynamically and appropriately expand and refine network components to maintain model sparsity and accuracy, thereby providing improved efficiency over traditional RBF and hybrid methods.\nMethod: OSRT utilizes online tree decomposition and adaptive RBF exploration to manage data streams effectively. The model grows the tree and refines each child node with RBF networks in response to new data, minimizing residual errors. It employs an incremental method to adaptively determine RBF centers, ensuring a sparse model while updating its structure to incorporate trends in streaming data. The adaptive RBF exploration intelligently adds nodes when necessary and prunes them based on significance and performance evaluation metrics. The model\u2019s efficacy was tested on datasets, comparing its performance against existing online RBF approaches, and it included methods like quasi-uniform subsequence generation and adaptive function termination.\nConclusion: The OSRT model demonstrated higher prediction accuracy and efficiency compared to existing online RBF networks. This was validated using benchmark time series datasets like Mackey-Glass and Lorenz time series. The model optimizes both tree depth and RBF parameters concurrently, showing adaptability in evolving data tasks. The paper concludes that OSRT is both a feasible and efficient technique for online learning in scenarios with large and dynamic datasets.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"The paper presents several novel ideas, by combining RBF networks, sparse regressison trees, and online updating for time series prediction.\", \"Weaknesses\": \"The paper lacks a principled approach to model design and evaluation, appearing to have little rationale in the combination of methods used beyond their adaptation from the recent literature, and their apparent heuristic value.  Typically one would expect a cross validation step as part of the algorithm when complexity parameters or thresholds are called for in a model.  \\n\\nThe exposition is hard to follow at best, and at times incomplete, or the symbols are incorrect. \\nFor instance, in Section 2: \\n\\n- Gaussian kernel is designated \\\\theta, but in the approximation the character \\\\phi is used -- are these the same thing? Note that \\\\theta is reused with a different meaning in Equation (17).\\n\\n- After equation (2) the phase \\\"Where N\u03c7 is the number of neurons in this node, \u03b4l is the shape parameter.\\\" makes no sense since neither variables appear in the previous formula.  The rest of that paragraph has similar problems with reference to variables not introduced in the equations that it attempts to explain. \\n\\nIn general, one needs a principled method for determining the complexity of the model, e.g. the number of nodes, such as cross validation, or use of complexity penalty terms, e.g. in BIC. Is the maximum tree depth (Section 2.2) something one calculates, or is it a parameter one setd? Simply considering when \\\"the increase in the number of nodes no longer yields significant improvements in approximation quality\\\" will lead to overfitting. \\\"Significant improvement\\\" is not a principled method.\", \"Questions\": \"There are many terms introduced in the explanation of the model that are introduced but not explained:  Could you describe the tree in terms of its layers?  What is the \\\"split rule\\\" and what is the stopping condition referred in the paragraph following Equation (2)?  In what sense is it sparse? Is there a sparsification step?  What do you mean in Section 2.1 by \\\"quasi-uniform\\\"?  Are your \\\"mean points\\\" C the same as your centers? Honestly this as far as I got in the text.\"}"}]}
{"paper_id": "7VPTUWkiDQ", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper addresses the challenge of learning representations that can generalize to novel compositions of known concepts, which is crucial for improving machine perception to more closely resemble human perception. Object-centric representation learning has been proposed as a way to enable this type of compositional generalization, where each object in an image is represented by a distinct subset of latent code. However, a principled understanding of when and how object-centric representations can indeed generalize compositionally has been lacking.\nGap: There is a lack of theoretical and empirical understanding of when object-centric representations can be guaranteed to enable compositional generalization. There is also uncertainty regarding the advancements needed in object-centric learning to promote such generalization, and the obstacles that remain for achieving this goal.\nHypothesis: The hypothesis is that autoencoders that satisfy specific structural assumptions on the decoder and enforce encoder-decoder consistency will learn object-centric representations that provably generalize compositionally. This hypothesis is rooted in the development of a theoretical framework using identifiability theory that shows under what conditions compositional generalization is possible.\nMethod: The paper utilizes identifiability theory to explore when compositional generalization is possible for object-centric representations. The authors propose that an additive decoder structure in autoencoders, combined with a compositional consistency regularizer for the encoder, ensures compositional generalization. They define conditions such as additivity, where latents are decoded as the sum of individual slot-wise decodings, and compositional consistency, where the encoder must invert the decoder not only in-distribution but also out-of-distribution (OOD). The theoretical framework is validated through experiments on synthetic image data to demonstrate the practical relevance of their assumptions.\nConclusion: The main conclusion is that it is theoretically and empirically possible for autoencoders with certain structured decoders and regularized encoders to achieve compositional generalization. This is significant for advancing machine perception closer to human capabilities. The work lays the foundation for further theoretical and empirical explorations into achieving robust compositional generalization in object-centric learning.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"- The paper is very well-written.\\n- The theory is sound and significant for the community.\\n- The joint encoder-decoder framework for compositional generalization in autoencoders is quite elegant.\\n- The limitations of the framework and the additivity constraint on the decoder are adequately stated.\", \"Weaknesses\": \"Although they support the theory, the experiments are quite limited. For instance, these are all with only two slots with 16 dimensions each. See the questions section for additional information that would be interesting to see from experimentation.\", \"Questions\": \"- How does the effect of the consistency loss scale with the number of slots?\\n- What is the impact of how slot-supported the training data is? i.e. in Figure 2 (1), what is the impact of the width of the blue band on empirical effectivity?\\n- How does the method hold up on non-synthetic data, especially if you slightly relax some constraints? For instance, what if you have expressive slot-wise decoding, but allow for a low-expressivity non-linear combination at the end for rendering?\"}"}]}
{"paper_id": "lOwkOIUJtx", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The efficient use of visual information is a significant challenge in image classification and detection due to conflicting requirements such as high accuracy, low latency, and high energy efficiency. Biological visual systems are often more effective than artificial ones, utilizing mechanisms like foveal-peripheral sampling and saccades to enhance precision and energy efficiency. Foveal vision captures high-resolution details only in a small part of the visual field, while peripheral vision captures less-detailed information over a wider area. This approach reduces data input needs and increases robustness in visual tasks.\nGap: Despite advances in artificial visual systems inspired by biological mechanisms, existing approaches often do not fully capitalize on foveation and saccades to enhance data efficiency and accuracy. Most current systems do not integrate sequential saccades or perform comprehensive scene reconstruction, but instead require retraining classifiers with collected foveal-peripheral views, which is data-intensive and lacks efficiency.\nHypothesis: The authors propose that an artificial vision system that effectively integrates foveal-peripheral sampling, saccade control, and continuous scene reconstruction can significantly enhance image recognition performance while reducing data input requirements. This system could potentially achieve state-of-the-art performance with higher data efficiency and lower training complexity compared to existing methods.\nMethod: The proposed model processes visual information through three core components: a predictive reconstruction module, an evaluation module, and a policy module. The predictive reconstruction uses a three-layer ConvLSTM and self-supervised learning to reconstruct high-resolution images from foveal-peripheral views. The policy module employs reinforcement learning to optimize saccade decisions, thereby distributing visual focus intelligently across a scene. The evaluation module appraises the reconstruction quality using metrics such as SSIM or classification accuracy. The system was tested on the GTSRB and ImageNet datasets, demonstrating improved classification accuracy and reduced pixel usage.\nConclusion: The study introduces a novel framework combining foveal-peripheral sampling, saccade control, and scene reconstruction to enhance image recognition with reduced data input. Results show significant improvements in classification accuracy (e.g., 5% higher on ImageNet) with 70% less pixel usage compared to traditional methods. The framework is data-efficient, requiring less retraining effort, and can integrate with existing back-end models. The saccade model indicates high transferability, effectively extending its use to diverse visual tasks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 3, \"Strengths\": \"I find the method simple and useful, with interesting potential application. \\nIt is appealing that the method seems to be suitable for existing classification models (no retraining).\", \"Weaknesses\": \"## Major\\n\\nI am confused how the image information from the sequential glimpses is passed and integrated in the predictive reconstruction model. Much more space is spent on the background to the hybrid loss function than actually making explicit how the sequential image information is used to improve reconstruction.\\n\\nIn addition, the abstract states \\\"our model reduces the required input pixels by over 90% per frame while maintaining the same level of performance in image recognition as with the original images.\\\" I don't understand where to find support for this claim in the results. For example, in Figure 3, all subsampled models perform worse than the original. The data in Figure 4 are coming closest to the original; is this what is meant?\\n\\nAlso, please clarify whether the experiments in Figure 3 are conducted with the trained saccade control model (which one?). \\n\\n\\n## Minor\\n\\n- Instead of \\\"continuous saccades\\\" a better terminology would be \\\"sequential saccades\\\" or \\\"scanpaths\\\". See e.g. [2, 3, 4]\\n- There are now known to be three types of photosensitive cells: rods, cones and intrinsically-photosensitive ganglion cells [1, 8]\\n- You use SSIM but the relevant paper(s) are not cited (e.g. [7]).\\n- Heading 3.1 \\\"Periphrl\\\"\\n\\n## Literature\\n\\n1. Do, M. T. H., & Yau, K.-W. (2010). Intrinsically Photosensitive Retinal Ganglion Cells. Physiol Rev, 90.\\n\\n1. Hoppe, D., & Rothkopf, C. A. (2019). Multi-step planning of eye movements in visual search. Scientific Reports, 9(1), 144. https://doi.org/10.1038/s41598-018-37536-0\\n\\n1. K\u00fcmmerer, M., & Bethge, M. (2021). State-of-the-Art in Human Scanpath Prediction (arXiv:2102.12239). arXiv. http://arxiv.org/abs/2102.12239\\n\\n1. K\u00fcmmerer, M., Bethge, M., & Wallis, T. S. A. (2022). DeepGaze III: Modeling free-viewing human scanpaths with deep learning. Journal of Vision, 22(5), 7. https://doi.org/10.1167/jov.22.5.7\\n\\n1. Rosenholtz, R. (2016). Capabilities and Limitations of Peripheral Vision. Annual Review of Vision Science, 2(1), 437\u2013457. https://doi.org/10.1146/annurev-vision-082114-035733\\n\\n1. Watson, A. B. (2014). A formula for human retinal ganglion cell receptive field density as a function of visual field location. Journal of Vision, 14(7), 15. https://doi.org/10.1167/14.7.15\\n\\n1. Wang, Z., Simoncelli, E. P., & Bovik, A. C. (2003). Multiscale structural similarity for image quality assessment. The Thirty-Seventh Asilomar Conference on Signals, Systems & Computers, 2003, 1398\u20131402. https://doi.org/10.1109/ACSSC.2003.1292216\\n\\n1. Zele, A. J., Feigl, B., Adhikari, P., Maynard, M. L., & Cao, D. (2018). Melanopsin photoreception contributes to human visual detection, temporal and colour processing. Scientific Reports, 8(1), 3842. https://doi.org/10.1038/s41598-018-22197-w\", \"Questions\": \"- I would like to see how the hybrid reconstruction loss changes over timestep, and not just classification accuracy.\\n- The sampling of the periphery of individual pixels with small probability is not very like human vision. Effectively this is providing low pass information. Have the authors considered how the sampling density could be approximated more plausibly (e.g. [6])?\\n- Have the authors considered comparing scanpath strategies learned in this model to human scanpaths (e.g. [3, 4])?\"}"}]}
{"paper_id": "kXHEBK9uAY", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Diffusion-based generative methods have shown effectiveness in trajectory modeling with offline datasets, but they face challenges in computational efficiency and generalizing temporal abstractions for long-horizon tasks. Hierarchical planning, where high-level plans with subgoals guide subsequent low-level plans, offers more efficient sampling and gradient propagation.\nGap: Current diffusion-based planning methods, like the Diffuser, are dense and flat, leading to computational inefficiency. They also inadequately cover dataset distributions and struggle with long-horizon, high-dimensional tasks, lacking a hierarchical approach to improve these aspects.\nHypothesis: Combining the strengths of hierarchical planning with diffusion-based modeling can lead to more efficient and effective planning strategies for decision-making in long-horizon tasks.\nMethod: The proposed Hierarchical Diffuser uses two diffusers: a high-level 'Sparse Diffuser' for subgoal generation and a low-level diffuser for achieving these subgoals. Training involves splitting trajectories into segments, with subgoals defined as segment split points. The Sparse Diffuser is trained on trajectories of subgoals, facilitating large receptive fields with lower computation. Low-level planners train on segments between subgoals and facilitate fine-tuning. The method was tested on offline reinforcement learning benchmarks, achieving parallel planning of subgoals and improved generalization.\nConclusion: Empirical results show that the Hierarchical Diffuser outperforms non-hierarchical methods and other hierarchical planning methods on standard offline RL benchmarks, with better training and planning speeds. The method also enhances generalization capabilities for compositional out-of-distribution tasks. The hierarchical approach also aids in efficient planning for longer horizons.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"Discretizing planning into sub-goals with diffusion models is shown to be advantageous as more diverse scenarios can be solved. Low-level diffusion planning becomes task-agnostic.\\n\\nEven the sparse diffuser or SD version is better than diffuser because of increased receptive field. The authors validate this by showing that increasing the kernel-size (hence the receptive field) of diffuser leads to better performance but weaker generalization.\\n\\nSD with dense actions leads to better fitting of the sparse objective, a typical bottleneck in hierarchical RL.\\n\\nHD takes lesser time than Diffuser due to shorter high-level sequence and parallelly solving low-level plans for all the segments.\", \"Weaknesses\": \"The formulation has limited novelty. While a single diffuser is not sufficient for planning over long-horizons, the work introduces two diffusers: one sparse diffuser for planning sequence of sub-goals between initial state and goal, while a standard diffuser solves for individual sub-goal segments.\\n\\nGiven that there are also methods which perform state-only diffusion like Decision-diffuser [2], is it possible to perform relevant ablations to justify why states are not sufficient for having a good estimate of $J$? If you are following diffuser\u2019s codebase, are you using the diffusion sampled actions or a separate controller for Maze2d tasks like diffuser?\\n\\nSolving for sub-goals independently without the knowledge about the final goal might lead to non-optimal behavior. How do you ensure that the sampled path for one segment does not overlap with the paths for the other segment? How many trajectories do you sample for individual segments and for the sub-goal sequence?\\n\\nAlso, what happens when actions do not lead to the exact sub-goal (unless you use inverse dynamics actions) for the maze case? Because there is no-feedback from the low-level diffuser to high-level diffuser, it might become challenging.\\n\\nOverall, I acknowledge that the presented method is a promising revision of Diffuser which performs better in all aspects. However, I believe that the contributions are not significant enough.\", \"Questions\": \"See weakness above.\"}"}]}
{"paper_id": "ueTdErd5Ib", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper addresses contextual stochastic optimization problems where optimization must handle uncertain parameters from unknown, context-dependent distributions. Historically, a two-stage process of predict-then-optimize has been used, which often fails to produce optimal and robust results. Recent methods aim for end-to-end learning but focus on minimizing average cost without incorporating robustness, which is crucial for real-world applications.\nGap: Existing methods primarily focus on minimizing expected costs without adequately addressing robustness against worst-case scenarios. The unpredictability of uncertain parameters in optimization tasks remains problematic, as traditional methods either separate prediction from optimization or overly simplify the relationship between predictions and decisions.\nHypothesis: The authors propose a method that integrates learning with the optimization task, aiming to prescribe decisions maximizing the likelihood that costs remain below a user-defined threshold, enhancing robustness against uncertainty.\nMethod: The proposed method involves discretizing the feasible region into subsets to approximate the uncertain objective function deterministically within each subset. Then, a secondary optimization problem integrates these approximations to prescribe decisions, aiming for maximized robustness and minimized regret. Theoretical guarantees bound the regret of decisions, and experiments, including a real-world electricity generation problem, demonstrate robustness and reduced worst-case costs compared to traditional methods.\nConclusion: The proposed discretization framework for contextual stochastic optimization demonstrates competitiveness in average regret and significantly improved robustness over existing methods. Theoretical and experimental validations show reduced worst-case costs, suggesting practical advantages for applying this method in real-world scenarios.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"Proposed a new framework to solve robust contextual stochastic optimization problem. Please see my comments in Summary for details.\", \"Weaknesses\": \"Not fully clear presentation/discussion of their algorithm. Please see my comments in Summary for details.\", \"Questions\": \"Please see my comments in Summary for details.\"}"}]}
{"paper_id": "fjf3YenThE", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Hard-thresholding algorithms are crucial for solving \\(\\ell_0\\) constrained optimization problems in high-dimensional machine learning, offering sparse learning solutions by reducing memory usage and computational demands. Traditional gradient-based optimization becomes challenging due to NP-hardness from \\(\\ell_0\\) constraints, making zeroth-order (ZO) methods necessary when gradients are inaccessible or computationally hard to obtain, as in black-box adversarial attacks.\nGap: The existing Stochastic Zeroth-Order Hard-Thresholding (SZOHT) algorithm is the only one addressing \\(\\ell_0\\) sparsity constraints with ZO gradients. However, it suffers from a limitation on the number of random directions it can handle, due to conflicts between ZO gradient deviations and the expansivity of the hard-thresholding operator, hindering its practical applicability.\nHypothesis: By changing the focus to variance reduction, the conflicts between ZO gradients and the hard-thresholding operator can be mitigated. This approach can potentially remove the limitations on the number of random directions in the algorithm, leading to improved convergence rates and broader applicability.\nMethod: The paper introduces a generalized variance-reduced ZO hard-thresholding algorithm. This method uses historical gradients to reduce variance thoroughly, thereby eliminating restrictions on the ZO gradient step. The algorithm was theoretically analyzed under assumptions like restricted strong smoothness and convexity. The authors demonstrated the method's utility through experiments on portfolio optimization and black-box adversarial attacks, comparing its performance with existing algorithms.\nConclusion: The proposed variance-reduced ZO hard-thresholding algorithm significantly improves convergence rates and applicability compared to SZOHT by removing the limitations associated with the number of random directions. The algorithm was validated through experiments, achieving competitive performance on sparse optimization tasks in adversarial attacks and portfolio optimization.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"The idea of combining variance reduction with zeroth-order hard-thresholding methods should be novel\", \"Weaknesses\": \"- The proposed method is not clearly introduced and has many confusing points. \\n- Experiments is not very supportive for the proposed method, details can be found in Questions part\", \"Questions\": \"- My first concern is on the usefulness of zeroth-order oracles. From experiments in this paper, it seems that we have easy access to gradient information in all these applications. Therefore, why do we need to emphasize zeroth-order algorithms? The authors may consider some other applications where gradient information is hard to obtain. \\n- Another concern is on the setting of L0 regularization. While there exist many other approaches on obtaining sparse solutions (e.g., LASSO), the authors may need to justify the necessity of using L0 regularization here.  \\n- I am a bit confused on the key contribution of this work. The authors introduced two algorithms, pM-SZHT and VR-SZHT. What is the key difference of these two algorithms? Moreover, pM-SZHT is not compared in experiments, but SARAH/SAGA-SZHT is compared. This is also very confusing and may need some explanations. \\n- It seems that SAGA-SZHT is missing in both the main text and appendix. Given its good performance in Table 1, I am a bit confused on why it is not properly introduced. Are there some errors? \\n- Regarding experimental results, I am a bit confused by Figure 3. It seems that SZOHT converges faster than all other methods, but gradually becomes worse with more iterations. Some explanations may be needed here on why we need so many iterations after convergence. \\n- Also, the evaluation of few-pixels universal adversarial attacks may seem a bit restricted with only 10 images from the same class. It would be better if the authors can use more images from different classes to further justify the effectiveness of proposed method.\"}"}]}
{"paper_id": "sNtDKdcI1f", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Reinforcement Learning from Human Feedback (RLHF) has been effectively used to align large language models with human-desired properties like helpfulness. This technique involves training a reward model based on human feedback and optimizing the language model to maximize the reward. This process is often reported to enhance the quality of language generation tasks such as dialogue and question answering.\nGap: One of the consistent issues observed with RLHF is that it often leads models to produce unnecessarily longer outputs. This has not been fully analyzed, leaving it unclear if increased output length is a meaningful contributor to perceived improvements in model helpfulness, or merely an artifact of reward model biases.\nHypothesis: The primary hypothesis is that optimizing for response length is a significant, and possibly overestimated, factor contributing to the improvements reported in RLHF. The authors question whether it is the main driving force behind the reward improvements seen in RLHF implementations.\nMethod: The authors conducted experiments with three open-source preference datasets: WebGPT (question answering), StackExchange (technical question answering), and RLCD (multi-turn dialogue). They analyzed output length correlations with reward and experimented with interventions at various stages of the RLHF pipeline to mitigate the effects of length. These interventions included modifying the reward models, applying length penalties, and altering preference data to balance length biases. They also compared standard RLHF to a baseline where only output length was the reward, to assess the contribution of length to model output improvements.\nConclusion: The study finds that a large portion of the reward increases achieved through RLHF can be attributed to optimizing for longer outputs, more than previously acknowledged. This suggests existing reward models are biased towards length. The experiments showed that models rewarded solely based on output length achieved performance improvements comparable to full RLHF with sophisticated reward modeling, implying that the length is a significant factor but does not fully explain the quality gains, as RLHF with typical reward models also induces qualitative changes. The study highlights the need for more robust reward model techniques to truly capture human preferences beyond shallow correlations like length.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"--comprehensive evaluations on a few different RLHF settings/datasets for helpfulness optimization\\n\\n--nice in-depth exploration confirming the extent of a phenomenon (length correlations for helpfulness optimization) that i think hasn't been comprehensively analyzed previously\", \"Weaknesses\": \"--presentation: it's quite difficult to parse what several of the abbreviations mean in the tables (and honestly i'm still a bit confused on what exactly you're showing in the first couple of figures and tables after looking at them for several minutes). it would be great if you could make the figs/tables more self-contained, e.g., define abbreviations in the captions and summarize takeaways, so i don't have to keep bouncing between the figs/tables and the text.\", \"Questions\": \"--even if we're just optimizing for length, your model still learns something nontrivial, right? an interesting point of comparison might be an extra baseline where instead of sampling 8 outputs from the base LM and picking the longest one, you just sample one output and forcibly make it keep generating past the end - i imagine this would perform a lot worse. so intuitively, i feel that optimizing for length is also somehow optimizing for some less trivial notion of comprehensiveness, maybe?\\n\\n--just to confirm, weighted reward gain i should think of as measuring how much reward improves within a particular bin of length, i.e. the reward gain that comes from factors other than length?\\n\\n--would be curious also if you have any qualitative sense of what are the non-length axes along with helpfulness improves in your experiments (i.e., any intuitions for the remaining 10-30% of helpfulness that wasn't attributed to increased length?).\\n\\n--as you wrote in the conclusion, it would be interesting to just see a quick experiment on e.g., harmlessness or honesty, if only to confirm our intuition that length is primarily a heuristic for optimizing helpfulness and not really applicable to other common tasks we RLHF for. (not necessary if it's troublesome to setup though.)\\n\\n--i'm not very clear on the concrete actionable suggestions for practitioners based on your analysis? (though they're not strictly necessary - i see that you did try a few ways to regularize length already)\"}"}]}
{"paper_id": "cNi2EJ8OCh", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Local differential privacy (LDP) is important in data privacy, especially when dealing with sensitive data in untrusted environments. While differential privacy (DP) is well-studied, LDP remains less explored, particularly in the context of functional data classification. This is significant because functional data, observed over time or space, can reveal personal information. The challenge lies in managing the infinite dimensionality and preserving privacy while maintaining data utility.\nGap: The paper identifies a significant gap in the application of local differential privacy to the field of functional data classification, which is largely unexplored. Most existing research focuses on differential privacy or on multivariate data, not on functional data characterized by infinite dimensions.\nHypothesis: The authors propose that by leveraging the property of functional data\u2014being approximable by a finite set of basis functions\u2014they can reduce its dimensionality, thereby facilitating the construction of classifiers under the constraints of local differential privacy (LDP). They posit that this approach can effectively balance privacy and utility in functional data classification.\nMethod: The authors develop algorithms to construct functional classifiers under LDP in both single-server and multi-server settings. In single-server scenarios, they propose a strategy of using fewer samples to train multiple weak classifiers, reserving most samples for evaluation, and using techniques like model reversal and model averaging to improve classifier performance. In multi-server settings, they use federated learning to allow servers to benefit from shared knowledge. The method involves encoding, perturbation using a Laplace mechanism, aggregation, estimation, and leveraging model reversal and averaging strategies to enhance classifier accuracy under LDP.\nConclusion: The paper concludes that the proposed algorithms significantly improve the performance of functional data classifiers under LDP. Techniques like model reversal and model averaging, along with federated learning in multi-server environments, enhance the accuracy of classifiers while adhering to privacy constraints. These findings demonstrate the feasibility and effectiveness of the proposed methods in privacy-preserving functional data classification.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"1.The paper introduces a novel approach to functional data classification under Local Differential Privacy (LDP), a topic that has not been extensively explored in the existing literature.  \\n2.The introduction of model reversal is particularly innovative, as it provides a unique solution to enhance the performance of weak classifiers by inverting their parameters when their accuracy is below a certain threshold.\", \"Weaknesses\": \"1.Differential Privacy (DP) has already proven to be sufficiently effective in protecting individual privacy, for instance, in defending against membership inference attacks. Moreover, there are numerous existing methods within DP for adding noise to functional input data, where the idea of projecting onto a set of finite bases is also quite common. From this perspective, the innovation of projection might not be high enough, and the necessity of employing Local Differential Privacy (LDP) remains to be further examined.At the same time, the performance degradation caused by residuals has not been analyzed theoretically. The expressive power of a finite basis is quite limited, and for some functions, their residuals could be fatal.\\n\\n2.Regarding the ideas of model average and model reversal proposed by the authors, compared to traditional ensemble techniques, model average utilizes more clients in the evaluation process, aiming to select better weak classifiers. For each poor classifier, a model reversal approach is employed for improvement. However, since only a small number of clients are used to train the parameters, the classifiers might generally perform poorly; model reversal can enhance the performance of the learners, but it may not necessarily improve them to a satisfactory level. Theorem three provides an expected conclusion, but it does not directly lead to a high-probability bound; the theoretical analysis of model reversal is still lacking.\", \"Questions\": \"1..Are there specific scenarios or use cases where LDP provides a clear advantage over DP in functional data classification?\\n\\n2.The idea of projecting data onto a finite basis is quite common in the realm of DP. Could you clarify how your approach innovates or differs significantly from existing methods?\\n\\n3.The theoretical analysis of model reversal seems to be lacking. Could you provide more details or elaborate on the theoretical foundations of this technique?\"}"}]}
{"paper_id": "l9GaXJnMJ8", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper addresses the challenge of approximating large-scale stochastic processes represented by Markov systems. Various methods for approximating these processes have been developed over the years, including scenario tree generation and the use of probability metrics like the Wasserstein distance. The Wasserstein distance has applications in fields such as GANs, clustering, and optimal transport. Previous work has focused on approximating distributions or mixtures rather than kernels, and computational methods such as the Sinkhorn algorithm have been used for efficient calculation of transportation distances.\nGap: Current methods for approximating the kernels of Markov systems are computationally expensive, especially when nonlinear transition risk mappings are involved. There is a need for efficient approximation methods to replace complex kernels with simpler, finite-support counterparts, especially in dynamic programming applications like risk evaluations and optimal stopping problems.\nHypothesis: The authors propose that a generalized metric based on the Wasserstein distance can effectively approximate Markov system kernels. This metric can be used as the basis for a computational technique that simplifies the kernels into a discrete, easier-to-handle form without significant loss of accuracy, thereby facilitating the analysis and manipulation of Markov systems.\nMethod: The authors introduce a novel kernel distance based on the Wasserstein metric and propose a particle selection method to approximate Markov system kernels. They develop a subgradient algorithm that is decomposable and parallelizable, allowing for efficient computation without matrix operations. The algorithm recursively approximates the forward system using the kernel distance, selecting representative particles to form a simplified system. This approach is validated through numerical experiments, including an optimal stopping problem.\nConclusion: The proposed method provides an effective means of approximating complex Markov systems using a kernel distance metric. The subgradient algorithm offers a computational advantage over traditional methods, handling large-scale problems more efficiently. The approach is validated through comparisons with the binomial tree method, showing its utility in diverse applications. The paper highlights potential applications in reinforcement learning and dynamic risk assessment, suggesting further exploration of these areas in future work.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"1. The problem in this paper is interesting.\", \"Weaknesses\": \"1. The paper is generally poorly written and contains numerous typos in both the explanations and equations.\\n2. The experimental section could benefit from additional elaboration and expansion.\", \"Questions\": \"1. The definition 2.1 is very confusing. More specifically, it explains $Q(\\\\dot | x)$ is a kernel that transforms $x$ to the probability measure. It seems $Q(\\\\dot | x)$ is a probability density function estimated and it does not look like a kernel functions. Could you please elaborate this part?\\n2. Equation (9) presents a variation of the Wasserstein barycenter problem. However, the manuscript lacks a comprehensive discussion of the Wasserstein barycenter, and it would be beneficial to explicitly address this concept. Additionally, a comparison of the proposed method with relevant baselines is warranted.\\n3. Numerous typographical errors are present in the manuscript. For instance, on page 1, there is a repeated \\\"the the.\\\" Equation (6) is lacking the variable $p,\\\" and these issues need prompt correction for the manuscript to maintain clarity and accuracy.\"}"}]}
{"paper_id": "F7QnIKlC1N", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The study of molecular ground-state conformation is essential because it often determines a molecule's physical, chemical, and biological properties. Traditional methods for obtaining these conformations, such as density functional theory (DFT), are resource-intensive. Deep learning (DL) methods, particularly molecular representation learning (MRL) using graph neural networks, have shown promise in molecular modeling, mastering tasks like molecular property prediction and optimization. Some recent works have integrated 3D geometric information into neural networks to enhance performance.\nGap: Existing generative models primarily focus on producing multiple stable conformations instead of targeting the ground-state conformation, which involves additional screening processes to find the optimal one. Current methodologies either rely on labor-intensive computations or fail to accurately capture the necessary molecular geometry efficiently. Hence, there is a clear need for more efficient, accurate models that can predict the ground-state conformation directly from 2D molecular graphs.\nHypothesis: The authors propose that a novel network architecture based on a Graph Transformer with a new self-attention mechanism, called Molecule Structural Residual Self-Attention (MSRSA), can efficiently predict the 3D ground-state conformation of molecules from their 2D topological architectures. This method should be competitive with or superior to existing state-of-the-art models and open-source tools like RDkit.\nMethod: The GTMGC model is presented as a Graph Transformer based network designed to predict molecular conformations end-to-end. It uses the MoleBERT Tokenizer to provide context-aware tokenized inputs, reducing representational redundancy of atoms. Positional encoding is applied using Laplacian Positional Encoding from the adjacency matrix. The core of the model is the MSRSA mechanism, which refines the self-attention process by incorporating adjacency and interatomic distance matrices to bias attention scores, preserving global and local molecular structure information. The model was trained and validated on the Molecule3D and QM9 datasets, showing significant enhancement in conformational prediction accuracy.\nConclusion: The GTMGC model demonstrated remarkable improvement over current state-of-the-art methods and exceeded the performance of established algorithms in RDkit based on benchmarks from the Molecule3D and QM9 datasets. The proposed MSRSA module not only enhanced model performance for conformational prediction but also proved effective for other molecular property predictions, illustrating its robustness and general applicability.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The paper is very clear and pleasant to read.\\n2. The integration of various existing frameworks for the task of molecules' ground state conformation is original to some extent.\\n3. The performances compared to the baselines are significant.\", \"Weaknesses\": \"I believe there are two main weaknesses:\\n\\n1. Novelty: There are plenty of previous works (not cited) implementing Transformer architectures that implement the elements of the proposed self-attention, e.g., [1] uses the adjacency matrix, [2] makes use of the distance matrix (even cited in the manuscript), while the weighted summation is ubiquitous in many fields using Transformers [3]. The initial encoding is obviously not a technical contribution.\\n\\nThus, the contribution may be summarized as the integration of existing methods/approaches, augmented with the regularization loss on the distance matrix (the \\\\beta seems to bring very minor improvement).\\n\\n2. Strange results: I may be wrong, but according to Table 3 (ablation study) a *simple Transformer* architecture without any addition to the self-attention reaches 0.4395 (MAE) which is already better than all the other baselines. This is problematic. Also, the final improvement is only ~1%. Finally, the reported results seem to take the best-ablated model results for each metric, which is wrong.\\n\\nMoreover,  we have:\\n\\n3. The advantage of MoleBert over the standard atomic encoding is extremely shallow or even worse.\\n4. Lack of comparison with other Transformer based methods.\\n\\n[1] *Molecule attention transformer.*\\n\\n[2] *Geometric transformer for end-to-end molecule properties prediction.*\\n\\n[3] *Axial-DeepLab*\", \"Questions\": \"Currently, it seems the proposed contributions don't bring any advantage over a simple (large) transformer model.\\n\\n1. One needs to know the capacity of the model in order to assess the origin of the good performance. \\nAccording to weakness 2, it seems the good performances are obtained *almost solely* from a large/powerful standard Transformer model. From Table 5, the model seems much bigger than other methods. Also, the discrepancies between the tables are disturbing (one single metric should be used for the model validation).\\n\\n2. Ablating the initial LPE. \\n\\n3. It would be beneficial to have a comparison performance with (at least one) other molecule transformer-based methods such as [4,5,6] or others at a similar capacity.\\n\\n4. It would be interesting to look at the learned weighting parameters (\\\\gamma) of the self-attention to better understand the contribution of each (maybe even adding a weighting to the global attention).\\n\\n\\n[4] Relative molecule self-attention transformer.\\n\\n[5] 3dtransformer: Molecular representation with transformer in 3d space.\\n\\n[5] Geometric transformer for end-to-end molecule properties prediction.\"}"}]}
{"paper_id": "Cw6lk56w6z", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: In-context learning (ICL) has emerged as the default method for leveraging large language models (LLMs), where models implicitly learn tasks from given instructions and demonstrations. This paradigm, supported by alignment techniques such as instruction tuning, has become foundational for many applications, providing enhanced capabilities to serve diverse human needs.\nGap: The paper identifies that ICL struggles with specification-heavy tasks\u2014those requiring complex and extensive specifications which often demand significant time for human mastery. ICL's performance on these tasks usually does not reach half of the state-of-the-art results from fine-tuned models.\nHypothesis: The authors propose that the failure of ICL in handling specification-heavy tasks is not due to inherent flaws in LLMs but stems from inadequate alignment methods. They hypothesize that with proper fine-tuning and alignment, LLMs can perform well on these tasks.\nMethod: The study involves comprehensive experiments testing ICL across 18 specification-heavy tasks using various modern LLMs, including GPT-4 and FLAN-UL2, to identify specific performance issues. The authors then fine-tune LLMs on these tasks to assess potential performance improvements, also performing instruction tuning to evaluate if enhanced alignment could improve ICL effectiveness.\nConclusion: The study concludes that the challenges faced by ICL with specification-heavy tasks are primarily due to shortcomings in current alignment methods rather than limitations of the LLMs themselves. Fine-tuning can dramatically improve performance, signaling potential in optimizing alignment further to meet sophisticated human demands.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"1. The paper highlights an interesting challenge to the in-context-learning paradigm that even today's most powerful LLMs struggle with. \\n\\n2. The paper presents promising initial results that demonstrates the potential of addressing the challenge of specification-heavy tasks via more focused alignment.\", \"Weaknesses\": \"1. The success of the ICL approach highly depends on the amount and quality of information provided in the prompts. The results in table 1 are all based on a shortened version of the task description. Examples in the appendix show that such shortened descriptions are too concise, e.g., they do not even contain a single sentence definition of each label. The provided few-shots do not cover all labels. Longer prompts can still fit in at least a subset of the LLMs used for the experiments. The paper needs to experiment with more detailed prompts to confirm that the poor accuracy of the ICL approach is not due to the very concise set of instructions.\\n\\t \\n2. The alignment experiments in Section 4.2 only hold out one task at a time, but still tasks of the same type are included in the alignment dataset. The paper needs to report results with a whole task type (e.g., All 4 RE tasks) is held out to confirm that the gain in accuracy is not really due to similarity between tasks.\", \"Questions\": \"1. Comparing table 1 to table 4, Aligned ICL has worse accuracy than the unaligned  baseline on 3 tasks MAVEN-Subevent and the two sentiment analysis tasks. Do you have an explanation or an intuition for why that is the case for those three datasets?\\n\\t\\n2. In Section 3.1, the paper says that \\\"LLMs ignore all the contexts in 18 instances\\\". How did you find out about that?\"}"}]}
{"paper_id": "rmA6uFEgIY", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Contrastive Language-Image Pre-training (CLIP) has achieved significant success in 2D vision tasks by leveraging vast amounts of image-text pairs, demonstrating extraordinary zero-shot recognition capabilities. This approach has inspired adaptations in the 3D domain, where integration with point clouds is emerging as a promising paradigm for open-world understanding. Existing 3D contrastive learning methods, such as ULIP and CLIP2, have begun exploring triplet alignments between image, text, and point cloud modalities, achieving substantial improvements in 3D recognition tasks.\nGap: Current methods in 3D contrastive learning often rely on simplistic correspondences between modalities (e.g., point-text, point-image) and fail to fully leverage the complementary attributes of these modalities, such as the semantic richness of images and the geometric depth of point clouds. Moreover, multi-view learning, although explored in supervised settings, remains underutilized within contrastive paradigms, and comprehensive training protocols for 3D models are sparse.\nHypothesis: The hypothesis is that by creating a joint representation that synergizes multi-view images and cross-modal contrastive learning, one can significantly enhance the accuracy and generalizability of 3D representations. This approach is expected to improve alignment with text modalities and bolster the overall depiction of real-world 3D objects.\nMethod: The authors propose MixCon3D, which incorporates a joint representation alignment mechanism between images and point clouds and integrates multi-view images. This method involves concatenating image and point cloud features to form a unified representation, which is then aligned with text representations through additional contrastive loss terms. The approach also leverages an improved training recipe, incorporating a large batch size, a cosine learning rate schedule with warmup, EMA stabilization, and separate temperature parameters for different modalities. The effectiveness of this method is evaluated on datasets such as Objaverse-LVIS and ScanObjectNN.\nConclusion: The proposed MixCon3D method significantly improves performance over existing baselines, surpassing previous state-of-the-art results on several challenging benchmarks, including a 5.7% accuracy improvement on the Objaverse-LVIS dataset. The approach enhances the generalizability of 3D representations, as evidenced by its performance on both synthetic and real-world scanned datasets. Furthermore, MixCon3D demonstrates effectiveness in cross-modal tasks, such as text-to-3D retrieval and point cloud captioning, indicating a well-aligned multi-modal representation space.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 5, \"arXiv\": 2305, \"Strengths\": \"1. The paper is well-written and easy to understand. The approaches are easy to follow.\\n2. Experiments state that the method achieves state-of-the-art performance, and the quantitative shows great robustness on the downstream tasks. \\n3. The motivation is clear and reasonable.\", \"Weaknesses\": \"1. Additional Comparisons:\\nYour zero-shot classification experiments on the ModelNet40 and other datasets (Table 2) offer valuable insights. To further substantiate your findings, it could be beneficial to include comparisons with some additional methods that have shown promise in this domain, such as JM3D [1] and ULIP-2 [2]. Such comparisons could provide a more comprehensive view of how your approach stands in the context of the existing state-of-the-art methods and highlight the unique contributions of your work more effectively.\\n\\n2. Clarification of Contribution:\\nYou have noted the introduction of multi-view images in the context of contrastive learning across multiple modalities as a key contribution to your paper. It is indeed an interesting aspect of your research. However, to strengthen your claim, it would be constructive to discuss how your approach differentiates from or builds upon prior works like JM3D [1] and ULIP-2 [2], which have also explored multi-view images within a similar framework. Clarifying this could help readers appreciate the novel aspects of your methodology and its significance in the field.\\n\\n3. Expansion of Quantitative Experiments:\\nThe paper presents intriguing insights into the arithmetic of multi-modalities. To solidify these findings, it might be valuable to expand on the quantitative experiments. Additional data or more varied testing conditions could offer deeper validation of your hypothesis and might also uncover further interesting aspects of your model's behavior. This would not only bolster your current claims but could also provide a richer resource for readers interested in the intricacies of multi-modal contrastive learning.\\n\\n[1] Beyond First Impressions: Integrating Joint Multi-modal Cues for Comprehensive 3D Representation, ACM MM 23\\n\\n[2] ULIP-2: Towards Scalable Multimodal Pre-training For 3D Understanding, arXiv: 2305.08275\", \"Questions\": \"1. Multi-View Analysis: Could you specify the number of views used during training and consider including an ablation study on this feature? This would greatly clarify the contribution of the multi-view approach.\\n\\n2. Extended Experiments: Exploring fine-grained tasks like semantic segmentation, as in ULIP and JM3D, could offer deeper insights and substantiate your model\u2019s versatility.\\n\\n3. Clarification on Table 5: Please clarify the meaning of \\\"no point cloud\\\" in rows 2 and 3, to better understand how these metrics are compared without point cloud inputs.\\n\\n4. Typographical Corrections: I've noticed a minor typographical error on the 10th row of the 8th page, where the word \\\"use\\\" is repeated.\"}"}]}
{"paper_id": "QQ5eVDIMu4", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Graph Neural Networks (GNNs) have shown effectiveness in processing data from specific domains, but their ability to handle diverse and unseen distributions remains a challenge. Existing literature addresses specific types of distribution shifts like graph size or node degree, which are limited when handling multifaceted, real-world distribution shifts.\nGap: Previous methods primarily focus on single-type distribution shifts, which don't account for complex, nuanced shifts present in real-world datasets. These shifts can involve multiple aspects simultaneously, such as node interactions and feature alterations, which conventional methods fail to accommodate, hindering GNN's generalization.\nHypothesis: The authors propose that graph distribution shifts can be considered as a mixture of components, with each component representing different types of shifts. Tackling distributional challenges by decomposing them into these components could improve GNN generalizability across complex shifts.\nMethod: The proposed GraphMETRO framework incorporates a mixture-of-experts (MoE) architecture, consisting of a gating model and multiple expert models aligned in a common representation space. The gating model identifies significant mixture components influencing the distribution shift, while each expert generates representations invariant to a specific type of component. These are aggregated to produce a representation that accounts for the complex distribution shift. The method is validated through experiments on synthetic and real-world datasets showing better performance over baseline methods.\nConclusion: GraphMETRO effectively handles complex distribution shifts by leveraging a mixture-of-experts approach. It outperforms existing methods on synthetic shifts and achieves state-of-the-art performance on real-world datasets, demonstrating its potential for improving GNN generalization capabilities in practical applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"- The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\\n- Real-world datasets back the claims through experiments.\\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\", \"Weaknesses\": \"- The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \\\"previous research has concentrated on addressing specific types of distribution shifts.\\\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \\\"environment generators\\\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\\n\\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\\n\\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\", \"Questions\": \"Please check Weaknesses.\"}"}]}
{"paper_id": "8JCn0kmS8W", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Recent advancements in audio generation models have primarily focused on domain-specific applications such as speech transcriptions and audio captions. These systems struggle to handle the complexities involved in generating comprehensive audio containing various elements like speech, music, and sound effects, tailored to controllable conditions.\nGap: Existing audio generation systems are limited in their ability to create harmonious, compositional audio that integrates multiple audio elements controlled by text instructions. They often fail to meet the demands of real-world scenarios necessitating sophisticated audio storytelling from textual descriptions.\nHypothesis: WavJourney proposes using Large Language Models (LLMs) to generate a comprehensive audio creation framework that can orchestrate multiple audio models to produce storytelling audio content from textual descriptions. This methodology aims to harmonize speech, music, and sound effects into a coherent audio script that can be executed as a computer program.\nMethod: WavJourney employs LLMs to generate an audio script from text instructions. This script serves as a structured semantic representation of the desired audio elements, detailing their type, layout, and attributes. The script is then compiled into a computer program, where task-specific audio generation models and computational functions are invoked to produce the final audio content. This system is designed to be compositional and interpretable, leveraging different expert audio models without additional training. The framework is evaluated on text-to-audio benchmarks including AudioCaps and Clotho.\nConclusion: WavJourney successfully synthesizes realistic audio aligned with semantic, spatial, and temporal conditions described by text, achieving state-of-the-art results on benchmarks. It demonstrates potential in crafting engaging storytelling audio and facilitates human-machine co-creation in multi-round dialogues. The introduction of a multi-genre story benchmark and subjective evaluations highlights WavJourney's effectiveness, although areas like efficiency and artificial composition remain as potential limitations.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes a novel task, text-to-audio storytelling. The task opens up a huge space for AI-powered general audio creation.\\n\\n2. The proposed system outperforms or achieves on-par  performance against previous SOTA methods, i.e., AudioGen and AudioLDM, in both objective and subjective metrics.\\n\\n3. The new established benchmark is another good contribution.\\n\\n4. The developed prototype and demos look amazing.\", \"Weaknesses\": \"1. The method proposed has limited methodology contribution to the research community. The system reflects a crafted engineering pipeline by combining several off-the-shelf audio expert models through LLM. There is limited contribution in terms of machine learning algorithms.\\n\\n2. As sort of an ensemble of expert models, the paper does not include any system-level ablation studies by comparing against other design choices.\\n\\n    a. How much does the system rely on the capabilities of LLMs? How would it perform if LLM is not GPT-4, but rather those open-source alternatives like Llama.\\n\\n    b. The script writer acts like an agent. There are numerous works and existing methods working on connecting LLM to perform various tasks, e.g., AutoGPT. But the authors didn\u2019t compare against these agent-based system design.\", \"Questions\": \"See weakness.\"}"}]}
{"paper_id": "bYQkOPvgDw", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Graph Neural Networks (GNNs) are widely used in various fields, but the presence of noisy labels (due to errors in manual labeling or incomplete data) can degrade the performance of these networks. While the issue of graph perturbation and attack has been extensively studied, less attention has been given to label noise. Existing methods for dealing with label noise often assume label smoothness, which is not applicable to heterophilous graphs, and perform poorly in high noise-rate scenarios.\nGap: There is insufficient research on robust GNNs that effectively handle label noise, especially in heterophilous graphs and scenarios with high noise rates. Existing methods heavily rely on the label smoothness assumption and are not tailored to the unique challenges presented by graph-structured data.\nHypothesis: A probabilistic graphical model-based framework can be used to develop robust GNNs that are effective against noisy labels, without relying on the label smoothness assumption. Such a framework should perform well on both homophilous and heterophilous graphs, as well as in high noise-rate situations.\nMethod: The authors propose PRGNN, a novel probabilistic graphical model-based framework that includes two versions: PRGNN-v1 and PRGNN-v2. PRGNN-v1 maximizes the likelihood of clean labels based on graphs only, using a Bayesian network. PRGNN-v2 incorporates noisy labels into the Bayesian network, improving the generation of clean labels. The framework employs three independent GNNs to generate noisy label predictions, reconstruct clean labels, and incorporate prior knowledge. A variational inference approach is used, with evidence lower bounds (ELBOs) as objectives, and a contrastive loss is added to leverage supervision from unlabeled data. The approach is evaluated on datasets with varying noise levels and graph types.\nConclusion: The proposed PRGNN framework outperforms existing methods across various noise types and rates, particularly in high noise-rate situations. It demonstrates the ability to work effectively in both homophilous and heterophilous graphs, thanks to its disregard for the label smoothness assumption and its use of probabilistic graphical models to handle label noise.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"1. Overall, the paper presents a novel and promising approach to handling label noise in graph neural networks based on probabilistic graphical model.\\n\\n2. The sequentially introduced models PRGNN-V1 and PRGNN-V2 are clearly depicted and the proposed objectives are well motivated. \\n\\n3. Experiments are conducted on both noisy Homophilous and Heterophilous graphs. Clear improvements are observed compared to the implemented methods.\", \"Weaknesses\": \"1. Several crucial points remain unclear. First, it is still hard to understand why we need the uncertainty modeling. One might directly apply the encoder-decoder models equipped with the corresponding losses to handle the noisy labels. To be specific, what happens if we only use Q and P_\\\\varphi without the prior distribution, and conduct the training without the prior-related losses? The second question is related. From the experiments in Figure 3, the weighted cross-entropy loss wL_CE(P_\\\\varphi, Y_N) is much more important than others. So, it seems it is much more important to balance the loss between reconstruction of clean labels and that of noisy labels, other than the probabilistic modelling particularly given the label space (which is the random variable we focus on) is of small dimension. Third, the authors claim that the proposed probabilistic graphical model is able to avoid the issue of label smoothness in existing methods. However, it seems this is attributed to the application of prototype label vectors, instead of simple concatenation of labels with node embeddings for message passing. This is also observed in Figure 3. Overall, to make the proposed models effective, the authors introduce various techniques. However, it appears that the one which demonstrates consistent success is not aligned with the contributions claimed by the authors. \\n\\n2. About the introduction of the baselines, it is unknown which one applies label smoothness, and which one is proposed for Heterophilous graphs.\\n\\n3. It seems this paper is only applicable for the transductive setting not inductive setting. \\n\\n4. The last question is on the general research domain for learning from noisy graphs. While it is true that many data we process in practice definitely contain different kinds of noise, the proposed methods are always evaluated in the scenarios where we add artificial perturbation to the clean data. My point is we actually need some real tasks and perhaps some fixed benchmarks to evaluate the performance of different methods, such that we can clearly justify how the progress is made and how the challenges are addressed.\", \"Questions\": \"See the weaknesses above.\"}"}]}
{"paper_id": "uqxBTcWRnj", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the integration of neural networks and symbolic systems, highlighting the recent trend of incorporating symbolic interpretability, compositionality, and logistics into neural networks. This is motivated by cognitive science studies suggesting that symbolic representations in the human brain transition from neural perception and develop through interaction with the environment. Traditional neural networks rely heavily on distributed representations, while symbolic systems use explicit structural representations to illustrate inputs.\nGap: Current methods integrating neural and symbolic systems do not fully exploit symbolic power and primarily enable them to work synergistically rather than truly combining the strengths of both. There exists an essential disparity between low-level distributed representations of neural networks and the explicit symbolic representations of symbolic intelligence. Moreover, symbol grounding, which links predicate representations to visual components, remains challenging and often requires labor-intensive supervised learning.\nHypothesis: The authors propose that a transitional representation system, arising from unsupervised learning, can bridge the gap between neural and symbolic representations. This system involves creating intermediate representations that both maintain high-dimensional details and convey structural information about input semantics. By achieving this, it becomes possible to implicitly learn symbolic knowledge such as visual parts and their relations from visual observations.\nMethod: The paper introduces a Transitional Dictionary Learning (TDL) framework, which involves learning dictionaries using an Expectation Maximization (EM) algorithm through online prototype clustering. This is coupled with a game-theoretic diffusion model to decompose visual input into parts. The framework aims to learn reusable and compositional predicates. Additionally, new metrics, clustering information gain and heuristic shape score, are proposed for model evaluation. The method was tested on three compositional visual object datasets to demonstrate its capability of learning symbolic features without relying on predefined visual features like colors or textures.\nConclusion: The experiments show that the TDL framework effectively discovers compositional patterns, significantly outperforming other unsupervised part segmentation methods that depend on visual features from pre-trained models. The introduced metrics align well with human evaluations, illustrating improved interpretability. This research takes a significant step towards developing neural networks that integrate symbolic reasoning capabilities, supporting the hypothesis of a seamless transition from neural to symbolic representations.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 3, \"doi\": 10, \"Strengths\": \"The paper is well-written and easy to understand.  There are good explanations for each step of the approach.  The \\\"Transitional Representation\\\" section does a really good job of approaching the symbolic and neural representations.  \\n\\n\\nThe method is topical and will be of interest to the ICLR community and the method seems to be novel for how to produce a dictionary of neuro-symbolic part segmentation.  \\n\\nI really like the overarching goal for self-supervised part segmentation and the method seems to attack the problem directly.  The neural symbolic approach to ML has been of interest for a while and part segmentation is a good problem to apply it towards.\", \"Weaknesses\": \"The biggest disappointment was not doing this on real visual data rather than on LineWorld data.  This is still useful with just LineWorld but showing on realworld data would be much more impressive.  \\n\\nRunning human evaluations requires an IRB or something similar not mentioned here.   This needs to be stated (anonymously) that you did actually go through someone to ensure the human experiments were done properly.  \\n\\nFor the \u201cCompositional Representation\u201d related work, please add references to older approaches such as Bag of Words such as:\\nL. Fei-Fei and P. Perona, \\\"A Bayesian hierarchical model for learning natural scene categories,\\\" 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05), San Diego, CA, USA, 2005, pp. 524-531 vol. 2, doi: 10.1109/CVPR.2005.16.\\n\\nCsurka, Gabriella, Christopher Dance, Lixin Fan, Jutta Willamowski, and C\u00e9dric Bray. \\\"Visual categorization with bags of keypoints.\\\" In Workshop on statistical learning in computer vision, ECCV, vol. 1, no. 1-22, pp. 1-2. 2004.\\n\\nThe citations needs to reference the actual venue such as this one should not just refer to Open Review (be wary of using automated citations):\\nYann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62, 2022.\\n\\nFormular 1 -> Equation 1\", \"Questions\": \"For Figure 3, could you compare a more conventional approach to compare against to see if this approach is causing it to be separated verse just from the data?  \\n\\nHave you tried this on more complex data 2D images?\\n\\nCan you elaborate on exactly what the human criteria were that they were evaluating?\"}"}]}
{"paper_id": "Fn655mJ4bv", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Structured output models are used in many applications to map input vectors to high-dimensional output spaces, like image segmentation and multi-label classification. These models leverage dependencies among output variables for better prediction accuracy. Methods like graphical models use energy functions for inference, but interpreting these models is complex due to their non-linear and intricate nature.\nGap: Existing interpretation methods for structured output models, such as model-based and post-hoc techniques, often overlook correlations among output variables, leading to potentially suboptimal explanations. Interpretations are challenged by the complex interactions present in the computational paths of these models.\nHypothesis: Incorporating correlations among output variables when interpreting a structured output model can lead to higher-quality explanations. A globally-trained local interpreter can effectively identify important features affecting a target output variable by considering structural dependencies during its training process.\nMethod: The SOInter method introduces a deep energy-based interpreter trained to explain structured output models by focusing on a single output variable as a target. The interpreter function is trained using an energy-based approach that accounts for correlations between output variables. The method relies on a two-phase energy network training: pre-training using existing techniques and fine-tuning alongside interpreter training. The interpreter uses the Gumbel-Softmax trick to select important features and evaluates performance via a custom loss function comparing energy values of predicted outputs.\nConclusion: SOInter demonstrated effectiveness in interpreting structured output models, outperforming existing techniques like Lime and Kernel-Shap in experiments with synthetic and real datasets. It integrates structural information during training, resulting in improved explanation quality, especially when correlations between output variables are significant. The method maintains computational efficiency and scalability, making it suitable for applications with large input features.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"Strengths: \\nOverall, the paper is well written and reads easily. \\nThis work presents one of the first approaches to post-hoc interpretation of structured output prediction. The proposed approach applies when the output to be predicted is a binary vector which includes a broad variety of tasks like multi-label classification, semantic segmentation... Any structured output method that predicts a bag of items for instance (bag of substructure) will be also eligible (even if not considered in this work, except for text).\\nThe optimization problem solved to learn the interpreter is appealing with a nice way to rely on the difference of energies associated to the perturbation of inputs by the interpreter. This is really the strong novelty of the paper, for me far beyond its application to structured output prediction.\", \"Weaknesses\": \"*The paper takes a specific angle to intepretability of structured output prediciton, by considering the input features as tabular data. When dealing with images at least, identifying \\\"independently\\\" the important pixels involved in the prediction is not what I expect from explainability.  I would be interesting for raw data like images by identifying a region in the image or a concept as a function of the input space as an explanation. I think a discussion here is expected. \\n\\n*The learning algorithm is not sufficiently well documented and I have questions about its robustness against the choice of hyperpameter : is it robust to tau ? how does the learning algorithm react if we change k ? do we obtain close results if change k by k-1 or k+1 for instance ? What the impact of these parameters on the final \\\"explainability\\\"\", \"Questions\": \"Please see questions above as well.\\n\\n1) Behaviour of the learning algorithm (see previous remark).\\nI would like to have more insights about the behaviour of the learning algorithm - I would like to see a study about the robustness of leanring when varying k. \\n\\n2)   Is it possible to incorporate in SOInter a way to encourage the identification of correlated input features for instance by taking into account the relationship between features ?\\n3) Did you study the robustness of your approach against noise in test images ?\\n4) on text multi-label classification you proposed as an evaluation metrics the post-hoc F1 and the relative F1\\nPlease re-formulate more clearly the relative F1 (I think there is a typo).\\nThe deceptive results may be due to the nature of  input text representation: even the words that are not considered as important by the interpreter can help to give some context and improve the performance. Can you comment on that ?\"}"}]}
{"paper_id": "DmDpu3r8iU", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the advancements in Large Language Models (LLMs) and the emerging concern about their alignment with human values. Although LLMs have enhanced capabilities, their potential misalignment poses risks, which is difficult to evaluate due to their complex and adaptive nature. Some existing methods have tried evaluating LLMs' understanding of values and alignment through different cultural and ethical perspectives, but these are limited in assessing deeper understanding.\nGap: Existing methods focus on LLMs' emergence of values and ethical considerations but do not address their understanding of values\u2014how LLMs categorize their responses within value frameworks and understand the reasons behind these categorizations. This paper identifies the need for evaluating both 'know what' and 'know why' elements of value recognition and understanding in LLMs.\nHypothesis: The authors hypothesize that a true understanding of values in LLMs requires assessment of both 'know what' (categorization of responses to value types) and 'know why' (understanding the reasoning behind these categorizations).\nMethod: The Value Understanding Measurement (VUM) framework is proposed to evaluate LLMs' understanding of values through the discriminator-critique gap (DCG) approach. Using the Schwartz Value Survey, the study develops a dataset with GPT-4 annotations to evaluate ten values. Five LLMs' responses are assessed on alignment with these values and their reasoning behind this alignment, using a combination of discriminator scores for 'know what' and critique scores for 'know why'.\nConclusion: The study finds that while 'know what' capabilities are impacted by scaling laws, 'know why' capabilities remain consistent across models, suggesting that LLMs might simulate understanding without genuinely grasping the values. This presents risks in plausible but context-driven explanations without true value alignment. Additionally, inherent understanding of risky values like 'Power' is insufficient in LLMs, influenced more by context than the models' inherent capabilities.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"rumor\": 20, \"Strengths\": \"- This paper studies a very interesting problem: what values are hold in the LLMs. Understanding the correct way to probe and measure this will be beneficial to the whole community.\\n- To me, the way the authors proposed to achieve this goal is novel.\", \"Weaknesses\": \"- I am confused by several parts when reading the paper:\\n  - In Figure 1, it's super unclear what is fed to the model in each step. Is the whole history included? It's more clear when reading through 4.3 but it would be much better if that can be reflected in the Figure. Also, it seems that the critique model only takes the two reasoning text as inputs. Then how does it deal with the contextualized phrases like \\\"the values I mentioned\\\"?\\n  - In 5.1.2 the authors mentioned they have a setting about \\\"Add prompts that make it relevant to the values\\\". I am also confused by this part. What are the prompts used here and where is the results? I would be better if the authors can \\\"dry run\\\" several examples for each setting in the appendix, which could help readers understand the whole process much easier.\\n  - I also had a hard timing parsing Figure 5. \\n\\n- The authors claimed that \\\"The scaling law (Kaplan et al., 2020) significantly impacts \\\"know what\\\" but not much on \\\"know why\\\", which has consistently maintained a high level\\\". I am not convinced by the argument:\\n  - Model parameters are not the single factor that affect the models' performance. For example, how many tokens they have been trained on, and what alignment data/methods are used heavily affect models' performance. Furthermore, there is no official statement from OpenAI about the size of GPT-3.5-Turbo (rumor: 20B model) and GPT-4 (rumor: MOE of 200B model). How is that possible to make such a claim without knowing the details of the two models?\\n  - I am also not convinced by the \\\"know why\\\" argument. Seen from Figure 3 and Figure 4, all models achieve quite similar scores. However, could it be that the GPT-4 doesn't have the capability to distinguish those models' outputs from the baseline reason? I saw the authors mentioned they did some kind of human study in Appendix B.2, but all the details about this human study is missing. I would like to see a thorough human study and analysis about this part.\", \"Questions\": \"see weakness\"}"}]}
{"paper_id": "HFtrXBfNru", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Graph Neural Networks (GNNs) are increasingly being applied in areas such as social networks, smart cities, and drug discovery due to their capability to model complex relations. However, they face performance issues as the underlying graphs evolve over time, which leads to representation distortion and subsequent performance degradation.\nGap: Existing GNNs struggle to maintain accurate representations over time due to the evolving nature of graph data. This change occurs as nodes and edges are added or modified, or new features are introduced, making it challenging to estimate the temporal generalization performance without continuous human annotation.\nHypothesis: The authors propose that an adaptive feature extractor using self-supervised learning can effectively minimize representation distortion and improve generalization estimation in evolving graphs. They introduce 'SMART', which employs self-supervised graph reconstruction techniques to enhance temporal generalization estimation.\nMethod: The proposed method, SMART, incorporates self-supervised learning for temporal generalization estimation. It employs an adaptive feature extractor achieved through graph reconstruction losses, namely structure and feature reconstruction. This is tested on synthetic Barab\u00e1si\u2013Albert random graphs and four real-world evolving graphs. The methodology involves refining theoretical lower bounds on representation distortion, performing graph-level reconstruction post-deployment, and evaluating temporal generalization estimation using metrics such as MAPE.\nConclusion: SMART demonstrates outstanding performance in estimating temporal generalization, as evidenced by experiments on synthetic and real-world data. The adaptive feature extractor significantly reduces information loss, improving performance estimation without relying on additional human annotations post-deployment. The study also highlights the critical role of structure graph reconstruction in minimizing prediction error.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"1.\\tThe general motivation for analyzing the generalization of evolving graphs is interesting and important. It is also interesting to see the theoretical analysis with a synthetic random graph demonstration with a closed-form generalization error.\\n2.\\tThe presentation quality of this paper is in general satisfactory, in terms of the organization and figure demonstration.\\n3.\\tThe experiments and analysis on the proposed method itself are relatively sufficient including model component ablation and hyper-parameter study.\", \"Weaknesses\": \"1.\\tOne major concern is the lack of baseline methods. The proposed method is only compared with a linear regression, which turns out to be a naive baseline. The methods mentioned in the related work D.2 are not compared. Moreover, only the MAPE with standard error is evaluated. More evaluation metrics are expected.\\n2.\\tIt will be better if the methods are evaluated on a few more state-of-the-art backbones to further enhance the effectiveness of the proposed method.\\n3.\\tThe assumptions made to conduct theoretical analysis and bound derivation could be a concern as they are often too strong. Evolving graphs also contain nodes disappearing in many cases(e.g. sensor networks with possible malfunction over time), which means the assumption of an ever-growing network can be often invalid. Moreover, the zero-mean requires a non-min-max preprocessing of features. The authors need to be specific about the claims.\\n4.\\tIt would be better to sketch the connection between the information loss and the form in Equation 2, section 3.2.\\n5.\\tIt would be better to define some notations such as the function l, and generalization error beforehand for better understanding of the idea.\", \"Questions\": \"Please see the weaknesses.\"}"}]}
{"paper_id": "eNoiRal5xi", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Domain Generalization (DG) aims to enhance the transferability of models from a source domain to unobserved domains. Traditional methods include alignment-based, augmentation-based, and regularization-based approaches, but they often underperform when domain information is limited. Sharpness-aware optimizers such as SAM focus on flattening the loss landscape in parameter space to improve generalization across domains, backed by PAC-Bayesian theories.\nGap: While SAM and its variants have improved DG, they primarily focus on parameter space perturbation, lacking theoretical guarantees for generalization to unknown domains that are not theoretically accessible. These methods do not address unexplored potential in data space perturbation for generalizing to unknown domains.\nHypothesis: The paper proposes that combining parameter space perturbation with data space perturbation can improve model generalization to unknown domains. By aligning the loss landscape between a source domain and perturbed (simulated unknown) domains, grounded in flat minima, the model should generalize better across unseen domains.\nMethod: The authors introduce Unknown Domain Inconsistency Minimization (UDIM), which aims to minimize the discrepancy in loss landscapes between the source domain and unknown domains using parameter and data perturbations. Unknown domains are simulated by perturbing instances from the source dataset. The UDIM framework combines SAM optimization with an objective that empirically bounds the true objective of the DG task using both parameter perturbation and data space exploration. UDIM's effectiveness is evaluated on several DG benchmark datasets, demonstrating statistically significant improvements, especially in scenarios with more restrictive domain information.\nConclusion: UDIM consistently outperforms SAM variants across multiple DG benchmark datasets, highlighting its superior generalization capability in unseen domains. By reducing the loss landscape inconsistency between source and unknown domains through combined parameter and data perturbation, UDIM provides a robust solution to the domain generalization problem.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1.This work extends the parameter perturbation in existing SAM optimization to data perturbation, achieving loss landscape alignment between source domains and unknown domains. Experiments show the validity of the proposed objective.  \\n\\n2.This work establishes an upper bound for DG by merging SAM optimization with the proposed objective.\\n\\n3.The proposed objective can be combined with multiple SAM optimizers and further enhance their performance, demonstrating the necessity of the loss landscape consistency between source and unknown domains.\", \"Weaknesses\": \"1.I believe that the proposed data perturbation method is consistent in both ideology and essence with traditional domain augmentation and adversarial attack techniques. So, what is the main difference and advantage of the proposed objective? And what if combining some domain augmentation techniques with the SAM optimizers?\\n\\n2.How to guarantee that the perturbed data is still meaningful, rather than generating some noisy samples? If so, will enforced the loss landscape alignment across domains bring negative impact? Besides, the unknown distribution may not necessarily be within the scope of perturbed data region.\\n\\n3.What is the sampling strategy for sampling data from $D_s$ to generate perturbed data?\\n\\n4.Is the optimization only conducted on the perturbed samples during the formal training phase? Would it be more stable to train on both the source and perturbed samples simultaneously?\\n\\n5.Since the training procedure involves gradient calculation, what is the time complexity after applying the BackPACK technique?\", \"Questions\": \"See weakness.\"}"}]}
{"paper_id": "MsOcVFzv8D", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Multi-domain text classification (MDTC) is an approach in supervised machine learning where the goal is to use available resources from various related domains to improve classification accuracy in a target domain, particularly when labeled data in the target domain is scarce compared to related domains. Techniques like adversarial training and the shared-private paradigm have been used for MDTC to achieve state-of-the-art performance. These methods focus on creating domain-invariant features that can be transferred across domains. Domain adaptation strategies, including statistical matching and discrepancy minimization, have also been effectively applied to this end.\nGap: Most current MDTC methods lack theoretical guarantees, which poses a significant obstacle to their development and improvement. The existing methods largely develop their effectiveness based on empirical observations rather than solid theoretical foundations. While techniques like f-divergence provide some theoretical backdrop, a broader generalization bound for MDTC is missing.\nHypothesis: Incorporating margin discrepancy, a metric used to measure domain divergence, into the framework of adversarial training for MDTC can provide a robust theoretical foundation and improve model performance. The extension of this theoretical analysis can bridge the existing gap between theory and practical algorithm development in the field of MDTC.\nMethod: The authors propose a method called Margin Discrepancy-based Adversarial Training (MDAT) for MDTC, guided by theoretical analysis. They first conduct a theoretical decomposition of the MDTC task into multiple domain adaptation tasks, using margin discrepancy to measure domain divergence. A new generalization bound for MDTC based on Rademacher complexity is formulated. Following this theoretical framework, the MDAT method is developed. It involves a shared feature extractor, domain-specific extractors, and two classifiers, where adversarial training is employed to use margin discrepancy for minimizing domain divergence. Empirical validation was done on two MDTC benchmarks, using the Amazon review dataset and FDU-MTL dataset.\nConclusion: The proposed MDAT method outperforms previous state-of-the-art MDTC approaches in empirical tests. The experimental results showcase its superiority in terms of average accuracy and its effectiveness across individual domains within both datasets. The method is supported by a new theoretical framework which provides guarantees about its performance, thereby addressing the noted gap and providing a more comprehensive understanding of MDTC algorithms.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 5, \"Strengths\": \"Most works in MDTC are empirical, while this paper, to this reviewer, appears to be the only one that is strongly justified theoretically.  Bringing the approach of [Zhang et al, 2019] to the context of MDTC is the key contribution of this work. Extending the development of [Zhang et al, 2019] from domain adaptation to MDTC as done in this paper is natural and sound.\\n\\nThe paper is in general well written, and an easy read.\", \"Weaknesses\": \"The theoretical development closely follows [Zhang et al, 2019]. In fact, to this reviewer, all theoretical results are merely re-stating the results of [Zhang et al, 2019] in the context of MDTC. In other words, the novelty in theoretical analysis is very thin. -- This is in fact not a weakness; I bring it up here just to mean that the theoretical analysis in this paper should not be regarded as a \\\"strength\\\" of any significant magnitude, despite the theoretical results presented.  I suggest the authors clearly state in Section 4.1 that the theoretical analysis closely follows that in [Zhang et al, 2019], so as not look over-claiming.\\n\\nI think the paper is above the acceptance threshold, although marginally.   \\n\\nIn below, I declare a confidence level of 5, which I need to explain. I did not check the proofs. But since the application of [Zhang et al, 2019] is so natural in this context, I think the presented results must be correct (that is also why I do not consider the theoretical development novel). I have not read all literature of MDTC.  So my confidence level \\\"5\\\" only refers to the correctness of the paper. If there were another work that has already exploited [Zhang et al, 2019], then my rating of the paper would drop below the acceptance threshold.\", \"Questions\": \"NA\"}"}]}
{"paper_id": "Pa6SiS66p0", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Human brains have a remarkable capability for continual learning (CL), incorporating information from multiple modalities (audio, visual, etc.) to enhance perception and cognition. This multi-modal integration improves memory and perceptual learning by creating comprehensive environmental representations. However, deep neural networks (DNNs) typically suffer from catastrophic forgetting when learning new tasks in sequence, particularly in a unimodal setting, which limits their deployment in dynamic environments. Previous work in this area often emphasizes unimodal approaches, primarily within the visual domain, and multi-modal learning has primarily been explored in i.i.d. settings for specific applications, such as action and audio-visual speech recognition.\nGap: The study identifies a gap in the exploration of leveraging multiple modalities to mitigate forgetting in DNNs. Specifically, there is insufficient understanding and techniques for combining multiple modalities to enhance continual learning in DNNs, particularly in challenging scenarios like class incremental and domain incremental learning.\nHypothesis: The integration of multi-modal learning into DNNs can enhance their lifelong learning ability, allowing models to develop more accurate and robust representations less prone to modality-specific regularities and catastrophic forgetting by integrating complementary information from multiple modalities.\nMethod: The study introduces a benchmark for multi-modal continual learning based on the VGGSound dataset, which includes paired visual and audio data. It proposes a rehearsal-based multi-modal continual learning method that utilizes the relational structural similarities between data points in each modality for integration and alignment. This approach includes maintaining modality-specific representations, which are then aligned and consolidated into a unified representation space. The experiments were conducted across various challenging continual learning settings like class-incremental (Seq-VGGSound), domain-incremental (Dom-VGGSound), and generalized class-incremental learning (GCIL-VGGSound), comparing the proposed method, SAMM, against a baseline experience replay method on multi-modal datasets.\nConclusion: The study concludes that integrating multiple modalities in DNNs for continual learning significantly reduces catastrophic forgetting, enhances adaptability, and provides a better stability-plasticity trade-off. The proposed multi-modal continual learning method, SAMM, improves both multi-modal and single-modality inference capabilities and sets a strong baseline for further research using the VGGSound-based benchmark. It also highlights the potential of multi-modal learning to develop robust models that can adapt to dynamic and evolving environments.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"+ The proposed benchmark covers three CL scenarios and can be beneficial to the community. \\n+ The analysis in Section 3 makes sense and provides empirical evidence for the superiority of multiple modalities over single modality in CL.\\n+ The paper has good motivation and is well organized.\", \"Weaknesses\": \"My major concern with this paper is the lack of comparison and experiments. The evaluation seems a bit weak to me as all the experiments are conducted on VGGSound only, and the baseline Experience Replay for comparison with the proposed approach is from 2018. I wonder if the authors could apply some more recent unimodal CL approaches ([1][2] etc.) to the problem.\\n\\n[1] SS-IL: separated softmax for incremental learning   \\n[2] Class-incremental learning by knowledge distillation with adaptive feature consolidation.\\n\\n---\\nAlso, in terms of comparison with multimodal CL approaches:\\n+ (1) Could the authors further clarify why the proposed approach can not be applied to vision-language? What is the advantage of the proposed benchmark compared with [3], besides the modality difference? \\n+ (2) I understand that [4] is published after the submission ddl, but it would be good if the authors could comment a few sentences about the differences with them in the rebuttal.  \\n\\n[3] Climb: A continual learning benchmark for vision-and-language tasks.  \\n[4] Audio-Visual Class-Incremental Learning\\n\\n---\\nFor the semantic-aware feature alignment, I wonder if the authors can provide some visualization examples to demonstrate that the model indeed learns the desired modality-specific features, such as Figure 4 in [5].\\n\\n[5] The Modality Focusing Hypothesis: Towards Understanding Crossmodal Knowledge Distillation\\n\\n---\\nTypo, Figure 4 caption, \\\"leverage leverages\\\"\", \"Questions\": \"See weaknesses.\"}"}]}
{"paper_id": "bAMPOUF227", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large Language Models (LLMs) have shown significant abilities in in-context learning through prompt engineering, and recent advancements in generative models have expanded their applications. However, LLMs face challenges like resource demands, slow inference, and susceptibility to hallucinations. In contrast, Supervised Language Models (SLMs) are more cost-efficient and can rapidly adapt to specific tasks, although they lack the broad capabilities of LLMs.\nGap: Existing research has primarily treated LLMs and SLMs as separate paradigms and hasn't thoroughly explored the use of task-specific SLMs to systematically enhance LLMs' in-context learning, particularly in out-of-distribution (OOD) scenarios.\nHypothesis: The authors hypothesize that SLMs, which are fine-tuned on specific tasks, can enhance LLMs' performance during inference by providing task-specific supervised knowledge. This can help LLMs generalize better to OOD data and reduce hallucinations.\nMethod: The paper introduces a framework called SuperContext. It integrates the predictions and confidence scores from task-specific SLMs into the in-context learning prompts of LLMs. This method is tested using Llama 2 and ChatGPT, with experiments conducted on datasets for OOD tasks and question answering. The approach uses 16 curated datasets across 9 tasks, with both zero-shot and few-shot experiments to evaluate performance compared to baseline methods.\nConclusion: SuperContext is shown to enhance the performance of LLMs in handling OOD tasks, improving generalizability and factuality compared to baseline LLM and SLM models. SuperContext with integrated SLM knowledge improves performance significantly, particularly in reducing hallucinations in generative tasks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"The proposed approach serves as a plug-in method, resulting in enhanced versions of Llama 2 and ChatGPT that outperform their original counterparts in terms of generalizability and factuality. SuperContext can bring decent performance benefit compared to few-shot in-context learning and outperform original SLMs and LLMs with both zero-shot and few-shot settings.\", \"Weaknesses\": \"1. I disagree with the statement in the introduction that says, \\\"However, since our goal is to allow reliable task adaptation rather than knowledge acquisition, the consulting agent becomes SLMs rather than search engines.\\\" It seems that the approach in this paper is quite similar to what Hugging Face's [1] GPT does, with the only difference being that it uses its own trained API instead of Hugging Face's SOTA small model API.\\n\\n2. The paper lacks significant novelty, essentially enhancing the output of small models with larger models. However, it extensively validates the effectiveness of this approach (even though Hugging Face's GPT has also done similar validations).\\n\\n3. The paper intentionally incorporates confidence when using the output of small models, but it lacks a detailed ablation study on the role of confidence. I am particularly interested in understanding the significance of confidence in this context.\\n\\n[1] HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face. NeurIPS 2023\", \"Questions\": \"None\"}"}]}
{"paper_id": "mvGa1ikBD3", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Simulating nonlinear and anisotropic materials is crucial for applications in engineering, computer graphics, and robotics. Traditional mesh-based simulations are accurate but computationally expensive, which limits their use in interactive settings. Recent developments in graph neural networks (GNNs) offer a potentially more efficient alternative for such simulations.\nGap: Existing GNN-based methods for simulating material deformations are limited to isotropic materials, as they do not account for directional deformations and thus cannot model anisotropic properties.\nHypothesis: The authors propose that incorporating directional encodings in GNNs can preserve deformation directionality during message passing, thus enabling accurate modeling of anisotropic materials.\nMethod: The proposed GNN architecture involves a new edge feature decomposition where edge features are aligned with basis vectors to capture directional dependencies. The network follows an encoder-processor-decoder design and utilizes a physics-based, self-supervised training loss focusing on dynamic equilibrium. The approach involves training on a graph representing the mesh of a deformable object, where edge features include fiber orientations which are used to factor anisotropic properties during deformation simulation.\nConclusion: The authors' approach successfully models anisotropic materials by preserving directional deformation information, achieving better performance than existing GNN-based simulations in both convergence speed and accuracy. Their method also maintains volume preservation and accurately simulates stress-strain behavior in anisotropic materials, even outperforming traditional mesh-based GNN approaches.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"The paper addresses a useful open question within the emerging topic of learned simulators, equipping message-passing networks with anisotropic elasticity.  The solution is simple but effective, and experimental results show a meaningful improvement over the baseline method.  Overall, the writing is clear, and experiments seem reproducible.\", \"Weaknesses\": \"The technical contribution seems potentially incremental from prior work (MeshGraphNets) -- however, this is not necessarily an issue, as the experiments are well-designed, the results are solid and finding are conclusive.\\n\\nI would be interested to see additional experiments, beyond the cantilever (and cantilever-like) setup, such as simulation with collision/contact.  This is not a requirement though, as the existing experiments are quite informative.\", \"Questions\": \"- I am curious about the \\\"material space bases\\\" mentioned on page 4.  Could you elaborate on how the \\\"material space bases\\\" are defined exactly?  How are these defined relative to global coordinates, and are they defined in a canonical way?\\n\\n- Would defining the local bases in a different way change the three axis-aligned weights/coefficients that are computed during preprocessing?  In turn, would this make it difficult to learn?  (Presumably the x-axis is aligned with the edge, what about the others.). Thanks, look forward to your reply.\"}"}]}
{"paper_id": "LYGHdwyXUb", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Multi-task Reinforcement Learning (MTRL) revolves around learning multiple tasks simultaneously, potentially improving sample efficiency by sharing information across tasks. Existing MTRL methods mostly focus on sharing data and parameters, but they often overlook behavior sharing, which involves the transfer of learned actions between tasks.\nGap: Existing behavior-sharing approaches in MTRL are limited because they directly imitate policies from other tasks. This approach can be suboptimal if different tasks require different actions for the same states. The challenge is to preserve task-specific optimality while leveraging behavior sharing across tasks.\nHypothesis: The authors propose a novel behavior-sharing formulation that selectively leverages policies from other tasks as the current task\u2019s behavioral policy, thereby improving learning efficiency and sample use across multiple tasks.\nMethod: The proposed framework, Q-switch Mixture of Policies (QMP), estimates the shareability between task policies and uses them as behavior proposals to collect training data. The method involves a Q-switch that assesses the quality of behaviors from other tasks based on the agent's current state and training progress. Only those behaviors deemed beneficial are selectively incorporated. The approach was empirically tested on various MTRL environments involving manipulation, locomotion, and navigation tasks, demonstrating improved sample efficiency.\nConclusion: The QMP framework successfully improves sample efficiency by effectively identifying and sharing beneficial behaviors across tasks. The method is complementary to parameter-sharing strategies, and empirical results show enhanced performance in complex environments involving diverse tasks with potentially conflicting behaviors.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1.\\tThis paper is well-written and easy to follow\\n2.\\tSharing learned behaviors among tasks is an interesting and important topic in RL.\\n3.\\tAuthors conducted extensive experiments and analysis to empirically illustrate the effectiveness of QMP.\", \"Weaknesses\": \"1. Lack of theoretical analysis on the convergence (rate) of QMP.\\n2. Lack of intuition and detailed analysis on why the proposed method works (i.e., why could QMP make sharing desired behaviors among tasks possible? Please see Question 2 for my concern).\\n\\nPlease refer to my questions below for my concerns.\", \"Questions\": \"1. Why do you choose to roll out H steps instead of just one step after choosing one policy to collect data? QMP just uses the Q function under a particular state to choose the behavioral policy, which cannot guarantee that the selected behavior policy is helpful for the current task after stepping out of the considered state.\\n2. Will QMP cause undesired behaviors sharing? As I mentioned in Question 1, the selected behavioral policy will roll out for H(>1) steps, which may incurs sub-optimal behaviors.\\n3. Why choosing the behavioral policy based on the learned Q-function to collect data will essentially share desired behaviors among tasks? I understand that using Q-function as a proxy may help select more better actions. However, the Q-function may be biased and not learned well during training, which could even hurt the learning process.\\n4. Will QMP even slow down the training process? Say, the learned policy for the current task proposes an action, which is optimal but has an under-estimated Q function. Due to the biased Q function, QMP selects another policy to collect data, which chooses a sub-optimal action. Although the TD update will fix the estimated Q value of the sub-optimal action, it may be more efficient if we directly update the Q value of the optimal action, the thing that we really care about.\\n\\nI am willing to raise my scores if you could solve my concerns.\"}"}]}
{"paper_id": "ZlEtXIxl3q", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Fitness functions map biological sequences to desirable properties, playing a crucial role in protein engineering by predicting sequence performance based on limited data. Global epistasis models have been effective in learning these functions, assuming a simple latent function transformed by nonlinearity. Existing methods often struggle with these transformations.\nGap: Existing methods like Mean Squared Error (MSE) loss are inefficient in learning fitness functions influenced by global epistasis because the nonlinearity they introduce does not admit sparse representations, making them hard to learn from limited data.\nHypothesis: Contrastive loss functions, such as the Bradley-Terry loss, can more effectively learn the sparse latent functions implied by global epistasis models by focusing on ranking rather than numerical accuracy, which bypasses the issues caused by nonlinearity.\nMethod: The paper applies contrastive losses to recover sparse latent fitness functions from data subjected to global epistasis. Simulations were run using fitness functions sampled from NK models and transformed by various nonlinearities. These models were trained with both MSE and contrastive losses, and their performance compared across simulated and real-world datasets, validating their approach on FLIP benchmark tasks.\nConclusion: Contrastive losses consistently enable better recovery and ranking of fitness functions from globally epistatic data than MSE, as demonstrated by improved performance on benchmark tasks. This method proves more data-efficient and robust, even under incomplete or noisy conditions, making it a practical alternative for protein fitness predictions.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 2, \"Strengths\": \"1. Contrastive learning has demonstrated promise in the field of computer vision; this paper shows a novel application of this concept to the field of Biology.\\n\\n2. The paper has a strong theoretical foundation and is well presented. \\n\\n3. Quantitative results are convincing and promising.\", \"Weaknesses\": \"1. The empirical evaluation has focused only on a single benchmark, FLIP. It would strengthen the paper if the approach was validated with even more datasets.\", \"Questions\": \"1. Is there any way to extend the empirical evaluation beyond the FLIP benchmark?\"}"}]}
{"paper_id": "fQHb1uZzl7", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: In the field of computer vision, finding visual correspondences between images is a key problem with many applications such as SLAM, augmented reality, and structure from motion. Traditional methods focus on sparse correspondences, detecting key points and matching them across images, while dense correspondence aims to match all pixels between images. Recent research in visual correspondence has utilized either feature aggregation or cost aggregation for improving dense matching tasks, each having its own advantages and limitations.\nGap: Many existing methods either focus solely on feature aggregation or cost aggregation for dense matching tasks, without leveraging the potential strengths of combining the two. Feature aggregation aligns similar features within and across images, while cost aggregation ensures coherence in flow estimates across pixels. However, relying on only one type might not fully utilize the potential information available.\nHypothesis: The paper hypothesizes that by effectively unifying feature aggregation and cost aggregation within a Transformer-based framework, enhanced dense matching performance can be achieved due to the complementary strengths of both aggregation types.\nMethod: The authors propose a new architecture called UFC (Unified Feature and Cost Aggregation Transformers) which utilizes Transformers to unify both feature and cost aggregation through self- and cross-attention mechanisms. The method combines feature descriptors and cost volumes, using integrative self-attention to handle both types of data simultaneously. This approach is further refined in a coarse-to-fine hierarchical manner to improve robustness, culminating in multi-scale predictions to enhance the final correspondence estimation. The network produces multi-scale predictions, computes their confidence scores, and selects the most confident flow for final prediction. The method is evaluated on various benchmarks for semantic and geometric matching, demonstrating significant improvements over existing techniques.\nConclusion: The proposed UFC method effectively integrates feature and cost aggregation, leading to improved performance in dense matching tasks, as demonstrated on various benchmarks. The Transformer-based architecture capitalizes on self- and cross-attention mechanisms, allowing for robust and efficient estimation of correspondences even under challenging conditions such as large intra-class variations and geometric deformations.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"1) This paper is generally presented well;\\n\\n2) The idea of use cost score matrix for both self- and cross-attention feature updating is simple and effective;\\n\\n3) Experimental results are good.\", \"Weaknesses\": \"1)  Section 5.1. It is quite blurry for me whether previous state-of-the-art methods in Table 1 and 2 are trained on the same datasets; For example, the proposed method is trained on the DPED-CityScape-ADE and MegaDepth datasets;\\n\\n2) The performance of the proposed method on the optical flow (KITTI, Sintel) task is blurry for me;\\n\\n3) It's good to see the improved matching performance on the HPatches dataset. However, I want to see whether the improved matching would lead to better Rotation and translation estimations. \\n\\n4) Using cost score matrix for both self- and cross-attention feature updating is good. However, this contribution may be constrained to large overlapping ratio between images. If pairwise images have small overlapping ratio, the cost score matrix is noisy, and may provide wrong guidance for feature updating. Would you please check whether the proposed method works on some challenging image pairs from the MegaDepth dataset.\\n\\n5) Please show some failure cases.\", \"Questions\": \"Please refer to the weaknesses.\"}"}]}
{"paper_id": "DL7JWbdGr3", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Forecasting epidemic trends is crucial for public health decision-making. Deep learning methods have shown promise in outperforming traditional mechanistic models by learning from past epidemic data. Projects like Tycho have aggregated large datasets from past diseases, presenting an opportunity for leveraging this rich source of historical data.\nGap: Traditional epidemic forecasting methods often only focus on the specific epidemic of interest and fail to leverage the wealth of historical data from different diseases due to challenges in data heterogeneity and lack of methodologies to incorporate such diverse data. Moreover, previous models do not benefit from pre-training paradigms effectively used in other domains like language and vision.\nHypothesis: Pre-trained Epidemic Time-Series Models (PEMS) can effectively learn useful priors from heterogeneous epidemic data using self-supervised learning tasks, allowing them to be efficiently fine-tuned for various downstream epidemic analysis tasks, resulting in improved performance over current state-of-the-art models.\nMethod: The authors introduce PEMs, which involve pre-training transformer-based models on multiple datasets from different diseases using self-supervised learning tasks, designed to capture epidemic dynamics. They utilize tasks such as random masking (RANDMASK), last segment masking (LASTMASK), peak masking (PEAKMASK), and seasonal detection (SEASONDETECT) to learn valuable priors. Data is sourced from diverse datasets including Project Tycho and influenza datasets from the US and Japan. The models are evaluated against existing forecasting benchmarks, focusing on efficiency and robustness to new epidemics.\nConclusion: PEMs significantly outperform existing state-of-the-art models in various epidemic forecasting tasks by providing 11-24% better forecasting accuracy and improved peak prediction. The pre-training scheme enhances training efficiency and allows generalization to new diseases not seen in the pre-training phase, showcasing the model's adaptability and utility in real-world epidemic monitoring.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"- originality: This is a new application domain for SSL as far as I know, but the methodology is not particularly new.\\n\\n- quality: The submission is well written and formatted, with strong experimental results and ablations.\\n\\n- clarity: most of the training details are there, though much is relegated to the appendix.\\n\\n- significance: the work is significant as an application of SSL to a specific domain with greater performance, and finds relevance in the current post-pandemic context.\", \"Weaknesses\": \"- sounds like your segments of time series is basically patching, similar to (Nie et al. 2022). If so, why do you achieve so much better results than PatchTST? Is it just the finetuning?\\n\\n- I'm not sure I understand why inputting each time step as a single token would lack \\\"semantic meaning\\\", seeing as the output representations of a transformer for a given token are influenced by all the other tokens in the input sequence (i.e. \\\"contextual embeddings\\\")\\n\\n- How do you normalize the training process of your method wrt baselines? How can you disentangle the architecture's influence from the influence of the training process and the influence of using additional compute?\\n\\n- unclear what is being evaluated in appendix D table 6 since it's not known how the baselines are trained. For example, it is clearly remarked that no pre-training is performed for the PT baseline, but not why.\\n\\n- why are the results around one of the contributions (Significant improvement in data and training efficiency and adaptability to novel epidemics) only in the appendix?\", \"Questions\": \"- There might be bleed between the influenza data from 2001 to 2010 and the crypto data from 2006 to 2012 given the temporal alignment, but I'm not knowledgeable enough to judge how strong the overlap could be. Is the argument that, since influenza and crypto have different inherent dynamics, they shouldn't influence each other?\\n\\n- How do you choose the value of T, i.e. the length of the input time series?\\n\\n- How do you justify the default choice of gamma = 0.1 for lastmask ? Seems like 0.2 performs well too.\\n\\nOverall, I am rating this a 5 (marginally below acceptance) before discussion, since it is hard to disentangle the impact of the supervised pre-training from the architectural design choices and the increased compute/data. There are no details on how the baselines are trained, so these comparisons do not properly motivate which components of PEM lead to its superior performance. I am very amenable to changing this rating given discussions around how the baselines were implemented for fair comparisons.\"}"}]}
{"paper_id": "q4Bim1dDzb", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Inverse rendering seeks to estimate scene properties such as geometry, materials, and illumination from multi-view 2D images. Traditional methods like Neural Radiance Fields (NeRF) adapt these for inverse rendering by implicit neural representations, but require substantial computational resources due to complex mappings involved in separate modeling of each property.\nGap: Implicit learning methods for inverse rendering face low optimization efficiency as they separately model each scene property using deep neural networks, leading to long processing times. Additionally, modeling illumination with environment maps in explicit representation methods remains computationally intensive and struggles with complex real-world lighting conditions.\nHypothesis: The authors hypothesize that a unified voxelization framework, capable of modeling geometry, materials, and illumination concurrently through explicit scene representation, can significantly accelerate inverse rendering.\nMethod: The authors propose UniVoxel, which utilizes a unified voxelization framework to encode scenes into latent volumetric representations. This includes two key elements: SDF for geometry and a semantic field for materials and illumination. The approach leverages lightweight MLP networks to learn these properties efficiently, using Spherical Gaussians for modeling local incident light radiance, integrating illumination into the framework without complex ray tracing. The method was tested on benchmarks with reduced per-scene training time and favorable reconstruction quality.\nConclusion: UniVoxel demonstrates superior optimization efficiency compared to state-of-the-art inverse rendering methods, reducing training time significantly while maintaining high rendering quality. It effectively integrates illumination modeling and supports varying lighting conditions, showcasing its robustness across diverse datasets.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"+This submission proposes a unified grid-based representation that incorporates geometry and material properties. The grid-based representation has shown to be more friendly for optimization.\\n\\n+Optimization becomes more straightforward with shallower MLPs. The combination of grid-based volume representation and shallow MLPs leads to more efficient optimization. \\n\\n+Spherical Gaussian (SG) representation exhibits higher-order frequency characteristics for illumination compared to spherical harmonics (SH).\", \"Weaknesses\": \"-My major concern is that the utilization of feature grids for more efficient training (the major claim) is not a novel insight. For instance, in InstantNGP, it has been demonstrated that the combination of dense hashable grids and shallower MLPs significantly enhances efficiency. Furthermore, the memory concerns highlighted in this submission can potentially be alleviated by adopting more efficient data structures for the grid, such as hashable grids as employed in InstantNGP.\\n\\n-The observed performance improvement is incremental and does not surpass the performance achieved by previous methods in some cases. For instance, a notable limitation of the proposed method is that it often results in albedo estimates that include shading components as shown in Fig.5 first row, in contrast to TenSor RT.\\n\\n-The assertion that SH cannot effectively model higher-frequency illumination may not be conclusive, as it is based on testing only up to order-3 SH. Increasing the order of SH could potentially address this limitation. To provide a more compelling evaluation, it is advisable to ensure an equal number of parameters when comparing SG and SH representations.\\n\\n-Details missing: It is not mentioned clearly how the global illumination and self-occlusion are handled during relighting. Please see questions below for more detailed questions.\", \"Questions\": \"1. For SG-based illumination representation, how many parameters are used for representing the Gaussians? Is it comparable to the SH counterpart? \\n\\n2. How are global illumination and self-occlusion handled during relighting? During scene reconstruction, those could be baked in the SG/SH-based per-point illumination. But during relighting, the per-point environment might should be different.\"}"}]}
{"paper_id": "YIls9HEa52", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper addresses the dimensionality reduction of neural activity and behavior using unsupervised methods, particularly focusing on the recurrent switching linear dynamical system (rSLDS). This model is known for its ability to describe the latent dynamics of neural spike train data through discrete switches between a finite number of linear dynamical systems. However, traditional Hidden Markov Models (HMM) on which these models are based often come with limitations such as a fixed number of states and lack an inherent geometry over states.\nGap: The current rSLDS models suffer from limitations including a fixed number of discrete states across trials and a lack of latent structure or organization of these states, which limits their applicability to trial-varying data in neural analysis. Additionally, existing hierarchical Dirichlet processes can handle flexible state cardinality but do not support recurrence or other dependencies between discrete states allocations.\nHypothesis: The authors propose an infinite recurrent switching linear dynamical system (irSLDS) model that integrates a distance-dependent Chinese restaurant process (dist-CRP) with latent geometry and flexible state cardinality. This model is designed to overcome the stated limitations by allowing for a semi-Markov state process and capturing recurrent dependencies in the state transitions, enriched by a PDE framework for efficient computational inference.\nMethod: The proposed irSLDS model introduces a semi-Markov discrete state process leveraging properties of the distance-dependent CRP. A partial differential equation (PDE) approach is employed to create a semi-parametric formulation for dynamical sufficient statistics for discrete states. The irSLDS model is validated on synthetic data, demonstrating its efficacy and then applied to electrophysiological data from mice during decision-making tasks. The study employs variational Laplace-EM for inference and utilizes parallel computing methods to enhance computational performance.\nConclusion: The irSLDS model effectively addresses the limitations of traditional rSLDS models by providing a flexible and efficient framework for analyzing trial-varying neural data. It is able to uncover non-stationary processes in both synthetic and real mice neural data, capturing relevant dynamics not possible with fixed-state models. The introduction of a latent geometric structure over states also facilitates better interpretability and application across various neural analysis settings.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"The paper's originality derives from improvements it makes in the class of AR-HMM and SLDS models by removing exchangeability without removing input recurrence. This enables greater efficiency with expressibility. Overall, this work nicely integrates the heat-equation generalization and recurrent switching state space models. They present overall improvements on two different experiments with greater efficiency of parameters. Visuals are strong, particularly ones pertaining to the flow fields and the switching states of the rSLDS and irSLDS.\", \"Weaknesses\": \"Most of the comparisons are done between rSLDS and irSLDS, yet the results are not convincing and substantially novel in the experiments conducted. If the authors are adamant about just comparing these two models, then it would be nice to see comparisons at greater scales instead of just testing 4-8 states for NASCAR and the IBL experiments. The results at just these scales do not appear novel.\\nFurthermore, both experiments show that the irSLDS models uncovered switching at task-relevant states, yet it isn't clear why adding the heat equation prior itself is the best choice in discovering this switching. The authors discuss their reasons for using the heat equations but don't explain further into infinite priors that possibly could have achieved the same if not more.\\nThe efficient of the parallel scan is not clear. Other works that utilize scans to efficiently compute the transition probabilities such as S4D (Gu et al., 2022), DSS (Gupta et al., 2022), S5 (Smith et al., 2023) are done on diagonalized transition matrices. The connection to the parallelization of the linear dynamics in Equation 8 is unclear since the transition matrix in this work is not diagonalized, so this work does not reap the same computational benefits that they claim, and their algorithm cannot scale effectively since it incurs cubic cost.\", \"Questions\": \"Smith et al. is cited in reference to the use of a scan operation to compute transition probabilities. However, S5 assumes a diagonal state matrix to efficiently compute the linear recurrence which takes a different form than the time-varying transition matrices in this work. How can this scan method scale efficiently?\\nWhy does the heat equation prior directly enable discovery of greater fluctuations/switching in the data? Can you attribute the performance gains to this specific infinite prior?\\nWhy not compare to other linear state space models and generalize outside of rSLDS and irSLDS that can allow for input-dependent transitions and recurrent relations without the explicit use of the influence functions as a geometric prior?\"}"}]}
{"paper_id": "xh0XzueyCJ", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Diffusion models have shown transformative capabilities in graph generation, enabling efficient and realistic graph generation for various applications such as network optimization, social network analysis, and drug design.\nGap: Current diffusion models for graph generation lack precise control over non-differentiable constraints (hard constraints), which limits their applicability in practical scenarios where specific structural properties need to be controlled, such as in drug discovery.\nHypothesis: The authors propose that precise control over graph generation to satisfy hard constraints can be achieved by using projected diffusion methods, specifically PRODIGY, which allows for plug-and-play control without the need for retraining existing models.\nMethod: PRODIGY is a plug-and-play approach that samples from any pre-trained diffusion model, enforcing specified constraints by projecting the denoised samples at each timestep onto the constrained space. The method finds efficient projection operators for various graph constraints and uses a controlled sampling process that ensures specified hard constraints are met while maintaining data distribution fidelity. The method is tested on both generic and molecular graphs, improving constraint satisfaction significantly.\nConclusion: The proposed PRODIGY method successfully enables control over the generation of graphs to satisfy hard constraints, significantly improving performance on constraint satisfaction in both generic and molecular graph generation tasks. The method shows flexibility and effectiveness, maintaining proximity to the original data distribution while satisfying constraints.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. Precise control with arbitrary constraints is a desired property for graph generation. The authors propose a general solution for certain classes of constraints. The authors derive closed-form solution for many commonly used constraints under the proposed projection approach. The detailed derivation may also inspire future research where new constraints may appear.\\n2. The empirical results are comprehensive. The authors show the proposed method can effectively control the generation direction towards graphs that satisfy the constraints, on both 2D graphs and 3D graphs. They further demonstrate the sensitivity of the approach to the interpolation value $\\\\gamma_t$$. Analysis on the efficiency of the approach is also provided.\", \"Weaknesses\": \"1. Though when the constraints cover all the graphs in the test set, the generated distribution is largely unaffected, in scenarios where the constraints do take effect, the generated distribution obviously deviates from the original distribution. And the deviation is model-sensitive. For example, in Table 2, with edge count as the constraint, the performance of GDSS downgrades by two folds on Clus. while EDP-GNN is largely unaffected. Also, on the dataset of Enzymes and Grid, both GDSS and EDP-GNN suffer a lot from the constraints. Furthermore, Figure 12 shows a lot of unrealistic molecular graphs (e.g. very large rings, disconnected graphs, very long bridging bonds) with the constraints on atom count.\\n2. The proposed projection paradigm main only applied to constraints with simple calculation process so that the optimization problem has a closed-form solution. When the constraints become complicated (e.g. involves non-linearity), it might be arduous or impossible to find such closed-form solution.\", \"Questions\": \"1. Maybe a typo in section 4 right under equation 3: $\\\\Pi_c(z)\u00a0=\u00a0\\\\arg\\\\min_{z\\\\in\u00a0c}||z-x||^2$\uff0cshould be $\\\\Pi_c(x) = \\\\arg\\\\min_{z\\\\in c}||z-x||^2$\"}"}]}
{"paper_id": "3pWSL8My6B", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: In explainable AI, one key challenge is determining whether the knowledge encoded by deep neural networks (DNNs) can be considered symbolic. Previous studies have observed some symbolic concept emergence in DNNs, but there had not been a formal definition or proof. Concepts in DNNs as symbolic primitives could bridge cognitive science, neuroscience, and mathematics, although this paper focuses purely on the mathematical and algorithmic perspectives while ignoring cognitive issues.\nGap: Despite empirical observations suggesting that DNNs encode a few salient interactions that symbolize concepts, there has been no formal proof demonstrating how and when these symbolic interaction primitives emerge in DNNs. Addressing this could profoundly impact both theoretical and practical aspects of AI.\nHypothesis: The paper hypothesizes that under specific conditions, most DNNs encode only a small number of sparse interactions between input variables, which can be considered as symbolic primitive inference patterns. These conditions include (i) zero high-order derivatives of network output, (ii) DNNs delivering higher confidence with less occlusion, and (iii) consistent confidence levels in occluded samples.\nMethod: The authors introduce a theoretical framework proving the emergence of sparse interaction primitives by setting three conditions: zero high-order derivatives (Assumption 1-\u03b2), task-driven monotonicity in strategic occlusion (Assumption 2), and bounded degradation on occlusion confidence (Assumption 3). The authors use Harsanyi interactions to define the symbolic primitives and perform experiments on DNNs, including image and natural language datasets, to illustrate interaction sparsity and evaluate p values tied to masking effects.\nConclusion: The study proves that symbolic interaction primitives are a common phenomenon across various DNNs trained for different tasks under defined conditions. It establishes the link between sparse interactions and the nature of tasks, offering a foundation for conceptualizing DNNs in explainable AI. Future work aims to tighten focus and clarify conditions for interaction emergence.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 2, \"Strengths\": \"* The paper is well-written and clear.\\n* The paper contributes novel proofs for theorems 2 onwards.\\n* The paper appears well-placed in the relevant literature.\\n* The papers theoretical results appear significant and relevant to the ICLR community.\", \"Weaknesses\": \"* The claim of to \\\"prove the emergence of symbolic concepts (or more precisely, sparse primitive inference patterns) in well-trained AI models.\\\" (Line 1), appears too broad and un-substantiated, specifically encompassing \\\"AI models\\\". It appears the evidence presented in the paper is only applicable and demonstrated for Deep Neural Networks (DNNs). For example, I cannot see how this proof or claims can be applied to other AI models that are not DNNs, such as Random Forests, which considers the outputs of all trees; k-NN's, that use all features to compute distances, or Boosting algorithms (e.g., Gradient Boosting Machines). Specifically, the empirical evidence presented in the paper in Figure 3, Figure 4, Figure 5 and Table 1, only apply to deep neural network models, and there is no other AI models that are not DNN's to support this claim.\\n* It is not clear how the input variables should be grouped to form the set of $S$ input variables. For example, in the dog image example, \\\"image regions\\\" are proposed; however, there is no indication or discussion on how large these \\\"image regions\\\" should be for the approach and proof to work. For example, does this sparse primitive approach still hold if the \\\"image regions\\\" are too small, e.g., each region is a single pixel, and does it still hold if the \\\"image regions\\\" are too large, e.g., encompassing most of the image, or even half of the image. Some discussion on this, and how to select these groupings could be informative to the reader.\\n*  Missing error bars; and large error bars. There appears to be missing error bars for Figure 4, Figure 5 (a), Table 1 (top row: Percent of samples with monotonicity). The provided error bars in Table 1 and Table 2, are so large that they overlap; leading the reader to suspect that the claim of the input samples satisfying the monotonicity assumption is not empirically verified, as the numeric results in Table 1 are not statistically significant, as the error bars overlap.\", \"Questions\": \"* Can you clarify if the core proof of the paper applies to other AI models that are not DNN's? If it does, can you provide evidence for this, and if it does not, perhaps think of refining your claims, and title to be only applicable to DNNs.\\n* Can you clarify how the reader should group input variables to form the set of $S$?\\n* Can you include error bars for the top row of Table 1, and include error bars for Figure 4 and Figure 5 (a). Also can you re-run with more random seeds to see if the error bars in Table 1 reduce.\"}"}]}
{"paper_id": "YGTSLDAPqb", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Domain adaptation is critical in scenarios where machine learning models trained on labeled source domains encounter significant performance drops when deployed on out-of-distribution (OOD) target domains. This issue is prominent in fields like astronomy, where labeled data is limited to bright, nearby objects, leaving datasets of faint, distant objects largely unlabeled. Existing strategies leverage self-supervised pretraining on unlabeled data, but these approaches may not always yield the desired improvements in adapting models to OOD data.\nGap: The existing approach of standard fine-tuning following self-supervised pretraining does not consistently enhance OOD performance over purely supervised training on labeled source data. There is a disconnect because generic data augmentations used in pretraining may not effectively bridge the input space between source and target domains.\nHypothesis: To resolve the limitations of conventional fine-tuning techniques, the hypothesis is that self-supervised pretraining should be followed by fine-tuning with targeted augmentations specifically designed to account for known distribution shifts between the source and target domains, thereby improving OOD performance.\nMethod: The proposed 'Connect Later' framework entails two phases: first, self-supervised pretraining with generic augmentations to develop robust representations within both source and target domains. Second, a fine-tuning phase employs targeted augmentations, crafted based on the understanding of distribution shifts to better align the domains. This methodology is validated across four real-world datasets, involving different tasks such as astronomical object classification, redshift regression, wildlife species identification, and tumor detection.\nConclusion: Connect Later demonstrated superior performance, achieving state-of-the-art results in multiple benchmarks. The framework notably improved OOD performance by 12% in the AstroClassification task, reduced RMSE in Redshifts by 11%, and provided similar improvements in iWildCam-WILDS and CAMELYON17-WILDS datasets. The study validates that using targeted augmentations post pretraining can significantly bolster OOD error metrics, addressing previous inefficacies in fine-tuning alone.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"This paper looks at an interesting problem: Unsupervised domain adaptation is a very relevant problem (e.g., in many cases practitioners have labelled datasets available from a curated/lab setting, but need to generalize to actual deployment conditions). At the same time, self-supervised learning has become very popular, but its performance in the context of unsupervised domain adaptation (UDA) is still unclear.\\n\\nThis paper introduces a new dataset (RedShifts), and contains some encouraging results on two different domains (camera trips and astronomical observations) while performing several relevant ablations (model scale, whether to train the last layer before fine-tuning, strength of pre-training augmentations).\\n\\nThe paper is relatively easy to read, with good writing and a clear structure.\", \"Weaknesses\": \"Overall, I find this paper lacking in a variety of areas:\\n\\nThe authors' refer several times to Shen et al. (2022). This paper argues that contrastive pre-training is beneficial as long as data augmentations ensure that augmented examples must be more likely to change class or domain than changing both. The authors of this manuscript seem to argue in section 3 that the bad results they observe are explained by a violation of this assumption from Shen et al. (2022). However, this isn't tested rigorously (see section 6 of Shen et al., 2022, for examples on how to evaluate connectivity on real world datasets).\\n\\nA second shortcoming is that different self-supervised methods were used for the iWildCam-WILDS dataset (SWaV contrastive learning) and the astronomical time-series (masked autoencoding). This makes it hard to draw conclusions from table 1, since it is unclear which differences can be attributed to the use of different pre-training objectives.\\n\\nThe results on the iWildCam dataset are not very strong (given the means and standard deviations listed, there's a 20% chance that ERM + targeted augs outperforms Connect Later for any run). This leaves the improvements in the astronomical time-series datasets as the stronger proof of Connect Later's performance. But these results are a bit confusing to me: How is it that standard fine-tuning improves in-domain performance (tables 1 and 2)? This suggests to me that these datasets are possibly very small and benefit strongly from the regularization that pre-training provides. In that case, is Connect Later really addressing a domain shift issue? Or is it just addressing a regular overfitting issue? The latter seems plausible given the very small size of the dataset (6,74 objects). I would argue that in order to draw conclusions, the method should be tested on a wider variety of larger datasets.\\n\\nA baseline that seems to be missing as well is the use of general augmentations during fine-tuning (my understanding is that standard fine-tuning uses no augmentations at all). This would clarify whether it is important to use targeted augmentations during fine-tuning, or whether any form of augmentations would be fine.\\n\\nOverall, I find the insights that this paper provides limited: It is not surprising to me that targeted augmentations would help with fine-tuning, given that they were shown to help with regular ERM. The bigger question to me is the one raised in section 3: Why does pre-training not always lead to increased OOD performance? This question remains largely unanswered. I think this paper would be better if it provided, for example, (1) clear evidence that the assumptions from Shen et al. (2022) are routinely violated in real-world datasets and commonly used augmentations, (2) a thorough evaluation of how different pre-training techniques (MAEs, contrastive learning) perform in the UDA setting, or (3) strong evidence that targeted augmentations are particularly important compared to general augmentations.\", \"Questions\": \"See weaknesses.\"}"}]}
{"paper_id": "KrWuDiW4Qm", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Physics-informed machine learning (PIML) seeks to integrate physical knowledge with machine learning to enhance predictions across diverse fields like climate science and biological sciences. Traditional PIML methods work well for in-distribution tasks but struggle with out-of-distribution (OOD) scenarios where the test data has different parameters or initial conditions from the training data.\nGap: Existing PIML methods perform poorly on OOD tasks because they rely on spurious associations learned by standard machine learning components and lack robustness to changes in initial conditions and ODE parameters.\nHypothesis: The authors propose that by using a meta-learning procedure to discover causal structures, PIML can be enhanced to improve robustness to OOD scenarios.\nMethod: The proposed approach, MetaPhysiCa, builds on meta-learning to improve forecasts across OOD settings. The method involves learning a shared causal structure across different tasks with varying initial conditions and ODE parameters. It combines causal structure discovery via continuous optimization with an invariant risk minimization-type penalty, and applies a two-layer architecture to model the underlying system dynamics. The approach was tested using simulated data for damped pendulum systems, predator-prey models, and epidemic modeling, using tasks with varying initial conditions and parameters.\nConclusion: MetaPhysiCa outperformed state-of-the-art PIML and deep learning methods on three different OOD tasks, achieving 2\u00d7 to 28\u00d7 lower errors. The method successfully identified the true causal structures, showing improved robustness in OOD forecasting settings. However, the approach may require better optimization to avoid learning stiff ODEs when extended to more complex models.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 5, \"Strengths\": \"1. Develop a meta-learning technique for identifying the underlying governing equations of dynamical systems.\\n\\n2. Formulate it as a bi-level optimization problem\\n\\n3. The evaluation results have demonstrated the good performance of the proposed method over baselines.\", \"Weaknesses\": \"1. How many basis functions are used in the experiments? If there is no prior knowledge, the number of basis functions will be very large.\\n\\n2. The proposed method still adopts the SINDy-like approach in which all key terms should be included in a set of basis functions. It may not work on complex equations like $dx/dt = 1/(x_1^2 + sin (x_2^3+x_3))$\\n\\n3. Conduct experiments on more complex systems in the CoDA paper, such as reaction-diffusion system and Navier-Stokes system\\n\\n4. I am curious about the causal structure in Fig. 4. Which two terms in each of the three dynamical systems have causal relationships? Please give an example to explain it. It seems that the authors did not discuss this in the Evaluation section.\\n\\n5. Please conduct experiments on noisy data.\", \"Questions\": \"Please see the comments above.\"}"}]}
{"paper_id": "KNzL1nglNB", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Empirical Risk Minimization (ERM) is a fundamental principle in machine learning that uses class labels as supervision information to guide the learning of labeled samples, achieving great success in many applications. However, it heavily relies on labeled data, which poses a challenge in scenarios with limited or no labeled samples but abundant unlabeled samples. This is common in real-world applications like industrial defect detection, where labeled data can be expensive and time-consuming to create. Existing methods such as entropy minimization (EntMin) and techniques in semi-supervised learning (SSL) and unsupervised domain adaptation (UDA) aim to fully utilize unlabeled data to improve generalization but have limitations in maintaining prediction diversity and avoiding bias towards dominant classes.\nGap: In scenarios where labeled data is insufficient, the reliance of ERM on labeled samples makes it suboptimal. Existing methods like EntMin tend to suffer from decreased prediction diversity, pushing unlabeled samples toward dominant categories. Thus, there's a need for an approach that effectively utilizes unlabeled samples while maintaining both prediction discriminability and diversity.\nHypothesis: The authors propose that a Label-encoding Risk Minimization (LRM) approach, inspired by the phenomenon of neural collapse, can be used to address label insufficient scenarios. By estimating and aligning label encodings for unlabeled samples, LRM can potentially enhance prediction discriminability and diversity, serving as an effective extension to ERM for scenarios with limited labeled data.\nMethod: The paper introduces LRM, which aims to generalize ERM for unlabeled samples by utilizing label encodings as guidance information. The approach involves calculating prediction means (i.e., estimating label encodings) for unlabeled samples and minimizing their discrepancy with ground-truth label encodings. Extensive experiments were conducted in various label insufficient scenarios, including SSL, UDA, and semi-supervised heterogeneous domain adaptation (SHDA). LRM was applied as a plugin to enhance the generalization performance of existing models in these scenarios. The paper also includes theoretical analysis illustrating the relationship between LRM, neural class-mean collapse, and ERM, and compares it against EntMin and other existing methods.\nConclusion: The proposed LRM significantly improves performance in scenarios with insufficient labeling compared to traditional ERM and other existing methods like EntMin and BNM. LRM was effective in maintaining prediction diversity and reducing the dominance of large categories. The theoretical analysis demonstrated a link between label encoding and neural collapse, while empirical evaluation on multiple datasets in SSL, UDA, and SHDA scenarios validated its effectiveness. The results highlight LRM's potential as an effective solution in tackling label insufficient situations, paving the way for its application in other areas with similar challenges.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"- The authors have demonstrated the cause of NCC through the use of label encoding and have further enhanced it to improve network performance. \\n- The evidence supporting NCC and LRM is explicitly presented.\\n- Experimental results show that LRM achieves good performance in various tasks such as semi-supervised learning (SSL), unsupervised domain adaptation (UDA), and semi-supervised heterogeneous domain adaptation (SHDA).\", \"Weaknesses\": \"- Although authous conducted experiments on a variety of tasks, for each task they only ran experiments on specific datasets. For example, for UDA, they only ran experiments on Office-31 and did not evaluate on many other UDA datasets.\\n\\n- Authours also do not have experimental results on large scale data. It is not confirmed that their algorithm is useful when training a model from scratch on large scale training data.\", \"Questions\": \"- Can we say that domain adaptation is a situation where there are not enough labels? A typical domain adaptation is a setting where there are no target labels at all, and the amount of labels in the source data is irrelevant. What does it mean to have insufficient labels in this situation?\\n\\n- Is there a reason why you didn't perform experiments on a large scale dataset such as ImageNet?\"}"}]}
{"paper_id": "uNrFpDPMyo", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Autoregressive language models based on the Transformer architecture have become popular due to their success across various applications, but they face challenges related to computational complexity and GPU memory consumption, especially when using the Key/Value (KV) cache mechanism for generative inference.\nGap: Despite the benefits of the KV cache mechanism in large language models for generative inference, its memory consumption increases significantly with model size and generation length, making it difficult to manage within the GPU memory without resorting to offloading, which can add overhead and affect performance.\nHypothesis: The authors propose that by utilizing the intrinsic structure of attention modules, it is possible to adaptively construct a KV cache to reduce memory consumption without impacting the quality of generative inference, potentially eliminating the need for offloading.\nMethod: The authors introduce FastGen, an adaptive KV cache compression technique that employs lightweight attention profiling to determine the structure of attention heads. Based on this profiling, the method adapts the KV cache by evicting certain contexts or employing different caching policies per head. Experiments were conducted using LLaMA models across various benchmarks to demonstrate the efficacy of this approach in reducing GPU memory usage with minimal impact on generation quality.\nConclusion: FastGen effectively reduces GPU memory consumption by adaptively compressing the KV cache based on attention head structures without the need for expensive retraining or fine-tuning. It achieves significant compression with little impact on generation quality and is a plug-and-play solution that can be integrated into existing models.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 4, \"LLMs\": 1, \"Strengths\": \"- The paper introduces valuable insights drawn from LLMs: 1. Different structure in different attention 2. The same head structures persist. These insights are well-supported with empirical data and references to existing literature. \\n\\n- The authors leverage these insights to come up with an effective compression method that adapts to the structure of each attention head. The results show consistent compression rate and model quality improvement over prior SoTA fixed compression mechanisms.\", \"Weaknesses\": \"- The paper could benefit from presenting actual GPU inference performance results using FastGen and comparing them with other compression methods. Additionally, providing a runtime breakdown would offer more insights into the overhead caused by the profiling, compression, and decompression processes.\\n- It would be nice to look into the structure of KV in the multi-query attention design.\", \"Questions\": \"-\"}"}]}
{"paper_id": "pvhyBB86Bt", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Deep neural networks (DNNs) are powerful tools in machine learning but suffer from training difficulties due to issues like vanishing or exploding gradients, particularly as depth increases. This issue is intrinsically linked to the behavior of the network's Jacobian matrix, which represents the sensitivity of output predictions to input alterations. Prior research has highlighted the importance of initialization schemes for stabilizing the Jacobian, with works such as Pennington et al. (2017) focusing on this aspect at initialization using random matrix theory.\nGap: Existing studies primarily address the stability of the Jacobian at the initialization phase and do not explain its behavior during training or under common scenarios like network pruning, leaving a gap in understanding how Jacobian stability is maintained after initialization and during training.\nHypothesis: The hypothesis of this study is that if a neural network's Jacobian is stable at initialization, it remains stable throughout training. This suggests that proper initialization plays a crucial role in ensuring the stability of the Jacobian throughout the entire training process.\nMethod: The authors extend traditional analyses by utilizing recent results from random matrix theory, particularly those by Brailovskaya and van Handel (2022), to propose a general stability theorem for the Jacobian that covers training. They analyze the behavior of the Jacobian matrix during training using Multi-Layer Perceptrons (MLPs) as a model and investigate its behavior in sparse networks and under non-iid weight initialization.\nConclusion: The study concludes that if a network's Jacobian is stable at initialization, it remains stable through training, which aligns initiation phase with overall training performance. The paper further identifies phases of Jacobian behavior during training, providing evidence for the stability of sparse networks when weights are suitably scaled after pruning.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"The subject explored in this paper is of profound interest. It poses an essential and inherent inquiry regarding the maintenance of stability for the Jacobian matrix in neural networks from initialization throughout training. This is the inaugural study on this particular topic.\\n\\nThe manuscript is well written and has a clear structure.\\n\\nThe paper employs novel methodologies to examine the spectrum of the Jacobian matrix. The findings from Bandeira et al. (2021) are indeed aptly integrated into the analysis of neural networks and warrant recognition within the learning theory community.\", \"Weaknesses\": \"Assumption 2 appears to be quite stringent, as it presumes certain conditions throughout the entire training process. While there is empirical support provided, it lacks a solid theoretical basis, and the given intuitive reasoning doesn't resonate with me. This assumption seems crucial for this study.\\n\\nRegarding Theorem 3, I'm not entirely certain, but my interpretation is that it essentially says: \\\"In the initial stages of training, we are in the kernel regime with a stagnant feature, leading to a static norm of the Jacobian.\\\" Is the challenge in proving this mainly technical? That's how I currently see it. Furthermore, the example in Theorem 4 seems overly simplistic. Perhaps analyzing it using multi-dimensional input in deep linear networks would be more illuminating? Lastly, the assertion that gradient-based training inherently pushes the network towards smaller gradients, causing the Jacobian norm to decrease, doesn't sit right with me. In the end, even with a zero gradient, the Jacobian norm remains non-zero.\", \"Questions\": \"Why haven't you detailed the RMT tools you used in the main content? Although you mention using new tools in the abstract, there isn't sufficient explanation in the main body of the text.\"}"}]}
{"paper_id": "izdFGwDgvW", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Vision-language models such as CLIP, ALIGN, and BLIP have made significant progress in zero-shot image classification by combining contrastive and generative methods, allowing both visual and textual inputs to be projected into a joint latent space for better predictions. Despite their success in zero-shot classification, these models often underperform compared to in-distribution supervised fine-tuning and struggle with out-of-distribution (OOD) generalization.\nGap: Existing vision-language models struggle with OOD generalization in image classification. Although the correct class often appears within the Top-K predicted classes, these models frequently fail to identify the correct class at the top. Moreover, existing methods either require fine-tuning, large datasets, or rely on handcrafted prompts, which can be complex or not feasible in every scenario.\nHypothesis: The paper posits that enforcing consistency between image-conditioned and caption-conditioned predictions can improve the OOD generalization capabilities of vision-language models. By leveraging both image and caption embeddings, it is possible to guide the model towards the correct class within the Top-K predicted classes, thereby enhancing classification accuracy without the need for additional training.\nMethod: The proposed Image-Caption Encoding (ICE) method involves utilizing both image and caption embeddings during evaluation. First, images and class labels are embedded using standard methods. Then, captions are generated using the CoCa model's text decoder, attended by all image encodings. These captions are re-encoded as embeddings. During inference, probabilities for each class are computed separately from the image and caption embeddings with the cosine similarity and softmax operation. A weighted sum of these probability distributions determines the final prediction, where the caption's weight is dynamically adjusted based on the relative confidence of the image and caption probabilities, calculated from their standard deviations.\nConclusion: The ICE method improves zero-shot classification by leveraging the union of image and caption probabilities to make more accurate predictions in an OOD setting. This approach can be easily combined with other zero-shot methods, enhancing Top-1 OOD accuracies by an average of 0.5% and up to 3% on several challenging datasets, without requiring additional training. This showcases the potential of captions to provide complementary information that enhances model predictions.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"- The paper is well written and the organization of the paper is clear.\\n- Experiments are comprehensive with multiple downstream datasets under the zero-shot and few-shot setting.\", \"Weaknesses\": \"- It remains unclear why the text decoder from CoCa is used. It seems that the proposed method only requires a textual description of the input image (any off-the-self high-quality caption models may work). It would be interesting to compare with the performance when using other caption models.\\n\\n- Compared to CLIP, the approach requires an additional caption model (as the image decoder) and is dependent on the quality of the caption, which can be hard to measure. \\n\\n- Empirical improvement in the few-shot setting seems marginal compared to the baselines. It may be arguable whether the additional computational cost (of forwarding passing the image to the text decoder) is desirable.\", \"Questions\": \"- Can authors explain why CoCa is used, instead of an arbitrary caption model? If we replace CoCa with SoTA caption model, will the performance be significantly improved? \\n\\n- The performance gain in the few-shot setting seems marginal (Table 2). Does this mean that in the few-shot adaptation setting, we may just need a few images, instead of using an additional decoder?\"}"}]}
{"paper_id": "AMDKqZcZbi", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper addresses the challenge in machine learning where models struggle to adapt to new tasks quickly while retaining proficiency on previously learned tasks. This issue, known as catastrophic forgetting, is notably absent in biological systems where animals display remarkable abilities to learn and remember through specialized neural architectures such as the entorhinal-hippocampal circuit. The authors draw parallels to these biological systems to propose a solution for rapid, continual learning in machine learning models.\nGap: Existing machine learning models often excel at either quick adaptation (rapid learning) or maintaining knowledge over time (continual learning) but rarely achieve both. This limitation constrains their ability to tackle tasks that require the combination of these abilities, leading to a phenomenon called catastrophic forgetting.\nHypothesis: The hypothesis is that a biologically inspired neural architecture, combining a content-addressable heteroassociative memory system reminiscent of the entorhinal-hippocampal circuit with a spatially invariant convolutional network architecture, can achieve both rapid learning and retention of previously acquired knowledge, thus avoiding catastrophic forgetting.\nMethod: The authors propose a task, the sequential Morris Water Maze (sWM), to evaluate their model's performance. The model comprises a structured neural architecture called MESH, which includes a content-addressable memory system mimicking the hippocampal circuit. Observations are mapped to grid cell patterns, processed by a fixed convolutional neural network (CNN) to create spatially invariant representations, and associated with actions using an attention mechanism. The system is tested for its ability to learn rapidly in new environments and retain knowledge across multiple settings, addressing both intra-environment adaptation and inter-environment retention.\nConclusion: The proposed model outperforms several baseline methods in both continual and few-shot learning contexts. It successfully learns new environments quickly without forgetting past environments, showcasing the potential advantage of incorporating biologically inspired architectures into machine learning models for tasks requiring both rapid learning and memory retention.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"The new benchmark is a valuable contribution to the continual learning community. The method has a strong neuroscientific grounding and it brings together existing components in an original way. The paper is well presented and nicely structured. The writing is clear and the figures are very helpful in conveying the main points of the argument. The ablation study provides sufficient justification for the individual design choices.\", \"Weaknesses\": \"The empirical evaluation is the main weakness of the paper. The authors compare their method to only two continual learning baselines, both of which are quite old. In addition, the replay buffer sizes that are used in the experiments are rather small (200-1800) The proposed method seems to be custom-designed for the navigation task, so while it can serve as a model of how rodents learn to navigate in new environments, it is not a practical continual learning method that could be applied to an arbitrary task. To give existing baselines a fighting chance, I would recommend having two separate networks: one mapping observations and goal position into some latent representation and another mapping these to actions. In the first environment, train both networks. For each new environment, re-train only the first network.\", \"Questions\": \"Why does replay buffer exhibit such poor performance on the last task?\\n\\nWill the benchmark be made available? Is it a framework to produce random environments or just a fixed dataset?\\n\\nAre the associations between displacement representations and actions simply memorised by the attention module?\\n\\nIs the grid code of the goal location available to your method straight away?\\n\\nHow is the goal location provided to the network for replay and naive methods?\\n\\nWhat exactly is stored in the rehearsal buffer? Observation-action pairs?\\n\\nHave you tried increasing the grid size of the environment?\"}"}]}
{"paper_id": "NkYCuGM7E2", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The existing learning-based autonomous driving (AD) systems often struggle with understanding high-level information, generalizing to rare events, and providing interpretability. This is largely due to their reliance on predefined rules and reward functions, which limit their ability to adapt to various scenarios and make decisions that are comprehensible to humans.\nGap: The paper addresses the limitations of current learning-based AD systems, which struggle with comprehending high-level information, adapting to rare driving scenarios, and lacking interpretability in their decision-making processes.\nHypothesis: The authors propose using Large Language Models (LLMs) as the decision-making component in AD systems to mimic human-like reasoning. They hypothesize that LLMs can enhance AD systems by leveraging commonsense reasoning and interpretable decision-making capabilities, improving safety, efficiency, and adaptability in complex driving scenarios.\nMethod: The method involves integrating LLMs to conduct high-level decision-making for AD. The system uses a cognitive pathway framework that involves a chain-of-thought process, allowing LLMs to work through sub-problems systematically. The decisions made by LLMs are translated into actionable commands for low-level controllers using guided parameter matrix adaptation, focusing on Model Predictive Control (MPC) systems. Experiments were conducted to assess the capability of this integrated system in both single and multi-vehicle scenarios, including typical and complex traffic situations.\nConclusion: The paper concludes that employing LLMs as high-level decision-makers in AD systems outperforms traditional methods, especially in single and multi-vehicle tasks. The system improves traffic flow efficiency, decreases safety penalties, and adapts better to complex scenarios, such as navigating intersections or coordinating multiple vehicles. It showcases the potential of LLMs to function as interpretable, effective components in AD, suggesting promising directions for further research.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. Combining LLM and MPC is a smart way to leverage the high-level reasoning capability of LLM. Since LLM is not good at low-level control as shown in previous literature, using MPC to output low-level actions helps incorporate strong prior knowledge into the whole pipeline.\\n\\n2. Rare and long-tail events put great challenges to data-driven algorithms. Using common sense to solve such long-tail problems is a promising way. Since LLM already contains a lot of common sense, this paper is a good attempt to explore this direction.\\n\\n3. The paper is well-written and easy to follow. Figure 2 provides a clear illustration of the entire pipeline, which makes me easy to understand the method proposed in this paper.\", \"Weaknesses\": \"1. A common question for LLM-based agents is the trustworthiness. When used for safety-critical applications like autonomous driving, the requirements for safety are much higher than QA or code generation tasks. As far as I know, GPT3.5 and GPT4 have uncontrollable randomness even with temperature = 0. Does this randomness influence the running results? How to ensure that LLM always outputs reasonable results. Are the results consistent if the version of the ChatGPT model changes?\\n\\n2. How about the running frequency of the entire system? Since driving requires a high frequency to make decisions, the speed of processing the information with language may not be fast enough. This is one of the biggest challenges of using LLM for driving, which may be unaffordable and unsolvable with a general-purpose LLM. Using LLM which can only communicate with downstream modules with language wastes a lot of time on extracting information.\\n\\n3. In the first paragraph of the introduction, the authors provide an example \u201cHuman drivers intuitively know that according to traffic rules, they should slow down and yield, even if it is technically possible to speed through\u201d. This example is confusing to me as it just shows two types of common driving styles of human drivers (aggressive and conservative styles). I am not aware of any complex rules or reward functions here since it is easy to force the driving algorithm to be conservative with some simple rules. Actually, considering the complexity of the pipeline designed in Figure 2, I am not sure which one is more complex. Maybe several simple cost functions in an MPC can already solve the scenarios in the experiment part.\\n\\n4. It seems that one important motivation for using LLM for driving is solving rare and long-tail events. However, I only find Figure 1 discusses one specific case where a breakdown vehicle is stopped in the middle of the road in a roundabout. I can\u2019t find any evidence or experiment results showing that LLM broadly solves long-tail problems. \\n\\n5. In the evaluation part, only one RL algorithm (not mentioned which algorithm exactly) and one MPC are compared. Since modern AV algorithms are usually complex, I don\u2019t think such a comparison is not convincing enough to demonstrate the advantage of the LLM method.\", \"Questions\": \"1.\\tCould the authors report the running time of all methods in the experiments? \\n2.\\tThere are 5 scenarios mentioned in Table 1 but no detailed explanation of them. I cannot evaluate the realism and difficulty of them.\\n3.\\tCould the authors explain more about the interpretability of the LLM method? Does this interpretability mean the reason for decision-making? Why is such interpretability useful and can we always trust such interpretability? If the interpretability is something shown in Figure 1, I think all collision avoidance algorithms can provide the reason for the decision.\"}"}]}
{"paper_id": "tth2qXY7RU", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Deep Neural Networks (DNNs) have achieved state-of-the-art performance across a variety of domains, leading to increased model sizes and computational demands. Traditional quantization approaches, both integer and floating-point, have been employed to address these challenges by reducing model sizes and improving computational efficiency. However, these techniques often face trade-offs between accuracy and computational benefits. Emerging research focuses on custom data types and hardware designs, highlighting piecewise and block floating-point quantization as promising methods, though they still struggle with handling data outliers effectively.\nGap: Despite improvements, existing quantization methods, including piecewise and block floating-point techniques, still face challenges in balancing the precision needed to represent diverse data distributions efficiently, particularly in handling data outliers within a limited bit budget.\nHypothesis: The authors propose the Super Floating-Point (SuFP) as a new data type and quantization method that employs multi-region piecewise quantization and a tensor-wise scalable bias. This approach aims to improve memory footprint and logic efficiency without compromising accuracy, by flexibly adapting to varying data distributions and efficiently representing both dense near-zero data and outliers.\nMethod: The SuFP method splits data distribution into multiple regions, allowing each region to have optimized precision through a variable encoding field. This supports both dense data regions near zero and sparse regions with outliers. The scalable bias is applied at the tensor level and requires only a minor addition operation, enhancing adaptability to diverse data distributions. SuFP is implemented with integer arithmetic units and shifters, creating a compact hardware design. The approach was tested across various domains, including vision, language, and generative models, and evaluated against traditional FP32, FP16, and other quantization methods using metrics such as accuracy, memory footprint reduction, throughput per area, and energy efficiency.\nConclusion: SuFP successfully maintains or surpasses the accuracy of full-precision floating-point (FP32) with significant improvements in computational capability and energy efficiency\u20149.00\u00d7 and 17.04\u00d7 respectively over FP32 and better than MSFP and BSFP. The SuFP approach, by combining multi-region quantization with scalable bias and efficient hardware implementation, delivers an effective solution for quantization challenges, offering significant benefits in both accuracy and hardware efficiency.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"+ The paper is well-written and easy to follow. Illustrative figures are well plotted and easy to understand.\\n+ Co-designing the hardware MAC architecture for the proposed data type SuFP enables better hardware efficiency.\\n+ The evaluation benchmarks are diverse, including both vision and language tasks.\", \"Weaknesses\": \"+ The ablation study lacks the improvement breakdown on piecewise data representation and tensor-wise scalable bias.\\n+ The accuracy evaluation experiments lack results on MSFP and BSFP for larger models (Table 3). The efficacy of the proposed SuFP on Large Language Models such as LLaMa2 is also unclear.\\n+ The proposed SuFP saves 6% memory, with 1.05x throughput improvement and 1.03x energy savings over MSFP (MX9). The improvement is marginal.\\n+ MSFP have multiple versions: MX4, MX6 and MX9. It is unclear how SuFP narrows its bit width.\", \"Questions\": \"Please answer the questions in the weakness section.\"}"}]}
{"paper_id": "jdynlBj3b0", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Multi-label learning (MLL) involves associating an instance with multiple labels and is used in applications like image annotation and text classification. Single-positive multi-label learning (SPMLL) addresses the challenge of high annotation costs by training models using only one positive label per instance. Existing SPMLL methods rely on pseudo-labels assuming identical class-priors across categories.\nGap: The assumption of identical class-priors in SPMLL methods is unrealistic for real-world applications where class-priors vary, leading to biased pseudo-labels and suboptimal model performance.\nHypothesis: The paper proposes that incorporating class-priors into SPMLL can improve model performance by providing a more realistic framework for training, thereby addressing the bias introduced by the assumption of identical class-priors.\nMethod: The authors introduce CRISP (Class-pRiors Induced Single-Positive multi-label learning), which estimates class-priors using a class-priors estimator. This estimator uses a threshold-based approach to converge to ground-truth class-priors. An unbiased risk estimator derived from the estimated class-priors approximates the optimal risk minimizer on fully supervised data. Ten benchmark datasets were used to validate the method, showing its superior performance to existing approaches.\nConclusion: The CRISP framework effectively integrates class-prior estimation into SPMLL, leading to more accurate models by removing unrealistic assumptions about class-priors. The method showed significant performance improvements across multiple datasets, confirming the importance of considering class-priors in SPMLL.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 3, \"Contribution\": 4, \"Rating\": 8, \"Confidence\": 3, \"Strengths\": \"1. Strong empirical performance\\n2. The proposed method is theoretically principled with a convergence guarantee\\n3. The unbiased risk estimator is simple and intuitive\", \"Weaknesses\": \"1. Clarity of writing. I found the problem setting to be unclear until I finished section 4.  One suggestion would be to add more details about the setup in the preliminary setting e.g. the absolute loss function, before diving into deriving the estimators. Also, changing the order by deriving the risk estimator before the class-priors, may provide a better motivation on why we need to estimate the class-priors.\", \"Questions\": \"1.  The algorithm relies on iteratively estimating the class-prior from f and then using it to update f. Is it possible if there is a failure mode ?\\n2.  Would it be possible to extend this type of estimator to a different loss than the absolute loss?\"}"}]}
{"paper_id": "xelrLobW0n", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large Language Models (LLMs) have significantly advanced natural language processing by combining extensive pretraining with fine-tuning on human-generated data. Despite these advancements, the continual learning capabilities of LLMs remain underexplored, particularly in retaining performance across sequential tasks without catastrophic forgetting.\nGap: Existing continual learning benchmarks are inadequate for assessing state-of-the-art LLMs, mainly because they include simplistic tasks and fail to consider crucial elements like generalization, instruction adherence, and safety of LLMs.\nHypothesis: The paper proposes that TRACE, a novel benchmark with diverse and challenging tasks, combined with the Reasoning-augmented Continual Learning (RCL) approach, can better evaluate and enhance the continual learning capabilities of LLMs while mitigating catastrophic forgetting.\nMethod: TRACE benchmark consists of eight datasets covering domain-specific tasks, multilingual capabilities, code generation, and mathematical reasoning. It also introduces new evaluation metrics: 'General Ability Delta,' 'Instruction Following Delta,' and 'Safety Delta.' The study uses six aligned LLMs in various experimental settings, including sequential fine-tuning techniques (like LoRA and Replay) and evaluates their performance using these metrics. The RCL method augments training by integrating reasoning paths, leveraging GPT-4 for reasoning annotation, followed by supervised training with these enhanced datasets.\nConclusion: Experiments demonstrate that traditional fine-tuning leads to significant declines in LLMs' general abilities and instruction-following capabilities. The RCL approach shows promise in retaining reasoning capabilities with less catastrophic forgetting, suggesting that reasoning-augmented learning strategies could balance the need for task-specific performance and the preservation of LLMs' inherent abilities.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"1. The authors have considered somewhat large language models for the evaluation. It would be very grateful if the authors could compare it on a bigger scale, e.g., llama2 70b-chat, falcon-40b instruct.\", \"Weaknesses\": \"1. While I do agree that some papers, e.g., instruction tuning, have used the past continual learning benchmarks, TRACE also shares the same issue. After publishing this benchmark, there are possibilities that some may use this dataset for pre-training. So, I think continual learning (CL) benchmarks all shares a similar problem, so this is hard to claim as a problem of prior CL benchmark (since this paper also have the same issue). \\n\\n2. For me, it is hard to understand i) why this benchmark has novelty for CL and 2) why this has novelty for alignment LLMs\\n- i) TRACE is a mix of existing challenging benchmarks and put them in a sequential manner. It is hard to see novelties or specialization for continual learning (except they have put it in a sequential manner). \\n- ii) The only thing this benchmark is specialized to alignment LLMs is that they propose a metric that is related to alignment LLMs. But these metrics are also a naive extension of prior continual learning metrics (the overall performance and backward transfer suggested by [1]). \\n\\n3. One important discussion in continual learning is about forward transfer [1,2,3]. Considering forward transfer metric is also needed.\\n\\n4. As a benchmark paper, I think it is very important to compare the prior CL methods, where this paper only shows a few baselines. For instance, considering the following methods will be helpful (see some baselines in [4]) or consider [5,6,7].\\n\\n5. The overall writing should be improved. For instance, **the main text cites the results in the Appendix too much** (Figure 5, Table 6). If the table and figures are important, the authors should put them in the main text. Especially, the results of reasoning-augmented continual learning are important but missing in the main text.\\n\\n6. Using GPT-4 is not a good choice for continual learning (even for reasoning), since we don't know which dataset they have used to train the model.\\n\\nOverall, it is somewhat hard to find a novelty as a continual learning benchmark (and typically specialized for aligned LLMs), and it is hard to find novel metrics. Furthermore, the paper needs more consideration regarding forward transfer, more comparison of multiple frameworks, and improving the overall writing. \\n\\nPlease find the reference below\\\\\\n[1] Lopez-Paz and Ranzato, Gradient episodic memory for continual learning, NeurIPS 2017\\\\\\n[2] Wolczyk et al., Continual World: A Robotic Benchmark For Continual Reinforcement Learning, NeurIPS 2021\\\\\\n[3] Chen et al., Is Forgetting Less a Good Inductive Bias for Forward Transfer? ICLR 2023\\\\\\n[4] Jang et al., Towards Continual Knowledge Learning of Language Models, ICLR 2022\\\\\\n[5] D'Autume et al., Episodic memory in lifelong language learning, NeurIPS 2019\\\\\\n[6] Huang et al., Continual learning for text classification with information disentanglement based regularization, NACCL 2021\\\\\\n[7] Razdaibiedina et al., Progressive prompts: Continual learning for language models, ICLR 2023\", \"Questions\": \"I think the result of Vicuna-13B-V1.5 in Table 2 should be 0.\"}"}]}
{"paper_id": "r1IbewSnqq", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Federated Learning (FL) allows parties to collaboratively train a global model while keeping their local data private. Traditionally, the aggregation of local models in FL resembles a public decision-making process similar to elections, often using a simple weighted averaging method known as FedAvg. However, this aggregation method follows the one person one vote (1p1v) principle, making it vulnerable to poisoning attacks where malicious parties can influence the global model by adjusting their local model updates based on the amount of data they have shared.\nGap: The primary issue with FedAvg and similar methods is their susceptibility to poisoning attacks due to their reliance on the 1p1v principle, which allocates influence based on the size of data used in training rather than the reliability or trustworthiness of the participating parties. This can give undue influence to malicious parties, potentially skewing the global model.\nHypothesis: The authors propose that using a quadratic voting (QV) system in federated learning, termed FedQV, will mitigate the vulnerability to poisoning attacks by allowing parties to allocate more influence to updates they view as more valuable, with influence growing with the square root of updates instead of linearly as in 1p1v. The authors hypothesize that this approach not only discourages malicious behavior by limiting the impact of large datasets from malicious parties but also improves the overall robustness of model aggregation.\nMethod: The method involves adopting a quadratic voting mechanism where each party is allocated a voting budget. During each aggregation round, parties can spend a portion of their budget to influence the global model according to their preferences. The spending is designed such that the impact of votes is based on the square root of the number of credit votes spent. A truthfulness mechanism is added, using masked voting rules and budget limitations to prevent parties from predicting the impact of their votes, discouraging deceitful strategies. Unequal voting budgets are also used, where benign parties are allocated more influence based on a reputation score to further reduce the risk of poisoning attacks. The performance of the proposed FedQV method is tested against well-known real-world datasets and various state-of-the-art poisoning attacks.\nConclusion: The study concludes that FedQV offers improved robustness against poisoning attacks compared to traditional methods like FedAvg. The experiments demonstrate that FedQV can effectively limit the damage caused by malicious updates without sacrificing performance and can be seamlessly integrated with existing Byzantine-robust and privacy-preserving mechanisms to enhance its defense against both poisoning and privacy attacks. The proposed method is shown to consistently outperform FedAvg, especially under local model poisoning attacks, and the employment of reputation-based voting budgets further increases its effectiveness.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"The paper proposes novel ideas by taking techniques from different domains.\\n\\nI think that the inclusion of incentive-based approaches to deal with corrupted parties is currently less explored that other techniques and can potentially provide new means to understand which attacks are realistic. Therefore, this is a valuable aspect of the contribution. \\n\\nOther ideas such as the integration of robust voting techniques and the use client-side measures are original and constructive in the discussion of byzantine resilience. In particular, techniques that are compatible with privacy enhancing technologies such as secure aggregation are valuable, especially given that an important use case of federated learning is privacy preserving machine learning.\", \"Weaknesses\": \"However, the contribution presents major problems which are listed below: \\n\\n1- The threat model of the paper is not clear. A well defined threat model is crucial when studying defenses against active adversaries. In the current contribution, I do not see what are the adversaries' possible behaviors and what are they willing to risk to harm the model (e.g. are they able to collude?). Therefore, it is not clear in practice in which scenarios would the truthfulness and convergence properties hold. This makes hard to evaluate the real impact of the contribution. \\n\\nFor example, consider an adversary which controls many parties and each party sends a harmful update in a different round. The adversary is willing to sacrifice parties to harm the model. Would this affect the truthfulness property? \\n\\n2- Clarity. The contribution is a combination of many ideas taken from different domains. However, the inclusion of each ingredient is not properly justified:\\n\\n2a- The paper introduces quadratic voting (QV) as a defense to reduce the impact of malicious users that falsely claim to have large datasets. However, the paper does not show how this kind of attack can affect other current Federated Learning defenses in practice and how QV is an effective defense. Moreover, the paper does not evaluate the resilience against any attack in which the adversary falsely claims to have a large dataset. Therefore, I am not sure if the initial motivation of QV is present in practice. \\n\\n2b- The paper introduces adaptive budgets as a reputation system. The original FedQV (Algorithm 1) already seems to penalize contributions. Therefore, the reputation protocol (Algorithm 2) appears a bit redundant. In the following sections of the paper, it is not clear when FedQV-Alg1 and FedQV-Alg2 are applied. \\n\\n2c- Compatibility with privacy enhancing primitives. In Section 3.2, the paper claims that the compatibility with privacy enhancing primitives (Secure Aggregation, Fully Homomorphic Encryption, Differential Privacy) is one of its main benefits of FedQV. However, I do not see a clear compatibility. For example, in Secure Aggregation the aggregation is not done by the server. Therefore, it not clear (i) how can the server adjust the weights of the aggregation properly and (ii) how can clients share similarity scores and local dataset size with the server without breaking the privacy guarantees. \\n\\n2d- Meaning of the truthfulness property: Section 4.2 claims that any possible corrupted contribution diminishes the probability of harm as server penalties reduce the influence of corrupted parties. This gives the impression that being honest is better for local utility than performing an attack. However, this is not what we can see empirically in section 5.  \\n\\n3- Evaluation of FedQV:  the proposed protocol claims to be easy to integrate with other resilience measures because similarities are computed in the client side. As said, this is an interesting feature, but is particularly vulnerable to client-side manipulations of the similarity scores and dataset sizes. It is true that protocol is evaluated against QV-adaptive, an attack that exploits this vulnerability. However, delegating the control of the defenses to possibly malicious clients is a high risk to take. Therefore, its evaluation of this risk requires a more comprehensive treatment. \\n\\nFor example, an important aspect of the evaluation which I feel is missed is (as said in point 2a) the resilience against an attack in which the adversary falsely claims to have a large dataset.\", \"Questions\": \"Please elaborate on the weaknesses outlined above.\"}"}]}
{"paper_id": "i4kDKfllrz", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Deep learners often perform well under the closed set assumption, where classes are predefined during training. However, they struggle in open set conditions, where models must recognize both known trained classes and potentially infinite unknown classes. Open Set Recognition (OSR) is developed to address this challenge, allowing models to identify whether an input comes from the known classes or an unknown class from the broader set.\nGap: Existing open set recognition methods use a single function for distinguishing between known and unknown classes as well as for classifying within known classes. This dual-purpose approach leads to suboptimal performance because the function is not specialized for either task.\nHypothesis: The authors propose that embedding spaces should distinctly separate known and unknown classes, with specialized functions for each task: known/unknown discrimination and fine known class distinction. By using distinct functions, each can be optimized for their specific task, leading to improved performance.\nMethod: The authors introduce Synergistic Classification and unknown Discrimination (SCAD). SCAD deploys two separate networks: one for embedding data into spaces where known and unknown classes are distinct, and another for classifying among known classes. The embedding network uses a triplet loss to enforce separation, using a representative set of 'known unknowns.' A prototype-based distance measure helps decide if a sample is from a known or unknown class. A classifier then provides specific labels for known samples. Performance is measured using AUROC and correct classification rate across several datasets.\nConclusion: SCAD outperforms existing methods in open set recognition by using distinct functions for known/unknown discrimination and known class classification. It effectively separates knowns and unknowns in the embedding space and shows superior performance in both AUROC and classification accuracy, handling both known/unknown distinction and classification tasks more efficiently.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"The task of open set recognition is of interest to the field.\", \"Weaknesses\": \"1. The presentation of the paper can be confusing sometime. E.g. in Fig 2., please explain in the caption the 'know knowns' and 'known unknowns', which are very confusing. This makes understand the core idea inaccessible.\\n2. Large-scale experiments like original Imagenet and iNaturalist is needed. Current experiments are too small-scale to verify the proposal.\\n3. some ablation studies are missing. E.g. \\\\beta in the Eqn. 5.\", \"Questions\": \"1. Fig.1b hypothesize that the embedding space should be distinctly separable between the known classes and unknown classes. However, in Fig 4, I did not see distinc separation between the two. I also do not think the hypothesis holds in general. Can authors expalin.\"}"}]}
{"paper_id": "1P92J25hdf", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the integration of inductive bias (IB) in neural networks, which enhances modeling capabilities by incorporating preferences that improve generalization ability. In the context of computer vision, general IB is associated with CNNs and Transformers, leveraging scale invariance and long-range dependencies. Stereo matching, a geometric vision task, typically does not effectively utilize general IB due to volume-level scale invariance issues and the need for real-time processing.\nGap: Stereo matching often neglects volume-level scale invariance and struggles with capturing long-range dependencies without utilizing time-consuming Transformers, which limits its real-time applicability. Additionally, the potential of specific IBs associated with volume structures in stereo matching is underexplored.\nHypothesis: The authors hypothesize that introducing general IB into stereo matching can be achieved through a novel model, UStereo, which incorporates inter-layer fusion and specific volume structures, addressing both scale invariance and long-range dependencies without sacrificing real-time performance.\nMethod: The authors propose UStereo, a model that applies inter-layer fusion to address scale invariance issues and a strategy to capture long-range dependencies through shallow convolutional stacks without relying on Transformers. The model uses a Dense Scale-Aware Fusion (DSF) to connect volumes across resolutions and introduces a Mixed Direct Long-Range Compensation (MDLC) block to enhance long-range dependency capturing. Additionally, the paper presents a study on the roles of specific IB in volume structures through deep supervision techniques and original statistical indicators to evaluate volume distributions during inference.\nConclusion: The experiments demonstrate that UStereo achieves competitive performance with improved speed and robustness compared to existing volume-based methods. The model efficiently incorporates general IB into stereo matching. The study of specific IB reveals that volumes used in stereo matching can indeed function as confidence volumes and that a concentrated disparity distribution within volumes enhances model performance.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. This paper builds a new stereo matching network UStereo that can achieve real-time inference and great accuracy.\\n2. They propose to think about the specific IB in cost volume representation, which is interesting.\", \"Weaknesses\": \"1. The writing needs improving. Introducing the IB concept makes it hard to understand what the authors claim. I wonder why it needs to introduce IB instead of simply exploring scale invariance and long-range dependencies.\\n2. The results of the KITTI benchmarks are not impressive. Among real-time methods, we find CGI-Stereo, HITNet, and Fast-ACVNet+ get better performances.\\n3. There is a lack of evaluating the generalization ability in common settings. This paper claims UStereo has robust generalization, however, the common setting for generalization evaluation is on the KITTI, Middlebury, and ETH3D datasets.\", \"Questions\": \"1.  I still wonder what is the specific inductive bias in the volume representation. I think using the cost volume as confidence representation has been explored, especially in the are of Muli-view Stereo. So I want to know if there is any other specific IB proposed in this paper.\"}"}]}
{"paper_id": "6hzNVNSz8O", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Line search methods have been shown to improve the performance of stochastic gradient descent (SGD) by making the learning rate superfluous. Recent research has suggested that line search approaches can effectively overcome the need for hyperparameter tuning in learning rates by evaluating the loss function at multiple gradient points, thereby finding an adaptive learning rate.\nGap: Current line search methods such as the Stochastic Line Search (SLS) have problems with mini-batch noise and are inefficient for larger scale datasets and more complex data domains. There are also stability issues with standard implementations of line search, such as those seen with large step size variations over different runs.\nHypothesis: The paper proposes that an enhancement to Armijo line search, called SaLSa, which incorporates exponential smoothing (momentum) and reduces the line search frequency, can provide a more stable and effective line search method that is better suited for stochastic mini-batching and larger datasets.\nMethod: The SaLSa method enhances Armijo's line search by incorporating an exponential smoothing factor to account for mini-batch noise and reduce the impact of an erratic step size change. The method also limits the frequency of line search computation to improve efficiency. These modifications are implemented as a PyTorch optimizer and tested against SGD and Adam optimizers across a variety of datasets and architectures, including Transformers, CNNs, and MLPs. Major datasets used include ImageNet and the Glue dataset collection for NLP tasks.\nConclusion: SaLSa achieves better performance in terms of lower loss values and higher average accuracy compared to prior methods and tuned optimizers. The results demonstrate SaLSa's robustness and efficiency without requiring extensive hyperparameter tuning. Its implementation as a PyTorch optimizer makes it accessible and practical for a broad range of machine learning applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"The paper is clearly written. The motivation and theoretical analysis are clear. The thoroughness of the experimental setup is excellent. The contribution is very significant and I think likely to be very useful to machine learning practitioners in general.\", \"Weaknesses\": \"While the presented results are very compelling, it would be even stronger to show experiments on Transformer models in a pretaining setting as well. Additionally, it would be beneficial to compare different sizes of Transformer (and CNN) to show the persistence of the benefit of the method across the scaling of the model. These experiments may plausibly be left as future work (the presented ablations are more important and very thorough) but they represent practical questions that ML practitioners will have, so would strengthen the current work if included.\", \"Questions\": \"Nits:\\nFigs 4 and 5: it is a little difficult to read these plots, as you have to go back and forth visually between the plot and the legend to see which curve is which. Consider making Salsa lines dashed / SLS lines dotted / baselines solid (or something of that nature) to help improve readability.\\nFig 5: loss label missing at top of (a), accuracy label missing at top of (d)\\nFig 1: fix occluded label text for y axis\"}"}]}
{"paper_id": "QO3yH7X8JJ", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Diffusion-based Generative Models (DGMs) have recently shown great promise in enhancing super-resolution (SR) tasks by synthesizing high-quality visual content. Traditional SR methods are often constrained by a fixed, integer-based scaling architecture, and recent solutions have typically involved training architecture-specific DGMs from scratch, or performing extensive fine-tuning and distillation on pre-trained models, which are resource-intensive.\nGap: Existing methods for super-resolution tasks are often tied to specific integer scaling factors and rely heavily on architecture-specific models that demand significant training and hardware resources. These methods fail to address the need for arbitrary-scale super-resolution (ASSR), which can efficiently adapt to diverse upsampling scales without preparing different models for each scale.\nHypothesis: The authors propose that the super-resolution capabilities of existing pre-trained diffusion generative models can be effectively harnessed without additional fine-tuning or distillation by implementing a simple but effective noise injection methodology.\nMethod: The proposed Diff-SR method leverages pre-trained DGMs by injecting an optimal level of noise into low-resolution images before applying the DGM's backward diffusion process. This approach is built on the concept of the Perceptual Recoverable Field (PRF), which quantifies the appropriate amount of noise required to achieve a balance between maintaining low-level fidelity and reducing high-level distortion. Extensive experiments demonstrate the effectiveness of this approach across multiple datasets, using metrics such as FID, PSNR, and SSIM for evaluation.\nConclusion: Diff-SR achieves superior performance over existing methods for arbitrary-scale super-resolution tasks, showcasing significant improvements in image quality across various scales, as evidenced by lower FID scores and competitive PSNR and SSIM metrics. The method is validated through extensive experiments highlighting its flexibility and adaptability to different upsampling scenarios without retraining specific models.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 5, \"Strengths\": \"1. This paper presented a new perspective that extends a pre-trained diffusion model into an arbitrary-scale super-resolution by adding a specific amount of noise to LR images before the diffusion model backward diffusion process. The idea is interesting.\\n2. This paper proposed a metric Perceptual Recoverable Field for achieving the optimal trade-off between fidelity and realness.\", \"Weaknesses\": \"Please refer to the Questions.\", \"Questions\": \"1. Since the proposed method is for arbitrary-scale super-resolution, it is important to provide the quantity results comparison of arbitrary-scale such as 1.6X, 2.7X, 3.5X, and 4.5X. Also, the visual comparison of different scales comparison of the proposed method on the same image is also important to evaluate this work.\\n2. It mentioned that the used pre-trained diffusion model is 256*256. How does this model achieve arbitrary-scale super-resolution? For example, the low-resolution image is sized 100*100, how can we achieve 3.5X super-resolution?\\n3. Please give a reference for the value of noise level in an interval of 0.1.\\n4. Can the proposed method extend in real-world arbitrary-scale super-resolution? \\n5. Adding different noise levels when dealing with different scales, the higher noise means lower fidelity. When facing larger scale super-resolution, that means very low fidelity. Is there a big visual change between X2 and X4 results?\\n6. How does the proposed method maintain the trade-off between fidelity and realness? \\n7. In Line 22 of Page 2, this paper mentions high-level signature of generated content. What does it mean? \\n8. In Line 3 of Page 5, should the low-resolution image be high-resolution image? Otherwise, it is hard to understand this sentence.\"}"}]}
{"paper_id": "bQWE2UqXmf", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large language models (LLMs) have advanced significantly in text generation, enabling tasks like question answering and code synthesis. However, this proliferation poses risks such as cybersecurity threats, misinformation, and academic dishonesty. Detecting AI-generated text is crucial to mitigate these issues. Traditional detection methods, using token log probabilities and other high-dimensional features, struggle with newer LLMs and are incompatible with black-box LLMs, which only provide word output via APIs.\nGap: Existing models for detecting AI-generated content often rely on internal LLM features like token log probabilities, which are not accessible in black-box models such as GPT-3.5 and GPT-4. Furthermore, these methods use high-dimensional features prone to overfitting and are not robust to novel content or adversarial attempts to bypass detection.\nHypothesis: The hypothesis is that LLMs consider machine-generated text as high quality, leading to fewer edits during a rewrite task than with human-written text. Consequently, detection of machine-generated text can be achieved by analyzing the rewriting behavior of LLMs, specifically by measuring the editing distance between the original and rewritten texts.\nMethod: The method, Raidar, detects AI-generated content by prompting LLMs to rewrite input text and calculating the editing distance of the output without relying on high-dimensional features. The editing changes are measured using bag-of-words edits and Levenshtein distances. Raidar operates on the output from any LLM, making it compatible with black-box models. It was empirically tested on datasets across varied domains, such as news articles, student essays, code, and Yelp reviews. The approach analyzes invariance, equivariance, and output uncertainty of text to discern AI-generated text from human-written text.\nConclusion: Raidar significantly enhances detection capabilities for AI-generated text over existing methods by leveraging LLMs' rewriting behavior. It improves F1 detection scores across multiple domains, demonstrating effectiveness even on novel content and resistant to adversarial attempts. The reliance on simple symbolic text properties and editing distances ensures robustness and compatibility with the latest LLMs.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"The paper discusses work on an interesting framing of detecting text generated by large language models by gauging how much rewrites (edit distance) it gives to a piece of text with the hypothesis that it considers machine-generated texts, regardless of specific models, as high quality ones compared to human-generated texts. The simplicity of the method is what I appreciate the most with the paper. I also appreciate that the author/s are able to construct a good research plan for the methodology of the study and the formalization of the task. There are some revisions suggested below to improve the structure and organization of the paper, particularly the uniformity and presentation of results.\", \"Weaknesses\": \"The paper starts well with the introduction, statement of motivation, discussion of hypothesis, and proposed method but went downhill with the presentation of results.\\n\\nFirst, in the discussion of datasets and model baseline used, the definition and characteristics should definitely be expanded by including the size of the dataset plus other unique characteristics of the data that\u2019s worth mentioning as well as the quantified performance of the baseline models just to give an idea to the readers of their relevance and why they are being used/compared in the study. \\n\\nSecond, the discussion on Section 4 seems to be too condensed and there are sudden insertions of models for comparison. Claude was tested but was not mentioned in the baseline models. Why is that and why is Claude a viable candidate for comparison? \\n\\nThird, there is no concrete / definitive discussion for the results presented in Table 1 and 2 and these are the bread and butter of the study. The table captions and Section 4.3 simply state the observed increments against the previous systems and nothing more. I would have appreciated it if there are more discussions on particular weaknesses in the texts by previous works which the current work addresses, and give some examples of this. Why are proposed methods get big performance jumps, say in the code dataset and Arxiv dataset, compared to natural language datasets? In the robustness experiments, why are the previous detector models left out? It is important to still consider them in the mix for better comparison of true performance. These discussions should be in order.\\n\\nThe statement \u201ctext from the Ada model is most easily detected, possibly due to its discernibly lower quality compared to human-written content\u201d is speculation. Some form of quantitative proof is required for this.\\n\\nIn terms of strengthening the paper, I strongly suggest the author/s to also experiment with second-language or non-native speaker writing corpora such as TOEFL or ESL essay corpus to gauge if the proposed method also exhibits that particular limitation of confusing AI-generated texts with non-native speaker-written texts. This has been a known limitation across all detectors of machine-generated texts. Refer to the existing works (https://arxiv.org/abs/2304.02819) and databases online (https://www.clarin.eu/resource-families/L2-corpora, https://uclouvain.be/en/research-institutes/ilc/cecl/learner-corpora-around-the-world.html) for the data to be experimented with. Regardless of the result, this would be a great addition and insight to the paper.\\n\\nLastly, the paper is littered with inconsistencies and wrong format for in-text citations which makes it very confusing to read. For example, it\u2019s better to read referenced text as \u201cGhostbuster (Verma et al., 2023) instead of Ghostbuster Verman et al (2023)\u201d. It seems like the Verma is included in the name of the model but it\u2019s not. There are also cases where the model is \u201cGPT-3.5-Trubo\u201d and then becomes \u201cGPT-3.5-turbo\u201d. Diagrams in Figure 6 should conform to uniform range of [0.0,1.0] in the y axis.\\n\\nI still see the importance and contribution of the study but major revision is required for the paper in order for readers to fully comprehend and appreciate the work.\", \"Questions\": \"1. How large is the feature set or dimension then for training the classifiers? Did the author/s conduct and hyperparameter optimization with the classifiers?\\n\\n2. How is the setup for comparison done for preliminary experiments shown in Table 1 and 2? What classifier was used for these experiments? There were two algorithms mentioned (LR and XGBoost) but it was never specified in the tables. The same way goes for Tables 4 and 5. I\u2019m confused as a reader as to which classifier was used for the presentation of results. Revising this part and enriching the details should be in order.\\n\\n3. Is the number of changes (deletions, insertions) done by the selective generative models between humans and machine-generated texts statistically significant? The visualization in Figure is nice but this would have been an additional supporting preliminary information to add to the hypothesis.\\n\\n4. There are six datasets chosen for experimentations but why are only subsets or some of it are used for results presented in Table 3, 4, and 5?\\n\\n5. The statement in Section 4.4 goes \u201cIn Table 5, while all models achieve significant detection performance\u201d. So a statistical test was conducted? Can you give the details for this? Otherwise, I suggest using the word substantial instead.\\n\\n6. Why is only the Yelp data used for special test cases such as effect of content length? Why not the other datasets as well? This seems very selective without reason.\"}"}]}
{"paper_id": "KKZaj2QS3G", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Time series analysis is crucial for many practical applications, such as finance, medicine, and engineering. Labeling time series data is often costly and complex due to intricate patterns and privacy concerns. Unsupervised learning, particularly contrastive self-supervised learning, offers a promising avenue for developing informative representations without labeled data.\nGap: The existing research on time series representation learning has not explicitly addressed the impact of noise, which is a significant factor affecting task accuracy. Moreover, there is a lack of efficient and lightweight encoder architectures that can capture robust and informative representations for various downstream tasks.\nHypothesis: The authors propose that employing a noise-resilient sampling strategy, together with a scalable and robust encoder design, will enable effective learning of time series representations that are consistent and informative even in the presence of noise.\nMethod: The proposed method, CoInception, integrates a noise-resilient sampling strategy that utilizes a spectrum-based low-pass filter to create perturbed series for robust representation learning. This strategy preserves essential time series characteristics while filtering out noise. The method uses an encoder architecture combining Dilated Convolution with Inception blocks to effectively capture wide receptive fields while maintaining model scalability and robustness. The learning is guided by a hierarchical triplet loss to enforce noise resilience and ensure alignment and uniformity in the embeddings for tasks like forecasting, classification, and anomaly detection.\nConclusion: The CoInception framework consistently outperforms existing state-of-the-art methods across multiple benchmarks for forecasting, classification, and anomaly detection in time series data. It achieves this while being more parameter-efficient, highlighting its scalability and robustness in handling noise-afflicted time series tasks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"The paper certainly represents a significant amount of work by the authors, especially in the experimental section. The related work section is also extensive.\", \"Weaknesses\": \"The paper does not do a good job positioning itself in the vast literature of timeseries modeling. It is not clear how the paper extends the state of the art in this area, and whether/how the proposed method is new compared to some fairly basic signal processing concepts, such as low-pass filtering to reduce noise.\", \"Questions\": \"The paper does not do a good job positioning itself in the vast literature of timeseries modeling. Some key statements in the paper show this issue more clearly. For example:\\n\\n\\\"A common shortcoming emerges in existing methods: none explicitly address noise in time series data alongside an appropriate strategy for handling this unwanted signal. Unlike other data types, real-world time series often harbor substantial noise, severely impacting task accuracy\\\"\\n\\nThis statement is hard to accept. Of course all or most prior work in this area considers that the time series will be noisy. This is one of the main reasons we use neural networks, as opposed to other models that are much less capable to deal with noisy data. \\n\\nThen, there are some sentences that are hard to understand. For example: \\n\\n\\\"we devise a sampling strategy based on the insight that the noisy signal combined with the original series shouldn\u2019t disrupt time series frameworks. In essence, frameworks should produce consistent representations given noise-free or raw series (noise-resiliency characteristics).\\\"\\n\\nWhat \\\"Frameworks\\\" are the authors talking about? \\n\\nAdditionally, there are some aspects of the model, or claims about the model, that are very basic but they are presented as an important technical contribution. For example:\\n\\n\\\"To achieve this, we shift our focus from the temporal domain to the spectral domain. Here, we employ a spectrum-based low-pass filter to create correlated yet distinct views of each input time series. These augmented views serve as positive samples of the raw series, effectively capturing the desired noise invariance. The advantages of this low-pass filter-based augmentation are twofold: (1) the filter preserves key characteristics such as trend and seasonality, ensuring deterministic and interpretable representations; (2) it eliminates noise-prone high-frequency components, improving noise resilience and enhancing downstream task performance by aligning the raw signal representation with the augmented view\\\"\\n\\nOf course there is nothing particularly new in the previous paragraph. Working in the spectral domain and performing LPF to remove high-frequency noise are elementary operations that are typically taught at the level of undergraduate signal processing courses. \\n \\nConsidering the previous weaknesses, it is hard to provide specific technical comments to the authors, given that (at least to this reviewer) it is not clear if the paper has something really new to contribute in the area of timeseries modeling.\"}"}]}
{"paper_id": "QFo2wxQEW6", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Endovascular interventions are used for diagnosing and treating vascular diseases using flexible tools like guidewires and catheters. Recently, robotic systems have been developed to assist in these interventions, allowing remote operation by surgeons. However, these systems generally follow a master-slave model, heavily relying on surgeons and lacking autonomous capabilities. Current progress is limited due to the reliance on closed-source simulators and the difficulty in obtaining large-scale datasets for machine learning training due to costly and complex medical setups.\nGap: Progress towards autonomous catheterization is hindered by the widespread use of closed-source simulators and physical phantoms. Additionally, it's challenging to acquire large-scale datasets needed for training machine learning algorithms due to the expensive medical procedures involved.\nHypothesis: Developing an open-source, real-time simulator for endovascular interventions can significantly advance research and development in autonomous catheterization systems by facilitating machine learning algorithm training and addressing the limitations of existing closed-source environments.\nMethod: The authors introduce CathSim, the first open-source, real-time simulator specifically for endovascular interventions. CathSim is built on MuJoCo due to its computational efficiency and machine learning ecosystem integration. The simulator includes anatomically accurate models from Elastrat and supports AR/VR integration. It features a modular, upgradable design with high-fidelity visualizations and realistic guidewire simulation. CathSim is validated through experiments comparing its force measurements with real-world scenarios, using a load cell in physical models. The simulator also integrates an expert navigation network using Convolutional Neural Networks (CNN) and Multi-Layer Perceptrons (MLP) to process multimodal inputs, facilitating autonomous learning tasks such as imitation learning and force prediction.\nConclusion: CathSim provides a reliable open-source platform for simulating endovascular interventions, capable of mimicking real-world scenarios. The expert navigation network developed based on CathSim demonstrates effective performance in tasks like imitation learning and force prediction, suggesting the simulator's potential to accelerate autonomous catheterization research. The simulator allows for diverse algorithm development and is an invaluable asset for the medical simulation community.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"* Open source simulator\\n* Simulator is validated via comparison to real world robot\\n* Expert navigation results beating the human baseline\", \"Weaknesses\": \"* ENN model architecture is not state-of-the-art, e.g. no transformer\\n* Unclear how trustworth the user study is. 10 participants in user study is a small sample size and it\u2019s unclear how they were recruited and how trustworthy their judgements are.\\n* Lack of relevant details, e.g. what is the expert policy that the ENN is trained with. Why don\u2019t you show results for the expert policy in the table?\\n* If an expert policy exists, why do you train an ML model (ENN) model for the expert navigation task?\\n* Some figures / tables are not easy to understand on their own, e.g. Table 2, for which e.g. instead of Q1 you could write \u201cAnatomical accuracy\u201d.\", \"Questions\": \"Overall, this seems to be very relevant work. However, in terms of ML it is not pushing the boundary of the state-of-the-art. This work might fit better to a more applied conference.\"}"}]}
{"paper_id": "R1crLHQ4kf", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Automatic Speech Recognition (ASR) systems are vulnerable to adversarial attacks which can mislead them into producing incorrect outputs by adding imperceptible noise to the audio input. These adversarial examples pose significant security threats, such as over-the-air attacks on commercial devices, where malicious inputs can lead to incorrect system actions. Current ASR systems, often based on deep learning, do not possess inherent robust defenses against these attacks.\nGap: Existing methods to counter adversarial attacks either focus on improving model robustness or detecting the presence of adversarial examples. Many of these approaches rely on modifying input data or adversarial training, which can be computationally expensive and sometimes ineffective when attackers adapt their methods. There is a need for an efficient detection mechanism that can work across varying ASR systems and is not susceptible to adaptive attacks.\nHypothesis: The authors propose that by analyzing the probability distribution of output tokens generated by an ASR system, it is possible to detect adversarial examples efficiently. The distributions at each time step are expected to exhibit certain statistical characteristics that can help distinguish between benign and adversarial inputs.\nMethod: The method involves measuring characteristics of the probability distribution over output tokens in each time step. These characteristics include the median, maximum, and minimum of output probabilities, entropy, and divergences such as Kullback-Leibler and Jensen-Shannon divergence between time steps. Binary classifiers, including Gaussian classifiers, ensembles, and neural networks, are then used to differentiate between benign and adversarial examples. The method is tested across multiple ASR systems and datasets to ensure generality, and adaptive attacks are formulated to test the robustness of the detection strategy.\nConclusion: The proposed detection approach achieves high accuracy, with AUROCs exceeding 99% for distinguishing adversarial from benign data in various ASR models. Although adaptive attacks can reduce detection accuracy slightly, they become more apparent and easier to filter due to increased noise levels. This approach provides a robust means of leveraging the statistical properties of output distributions to effectively differentiate between normal and adversarial inputs, offering a new defensive layer for ASR systems.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"1. This paper proposes a simple and effective method for detection adversarial examples for ASR systems.\\n2. The paper is presented with comprehensive experiments. The authors not only present the benign detection performance of adversarial attacks but also analysis the robustness under adaptive attacks with known detection method.\", \"Weaknesses\": \"1. Detection performance is only evaluated on limited adversarial attack methods. The authors only evaluate their method on C&W attack and Psychoacoustic attack. More attack methods like [1] [2] and even some black box methods like FAKEBOB [3] are still needed to be included to prove the general performance of the detection method.\\n2. Lack of comparison with other audio adversarial example detection methods like [4].\\n\\n[1] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.\\n\\n[2] Alzantot, Moustafa, Bharathan Balaji, and Mani Srivastava. Did you hear that? adversarial examples against automatic speech recognition. arXiv preprint arXiv:1801.00554 (2018).\\n\\n[3] Guangke Chen, Zhe Zhao, Fu Song, Sen Chen, Lingling Fan, Feng Wang, and Jiashui Wang. To\\u0002wards understanding and mitigating audio adversarial examples for speaker recognition. IEEE Transactions on Dependable and Secure Computing, 2022.\\n\\n[4] Rajaratnam, Krishan, and Jugal Kalita. Noise flooding for detecting audio adversarial examples against automatic speech recognition. In 2018 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT), pp. 197-201. IEEE, 2018.\", \"Questions\": \"In Section5.1, four guiding principles are provided on selection process. Do such principles limit the adversarial attack implementation? For example, there should be an equal number of tokens in both the original and target transcriptions. Would there be other attack scenarios like tokens insertion or deletion?\"}"}]}
{"paper_id": "9GviaQcGnx", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Regularization techniques, like weight decay and L2 regularization, are commonly used in deep learning to reduce overfitting by applying uniform penalties to all model parameters. However, these techniques may not be optimal for all parameters, as each may benefit from different regularization strengths. Previous works have explored adaptable weight decay and layer-wise scheduling to improve final performance, indicating the potential benefits of dynamic regularization strategies.\nGap: Current weight decay methods apply a uniform penalty to all parameters, which may not be ideal for all neural network parameter groups. Different parameters could benefit from varying regularization strengths, yet implementing such dynamic regularization typically requires setting multiple hyperparameters, complicating the regularization strategy.\nHypothesis: Constrained Parameter Regularization (CPR) can provide individualized and dynamic weight regularization by setting upper bounds on parameter group norms. This approach eliminates the need for separate penalty coefficients and can potentially outperform traditional weight decay and L2 regularization by dynamically adapting regularization strength without additional runtime overhead.\nMethod: CPR is implemented by transforming the regularization problem into a constrained optimization problem where an upper bound is enforced on a statistical measure (such as L2-norm) of different parameter groups. This is solved using an adaptation of the augmented Lagrangian method. CPR requires two hyperparameters: a regularization bound and an update rate, with an adaptive mechanism proposed to adjust bounds during training. The approach was tested against traditional weight decay in tasks involving grokking, computer vision, and language modeling. CPR\u2019s effectiveness was demonstrated using ResNet and GPT2 models on CIFAR100 and OpenWebText datasets, without incurring extra runtime costs.\nConclusion: Constrained Parameter Regularization effectively counteracts the grokking phenomenon and shows improved or on-par performance relative to traditional weight decay in various neural network tasks. The use of CPR allows for efficient and individualized adaptation of parameter regularization during training, making it a potentially more flexible alternative to existing methods while keeping hyperparameter tuning minimal.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"1\uff09 The authors introduce a constrained parameter regularization that does not uniformly \\napply a constant penalty to all parameters.\\n2\uff09 The authors provide an open-source implementation of CPR, which can be easily adjusted \\nby replacing the optimizer class.\", \"Weaknesses\": \"1\uff09 The authors should provide a clearer exposition of the problem statement and research \\nmotivation in the abstract.\\n2\uff09 In the related work section, the authors should provide a more comprehensive review of \\nprior research.\\n3\uff09 The experimental results conducted by the authors did not show a significant \\nimprovement in optimization performance, and they did not provide additional \\nexperimental results or comparisons related to existing work.\\n4\uff09 While applying different regularization strengths to each parameter group may seem \\nintriguing, the authors have not discussed the necessity and potential advantages of this \\napproach compared to traditional methods.\", \"Questions\": \"N/A\"}"}]}
{"paper_id": "Ts95eXsPBc", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The current primary approach for modeling episodic memory in AI systems is through transformers that store temporally ordered experiences. This method overlooks the spatial dimension, which is significant in cognitive processes as suggested by cognitive science.\nGap: The current implementation of episodic memory using transformers does not incorporate spatial information, making it unclear how the underlying structure can be extended to include the spatial axis and what benefits this could provide.\nHypothesis: Incorporating spatial information into transformer models will enable the creation of a place-centric episodic memory that considers both temporal and spatial dimensions, potentially improving memory utilization efficiency and enhancing accuracy in place-centric tasks.\nMethod: The researchers developed Spatially-Aware Transformers (SAT) that incorporate spatial information and introduced an Adaptive Memory Allocator (AMA) based on reinforcement learning to optimize memory utilization. They conducted experiments across various environments and tasks such as prediction, generation, reasoning, and reinforcement learning to demonstrate the effectiveness of these models.\nConclusion: Spatially-Aware Transformers improved spatial reasoning tasks and memory efficiency, showing advantages across multiple machine learning tasks. The introduction of the Adaptive Memory Allocator allowed flexible memory management strategies beyond traditional approaches, confirming the potential benefits of incorporating spatial information into episodic memory models.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"- This paper studies the important problem of utilizing and managing spatial and temporal memory experienced by embodied agents that are essential to address various downstream tasks. Particularly, it considers the practical issue of memory constraint and perform experiments based on the popular transformer architectures. The research presented in this paper is very well motivated.\\n\\n- The paper is very technical solid. Almost all arguments are well-supported/justified by the highly-relevant and classic references and thorough experiments (in Appendix). It carefully compares place- and time-centric store and read, and progressively studies FIFO to the proposed Adaptive Memory Allocator based on SAT. I believe the extensive settings and results presented in this paper have the potential to inspire many future works.\\n\\n- Overall, this paper was a very enjoyable read to me. All details have been clearly presented (especially with the Appendix and all nice visualizations); The paper is nicely-structured, concise but contains massive valuable information. I believe many arguments and thoughts presented in this paper will be very constructive to relevant future research.\", \"Weaknesses\": \"- The title of this paper is very misleading\\n    - The paper does not propose any new formation of the transformer architecture to better model spatial information, but focuses on managing agents' episodic memory for addressing different downstream tasks. \\n\\n- This paper overclaims several contributions.\\n    - The paper says \\\"we are the first to motivate, conceptualize, and introduce the notion of transformers capable of utilizing explicit spatial information\\\", but as the authors mentioned that defining and managing external memory have been extensively studied in previous machine learning literatures. It simply compares place- and time-centric store and hierarchical read methods that are intuitively suitable for different downstream tasks.\\n    - One good example is the use of topological graph that stores observations of keypoints for agents in visual navigation, e.g., the widely applied DUET agent [1] for vision-and-language navigation [2], which is essentially close to the proposed SAT-PM-PH model. \\n    - The experiments of action-conditioned image generation (Exp-5) and reinforcement learning agents (3.3) are not very convincing to me. Exp-5 is not a practical setting and 3.3 is relatively simple that cannot represent other reinforcement learning agents, see more below.\\n\\n- Missing experiments.\\n    - I think this paper lacks investigation on the more recent embodied agents and their memory management approaches. \\n    - The experiments presented in this paper are relatively small-scale and simple. I am concerning how the proposed methods might impact recent research that often apply more capable networks (and massive data) to learn generic spatiotemporal priors to facilitate representing and memorizing the observations. e.g., Figure 3 - an agent might be able to create a very compact representation for all ballet rooms from a single visit to each room? \\n    - Following my previous point, I think this study emphasizes the scenario of memory constraint, but the experiments only considers small memory capacity, negelating the difference in representation (i.e., how to store an observation) and the difficulty in grounding from the queries to relevant memories. Overall, the generalization and practical influence of the proposed methods is not very clear to me.\\n    - I think a valuable baseline is missing here: without explicitly defining any memory management strategies but gives the agent a certain memory budget and asks it to learn to update the memory by itself by training the agent on a mixture of data and tasks (e.g., a more general version of DNC mentioned in Appendix D.4).\\n\\n[1] Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language Navigation. Chen et al., CVPR2022.\\n\\n[2] Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments. Anderson et al., CVPR2018.\", \"Questions\": \"I hope the authors can address some of my concerns in Weaknesses. I don't have any other questions here.\"}"}]}
{"paper_id": "0jsfesDZDq", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Recurrent Spiking Neural Networks (RSNNs), inspired by the human brain, have emerged as efficient models for processing spatio-temporal data. They utilize spikes for efficient learning and have been shown to improve with diversity in neuronal parameters, known as heterogeneous RSNNs (HRSNNs). Previous research has demonstrated that exploiting this heterogeneity can optimize network performance, but such models also suffer from increased complexity with more neurons and dense connections, making them hard to optimize.\nGap: Traditionally, sparse RSNNs are created by first training a dense model for a specific task and then using activity-based pruning methods that focus on removing low-activity neurons. This approach often results in models that are highly customized for specific tasks and may not generalize well across different tasks, leading to poor overall efficiency and adaptability.\nHypothesis: The authors hypothesize that a task-agnostic pruning method can be devised that prunes large, untrained RSNN models to form sparse, generalizable RSNNs, without sacrificing performance. By leveraging Lyapunov exponents and graph sparsification, a universally robust network can be achieved that maintains stability and flexibility across different tasks.\nMethod: The authors introduce the Lyapunov Noise Pruning (LNP) method, which involves pruning untrained, initialized RSNN models. The LNP employs spectral graph sparsification and leverages Lyapunov exponents to determine neuron and synapse removal, optimizing for both the network topology and neuronal time constants. The method does this while maintaining stability, without initial training on specific tasks. Experiments were conducted on image classification (CIFAR10 and CIFAR100) and time-series prediction tasks using synthetic (e.g., Lorenz and Rossler systems) and real-world datasets (Google stock and wind prediction).\nConclusion: The LNP method results in sparse RSNNs that are flexible and efficient, maintaining performance across multiple tasks. Compared to traditional pruning methods, LNP enhances computational efficiency by reducing neuron and synapse count while maintaining accuracy. The method shows greater generalizability and robustness due to its task-agnostic nature, making it advantageous in scenarios with constrained computational resources.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"The LNP enables pruning the network prior to training, and effectively enhancing the sparsity while maintaining commendable performance.\", \"Weaknesses\": \"1. The authors should elucidate the motivation behind their proposed method. What significance does their method hold for the field of spiking neural networks?\\n2. The network model and task employed in the experiments are too simple and do not need pruning. I am more intrigued by the adaptability of the LNP method under more intricate situations.\\n3. The authors provided a detailed description of the latest pruning works in SNN in the second paragraph of the Introduction. However, their results are not referenced in Table 1. Meanwhile, the compared works in Table 1 lack reference links.\\n4. The equation 3 appears abrupt. I don\u2019t understand why the spike frequency of untrained SNN can be used to construct this linearized model. And how to calculate the matrix D and L?\\n5. Activity Pruning needs to provide citation or explanation.\\n6. Lorenz and Rossler are unfamiliar datasets in the field of SNN. Could you please provide the citations?\", \"Questions\": \"See the weakness please.\"}"}]}
{"paper_id": "eJhgguibXu", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: In model-based reinforcement learning, agents use learned models of environment dynamics to improve policies. Models help generate experiences for learning policies or planning outcomes, enhancing sample efficiency and adaptability to new tasks by reducing the need to relearn environment fundamentals. Despite this, model accuracy limits these benefits, as inaccuracies can cause failures when agents trust them in erroneous states. Humans, however, effectively use imperfect intuitive physics models to navigate and reason about the world.\nGap: Existing model-based reinforcement learning approaches face challenges with model inaccuracies and compounding errors over long rollouts, which can lead to failures in policy learning and hinder exploration efficiency, especially in environments with continuous action spaces and sparse rewards. Current literature focuses on mitigating these inaccuracies rather than leveraging them for exploration.\nHypothesis: The authors hypothesize that learned approximate models can be used to guide exploration towards useful regions of the state and action space, thereby improving sample efficiency and policy learning in complex environments with large action spaces and sparse rewards.\nMethod: The authors propose learning dynamics models for intuitive physics tasks using graph neural networks (GNNs) that incorporate object, relational, and event structures. By training these models in a task-agnostic way with data collected from object collision interactions, they develop methods to identify rewarding action clusters. These clusters guide exploration in policy learning through an epsilon-greedy strategy, combined with Deep Deterministic Policy Gradients (DDPG), to improve learning efficiency. The models were applied to the CREATE environment for physics-based puzzles, targeting tasks requiring precise tool placements to achieve goals.\nConclusion: The study demonstrates that approximate models learned using GNNs can effectively generalize to unseen tasks and improve policy sample efficiency by guiding exploration towards rewarding action spaces. This approach accelerates convergence in reinforcement learning tasks, suggesting that approximate models can be advantageous for exploration in environments with challenging features such as large action spaces and sparse rewards.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"The paper is mostly fairly easy to understand and aims to tackle an important issue of efficient exploration.\", \"Weaknesses\": \"The description of the experiments needs improvement. Moreover, the results are not very convincing. Further details are described below.\", \"Questions\": \"1.\\tFigures 2,3 and 4 need more descriptive captions as well as better descriptions in the text.  For example, what do the symbols correspond to in Fig 3? Similarly, in Fig 4, a legend (along with better descriptions) would have made it much easier to interpret the results. \\n\\n2.\\tWhy is it that the relative Goal hits in Fig 5a are much higher compared to other tasks?\\n\\n3.\\tIt is not immediately clear why the approach applies only to intuitive physics based tasks. I think this point needs to be emphasized better in the introduction.\\n\\n4.\\tOver how many trials were the experiments conducted in Fig 6? In general, from the learning curves, the learning does not look stable.\\n\\n5.\\tDoesn\u2019t exploration epsilon=0 imply no model guidance? If so, for say, task 4, they both reach the same asymptotic performance. Why is this the case, while in some of the other tasks, the asymptotic  performances are very different?\\n\\n6.\\tAblations showing the effect of $\\\\epsilon_{threshold}$ are missing. \\n\\n7.\\tAs claimed in the last line on Page 7, why would the policy learning diverge? I believe that the learning would still occur (as DDPG learns off-policy) but due to a lack of exploitation, the learning curves would not reflect the learnt policy.\\n\\n8.\\tPerhaps the authors could have considered Phyre environments (https://ai.meta.com/tools/phyre/) to validate their approach.\\n\\n9.\\tIn Fig 6, I assumed the orange color always corresponds to exploration epsilon of 0, but for task 6, the colors are swapped.\\n\\n10.\\t\u201cBy using\u2026however uncertain\u201d \u2013 This sentence towards the end of the introduction is too long and can be phrased better.\"}"}]}
{"paper_id": "HwQ8NVvdmm", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Continual learning allows machine learning models to adapt to new data and tasks over time without forgetting previously learned tasks. This is crucial for applications that need in-field adaptation, such as human activity recognition (HAR) on edge devices with privacy and low latency requirements. Traditional approaches to continual learning are resource-expensive, making them impractical for resource-constrained environments such as edge devices. Reducing computational precision through fully quantized training (FQT) can decrease memory usage and computational demand, making models more suitable for edge deployment.\nGap: While FQT can improve efficiency, aggressive quantization often leads to significant drops in model accuracy. There is limited research on applying FQT with continual learning (CL) techniques, particularly for resource-constrained environments. This raises the question of whether FQT can be effectively integrated with CL to maintain accuracy while leveraging the benefits of quantization.\nHypothesis: The authors propose that leveraging Hadamard transforms, combined with integer-only quantization and appropriate techniques such as tiled matrix multiplication and stochastic rounding, can enable fully quantized continual learning that maintains accuracy close to that of higher precision models while reducing resource consumption.\nMethod: The proposed method integrates Hadamard transforms with quantization, applying them to the backward pass during training to use available quantization levels more efficiently. This involves matrix multiplication in the Hadamard domain, integer quantization of weights and activations to 4-bits, and 8-bit accumulation. The authors also determine certain tensors needing stochastic rounding to mitigate quantization noise. They evaluate their method, called Hadamard Domain Quantized Training (HDQT), using CIFAR100 and several HAR datasets in a class-incremental learning setting, testing against popular CL techniques like LwF, iCaRL, and BiC.\nConclusion: The study finds that using HDQT, it is possible to achieve quantized training with minimal accuracy loss: less than 0.5% for HAR datasets and less than 3% for CIFAR100, compared to floating-point baselines. Moreover, they observe that quantized continual learning does not exacerbate catastrophic forgetting compared to non-CL training. HDQT also shows significant improvements in computational efficiency and energy consumption, making it well-suited for edge applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"* This work is clearly written and proposes a principled and efficient method of solving the non-uniformity of gradients during backward propagation.\\n* Whereas previous works have proposed log scaling of values, this is impractical to implement efficiently on hardware. Also, it is difficult to guess the necessary value range beforehand. The Hadamard transform is both efficient to implement and is ideally suited to removing gradient spikes while preserving information. As such, there is potential for Hadamard domain backpropagation in reduced-precision training. \\n* In the backpropagation process, utilizing HDQT allows setting the input and accumulation bit-width to 4 and 8 bits, respectively, showcasing improved energy efficiency compared to conventional CIL methods.\\n* The authors present promising results on image datasets (CIFAR100) and human activity recognition datasets (DSADS, PAMAP2, HAPT), demonstrating a degradation of \u2264 2.5% through quantization in vision tasks.\", \"Weaknesses\": \"* It is difficult to understand the motivation for connecting continual learning with integer valued training. While edge devices have fewer resources for training, they would not be expected to perform much training or have it completed quickly. Therefore, it would be difficult to justify the additional expense in both software and hardware when the same result can be obtained simply by waiting for a few more hours. \\n* More experiments on different domains are necessary to support the claims of the efficacy of Hadamard domain quantization. For a fairer comparison, there should be a comparison using other methods used in fully quantized training when applied to continual learning. Moreover, there should be a comparison with previous methods when training from scratch, not just during continual learning.\\n* The method proposed in the paper would be difficult to implement due to the requirement for stochastic rounding, which cannot be easily implemented in hardware, integer-based or otherwise. Even if such hardware could be implemented, this would require custom circuitry solely for that purpose.\", \"Questions\": \"* In Section 4.4, when discussing the energy estimates derived from the accelerator cost model for HDQT application, there is a concern that if the training time increases due to HDQT, it may consume more energy than conventional methods. Can the experiments include a comparison of training times when applying HDQT?\\n* When examining the results for CIFAR100 in Table 1, the accuracy degradation appears to be relatively higher compared to other datasets. It seems that the trend may vary depending on the dataset or network architecture. Have there been results applying the method to more complex network architectures or larger datasets?\\n* While there are advantages to performing matrix multiplication as integer-arithmetic units through HDQT, it is likely that the overhead for Quantization and Stochastic Quantization cannot be ignored, especially as the batch size increases. I am curious about the energy efficiency, including the overhead for Quantization and Stochastic Quantization, as the batch size becomes larger.\"}"}]}
{"paper_id": "XTwwtlEfTF", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Multimodal learning uses data from multiple sources (e.g., text, images, audio) to improve performance in tasks like image segmentation and sentiment analysis. Robustness against missing or corrupt modalities at test time is a critical challenge as performance often degrades significantly without all modalities.\nGap: Existing multimodal networks face significant performance drops when one or more modalities are missing during testing, and current solutions either require specific training strategies or perform poorly on untrained modality combinations.\nHypothesis: Parameter-efficient adaptation of pretrained multimodal networks using low-rank adaptation and modulation of intermediate features can enhance robustness to missing modalities, partially mitigating performance drops and possibly outperforming dedicated networks for available modality combinations.\nMethod: The authors propose a parameter-efficient adaptation for pretrained multimodal networks where low-rank matrix adjustments and modulation of intermediate features (e.g., through scale and shift layers) account for missing modalities. They conducted experiments across diverse datasets for tasks including RGB-thermal and RGB-depth semantic segmentation, material segmentation, and sentiment analysis, highlighting the adaptability of their method. The adaptations require less than 0.7% additional parameters compared to the total model parameters.\nConclusion: The proposed adaptation method successfully improves model robustness to missing modalities, showing better performance than baseline and existing methods across several datasets and tasks. The method is versatile across different multimodal challenges and efficiently adapts to varying modality combinations while requiring minimal additional parameters.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"- Development of multimodal learning algorithms robust to missing modalities is crucial since observations from some modality could go often be missing due to sensor/hardware failure.\\n\\n- The proposed method is effective and importantly efficient as it introduces a very small set of parameters towards making a pre-trained model robust to missing modalities.\\n\\n- The paper is mostly well-written and easier to read.\\n\\n- Experiments have been performed on two different tasks of multimodal semantic segmentation and multimodal sentiment analyses. Results claim to surpass the existing methods and (often) dedicated methods when the modalities are missing at test time.\", \"Weaknesses\": \"- Although the idea is simple and effective for achieving parameter-efficient adaptation of models, the application of existing techniques (i.e. low rank adaptation, scale and shift transformations, and BitFit) to adapt a pre-trained model on a subset of modalities seems low from novelty aspect.\\n\\n\\n- There is no insight and/or analyses into how the proposed paramater-efficient is helpful towards making the model robust to missing modalities. It is important to develop the grounding of the proposed method since only showing improvements over the baseline with empirical results doesn't help much towards this.\\n\\n- The paper doesn\u2019t mention any clear justification on why SSA was chosen over the LoRA and BitFit to report the results in the paper.\\n\\n- Comparison is missing with any of the existing method for the task of multimodal sentiment analyses. \\n\\n- Why CMU-MOSI and CMU-MSEI datasets were chosen to evaluate and report the performance of the method?\", \"Questions\": \"- There is no study that fairly compares the parameter efficiency of the proposed method with competing methods?\\n\\n- What are the possible reasons for relatively lower performance of other adaptation strategies, compared to SSA, in Table 4? Some explanation would be helpful to better understand their comparison in missing modalities scenario.\"}"}]}
{"paper_id": "lt6xKGGWov", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Feature selection methods are crucial for predictive modeling, either as wrappers that use the learning model as a subroutine or as filters that select features based on a dependency score between features and the target without needing to know the learning procedure. Recently, the Mutual Information Neural Estimator (MINE) has been introduced as a modern method to estimate mutual information.\nGap: Current feature selection methods often examine features one at a time, which limits their ability to capture complex dependencies between features and targets.\nHypothesis: Using neural estimation of mutual information to evaluate subsets of features as an ensemble can capture sophisticated relationships between features and targets, enabling more accurate feature selection.\nMethod: The paper introduces MINERVA, a feature selection filter that uses MINE to estimate mutual information for ensembles of features. It employs a neural network to approximate functions for the Donsker-Varadhan representation of Kullback-Leibler divergence. The feature selection process involves optimizing a loss function with regularization terms to ensure sparsity and control the magnitude of weights. MINERVA is implemented in the minerva package and is tested on synthetic datasets to demonstrate its effectiveness in capturing complex dependencies and performing exact feature selection.\nConclusion: MINERVA successfully captures sophisticated non-linear dependencies between features and targets, performing exact selection where existing methods either fail or perform non-exact selections. The proposed method shows improved accuracy in synthetic benchmarks, underscoring its capability to handle complex feature-target relationships.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"The authors introduce a method that selects features based on neural estimation of mutual information.\\n\\nThe method outperforms existing techniques in two synthetic datasets.\", \"Weaknesses\": \"The experiments were limited to two synthetic datasets, casting doubt on the method's applicability to real-world datasets. Multiple factors can influence model performance. These include sample size, hyperparameters $c_1$ and $c_2$, the neural network architecture, and the noise that cannot be explained by the input $\\\\mathbf{X}$. Without a comprehensive analysis of these factors, the general efficacy of the proposed method remains uncertain.\\n\\nEquation (1) provides an estimate for mutual information. Yet, once the constraints are introduced in Equation (5), it's unclear if the function still acts as a mutual information estimator. This requires more in-depth theoretical exploration.\\n\\nThe definition of $\\\\phi$ in Line 11 of Algorithm 1 is unclear.\", \"Questions\": \"1. Does the proposed method work in real-world datasets?\\n\\n2. Is Equation(5) an estimator of mutual information?\\n\\n3. How if $\\\\phi$ in Line 11 of Algorithm 1 defined?\"}"}]}
{"paper_id": "Mylk5iamJC", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The classic recognition problem assumes that all test classes are known during training, known as closed-set recognition (CSR). Open-set recognition (OSR) requires models to reject unknown classes not encountered during training. Discriminative models struggle with OSR due to the absence of unknown samples, leading to methods that focus exclusively on either CSR or OSR, compromising the other.\nGap: Existing methods are often optimized for CSR or OSR but not both, due to difficulties in learning decision boundaries without unknown samples in OSR. This results in poor performance trade-offs between CSR and OSR.\nHypothesis: The paper proposes that CSR and OSR share the same goal when approached with generative models, as both can be minimized using maximum likelihood estimation on known class training data. Therefore, a single latent generative model can address both problems simultaneously.\nMethod: The authors introduce the Latent Gaussian Mixture Model (L-GMM) with a collaborative training algorithm. This model uses an encoder to map inputs to a latent space and a density estimator to compute probability densities via a neural Gaussian Mixture Model. The end-to-end training algorithm separates and synchronizes the generative learning (density estimator) with maximum likelihood estimation and the discriminative learning (encoder) using a loss derived from the generative model. This framework is tested on image recognition and segmentation tasks for both CSR and OSR, demonstrating superior performance to discriminative counterparts and existing specialized methods.\nConclusion: L-GMM achieves better closed-set recognition performance than discriminative models when trained from scratch, and also excels in open-set recognition without needing modifications or prior knowledge. The proposed method effectively combines generative and discriminative modeling, offering a unified approach to address both CSR and OSR problems.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"The solution of using latent Gaussian mixture model in the encoder to tackle the close-set recognition is valid.\", \"Weaknesses\": \"(1) The novelty of the paper is pretty limited as similar ideas for using Gaussian mixture model in variational inference to tackle open-set classification has been studied and published.\\nFor instance, \\\"Open-Set Recognition with Gaussian Mixture Variational Autoencoder\\\" by A. Cao at in AAAI 2021\\n\\n(2) Compared to the previous work as mentioned, this paper did not provide any new insight and fails to clarify the distinction with the similar paper mentioned above.\\n\\n(3) The derivation of latent Gaussian mixture model is very common and similar derivation has been shown in previous published work including \\nhttps://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=5fda8f8bd7ff10db1ee3ab558fd41e552f008fb3\\n(i)\\\"Robust Estimation of Gaussian Mixtures from Noisy Input Data\\\", CVPR 2008\\nand \\n(ii) https://ieeexplore.ieee.org/abstract/document/9523628, TNNLS 2023 \\\"Robust Semisupervised Deep Generative Model Under Compound Noise\\\"\\nThese papers have clearly shown that Gaussian mixture model can effectively handle the outliers and noisy labels as well as tell samples from unknown class for recognition in generative model including VAE and GAN. In this case, it is not clear if there is really new insight and contribution that the current manuscript conveys. Modules including discrimination loss, density estimators have also been well studied in the previous work. If we look at the math, there is really very little difference except small tweaking.\", \"Questions\": \"It is important to increase the novelty by providing new insights with new approaches instead of providing small extensions with existing method.\\n\\nIt is not clear what is the main contribution of this paper as the novelty is too limited and the discussion with the existing methods is also important and necessary.\\n\\nIt is necessary to show the distinction of the current paper from the previous work shown in 3(i) and 3(ii) in order to stand on the safe ground in terms of novelty.\"}"}]}
{"paper_id": "zFWKKYz2yn", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Recurrent Neural Networks (RNNs) are effective at modeling temporal patterns and have been explored for their capabilities in recognizing grammatical structures. To understand RNNs better, researchers have focused on extracting interpretable rules in the form of Deterministic Finite Automata (DFA) from RNNs. Two main approaches for rule extraction are quantization and equivalence query (L* algorithm).\nGap: Current studies do not adequately address the stability of these rule extraction methods, which is crucial as instability can lead to failure in extraction. Additionally, there is insufficient empirical analysis comparing these methods across various RNN architectures, especially in less-than-ideal training conditions (partially trained networks).\nHypothesis: Quantization-based rule extraction is hypothesized to be more stable than equivalence query-based methods, particularly for partially trained RNNs. Additionally, higher-order RNNs like O2RNN are expected to encapsulate more stable rules compared to first-order RNNs.\nMethod: The study conducts extensive experiments on 3600 RNN models trained on datasets from 7 Tomita grammars and 4 Dyck grammars, using 4 different RNN architectures (LSTM, GRU, O2RNN, and MIRNN). Rule extraction was performed using quantization methods (k-means and SOM) and L* equivalence query methods. The stability of DFAs was analyzed by their state counts and performance consistency across random initialization seeds.\nConclusion: Quantization-based methods generally outperform L* in stability across both Tomita and Dyck languages. O2RNN shows superior stability and performance in DFA extraction compared to other RNN types. The study demonstrates that quantization-based extraction maintains stability even in partially trained conditions, making it a promising approach for practical machine learning applications where perfect accuracy isn't reached.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 4, \"Contribution\": 4, \"Rating\": 5, \"Confidence\": 1, \"Strengths\": \"The paper is well presented, by providing substantial background information and experiments.\", \"Weaknesses\": \"See questions.\", \"Questions\": \"Honestly, my familiarity with current RNN research, including rule extraction strategies, is limited. Consequently, I find it inappropriate to assess the topic, method, or the novelty of the work. However, I do have several questions for discussion.\\n\\n- It would enhance the paper if the authors could furnish additional insights or intuition regarding the significance of exploring the stability of rule extraction strategies. Alternatively, a clearer definition of stability in this context would also be helpful. In the current work, stability appears to be synonymous with the variance of performance.\\n\\n- A more thorough justification for the conclusion would be beneficial. Instead of solely benchmarking several methods across various settings, delving into the rationale behind the conclusions drawn would strengthen the paper and provide a more comprehensive perspective.\"}"}]}
{"paper_id": "eEslYpY6Yq", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Graph Neural Networks (GNNs) have become a state-of-the-art approach for various graph-related tasks due to their efficiency in aggregating features from neighboring nodes. Mixup, on the other hand, is a data augmentation technique that averages features and labels from multiple samples to generate new examples. Both techniques utilize information aggregation from multiple samples to enhance learning models.\nGap: The paper addresses the gap in understanding the potential connection between graph convolution operations used in GNNs and Mixup techniques. There is a lack of exploration on whether these seemingly unrelated techniques are fundamentally connected or could be unified under a common framework.\nHypothesis: The main claim of the paper is that graph convolution can be mathematically expressed as a special form of Mixup under two mild conditions: the Homophily Relabel condition where the target node's label is assigned to its neighbors, and the Test-Time Mixup condition where feature mixing is performed during the testing phase.\nMethod: The paper uses a mathematical framework to establish the equivalence between graph convolution and Mixup. It demonstrates that both Graph Convolutional Networks (GCN) and Simplified Graph Convolution (SGC) can be reformulated to adhere to the Mixup technique under specified conditions. The equivalence is validated by training a Multi-Layer Perceptron (MLP) with homophily relabeling and test-time Mixup and comparing its performance to that of GCNs. Various experiments with different data splits and well-known datasets (Cora, CiteSeer, and PubMed) are conducted to provide empirical support for the claims.\nConclusion: The findings indicate that graph convolution is indeed a form of Mixup when homophily relabeling is applied during the training phase and Mixup is performed during testing. This revelation offers a new perspective on understanding graph neural networks, suggesting that efficient network designs can be achieved by integrating Mixup techniques. Additionally, the novel insights into GNNs could lead to development of simpler yet effective versions of these networks, promising comparable performance with reduced complexity.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"- **Novel and original.** The paper proposes a very interesting interpretation of graph convolution, which to the best of my knowledge, has not been studied yet.\\n- **Great presentation.** Paper is well-written with clear mathematical derivations that are easy to follow.\", \"Weaknesses\": \"- **Overall significance is unclear and claimed practical/theoretical implications lack support.**\\n  - [W1] While Section 6 mentions how TMLP and HMLP can be training-efficient with large-scale graphs since they are MLP-based, there are no experiments to compare computational costs and demonstrate this claim. I also suspect the gain in efficiency would be fairly small since graph convolution can be highly optimized via sparse-dense matrix multiplications.\\n  - [W2] The theoretical implications also seem a bit far-fetched: there is a rich line of work on the theoretical expressivity of GNNs [A, B, C, D, E], yet the paper does not convey any clear connection between Mixup and such work to support its theoretical significance. This could most likely constitute another research paper on its own, but any further discussion on what makes \\\"the Mixup perspective\\\" particularly interesting with respect to GNN expressivity would be helpful.\\n- **Discussion on empirical results do not provide much insight and are often unconvincing due to lack of evidence.**\\n  - [W3] Section 3.2 and Figure 3 show that HMLP and GCN share the same test curves, but not the training curves, which the paper conjectures to be \\\"because the labels of nodes are changed\\\". I think the discussion could go a bit deeper (maybe via proper ablation),  considering that the training dynamics of HMLP differs from that of GCN in ways other than the node-labels: not only are all the node-labels mixed, but also 1) nodes in the test-set are used for training HMLP (not the case for GCN) and 2) the model parametrization is different. \\n  - [W4] Section 4.2 mentions how \\\"overdoing Mixup may reduce accuracy\\\", but this observation is not evident from the results in Figure 7, nor is the discussion consistent with the claim that \\\"Test-Time Mixup enhances model robustness\\\". Are there any experimental results that can support this observation? Is this discussion in any way related to the oversmoothing problem of graph convolution [F]?\\n  - [W5] Figure 8 shows a fairly large discrepancy between GNNs and Mixup on the PubMed dataset. Considering that the main claim is that GNNs and Mixup are equivalent, why are we seeing this discrepancy on performance despite PubMed being a homophilic network just like Cora and CiteSeer?\\n\\n[A] Morris et al., Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks. (AAAI 2019)\\\\\\n[B] Xu et al., How Powerful are Graph Neural Networks? (ICLR 2019)\\\\\\n[C] Maron et al., Provably Powerful Graph Networks. (NeurIPS 2019)\\\\\\n[D] Zopf, 1-WL Expressiveness Is (Almost) All You Need. (IJCNN 2022)\\\\\\n[E] Feng et al., How Powerful are K-hop Message Passing Graph Neural Networks. (NeurIPS 2022)\\\\\\n[F] Li et al., Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning. (AAAI 2018)\", \"Questions\": \"- [Q1] I am not sure whether Section 2.4 is really necessary on its own as it does not seem to provide significant information. The section could instead be merged to the Related Work paragraph in Section 1.\\n- [Q2] Question on HMLP: given the example graph in Figure 2, HMLP uses $(x_i, (0.5, 0.5))$ for training, yet the ground-truth sample during test time will be just one of $(x_i, (1, 0))$ or $(x_i, (0, 1))$. In this case, wouldn't the difference in the labels be detrimental for test-time performance since the model will be trained to output $(0.5, 0.5)$ given $x_i$?\\n- [Q3] How are the \\\"MLP\\\"s and \\\"Mixup\\\"s different for each row of plots in Figure 8? Was slightly confused why the same method appears in different graphs with different accuracies.\\n- [Q4] The end of Practical Potentials paragraph in section 6 mentions that HMLP \\\"eliminates the need for connection information during both training and testing\\\". Is this true? Although the model being trained may not require connectivity information, the Homophily Relabeling taken place prior to training does seem to require the connectivity.\\n\\nThere are also several typos:\\n- End of Section 1: \\\"be considered as a generlized ...\\\" -> \\\"be considered as a generalized ...\\\"\\n- Notations paragraph in Section 2: \\\"one-hot labels $Y \\\\in \\\\\\\\{0,1\\\\\\\\}^{N \\\\times (C-1)}$...\\\" -> Shouldn't it be $Y \\\\in \\\\\\\\{0,1\\\\\\\\}^{N \\\\times C}$ with $C$ classes?\\n- Top of page 5: \\\"We rewrite the $\\\\tilde{a}i \\\\cdot X$...\\\" -> should be subscript \\\"$\\\\tilde{a}_i \\\\cdot X$\\\"\\n- Section 2.4: \\\"... understanding of this phoneme ...\\\" -> \\\"... understanding of this phenomenon ...\\\"\\n- End of Section 3.1: \\\"Thus in the next, we propose ...\\\" -> \\\"Thus in the next paragraph, we propose ...\\\"?\\n- Bottom of page 7: \\\"In the Figure 4, the performance ...\\\" -> Wrong reference. \\\"In Figure 6, the performance ...\\\"\\n- End of Section 4.1: \\\"The node representation clearly show that what TMLP ...\\\" -> \\\"... clearly show what TMLP ...\\\"\"}"}]}
{"paper_id": "vnKf1jG5eK", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Recent advancements in AI have led to the development of sophisticated multimodal generative models, leveraging both language models and vision models. The progress in the field can be attributed to large language models (LLMs) for text generation and diffusion models for image generation. Collaborative models like Visual ChatGPT and MM-REACT have emerged, which integrate visual information into text descriptions via pre-trained tools. These models heavily rely on pre-trained components and often struggle with requiring extensive training data or dependencies on encoders like CLIP and ImageBind for modality alignment.\nGap: Current multimodal models face limitations such as dependency on extensive training data to bridge the gap between modalities and a scarce ability to generate comprehensive multimodal responses. They frequently employ CLIP or ImageBind encoders that, while effective at learning joint representations for text and images, require massive datasets to map these encodings to language model spaces effectively. Additionally, these models are generally constrained to understand multimodal content rather than generate it.\nHypothesis: The authors propose that a model based on a bidirectional conditional diffusion model, dubbed BiDiffuser, can more efficiently handle the interaction between modalities by minimizing the necessity for vast datasets and complex pre-training, thus improving the generation capabilities in multimodal tasks such as image-to-text and text-to-image generation.\nMethod: EasyGen consists of a bidirectional conditional diffusion model named BiDiffuser and a large language model (LLM). It is designed to manage image-to-text generation tasks by synchronizing the embedding space of BiDiffuser with that of an LLM using a simple projection layer. BiDiffuser converts images into text format for semantic alignment with the LLM. For text-to-image generation, it utilizes the LLM to generate detailed textual descriptions which BiDiffuser interprets to produce visual content. The training involves fine-tuning the BiDiffuser specifically for image-to-text and text-to-image tasks, with the integration layer conditioned via semantically aligned inputs from multimodal datasets, allowing the model to work efficiently in a lab setting with relatively low computational resources.\nConclusion: EasyGen demonstrates the effectiveness of leveraging a bidirectional conditional diffusion model and LLMs for multimodal generation, performing on par with existing state-of-the-art models while utilizing significantly less training data. The approach showcases robust multimodal generation capabilities with efficient training requirements and improved adaptability through simplified architectural adjustments, marking a significant contribution to the field of AI-generated content.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 5, \"Strengths\": \"- The idea of tuning UniDiffuser as LLM encoders via image-text bidirectional generation is simple and novel, and the results are interesting to me.\\n- The targeted problem of multimodal dialogue is relevant and trending, especially in the modern LLM era.\\n- The method seems very data-efficient, which is good.\", \"Weaknesses\": \"My primary concern is the limited applications and evaluations. For multimodal dialogue, when considering long context conditional image generation or image conditional (multi-images or single-image) image generation like image edition/translation (it is, however, what UniDiffuser initially can do), only generating text inputs as diffusion modal conditions is limited. \\n\\nFor the evaluations, the current results only include traditional short QA or image captioning tasks. However, one of the advantages of LLMs is that they are mighty at world knowledge and long-context reasoning. It would be better if results on modern benchmarks like MM-Vet and MMBench were provided. Also, the most commonly used VQAv2 results are not provided.\\n\\nOn the other hand, since the method converts images to texts to obtain multimodal understandings, lots of information may be diluted or missing. It will harm the understanding that requires detailed knowledge, such as OCR-related tasks (results on TextVQA are also encouraged).  Besides, results on commonly used text-to-image generation benchmarks are required, such as COCO FID results. \\n\\nIt is somewhat overclaimed to criticize the large data requirement of CLIP/ImageBind models since UniDiffuser is also trained on large-scale corpora using large amounts of resources that cannot be affordable by most labs.\\n\\nMissing citations and discussions. The method of first converting images to texts and then feeding them to LLMs is the same as Img2LLM [A], but it is not cited or discussed. Besides, several closely related concurrent works should be discussed [B-F].\\n\\n[A] From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models.\\n[B] DreamLLM: Synergistic Multimodal Comprehension and Creation.\\n[C] NExT-GPT: Any-to-Any Multimodal LLM.\\n[D] MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens.\\n[E] Generating Images with Multimodal Language Models.\\n[F] Kosmos-G: Generating Images in Context with Multimodal Large Language Models.\", \"Questions\": \"- What about COCO captioning results on `test` split?\\n- Is is possible if some language capability can be provided? Such as MMLU and HellaSwag benchmarks.\"}"}]}
{"paper_id": "eIYDKNqXuV", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Unsupervised clustering is essential for understanding the inherent patterns within datasets, especially when dealing with unlabeled or partially labeled data. Traditional methods like K-Means, DBScan, and fuzzy C-Means often face challenges with non-linear and complex datasets. Advanced methods like t-SNE, UMAP, and spectral clustering tackle such challenges but at high computational costs. There's a need for methods that are efficient and scalable for large and complex datasets.\nGap: Existing advanced clustering methods for non-convex or complex manifold data structures are often computationally intensive and require extensive hyperparameter tuning. This limits their scalability and efficiency with large-scale datasets or those with complex structures.\nHypothesis: The authors propose 'Village-Net Clustering,' a two-phase clustering algorithm that combines the simplicity and speed of K-Means with a novel community detection method to effectively cluster complex datasets. The method autonomously determines the number of clusters, promising both computational efficiency and scalability.\nMethod: Village-Net Clustering starts with K-Means to divide the dataset into partial clusters called 'villages.' These villages then form nodes in a weighted network indicating proximity relationships. The Walk-Likelihood Community Finder (WLCF) algorithm is used to detect communities within this network. The method can determine the number of clusters automatically and has a time complexity of O(N*k*d), making it efficient for large datasets. The experimental setup involved benchmarking against several real-world datasets (such as MNIST and Letters) to evaluate performance using metrics like NMI.\nConclusion: Village-Net Clustering demonstrates competitive performance with state-of-the-art methods, particularly excelling in speed and efficiency. While it performs well on real datasets, its accuracy can be limited by the initial K-Means step. The method's scalability and runtime efficiency provide a significant advantage, though future work may focus on increasing accuracy through iterative refinement of cluster representations.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The proposed method seems to be accurate and efficient in practice.\", \"Weaknesses\": \"1. The contribution of the paper is not so clear. The idea of over-clustering + merging is not new, which can be find in many previous methods, like BIRCH.\\n2. The proposed method is a combination of different techniques, but it is not clear why this combination is unique and which specific unsolved problem it can handle.\\n3. The comparison methods are weak, only K-means and DBSCAN.\\n4. The performance of the proposed method strongly depend on K-means and hyper-parameters. \\n5. The authors violated the double blind rule.\", \"Questions\": \"See above weakness.\"}"}]}
{"paper_id": "ttRSBZiCiu", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Recent advancements in training algorithms and network architectures have enabled the production of high-fidelity audio by deep generative models, particularly in the field of speech synthesis. Previous approaches include autoregressive models that produce audio waveforms from mel-spectrograms, flow-based models that require invertibility and are constrained by design, and Generative Adversarial Networks (GANs) that efficiently generate high-quality waveforms. Diffusion probabilistic models have been popular due to their ability to transform prior distributions into complex audio distributions without the need for extensive parameterization, although they are often computationally expensive during inference.\nGap: Despite improvements in generative model architectures and inference speed, there is a need for more efficient and flexible models that can produce high-quality audio in fewer steps without the typical constraints of invertibility (as in flow-based models) or the extensive computational requirements of diffusion models.\nHypothesis: The authors propose that a new model, WaveFluid, which learns a velocity field through adversarial training rather than relying on solving a pre-defined partial differential equation, can efficiently synthesize high-fidelity audio from mel-spectrograms. By leveraging a two-stage model and reparameterization techniques, WaveFluid aims to improve memory efficiency and training speed.\nMethod: WaveFluid uses a two-stage adversarial model conditioned on mel-spectrograms for speech synthesis. The first stage is a deterministic function that up-samples mel-spectrograms, and the second stage is a probabilistic refiner that uses a learned velocity field to generate waveforms. Adversarial training is employed to directly learn the velocity field, and reparameterization techniques are used to reduce the memory footprint and improve training efficiency. Modular design features like attention blocks, up-sampling, and down-sampling shortcuts are introduced to enhance model performance.\nConclusion: The WaveFluid model demonstrates competitive performance in generating high-fidelity audio compared to previous diffusion and GAN-based vocoders, notably achieving comparable quality with only one inference step. This model shows promise in efficiently generating high-quality audio with reduced computational overhead, presenting an effective alternative to existing methods in speech synthesis.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 5, \"Strengths\": \"1.The physical perspective of using fluid equations to elucidate the modeling of probability distributions in generative models is highly innovative. It provides a theoretical foundation for better analyzing existing generative models and designing more optimal ones.\\n\\n2.The article offers a comprehensive mathematical analysis of using fluid equations to explain diffusion models and Poisson flow models, providing strong evidence for this analytical approach.\\n\\n3.The proposed model achieves a good balance between audio quality in generation and generation speed. The provided samples sound of good quality.\", \"Weaknesses\": \"1.The experimental part is relatively limited. More advanced GAN-based vocoders, such as BigVGAN, may be compared as baselines. And additional metrics like MCD may be calculated. For results unseen speakers, evaluating speaker similarity with metrics like SMOS would also be valuable.\\n\\n2.There's a lack of analysis concerning the model's size, including an analysis of the impact of different parameter counts on the model's performance and an analysis of the effect of varying parameter sizes of different baselines.\\n\\n3.As described in the article, this method is only applicable to specific tasks with small information gaps between conditions and data (such as mel-conditioned audio generation) and is not suitable for general-purpose generation. This limitation restricts its broader application.\", \"Questions\": \"Why are different datasets used for subjective and objective evaluations of unseen speaker results? Both results on LibriSpeech and Speech Command may be used to calculate MOS and objective metrics.\"}"}]}
{"paper_id": "aAE44ivBtx", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Machine learning models, especially deep learning ones, are vulnerable to data poisoning attacks where adversaries inject malicious samples to alter model behavior. Clean-label poisoning is a sophisticated variant where such perturbations are human-imperceptible, making them harder to detect and filter out.\nGap: Existing defenses against clean-label poisoning either lack provable guarantees or significantly impact model utility. Some defenses rely on impractical assumptions about the attacker's capabilities or specific knowledge about the test input.\nHypothesis: A pre-trained diffusion model can sanitize training data by removing adversarial perturbations, acting as a certified defense against clean-label poisoning attacks while maintaining high model utility.\nMethod: The authors propose using a denoising diffusion probabilistic model (DDPM) to pre-process training data by removing adversarial perturbations before the model training. This approach decouples the defense from the training algorithm, allowing for certification. They test their defense on CIFAR-10 against seven clean-label poisoning attacks in different scenarios, comparing clean accuracy and attack success rates against other defenses.\nConclusion: The proposed diffusion model-based defense effectively reduces the success of targeted clean-label poisoning attacks to 0\u201316% with minimal impact on test accuracy. It outperforms existing poisoning defenses by maintaining higher model utility and should be used as a baseline for developing stronger attacks and defenses.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The paper introduces a defense method based on DDPM to counter clean-label poisoning attacks, and it demonstrates strong performance against various attack methods.\", \"Weaknesses\": \"1. The primary results presented in Table 1 are obtained through empirical experiments rather than relying on certified defense methods that guarantee a specific radius where the model is proven to provide the correct predictions. I would suggest a mathematical formulation to provide a certified bound.\\n\\n2. Randomized smoothing has already been established as an effective technique for ensuring l2 or l_infinite robustness against various types of perturbations, including backdoor perturbations. Moreover, it offers a clear and established framework for calculating certified l2 or l_infinite robustness bounds. The novelty of this paper is somewhat limited, as its core idea revolves around utilizing randomized smoothing to provide certified defense specifically against backdoor perturbations.\\n\\n3. Experiments on the indiscriminate data poisoning [1] should also be included.\\n\\n[1] Yiwei Lu, Gautam Kamath, and Yaoliang Yu. 2023. Exploring the limits of model-targeted indiscriminate data poisoning attacks. In Proceedings of the 40th International Conference on Machine Learning (ICML'23), Vol. 202. JMLR.org, Article 949, 22856\u201322879.\", \"Questions\": \"See weakness above.\"}"}]}
{"paper_id": "2PKZtPMyvI", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Transformers have become a dominant framework in sequential modeling, particularly in NLP, outweighing models like CNNs due to their ability to handle large datasets effectively. They are used in models such as BERT and GPT-3, and various innovations like Reformer and Linformer have been developed to enhance performance by reducing computational complexity.\nGap: Despite the widespread use of transformers and numerous architectural advancements, their underlying operational mechanics remain not well-understood, particularly how embedding dimensions and layer depth affect performance.\nHypothesis: The intrinsic dimension (ID), representing the minimum parameter count for effective modeling, can provide insight into the geometrical characteristics of data representations learned by Transformers, revealing trends in embedding dimensions, layer depth, and task performance.\nMethod: The study uses intrinsic dimension (ID) analysis to explore the geometric characteristics of data representations in Transformers. It conducts several text classification experiments using the TwoNN method to estimate IDs across different layers of Transformer models and examines relationships between ID, embedding dimensions, and classification performance across three datasets: IMDB, AG, and SST2.\nConclusion: The study finds that higher terminal feature IDs in Transformers correlate with lower classification error rates, opposite to CNNs. ID tends to decrease with layer depth, especially in complex architectures. Nonlinear dimension reduction is achieved in Transformers, with varying sequence lengths impacting ID and performance, supporting the efficacy of data reduction in training.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 1, \"Confidence\": 4, \"Strengths\": \"None\", \"Weaknesses\": \"This paper seems to be a study. And an incomplete one. It goes from related work where they describe the TwoNN method proposed by another paper, to experiment results. There is no technical section? It is also written kind of strangely.\", \"Questions\": \"See weaknesses\"}"}]}
{"paper_id": "gENfMmUIkT", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Smart surveillance systems are increasingly necessary for urban safety and are often implemented using the IoT paradigm, which integrates edge and cloud computing. These systems face challenges such as resource management, bandwidth, and message latency, which are critical for effective object detection using convolutional neural networks (CNNs) on resource-constrained devices.\nGap: While many smart surveillance systems employ strategic pipeline steps to enhance efficiency and accuracy, they typically rely on hybrid edge-cloud architectures. There is a lack of solutions that address computational constraints specifically focusing on multi-class object detection solely on edge devices, without the reliance on cloud-based resources or GPUs.\nHypothesis: The authors propose that a pipeline-based approach to object detection on edge devices can preserve detection accuracy while significantly improving processing speeds and reducing computational burdens, making it suitable for use in resource-constrained IoT scenarios.\nMethod: The proposed method involves a multi-step pipeline leveraging object size and correlation to optimize processing time and accuracy. The embedded pipeline involves steps for detection of people, cars, guns, and license plates, selectively zooming and processing based on detected context. YOLO-based CNNs (specifically YOLOv3 and YOLOv4 Tiny versions) are used, trained on a dataset of 135,000 images. The pipeline was evaluated using real hardware consisting of a Labrador 641 single-board computer in a field-proven environment. The setup included a USB camera and additional support from a microprocessor-based subsystem for feature collection and communication.\nConclusion: The pipeline approach outperformed traditional setups by reducing execution time by a factor of three and enhancing mean average precision of object detection. The system was effectively used in a surveillance context, confirming its suitability for IoT devices and edge computing applications, providing significant improvements in practical testing scenarios.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 1, \"Confidence\": 5, \"Strengths\": \"-\\tThe paper addresses a significant issue concerning object detection on resource-constrained IoT devices in the context of smart surveillance systems. Ensuring safety in metropolitan areas is a pressing concern, and the proposed approach aligns with the needs of smart cities and law enforcement.\\n-\\tCollaborative testing with the police department in real-world scenarios adds credibility to the research and demonstrates the practical applicability of the proposed approach.\", \"Weaknesses\": \"-\\tThe work seems to be an implementation of already existing and popular YOLO architecture on custom dataset.\\n-\\tThe work does not compare the work with other works that have been developed for surveillance.\\n-\\tThe pipeline-based approach uses multiple runs of the YOLO model to detect objects which adds more time delay to detection missing other important potential events while processing one event.\\n-\\tWhile the paper provides some quantitative data, a more detailed analysis of the results, especially in the pilot testing, would enhance the paper's completeness.\", \"Questions\": \"-\\tHow many classes are present in the custom dataset being used in this work? \\n-\\tIs the model trained on edge device (Labrador)? Or was it just used for Inference?\\n-\\tWhat is the frame rate used in this methodology for detection? \\n-\\tWhat measures have been taken to detect occluded objects?\\n-\\tSince the use case is a mobile surveillance and involves testing in real world scenario, what are the processes in place to identify false events transmitted?\\n-\\tHow do you keep up with the event generation when subsequent runs are needed? E.g. For generating an event from S3, you would need 8 seconds(1.5s+ 0.5s +6s). Does the system not detect any other events while it is processing one event?\"}"}]}
{"paper_id": "JzvIWvC9MG", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Game theory provides a mathematical framework for modeling interactions among self-interested agents, typically leading to Nash equilibria. However, the parameters of the payoff functions driving these interactions are often unobserved in real situations, necessitating methods for inverse game theory, which infers these parameters from observed equilibria.\nGap: Existing methods for inverse game theory, particularly in multiagent environments, struggle with computational complexity and accuracy, especially when dealing with stochastic elements and real-world data with unclear payoff structures.\nHypothesis: The authors hypothesize that formulating inverse game theoretical problems as generative-adversarial min-max optimization problems can allow for efficient (polynomial-time) computation of inverse Nash equilibria and related predictions.\nMethod: The authors develop polynomial-time algorithms relying on min-max optimization frameworks that utilize exact or stochastic first-order oracles to infer game parameters from observed equilibriums. Specifically, they extend these methods to inverse multiagent simulacral learning, aiming to replicate given observations using computed parameters and equilibria.\nConclusion: The proposed methods demonstrate effective prediction on synthetic economic games and outperform traditional ARIMA models in forecasting electricity prices using Spanish market data, indicating both enhanced computational efficiency and prediction accuracy in complex multiagent settings.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 3, \"Contribution\": 4, \"Rating\": 10, \"Confidence\": 3, \"Strengths\": \"The simple formulation of the set of inverse Nash equilibria (NE) as a min-max\\ngame is elegant and appears to be original. If it is indeed original, for this\\nalone, the paper merits publication and should be highlighted.\\n\\nThe paper overall is well written and showcases immediate applications of the\\nproposed solution to an important and practical domain as a proof-of-concept. I\\nbelieve these results are significant and will be impactful.\", \"Weaknesses\": \"As an easily rectified issue, Figure 1 could have been better represented by\\nplotting residuals over time or, by subsampling the data, plotting mean\\nresiduals with error bars.\\n\\nAs a minor complaint, I do not prefer the language of \\\"generative-adversarial\\\"\\n(especially not in terms of a \\\"discriminator\\\"), even if this is the closest\\nanalogy familiar to machine learning practitioners: This is a standard min-max\\noptimization problem that need not be wed to the ML setting.\", \"Questions\": \"Remark 1 is indeed interesting, but it is not obvious. Did I miss an associated\\nproof or example?\\n\\nWhy was the proof of Theorem 3.2 omitted? Was it rephrased to appear as Theorem\\n6.1 in the supplementary material?  Establishing the convergence rates of\\nvarious algorithms is not my expertise, but the results seem reasonable,\\nespecially given assumptions of convexity and Lipshitz smoothness.\"}"}]}
{"paper_id": "5rrYpa2vts", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The proliferation of fake news on social media has become a significant issue, with substantial efforts made over the past decade in developing methods for detecting fake news. Initial efforts included using manually crafted textual features, and later, deep learning methods like LSTM and RNN, which evolved to incorporate external knowledge alongside textual features. Recent models like KAN and Dual-CAN have attempted to integrate external evidence and social information, respectively, to improve the accuracy of fake news detection.\nGap: Current fake news detection models have limitations in maintaining longer text dependencies and capturing complex semantic relationships. They also face challenges in integrating external knowledge efficiently. Models like KAN and FinerFact have shown reliance on single-entity contexts or socially sourced claims, which can be less reliable or manipulated.\nHypothesis: The authors propose that by incorporating Abstract Meaning Representation (AMR) and external knowledge from Wikidata, specifically through a novel graph representation and evidence linking algorithm, a more robust and accurate model for fake news detection can be developed.\nMethod: The proposed model, EA2N, combines a language encoder and a graph encoder to integrate transformer-encoded textual features with affective lexical features. The graph encoder constructs an AMR graph, enriching it with external knowledge from Wikidata to form a WikiAMR graph. A path-aware graph learning module is used to capture semantic relationships among entities. The model predicts the veracity of news by integrating features from both the language and graph encoders, using a transformer for classification. The model was evaluated using datasets from PolitiFact and GossipCop, comparing its performance to state-of-the-art models.\nConclusion: EA2N, leveraging AMR and external knowledge, outperformed state-of-the-art models in fake news detection on benchmark datasets. The integration of evidence through the WikiAMR graph was shown to be effective, suggesting this method's potential for improving semantic understanding in NLP applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"- The authors provide an effective amalgamation of the graph- and text-based pipeline for learning representation (although not entirely novel).\\n- The figures and tables give a clear picture of the underlying model and experimentation.\\n- The authors perform the much required ablation studies.\", \"Weaknesses\": \"The proposed architecture seems to be an attempt to combine several pre-existing architectures -- i.e. Language Encoder (BERT [6] + FakeFlow [7]), Path-aware Graph Learning Module (Graph Transformer [8]) and Graph Generation and Integration ($\\\\mathcal{A}^*$ search with a TagMe API [9] -based heuristic). While these architectures have been amalgamated to propose a new model, these aren't *novel* contributions. The paper isn't well written (in some places) in the sense that several key notations are missing, and the narrative of the work could have been improved. All the baselines have been directly adopted from the FinerFact [5] paper. Such practices should be avoided. I have highlighted several weaknesses and concerns under the \\\"Questions\\\" section.\\n\\n\\n[1] Zixuan Zhang and Heng Ji. 2021. Abstract Meaning Representation Guided Graph Encoding and Decoding for Joint Information Extraction. (NAACL-HLT 2021)\\n\\n[2] Shu, Kai, et al. \\\"Fakenewsnet: A data repository with news content, social context, and spatiotemporal information for studying fake news on social media.\\\" Big data 8.3 (2020): 171-188.\\n\\n[3] Martinez-Rodriguez, Jose L., Ivan L\u00f3pez-Ar\u00e9valo, and Ana B. Rios-Alvarado. \\\"Openie-based approach for knowledge graph construction from text.\\\" Expert Systems with Applications 113 (2018): 339-355.\\n\\n[4] Yi-Ju Lu and Cheng-Te Li. GCAN: Graph-aware co-attention networks for explainable fake news detection on social media. In ACL, pp. 505\u2013514, Online, July 2020. ACL.\\n\\n[5] Yiqiao Jin, Xiting Wang, Ruichao Yang, Yizhou Sun, Wei Wang, Hao Liao, and Xing Xie. Towards fine-grained reasoning for fake news detection. AAAI, 36:5746\u20135754, 06 2022.\\n\\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL Volume 1 (Long and Short Papers), pp. 4171\u20134186, Minneapolis, Minnesota, June 2019. ACL.\\n\\n[7] Bilal Ghanem, Simone Paolo Ponzetto, Paolo Rosso, and Francisco Rangel. Fakeflow: Fake news detection by modeling the flow of affective information. In 16th EACL, 2021.\\n\\n[8] Deng Cai and Wai Lam. Graph transformer for graph-to-sequence learning. In AAAI, pp. 7464\u2013 7471. AAAI Press, 2020.\\n\\n[9] Paolo Ferragina and Ugo Scaiella. Tagme: On-the-fly annotation of short text fragments (by wikipedia entities). In ICIKM, CIKM \u201910, pp. 1625\u20131628, New York, NY, USA, 2010. ACM. ISBN 9781450300995.\", \"Questions\": \"**Evidence Integration with AMR**: There seem to be several incomplete parts, which need to be explained.\\n\\n---  In Entity-level Filtering $(\\\\mathcal{R}^{(S,D)}_{ELF} = Relatedness(v_s^{wiki}, v_d^{wiki}))$, the authors have not mentioned what the $Relatedness(.)$ function is? Without an ***explicit*** definition of the function, it is diffiucult to assess the working of the ELF and CLF algorithms since the $Relatedness(.)$ function seems to be the \\\"main\\\" heuristic being used here.\\n\\n- From a look at the Appendix, it seems (***implicitly***) that the authors use the TagMe API to compute the relatedness, but what \\\"explicitly\\\" is the function definition used? \\n\\n--- The Figure 2 (and following text) mentions that the authors use $\\\\mathcal{A}^*$ search on the Wiki-graph to find the (optimal) path between source $v_s^{amr}$ and destination $v_d^{amr}$. However the Algorithm 2 described as \\\"Context-level Filtering\\\" doesn't seem to represent $\\\\mathcal{A}^*$ search. \\n\\n- For $\\\\mathcal{A}^*$ search, $f$ (total cost) = $g$ (current cost) + $h$ (heuristic approximation of the future cost). Here, $f$ (total cost) must be the criterion for choosing the next node in path. If we assume that, $h = \\\\mathcal{R}^{(i,D)}_{CLF} = Relatedness(v_i^{wiki}, v_d^{wiki}) > \\\\delta$ is the heuristic cost to reach the destination $d$ from $i$, what is $g$? The authors don't provide information for that. Incase $g$ is being ignored (or taken 0), it becomes a *greedy-heuristic* search, and not $\\\\mathcal{A}^*$. \\n- A possible $\\\\mathcal{A}^*$ variant could have been:\\n\\n$g + h = \\\\mathcal{R}^{(S,i)}\\\\_{CLF} + \\\\mathcal{R}^{(i,D)}\\\\_{CLF} = Relatedness(v_s^{wiki}, v_i^{wiki}) + Relatedness(v_i^{wiki}, v_d^{wiki}) > \\\\delta*$\\n\\n\\n**Relation Encoder in Path-aware Graph Learning Module**: For every pair of entities $(v_s^{amr}, v_d^{amr})$ in $\\\\mathcal{G}^{amr}$, two distinct sources of relational data exist in $\\\\mathcal{G}^{WikiAMR}$: AMR-based relations and evidence-based relations from Wikidata:\\n\\n- Using the notation from paper: $\\\\mathcal{G}^{WikiAMR} = \\\\mathcal{G}^{amr} \\\\cup \\\\sum_{s, d} \\\\mathcal{P}^{wiki}(v_s^{amr}, v_d^{amr})$.\\n- Consider the shortest path denoted as $sp_{s \\\\rightarrow d} = \\\\set { e(v_s^{amr}, n_1), e(n_1, n_2), \\\\dots, e(n_k, v_d^{amr})\\\\}$ within $\\\\mathcal{G}^{WikiAMR}$. Here, $e^{amr}$ represents an edge within $\\\\mathcal{G}^{amr}$, and $e^{wiki}$ represents an edge within $\\\\sum_{s, d} \\\\mathcal{P}^{wiki}(v_s^{amr}, v_d^{amr})$. Therefore, the collective edge set $e^{WikiAMR} = e^{wiki} \\\\cup e^{amr}$.\\n- Let $sp^{amr}\\\\_{s \\\\rightarrow d} = \\\\set {e^{amr}(v_s^{amr}, n_1), e^{amr}(n_1, n_2), \\\\dots, e^{amr}(n_k, v_d^{amr})\\\\}$ be the shortest path that relies solely on AMR-based relations (i.e., exclusively within $\\\\mathcal{G}^{amr}$). Simultaneously, $sp^{wiki}\\\\_{s \\\\rightarrow d} = \\\\set {e^{wiki}(v_s^{amr}, n_1), \\\\dots, e^{wiki}(n_k, v_d^{amr})}$ represents the shortest path utilizing only evidence-based connections.\\n- As indicated by Table 2 and Figure 5, a significant portion of entities are linked through evidence-based connections with just one hop, meaning they are connected directly. In other words, for most entity pairs, the shortest path is the \\\"direct\\\" evidence path with a single edge originating from the Wiki graph. This can be represented as $sp_{s \\\\rightarrow d} = sp^{wiki}\\\\_{s \\\\rightarrow d} = \\\\{ e^{wiki}(v_s^{amr}, v_d^{amr})\\\\}$ (Because, as per Table 2, in most cases, the number of edges in $sp^{wiki}\\\\_{s \\\\rightarrow d}$ (being 1 in majority cases), is less than or equal to that in $sp^{amr}_{s \\\\rightarrow d}$.\\n\\nIn essence, this implies that in most cases, when encoding a relationship $r_{s \\\\rightarrow d}$, the relation encoder would **overlook the AMR-based relation information**. While Wikidata relations are valuable, disregarding AMR-level relations may not be technically justified. Therefore, the authors should contemplate how to adapt the relation encoder in the graph transformer to effectively integrate both information sources.\\n\\n**Abstract Meaning Representation**: I have a few major concerns about using an AMR representation here:\\n\\n- The average length of news articles in the Gossipcop dataset is **~600 words** [2] (Some articles are as large as 1000 words). In the case of such large input length, the AMR graphs are going to be \\\"very\\\" noisy. How do the authors handle this case?\\n- The authors don't elaborate on the intuition behind using the Abstract Meaning Representation (\\\"Why specificaly AMR?\\\")? There can be other (more sophisticated) variants of AMR like **AMR-IE** [1] (which uses an AMR guided graph decoder to extract knowledge elements based on the order decided by the hierarchical structures in AMR) which seem more relevant owing to the \\\"integration of external knowledge\\\" used in this work. What about knowledge graphs other than AMR (eg. OpenIE-based approaches [3])?\\n- It seems that the authors have used an off-the-shelf pretrained AMR parser, however they have not provided any details about the same. Was the AMR parser finetuned? (I am guessing not!)\\n\\n**Insufficient Experimentation**: \\n\\n- The presented results are exclusively based on two datasets, Politifact and Gossipcop, both of which are part of the FakeNewsNet database. To ensure the generalizability of the proposed architecture, it is essential for the authors to include results from additional datasets spanning various domains or social media platforms. Moreover, it's worth noting that Politifact, one of the datasets, contains only 815 news articles.\\n- Several of the selected baseline models do not provide a fair basis for comparison. For instance, GCAN [4] incorporates the propagation path of the tweet and user profiles in addition to the source tweet content, features not utilized by the proposed model.\\n- A logistical concern arises from the fact that all the baseline models and their results, including SVM, DTC, RFC, GRU-2, FF, B- TransE, KCNN, GCAN, and KAN, appear to **have been directly borrowed from the FinerFact** [5] paper. Such practices should be avoided, and the authors should explicitly mention the sources of these baseline models in their paper.\\n- In the ablation study examining different variants of the proposed model, such as LM, AMR, LE|AMR, and LE|WikiAMR, the authors should provide significance hypothesis test results (e.g., T-test) alongside the standard deviation of the metrics across multiple runs. Including these statistical measures would enhance the interpretability of the results.\"}"}]}
{"paper_id": "i7LCsDMcZ4", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Event cameras, which are inspired by biological visual sensors, capture changes in brightness at each pixel asynchronously, mimicking the human eye. They offer benefits like low latency and high dynamic range, making them suitable for challenging conditions. Spiking Neural Networks (SNNs), modeled on how biological neurons operate, are designed to handle such event-based data. Despite this, SNNs face overfitting issues and lack saliency map generation methods, which are crucial for effective data augmentation and generalization.\nGap: There is no existing method for extracting saliency maps and class activation maps (CAMs) from Spiking Neural Networks (SNNs), which is a critical step for effective data augmentation and reducing overfitting in event-based data classification.\nHypothesis: The authors propose the hypothesis that by using relevance propagation techniques (Spiking Layer-Time-wise Relevance Propagation - SLTRP, and Spiking Layer-wise Relevance Propagation - SLRP), they can accurately generate saliency maps and class activation maps (CAMs) for SNNs. This will enable more effective event data augmentation, improving the generalization ability and performance of SNNs in classification tasks.\nMethod: The authors introduce two relevance propagation methods: SLTRP and SLRP, adapted from Layer-wise Relevance Propagation (LRP) for SNNs. SLTRP provides temporal precision, while SLRP is computationally less intensive. These methods are used to generate stable and accurate CAMs and saliency maps. Using these maps, the authors develop two data augmentation techniques: RPGDrop, which removes events in regions of high saliency, and RPGMix, which combines events while maintaining clear texture features by avoiding overlapping the most salient object areas. The methods are integrated into a framework called EventRPG, which combines RPGDrop and RPGMix with traditional augmentation techniques like geometric augmentations.\nConclusion: EventRPG, using the newly proposed SLTRP and SLRP methods, significantly enhances the generalization capability of SNNs. The approach achieves state-of-the-art performance on several event-based object and action recognition tasks, demonstrating that SNNs can be trained more effectively with relevance-guided data augmentation. However, the application is currently limited to classification tasks, suggesting potential future work in expanding these methods to other neural network tasks beyond classification.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"- The derivation of the CAM and saliency map of SNNs itself is a clear contribution. The results in Table 1 prove the correctness of this\\n- EventRPG is able to consistently improve performance across event datasets\\n- The time cost of EventRPG is comparable to similar augmentations in conventional vision\", \"Weaknesses\": \"My main concern is regarding the experiments:\\n- The paper motivates the need for event data augmentation with the statement that \\\"the lack of huge event-based datasets similar to Imagenet prevents us from improving the model performance on relatively small datasets using a Pretrain-Finetune paradigm\\\". However, there is an event camera version of ImageNet available [1], and its paper shows that pre-training on N-ImageNet can greatly improve the accuracy on other datasets via transfer learning. Therefore, this statement in the Introduction is wrong\\n- Since N-ImageNet is available, I would like to see results on this dataset. This is similar to conventional vision research on data augmentation, where ImageNet is the best testbed. If the authors are not able to train on N-ImageNet, the mini subset can be considered, though I do not think that is a comprehensive benchmark\\n- The authors compare EventRPG with conventional vision methods such as Grad-CAM in Table 1. Thus, I wonder if it is possible to compare with Puzzle Mix and SaliencyMix in Table 3?\\n- NDA applies the method to unsupervised contrastive learning and shows promising results. Is it possible to conduct such experiments using EventRPG?\\n\\n[1] Kim, Junho, et al. \\\"N-ImageNet: Towards robust, fine-grained object recognition with event cameras.\\\" ICCV. 2021.\", \"Questions\": \"See Weaknesses\"}"}]}
{"paper_id": "98g9NdJPxm", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Federated learning (FL) is a collaborative learning paradigm designed to protect data privacy by training models locally on devices and only sharing model updates with a central server. Despite this, recent research has shown that FL is vulnerable to data reconstruction attacks, where adversaries can reconstruct training data from shared model updates.\nGap: Existing research on data reconstruction attacks in federated learning lacks a theoretical framework to understand the attacks' effectiveness and limitations. Most current work is empirical, which makes it difficult to fairly compare different attack methods and understand the extent of data reconstruction possible under these attacks.\nHypothesis: The authors propose that a theoretical framework can be developed to accurately bound the data reconstruction error for various attacks in federated learning, allowing for a fair comparison of their effectiveness. They hypothesize that an attack's effectiveness can be reflected in its reconstruction error bounds.\nMethod: The authors introduce a theoretical framework to analyze data reconstruction attacks in FL. This involves deriving a bound for the reconstruction error based on the Lipschitz constant of the reconstruction function. They detail the conditions under which their theoretical results hold. The framework is tested on various datasets using current attack methods like DLG, iDLG, and InvGrad. They employ a custom unrolled feed-forward network to compute the upper bound of the Lipschitz constant necessary for error estimation.\nConclusion: The theoretical framework allows for a systematic comparison of different data reconstruction attacks in FL. It confirms that certain methods, like iDLG, have inherent advantages over others like DLG in terms of attack effectiveness. The framework's results are consistent with empirical observations on dataset reconstructions, though there is some discrepancy in other cases. The study sets a foundation for future work to refine these bounds and extend the framework to non-convex losses.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The clarity is good. The flow of the writing is smooth.\\n2. In the introduction section, the paper points out three meaningful limitations of existing attack methods: high sensitivity to the initialization; evaluation highly depending on the choice of model snapshot; and lack of theoretical analysis.\", \"Weaknesses\": \"My main concern is about the significance. In details\\n1. Theorems on the attack upper bound need unrealistic assumptions.\\n-  The FL model is required to be strongly convex and smooth, etc. This is far away from the scenario where those attack methods are located -- those attack methods are commonly evaluated for deep neural networks as the input is the image in the evaluation.\\n- The reconstruction method is assumed to have a Lipschitz constant. However, the attack methods studied in the paper are optimization-based attacks and do not have an explicit Lipschitz constant. Although the paper proposes a method to approximately calculate the constant, there's no guarantee how large the approximation error would be. This might introduce an additional arbitrarily large noise to calculate the upper bound.\\n2. As shown in the empirical results, there is a large gap between the upper bound and the attack performance. It might be hard to indicate a useful conclusion from this bound.\", \"Questions\": \"Please see the weakness above.\"}"}]}
{"paper_id": "PhMrGCMIRL", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The development of AI models that generalize across tasks and domains has been a longstanding challenge in AI research. Traditional models trained using empirical risk minimization often face performance degradation when the test data distribution differs from the training data. Foundation Models have enabled the creation of expert models through fine-tuning, but single experts still struggle with generalization across diverse data distributions. Combining experts with complementary expertise to enhance generalization is a common practice in machine learning, but existing methods often assume that experts are trained and tested on the same distribution.\nGap: The paper identifies a gap in existing methods for combining expert models, specifically when these experts have complementary expertise across different data domains. Traditional ensembling and Mixture of Experts models often assume experts are trained and tested on similar data distributions, which can lead to poor performance when generalizing to out-of-distribution data where none of the experts perform well individually.\nHypothesis: The authors propose that fusing pre-trained expert models with complementary expertise into a new 'Fusion of Experts' (FoE) model can improve performance on data distributions that contain a mix of the domains for which the experts were originally trained. This approach is expected to perform better than using individual experts alone or traditional ensembling techniques.\nMethod: The Fusion of Experts (FoE) approach reformulates the problem of combining expert outputs into a supervised learning task. The method is applicable to both discriminative and generative tasks. It involves training a 'fuser' model that takes in the outputs from K expert models, each trained on different data domains, and learns to predict the best output for a given input. The approach is extended to a 'frugal' setting using a sequential algorithm to minimize the number of individual expert calls during inference by solving a graph-based shortest-path problem. Experiments were conducted on tasks such as image and text classification, text summarization, and question answering.\nConclusion: The FoE method demonstrated significant improvements over individual experts, confidence-based fusion, and traditional ensemble methods across various tasks, achieving performance close to an oracle model which always selects the best expert. The FrugalFoE variant effectively reduces the computational cost by minimizing expert calls without sacrificing accuracy, especially in settings where FoE naturally exhibits frugality by relying on a subset of experts. The proposed method shows promise for enhancing model generalization across mixed-domain data distributions.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"- The paper tackles a novel and significant issue: optimally leveraging different models for diverse tasks. While foundational models generally perform well across various tasks, they have differing strengths. Thus, an approach to effectively combine these models represents a significant advancement.\\n- The methodology, FrugalFoE, is both technically sound and innovative. It offers a clear problem formulation and further minimizes the need to query all the expert models without sacrificing accuracy.\\n- The experiments are extensive and cover a range of applications, including image classification, summarization, MMLU, and text evaluation. This demonstrates the method\u2019s versatility and effectiveness.\\n- The paper effectively situates its work within the broader context of ensemble learning, mixture of experts, and federated learning, thoughtfully introducing these methodologies and their limitations.\", \"Weaknesses\": \"- A primary limitation is the assumption that data for training the fuser are readily available. The proposed approach requires a labeled dataset to train the fuser network. In the discriminative case, we feed the input example to different individual models, take model outputs as the inputs to the fuser network, and train the fuser to predict the label of the input example. Similarly, in the generative case,  we feed the fuser network with individual model outputs and train the fuser to predict the best model index, which is obtained by feeding the labeled data to individual models and selecting the model that achieves the best performance. This prerequisite may not be realistic in practical scenarios, where there is no or only a few labeled data. If a large labeled dataset is available, it might be more efficient to fine-tune a foundational model or employ few-shot learning. A pivotal aspect of this research should be the generalization capabilities of the trained fuser across different tasks (e.g,. train the fuser on some tasks and test the fuser on unseen tasks), which would significantly enhance the paper's contribution.\\n- The paper lacks an in-depth analysis of the fuser network. Although it is described as a lightweight MLP network, there's no exploration of how different architectures (e.g., simpler networks like linear models or more complex ones like transformers) might impact performance. Additionally, details on the training configurations, such as learning rate, epochs, dataset, and stopping criteria, are missing. The influence of the $K$ parameter in the $K$-NN component of the fuser network is also unclear. What is the rationale for choosing $K=9$?\\n- The experiment sections lack in-depth descriptions (see questions). The paper would benefit from reallocating less critical sections (like the connection between FrugalFoE and the A* algorithm) to the Appendix and expanding on the experimental details.\", \"Questions\": \"- Page 8, Table 2: Could you clarify the distinction between \\\"TFN Expert Only\\\" and \\\"TFN Expert,\\\" and between \\\"Poem Expert\\\" and \\\"Poem Expert Only\\\"?\\n- Page 8, Table 2: What are the results for the \\\"confidence-based fusion\\\" and \\\"ensemble\\\" baselines? The same question applies to Tables 3 and 4. The confidence-based fusion seems less intuitive in the generative case, how about simply selecting the maximum confidence at each decoding step?\\n- Page 9, Table 4: Could you explain more details of FoE (Expert 1), FoE (Expert 2), and FoE (Expert 3)? Why does adding more experts appear to degrade performance?\\n- Page 4: The statement \\\"As long as there is a label shift among the domains, we expect $E[f(X_k)] = E[Y_k]$\\\" needs clarification. Why is this expected?\\n- Page 6: The phrase \\\"Then $\\\\lambda$ can be interpreted as the final error rate reduction we want to achieve\\\" \u2013 could you expound on the reasoning behind this?\\n- Page 1: The statement \\\"our emphasis is on generalization to test data distributions where none of the experts perform well individually\\\". Shouldn't it be \\\"a few\\\" instead of \\\"none\\\" based on the problem formulation?\\n- Page 6: The condition \\\"If ... <0 we terminate the search\\\" should be\\\">0\\\"?\"}"}]}
{"paper_id": "y3dqBDnPay", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the importance of self-supervised representation learning for multimodal data, which involves integrating and embedding information from various sources without additional labeling. This approach is useful in diverse applications such as clustering, text-to-video retrieval, and temporal action localization. Existing methods mostly focus on pseudo-label-based and contrastive-based approaches, often failing to capture the complex high-order relationships inherent in multimodal data such as inter- and intra-modality correlations.\nGap: Existing methods in self-supervised multimodal representation learning often overlook high-order inter- and intra-modality correlations inherent in real-world multimodal data. Additionally, these methods generally lack clear principles for effective multimodal fusion, which can lead to information loss or redundancy.\nHypothesis: HyperRep is proposed as a method to address the shortcomings of existing multimodal representation learning techniques. It combines hypergraph-based modeling to capture high-order inter- and intra-modality correlations and a self-supervised multimodal fusion information bottleneck principle to constrain the solution space, ensuring a more effective fusion of multimodal data.\nMethod: HyperRep employs a hypergraph-based representation to model high-order correlations in multimodal data. The method involves constructing two types of hypergraph structures: an instance hypergraph for inter-modality relationships and a modality hypergraph for intra-modality relationships. It uses a propagation process where information from modality-specific and instance-specific hyperedges is aggregated back to vertices. Furthermore, it implements a self-supervised Multimodal Fusion Information Bottleneck (MFB) principle, which maximizes mutual information shared across modalities while minimizing modality-specific information. The method was tested on public datasets for tasks such as clustering, text-to-video retrieval, and temporal action localization.\nConclusion: The proposed HyperRep method demonstrated superior performance compared to state-of-the-art methods across several metrics on different datasets. The results confirmed that the high-order correlations and multimodal information bottleneck principle are crucial for effective multimodal representation learning. The method achieves effective and efficient embedding of multimodal data, showing potential for broad applications in machine learning.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. This paper provides sufficient and detailed formal definitions and necessary proofs.\\n2. The application of hypergraphs in this paper may inspire the field of multimodal self-supervised learning.\\n3. The authors conducted extensive ablation experiments on multiple datasets.\", \"Weaknesses\": \"1. Motivation: The paper highlights the use of hypergraphs to capture higher-order correlations among modalities. However, I find the paper insufficient in explaining and analyzing why higher-order correlations are essential for multimodal self-supervised learning. This is especially relevant since most existing methods, such as the CLIP[1] series and ImageBind[2], rely on pair-wise multimodal self-supervision.\\n2. Method: The paper could improve the clarity and presentation of MFB. For instance, how does Eq. 12 relate the mutual information of shared and instance features? A clearer explanation of the illustration would be helpful. Also, the MFB module in Figure 2 is vague, and a more specific illustration could enhance the readers\u2019 comprehension of the method.\\n3. Experiment: The paper lacks comparisons with recent (2023) methods, as the only one mentioned has a huge performance gap with the classical methods. Could the paper also compare with more popular Foundation models, such as VideoCLIP[3] and OmniVL[4]? Moreover, Table 1 shows that graph-based methods perform poorly, and Table 2 reveals a drastic performance drop after removing higher-order correlations. However, AGC achieves high performance without using higher-order correlations. Does this imply that higher-order correlations are crucial for the proposed method, but not for the multimodal self-supervised learning task? \\n\\n[1] Learning Transferable Visual Models From Natural Language Supervision. ICML 2021.\\n[2] IMAGEBIND: One Embedding Space To Bind Them All. ICCV 2023.\\n[3] VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding. EMNLP 2021.\\n[4] OmniVL: One Foundation Model for Image-Language and Video-Language Tasks. NeurIPS 2022.\", \"Questions\": \"How come the MIL-NCE performance in Table II differs so much from the original paper? Are there any differences in the experimental settings?\"}"}]}
{"paper_id": "i7P2mK3x3o", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Flow-based models are prominent in generative tasks, especially normalizing flows, where neural networks are used to transport a data distribution to a normal distribution. These models have broad applications in density estimation, generative modeling, and likelihood inference. Typical models rely on applying a neural ODE framework to achieve continuous transformation in high-dimensional spaces.\nGap: The existing flow-based models predominantly focus on transport from a data distribution to a normal distribution. However, there is a gap in the ability to transport between arbitrary distributions P and Q, particularly when both are accessible only through finite samples, without resorting to the normal distribution as an intermediary.\nHypothesis: The paper hypothesizes that a flow-based neural ODE model can be developed to compute a dynamic optimal transport map directly between two general distributions P and Q in high-dimensional space, by minimizing transport costs and using empirical samples from both distributions.\nMethod: The authors propose Q-flow, a flow-based neural ODE model that learns an invertible optimal transport map between distributions P and Q. This is achieved by minimizing the transport cost and using two sets of samples from P and Q. The model uses a neural ODE framework to train the flow network and refine it through an iterative process. The transport map is designed to satisfy the continuity equation and minimize a combination of KL divergence and Wasserstein-2 transport cost. The model is demonstrated on tasks like density ratio estimation and image-to-image translation, capitalizing on its continuous-time formulation and invertibility.\nConclusion: The method shows strong empirical performance in high-dimensional data applications, such as mutual information estimation, energy-based generative modeling, and image-to-image translation. The Q-flow model manages to efficiently and accurately learn the optimal transport between samples of different distributions, facilitating downstream tasks and potentially serving as a new tool for generative models and distribution interpolation.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"- sufficiently clearly written paper\\n- natural idea of the algorithm\\n- detailed description of the algorithm and experimental study \\n- discussion of the features of the computational implementation of the algorithm\\n- interesting practical results\", \"Weaknesses\": \"- the title of the paper is \\\"Computing high-dimensional optimal transport by flow neural networks\\\". However, a significant part of the paper is devoted to benchmarking of the capability of the algorithm to perform density ratio estimation. So it is not clear what is the main aim of the paper - to compute OT, or to estimate log density ratio\\n\\n- If the main aim is DRE, then it is necessary to provide detailed comparison with other DRE methods, as there are many papers on this topic. E.g., what is the difference of the proposed approach with the approach https://openreview.net/forum?id=kOIaB1hzaLe\\n\\n- it is not clear why the proposed algorithm estimates optimal transport\\n\\n- experimental results to verify efficiency of computed W2 high-dimensional optimal transport are not enough to claim accuracy and efficiency of the proposed approach. E.g. the authors consider some image translation tasks, but FID score used to characterise accuracy does not guarantee that the computed W2 high-dimensional optimal transport map is accurate.\", \"Questions\": \"- it is not clear how to tune a value of gamma in (3). Any recipes for automatic tuning?\\n\\n- page 9: \\\"Meanwhile, since our Q-flow model learns a continuous transport map from source to target domains, it directly provides the gradual interpolation between the source and target samples along the dynamic OT trajectory as depicted in Figure 4b.\\\"\\n\\nAny comments on why the trajectory corresponds to OT trajectory? To construct a flow the authors optimise (3), which contains two terms, and it is not clear why such optimisation formulation guarantees any optimality or that the mapped distribution coincides with the second distribution q. \\n\\n- since the authors claim they compute W2 optimal transport, it is important to benchmark their approach on problems with ground truth solutions. There exist such benchmark, see https://github.com/iamalexkorotin/Wasserstein2Benchmark (Do Neural Optimal Transport Solvers Work? A Continuous Wasserstein-2 Benchmark, NeurIPS 2021)\\n\\n- page 18 (the second line after the displayed formula 15): Why Q = N(0,I_d) if P = N(0,Sigma)?\\n\\n- page 19: it is not clear how gamma = 0.5 was selected. Why not 0.6?\"}"}]}
{"paper_id": "iI7hZSczxE", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Disentangled representation learning has been vital in fields such as computer vision and natural language processing due to its benefits in interpretability, fairness, and generalization. Previous efforts on time series have faced challenges in generalization, especially regarding energy consumption data like Non-Intrusive Load Monitoring (NILM). Standard disentanglement methods often assume statistical independence of latent attributes, which can lead to non-identifiability when real-world data exhibits correlated attributes.\nGap: Existing methods for disentangled representation learning in time series assume attribute independence, which is often not valid due to real-world correlations between attributes. This leads to issues in identifying disentangled representations and impacts the ability to handle real-world scenarios like energy consumption disaggregation.\nHypothesis: The hypothesis is that by relaxing the assumption of statistical independence to independence-of-support via contrastive learning, it is possible to achieve more robust disentangled representations for time series data with correlated attributes. This approach should enable improved identification and generalization of representations across diverse and correlated scenarios.\nMethod: The authors propose the Disentanglement under Independence-Of-Support via Contrastive Learning (DIOSC), which minimizes support overlaps of latent variables using contrastive learning. This is combined with l-variational inference layers incorporating self-attention mechanisms to address temporal dependencies and achieve robust representation learning. The method involves a single encoder/decoder for efficient disentanglement, using contrastive objectives to maintain support independence. A new metric, the Time Disentangling Score (TDS), is proposed to measure disentanglement quality.\nConclusion: DIOSC demonstrates improved disentanglement and robustness in handling correlated attributes in time series data, outperforming traditional methods by up to 61.4% in specific metrics. The approach enhances generalization on downstream tasks like NILM, highlighting the potential of contrastive learning combined with attentive l-variational inference in addressing real-world generalization challenges.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 1, \"Confidence\": 3, \"Strengths\": \"The main strength of the paper is its approach to tackling the important problem of disentangled representation learning, which may contribute to reducing carbon footprint.\", \"Weaknesses\": \"The paper lacks proper organization and has a tendency to include some unproven (or wrong) claims. For instance, in the introduction, the authors mention that \\\"Disentanglement via VAE **can be achieved** by a regularization term of the Kullback-Leibler divergence[],\\\" which is not necessarily true without certain strong assumptions and underlying conditions. The Beta-VAE paper has some qualitative evidence showing how the images are more disentangled compared to VAE. The authors also claim, \\\"Rather than training separate auto-encoders for individual appliances,\\\" which requires empirical validation with proper citations.\\n\\nIn addition to these issues, the notations used are very confusing and are not defined before they are referenced. For example, in the proposed method section, the notation $z_m^+$ is used without description it. It is also unclear how it differs from $\\\\bf{z}$ or $z$.\\n\\nThe main goal of this paper remains unclear to me. For example, the authors mentioned, \\\"The primary goals of this work are twofold: to effectively address the NILM problem and to obtain a disentangled representation of input data.\\\" However, it is unclear what the NILM problem is, what the nature of the input data is, and how the authors plan to achieve a disentangled representation that distinguishes itself from previous works. One issue might be that the problem statement and preliminaries are somewhat intertwined.\\n\\nThe color meaning used in the tables of result section is not clear. Even it is not clear how TDS (as a metric) has been compared with VAE and Beta-VAE in Table 1.\", \"Questions\": \"- I strongly suggest the authors make their main contributions clear at the end of the introduction. \\n\\n- There is no related work section.\"}"}]}
{"paper_id": "YjG29CIOP7", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Data-free Meta-learning (DFML) focuses on enabling learning from a collection of pre-trained models without accessing their original training data. It's a contrast to traditional meta-learning that relies on data from related tasks. DFML is particularly relevant in privacy-sensitive scenarios where only pre-trained models are available, such as from repositories like GitHub. The concept was introduced by works like Wang et al., 2022, and Hu et al., 2023, highlighting its capability in creating generalizing meta-learners.\nGap: Existing DFML methods face two core vulnerabilities: Task-Distribution Shift (TDS), where the distribution of synthetic tasks changes during continuous learning leading to overfitting, and Task-Distribution Corruption (TDC), where task distributions can be corrupted by model poisoning attacks via deceptive pre-trained models. These issues hamper the practical applicability of current DFML techniques.\nHypothesis: The authors propose a new approach to develop robust DFML methods that can effectively address TDS and TDC. TEAPOT leverages a memory-based strategy to handle TDS by maintaining old task memories and diversifying pseudo task distributions. ROSY, a model selection policy, is introduced to tackle TDC, ensuring reliable model selections using reinforcement learning.\nMethod: To address TDS, TEAPOT introduces a memory-based approach that maintains old tasks' memory to prevent over-reliance on new tasks. It uses interpolation with classes from different tasks to diversify pseudo task distributions. To counter TDC, the authors introduce ROSY, which uses a learnable reliability score to rank and select models, optimized via policy gradients due to the non-differentiable nature of model selection. The approach's effectiveness was validated using extensive experiments on multiple datasets, including CIFAR-FS, MiniImageNet, VGG-Flower, and CUB, demonstrating superior performance compared to previous baselines.\nConclusion: The authors' approach, TEAPOT combined with ROSY, successfully addresses the vulnerabilities of TDS and TDC in DFML. TEAPOT significantly enhances the stability and robustness of learning processes against task-distribution shifts, achieving higher accuracy and generalization. ROSY further improves robustness against task-distribution corruption by effectively filtering deceptive models. The proposed methods demonstrate a significant empowering leap for DFML, ensuring it is applicable in broader, real-world situations.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"+ The paper discusses robustness for the first time in the relatively new meta-learning problem setting of DFML.\\n+ The paper proposes an effective method for replaying past tasks in DFML and confirms its effectiveness through experiments.\\n+ The paper proposes a reinforcement-learning-based model selection method that can maintain meta-testing performance even when unreliable models are included.\", \"Weaknesses\": \"- The risk of TDC is avoidable through sanity checks with simple data collection, and thus there is little motivation to introduce the proposed method with high computational complexity using RL. In fact, the experiments provided in the paper use pre-trained models in the medical domain for the attack, but these are not difficult to determine by simply inputting data obtained from an image search engine or a text image model such as Stable Diffusion without privacy concerns. It is unnatural for the problem setting to allow data inversion for meta-learning but not to allow such a simple preprocessing. We could not agree with the practicality of TDC's problem setting unless it shows important use cases or experimental results, indicating it cannot be solved without introducing the proposed method.\\n- The replay-based method in TEAPOT has already been well studied in the context of continual learning [a], and there is nothing new about it other than the task (i.e., DFML) to which the method is applied.\\n- Section 4 is misleading because of the mixture of existing and proposed methods. In particular, the second paragraph is almost the same as in the previous study [b].\\n- Experiments are conducted only on 4-layered convolution neural networks, and the performance is not verified in a realistic setting with heterogeneous architectures. Since the paper considers a practical problem setting for DFML, experiments using more architectures would help make the paper more convincing.\\n\\n[a] Wang, Liyuan, et al. \\\"A comprehensive survey of continual learning: Theory, method and application.\\\" arXiv preprint arXiv:2302.00487 (2023).\\n\\n[b] Hu, Zixuan, et al. \\\"Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning.\\\" CVPR 2023.\", \"Questions\": \"- Can the risk of TDS be avoided by splitting the inverted dataset for validation? In the context of generative models, methods for validation and model selection using synthetic data have been proposed [c].\\n- Can model-poisoning attacks be detected as OOD by visualizing or embedding data recovered from a pre-trained model into an existing model? The previous study [b] actually visualized the restored data, and pre-trained neural networks can be used for OOD detection [d].\\n- How much does the computational complexity of ROSY reinforcement learning increase relative to the baseline (PURER)?\\n\\n[c] Shoshan, Alon, et al. \\\"Synthetic data for model selection.\\\" ICML 2023.\\n\\n[d] Lee, Kimin, et al. \\\"A simple unified framework for detecting out-of-distribution samples and adversarial attacks.\\\" NeurIPS 2018.\"}"}]}
{"paper_id": "lf8QQ2KMgv", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Recent studies have suggested that memorization in deep learning models, the ability to associate training data with random labels, is essential for generalization, though intuitively memorization does not align with model generalization capabilities. Notably, Feldman and Zhang (2020) demonstrated that memorization is necessary for achieving optimal generalization error, claiming significant accuracy degradation when memorized points are removed from datasets.\nGap: The paper identifies several methodological errors in prior work by Feldman and Zhang (2020) that lead to an overestimation of memorization's impact on test accuracy. These include lack of model convergence, data leakage, and sub-population shift, which skew the perceived necessity of memorization for generalization.\nHypothesis: The authors hypothesize that after correcting for identified methodological errors, memorization does not significantly impact prediction accuracy. Consequently, memorization is not necessary for generalization contrary to prior assertions.\nMethod: The authors re-evaluate the impact of memorization on generalization by fixing errors in the original methodology of Feldman and Zhang (2020). They address model training convergence with weight decay, remove test data duplicates from memorization assessments to eliminate data leakage, and ensure sub-populations are appropriately represented. The study uses the CIFAR-100 dataset, training Resnet50 models and thoroughly analyzing the impact of removing so-called memorized points from the dataset after correcting for identified errors.\nConclusion: The corrected analysis reveals that the true impact of memorization on test set accuracy is significantly lower than previously reported. The original decline in accuracy after removing memorized points was largely due to training and data issues, not actual memorization necessity. The study finds no substantial impact of memorization on generalization, challenging the need for memorization in achieving high model generalization abilities.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"Despite there being ample room for further improvement of the paper, there are interesting visualizations of memorized and sub-population shifts in the paper.\", \"Weaknesses\": \"In general, the presented study appears to be inadequately explored, leaving more room for further empirical research and justification of observations based on theoretical foundations. The stated issues do not necessarily contradict the findings of the previous work, especially in the case of the \\\"Sub-population Shift\\\" argument, which I will explain further in my review. Therefore, this paper may not be suitable for this particular venue and might be better suited as a workshop paper or considered for the reproducibility track to facilitate further experimentation and validation of the results.\\n\\nI am strongly leaning towards rejecting the paper; however, I will try to provide my understanding, concerns, and suggestions for the authors so they can reevaluate their work and make a better presentation of their study.\\n\\nConcerns, understanding, and suggestions: \\n1. The introduction section does not adequately define the problem, and the background section has not provided sufficient background information.\\n2. Regarding errors stated in the paper:\\n    1. \u201cLack of Convergence: Models\u201d: Why should models be trained to their maximum test set accuracy? You don't have access to the test set during training!\\n    2. \u201cData Leakage: Training\u201d: The authors of the original work stated this in their paper in Figure 3 that there is a close match of memorized points in the test set.\\n    3. \\\"Sub-population Shift\\\": The argument does not necessarily contradict the previous work because it aligns with what they have been arguing in their paper: memorization is necessary due to the long-tail distribution. Both mislabeled and sub-population data points are part of the long tail; therefore. It does not necessarily mean that the model memorized the points associated with sub-populations. They have high influence scores because they are rare in the data distribution, and the model exhibits a similar behavior of memorization.\\n3. There is an argument in the paper that the definition of memorization from the original work is incorrect, but there is no strong justification for this claim, and it is deferred to future research. I would suggest the authors first review their observations and results and then use them to formulate a more robust definition of memorization. In that case, your work would be better supported.\\n4. There are many repetitions of information with different formats conveying the same content in the paper, such as in the Introduction, Background, Related work, Discussion, and Conclusion. The presentation format could be significantly improved.\\n5. The \\\"Data Leakage\\\" in the Background section is not sufficiently well explained. I have read the \\\"Do CIFAR-10 classifiers generalize to CIFAR-10?\\\" paper, and the context provided in your paper is what they have already explained, and it can be moved to the appendix.\\n6. Table 1: It is not necessary to provide a table for symbols in the main pages; it can be moved to the appendix.\\n7. A large portion of the paper is dedicated to understanding the original paper, and it is defined in a way that favors the introduced issues mentioned at the beginning of your paper. It can be shortened, and the detailed discussion can be left in the appendix.\\n8. Section 5.1 (SETUP): You should check what your models memorize with the modifications you have made, not just the 1015 memorized points provided by the original authors.\\n9. The implementation of fixes is not provided along with the submission.\", \"Questions\": \"Questions: \\n1. The previous work that you studied is based on a theoretical background. Could you please delve into the details of the theory part and justify your observation? I believe that if your argument is completely true, it should also be possible to find a flaw in the theory of the work.\\n2. Section 5.1 (SETUP): How did you define which points impact the same test point?\\n3. Section 4.2 (ERROR: DATASET LEAKAGE): Are you assuming that duplicate points are only found in memorized points?\"}"}]}
{"paper_id": "uMAujpVi9m", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Pocket representations are crucial in biomedical applications such as druggability estimation and ligand affinity prediction. Traditional handcrafted features and data-driven deep learning methods have shown potential but are limited by the lack of extensive protein-ligand complex data for large-scale pretraining.\nGap: The main gap is the scarcity of protein-ligand complex structures available in databases like PDB, which restricts the large-scale pretraining needed for effective modeling of pocket-ligand interactions.\nHypothesis: The authors propose that by pretraining pocket representations using segmentations of high-resolution atomic protein structures into drug-like fragments, and aligning these with pseudo-ligand representations from pretrained small molecule encoders, they can better simulate ligand-receptor interactions and overcome the data scarcity problem.\nMethod: The authors introduce ProFSA, a method that first segments protein structures into drug-like fragments and corresponding pockets to simulate ligand-receptor interactions, generating over 5 million pseudo-complexes. These are used to train a pocket encoder contrastively against pseudo-ligand representations from a fixed pretrained small molecule encoder. The process also involves careful data distribution alignment to mimic real ligand interactions.\nConclusion: ProFSA significantly outperforms existing methods on tasks like pocket druggability prediction, pocket matching, and ligand binding affinity prediction, demonstrating the potential to mitigate the protein-ligand data scarcity issues and opening up avenues for using diverse protein structure databases for pretraining.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"1. The authors present a novel pairwise data synthesis pipeline by extracting pseudo-ligand-pocket pairs from protein data\\n2. The authors develop large-scale datasets and new pretraining methods to exploit the full potential of pocket representation learning, emphasizing the interactions between pockets and ligands.\\n3. ProFSA achieves significant performance gains in a variety of downstream tasks.\", \"Weaknesses\": \"1. The evidence for the construction of the pseudo-ligand is not clear.\\n2. Ablation studies evaluating the impact of critical design choices like fragment sizes, distance thresholds for pockets would provide useful insights.\\n3. While terminal corrections are applied to address biases from breaking peptide bonds, the pseudo-ligands may still exhibit substantial discrepancies from real drug-like ligands.\", \"Questions\": \"1. Why do the authors choose peptides to replace small molecules, and is this choice reliable? Have the authors considered other potential ways to further close the gap between pseudo-ligands and real ligands, either through data processing strategies or by fine-tuning on downstream tasks?\\n2. Section 3.1, second paragraph, line 4, what do the N Terminal and C Terminal refer to?\\n3. Why fixed the molecule encoder in contrastive learning, i.e., the encoder that encodes the pseudo-ligand.\\n4. Could ProFSA be extended to other tasks like protein-protein interaction prediction? How might the pipeline and contrastive approach need to be adapted?\"}"}]}
{"paper_id": "uhtQyRrTzY", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Dense vision tasks such as depth prediction, surface normal estimation, semantic segmentation, and pose estimation typically rely on pretrained representations. Self-supervised learning has shown potential in this area, but dense prediction tasks have struggled to replicate the success seen in object recognition due to a lack of suitable datasets. Current datasets rely heavily on annotated 3D meshes and simulated environments, which limit their applicability to real-world data sources where such metadata is absent.\nGap: There is a lack of large-scale, real-world multi-view datasets without requiring 3D annotations such as meshes or camera parameters. This limitation restricts the scalability of existing self-supervised learning approaches for dense vision tasks, such as CroCo, which depend on synthetic datasets with detailed 3D annotations.\nHypothesis: The authors propose a data-curation method, MIMIC, that automatically generates large-scale multi-view datasets suitable for dense vision tasks using unannotated real-world videos and synthetic environments. They hypothesize that by leveraging real-world data in combination with computer vision techniques such as SIFT and RANSAC, they can achieve superior performance in dense prediction tasks compared to existing methods relying on annotated datasets.\nMethod: MIMIC is a dataset curation approach that automatically generates multi-view image pairs from real-world videos and synthetic environments without requiring additional annotations. The process involves identifying potential data sources, using SIFT keypoint detection and homography transformation to find correspondences, and filtering pairs based on overlap thresholds. Two datasets were created: MIMIC-1M and MIMIC-3M, which were then used to train models with masked image modeling objectives (MAE and CroCo). The models' performance was evaluated on downstream tasks like depth estimation, surface normals estimation, semantic segmentation, and others.\nConclusion: The study demonstrates that representations learned with MIMIC's automatically-generated datasets outperform those trained with expensive, annotated datasets like ImageNet-1K and synthetic dataset MULTIVIEW-HABITAT. Notably, MIMIC-3M provides substantial improvements in performance on dense geometric and dense object-related tasks even when fewer training samples are used. The method also shows promising scalability, allowing the potential creation of even larger datasets for better model training.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 3, \"Contribution\": 4, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"- Being able to mine large data for pairs is a relevant task in representation learning.\\n\\n- They outperform multiview habitat on their evaluations.\\n\\n- The method, being based on classical techniques like SIFT and RANSAC, should scale well.\\n\\n- The method is simple but effective.\", \"Weaknesses\": \"- (minor) some exposition on what the \\n\\n- This paper might be better suited for a computer vision venue\\n\\n- The paper targets dense vision tasks, but it would be interesting to see the method used to generate pairs for constrastive learning, as well as evaluations on non-dense tasks such as imagenet finetuning/linear probe accuracy.\", \"Questions\": \"Is there any reason this wouldn't be useful for general representation learning beyond dense tasks?\"}"}]}
{"paper_id": "TyFrPOKYXw", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large language models (LLMs) have revolutionized tasks like understanding instructions and complex reasoning. With their increased societal impact, ensuring their outputs are not harmful is critical. Safety alignment in LLMs is crucial, particularly in minimizing harmful responses through fine-tuning. Recent advances involve Reinforcement Learning with Human Feedback (RLHF), which aligns LLMs with human preferences but struggles with balancing helpfulness and harmlessness.\nGap: Current methods like RLHF face significant challenges in balancing the objectives of being helpful while being harmless, as the two can often conflict. For example, a model refusing to answer a harmful query might be considered safe but is unhelpful. Hence, achieving a model that is simultaneously helpful, safe, and responsive remains a challenge.\nHypothesis: The authors propose Safe Reinforcement Learning from Human Feedback (Safe RLHF) as a solution to the tension between helpfulness and harmlessness in LLM training. Safe RLHF hypothesizes that decoupling the objectives into distinct reward and cost models, and using constraint optimization, can effectively manage and balance these objectives.\nMethod: Safe RLHF involves a multi-stage process. First, it separates the training data into helpfulness and harmlessness components, avoiding bias in crowdworker feedback. Two separate models are trained: a Reward Model for helpfulness and a Cost Model for harmlessness. Using the Lagrangian method, it dynamically balances the objectives of maximizing helpfulness while satisfying harmlessness constraints. Experiments included three rounds of fine-tuning the Alpaca-7B model, guided by human preferences.\nConclusion: Safe RLHF successfully improves both the helpfulness and harmlessness of LLMs compared to conventional methods. It provides a dynamic mechanism to balance conflicting objectives, significantly reducing the probability of harmful responses while maintaining performance according to human evaluations. Safe RLHF offers a novel integration of Safe RL and RLHF frameworks, contributing to enhanced human value alignment in AI systems.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 4, \"Contribution\": 4, \"Rating\": 8, \"Confidence\": 3, \"Strengths\": \"1. Separation of rewards and costs is an excellent idea that probably resolves the optimization contradiction in RLHF of LLM. \\n2. The paper provides concrete experimental results demonstrating the effectiveness of Safe RLHF in enhancing model performance and reducing harmful responses.\", \"Weaknesses\": \"Minor suggestions in Questions.\", \"Questions\": \"1. Fig2.(a) the symbols of axis indexes are absent.\\n2. Could you add the scatter point of Beaver-v2 and Beaver-v3 in Fig.6(a) and (b)?\"}"}]}
{"paper_id": "dKju7tbe6D", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Visual reasoning and question answering require models to execute and combine multiple visual operations to make inferences. Iterative computation has been shown to be effective for this, as demonstrated by models like the MAC architecture. However, purely iterative approaches may require more steps and lead to poor generalization. On the other hand, parallel computation can efficiently handle mutually independent operations.\nGap: Current visual reasoning mechanisms are typically either purely iterative or parallel, which can lead to inefficiency and poor generalization in scenarios where both computation methods could be beneficial.\nHypothesis: Integrating both iterative and parallel computation into a single neural reasoning architecture could enhance the efficiency and generalizability of visual reasoning models compared to using iterative or parallel methods alone.\nMethod: The authors propose the Iterative and Parallel Reasoning Mechanism (IPRM), which models memory as sets of operation and result states. Given visual and language inputs, IPRM iteratively performs three main computations at each reasoning step: Operation Formation, Operation Execution, and Operation Composition. This approach dynamically composes and executes multiple operations simultaneously while also allowing step-by-step iterative processing. The method was tested on visual reasoning benchmarks such as GQA, CLEVR, CLEVR-Humans, NLVR, and CLEVR-CoGen, and compared against established models like MAC and transformer-based attention mechanisms.\nConclusion: IPRM outperforms existing reasoning mechanisms on several benchmarks, demonstrating stronger generalization and efficiency while using fewer parameters and computation steps. It achieves state-of-the-art zero-shot performance on the CLEVR-Humans dataset and excels at transfer learning tasks, notably outperforming prior methods on the NLVR benchmark. The combination of iterative and parallel processing aids the learning of more general reasoning strategies.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"1. The motivation for the architecture is interesting and reasonable.\\n2. The modular nature of the reasoning pipeline makes the overall steps seem intuitive. \\n3. The experiments show improved performance on several benchmarks over other methods that do not use explicit parallel / iterative reasoning.\", \"Weaknesses\": \"One of the main things that needs to be addressed is the part of the architecture that does the parallel reasoning. From the architecture description, this seems to arise of the first dimension in the some of the internal state matrices of dimension $N_m$. Is it due to the masks in the Operation Composition stage that make these operations \\\"parallel\\\" so that these internal states attend to different parts of the input parallelly? I am trying to understand the difference between having a larger internal state versus explicit parallel reasoning. Where is the structure enforced in the architecture that requires parallel processing when possible?\\n\\nRelated to the above, there is one line that says identity attention is used to \\\"enable a higher degree of operation composition\\\". I don't follow this and needs clarity. Overall, I am not sure how this operation composition step is different and not just implicitly subsumed into earlier architectures, where is the parallel processing explicitly arising from?\\n\\nI think the iterative nature, on the other hand, is immediately due to the recurrent nature of the algorithm.\\n\\nThere seem to be many design decisions that are not immediately clear as being intuitive, and need further explanation. This is true of the Operation Composition stage. It seems to me that this stage is supposed keep a record of the operations performed up to a time step. It is said that operation states are composed with states from multiple time steps in the past. I don't follow this, isn't the M matrix already encoding memory of states in some way, why look back multiple time steps? This stage of the architecture is difficult to follow.\\n\\nThe visualizations in the supplement for one example seem to be pointing to some interesting aspects of the algorithm. But I don't see where the parallel reasoning is being utilized or even the iterative aspects. I think a clear explanation of the figure is needed, I don't understand how to interpret the memory states, the parallel processing, or how the prediction \\\"cylinder\\\" came into being. Perhaps visualizing other matrices and attention masks is also important here.\\n\\nMany more visualizations should be added to illustrate these important parts of the algorithm. Otherwise, it is hard to say whether it is indeed the parallel and iterative structure that is helping or a different issue.\\n\\nSome of these questions also arise because the numbers themselves are also hard to interpret. I don't see why test samples for \\\"logic\\\" type in Table 1 need parallel computation. The reasoning the authors provide are circular and seem to be based on the numbers themselves.\", \"Questions\": \"A minor question, is $N_m$ the same as M in Table 4?\"}"}]}
{"paper_id": "jH67LHVOIO", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Modern language models (LMs) demonstrate impressive abilities but struggle with hallucination, where they produce incorrect but confidently asserted facts. Calibration of LMs, which aligns model confidence with actual correctness likelihood, is vital for reducing hallucinations and enhancing trust in model outputs. Traditional calibration methods either involve post-processing that doesn't affect rank order of possibilities (and thus are limited) or full model training, which is impractical for large models due to computational costs.\nGap: Existing LM calibration methods (post-processing and full re-training) inadequately address the need for efficient and effective recalibration, particularly for large models where computational cost and flexibility are concerns. Moreover, there is a lack of focus on calibrating LMs for long-form text generation beyond short-form tasks.\nHypothesis: The authors propose that a lightweight calibration mechanism, which requires only minimal additional parameters and can adjust the relative confidence rankings of model outputs, can effectively calibrate LMs across tasks varying in response length and improve model trustworthiness.\nMethod: The authors introduce LitCab, which adds a linear layer on top of an LM's last hidden layer to predict a bias term that adjusts output logits. This lightweight mechanism involves less than 2% additional parameters of the LM and uses a contrastive max-margin objective to train. For testing, LitCab's effectiveness is benchmarked using a new suite (CaT) that comprises eight tasks covering both short- and long-form responses. These tasks include a variety of commonly used datasets for testing phrase, sentence, and paragraph level generations. Extensive evaluations are conducted on models including Llama2-7B, GPT, and LLaMA families, using ECE, Brier score, and selective classification metrics.\nConclusion: LitCab effectively improves calibration across all tasks and outperformed traditional methods and confidence estimation approaches in terms of reducing calibration errors. Notably, it demonstrated better data efficiency and was computationally more efficient. Key insights include: larger models within a family are better at short response tasks, GPT models showed superior calibration despite fewer parameters, and improper fine-tuning could degrade calibration. The findings suggest that calibration should be optimized alongside model accuracy.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 3, \"Strengths\": \"1. The model calibration is highly meaningful, especially for nowadays LLMs, and the proposed method can be used for most of the open-source models.\\n\\n2. The proposed method is simple yet effective, although it needs some training examples collected by ChatGPT and GPT-4.\\n\\n3. The collected datasets CAT can also be useful for evaluating the methods for LLMs calibration.\", \"Weaknesses\": \"1. The paper lacks discussion and comparison of some related works [1][2].\\n\\n- [1] Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. arXiv 2023.\\n\\n- [2] Eliciting Latent Predictions from Transformers with the Tuned Lens. arXiv 2023.\\n\\n2. There are still some efficient fine-tuning methods (e.g. LoRA) that can leverage additional training data, but the paper lacks results compared with such methods.\", \"Questions\": \"1. It appears that your method involves using GPT-4 to generate training data, which has also been widely used in recent research. Does this introduce potential unfairness in the comparison? If the training data cannot be generated using GPT-4 or if its quality is poor, would the method still be effective?\\n\\n2. The proposed method seems to have minimal differences compared to the original LM, and the experimental results also indicate that the original LM performs not very badly. Therefore, what are the main advantages of the proposed method? How does it improve LLMs qualitatively or quantitatively?\\n\\n3. Why do you use different thresholds (e.g., cov@50, cov@90, cov@60) for different datasets?\"}"}]}
{"paper_id": "Gsn43rbDwp", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the challenges faced by quantum machine learning in the Noisy Intermediate-Scale Quantum (NISQ) era due to limited quantum resources. It highlights previous work on quantum computing's potential for exponential acceleration in high-dimensional data processing and its limitations in current devices. Graph-structured data, a focus area in machine learning, is notoriously difficult for classical neural networks, which struggles with complex graph data structures. Quantum computing, with its high-dimensional state representation, offers a promising avenue for addressing this challenge.\nGap: Despite advances in quantum computing, there is uncertainty regarding the application of quantum computing methodologies, specifically single qubit techniques, to complex neural network architectures like graph neural networks (GNNs). Current methods struggle to efficiently process intricate graph data due to resource constraints in quantum circuits, particularly in the NISQ regime.\nHypothesis: The paper proposes that a single qubit architecture can be effectively used for graph neural networks by employing a novel graph embedding technique based on quantum walks to reduce the parameter demands usually required by graph data processing.\nMethod: The paper introduces a Single-Qubit Quantum Graph Neural Network (sQGNN) approach. The method comprises three steps: (1) a graph embedding method using discrete-time quantum walks to convert graph data into vectors; (2) encoding the graph data into quantum circuits using a single qubit, thereby minimizing parameter use; and (3) constructing a single-qubit quantum graph neural network using quantum rotation gates for training. The evaluation involved experiments on publicly available datasets like MUTAG and PTC, deployed on both simulated and real quantum environments (IBM quantum computers). The method employs variational quantum circuits optimized using Quantum Natural Gradient, ensuring robustness against noise and efficient learning through reduced parameters and avoidance of the 'barren plateau' phenomenon.\nConclusion: The sQGNN effectively utilizes limited quantum resources by performing graph neural network computations on a single qubit, achieving strong performance on both simulated and real quantum devices. It is robust to noise and circumvents common issues faced by variational quantum algorithms like the 'barren plateau.' The approach significantly reduces model parameters and is easily adaptable to hybrid systems, making it a promising candidate for scalable quantum machine learning applications in the current NISQ era.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"The paper introduces new method using quantum neural networks by combining the existing ideas to efficiently classify graph datasets.  The method uses only single qubit thereby reducing the quantum resources required as well as mitigating the barren plateau issues. The method is also demonstrated to show robustness against noises. The quantum features also enable the method to reduce the parameters required from thousand to twelve.\", \"Weaknesses\": \"Section 2 which describes the method is not written clearly and is hard to follow the steps leading to embedding of graph. Also, there is lack of continuity and proper explanation of variables used in equations in section 2.3. Also, from the experiment results shown in figure 3, the method is doubtful as the success probability is marginal for Class 1. There are a few other issues that is raised in the questions section below.\", \"Questions\": \"1.\\tWhy there is a difference in success probability for Class 0 and 1? It seems that the QGNN is not able to change the initial state.\\n2.\\tWhy the simulations in ideal simulated environment as shown in table 1 has lesser fidelity than using noisy real quantum computers? Providing a figure similar to figure 3 for ideal simulated environment is good for comparison.\\n3.\\tThe error bars for the results seems very high (in table 1 and 2). Is there a way to confine the errors?\\n4.\\tUsing two qubits for encoding seems to worsen the training as shown in table 1. Also, the explanation given for the reduction in standard deviation of dual model is not clear as increasing the number of qubits usually tend to increase the error rates. \\n5.\\tThe literature about existing methods and its pros and cons compared to the new method is missing.\"}"}]}
{"paper_id": "jHdz0CIS2y", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Voice conversion research typically focuses on disentangling speech into separate representations for speaker characteristics and linguistic content. However, this approach can lose finer nuances like accent and emotion. Recent works leverage self-supervised learning for extracting such attributes without text.\nGap: Existing methods that disentangle speech representations often result in loss of important nuances, and reliance on heuristic voice perturbation algorithms limits the diversity of training transforms. Current methodologies struggle with effective voice conversion, especially in cross-lingual and zero-shot contexts.\nHypothesis: The authors propose that using entangled speech representations from self-supervised learning and speaker verification provides a more effective base for voice conversion models, and these models can be iteratively improved through self-synthesized examples, enhancing speaker similarity and overall results in voice conversion tasks.\nMethod: SelfVC leverages entangled speech representations from a self-supervised learning model and a speaker verification model to train a controllable voice conversion system. Key techniques include: deriving prosodic information from audio, an iterative refinement training strategy using self-synthesized examples to improve the synthesis model, and operating without text, thus ensuring applicability across languages. The method is assessed through zero-shot voice conversion tasks and compared against existing models using a range of naturalness, speaker similarity, and intelligibility metrics.\nConclusion: SelfVC shows significant improvements over baseline models and prior work, achieving state-of-the-art results in zero-shot voice conversion. The integration of self-synthesized examples during training substantially enhances speaker similarity, and the model is competent across various languages and applicable to diverse tasks such as zero-shot conversion and cross-lingual voice conversion.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"2023\": 1, \"Strengths\": \"* The paper combines strategies from voice conversion and singing voice conversion to improve zero-shot voice conversion. One noteworthy contribution is the introduction of a novel training strategy that utilizes self-synthesized examples for data augmentation and iterative improvement of the generation model. This represents a significant advancement from traditional approaches that heavily relied on heuristic transformations.\\n\\n* The paper conducts comprehensive experiments. By extensively comparing the proposed framework with several baseline models, the authors effectively demonstrate its efficacy. \\n\\n* Overall, the paper successfully presents a valuable contribution to the field of voice conversion and offers an approach for achieving zero-shot voice conversion.\", \"Weaknesses\": \"* The main framework presented in this paper appears to draw from existing voice conversion and singing voice conversion techniques, such as utilizing SSL features, speaker embeddings, and incorporating prosody information like duration and pitch. These strategies resemble prior work ([1-3]) in the field. Additionally, the synthesizer's approach showcases similarities to methods used in speech synthesis, specifically resembling techniques employed in FastSpeech 2 [4]. To support these statements and provide a comprehensive overview of the related work in the field, it would be beneficial for the authors to provide appropriate citations and conduct comparative analyses.\\n\\n* Moreover, the claim of the framework's efficiency in scaling to other languages through the introduction of SSL features is not unique to the authors' proposed model since similar approaches have been explored elsewhere.\\n\\n* Additionally, it is worth noting that the concept of data augmentation using self-synthesized examples has been discussed in the literature, particularly in the context of speaker verification [5]. \\n\\n* Furthermore, the experimental results suggest that the inclusion of self-synthesized examples for data augmentation yields only a marginal improvement in the model's performance.\\n\\n* Consequently, a deeper analysis or controlled study could be conducted to better isolate and understand the specific effects of the proposed training strategy from other contributing factors.\\n\\nReferences:\\n\\n[1] Jayashankar T, Wu J, Sari L, et al. Self-Supervised Representations for Singing Voice Conversion[C]//ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023: 1-5.\\n\\n[2] Hussain S, Neekhara P, Huang J, et al. ACE-VC: Adaptive and Controllable Voice Conversion Using Explicitly Disentangled Self-Supervised Speech Representations[C]//ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023: 1-5.\\n\\n[3] Maimon G, Adi Y. Speaking style conversion with discrete self-supervised units[C]//EMNLP 2023. EMNLP, 2023.\\n\\n[4] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, \u201cFastspeech 2: Fast and high-quality end-to-end text to speech,\u201d[C]//International Conference on Learning Representations (ICLR), 2021.\\n\\n[5] Cai D, Cai Z, Li M. Identifying Source Speakers for Voice Conversion Based Spoofing Attacks on Speaker Verification Systems[C]//ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023: 1-5.\", \"Questions\": \"* The authors should clarify what specific aspects of their framework, beyond the use of SSL features, contribute to this scalability to strengthen the claim of originality in this regard.\\n\\n* Could the authors elucidate the unique characteristics or modifications they have made within their framework that differentiate it from other SSL-based voice conversion models?\\n\\n* The introduction of the self-synthesized examples as a data augmentation method is intriguing. However, its efficacy and broader applicability remain questions. Can the authors provide experimental evidence or analysis showcasing the generalizability of this data augmentation technique across various methods?\"}"}]}
{"paper_id": "3NXhwkZGjz", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The field of domain adaptation, specifically Unsupervised Domain Adaptation (UDA), strives to address the challenge when the distribution of training and testing data varies, which is known as domain shift. UDA enables transferring knowledge from labeled source domains to unlabeled target domains, thereby reducing the dependency on annotated data in the target domain. Recent approaches have focused on source-free UDA, where only a pre-trained model from the source domain and unlabeled target domain data are used, addressing privacy concerns related to accessing labeled source data.\nGap: The primary challenge in Source-Free Unsupervised Domain Adaptation (SFUDA) is generating reliable supervision from unlabeled target data alone. The accuracy of initial label predictions can be poor due to the domain shift, and using these inaccuracies for model adaptation can yield misleading outcomes.\nHypothesis: The authors propose that considering multiple prediction hypotheses for each sample and analyzing the rationale behind each can help identify the most likely correct hypotheses. This reliable set of hypotheses can serve as pseudo-labeled data to enhance a semi-supervised learning process for domain adaptation without relying on source data or target labels.\nMethod: The authors introduce a three-step adaptation process: 1) Model Pre-Adaptation: To reduce domain shift by promoting smooth predictions using a newly constructed memory of image features and their nearest/furthest neighbors. 2) Hypothesis Consolidation: Multiple hypotheses per sample are created, with rationale representations derived from GradCAM. By comparing these to class-wise rationale centroids, the most reliable hypothesis (based on close proximity to rationale centroids and lack of conflicting rationales) is selected. 3) Semi-Supervised Learning: Utilizing the reliable pseudo-labels obtained previously in the standard FixMatch semi-supervised learning algorithm for model adaptation, which uses consistency regularization and pseudo-labeling.\nConclusion: The proposed method demonstrates state-of-the-art performance in SFUDA tasks, effectively transforming this domain adaptation problem into a semi-supervised learning problem. By consolidating hypothesis rationales, the approach enhances the pseudo-label reliability, leading to improved training outcomes. The method's flexibility allows it to be integrated into existing approaches, thereby bolstering their adaptation performance without requiring access to source data.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"- To the best of my own knowledge, this paper is the first attempt to use multiple source hypotheses in SFUDA and to transform SFUDA into a semi-supervised learning task. They provide a novel view and framework for SFUDA.\", \"Weaknesses\": \"- The motivation of the multiple hypotheses setting is not clear. These hypotheses are pre-trained on the same domain instead of multiple domains like previous works. Is this setting common in practice? I think that multiple high-quality hypotheses on one specific domain are not easy to access. It seems that repeatedly training these hypotheses is too wasteful. Overall, the motivation should be further clarified and providing several practical cases will be better.\\n- Assume the above issue can be explained, then how to choose the best $\\\\tilde{k}$ for another task? $\\\\tilde{k}=4$ seems to be acquired from DomainNet-126 dataset, but this hyper-parameter may be not suitable for others. This issue also exists for the choice of $\\\\tau_1$ and $\\\\tau_2$.\\n- The Eq. (4) seems to be wrong. For the latter item, $\\\\hat{y}_u$ is an integer but $p(\\\\mathcal{A}_s(x_u))$ is a vector.\\n- The performance of the proposed method is not SOTA on some sub-tasks.\", \"Questions\": \"- According to Table 5, more hypotheses will degrade the method's performance. Intuitively, more hypotheses should lead to better performance, because more candidates have a greater chance of getting the true pseudo-labels. Can you explain this strange phenomenon?\"}"}]}
{"paper_id": "ZEZ0CPmoSI", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Non-convex optimization is a significant challenge in machine learning, especially when tackling high-dimensional or non-linear problems. Traditional methods often simplify these problems into convex ones, despite the practical benefits of directly handling non-convex optimization. The concept of matrix smoothness extends typical smoothness assumptions by leveraging matrix information, offering potential improvements in supervised model training, including communication efficiency in distributed systems using compressed gradient descent (CGD).\nGap: Existing approaches struggle with efficient optimization of smooth and non-convex functions, particularly regarding convergence and the impact of stepsize in both single-node and distributed settings. Traditional scalar stepsizes in compressed gradient methods do not effectively exploit the structure of non-convex problems.\nHypothesis: Utilizing a matrix-valued stepsize in compressed gradient descent algorithms for minimizing matrix-smooth non-convex objectives can improve convergence rates compared to scalar stepsizes. These matrix stepsizes can be tailored in a layer-wise manner to exploit the underlying model structure, potentially enabling 'compression for free.'\nMethod: The paper proposes two CGD variants with matrix stepsizes, analyzed in both single-node and distributed settings. It introduces a novel 'layer-wise' design for compression and stepsize matrices, particularly useful in neural networks. Theoretical analysis focuses on the convergence properties of the algorithms, employing assumptions on matrix smoothness and boundedness of functions. The proposed methods are demonstrated with empirical evidence of faster convergence compared to scalar alternatives.\nConclusion: Matrix-valued stepsizes in CGD algorithms improve convergence for non-convex optimizations, particularly when matched with model structures via layer-wise design. The results show practical gains in communication efficiency in distributed settings, supporting the idea of 'compression for free.' Empirical results validate the theoretical findings, demonstrating better performance than traditional methods.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"The paper investigates matrix step sizes of compressed gradient descent and provides convergence analysis.\", \"Weaknesses\": \"see questions.\", \"Questions\": \"1) The paper claims that the matrix step size improves performance but introduces problems. The presence of the neighboring term in Theorem 3 creates a significant neighborhood when $\\\\Delta^{inf}$ is large.\\n2) This paper uses the compressed gradient as shown in (3), which is a direct compression on the full gradient. Given the prevalence of error feedback techniques in handling compression errors, it is pertinent to consider whether the proposed matrix step size can be extended to incorporate error feedback methods. \\n3) The paper does not consider the use of SGD, local update, and partial participation techniques in distributed settings. \\n4) The paper introduces det-CGD1, but it is unclear why this algorithm, specifically in a distributed setting, can obtain Algorithm 1.\"}"}]}
{"paper_id": "uU0Adp7Sfo", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Generative adversarial networks (GANs) are used to generate synthetic data by creating a competition between a generator and a discriminator. However, the traditional competition-only setup of GANs is insufficient when broader insights into data distributions are required, such as in multi-modal data generation and image super-resolution.\nGap: Traditional GANs suffer from a limitation in addressing the need for applications that require the incorporation of additional knowledge beyond just replicating the authentic data distribution. This includes challenges in multi-modal generation and the ability to replicate images accurately, as the standard competitive nature lacks incentive for generator and discriminator to collectively meet these broader requirements.\nHypothesis: The paper hypothesizes that by developing a collaborative-competitive framework (CCGAN), which blends competition with collaboration, it is possible to enable more informed data generation that respects additional requirements beyond the authentic data distribution, while theoretically preserving equilibrium and avoiding training collapse.\nMethod: CCGAN introduces a new collaboration-competitive scheme by incorporating regularization as a divergence metric that allows for a theoretically well-defined game-theoretic setup. This approach harmonizes competition and collaboration losses, thereby reducing the complexity of solving optimality. CCGAN is designed with a novel regularization term that breaks the mutual dependency between generator and discriminator, allowing for exploration of a broader parameter space. Comprehensive experiments were performed using 4 datasets to validate CCGAN's performance compared to baseline models, specifically focusing on generating images of regularly shaped objects.\nConclusion: CCGAN, by providing a unified framework and ensuring the presence of an equilibrium point, overcomes the limitations of pure-competition GANs. It leads to consistently high-quality data generation, providing both theoretical and empirical improvements. CCGAN shows better performance, especially in regular-shaped object generation, compared to baseline GAN models and is robust to training collapse. It facilitates stable data generation that addresses broader requirements, as demonstrated by several experiments.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 5, \"Strengths\": \"- Promising idea support by theory proofs \\n- The newly introduced method does not cause additional significant computational cost. \\n- Nice application demo on multiple vision tasks.\", \"Weaknesses\": \"1. Formatting is not quite right. \\n\\n2. There are existing multiplayer methods to improve the stability of GAN training, which changes the original GAN competition setting by introducing additional players. Would like to hear some words from the authors about the advantages or disadvantages compared methods in this setting. \\n- https://arxiv.org/abs/2101.07524\\n- https://arxiv.org/abs/1907.02690\\n- https://arxiv.org/abs/1709.03831\\n\\n3. Over claim of the visual results\\n- All the visual results are too small. I can\u2019t really see the difference unless I zoom hard on a giant monitor, which is not possible from the printed version. \\n- Figure 3 column 3,4: Not really better than baseline\\n- Figure 4b: I can\u2019t see what the original mask is, again too small to show the details.\", \"Questions\": \"1. It sounds like the proposed method should work with the original GAN method. Why not a direct comparison with it on an image generation task with FID / IS scores? \\n\\n2. \u201cWe prove that the generator and discriminator is not mutually dependent in CCGAN\u201d Can you combine it with SOTA methods like StyleGAN3?\\n\\n3. Training details are missing from the experiment section. Resolution? Sample size?\"}"}]}
{"paper_id": "NvSwR4IvLO", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The rapid advancements in Large Language Models (LLMs) enable them to perform various tasks efficiently, posing potential security risks due to misuse in plagiarism, fake news generation, and more. Recent research attempts to identify AI-generated texts using model signatures and watermarking techniques.\nGap: Currently proposed AI-text detectors are not reliable in real-world applications as they are vulnerable to attacks, particularly paraphrasing, which significantly reduces their efficacy. There's a necessity to evaluate the robustness of these detection methods against adversarial strategies.\nHypothesis: Existing AI-text detectors, including watermarking-based schemes, neural network-based classifiers, zero-shot classifiers, and retrieval-based detectors, are susceptible to evasion and spoofing attacks, rendering them ineffective in reliably distinguishing AI-generated from human-generated text.\nMethod: The authors develop a recursive paraphrasing attack using neural network-based paraphrasing models, applied to AI-generated texts to evade detection. They conduct experiments with different detector types and demonstrate this attack on passages of around 300 tokens. They also present a spoofing attack showing how adversaries can mislead detectors. Finally, they provide a theoretical framework connecting the effectiveness of a detector to the total variation distance between human and AI text distributions, using a paraphraser to reduce this distance.\nConclusion: The study reveals that current AI-generated text detectors are unreliable in practical scenarios, as they fall victim to paraphrasing and spoofing attacks. These findings show the inherent difficulty in creating robust detectors capable of differentiating human and AI text without substantial trade-offs. The research suggests that as AI models become more sophisticated, existing detectors will find it increasingly difficult to function effectively.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"* The authors address the important problem of reliable AI-generated text detection. This problem is likely to become increasingly important with the rapid rise in popularity of large language models in society.\\n\\n* The proposed approach is simple yet effective in undermining the reliability of current AI-generated text detectors. The simplicity of the attack may also make it more likely to generalize to different models and domains.\\n\\n* The authors perform a human evaluation using Amazon's Mechanical Turk (MTurk) to evaluate the resulting paraphrased text passages. Their results suggest the original content of the text passage is preserved in addition to the grammar and overall text quality.\\n\\n* The authors also discuss the overall hardness of AI text detection, providing a formal upper bound of detection performance based on the total variation (TV) between AI-generated and human-generated text distributions.\\n\\n* The paper is well-written and easy to follow.\", \"Weaknesses\": \"* The experiments only use text passages from a single dataset, and tend to only evaluate a single model for each detector type. Evaluations on a wider range of datasets and detectors would greatly strengthen any generalizable claims regarding the proposed paraphrasing attack.\\n\\n* Lack of baseline methods. In many of the experiments, the proposed attack is the only method being evaluated, have the authors compared their attack with similar attacks?\\n\\n* I appreciate the human evaluation in the \\\"Watermarked AI Text\\\" section, however, this type of evaluation is missing in the other experimental sections. For example, Figure 5 claims a significant drop in detector accuracy with minimal degradation in text quality; however, it's unclear to me how significantly text quality degrades based on a perplexity score increase from 6.15 to 13.55.\\n\\n* The insight that smaller TV between AI-generated and human-generated text distributions leads to more difficult detection problems seems rather obvious. Although the authors show that more complex models can lead to smaller TV distances, the authors do not provide any empirical evidence that smaller TV distances actually lead to more difficult AI text detection.\\n\\n* There is no empirical runtime evaluation of the proposed attack.\\n\\n* There are several grammatical errors throughout the paper, consider using a service like Grammarly to fix these issues.\\n\\n* Figure 9 is not colorblind friendly.\", \"Questions\": \"* How is TV distance defined, and why is it difficult to compute for larger datasets?\\n\\n* How did the authors determine 5 rounds of paraphrasing to be sufficient?\"}"}]}
{"paper_id": "6yJuDK1DsK", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Test-time adaptation (TTA) refers to the challenge of adapting a pre-trained model to handle test data distributions that differ from the training (source) distribution, often due to non-stationary environments. Lifelong TTA is a more complex setting where the test distribution continues to change over time, requiring improvements that manage adaptation without forgetting the source model's knowledge and controlling computational cost.\nGap: Existing lifelong TTA methods require adaptation of all model parameters, which is computationally expensive and memory-intensive. This limits their usability on edge devices for rapid inference. Additionally, these methods may suffer from significant forgetting over time and often require access to the source data for warm-starting adaptation, which may not always be feasible.\nHypothesis: FEATHER proposes a solution that introduces a lightweight adapter to the pre-trained model, requiring fewer additional parameters to perform effective test-time adaptation. This method aims to keep the source model frozen, thus separating source and target domain knowledge to prevent forgetting and error accumulation. Moreover, FEATHER eliminates the need for source data access during adaptation.\nMethod: FEATHER inserts a minimal number of adapters\u2014configurations of group-wise and point-wise convolutions\u2014into a pre-trained network. These adapters are initialized to act as identity functions, ensuring unchanged initial behavior in the source domain representation. During test-time, only adapter parameters are updated using unsupervised learning methods on the test data, while the rest of the model remains constant. The study validates the effectiveness of FEATHER through experiments on CIFAR-10C, CIFAR-100C, ImageNetC, and ImageNet3DCC datasets, demonstrating substantial parameter efficiency.\nConclusion: FEATHER achieves comparable or better performance than state-of-the-art lifelong TTA approaches with 85-94% fewer parameters needing adaptation, offering significant computational efficiency. Unlike some methods, it does not require access to source data during adaptation, remaining robust in dynamic environments on multiple challenging benchmarks. The method's lightweight design is crucial for real-world applications with practical hardware constraints.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. This paper is overall clearly clarified and well organized.\\n2. The use of adapters for lifelong test-time adaptation seems novel.\", \"Weaknesses\": \"1. The proposed method is only applicable to CNNs.\\n2. The authors argue for the importance of preserving the information of the source model in online TTA tasks. However, the proposed method does not exhibit outstanding performance compared to the CoTTA, which involves full fine-tuning of the model. Therefore, the authors fail to sufficiently explain why the use of adapters is suitable for online TTA tasks, apart from the fact that it reduces the number of trainable parameters.\\n3. The authors claim that the proposed method excels in terms of parameter update costs; however, the cost of training the adapters inserted between the model's layers, in terms of memory and computation, is not significantly lighter compared to full fine-tuning.\", \"Questions\": \"1. Is it possible to apply the proposed method to Transformers?\\n2. Why does CoTTA generally outperform the proposed method?\\n3. Could you compare the proposed method with other methods in terms of memory and computation?\"}"}]}
{"paper_id": "lBY65YaAho", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the advancements in AI facilitated by training large transformers using next-token prediction, highlighting successes in generative AI like ChatGPT. However, these approaches rely heavily on human supervision, which poses challenges due to the need for extensive human input and expertise, especially in specialized domains.\nGap: The reliance on human supervision for fine-tuning large language models presents a substantial challenge, as it is resource-intensive and limits scalability. There is also the issue of lack of diversity in self-generated data when AI models are used in place of human supervision, leading to rapid saturation in learning processes.\nHypothesis: The authors propose a novel paradigm called Exploratory AI (EAI) designed to autonomously generate high-quality training data, reducing dependence on human supervision by leveraging large language models to explore and create diverse data in the natural language space.\nMethod: The approach employs two main components: an actor that generates novel content and a critic that evaluates and guides the content generation process. EAI draws inspiration from unsupervised reinforcement learning pretraining and adapts it to large language models, focusing on novelty evaluation. The method involves using principles for data exploration such as rephrasing, restructuring questions, and creating new scenarios. EAI is tested on mathematical reasoning benchmarks like GSM8K and MATH, comparing it against human-supervised and other AI-supervised methods.\nConclusion: EAI significantly improves model performance on complex reasoning tasks, outperforming state-of-the-art AI supervision methods. It generates diverse training data without human involvement, suggesting it as a scalable and automated alternative to traditional fine-tuning methods. The study demonstrates EAI's superior sample efficiency and diversity in data generation, offering insights into how AI can autonomously enhance learning processes.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"The problem of having less human involvement in finetuning is an important topic. The link between unsupervised RL and this problem is very interesting. The proposed method seems to work on the experiment.\", \"Weaknesses\": \"1. To start with, I think the method is still relying a lot on human insights especially on some exact ways of generating \\\"diverse\\\" methods. The argument that this is a general method is not well supported enough, and I would love to see more experiments on different kinds of benchmark.\\n\\n2. The paper is missing ablation to see how exactly the critic help the results. In other words, it needs to compare with similar methods like self-instruct.\\n\\n3. While the diversity experiment in Fig3, it would be more interesting in seeing more visualization. Since you already have the embeddings, maybe do a T-SNE plot or something to better prove the diversity\\n\\n4. Finally, the method seems to have very weak link to unsupervised RL pretraininig, and in fact the model is not trained at all but simply prompted. And I don't see any thing related \\\"learning skills\\\".\", \"Questions\": \"See above\"}"}]}
{"paper_id": "lWe3GBRem8", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Prior work in reinforcement learning (RL) has explored the combination of offline RL, which allows for policy training on pre-collected static datasets, and online RL, which addresses exploration challenges. Offline RL is effective for learning from existing data, but often suffers from distributional issues, while online RL facilitates exploration by encouraging new state occupancy via exploration bonuses. These exploration bonuses aim to increase state coverage, but can introduce bias, leading to suboptimal policy performance even with increased exploration. Recent studies have suggested combining the strengths of both paradigms by leveraging offline RL techniques for exploitation while using online interaction for state exploration.\nGap: The authors identify a key issue in reinforcement learning: the bias introduced by exploration bonuses can result in suboptimal policies. In addition, there is a challenge in balancing exploration and exploitation, where exploration bonuses in online RL do not decay quickly enough, leading to policies that prioritize exploration over task-specific rewards. Existing methods that combine offline and online RL often end up with a policy that is biased by exploration bonuses during fine-tuning, reducing overall policy performance.\nHypothesis: The authors hypothesize that decoupling the exploration and exploitation phases in reinforcement learning processes\u2014using offline RL to retrain policies from online interactions\u2014can mitigate the exploration bias. By using an optimistic exploration policy for state-exploration and a separate, pessimistically-trained exploitation policy for evaluation, the framework can leverage the advantages of both offline and online RL, thereby potentially recovering more performant policies from online interactions.\nMethod: The authors propose the Offline-to-Online-to-Offline (OOO) framework which employs two distinct policies: an optimistic exploration policy for interacting with the environment and gathering data, and a pessimistically trained exploitation policy for evaluation. The process begins with pre-training the exploration policy on offline data using exploration bonuses. During online interaction, the exploration policy interacts with the environment, collecting additional data. After the interaction phase, an exploitation policy is trained from scratch on all collected data, using pessimistic offline RL approaches such as Implicit Q-Learning (IQL) or Cal-QL. This division aims to reduce the inherent bias introduced by exploration bonuses during evaluation, thereby fostering the development of a more focused exploitation policy that accurately reflects task-specific rewards. Experiments were conducted using D4RL benchmarks and OpenAI gym environments, employing methods such as RND for exploration bonuses and examining various RL algorithms to substantiate the framework\u2019s efficacy.\nConclusion: The OOO framework improves state-of-the-art performance on several benchmark environments by effectively separating exploration and exploitation. By employing decoupled policy learning, the framework demonstrates substantial performance increases over previous methods, showcasing improvements in environments where systematic exploration is critical. Furthermore, it proves beneficial in fine-tuning from incomplete offline datasets, where previous approaches may struggle, by recovering performant policies without being penalized by continuous exploration bias. The study suggests decoupling policies as a viable strategy to enhance RL exploration and exploitation balance and improve policy effectiveness.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"The paper is well-written and easy to follow. The proposed method is overall reasonable.  Besides, I really like the topic raised by the author. How to use offline data for safe and efficient online interaction to find a global optimal policy is a key issue. Currently, pessimistic training in offline RL indeed enabled the recovery of performant policies from static datasets. Thus, I think the topic is timely to be proposed, as it can be the potential last step of the offlineRL paradigm to ground RL in real-world applications.\", \"Weaknesses\": \"1. some important baselines seem missed: baseline of offlineRL + exploration (without decoupling) should be added; some offlineRL + decoupled exploration methods should be added (see below);\\n2. some related works should be discussed: decoupling the exploration policies and the exploitation policies (or target policies) is not a new idea in standard exploration studies. There are indeed many works [1,2,3,4] that have discussed this problem. There should be a more formal discussion on the differences between the decoupling in offlineRL and off-policy RL beyond the difference in the gradient computation methods they used.\\n3. the evidence of exploration bonuses bias in the learned policy should be discussed more explicitly. Maybe giving some visualizations for OOO and a standard offlineRL + RND algorithm is good.\\n4. In Figure 3, why use \\\"frozen\\\" RND  and a baseline to demonstrate the effects of exploration bias?\\n5. I am a bit confused about the paragraph \\\"What explains the improved performance in OOO RL?\\\". How can we derive the conclusion that \\\"These ablations suggest that mitigating the exploration bias by removing the intrinsic reward when training the exploitation policy leads to improved performance under OOO\\\" just by excluding the two hypotheses?\\n\\n\\n[1] Decoupled Exploration and Exploitation Policies for Sample-Efficient Reinforcement Learning. 2021.\\n\\n[2] Off-policy Reinforcement Learning with Optimistic Exploration and Distribution Correction. 2021.\\n\\n[3] Curiosity-Driven Exploration for Off-Policy Reinforcement Learning Methods. 2019.\\n\\n[4] Reinforcement Learning with Derivative-Free Exploration. 2019.\", \"Questions\": \"An open question for discussion: What do you think are the essential differences, the extra challenges, and the benefits of the standard exploration problem in online RL and the exploration problem in offline-to-online RL?\"}"}]}
{"paper_id": "H8Qg1IIMaR", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large language models (LLMs) and vision-language models (VLLMs) have shown significant proficiency in a range of language tasks. They have been particularly successful in multiple-choice question answering (MCQA), leading to their widespread use in various applications. These models' high performance on benchmarks suggests robustness and reliability comparable to human-level proficiency, such as in professional exams.\nGap: Despite their strong performance metrics, there is limited investigation into the robustness of LLMs and VLLMs, particularly their susceptibility to simple transformations like answer option permutations in MCQA tasks. This area remains underexplored, especially concerning vision-language models, leaving a gap in understanding their real-world reliability and robustness.\nHypothesis: The paper hypothesizes that LLMs and VLLMs exhibit a critical vulnerability to adversarial permutations of multiple-choice answers, which dramatically impacts their accuracy and performance. This sensitivity persists regardless of model size or type, indicating a systemic issue with how these models process or prioritize input information.\nMethod: The study conducts experiments on a range of MCQA datasets to test various LLMs and VLLMs, including models like Llama2, Vicuna, and GPT-3.5-turbo. The datasets include MMLU, ARC-c, BoolQ, SocialiQA, MedMCQA, and more. The authors use simple adversarial permutation attacks by shuffling answer options and observe the models' performance. They test permutations with and without in-context learning and compare with baseline model performance, measuring accuracy drop as a primary metric. Majority voting, contextual calibration, and confidence-based strategies were evaluated but primarily found ineffective.\nConclusion: The study concludes that LLMs and VLLMs are highly susceptible to simple permutation attacks, which significantly decrease their performance, often below chance levels. Current mitigation strategies, like majority voting and calibration, are not practical or fully effective. The findings raise concerns about the evaluation of these models' robustness, emphasizing a need for research focused on developing models or strategies intrinsically resilient to such adversarial manipulations.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"The paper addresses the robustness of widely used models, a pertinent topic given the real-world deployment of these models.\", \"Weaknesses\": \"1. The methodology's depth and novelty are not entirely clear. More analysis could be provided on how natural variations in the way are ordered may impact models.\\n\\n2. While the paper focuses on permutation-based vulnerabilities, it might benefit from a broader discussion on other potential vulnerabilities or comparisons to other attack methods.\\n\\n3. The experiments provided are on a specific dataset. It would be beneficial to see how the models fare on diverse datasets to ensure the findings, which are not specific to one dataset's characteristics.\", \"Questions\": \"See the above weakness\"}"}]}
{"paper_id": "LvlqEaXJbe", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper highlights the increasing use of digital images and computer vision in biological studies, covering areas such as evolutionary biology, ecology, and biodiversity. It notes the explosion of computational methods to extract biologically relevant information from images, yet most approaches so far have been tailored for specific tasks.\nGap: Current computer vision methods for biology are often bespoke and not easily adaptable to new contexts, questions, or datasets. This limitation results from these methods needing substantial expertise in machine learning and are typically designed to handle specific taxa or tasks.\nHypothesis: The authors propose the development of a foundation model, similar to those used in natural language processing, that can generalize across various biology tasks and datasets. This model should utilize the rich hierarchical taxonomy of biological data to support fine-grained classification tasks across the tree of life.\nMethod: The authors develop BioCLIP, a large-scale multimodal model trained on a new dataset called TreeOfLife-10M, which includes over 10 million diverse biology images. The model uses a CLIP-style multimodal contrastive learning approach, leveraging taxonomic structures to learn representations that generalize across taxa. They benchmark the model on several fine-grained classification tasks and perform intrinsic evaluations to assess its hierarchical representation.\nConclusion: BioCLIP significantly outperforms existing computer vision models in zero-shot and few-shot classification tasks. The model demonstrates strong generalization by learning a hierarchical representation that mirrors the tree of life, thus offering a versatile tool for biological research.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 5, \"Strengths\": \"1. The dataset collected in this paper is so far the largest biological image dataset which consists of over 10M images and 0.5M categories. Such a dataset is potentially very useful for training large models that target biology-related vision understanding tasks.\\n2. This work provides a detailed analysis of the effectiveness of training CLIP-type models on the collected dataset.\", \"Weaknesses\": \"1. The technical contribution is limited. The main contribution lies in the dataset collection and benchmarking of existing methods. \\n2. The presentation can be further improved. For example, it would be useful to add some examples with annotations from the dataset.\", \"Questions\": \"Even though this work does not provide much technical novelty, it takes a first step to build a large vision model and curate a highly diverse dataset. The analysis of the text types for the training of BIOCLIP is also interesting.\"}"}]}
{"paper_id": "18TfucMNTr", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Training deep neural networks is a challenging non-convex optimization problem characterized by local minima and saddle points. Incorporating noise into training can help escape local minima and improve convergence. Moreover, optimization by continuation, which involves solving a convex approximation of the objective function before gradually transforming it into the original non-convex function, has been applied successfully in various domains including computer vision and reinforcement learning. Despite the potential of continuation methods, challenges remain due to the presence of saddle points in deep learning optimization landscapes.\nGap: The paper identifies that while optimization by Gaussian continuation shows promise, saddle points require special treatment as they may trap the optimizer in local minima during the continuation process. There is a need to address these issues to improve the training of deep architectures and handle non-convex loss functions more effectively.\nHypothesis: The paper proposes that applying a form of regularization to the continuation optimizer can address the challenge posed by saddle points during optimization by Gaussian continuation. This approach is hypothesized to improve training outcomes for deep networks by reducing variability and enabling escape from local minima.\nMethod: The paper uses a continuation method that involves Gaussian convolution of the objective function. Starting with a large continuation parameter that ensures convexity, the paper gradually decreases this parameter while optimizing. A regularization term is added to the continuation optimizer to manage the impact of saddle points and stabilize the training process. The method also adapts Monte Carlo techniques to evaluate high-dimensional optimization efficiently. The performance of this method is assessed on benchmark problems including image classification with the MNIST dataset and learning dynamics with neural ordinary differential equations.\nConclusion: The study demonstrates that Gaussian continuation reduces the risk of being trapped in local minima, achieving better training results for deep learning models. It specifically shows greater stability and reduced variability in training outcomes. The continuation approach leads to faster convergence and higher accuracy compared to standard methods, supporting the hypothesis that the regularization of the continuation parameter can mitigate saddle point issues, thereby enhancing deep neural network training.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"- The paper gives a good introduction, formal treatment and extensive background of the basic concepts and background literature on Gaussian continuation\", \"Weaknesses\": \"- The numerical experiments seem severly flawed in multiple ways. The baseline MNIST classifier without continuation is nowhere near what a simple fully connected 2-layer neural network trained with SGD can achieve on MNIST in terms of test set accuracy (their baseline training set accuracy is below 80% while it is easy to get above 90% test accuracy and should be easy to memorize the training set on MNIST). Furthermore, the variance across different training runs the authors show in their figures, especially for the baseline (without continuation) looks very alarming. Their baseline training setup is too unstable to average across and be considered anything remotely close to a fair baseline.\\n- Given the minimal numerical results it does not become clear whether the added dimension of how to decay lambda properly is really given a robust solution with the approach described in the paper\", \"Questions\": \"- If the focus is on high dimensional deep learning problems perhaps do not use so much space on 1D intuitions (e.g. Figure 1 and Figure 2). It is also not clear to me that the saddle point example in 2.4 is very meaningful to how saddle points in the loss landscape for high dimensional neural networks look like. I would instead recommend to focus more on how Gaussian continuation interplays with the noise from SGD (different noise distribution than normal Gaussian) and its well known annealing affects typically leveraged through learning rate schedules. Does injecting noise with the different distribution really help and if yes in what circumstance in a high dimensional loss landscape? Some papers in the direction of this question (like https://proceedings.mlr.press/v162/orvieto22a.html) are already cited in the paper.\\n- Nit: In the abstract it says saddle points are responsible for getting stuck in local minima, but a saddle point is not a minimum, just a zero gradient point to get stuck in?\"}"}]}
{"paper_id": "oDYXpvnv5f", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Deep ensembles have been identified as promising methods for uncertainty quantification in high-dimensional regression and classification. However, they often fail to provide reliable estimates outside the training distribution, leading to overconfident predictions in out-of-distribution (OOD) scenarios.\nGap: Deep ensembles tend to produce overconfident uncertainty estimates when predicting on OOD data. This occurs because the ensemble members lack sufficient diversity in their predictions, which is exacerbated by the regularization techniques typically used during training.\nHypothesis: An ensemble of networks that fit the training data with large weights is likely to manage the trade-off between increasing ensemble output diversity and maintaining accurate in-distribution predictions.\nMethod: The method proposed involves adding an anti-regularization term to the training objective that penalizes small weights, encouraging networks to develop large weights. This approach is combined with a control process to maintain the loss on the training data within an acceptable threshold, without the need for hyperparameter tuning or OOD training data. The optimization problem is formulated to maximize diversity corresponding to a 'water-filling' model, and both theoretical analysis and empirical experiments were conducted to demonstrate the effectiveness of the method.\nConclusion: The proposed Deep Anti-Regularized Ensembles (DARE) method significantly improves uncertainty quantification for OOD data, providing better performance than recent deep ensemble and OOD detection methods in both regression and classification tasks. The method is practical and doesn't require OOD data or cross-validation for hyperparameter setting, making significant improvements in ensemble diversity and consequent reliability of uncertainty estimates.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"The paper does a good job of identifying and conveying an insight (larger weight variance leads to larger output variance), providing theoretical motivation, and then running experiments to test out that method.\", \"Weaknesses\": \"The main weaknesses of this paper are the limited set of experiments and mixed results. ResNet-32 on CIFAR-10 with only the final layer ensembled provides limited data. It's a fine experiment to include in the paper, but a larger-scale experiment with a fully-ensembled bigger model on a bigger dataset (even on ImageNet, which is not that large anymore) would provide much more information on the viability of this approach and whether it provides enough benefit. Concretely, looking at Table 2, both in-distribution accuracy and OOD detection is mixed for DARE, including worse results for the important near-OOD case of CIFAR-100 vs CIFAR-10. As noted in the limitations section, this paper is also missing comparisons to Bayesian neural nets (rank-1 BNNs and Prior networks could be nice experimental comparisons here). In the current environment, experiments with language models would also be extremely useful. Overall, the experimental setup is limited and the results are not that compelling in their current state.\", \"Questions\": \"As noted in the weaknesses above, I would suggest running larger experiments to better allow the reader to understand the efficacy of the approach. Uncertainty quantification is an important problem, so I encourage continued work here!\"}"}]}
{"paper_id": "YCPDFfmkFr", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The use of optimization layers in neural networks has gained popularity due to their capacity to solve diverse machine learning tasks and capture domain-specific knowledge. This involves incorporating differentiable optimization problems as layers, allowing for complex domain modeling within the architecture. Specifically, convex quadratic programming (QP) layers are of interest due to their rich modeling capabilities and computational efficiency. Differentiable optimization frameworks have been developed, but typically assume feasibility of the underlying optimization problems.\nGap: The main issue addressed is the potential infeasibility of QP layers during neural network training. Previous approaches often enforce feasibility through architectural constraints, which limits the modeling power and may require additional learning variables. These methods rely on maintaining primal feasibility to differentiate QPs, which is not always guaranteed in dynamic learning tasks like control and robotics.\nHypothesis: The authors propose that by leveraging primal-dual augmented Lagrangian techniques, they can compute derivatives for both feasible and infeasible QP solutions. This enables differentiating through infeasible QPs during training to move towards feasibility at test time, enhancing the expressiveness of QP layers.\nMethod: The method involves extending the definition of QP solutions to the closest feasible ones when infeasible, using the classical least squares sense. The authors introduce an Extended Conservative Jacobian (ECJ) framework to path-differentiate these solutions with respect to the parameters. They provide ways to compute ECJs in both forward and backward modes of differentiation. The practicality of this approach is demonstrated through the implementation of QPLayer, a C++ package for differentiating feasible and infeasible convex QPs, interfacing with common learning frameworks.\nConclusion: The proposed method shows that it is possible to handle infeasible QP layers in neural network training, thus broadening the range of layer architectures that can be effectively employed. By allowing the relaxation of primal feasibility constraints, the approach enables the training of QP layers to achieve superior predictive performance in certain tasks. Experimental results suggest that this method is numerically robust and efficient compared to existing methods, providing improved predictive power in structured tasks such as Sudoku solving.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"1. The paper studies an important problem.\\n2. The paper proposes a new approach to calculate gradients in convex QP layers.\", \"Weaknesses\": \"1. The paper is not easy to follow. See the comments below.\\n\\n2. The analysis in the paper is not sufficient. See the comments below.\", \"Questions\": \"1. The paper is not easy to follow. In some key sections or proofs, the authors suggests to read some other referecens to undertand the concepts. This makes reading the paper difficult for readers that are not working on this specific area of extended conservative Jacobian. To make the paper easier to read, the authors are suggested to add the basic concepts and ideas in the paper instead of requesting the audience to read a number of other reading materials.  \\n\\n2. To address the infeasibility issue for the QP layers, one straightfoward way is to introducing slack variables and add penalties about the constraint violation in the objective function. The authors are suggested to show the advantage of the proposed approach as compared to this straightforward approach.\\n\\n3. One application of the QP layers is to use it as a building block for existing iterative algorithms (similar to the approach in [R1] and [R2]). The authors are suggested to analyze the convergence performance of the iterative algoritms when using the propsoed conservative Jacobian as a building block.\\n\\n\\n[R1] Donti, P.L., Rolnick, D. and Kolter, J.Z., 2021. DC3: A learning method for optimization with hard constraints. arXiv preprint arXiv:2104.12225.\\n[R2] Donti, P., Agarwal, A., Bedmutha, N.V., Pileggi, L. and Kolter, J.Z., 2021. Adversarially robust learning for security-constrained optimal power flow. Advances in Neural Information Processing Systems, 34, pp.28677-28689.\"}"}]}
{"paper_id": "Kfpaq5CJPy", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Adversarial robustness is crucial for deep neural networks used in image classification due to their susceptibility to adversarial attacks, which are small perturbations that can cause the network to misclassify inputs. Traditional adversarial attack methods use Lp norms to bound perturbations, with methods like PGD being prominent. However, real-world distribution shifts, not bounded by any norm, such as those caused by weather conditions, also degrade performance. Image transformations such as DCT and DWT have been proposed to improve robustness by using them in adversarial training to handle common perturbations.\nGap: Standard adversarial attacks and defenses typically target the entire L\u221e space, which is often not efficient for generating visually similar adversarial examples or for generalizing against distribution shifts like common image corruptions.\nHypothesis: Attacking within a low-dimensional subspace of image representations defined by discrete transforms can yield adversarial examples with higher perceptual similarity to the original images and enhance the robustness of neural networks against a wider class of perturbations, especially when used for adversarial training.\nMethod: The authors propose a novel adversarial attack method that operates in a meaningful subspace of an image's transformed representation while constrained by the L\u221e pixel space using a non-linear programming technique called the barrier method. They instantiate this approach using three transformations: DCT, DWT, and Glow (a flow-based model). The barrier method enables efficient computation of perturbations without projections, making the attack feasible for adversarial training. They evaluated this on the CIFAR-10 and ImageNet datasets, comparing against standard PGD and other L\u221e attacks, and tested the models for robustness against common image corruptions using CIFAR-10-C.\nConclusion: The proposed method successfully produces adversarial examples that are visually and perceptually closer to the original images while maintaining a high attack success rate within the defined bounds. Training with these adversarial examples (especially with DWT) leads to better robustness against common corruptions while maintaining competitive natural accuracy compared to models trained under full L\u221e constraints. This supports the hypothesis that focusing on meaningful subspaces can bridge adversarial and corruption robustness, offering a new direction for adversarial strategies and practical robustness enhancements.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"The idea presented in the paper is very interesting and novel. Converting the adversarial attack problem into a linear programming problem and using adequate tools to solve it is a good catch. The authors have explained well the problems encountered during making such a conversion and strategies used to overcome it. E.g. using barrier method to remove need for projection, using first-order update rule instead of the usual minimization, dealing with discontinuity of $L^{\\\\infty}$ norm. Overall the problem is well formulated and explained.\\n\\nThe flow of the paper is clear and easy to understand. In terms of significance, this reformulation of adversarial attack is interesting and might be for the useful for the community to look at.\", \"Weaknesses\": \"The idea presented is novel but the experiments do not back it up well. \\n\\n1. The method finds examples that are less perturbed than other methods compared but the attack success rate is not upto par with other methods. In fact it is very low in some of the image representations used. The presented the trade-off between distance from original image and success rate does not show it to be very effective attack method. You can't really visually tell the difference between the adversarial images generated from this attack and PGD.\\n2. The attack methods are only compared on CIFAR - Imagenet would be a good addition as datapoint to the experiments.\\n3. No mention of the cost of attack, number of steps to reach the adv example and time taken by barrier method optimization.\\n4. For adversarial training, comparison has only been made with PGD. I believe other methods should be used in the comparison. \\n5. Again ImageNet not used for adversarial training, would be a good experiment to add.\\n\\nOverall the experiments do not showcase the effectiveness of method as adversarial attack or for training robust models compared to other state of the art methods. \\n\\nTypo:\\n\\nEqn 2 : $R^n$ -> $R^q$ ?\", \"Questions\": \"See weaknesses\"}"}]}
{"paper_id": "DUkYDXqxKp", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Autonomous driving has experienced significant growth in both academia and industry over the past decade. Traditional modular designs in autonomous vehicles use rule-based methods for perception, planning, and control. These systems can struggle with unseen scenarios, leading to failures. Learning-based methods have been developed to handle diverse situations through data-driven approaches. However, these systems often operate as black boxes, lacking interpretability, which poses ethical and legal challenges. Recent advances in multimodal large language models (LLMs) offer potential for enhancing interpretability by processing video and other non-text data into text form, although existing smaller language models have been limited in their flexibility and robustness.\nGap: Traditional end-to-end learning-based autonomous driving systems lack interpretability, functioning as black boxes that humans cannot understand. This has remained a significant challenge, hindering their trust and commercialization. Existing methods using smaller language models also fall short in providing adequate flexibility and generalization, especially when facing new or unexpected inquiries.\nHypothesis: Leveraging large language models (LLMs) in an end-to-end autonomous driving system can enhance interpretability by providing explanations for vehicle actions and reasoning, as well as answering diverse human inquiries. This approach could help demystify the black-box nature of current systems and improve user trust and interaction.\nMethod: The authors introduce DriveGPT4, a novel end-to-end autonomous driving system that utilizes a multimodal LLM framework. DriveGPT4 processes input from front-view RGB cameras and predicts vehicle control signals (speed and turning angle) in an end-to-end manner. It also provides natural language explanations and reasoning about the vehicle\u2019s actions in response to user inquiries. The system is trained using a custom visual instruction tuning dataset developed from the BDD-X dataset, which includes vehicle action descriptions, justifications, and control signals. ChatGPT is used to generate diverse conversation patterns to improve the model's conversational abilities. The model architecture includes a video tokenizer to convert video frames into text tokens, integrated with a large language model (LLaMA 2). Two training stages are used: pretraining on general video-text data and fine-tuning on the customized autonomous driving dataset.\nConclusion: DriveGPT4 demonstrates superior performance in comparison to conventional methods and video understanding models, both qualitatively and quantitatively. It provides better interpretability and control signal predictions and is capable of zero-shot generalization to unseen scenarios. The system successfully answers questions about vehicle actions and reasoning and shows potential for improving interaction and trust in autonomous driving applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"- The paper is basically clearly written and easy to understand.\\n- The authors demonstrate that the LLM-based method has the zero-shot generalization ability to other datasets. It is a good point to involve LLM or large-scale training for images and videos.\", \"Weaknesses\": \"Multiple overclaims and issues about the soundness.\\n\\n- *Important:* There are multiple sentences saying that \\\"constrained by the limited capacity of smaller language models, they can only address predefined human questions and provide inflexible answers, hindering their widespread application in real-world scenarios\\\". The reviewer admits that LLMs can answer out-of-domain questions, but is still wondering if the limitation lies in the LLM. I will explain this point based on the experiments.\\n  - The improvement of the fixed question-answering experiment is limited (82.1->82.4, especially considering this is evaluated by ChatGPT with great uncertainty). Meanwhile, LLMs take much longer inference time to output the much longer answers compared to small models and the original QAs. The computational complexity is not that important at the current stage but is still valuable for illustration.\\n  - The additional question-answering experiment demonstrates the effectiveness of DriveGPT4 which is finetuned on the generated data. Then how about fintuning ADAPT and other works on the data? The direct comparison seems unfair.\\n  - For the control prediction task, DriveGPT4 takes historical control signals as input while others do not. It can be observed from the ablation study, that the results of 'No control signals' are close to ADAPT-32. The contribution comes from the history information, rather than LLM, which is unfair.\\n  - It is an open question if flexible answers can help real-world applications of autonomous driving.\\n- It is not the **first** work on interpretable end-to-end autonomous driving. The authors have already listed several in the related works. Even language-based interpretability is not accurate as there are multiple types of interpretability. In my opinion, it is fair to say LLM-based. \\n- From the very beginning, is **interpretability** indeed the unsolved problem that hinders the commercialization and development of autonomous driving? As described in the first paragraph of the introduction, the popular modular-based methods' issue is not the interpretability.\\n\\nTechnical contribution is limited. There are contributions of instruction tuning for the dataset generation. The video tokenization part is based on Valley and the action part is very close to existing works such as RT-2.\", \"Questions\": \"- Why not use ground truth from the BDD dataset? The detection results from YOLOv8 could be inaccurate. I am wondering if there is a truck in the provided figure (Table 1).\\n- How bounding box coordinates are normalized? In which coordinates are they normalized?\\n- If the model can generalize to video games, it is worth trying generalizing to CARLA and evaluating it in a closed-loop manner.\\n- The short name 'DriveGPT4' is not very appropriate, as the core method is finetuned from LLaMA 2 and does not use GPT4.\"}"}]}
{"paper_id": "nUGFpDCu3W", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper explores the inner mechanisms of Transformer-based language models like GPT, specifically focusing on their ability to recall information learned during training. It highlights the challenge of understanding how these models work internally due to their complex, densely connected architecture. The study is part of the field of mechanistic interpretability, which aims to decipher the model computations into human-understandable components. Previous works have emphasized the crucial role of MLP layers as key-value memory stores, with early layers handling basic information storage and retrieval while later layers manage more complex tasks.\nGap: While it's known that language models store factual information in their weights, there's a lack of understanding regarding what information is stored, where exactly it is located within the model, and how it is retrieved. Specifically, the study looks at how GPT models handle tasks such as identifying matching brackets, which involve long-range dependencies, to better understand the information storage and retrieval mechanisms.\nHypothesis: The authors hypothesize that early MLP layers in Transformer models like GPT are responsible for storing and associating paired tokens, such as matching brackets. This association is expected to support existing theories about the hierarchical role of different layers within the Transformer architecture, where early layers store and retrieve basic (lower-level) information, while later layers deal with more abstract data.\nMethod: The study uses GPT-2 to conduct circuit analysis focused on a linguistic task: identifying closing brackets. Activation patching, embedding projections, attention analysis, and linear probes are utilized to trace how individual components of the model contribute to its final outputs. Short sequences with random tokens are used to isolate relevant behaviors, and experiments involve replacing specific activations with alternative values to see their effect on model predictions. The focus is on how information about open and closed brackets is stored and retrieved, using positive and negative patching to test the necessity and sufficiency of specific model components.\nConclusion: The study concludes that the first MLP layer in GPT models is responsible for storing and retrieving low-level linguistic information, such as bracket associations. This layer contains distinct representations for each type of bracket which can be swapped to influence predictions. While this corroborates earlier theories of MLP layers as key-value systems, the results show the retrieved information initially lies dormant until activated by later layers. This finding supports a hierarchical understanding of Transformer models, where different layers capture different types of dependencies or knowledge. Nonetheless, the results suggest differences in mechanism depending on the complexity of the task.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"- The authors performed a thorough study with different tests\\n- They show results which supports their claims\\n- The work represents a step forward towards understanding more DL\\nnetworks.\", \"Weaknesses\": \"- The outcomes are not surprising as it is expected that simple\\ninformation is stored in the first layer and as the information flows\\ninto deeper layers, more complex relationships are established\\n- In this sense, it is not very clear the significance of the reported\\nresults.\", \"Questions\": \"The authors focused mainly in short sequences and it is not completely\\nclear the behavior in much larger sequences.\\n\\nIt is not clear why the plots in Figure 7 are very different with\\ndifferent type of brackets. I was expected to see similar curves for\\nall types of brackets.\"}"}]}
{"paper_id": "VyMW4YZfw7", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses semi-supervised node classification (SSNC), which is a central task in graph-based learning where predictions are made on network data using features and network structure. Current solutions often involve complex Graph Neural Networks (GNNs) to exploit this structure.\nGap: Many modern GNN architectures are complex and may be more sophisticated than necessary for tasks like SSNC. Recent advances in GNNs might be partially attributed to changes in evaluation standards rather than genuinely improved models.\nHypothesis: Simpler, traditional nonparametric techniques applied in the spectral domain can replace many existing complex GNN designs while maintaining or surpassing state-of-the-art (SOTA) performance on SSNC benchmarks.\nMethod: The authors propose using classical nonparametric methods in the spectral domain to construct simpler GNN models. They implement a one-layer linear GNN model using different kernel functions and low-rank approximations of the adjacency matrix to create learnable propagation structures. The experiments evaluate these simplified models on common SSNC benchmarks like Cora, Citeseer, Pubmed, and others, comparing performance against existing SOTA models.\nConclusion: The proposed simpler spectral methods can achieve competitive performance compared to more complex GNN architectures. The absence of post-model augmentations in their method allows for cleaner theoretical analysis. The study also highlights how changes in evaluation conventions, such as dataset splits, can impact perceived model performance, suggesting that some improvements may not be purely due to innovative models.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 1, \"Confidence\": 4, \"Strengths\": \"Addressing the use of simpler methods to match and outperform complex DNN-based architectures.\", \"Weaknesses\": \"Lack of Novelty: The paper does not present significant novelty. While it challenges the use of complex GNNs with spectral approaches, it falls short of providing a strong theoretical analysis or thorough experimental validation to support its claims. To enhance the paper's contribution, it should either include a rigorous theoretical analysis or conduct additional comprehensive experiments, preferably in supplementary material.\\n\\nLimited Experiments: The paper contains a limited number of experiments, which is insufficient given the broad claims made. A supplementary material section should be added to provide a more thorough evaluation, including comparisons with existing methods, diverse datasets, and various hyperparameter settings.\", \"Questions\": \"- What motivated your choice of the spectral truncation factor and the choice of kernel functions? Did you conduct any sensitivity analysis on these choices?\\n\\n- Have you considered comparing your proposed approach with existing GNN architectures on a broader set of benchmark datasets to ensure the generality of your claims? The paper lacks an extensive experimental investigation on multiple benchmarks and algorithms with several ablation studies regarding the claim of the paper? Or rather a theoretical study of the phenomenon observed.\\n\\n- Could you elaborate on the limitations and assumptions of your method? Are there scenarios where your approach may not be as effective, and if so, what are they?\\n\\n- There are multiple typos in the paper. Could you please proofread?\"}"}]}
{"paper_id": "SUUckMrHNY", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Camouflaged instance segmentation (CIS) is a challenging task within computer vision, as camouflaged objects blend seamlessly with their background, making standard detection and segmentation difficult. Transformers have been successful in various vision tasks but require large-scale data. Recent advancements have produced large datasets for camouflaged object detection, such as COD10K and NC4K, allowing transformers to achieve superior performance in this field. However, current models struggle with boundary precision, often misidentifying parts of camouflaged objects as distinct entities.\nGap: Current transformer models for camouflaged instance segmentation do not accurately identify the boundaries of camouflaged objects. This limitation is due to a primary focus on object detection rather than boundary precision, resulting in multiple object predictions for single camouflaged targets when edges are unclear.\nHypothesis: Incorporating traditional computer vision descriptors, specifically Local Binary Patterns (LBP), into the transformer model will improve boundary detection for camouflaged objects. By extracting edge-sensitive local features through the proposed local-feature-aware transformer ('\u03b1-Former'), the performance on camouflaged instance segmentation tasks can be enhanced.\nMethod: The \u03b1-Former integrates traditional descriptors with transformers using a unique pipeline. It uses a binary filter (inspired by LBP) to extract edge-sensitive local features, and a feature aggregation adapter (FAA) to incorporate these features into the transformer without altering its input distribution. The model is evaluated against datasets COD10K and NC4K with an encoder-decoder design that includes an edge-aware feature fusion module, enhancing boundary predictions. A comprehensive set of ablation studies validates the contribution of each module to performance improvements.\nConclusion: The \u03b1-Former outperforms existing state-of-the-art solutions in camouflaged instance segmentation on the COD10K and NC4K datasets by 2-3 average precision points. The combination of binary filters, feature aggregation adapters, and edge-aware modules effectively enhances boundary prediction and segmentation accuracy of camouflaged objects, demonstrating the feasibility and robustness of integrating traditional descriptors into transformer-based frameworks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"* State-of-the-art performance on the camouflaged instance segmentation (CIS) task.\\n* Reasonable module design, such as learnable binary filter.\", \"Weaknesses\": \"* The effectiveness of the feature aggregation adapter is limited.\\nAccording to Table 2 in this paper, using adapter has a negative impact on the performance of traditional descriptors on NC4K. For example, AP of (LBP w/o adapter) 42.288, AP of LBP w/ adapter 41.794. And same situtation for Circle-LBP. Even for the Binary filter, the improvement is very limited. (42.610->42.936).\\n* Typo: There exists a mismatch figure caption between Figure 7 in this paper and Figure 2 in supplementary_material. (Predicted edge and Ground truth).\\n* Complex pipeline and missing ablation study: This paper introduces attention module, positional encoding, encoder, decoder, but missing results to demenstrate the effectiveness of each components and how each component help the camouflaged instance segmentation (CIS) task.\", \"Questions\": \"See Weakness.\"}"}]}
{"paper_id": "N2WchST43h", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Adversarial training is a strategy to build deep neural networks resistant to adversarial perturbations, where an adversary perturbs input data to maximize prediction loss and the learner updates the network to minimize this loss. Recent research focuses on analyzing the convergence of adversarial training using the neural tangent kernel (NTK) theory, particularly for large over-parameterized networks. However, adversarial training is computationally expensive, with time complexity traditional methods being \u2126(mnd) per iteration, due to the requirement to compute forward and backward passes for a network of width m, n input data of d dimensions.\nGap: The existing methods for adversarial training of neural networks are computationally intensive, with time complexity tied to the network's width, the number of samples, and the dimensionality of the input. The dominant cost per iteration is due to the activation of a large number of neurons, resulting in inefficiency and high computational expense, especially for large over-parameterized networks.\nHypothesis: The authors hypothesize that adversarial training can be made more efficient by leveraging the fact that only a sublinear number of neurons are activated in each network iteration. By using a shifted ReLU activation function and a novel data structure for neuron activation reporting, the time complexity per iteration can be reduced significantly to o(mnd).\nMethod: The authors analyze a two-layer neural network using a shifted ReLU activation function, which determines that fewer than o(m) neurons are activated per input data. They develop a high-dimensional search data structure (based on half-space reporting) to identify activated neurons efficiently. Through constructing a pseudo-network, they approximate the original neural network function and implement a gradient descent-based training algorithm (Algorithm 1) using this efficient neuron activation identification approach. Their method theoretically ensures convergence and significantly reduces the time complexity to (cid:101)O(m1\u2212\u0398(1/d)nd) per iteration.\nConclusion: The paper concludes that utilizing a shifted ReLU activation function and high-dimensional search data structures can successfully reduce the computational complexity of adversarial training. This advancement enables training of robust neural networks more efficiently, making it possible to perform adversarial training on large-scale networks with lower computational cost without compromising on convergence or robustness.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 3, \"Strengths\": \"The approach is novel where adversarial training that leverages geometric data structures at the expense of extra space. It extends the direction of first identifying the active neurons and then only taking dot product with those neurons. \\n\\nThe paper is well written and however, without empirical results or illustrative examples, the clarity of the paper's practical implications could be enhanced.  \\n\\nThe work is significant as it has a potential to further improve the direction of efficient adversarial training of robust neural networks.\\n\\nI have not conducted an exhaustive review of the convergence proofs; however, upon initial examination, they appear to be correct.\", \"Weaknesses\": \"The study lacks experiments, even on toy datasets, to determine whether its findings are applicable in practical scenarios.\\n\\nFurthermore, it has not been compared with existing works. Although it is primarily theoretical, clarity on how it measures up against established methods is missing.\\n\\nThe paper does not discuss how reducing training time might impact the robustness performance of the model. This trade-off between efficiency and robustness is crucial for practical use cases.\\n\\nLikewise, the trade-off between memory usage and efficiency warrants discussion. While the paper could comment on memory requirements, it may be that for only two-layer networks, the difference is minimal. However, it would still be beneficial to understand this aspect, perhaps by employing very wide neural networks to confirm it in a practical scenario.\\n\\nIt is also unclear whether the proposed methods can be effectively translated into actual GPU computations. If the approach results in underutilized GPUs, it might still be valuable but may not reduce the overall wall time. Some commentary on this feasibility would be constructive.\", \"Questions\": \"To translate the theoretical advantages into practical scenarios, the paper should present at least some toy experiments.\\n\\nIt is important to explore how reducing training time affects the model's robustness performance, as the trade-off between efficiency and robustness is crucial for practical use cases.\\n\\nThe same consideration should be given to the trade-off between memory usage and efficiency. It would be beneficial for the paper to comment on memory requirements. While the impact on only a two-layer network might be minor, understanding this trade-off is essential, and could be tested using neural networks with extremely high widths in practical scenarios.\\n\\nIn short, a toy experiment featuring a two-layer network that showcases practical, minimal use cases by comparing memory usage, computational efficiency, and robustness performance would be a valuable addition to the work.\"}"}]}
{"paper_id": "eGdhD93hZr", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Voice synthesis has seen significant advancements with the development of various applications, such as text-to-speech and voice conversion, usually built independently with distinct methodologies. The majority of current models rely heavily on annotated data. There is a growing interest in scaling these models to handle the diverse acoustic variations of human voice using self-supervised datasets, which can capture speaker identity, emotion, and prosody effectively.\nGap: The challenge lies in achieving zero-shot generalization for voice synthesis models when test data has acoustic diversities unseen during training. Additionally, existing models are developed in isolation for specific applications, resulting in scattered methodologies that lack a unified approach to voice generation and manipulation across languages and tasks.\nHypothesis: The authors propose MVoice, a large multimodal language model that can synthesize and manipulate multilingual voice signals at scale. The hypothesis is that MVoice, with its discrete token-based 'coarse-to-fine' design, can resolve scaling issues, provide superior voice quality, and adapt to various voice synthesis tasks in a unified framework without needing extensive annotations.\nMethod: MVoice employs a 'coarse-to-fine' design using self-supervised learning techniques: (1) Semantic Stage, which extracts semantic tokens from text or speech input to determine desired meanings, and (2) Acoustic Stage, which generates acoustic tokens conditioned on semantics. The model leverages a dataset of approximately 200K hours in 6 languages and covers various voice synthesis applications. MVoice uses a decoder-only transformer architecture to map inputs to semantic and acoustic tokens and integrates a unit-based vocoder to generate high-fidelity waveforms.\nConclusion: MVoice demonstrates state-of-the-art results in multilingual and cross-lingual zero-shot voice generation. It excels in generating high-quality, stylistically similar audio across various applications without requiring extensive supervisory data. The study highlights MVoice's scalability and control flexibility for handling diverse voice synthesis tasks. The model, however, has limitations concerning audio token dependency and computational demands for longer sequences.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"- Originality: \\nEven though discrete unit-based generative models been investigated before (e.g., Polyak et al. 2021, https://arxiv.org/pdf/2104.00355.pdf), this paper adds AR modeling and controllability to obtain a more unified model in terms of the variety of that can be performed with the same model. Some advantages of the model are\\n1. Experiments usually achieve good performance on several speech tasks as compared to the previously published studies. \\n2. Being controllable is an advantage of the model to adapt it to different use cases. \\n3. The modularity (semantic stage and then an acoustic stage) allows the model to perform different tasks.  \\n\\n- Quality: \\nThe paper proposed a two-stage speech model (semantic + acoustic models) which makes it somewhat interpretable. Experiments have been conducted on various tasks and the performance of those are on par with that of previous studies if not better. \\n\\n- Clarity: \\nAt a high-level, the paper is relatively easy to understand, but in terms of details, some parts have been left to the Appendix and it might make the paper difficult to read at times. \\n\\n- Significance: \\nSimilar to the comments in Originality above. This is a study towards investigating unified generative speech models.\", \"Weaknesses\": \"1. Experiments do not always show gain, and when they outperform, the improvements are limited. \\n\\n2. Even though the paper title mentions multilingual unified speech representations, the results on multilingual condition are worse than the monolingual ones. \\n\\n3. Overall, the main gains are coming from improved speaker similarity scores and not in terms of other evaluation criteria. \\n\\n4. The paper already mentions a few shortcomings and these are valid concerns. Regarding the dependency of the performance on the tokenizer quality makes the reason for using discrete tokens questionable. If there were continuous features, would it be better? If so, then wouldn't the first stage lose its property, i.e. being interpretable as semantic stage?\", \"Questions\": \"1. If there were continuous features, would the model be better? If so, then wouldn't the first stage lose its property, i.e. being interpretable as semantic stage?\\n\\n2. Which ASR models have been used to obtain the WERs on the synthetic datasets?\"}"}]}
{"paper_id": "le1UUMd45T", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Multiobjective combinatorial optimization problems (MOCOPs) are critical in various fields like routing, logistics, and planning, requiring solutions that consider multiple, often conflicting objectives. Traditional methods, such as evolutionary algorithms, have been employed to approximate Pareto optimal solutions, with recent advances in deep reinforcement learning (DRL) showing promise in solving single-objective combinatorial optimization problems (COPs). However, these have limitations in addressing MOCOPs effectively.\nGap: While DRL has been used to tackle MOCOPs, existing models based on the 'Learn to Construct' paradigm struggle with achieving sufficient proximity and diversity of solutions towards the true Pareto front. They heavily depend on model quality and lack comprehensive exploration of potential Pareto solutions in subproblem neighborhoods.\nHypothesis: The authors propose that embedding weight-related policy networks into multiobjective evolutionary algorithm (MOEA) frameworks within a 'Learn to Improve' paradigm can effectively guide the search direction and improve the quality and diversity of solutions for MOCOPs.\nMethod: The method involves a weight-related policy network (WRPN) designed to be integrated into MOEA frameworks like NSGA-II, MOEA/D, and MOGLS. It guides improvement operations based on a proximal policy optimization algorithm with a shared baseline to reduce variance. A quality enhancement mechanism improves the Pareto set via instance augmentation. Experiments were conducted on the multiobjective traveling salesman problem and multiobjective vehicle routing problem, demonstrating significant improvements over existing methods.\nConclusion: The proposed WRPN within the 'Learn to Improve' paradigm outperforms state-of-the-art methods, providing improvements in both proximity and diversity of solutions on classic MOCOPs. The approach, with its quality enhancement mechanism, shows superior results compared to existing DRL approaches and classical solvers like LKH under weighted decomposition schemes.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"L2I introduces a new DRL-based approach for MOCOPs. Unlike the traditional \\\"Learn to Construct\\\" methodology, L2I emphasizes iterative improvements. The L2I module has demonstrated adaptability as it can be integrated into various MOEA frameworks, such as NSGA-II, MOEA/D, and MOGLS. This mechanism, applying instance augmentation techniques, improves both the proximity and diversity of the Pareto set. The L2I methodology outperforms other state-of-the-art techniques on standard MOCOPs, even showing better performance than renowned solutions like the LKH solver for specific problems.\", \"Weaknesses\": \"Since the paper deals with the combinatorial optimization, decomposition methods are not sufficiently elaborated/reviewed.\", \"Questions\": \"How does L2I compare with methods follows a general scheme of \\\"learn-divide-and-conquer\\\" or \\\"divide-learn-and-conquer\\\"? In a sense, there are approaches that learn how to decompose a problem and there are approaches that decompose a problem before learning. I qualitative assessment may be sufficient.\"}"}]}
{"paper_id": "H6pf70GZVU", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Incremental learning (IL) is a method to enable AI systems to learn new tasks over time without catastrophic forgetting. Catastrophic forgetting occurs when a neural network forgets old tasks while learning new ones. Traditional methods address this by storing previous data in a memory buffer, but this is limited by memory size and privacy concerns. Recently, non-exemplar IL methods have emerged that store class centroids as prototypes to generate synthetic data, although these methods often have shortcomings such as proximity of class prototypes and inappropriate data distributions.\nGap: The current non-exemplar IL methods store class centroids and use Gaussian noise for data augmentation, which may lead to overlapping synthetic data and forgetting due to inappropriate representation of the data distribution. There is a need for methods that can maintain distinct, non-overlapping class representations without these limitations.\nHypothesis: Storing and optimizing a single prototype per class can significantly outperform current methods, avoiding the need for synthetic data replay and reducing catastrophic forgetting by ensuring that class prototypes are compact and distant from others in the representation space.\nMethod: The proposed method, YoooP, involves optimizing a single prototype per class using an attentional mean-shift algorithm to move the prototype to a high-density region of its class and minimizing its similarity to other classes. A variant, YoooP+, extends this by synthesizing replay data that preserves the angular distribution between the prototypes and historical data, avoiding Gaussian noise. The model is trained using a combination of prototype optimization, contrastive loss, and knowledge distillation to prevent catastrophic forgetting.\nConclusion: YoooP and its extension YoooP+ significantly outperform existing non-exemplar IL methods in terms of both accuracy and reduced forgetting. YoooP+ effectively stabilizes YoooP by using prototype-based synthetic data that mimics the original data distribution. These results demonstrate a new perspective on optimizing class prototypes for exemplar-free class-incremental learning.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"This paper proposes a novel approach to leave the class representative prototype considering the similarity with all real samples. The proposed method aims to be an efficient technique that can be easily applied to the existing prototype-based method.\\n\\nThis paper also proposes a data augmentation performed with the help of a rotation matrix. This strategy approximates real distribution better than conventional prototype augmentation techniques.\\n\\nIn base-0 experiments on CIFAR-100 and Tiny-ImageNet, the proposed methods achieve the best performance.\", \"Weaknesses\": \"-\\tThe scale of dataset (CIFAR-100, TinyImageNet) is small. Whether the proposed method works with bigger datasets is an important issue that needs to be addressed. \\n-\\tThe paper lacks the result with 20 phases which is a quite common configuration. \\n-\\tThe effect of an optimized prototype based on the attentional mean-shift method seems limited in many situations.\", \"Questions\": \"1) For verifying the strength of proposed augmentation, why don\u2019t you apply the new strategy to PASS instead of adding Gaussian noise to prototype? Would you show the performance of it?\\n\\n2) Could you show the classification confusion matrix result of your approach?\\n\\n3) In section 3, when define the probabilities over all classes C_(1:t ), isn\u2019t softmax(wG(F(x;\u03b8))) more accurate notation than softmax(wF(x;\u03b8))?\\n\\n4) The function c(,) in equation 2, equation 4 seems to be different. Is the c(,) in equation 2 cosine similarity function and one in equation 4 classifier (=G(F(p_k;\u03b8)))?\\n\\n5) How can we define the initial class-wise prototype p_k? Is it the same with the prototype used in PASS? (k-th class mean prototype)?\"}"}]}
{"paper_id": "g6Urb6ukm7", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The study of multivariate time series (MTS) is crucial across various fields due to its complexity involving synchronous variables over time. Using MTS label information is challenging due to label sparsity. Unsupervised representation learning methods, particularly contrastive learning, have been explored to generate effective MTS representations without label information, achieving promising results in various tasks.\nGap: Traditional contrastive learning methods for time series do not adequately consider the spatial relationships between the variables in MTS and exhibit high sensitivity to positive pair construction, leading to performance degradation. These limitations hinder the generation of robust and informative MTS representations suitable for diverse tasks like classification and forecasting.\nHypothesis: The authors propose that by employing a spatio-temporal contrastive representation learning approach (STCR), it's possible to generate high-quality representations that account for both spatial relationships and temporal dependencies in MTS. This approach aims to effectively mitigate the sensitivity to positive pair construction inherent in conventional methods.\nMethod: The method involves the proposed spatio-temporal contrastive representation learning approach (STCR) that encourages spatio-temporal consistency. The STCR framework includes random cropping, temporal embedding using random masking, spatial embedding with a K-nearest neighbor graph, and a projection head to generate representations. The framework uses two contrastive loss functions\u2014instance-wise and timestamp-wise\u2014to ensure learned representations capture both temporal and spatial information. Extensive experiments on classification and forecasting tasks are conducted to validate the method's efficacy, compared against state-of-the-art methods.\nConclusion: STCR generates informative representations that significantly outperform state-of-the-art methods on multivariate time series classification and forecasting tasks. While effective at capturing spatial and temporal relationships, STCR has limitations in processing times and datasets with very few variables. Future work may focus on improving computational efficiency and adapting the approach for datasets with limited spatial information.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"Experiments are extensive and the motivations are interesting.\", \"Weaknesses\": \"1. This work lacks novelty. The overall structure is similar to TS-TCC [1] and TS2Vec [2]. For example, compared to TS2Vec, this manuscript is just adding GCN to consider spatial dependencies within MTS data.\\n\\n[1] Time-Series Representation Learning via Temporal and Contextual Contrasting\\n\\n[2] TS2Vec: Towards Universal Representation of Time Series\\n\\n2. References are too old, and the authors have done limited literature review. There are works highly related to this work. For example, GCC [3] also conducted graph neural network and explore the effects of spatial consistency. I think the authors should discuss their differences.\\n\\n[3] Graph Contextual Contrasting for Multivariate Time Series Classification\\n\\n3. Contributions have not addressed the problems that the authors pointed out. The authors pointed out the limitation in the existing augmentation methods, i.e., existing methods cannot consider the distribution changes in time-series data. To address the limitation, the authors proposed a method namely random cropping. I cannot see the difference between the random cropping and existing method subseries. Meanwhile, I don't think that the proposed method can address the mentioned problem. In the example of Figure 2, the signals within [s1, s2] are still having different distribution with [e1, e2]. Following the point of the authors (changing distribution), the two subseries cannot be treated as positive pairs.\\n\\n4. The authors mentioned the importance of achieving spatial-temporal consistency. However, how to define the spatial consistency? Why can spatial consistency be achieved after you construct and process a K-NN graph with GCN?\", \"Questions\": \"1. What is the differences between this work and existing studies, such as GCC [3].\\n\\n[3] Graph Contextual Contrasting for Multivariate Time Series Classification\\n\\n2. Please clarify the reason why random cropping can address the problem of distribution changes.\\n\\n3. How to define spatial consistency, and why can spatial consistency be achieved after you construct and process a K-NN graph with GCN?\"}"}]}
{"paper_id": "LWDRiFzbHQ", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper addresses the need to evaluate the generalization capacity of classification models on out-of-distribution (OOD) test data without relying on ground truth labels, which is important due to the cost of obtaining test ground truths in real-world scenarios. Existing works propose unsupervised risk measurements to capture model properties such as confidence and invariance, which correlate with OOD accuracy. However, these indicators are often computed individually for each test sample, which can lead to unreliable results due to spurious model responses.\nGap: Existing OOD generalization assessment methods use isolated test samples to compute risk proxies like confidence and invariance, which can result in spurious model responses (i.e., excessively high or low confidence), thus compromising the reliability of the proxy as an indicator of generalization ability.\nHypothesis: The hypothesis proposed is that by integrating model responses of neighboring test samples into the correctness indicator of each test sample, the predicted generalization ability can be enhanced. This is because samples with high correctness scores among nearby samples are likely also to be predicted correctly, thus providing a more holistic measure of model accuracy.\nMethod: The proposed method introduces the Vicinal Risk Proxy (VRP), which incorporates neighboring samples' responses into the risk proxy calculation for each test sample, using a similarity-weighted sum approach. This vicinal assessment smooths out spurious scores and provides a more robust measure of model accuracy without relying on labels. The VRP can be applied to various generalization indicators like average confidence or effective invariance, showing consistent improvements over these baselines across multiple datasets and test scenarios.\nConclusion: The vicinal risk proxy provides a stronger correlation with model accuracy, especially on severe out-of-distribution test sets, by leveraging neighboring sample responses. This result demonstrates the effectiveness of VRP in addressing spurious responses, as corroborated by experiments showing improved correlations over existing methodologies. The approach broadens the applicability of risk estimation techniques while increasing reliability in evaluating model generalization.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"- The author provides very clear examples to introduce their method, which allows me to quickly understand the work done in this paper. In particular, Figure 1 helps me to understand the work done in this paper and how it is implemented as soon as I see it.\\n\\n- The author's formulas are very clear, especially the repeated declarations of symbols in each formula, which prevents me from having to flip back to the beginning of the paper to find the definition of a symbol when I encounter it later in the paper.\", \"Weaknesses\": \"The innovation of the paper is insufficient. In fact, as early as the beginning of 2020, I had tried a similar idea, which is almost identical to the approach described in this paper, and the paper only describes my first version of the attempt. Here is my experience and why I did not continue this research:\\n\\n\\n- Calculating d\u00b5 involves a significant amount of additional time, so I abandoned this approach since the performance of ensemble methods can surpass many calibration methods, and their biggest drawback is the substantial inference cost. If a method cannot outperform an ensemble in terms of time efficiency, its performance needs to surpass that of the ensemble, but I found that the performance of this method cannot exceed the ensemble method.\\n\\n- To address this issue, I used a neural network to fit the similarity of samples, i.e., training an additional branch on the original network to fit the vicinal score, which transfers the additional time cost to the training process and significantly reduces the time spent during the inference process. However, this method is still unsatisfactory for two reasons: \\n \\n  - 1. Confidence suffers from severe overfitting, as the confidence of the vast majority of samples in the training set is 1. \\n\\n  - 2. The proxy estimation of confidence depends on the surrounding samples, which means that it requires good confidence estimation of the surrounding samples, so this estimation does not fundamentally solve the problem.\\n\\n- Finally, I changed my strategy and used true class probability (TCP[1]) as the confidence of the vicinal sample, which can effectively alleviate the problem of poor confidence estimation of the vicinal sample, to some extent solving the issues.\\n\\nHowever, I ultimately did not continue to study this work because the method is not elegant enough.\\n\\n_[1] Corbi\u00e8re C, Thome N, Bar-Hen A, et al. Addressing failure prediction by learning model confidence[J]. Advances in Neural Information Processing Systems, 2019, 32._\", \"Questions\": \"I have no other questions about this paper. I hope the author can continue to conduct more in-depth research, and the previous research I just described can provide useful information for the author.\"}"}]}
{"paper_id": "eO6lXIWyxn", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Diffusion models have shown significant capabilities in generating a wide range of visual content, but they face challenges in accurately rendering text within images, often resulting in incorrect characters or poor integration with backgrounds.\nGap: Existing diffusion models struggle with text generation, specifically in producing accurate characters or words that integrate well with background images.\nHypothesis: Introducing a novel framework named ARTIST with an additional textual diffusion model focused on text structure learning can enhance the text rendering ability of diffusion models.\nMethod: The ARTIST framework involves pretraining a textual diffusion model to focus on text structure. This model is then used to fine-tune the visual model, allowing it to incorporate textual structure into images more effectively. Additionally, large-language models are utilized to better infer user intentions, enhancing generation quality. The method's performance is demonstrated on the MARIO-Eval benchmark.\nConclusion: The proposed method, ARTIST, significantly improves text rendering in diffusion models, achieving up to a 15% improvement in various metrics on empirical evaluations.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 5, \"Strengths\": \"- This paper is well-written and easy to follow.\\n- Text painting is a challenging task for current T2I models, and the idea of disentanglement between text and figure is well-motivated.\\n-  They provide lots of qualitative examples for the visual comparison.\\n- Their proposed ARTIST achieves notable improvements over the previous TextDiffuser, especially on the crucial OCR metric.\", \"Weaknesses\": \"- It seems that the ARTIST framework is the combination of LLM-Layout and TextDiffusers, which both have been proposed before. Not sure if this achieves ICLR's novelty bar.\\n- There are so many metrics used for the evaluation (FID, CLIP-S, and OCR). Which one is the most appropriate to evaluate the overall performance? Or is there any way to combine all of them as a final metric?\\n- As a generative task, a human generation should be conducted to compare the performance in a human aspect.\\n- Since ARTIST relies on LLM to derive the layout, there should be a discussion about the quality of the generated layout. Is it visually appealing or in a reasonable position?\\n- Missing reference about LLM-Layout: [NeurIPS'23] Compositional Visual Planning and Generation with Large Language Models / [arXiv'23] Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models\", \"Questions\": \"Please see the weaknesses\"}"}]}
{"paper_id": "fBlHaSGKNg", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Semi-Supervised Learning (SSL) has gained prominence due to its ability to harness large amounts of unlabeled data in scenarios where labeled data is scarce and costly to obtain, such as in fields requiring expert annotation like healthcare and finance.\nGap: Most existing SSL studies do not adequately address the efficient selection of samples for annotation under budget constraints. Previously, focus has been on leveraging labeled and unlabeled data to train models, while the selection of labeled samples has often been overlooked, which can impact the performance, especially with limited annotation budgets.\nHypothesis: Minimizing a novel criterion called \u03b1-Maximum Mean Discrepancy (\u03b1-MMD) through an efficient sample selection methodology named Unleashing the Power of Annotation (UPA) can enhance the generalization ability of SSL by selecting a diverse and representative subset for annotation, even under budget constraints.\nMethod: The approach employs a modified Frank-Wolfe algorithm, Generalized Kernel Herding without Replacement (GKHR), to minimize \u03b1-MMD. This method selects samples that balance representativeness and diversity. The UPA method's effectiveness is tested through experimental integration with popular SSL frameworks like FlexMatch and FreeMatch across datasets such as CIFAR-10, CIFAR-100, and SVHN, using a Gaussian kernel with CLIP extracted features for sample selection.\nConclusion: UPA significantly improves SSL performance across various setups by efficiently selecting diverse and representative samples for annotation, surpassing existing active learning (AL) and semi-supervised active learning (SSAL) methods even under constrained budgets. This demonstrates its potential for real-world applications where annotation resources are limited.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"1.\\tAn approach the optimizes both diversity and representativeness has advantage\\n2.\\tSome theoretical results support the advantage of the method.\\n3.\\tEmpirical results show some advantage over Random baseline and for cifar 10 clear advantage to using flexMatch with UPA\", \"Weaknesses\": \"1.\\tThe soundness of the paper is poor: \\n \\na.\\tmostly the empirical validation is lacking additional datasets to support the advantage of the method\\n\\nb.\\timprovement over random baseline is too modest in my view, 0.5 precent improvement when the STD is at 0.5 is not convincing\\n\\nc.\\twe are missing a simple ablation study: if you show results of flexMatch+UPA, you have to show also results of just flexMatch, otherwise it is not clear if the advantage is due to FlexMatch or the addition of UPA.\\n\\nd.\\t3 independent runs is not enough\\n\\ne.\\tWhy are you selecting the particular m values for each data set? why is it different for each data set? I would much prefer seeing a graph over a range of values.\\n\\nf.\\tNot enough method are used in the comparison, what about BADGE by Ash et al -  it is a classical diversity and uncertainty approach that could also be compared to show that representativeness and diversity is better (if it is indeed so\u2026)\\n\\n2.\\tThe requirement that m/n <=0.2 is quite weak and unrealistic, sometimes 0.2 can be a huge data set to annotate! How does that work with Active learning setting in which the budget is limited.\\n3.\\tClarity is also poor:\\n\\na.\\tThere is no pseudo code describing the method, or at least a set of steps\\n\\nb.\\tWhat is the input space over which the method is used? Is it the actual data? For coreset is it the penultimate layer representation, what are you using?\\n\\nc.\\tRemark 1 is rather confusing, if you present Thm 3.1 and then claim that thm 3.1 doesn\u2019t always work\\n\\nd.\\tFigure 1 can not be deciphered with out a basic explanation\\\\legend for the colors\\n\\ne.\\tEquation 6: is f_I_p(x_i) why is it defined only for x?\\n\\nf.\\tIn equation (8) what is K in B=2K?\\n\\n4.\\tThe computational complexity of O(mn) should be better explained? Wouldn\u2019t the kernel construction cost more?\\n\\n5.\\tOverall Im not convinced at all that the approach of diversification and representativeness is an optimal one. There are other important trade offs in active learning such as exploration-exploitation, which can outperform this UPA approach, in my view.\", \"Questions\": \"1.\\tPlease see the questions above in the \u2018weaknesses\u2019\\n2.\\tPlease explain what does the flexMatch method do , and why you chose it for your UPA approach.\\n3.\\tWhy do you think the diversification and representativeness is the best one for AL?\"}"}]}
{"paper_id": "B0OwtVEejJ", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Neural Architecture Search (NAS) methods based on weight-sharing enable the exploration of cell-based architecture spaces efficiently via supernets, reducing computational complexity. Gradient-based NAS methods like DARTS and DrNAS optimize architecture weights through gradient descent, traditionally focusing on cell-based spaces. Alternatively, weight-entanglement for macro-level search spaces allows for parameter sharing between similar operations, reducing memory usage by only requiring storage for the largest architecture in the search space. Typically, weight-entanglement methods involve a two-stage process: training a supernet followed by blackbox search using performance proxies from subnetworks.\nGap: The main gap identified is the lack of integration between weight-entanglement and gradient-based NAS methods. These two paradigms have primarily developed in isolation within separate communities due to compatibility issues. Existing methods have not effectively combined the benefits of gradient-based optimization with weight-entangled architecture spaces, which limits the potential for optimizing more complex and larger search spaces using gradient-based techniques.\nHypothesis: The authors propose that adapting gradient-based NAS methods to operate effectively within weight-entangled spaces can yield the advantages inherent to both paradigms: the efficient exploration and performance of gradient-based methods and the reduced memory requirements of weight-entangled approaches.\nMethod: The authors introduce TangleNAS, a generalized approach that extends single-stage NAS methods to weight-entangled spaces. This is achieved by superimposing operation weights, allowing for weight-sharing across different operation choices by zero-padding them to the size of the largest operation and summing them with architectural parameters. The approach integrates gradient-based NAS optimizers like DrNAS in weight-entangled spaces without additional memory overhead. The method is validated through comprehensive experiments across various search spaces, including cell-based, convolutional macro-level, and transformer architectures, using benchmarks like NASBench201, DARTS, and AutoFormer.\nConclusion: TangleNAS successfully bridges gradient-based and weight-entangled NAS methods, offering improved any-time performance, lower memory usage, and high accuracy in both cell-based and macro-level search spaces. It demonstrates that combining the advantages of both paradigms can lead to more efficient and effective neural architecture search. Empirical results show that TangleNAS outperforms conventional two-stage methods on several benchmark tasks, offering a compelling approach for NAS in complex architecture spaces.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"The paper proposes a scheme to adapt gradient-based methods for weight-entangled spaces in neural architecture search (NAS). This integration of weight-entanglement and gradient-based NAS is a new approach that has not been explored before. The paper also presents a comprehensive evaluation of the properties of single and two-stage approaches, including any-time performance, memory consumption, robustness to training fraction, and the effect of fine-tuning.\", \"Weaknesses\": \"1. The novelty is limited. Some works have also suggested that the weights of large kernel convolution operations can be shared in differentiable neural architecture search. For example, MergeNAS: Merge Operations into One for Differentiable Architecture Search (in IJCAI20)\\n2. The performance improvements are limited compared with the baselines.\", \"Questions\": \"1. How to entangle non-parameter operations, such as skip or pooling, as they have no weight compared to convolution.\\n2. The search cost of original DrNAS is 0.4 GPU-Days in DARTS search space. Whereas, in Table 4, the search time is 29.4 GPU-Hours. Can the authors explain the reason for the different search cost?\\n3. It would be beneficial to provide a more detailed explanation of the rationale behind the selection of specific search types, optimizers, and supernet types for the comparative evaluation.\u00a0\\n4. To discuss potential limitations and challenges in implementing the proposed scheme for adapting gradient-based methods for weight-entangled spaces would provide valuable insights. This could include addressing potential constraints, trade-offs, and practical considerations in real-world implementation.\"}"}]}
{"paper_id": "LfhG5znxzR", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Understanding neural networks is difficult because their hidden states are dense and continuous, making it hard to decipher their operations. Traditional software systems have sparse and discrete states, providing clarity and interpretability which neural networks often lack. Prior efforts have struggled with the superposition of features and the complexity arising from continuous activations.\nGap: The problem addressed is the lack of interpretability and control in neural networks due to their dense and continuous hidden states, which hinder understanding and manipulation of model behaviors.\nHypothesis: Introducing sparse and discrete bottlenecks, termed codebook features, into neural networks can enhance interpretability and controllability without significantly degrading performance. These bottlenecks use vector quantization to represent activations as discrete codes from a large codebook, making features sparse and interpretable.\nMethod: The authors propose a method involving finetuning neural networks, particularly Transformers, with trainable vector quantization bottlenecks at each layer. The method refines the network to replace continuous activations with a sparse combination of code vectors from a codebook. They apply these codebooks to the hidden layers of neural networks including a finite state machine dataset and natural language datasets, examining the assignment of network states to discrete codes and the causal impact of codes on network outputs. They use cosine similarity to select the most representative codes for activation, and validate their approach by examining the interpretability and control of these codebook features across tasks.\nConclusion: The method successfully creates neural networks with hidden states that are sparse and discrete, which enhances interpretability and provides a mechanism for controlling network behavior. The proposed codebook features enable networks to operate effectively with sparse representations, allowing them to maintain performance while providing intuitive ways to manipulate and understand their operations. This approach opens pathways for further interpretability and control in broader neural network architectures.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"* This work considers the problem of interpreting the intermediary layers and controlling the output of transformer networks, which are of significant interest to the machine learning community. Furthermore, quantizing features with codebooks is a popular technique which the work demonstrates leads to a minimal degradation in model performance.\\n* Experiments with the TokFSM dataset are intuitive and clearly demonstrate the ability to intervene in the output of a transformer trained with codebook quantization. Experiments such as the JS divergence with the target token distribution in Figure 4, are used to demonstrate effective intervention. Given that one has access to the semanticity of entries in the learned codebook, the proposed method seems to be effective at steering the output of transformer network.\\n* The work contains a comprehensive related works section in the appendix that clarifies the benefits of a discrete codebook over using a dictionary approach (referred to as \u201cfeatures-as-directions\u201d)\", \"Weaknesses\": \"* One shortcoming of the experiment in table 1 is that it has two dependent variables: both the quantization level k and the codebook size C. It is important to ablate changes to these two variables separately to better understand if they both independently provide benefits. There is a similar issue of two dependent variables being tested at the same time in Table 2 where both k is modified as well the features being quantized (i.e., attention vs mlp features). \\n* The benefit of sparsity in the combination of codebook entries has not been clearly articulated. In fact, in section F.1.1 of the manuscript, it is argued that the continuous combination of different atoms reduces interpretability. Selecting codebook entries via top-k cosine similarity is a naive approach that has not been compared to more recent sparse coding techniques (e.g., variational sparse coding methods like in Tonolini et al. 2020 and Fallah et al. 2022). Except for the potential increase in modeling capacity, the work does not demonstrate the benefit of quantizing with multiple codebook entries per token.\\n* Interpretation of codebook entries still seems to require manual intervention. It requires a user to find input data for which a codebook entry is often activated, which can be timely and costly. It is unclear how one would use current methods to find a codebook entry that corresponds with a certain semantic concept without manually performing a forward pass using data corresponding to that concept.\\n\\n\\nMinor:\\n* Some citations need revising in the bibliography (e.g., \u201cOn the role of scientific thought\u201d).\", \"Questions\": \"* Can the authors clarify the benefit from increasing k? Since the authors use cosine similarity between the activations and the codebook to pick the top-k entries, what would the difference be between each of these k codebook entries? It seems that taking k entries contradicts the viewpoint of \u201cfeatures-as-points\u201d, and may even lead to what the authors refer to as \u201csmuggling of information\u201d. I would expect that increasing k may improve performance of the model, at potential cost to interpretability.\\n* Out of curiosity, have the authors considered quantizing each vector along the feature components (i.e., divide the N features of each token into k blocks)? Is this what the authors refer to as \u201cgrouped codebooks\u201d in E3? If so, I believe this warrants discussion and attention in the main text. This would be an alternative to quantizing each token with k codebook entries that are very close in cosine similarity and be closer to the VQ-VAE setting.\\n* The notation and presentation of the different loss terms in section 2.1 can be made more clear. In the cross-entropy loss, x is used to denote a categorical random variable corresponding to a token being selected. In the reconstruction loss, to my understanding, x is referring to a continuous random variable corresponding to the activation in an intermediary layer, even though the variable a is used in an earlier section. Furthermore, k-codes are combined to quantize each activation. Is the MSE taken with the sum of these codes or each code individually?\"}"}]}
{"paper_id": "ruQkcBfzpm", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Modular vision-language models (Vision-LLMs) align pretrained image encoders with large language models (LLMs), offering a more computationally efficient alternative to end-to-end training of large vision-language models from scratch. Such end-to-end training is costly and usually focused on English due to the abundance of available high-quality English image-text data.\nGap: The field has chiefly focused on English-only Vision-LLMs, while multilingual models are primarily obtained via expensive end-to-end pretraining. This results in smaller model sizes and limitations due to the scarcity of multilingual image data combined with text-only multilingual corpora.\nHypothesis: By leveraging a pretrained multilingual LLM and machine-translating high-quality English vision-and-language data, it is possible to efficiently create a multilingual Vision-LLM using significantly fewer resources compared to traditional methods.\nMethod: The authors developed mBLIP by realigning an image encoder, initially designed for English, to a multilingual LLM. They leveraged a mix of vision-and-language tasks, using machine-translated high-quality English data to 95 languages. The training process utilized parameter-efficient methods and ran on consumer-grade hardware, handling only a few million training examples and adjusting only 124 million parameters.\nConclusion: mBLIP demonstrated competitive performance with state-of-the-art models on the IGLUE benchmark and outperformed certain larger models like PaLI-X in image captioning tasks, despite being trained on magnitudes less data and fewer parameters. This supports that aligning a multilingual LLM with a well-aligned image encoder can yield efficient and effective multilingual Vision-LLMs.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"The approach is interesting in the sense that it covers a reasonable use case, that of multi-lingual LVLM by aligning frozen models. It seems like the authors found a good space where a solution did not exist in the literature.\\n\\nThe proposed approach seems technically reasonable. \\n\\nThe paper is mostly clear, the target application is clear and the explanations are usually clear (although some issues are flagged later on). \\n\\nResults are positive, although there isn't a lot to compare against.\", \"Weaknesses\": \"The paper does not seem to have a strong technical contribution. On the pro side, I like the application, and there's some analysis of the impact of the different pieces in Table 3, which offsets somewhat the lack of technical contribution.\\n\\nI am a bit unsure about the impact of LoRA. A task prompt has much lower capacity to adapt to any downstream task compared to adding LoRA (especially if LoRA adapters are added to every 1x1 layer). So I'm wondering if part of the performance is due to LoRA vs no LoRA.  \\n\\nAlso, if there is enough data to train LoRA adapters, why then there is no data to train a task prompt? \\n\\nis \\\"re-aligning\\\" actually fine-tuning?\", \"Questions\": \"Any answers to the \\\"weakness\\\" above. In particular: did I miss something with regards to novelty? can the use of LoRA be better justified?\"}"}]}
{"paper_id": "yIv4SLzO3u", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Class Incremental Learning (CIL) is a continuous learning framework aimed at handling tasks sequentially with disjoint class sets, while facing challenges such as catastrophic forgetting, where models forget previously learned information upon learning new tasks. Traditional methods such as knowledge distillation, architecture expansion, and parameter regularization have been employed to address these issues but come with inherent limitations.\nGap: Existing methods for CIL struggle with preserving past knowledge without relying heavily on previous task data or adding significant computational overhead. This hinders their applicability, especially in scenarios where memory storage for previous data is limited or not feasible.\nHypothesis: The authors propose a novel approach leveraging weight ensemble and constrained optimization called Bound-and-Average (BaA), which integrates inter-task and intra-task weight averaging alongside bounded model updates to effectively reduce catastrophic forgetting in CIL without altering network architectures or learning objectives.\nMethod: BaA incorporates two types of weight averaging: inter-task averaging, which maintains knowledge from all previous tasks by averaging model weights, and intra-task averaging, which improves generalization on the current task by averaging weights across multiple checkpoints during training. Additionally, the bounded update technique restricts model updates to remain close to a base model from previous tasks, balancing stability and adaptability. The method is evaluated on standard CIL benchmarks like CIFAR-100 and ImageNet-100/1000, showing improvements over existing methods with minimal extra computational cost.\nConclusion: The BaA approach demonstrates superior performance in retaining past knowledge and adapting to new tasks compared to state-of-the-art methods, while maintaining low overhead. It offers a simple plug-in solution to enhance existing CIL frameworks and is shown to be particularly effective under conditions of limited memory budgets, proving robust against catastrophic forgetting across various tasks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 5, \"2022\": 364, \"Strengths\": \"The proposed method BaA is simple but effective, and the method can be integrated into existing methods to improve the performance without modifying their algorithm or architecture.\", \"Weaknesses\": \"(1) The contribution of this paper appears to be insufficient to fully support the article. Similar EMA methods are widely used in self-supervised learning and some incremental learning methods [1,2].\\n\\n[1]Yu C, Shi Y, Liu Z, et al. Lifelong person re-identification via knowledge refreshing and consolidation[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2023, 37(3): 3295-3303.\\n\\n[2]Liang M, Zhou J, Wei W, et al. Balancing between forgetting and acquisition in incremental subpopulation learning[C]//European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022: 364-380.\\n\\n(2) Lack of analysis experiments or visualization about how the proposed weight average mitigates the forgetting issue.\\n\\n(3) Tables 1 and 2 show that BaA can improve the performance of certain CIL methods. Experiments on the latest CIL methods in 2023 should be done to show that the BaA can also be integrated into the SOTA methods.\", \"Questions\": \"In Table 3, the results of the experiment are somewhat perplexing as the average new accuracy appears to be lower than the overall accuracy. In my assessment, the new accuracy represents the precision of classes of the current task, and the overall accuracy is expected to be lower than the average new accuracy due to knowledge forgetting.\"}"}]}
{"paper_id": "oKn2eMAdfc", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Capsule Networks (CapsNet) use capsule vectors to represent entities in images, with redundancy being a persistent issue leading to high computational costs and parameter counts. Traditional dynamic routing involves iterative processing and full connection between capsules, which contributes to redundancy and inefficiencies.\nGap: Despite pruning existing in previous works for CapsNets, redundancy and inefficiencies persist in deeper layers due to the reintroduction by dynamic routing. The iterative and connected nature of dynamic routing maintains high similarity among capsules leading to feature overlap.\nHypothesis: Introducing orthogonality and sparse attention routing to CapsNets can reduce redundancy, improve routing efficiency, and decrease the parameter count, overcoming the limitations of dynamic routing and traditional pruning methods.\nMethod: The OrthCaps model introduces a pruned capsule layer to reduce redundancy using L2 norms and cosine similarity. It replaces dynamic routing with orthogonal sparse attention routing, using a Householder orthogonalization approach to maintain capsule direction and diversity. Attention routing employs \u03b1-Entmax for sparsity, and a Capsule ReLU activation function is proposed to address the vanishing gradient issue prevalent in deep networks. Experiments were conducted on MNIST, SVHN, CIFAR10, CIFAR100, and FashionMNIST datasets.\nConclusion: OrthCaps significantly reduces parameters and computational costs while improving classification accuracy, outperforming state-of-the-art benchmarks on various datasets. The introduction of orthogonality and pruning strategies addresses the redundancy and inefficiencies of dynamic routing, and the Capsule ReLU activation function mitigates gradient vanishing issues. OrthCaps provides a pathway for more efficient and robust capsule networks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 1, \"Confidence\": 2, \"Strengths\": \"1. The idea of utilizing orthogonality is interesting.\\n2. Usage of Householder parameterization is natural and easy to understand.\", \"Weaknesses\": \"1. This paper is very hard to follow. I got completely lost even reading the Introduction. I would suggest the authors completely rewrite the paper to make it readable.\\n1.1 It is mentioned that attention routing is used. But how does it eliminate the need for iteration? \\n1.2 What is the fully-connected problem and how is that solved by sparsemax-based self-attention?\\n1.3 Why is a simplified attention-routing mechanism used with OrthCaps-S and what is it?\\n1.4 What are capsule vector direction shifts and how are they solved with orthogonality? Why does preserving the direction of capsule vectors mitigate feature interference?\\n1.5 What is the motivation of  Capsule ReLU? Why is that used with OrthCaps-D but not OrthCaps-S?\\n\\n2. Why is capsule ReLU designed so? What is the motivation for using BN on a scalar (||s_j||^2)? It is very unnatural, so I would expect a clear motivation.\\n\\nI rate a strong reject mostly because the writing quality of this paper makes it difficult for me to accurately understand the essence of the proposed method, thus preventing me from evaluating its technical correctness and significance.\\n\\nOf course, as I am not an expert in capsule networks, it may be due to my lack of relevant knowledge that I cannot understand this paper. If the writing of this paper is not the issue, that is, if AC and other reviewers can understand this paper without difficulty, please let me know, and I am willing to modify my rating.\", \"Questions\": \"See the weakness part.\"}"}]}
{"paper_id": "9IUZya8bCN", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Multi-label classification is critical in applications such as disease detection and image recognition, where inputs are associated with multiple ground-truth labels. Similar to multi-class classification, multi-label classifiers are vulnerable to data-poisoning attacks, where attackers can alter the training data to influence predictions. Existing defenses against such attacks have focused on multi-class classification, providing sub-optimal results when applied to multi-label classification.\nGap: Existing provable defenses against data-poisoning attacks are designed for multi-class classification and do not effectively extend to multi-label classification due to the complexity of multiple ground-truth labels per input. These methods assume a single-label per input, which leads to sub-optimal robustness guarantees when applied to multi-label scenarios.\nHypothesis: The paper hypothesizes that by jointly considering multiple labels when deriving provable robustness guarantees, it's possible to create an effective provable defense against data-poisoning attacks in multi-label classification.\nMethod: PoisoningGuard is introduced as a provable defense mechanism specifically for multi-label classification. The approach generalizes the concepts of bagging and Deep Partition Aggregation (DPA) to multi-label contexts. A key technical advancement is the ability to certify the number of labels predicted correctly (certified intersection size) despite data perturbations. This includes leveraging the law of contraposition and an extended version of the Neyman Pearson Lemma to address challenges in considering multiple labels simultaneously. The method is evaluated on three datasets where its robustness is compared against existing methods like bagging and DPA, showing superior performance.\nConclusion: PoisoningGuard significantly outperforms existing methods based on bagging and DPA when applied to multi-label classification, providing more effective robustness guarantees by jointly considering multiple labels. This demonstrates that it is possible to defend against data-poisoning attacks in multi-label settings using their generalized methods.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 5, \"Strengths\": \"Overall the quality of this paper is good.\\n1. Developing provable defenses against data poisoning attacks is an important topic.\\n2. The multi-label classification angle is interesting.\\n3. While I did not check the proofs line by line, I am familiar with these provable defenses that I think these results are reasonable.\", \"Weaknesses\": \"1.My major concern is in fact **a very specific mismatch** between the proposed defense and the setting of multi-label classifications.\\nSpecifically, the proposed defense **always predict k labels for a pre-defined constant k**, which is unnatural in multi-label classification. In typical multi-label classification, one does not really assume every sample must have exactly the same number of labels. For this reason, I am concerned **whether this is a reasonably good start for provable defenses in multi-label classification**. \\n\\nIn my opinion, the current formulation is more related to certifying top-k accuracy rather than multi-label classification. However, it is relatively trivial (I will still consider this as part of this work's contribution though) to generalize bagging/DPA for b to top-k accuracy since it is somewhat straightforward (at least for DPA) to allow each base classifiers to contribute multiple votes.\\n\\n2.There are some closely related but missing references [1-6 below]. [1-5] are closely related to provable defenses against poisoning attacks and [6] is about provable defenses for top-k predictions but for test-time attacks. Notably, the certificates of [4] essentially offer a solution for certifying top-2 predictions. Note that some of these references are in fact fairly new, thus while I suggest authors to discuss them, it is understandable that they are missing from the submitted version.\\n\\n\\n[1] Chen R, Li Z, Li J, Yan J, Wu C. On Collective Robustness of Bagging Against Data Poisoning. \\n\\n[2] Wang W, Levine A, Feizi S. Lethal Dose Conjecture on Data Poisoning. \\n\\n[3] Wang W, Feizi S. Temporal Robustness against Data Poisoning.\\n\\n[4] Rezaei K, Banihashem K, Chegini A, Feizi S. Run-Off Election: Improved Provable Defense against Data Poisoning Attacks. \\n\\n[5] Wang, W. and Feizi, S. On Practical Aspects of Aggregation Defenses against Data Poisoning Attacks. \\n\\n[6] Jia, J., Wang, B., Cao, X., Liu, H. and Gong, N.Z. Almost tight l0-norm certified robustness of top-k predictions against adversarial perturbations.\", \"Questions\": \"Please refer to Weakness. \\n\\n1. The major concern is about Weakness 1.\\nPlease clarify if you think there is anything missing.\\nIf you also find this to be reasonable, a direct fix from me will be to use either a fixed threshold or an adaptive one to decide whether each label is predicted or not. For example, instead of predicting a constant of k labels per sample, the ensemble can predict all classes with more than t votes for some constant t.\\nThis can actually simplify the computation of certificates and potentially make it much more suitable for the setting of multi-label classifications. However, this leads to critical changes to the proposed approach so revising&resubmiting to future venues may also be a good option.\"}"}]}
{"paper_id": "pTU2X9mUBe", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Last-mile delivery is a critical area in logistics and supply chain management due to urbanization and the rise of e-commerce. It constitutes the most expensive and time-consuming part of shipping. Researchers have focused on problems like route planning and ETA predictions. However, the absence of a publicly available, comprehensive dataset hampers research progress in this domain.\nGap: There is a lack of a widely accepted, publicly available last-mile delivery dataset. Existing efforts, such as Amazon's dataset, are limited in scale, scope, and comprehensiveness, lacking data from package pick-up processes and detailed courier information.\nHypothesis: The authors propose that LaDe, a large-scale, comprehensive, and diverse dataset from real-world last-mile delivery operations, can significantly advance research in logistics by providing critical data for developing and benchmarking new algorithms.\nMethod: The authors collected a dataset named LaDe from a large logistics platform, containing 10,677k packages processed by 21k couriers over six months across various Chinese cities. The dataset captures both pick-up and delivery scenarios. They analyzed its properties and verified its utility by benchmarking it against route prediction, ETA prediction, and spatio-temporal graph forecasting tasks using several baseline models.\nConclusion: LaDe is positioned as a critical resource for research in logistics, offering comprehensive data that can enhance the development of algorithms in last-mile delivery. Its scale and diversity make it suitable for a range of tasks beyond those benchmarked, potentially furthering progress in the field.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"S1. The provision of an open-access, real-world dataset in this article has a favorable and constructive influence on research within the domain of Pickup & Delivery.\\n\\nS2. Following the release of the dataset associated with this article, it can be employed to corroborate and verify findings from research papers that had not previously made their datasets openly available.\\n\\nS3. The dataset introduced in this article incorporates measures to protect crucial user privacy information.\", \"Weaknesses\": \"W1. The data fields of the dataset proposed in the article appear relatively straightforward and, in essence, do not significantly distinguish themselves from the data fields found in other existing Pick-up & Delivery datasets. \\n\\nW2. The dataset outlined in this research paper reveals certain limitations with respect to its incorporation of road network structure, a pivotal aspect for applications pertaining to pick-up and delivery services. Predominantly relying on GPS locations(x-y coordinates), while undoubtedly valuable, might not comprehensively address the intricacies inherent in real-world logistics operations. The absence of road network information in the dataset does raise valid concerns about its suitability for applications necessitating an in-depth understanding of spatial constraints and the optimization of routing, particularly in the last-mile express domain. To augment the dataset's applicability in such scenarios, it may prove advantageous to contemplate the inclusion of coarse-grained road network data. While acknowledging the privacy concerns associated with road network information, providing a more generalized form of road network data could still facilitate researchers in achieving a more comprehensive and accurate representation of the real-world scenarios under examination.\\n\\nW3. The dataset should acknowledge the significant impact of rush hours and traffic congestion on logistics. It is important to recognize that the road network structure undergoes dynamic and temporal changes over the course of several months. Therefore, it becomes paramount to incorporate a dynamic road network structure within the dataset that can adapt to and depict the wide-ranging traffic conditions and road configurations at different times. This dynamic approach not only bolsters the dataset's precision and relevance but also furnishes researchers with a more authentic representation of the continually evolving real-world logistics and last-mile delivery scenarios.\\n\\nW4. It is worth noting that several datasets, including prominent examples like the Didi Chuxing GAIA dataset and the New York City dataset, have been extensively employed in addressing pick-up and delivery problems. These datasets have formed the foundation of numerous benchmarks and research benchmarks, rendering them well-established within the field. Consequently, it is essential to delineate the distinctive advantages that the dataset introduced in this paper brings to the table in comparison to these existing datasets and benchmarks, particularly in the context of last-mile delivery research. Clarifying the unique attributes and contributions of this new dataset is crucial in highlighting its relevance and significance within the academic and research community. To accentuate the contributions of the article, it is essential to delve deeper into a comprehensive comparison that delineates the differences between the dataset tailored for Last Mile Delivery scenarios and datasets associated with broader Pick-up & Delivery contexts, including well-known examples like the DIDI and NYC taxi datasets. \\n\\nW5. Throughout the article, the authors seek to benchmark several aspects of applications, each of which boasts a significant body of existing works. Yet, the chosen methods in the article appear to fall short of encompassing the full spectrum of relevant works and categories within these benchmarked areas. In light of this, it is imperative that the authors offer more extensive elucidation regarding their decision to select these specific methods as baselines when there are numerous alternatives at their disposal. Providing a comprehensive explanation will not only enhance the transparency and academic rigor of their research but also enable the reader to grasp the underlying reasoning behind these choices.\", \"Questions\": \"Beyond the aforementioned weak points, the additional question is as follows.\\n\\nQ1. Does the proposed benchmark potentially unveil latent insights or yield noteworthy contributions to the field? Moreover, what benefits can be discerned from conducting experiments with this dataset in contrast to other pre-existing datasets?\"}"}]}
{"paper_id": "FNq3nIvP4F", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Recent advances in video generation have shown promising results in producing realistic content. However, these AI-generated videos are typically short, known as 'shot-level', and depict only one scene. For more extensive applications like storytelling, there is a need for generating coherent, long videos, or 'story-level' videos, which require creative transitions and predictions across different clips.\nGap: Current video generation methods focus on producing short videos, lacking the ability to generate long, coherent videos with smooth scene transitions. Techniques for scene transitions in traditional film rely on predefined methods which lack flexibility and creativity.\nHypothesis: The authors propose SEINE, a short-to-long video diffusion model that aims to generate high-quality, long videos. The model focuses on creative transition and prediction, ensuring coherence and satisfying the requirements of artistic storytelling across multiple scenes.\nMethod: The proposed SEINE model utilizes a random-mask video diffusion approach to generate transitions between scenes based on textual descriptions. By inputting the initial and final frames of scenes, along with text prompts, the model creates smooth transitions. SEINE is built on a pre-trained diffusion-based text-to-video model and includes a random-mask module to handle transitions and predictions. The model is trained using a combination of the WebVid10M dataset and other watermark-free datasets to enhance output quality. Performance is evaluated on metrics including temporal consistency, semantic similarity, and video-text semantic alignment through experiments.\nConclusion: SEINE demonstrates the ability to generate high-quality, long videos with creative transitions that maintain coherence and visual quality. The approach outperforms existing methods on various metrics, showing potential applications in long video generation, image-to-video animation, and autoregressive video prediction.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"This article addresses the limitation of existing models that can only generate shot-level videos and proposes a method to generate story-level videos using transitions. They extend an existing video generation framework and achieve impressive results in generating long videos. They also propose a reasonable evaluation framework to assess the proposed model, and a large number of demos and quantitative evaluations demonstrate the effectiveness of their approach. The contribution of this work is significant.\", \"Weaknesses\": \"The author should provide a more detailed description of the model for reproducibility, including training resources, training parameters, and so on. Additionally, the author should also report scores on commonly used evaluation metrics such as FID.\", \"Questions\": \"I wondered how many GPUs they used and how long it takes for training. Besides, as far as I know, FID is used to evaluate video generation quality in many papers, can they provide this to make their paper more solid?\"}"}]}
