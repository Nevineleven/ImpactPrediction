{"paper_id": "B0OwtVEejJ", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Neural Architecture Search (NAS) methods based on weight-sharing enable the exploration of cell-based architecture spaces efficiently via supernets, reducing computational complexity. Gradient-based NAS methods like DARTS and DrNAS optimize architecture weights through gradient descent, traditionally focusing on cell-based spaces. Alternatively, weight-entanglement for macro-level search spaces allows for parameter sharing between similar operations, reducing memory usage by only requiring storage for the largest architecture in the search space. Typically, weight-entanglement methods involve a two-stage process: training a supernet followed by blackbox search using performance proxies from subnetworks.\nGap: The main gap identified is the lack of integration between weight-entanglement and gradient-based NAS methods. These two paradigms have primarily developed in isolation within separate communities due to compatibility issues. Existing methods have not effectively combined the benefits of gradient-based optimization with weight-entangled architecture spaces, which limits the potential for optimizing more complex and larger search spaces using gradient-based techniques.\nHypothesis: The authors propose that adapting gradient-based NAS methods to operate effectively within weight-entangled spaces can yield the advantages inherent to both paradigms: the efficient exploration and performance of gradient-based methods and the reduced memory requirements of weight-entangled approaches.\nMethod: The authors introduce TangleNAS, a generalized approach that extends single-stage NAS methods to weight-entangled spaces. This is achieved by superimposing operation weights, allowing for weight-sharing across different operation choices by zero-padding them to the size of the largest operation and summing them with architectural parameters. The approach integrates gradient-based NAS optimizers like DrNAS in weight-entangled spaces without additional memory overhead. The method is validated through comprehensive experiments across various search spaces, including cell-based, convolutional macro-level, and transformer architectures, using benchmarks like NASBench201, DARTS, and AutoFormer.\nConclusion: TangleNAS successfully bridges gradient-based and weight-entangled NAS methods, offering improved any-time performance, lower memory usage, and high accuracy in both cell-based and macro-level search spaces. It demonstrates that combining the advantages of both paradigms can lead to more efficient and effective neural architecture search. Empirical results show that TangleNAS outperforms conventional two-stage methods on several benchmark tasks, offering a compelling approach for NAS in complex architecture spaces.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"The paper proposes a scheme to adapt gradient-based methods for weight-entangled spaces in neural architecture search (NAS). This integration of weight-entanglement and gradient-based NAS is a new approach that has not been explored before. The paper also presents a comprehensive evaluation of the properties of single and two-stage approaches, including any-time performance, memory consumption, robustness to training fraction, and the effect of fine-tuning.\", \"Weaknesses\": \"1. The novelty is limited. Some works have also suggested that the weights of large kernel convolution operations can be shared in differentiable neural architecture search. For example, MergeNAS: Merge Operations into One for Differentiable Architecture Search (in IJCAI20)\\n2. The performance improvements are limited compared with the baselines.\", \"Questions\": \"1. How to entangle non-parameter operations, such as skip or pooling, as they have no weight compared to convolution.\\n2. The search cost of original DrNAS is 0.4 GPU-Days in DARTS search space. Whereas, in Table 4, the search time is 29.4 GPU-Hours. Can the authors explain the reason for the different search cost?\\n3. It would be beneficial to provide a more detailed explanation of the rationale behind the selection of specific search types, optimizers, and supernet types for the comparative evaluation. \\n4. To discuss potential limitations and challenges in implementing the proposed scheme for adapting gradient-based methods for weight-entangled spaces would provide valuable insights. This could include addressing potential constraints, trade-offs, and practical considerations in real-world implementation.\"}"}]}
{"paper_id": "ZlEtXIxl3q", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Fitness functions map biological sequences to desirable properties, playing a crucial role in protein engineering by predicting sequence performance based on limited data. Global epistasis models have been effective in learning these functions, assuming a simple latent function transformed by nonlinearity. Existing methods often struggle with these transformations.\nGap: Existing methods like Mean Squared Error (MSE) loss are inefficient in learning fitness functions influenced by global epistasis because the nonlinearity they introduce does not admit sparse representations, making them hard to learn from limited data.\nHypothesis: Contrastive loss functions, such as the Bradley-Terry loss, can more effectively learn the sparse latent functions implied by global epistasis models by focusing on ranking rather than numerical accuracy, which bypasses the issues caused by nonlinearity.\nMethod: The paper applies contrastive losses to recover sparse latent fitness functions from data subjected to global epistasis. Simulations were run using fitness functions sampled from NK models and transformed by various nonlinearities. These models were trained with both MSE and contrastive losses, and their performance compared across simulated and real-world datasets, validating their approach on FLIP benchmark tasks.\nConclusion: Contrastive losses consistently enable better recovery and ranking of fitness functions from globally epistatic data than MSE, as demonstrated by improved performance on benchmark tasks. This method proves more data-efficient and robust, even under incomplete or noisy conditions, making it a practical alternative for protein fitness predictions.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 2, \"Strengths\": \"1. Contrastive learning has demonstrated promise in the field of computer vision; this paper shows a novel application of this concept to the field of Biology.\\n\\n2. The paper has a strong theoretical foundation and is well presented. \\n\\n3. Quantitative results are convincing and promising.\", \"Weaknesses\": \"1. The empirical evaluation has focused only on a single benchmark, FLIP. It would strengthen the paper if the approach was validated with even more datasets.\", \"Questions\": \"1. Is there any way to extend the empirical evaluation beyond the FLIP benchmark?\"}"}]}
{"paper_id": "6yJuDK1DsK", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Test-time adaptation (TTA) refers to the challenge of adapting a pre-trained model to handle test data distributions that differ from the training (source) distribution, often due to non-stationary environments. Lifelong TTA is a more complex setting where the test distribution continues to change over time, requiring improvements that manage adaptation without forgetting the source model's knowledge and controlling computational cost.\nGap: Existing lifelong TTA methods require adaptation of all model parameters, which is computationally expensive and memory-intensive. This limits their usability on edge devices for rapid inference. Additionally, these methods may suffer from significant forgetting over time and often require access to the source data for warm-starting adaptation, which may not always be feasible.\nHypothesis: FEATHER proposes a solution that introduces a lightweight adapter to the pre-trained model, requiring fewer additional parameters to perform effective test-time adaptation. This method aims to keep the source model frozen, thus separating source and target domain knowledge to prevent forgetting and error accumulation. Moreover, FEATHER eliminates the need for source data access during adaptation.\nMethod: FEATHER inserts a minimal number of adapters—configurations of group-wise and point-wise convolutions—into a pre-trained network. These adapters are initialized to act as identity functions, ensuring unchanged initial behavior in the source domain representation. During test-time, only adapter parameters are updated using unsupervised learning methods on the test data, while the rest of the model remains constant. The study validates the effectiveness of FEATHER through experiments on CIFAR-10C, CIFAR-100C, ImageNetC, and ImageNet3DCC datasets, demonstrating substantial parameter efficiency.\nConclusion: FEATHER achieves comparable or better performance than state-of-the-art lifelong TTA approaches with 85-94% fewer parameters needing adaptation, offering significant computational efficiency. Unlike some methods, it does not require access to source data during adaptation, remaining robust in dynamic environments on multiple challenging benchmarks. The method's lightweight design is crucial for real-world applications with practical hardware constraints.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. This paper is overall clearly clarified and well organized.\\n2. The use of adapters for lifelong test-time adaptation seems novel.\", \"Weaknesses\": \"1. The proposed method is only applicable to CNNs.\\n2. The authors argue for the importance of preserving the information of the source model in online TTA tasks. However, the proposed method does not exhibit outstanding performance compared to the CoTTA, which involves full fine-tuning of the model. Therefore, the authors fail to sufficiently explain why the use of adapters is suitable for online TTA tasks, apart from the fact that it reduces the number of trainable parameters.\\n3. The authors claim that the proposed method excels in terms of parameter update costs; however, the cost of training the adapters inserted between the model's layers, in terms of memory and computation, is not significantly lighter compared to full fine-tuning.\", \"Questions\": \"1. Is it possible to apply the proposed method to Transformers?\\n2. Why does CoTTA generally outperform the proposed method?\\n3. Could you compare the proposed method with other methods in terms of memory and computation?\"}"}]}
{"paper_id": "lt6xKGGWov", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Feature selection methods are crucial for predictive modeling, either as wrappers that use the learning model as a subroutine or as filters that select features based on a dependency score between features and the target without needing to know the learning procedure. Recently, the Mutual Information Neural Estimator (MINE) has been introduced as a modern method to estimate mutual information.\nGap: Current feature selection methods often examine features one at a time, which limits their ability to capture complex dependencies between features and targets.\nHypothesis: Using neural estimation of mutual information to evaluate subsets of features as an ensemble can capture sophisticated relationships between features and targets, enabling more accurate feature selection.\nMethod: The paper introduces MINERVA, a feature selection filter that uses MINE to estimate mutual information for ensembles of features. It employs a neural network to approximate functions for the Donsker-Varadhan representation of Kullback-Leibler divergence. The feature selection process involves optimizing a loss function with regularization terms to ensure sparsity and control the magnitude of weights. MINERVA is implemented in the minerva package and is tested on synthetic datasets to demonstrate its effectiveness in capturing complex dependencies and performing exact feature selection.\nConclusion: MINERVA successfully captures sophisticated non-linear dependencies between features and targets, performing exact selection where existing methods either fail or perform non-exact selections. The proposed method shows improved accuracy in synthetic benchmarks, underscoring its capability to handle complex feature-target relationships.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"The authors introduce a method that selects features based on neural estimation of mutual information.\\n\\nThe method outperforms existing techniques in two synthetic datasets.\", \"Weaknesses\": \"The experiments were limited to two synthetic datasets, casting doubt on the method's applicability to real-world datasets. Multiple factors can influence model performance. These include sample size, hyperparameters $c_1$ and $c_2$, the neural network architecture, and the noise that cannot be explained by the input $\\\\mathbf{X}$. Without a comprehensive analysis of these factors, the general efficacy of the proposed method remains uncertain.\\n\\nEquation (1) provides an estimate for mutual information. Yet, once the constraints are introduced in Equation (5), it's unclear if the function still acts as a mutual information estimator. This requires more in-depth theoretical exploration.\\n\\nThe definition of $\\\\phi$ in Line 11 of Algorithm 1 is unclear.\", \"Questions\": \"1. Does the proposed method work in real-world datasets?\\n\\n2. Is Equation(5) an estimator of mutual information?\\n\\n3. How if $\\\\phi$ in Line 11 of Algorithm 1 defined?\"}"}]}
{"paper_id": "Bvrc6kobWd", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Self-supervised learning (SSL), particularly contrastive learning, is a widely used framework that learns instance-level relationships using a similarity function, often cosine similarity, and cross-entropy. In face and speaker recognition, margins have been effectively utilized to enhance recognition performance by increasing inter-class distance and decreasing intra-class distance. While some attempts have been made to incorporate margins into contrastive learning, their effects have been primarily explained through decision-boundary analyses from face recognition tasks, without delving deeply into the gradient-level understanding.\nGap: The role of margins in contrastive learning, particularly through their effect on gradients, has not been well-explained or explored. Existing explanations are largely borrowed from face recognition and rely on decision-boundary-based approaches, which are not applicable to the unlabeled nature of contrastive learning tasks.\nHypothesis: The authors propose that analyzing the effect of margins on the gradients of contrastive learning can provide a new understanding that is not reliant on decision-boundaries and can reveal elemental effects that contribute to representation learning. This gradient perspective might offer directions for enhancing contrastive learning performance.\nMethod: The authors conducted a gradient analysis of the generalized contrastive learning loss, incorporating margins, to identify how these margins affect gradients. They specifically analyzed the gradient levels affected by margins, such as emphasizing positive samples, scaling gradients by angular margins, and scaling based on the ratio of summed logits. Experimental results using datasets such as CIFAR-10, CIFAR-100, STL-10, and TinyImageNet, evaluated the impact of these factors on generalization performance.\nConclusion: The study concludes that emphasizing positive samples and scaling gradients based on positive sample angles and logits are crucial for improving the generalization of contrastive learning across both seen and unseen datasets. Other factors provide marginal improvements. The gradient-based approach to understanding margins could lead to better exploitation of margins and improvements in contrastive learning frameworks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 1, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"* The approach is novel\\n* The authors ran several experiments with good ablation.\", \"Weaknesses\": \"* Shallow theoretical depth\\n* Hard to get some insight on what Fig.1,2 and the theoretical results in Sec.4 imply \\n* The authors claim that 4 effects of introducing the margin term are empirically are covered in Sec.5. First, it was hard for me to get those 4 messages from both theory (in Sec.4) and in experiments (in Sec.5). Let us say 4 effects are observed. Does it explain why introducing the margin is beneficial for contrastive learning? \\n* Probably many of these issues are from the quality of writing.\", \"Questions\": \"* In equation (3), it is unclear how the \\\\beta term in the denominator (inside the log) is coming out to the linear form in the righthand side. Probably have typo somewhere? When \\\\beta=0 (i.e., when only the positive samples are considered), the denominator inside the log becomes 0, which is also weird.\"}"}]}
{"paper_id": "FJjHQS2DyE", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper addresses unsupervised domain adaptation (UDA), which is a framework where a learning model is trained using labeled samples on a source domain and unlabeled ones on a target domain. Traditional approaches have focused on reducing feature distribution divergence based on the covariate shift assumption, which often fails under label shift conditions.\nGap: Most existing UDA methods achieve suboptimal performance under label distribution shift because they are based on the covariate shift assumption, which assumes that the feature distribution changes, not the label distribution. Label shift, where the marginal label distribution varies across domains, is a more realistic assumption in various settings, such as healthcare, but is underexplored.\nHypothesis: The authors propose a novel Conditional Adversarial Support Alignment (CASA) framework, which aims to minimize the conditional symmetric support divergence between source and target domains to achieve more discriminative feature representations for classification, particularly addressing label distribution shifts.\nMethod: CASA involves aligning the supports of conditional feature distributions between the source and target domains and proving a new theoretical target risk bound. A complete training process is presented that includes minimizing the source risk, target conditional entropy, and Lipschitz continuity of classifiers, and employing pseudo-labels to approximate support divergence reduction. The method is empirically validated on benchmark datasets like USPS to MNIST, STL to CIFAR, and VisDA-2017, with CASA outperforming state-of-the-art methods in various label shift scenarios.\nConclusion: CASA provides a more effective approach to UDA under label shift by aligning conditional feature supports, resulting in improved classification performance. The paper's theoretical contributions justify the benefits of this alignment over existing marginal support approaches, supporting its empirical success across benchmark tasks. Future work could extend CASA to more complex settings like domain generalization and universal domain adaptation.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"This paper proposed conditional adversarial support alignment (CASA) to minimize the conditional symmetric support divergence between the source’s and target domain’s feature representation distributions. Generally, the paper is well-written and easy to follow. They evaluate the model on several benchmarks from various types of results.\", \"Weaknesses\": \"The model's novelty is incremental over multiple loss functions. The alignment loss in Eq(10) is more like pair-wise alignment loss, which has been explored before for cross-domain graph alignment. It is hard to verify the novelty.\\n\\nFrom the experiments, they show the improvements when \\\\alpha decreases. However, there is no insight why this happens. It is better to discuss the intuition and data used behind. It needs more visualization to demonstrate the improvement.\", \"Questions\": \"The novelty clarification.\\nThe performance analysis.\"}"}]}
{"paper_id": "xelrLobW0n", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large Language Models (LLMs) have significantly advanced natural language processing by combining extensive pretraining with fine-tuning on human-generated data. Despite these advancements, the continual learning capabilities of LLMs remain underexplored, particularly in retaining performance across sequential tasks without catastrophic forgetting.\nGap: Existing continual learning benchmarks are inadequate for assessing state-of-the-art LLMs, mainly because they include simplistic tasks and fail to consider crucial elements like generalization, instruction adherence, and safety of LLMs.\nHypothesis: The paper proposes that TRACE, a novel benchmark with diverse and challenging tasks, combined with the Reasoning-augmented Continual Learning (RCL) approach, can better evaluate and enhance the continual learning capabilities of LLMs while mitigating catastrophic forgetting.\nMethod: TRACE benchmark consists of eight datasets covering domain-specific tasks, multilingual capabilities, code generation, and mathematical reasoning. It also introduces new evaluation metrics: 'General Ability Delta,' 'Instruction Following Delta,' and 'Safety Delta.' The study uses six aligned LLMs in various experimental settings, including sequential fine-tuning techniques (like LoRA and Replay) and evaluates their performance using these metrics. The RCL method augments training by integrating reasoning paths, leveraging GPT-4 for reasoning annotation, followed by supervised training with these enhanced datasets.\nConclusion: Experiments demonstrate that traditional fine-tuning leads to significant declines in LLMs' general abilities and instruction-following capabilities. The RCL approach shows promise in retaining reasoning capabilities with less catastrophic forgetting, suggesting that reasoning-augmented learning strategies could balance the need for task-specific performance and the preservation of LLMs' inherent abilities.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"1. The authors have considered somewhat large language models for the evaluation. It would be very grateful if the authors could compare it on a bigger scale, e.g., llama2 70b-chat, falcon-40b instruct.\", \"Weaknesses\": \"1. While I do agree that some papers, e.g., instruction tuning, have used the past continual learning benchmarks, TRACE also shares the same issue. After publishing this benchmark, there are possibilities that some may use this dataset for pre-training. So, I think continual learning (CL) benchmarks all shares a similar problem, so this is hard to claim as a problem of prior CL benchmark (since this paper also have the same issue). \\n\\n2. For me, it is hard to understand i) why this benchmark has novelty for CL and 2) why this has novelty for alignment LLMs\\n- i) TRACE is a mix of existing challenging benchmarks and put them in a sequential manner. It is hard to see novelties or specialization for continual learning (except they have put it in a sequential manner). \\n- ii) The only thing this benchmark is specialized to alignment LLMs is that they propose a metric that is related to alignment LLMs. But these metrics are also a naive extension of prior continual learning metrics (the overall performance and backward transfer suggested by [1]). \\n\\n3. One important discussion in continual learning is about forward transfer [1,2,3]. Considering forward transfer metric is also needed.\\n\\n4. As a benchmark paper, I think it is very important to compare the prior CL methods, where this paper only shows a few baselines. For instance, considering the following methods will be helpful (see some baselines in [4]) or consider [5,6,7].\\n\\n5. The overall writing should be improved. For instance, **the main text cites the results in the Appendix too much** (Figure 5, Table 6). If the table and figures are important, the authors should put them in the main text. Especially, the results of reasoning-augmented continual learning are important but missing in the main text.\\n\\n6. Using GPT-4 is not a good choice for continual learning (even for reasoning), since we don't know which dataset they have used to train the model.\\n\\nOverall, it is somewhat hard to find a novelty as a continual learning benchmark (and typically specialized for aligned LLMs), and it is hard to find novel metrics. Furthermore, the paper needs more consideration regarding forward transfer, more comparison of multiple frameworks, and improving the overall writing. \\n\\nPlease find the reference below\\\\\\n[1] Lopez-Paz and Ranzato, Gradient episodic memory for continual learning, NeurIPS 2017\\\\\\n[2] Wolczyk et al., Continual World: A Robotic Benchmark For Continual Reinforcement Learning, NeurIPS 2021\\\\\\n[3] Chen et al., Is Forgetting Less a Good Inductive Bias for Forward Transfer? ICLR 2023\\\\\\n[4] Jang et al., Towards Continual Knowledge Learning of Language Models, ICLR 2022\\\\\\n[5] D'Autume et al., Episodic memory in lifelong language learning, NeurIPS 2019\\\\\\n[6] Huang et al., Continual learning for text classification with information disentanglement based regularization, NACCL 2021\\\\\\n[7] Razdaibiedina et al., Progressive prompts: Continual learning for language models, ICLR 2023\", \"Questions\": \"I think the result of Vicuna-13B-V1.5 in Table 2 should be 0.\"}"}]}
{"paper_id": "bYQkOPvgDw", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Graph Neural Networks (GNNs) are widely used in various fields, but the presence of noisy labels (due to errors in manual labeling or incomplete data) can degrade the performance of these networks. While the issue of graph perturbation and attack has been extensively studied, less attention has been given to label noise. Existing methods for dealing with label noise often assume label smoothness, which is not applicable to heterophilous graphs, and perform poorly in high noise-rate scenarios.\nGap: There is insufficient research on robust GNNs that effectively handle label noise, especially in heterophilous graphs and scenarios with high noise rates. Existing methods heavily rely on the label smoothness assumption and are not tailored to the unique challenges presented by graph-structured data.\nHypothesis: A probabilistic graphical model-based framework can be used to develop robust GNNs that are effective against noisy labels, without relying on the label smoothness assumption. Such a framework should perform well on both homophilous and heterophilous graphs, as well as in high noise-rate situations.\nMethod: The authors propose PRGNN, a novel probabilistic graphical model-based framework that includes two versions: PRGNN-v1 and PRGNN-v2. PRGNN-v1 maximizes the likelihood of clean labels based on graphs only, using a Bayesian network. PRGNN-v2 incorporates noisy labels into the Bayesian network, improving the generation of clean labels. The framework employs three independent GNNs to generate noisy label predictions, reconstruct clean labels, and incorporate prior knowledge. A variational inference approach is used, with evidence lower bounds (ELBOs) as objectives, and a contrastive loss is added to leverage supervision from unlabeled data. The approach is evaluated on datasets with varying noise levels and graph types.\nConclusion: The proposed PRGNN framework outperforms existing methods across various noise types and rates, particularly in high noise-rate situations. It demonstrates the ability to work effectively in both homophilous and heterophilous graphs, thanks to its disregard for the label smoothness assumption and its use of probabilistic graphical models to handle label noise.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"1. Overall, the paper presents a novel and promising approach to handling label noise in graph neural networks based on probabilistic graphical model.\\n\\n2. The sequentially introduced models PRGNN-V1 and PRGNN-V2 are clearly depicted and the proposed objectives are well motivated. \\n\\n3. Experiments are conducted on both noisy Homophilous and Heterophilous graphs. Clear improvements are observed compared to the implemented methods.\", \"Weaknesses\": \"1. Several crucial points remain unclear. First, it is still hard to understand why we need the uncertainty modeling. One might directly apply the encoder-decoder models equipped with the corresponding losses to handle the noisy labels. To be specific, what happens if we only use Q and P_\\\\varphi without the prior distribution, and conduct the training without the prior-related losses? The second question is related. From the experiments in Figure 3, the weighted cross-entropy loss wL_CE(P_\\\\varphi, Y_N) is much more important than others. So, it seems it is much more important to balance the loss between reconstruction of clean labels and that of noisy labels, other than the probabilistic modelling particularly given the label space (which is the random variable we focus on) is of small dimension. Third, the authors claim that the proposed probabilistic graphical model is able to avoid the issue of label smoothness in existing methods. However, it seems this is attributed to the application of prototype label vectors, instead of simple concatenation of labels with node embeddings for message passing. This is also observed in Figure 3. Overall, to make the proposed models effective, the authors introduce various techniques. However, it appears that the one which demonstrates consistent success is not aligned with the contributions claimed by the authors. \\n\\n2. About the introduction of the baselines, it is unknown which one applies label smoothness, and which one is proposed for Heterophilous graphs.\\n\\n3. It seems this paper is only applicable for the transductive setting not inductive setting. \\n\\n4. The last question is on the general research domain for learning from noisy graphs. While it is true that many data we process in practice definitely contain different kinds of noise, the proposed methods are always evaluated in the scenarios where we add artificial perturbation to the clean data. My point is we actually need some real tasks and perhaps some fixed benchmarks to evaluate the performance of different methods, such that we can clearly justify how the progress is made and how the challenges are addressed.\", \"Questions\": \"See the weaknesses above.\"}"}]}
{"paper_id": "sKPzAXoylB", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Continual learning poses significant challenges for deep representation learning models, such as catastrophic forgetting and loss of plasticity. Catastrophic forgetting refers to the loss of previously acquired knowledge when new information is learned, and loss of plasticity is the diminishing ability of models to adapt to new information over time. Existing approaches often address these issues separately, mainly focusing on environments where task boundaries are known, and data can be stored and replayed, which is not feasible in streaming learning settings.\nGap: There is a lack of approaches that can simultaneously address catastrophic forgetting and loss of plasticity, particularly in streaming learning environments without relying on known task boundaries, replay buffers, or pretraining. This gap poses a challenge for real-world applications where non-stationary data streams must be processed online.\nHypothesis: The authors propose that by using Utility-based Perturbed Gradient Descent (UPGD), which incorporates perturbations and utility-based gradient updates, it is possible to address both catastrophic forgetting and loss of plasticity simultaneously. The approach optimizes gradient updates by adjusting them based on the importance of neural network units, thus protecting useful units and rejuvenating less useful ones.\nMethod: The proposed method, UPGD, combines gradient updates with utility-based perturbations. It determines the utility of network weights using a Taylor series approximation of the loss function, distinguishing between useful and less useful weights. Smaller updates are made to protect useful weights (which are likely to contribute to catastrophic forgetting if altered), while larger perturbations are introduced to less useful weights to rejuvenate plasticity. The effectiveness of UPGD was assessed using non-stationary streaming problems based on datasets like MNIST, EMNIST, CIFAR-10, and ImageNet. The model was further tested in reinforcement learning tasks using PPO to confirm its applicability to policy collapse issues.\nConclusion: Utility-based Perturbed Gradient Descent effectively mitigates both catastrophic forgetting and loss of plasticity in continual learning environments. The method demonstrates improved performance over traditional approaches that focus on only one of these challenges. By leveraging a utility-based approach, UPGD can dynamically adjust to the importance of learned representations, maintain plasticity, and protect against forgetting in both representation learning and reinforcement learning tasks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"**Clear Structure and Writing:** The paper benefits from a clear structure and concise writing style.\\n\\n**Addressing a Complex Issue:** The authors tackle an underexplored yet challenging problem, and I appreciate their efforts to address online continual learning.\\n\\n**Mathematical Foundation:** The definition of utility introduced in the paper is based on simple and sound mathematical derivations. \\n\\n**New metric:** The introduction of a new plasticity metric is a nice contribution to the relatively uncharted territory of streaming learning.\", \"Weaknesses\": \"**Unscaled perturbations:** My main point of issue is the reasoning behind the perturbations in the update rule. The authors claim that by adding the perturbation we are making the unimportant weights more plastic however I am not really convinced by this explanation I believe it requires elaboration both in the rebuttal and in the paper. \\n\\nAnother related issue with the proposed perturbation is the fact that all of them are getting drawn from the same standard normal distribution. This design choice is strange to me since the parameters of a neural network usually differ in magnitude from layer to layer. By adding an unscaled random perturbation to all of the weights we are ignoring this scale difference which I believe is sub-optimal. I know that in the unprotected version, they are getting weighted by different values but this particular scaling is more correlating with changes in the loss value rather than the parameter magnitudes.\\n\\nHighly relevant to the above issue, I believe it is also necessary to have an additional ablation study, investigating the role of having and not having the perturbations in the update rule. I also want to disentangle the effect of weight decay. The only time that UG is added in the ablation is in the presence of WD. More specifically I am curious about the following scenarios in Figure 8: \\n\\n* Added ablations:\\n    + SGD + UG + WP + WD (present in the paper)\\n    + SGD + UG + WP \\n    + SGD + UG + WD\\n    + SGD + UG\\n\\n    \\n**Including more diverse experiments:** Moreover, in the experiments section I believe the authors need to include more diverse experiments. All of the streaming tasks are permutations of the same task. Whether in the label or in the input space. It is not as obvious as the authors' claim that after the permutation of the input space the previously learned representations are not relevant anymore (end of page 6). In the input-permuted scenario, only the first layer needs to have significant change. This is especially true for the label-permuted tasks as the network does a good job of clustering the data up to the final FC layer. I encourage the authors to use the Cifar100 superclass dataset (or any similar sequence of tasks that does not simply rely on the permutation).\\n\\n**Visualization:** Finally, I believe the visualization needs several improvements: the legends on the plot are very hard to read (Fig 2, 3, 4, 5). Some colors are similar to each other and the width of the lines in the legends is too thin. (Especially in figure 4). In Figure 7, some numbers in dark blue cells are almost impossible to read.\", \"Questions\": \"**Q1:** Have the authors tried to use an scaled version of the perturbation that takes the magnitude of the parameters into account? (Other than the unprotected version). Also I would appreciate the if you could elaborate on the effect of perturbations.\\n\\n**Q2:** Could you also explain about the average online accuracy? it is stated that \\\"The average online accuracy is the percentage of correct predictions within each task.\\\" I cannot see the average part here. Is it calculating the accuracy on each task separately then averaging over the number of tasks?\"}"}]}
{"paper_id": "H8Qg1IIMaR", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large language models (LLMs) and vision-language models (VLLMs) have shown significant proficiency in a range of language tasks. They have been particularly successful in multiple-choice question answering (MCQA), leading to their widespread use in various applications. These models' high performance on benchmarks suggests robustness and reliability comparable to human-level proficiency, such as in professional exams.\nGap: Despite their strong performance metrics, there is limited investigation into the robustness of LLMs and VLLMs, particularly their susceptibility to simple transformations like answer option permutations in MCQA tasks. This area remains underexplored, especially concerning vision-language models, leaving a gap in understanding their real-world reliability and robustness.\nHypothesis: The paper hypothesizes that LLMs and VLLMs exhibit a critical vulnerability to adversarial permutations of multiple-choice answers, which dramatically impacts their accuracy and performance. This sensitivity persists regardless of model size or type, indicating a systemic issue with how these models process or prioritize input information.\nMethod: The study conducts experiments on a range of MCQA datasets to test various LLMs and VLLMs, including models like Llama2, Vicuna, and GPT-3.5-turbo. The datasets include MMLU, ARC-c, BoolQ, SocialiQA, MedMCQA, and more. The authors use simple adversarial permutation attacks by shuffling answer options and observe the models' performance. They test permutations with and without in-context learning and compare with baseline model performance, measuring accuracy drop as a primary metric. Majority voting, contextual calibration, and confidence-based strategies were evaluated but primarily found ineffective.\nConclusion: The study concludes that LLMs and VLLMs are highly susceptible to simple permutation attacks, which significantly decrease their performance, often below chance levels. Current mitigation strategies, like majority voting and calibration, are not practical or fully effective. The findings raise concerns about the evaluation of these models' robustness, emphasizing a need for research focused on developing models or strategies intrinsically resilient to such adversarial manipulations.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"The paper addresses the robustness of widely used models, a pertinent topic given the real-world deployment of these models.\", \"Weaknesses\": \"1. The methodology's depth and novelty are not entirely clear. More analysis could be provided on how natural variations in the way are ordered may impact models.\\n\\n2. While the paper focuses on permutation-based vulnerabilities, it might benefit from a broader discussion on other potential vulnerabilities or comparisons to other attack methods.\\n\\n3. The experiments provided are on a specific dataset. It would be beneficial to see how the models fare on diverse datasets to ensure the findings, which are not specific to one dataset's characteristics.\", \"Questions\": \"See the above weakness\"}"}]}
{"paper_id": "tth2qXY7RU", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Deep Neural Networks (DNNs) have achieved state-of-the-art performance across a variety of domains, leading to increased model sizes and computational demands. Traditional quantization approaches, both integer and floating-point, have been employed to address these challenges by reducing model sizes and improving computational efficiency. However, these techniques often face trade-offs between accuracy and computational benefits. Emerging research focuses on custom data types and hardware designs, highlighting piecewise and block floating-point quantization as promising methods, though they still struggle with handling data outliers effectively.\nGap: Despite improvements, existing quantization methods, including piecewise and block floating-point techniques, still face challenges in balancing the precision needed to represent diverse data distributions efficiently, particularly in handling data outliers within a limited bit budget.\nHypothesis: The authors propose the Super Floating-Point (SuFP) as a new data type and quantization method that employs multi-region piecewise quantization and a tensor-wise scalable bias. This approach aims to improve memory footprint and logic efficiency without compromising accuracy, by flexibly adapting to varying data distributions and efficiently representing both dense near-zero data and outliers.\nMethod: The SuFP method splits data distribution into multiple regions, allowing each region to have optimized precision through a variable encoding field. This supports both dense data regions near zero and sparse regions with outliers. The scalable bias is applied at the tensor level and requires only a minor addition operation, enhancing adaptability to diverse data distributions. SuFP is implemented with integer arithmetic units and shifters, creating a compact hardware design. The approach was tested across various domains, including vision, language, and generative models, and evaluated against traditional FP32, FP16, and other quantization methods using metrics such as accuracy, memory footprint reduction, throughput per area, and energy efficiency.\nConclusion: SuFP successfully maintains or surpasses the accuracy of full-precision floating-point (FP32) with significant improvements in computational capability and energy efficiency—9.00× and 17.04× respectively over FP32 and better than MSFP and BSFP. The SuFP approach, by combining multi-region quantization with scalable bias and efficient hardware implementation, delivers an effective solution for quantization challenges, offering significant benefits in both accuracy and hardware efficiency.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"+ The paper is well-written and easy to follow. Illustrative figures are well plotted and easy to understand.\\n+ Co-designing the hardware MAC architecture for the proposed data type SuFP enables better hardware efficiency.\\n+ The evaluation benchmarks are diverse, including both vision and language tasks.\", \"Weaknesses\": \"+ The ablation study lacks the improvement breakdown on piecewise data representation and tensor-wise scalable bias.\\n+ The accuracy evaluation experiments lack results on MSFP and BSFP for larger models (Table 3). The efficacy of the proposed SuFP on Large Language Models such as LLaMa2 is also unclear.\\n+ The proposed SuFP saves 6% memory, with 1.05x throughput improvement and 1.03x energy savings over MSFP (MX9). The improvement is marginal.\\n+ MSFP have multiple versions: MX4, MX6 and MX9. It is unclear how SuFP narrows its bit width.\", \"Questions\": \"Please answer the questions in the weakness section.\"}"}]}
{"paper_id": "fjf3YenThE", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Hard-thresholding algorithms are crucial for solving \\(\\ell_0\\) constrained optimization problems in high-dimensional machine learning, offering sparse learning solutions by reducing memory usage and computational demands. Traditional gradient-based optimization becomes challenging due to NP-hardness from \\(\\ell_0\\) constraints, making zeroth-order (ZO) methods necessary when gradients are inaccessible or computationally hard to obtain, as in black-box adversarial attacks.\nGap: The existing Stochastic Zeroth-Order Hard-Thresholding (SZOHT) algorithm is the only one addressing \\(\\ell_0\\) sparsity constraints with ZO gradients. However, it suffers from a limitation on the number of random directions it can handle, due to conflicts between ZO gradient deviations and the expansivity of the hard-thresholding operator, hindering its practical applicability.\nHypothesis: By changing the focus to variance reduction, the conflicts between ZO gradients and the hard-thresholding operator can be mitigated. This approach can potentially remove the limitations on the number of random directions in the algorithm, leading to improved convergence rates and broader applicability.\nMethod: The paper introduces a generalized variance-reduced ZO hard-thresholding algorithm. This method uses historical gradients to reduce variance thoroughly, thereby eliminating restrictions on the ZO gradient step. The algorithm was theoretically analyzed under assumptions like restricted strong smoothness and convexity. The authors demonstrated the method's utility through experiments on portfolio optimization and black-box adversarial attacks, comparing its performance with existing algorithms.\nConclusion: The proposed variance-reduced ZO hard-thresholding algorithm significantly improves convergence rates and applicability compared to SZOHT by removing the limitations associated with the number of random directions. The algorithm was validated through experiments, achieving competitive performance on sparse optimization tasks in adversarial attacks and portfolio optimization.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"The idea of combining variance reduction with zeroth-order hard-thresholding methods should be novel\", \"Weaknesses\": \"- The proposed method is not clearly introduced and has many confusing points. \\n- Experiments is not very supportive for the proposed method, details can be found in Questions part\", \"Questions\": \"- My first concern is on the usefulness of zeroth-order oracles. From experiments in this paper, it seems that we have easy access to gradient information in all these applications. Therefore, why do we need to emphasize zeroth-order algorithms? The authors may consider some other applications where gradient information is hard to obtain. \\n- Another concern is on the setting of L0 regularization. While there exist many other approaches on obtaining sparse solutions (e.g., LASSO), the authors may need to justify the necessity of using L0 regularization here.  \\n- I am a bit confused on the key contribution of this work. The authors introduced two algorithms, pM-SZHT and VR-SZHT. What is the key difference of these two algorithms? Moreover, pM-SZHT is not compared in experiments, but SARAH/SAGA-SZHT is compared. This is also very confusing and may need some explanations. \\n- It seems that SAGA-SZHT is missing in both the main text and appendix. Given its good performance in Table 1, I am a bit confused on why it is not properly introduced. Are there some errors? \\n- Regarding experimental results, I am a bit confused by Figure 3. It seems that SZOHT converges faster than all other methods, but gradually becomes worse with more iterations. Some explanations may be needed here on why we need so many iterations after convergence. \\n- Also, the evaluation of few-pixels universal adversarial attacks may seem a bit restricted with only 10 images from the same class. It would be better if the authors can use more images from different classes to further justify the effectiveness of proposed method.\"}"}]}
{"paper_id": "H9DYMIpz9c", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper addresses the importance of data quality and efficiency for training machine learning models, particularly focusing on autoregressive tasks like language modeling and sequential recommendation. Traditional approaches to handling large datasets for machine learning involve data downsampling or valuation to reduce costs, but these methods can be inefficient or intractable. Data distillation, which synthesizes data summaries for training models to a similar quality as training on full datasets, has been focused mostly on low-resolution data like images, whereas autoregressive tasks present unique challenges due to their discrete nature and large vocabulary spaces.\nGap: There is a lack of efficient data distillation techniques specifically tailored for autoregressive tasks, which involve a structured left-to-right causality of input and output sequences. Traditional data distillation methods are not easily applicable to autoregressive tasks due to computational inefficiencies and the inability to handle large discrete token spaces.\nHypothesis: The authors propose that their approach, FARZI, can effectively summarize large autoregressive datasets into smaller, synthetic sequences while maintaining or improving model performance, thereby reducing the cost of training large models.\nMethod: The FARZI method performs memory-efficient data distillation by using reverse-mode differentiation of the Adam optimizer with Hessian-Vector Products, and factorizes the high-dimensional event space into a latent space. This latent representation is intended to promote regularization implicitly. FARZI was tested on sequential recommendation and language modeling tasks, achieving 98-120% of the full-data performance utilizing only 0.1% of the original dataset size. The approach involves generating FARZI DATA as 3-D tensors of sequences of token distributions and employing a meta-learning framework to optimize the data summary effectively.\nConclusion: FARZI demonstrates that training state-of-the-art models on compact synthetic data derived from FARZI DATA is both computationally efficient and effective, achieving high performance with significantly reduced data inputs. This opens up possibilities for designing future large autoregressive models that are more resource-efficient and capable of scaling with less environmental and financial cost.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"- Originality and Significance: The latent parametrization that makes FARZI optimization friendly, and the proposed trick that enables reverse mode differentiation of Adam such that its memory complexity is independent of the number of inner loop steps are great contributions and of practical value.\\n- Quality and Clarity: The paper is well written with extensive experiments whose details and evaluations are that are clearly described. The results are impressive. The method is able to achieve better performance on downstream tasks compared with using the full dataset.\", \"Weaknesses\": \"- It is not clear whether this method will be practical and scale for larger language models and larger datasets. It would be great if the authors can elaborate on this.\\n- There is not a clear analysis of the total time gains of this method in comparison with training from scratch. Providing some values would make the case for this method more compelling.\", \"Questions\": \"Listed in weakness section.\"}"}]}
{"paper_id": "4kLVvIh8cp", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Offline RL, also known as batch RL, involves learning an optimal policy from pre-collected data without real-time interaction with the environment. Recent advancements have seen offline RL utilizing deep neural networks and achieving high performance in domains like gaming, robotics, and control systems, typically using linear function approximation. However, these models can struggle with large state-action spaces encountered in real-world applications, prompting interest in non-linear function approximations.\nGap: While offline RL with linear function approximation has been extensively studied and optimal results have been achieved, there is a lack of algorithms for non-linear function approximation that offer instance-dependent regret guarantees. Solving this could enable more effective algorithms in complex function class scenarios.\nHypothesis: The authors propose that a pessimism-based algorithm for non-linear function approximation could offer statistically optimal performance in offline RL by providing instance-dependent regret guarantees.\nMethod: The authors introduce an algorithm called Pessimistic Nonlinear Least-Square Value Iteration (PNLSVI). It includes three components: variance-based weighted regression, variance estimation, and a planning phase using pessimistic value iteration. The algorithm calculates a D2-divergence to quantify uncertainty and uses it to provide a tight regret bound. It is designed to be oracle-efficient, needing efficient regression and bonus oracles. Empirical variance estimation and regression reduction are conducted alongside a regular ridge regression to manage uncertainties about the function class without needing all errors or an entire policy space covered.\nConclusion: The PNLSVI algorithm is the first to offer statistically optimal instance-dependent regret bounds for non-linear function approximation. It generalizes pessimism-based linear approaches, provides a tight dependency on function class complexity, and achieves minimax optimality in linear approximation contexts, confirming its effectiveness and efficiency compared to existing methods.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"- The paper proposes an algorithm for offline RL with non-linear function approximation that has instance-dependent regret guarantees, which is new in literature.\\n- The author extend techniques used for linear MDPs (including variance-weighted ridge regression and reference-advantage decomposition) to nonlinear function classes.\\n- The paper is generally well-written and clear.\", \"Weaknesses\": \"- This paper poses a very strong converage assumption (Assumption 3.5), which may not be realistic in practice. In contrast, most pessimism-based offline RL papers that I'm aware of adopt weaker partial coverage assumptions. I wonder how valuable it is to prove an instance-dependent bound when we have to impose a uniform coverage condition.\\n- The lower bound cited by the paper only works for linear cases. The authors do not provide a matching lower bound to showcase the optimality of the proposed algorithm with general function approximations.\\n- The authors do not provide any numerical experiments in the paper. In my opinion, adding numerical experiments can better showcase the benefits of the proposed algorithm compared to other offline RL algorithms with general function approximation.\", \"Questions\": \"- Which part of the proof requires uniform data coverage while partial data coverage does not work?\\n- This paper considers general function approximation, yet I'm unclear what types of function classes & instance structures will satisfy the conditions listed in the paper. Can you give some concrete examples beyond linear MDPs?\\n- Can you provide a lower bound for general function approximations?\"}"}]}
{"paper_id": "CSpWgKo0ID", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large Language Models (LLMs) have emerged as powerful tools in text generation, demonstrating the capability to perform tasks that correlate with human cognitive skills, like reasoning and analogical thinking. As LLMs become more integrated into society and applications, understanding their behavior in interactive social contexts, particularly in terms of cooperation and coordination, is of significant importance. Traditional and behavioral game theory provides a structured method to explore these interactions, focusing on strategies beyond purely rational choices.\nGap: There is a limited understanding of how LLMs behave in social interactive settings, particularly when it involves repeated interactions with other agents. Previous studies have not thoroughly evaluated LLMs in repeated game settings to determine their strategic behavior in terms of cooperation and coordination, which is necessary to gauge their potential efficacy and trustworthiness in future applications.\nHypothesis: The study posits that behavioral game theory, when applied to LLMs, can uncover predictable behavioral patterns, especially concerning cooperation and coordination strategies. It is hypothesized that LLMs can perform optimally in self-interest-oriented games but might struggle in coordination games, revealing consistency or flaws in their interactive strategies.\nMethod: The researchers conducted experiments using LLMs, including GPT-4, in finitely repeated two-player, two-strategy games from different game families like the iterated Prisoner's Dilemma and the Battle of the Sexes. They used behavioral prompts to simulate interactions between LLMs and human-like strategies. Through a series of 144 different 2x2 game scenarios, they observed LLM performances which involved evaluating interactions across 10 rounds. Additionally, robustness checks were conducted by varying prompts and payoff matrices, and also using prompting techniques to improve LLM behavior.\nConclusion: The study finds that LLMs, such as GPT-4, generally excel in games that prioritize self-interest but perform less optimally in coordination-requiring games. GPT-4 is characterized by unforgiving behavior in competitive settings, like the Prisoner's Dilemma, and poor coordination in games like the Battle of the Sexes. Improving GPT-4's understanding and behavior through predicting others' actions and providing context about player mistakes showed potential for enhancing its coordination capabilities. This research lays groundwork for developing a behavioral game theory applicable to LLMs and highlights challenges in aligning LLM decisions closer to human social interaction ideals.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"The problem setup are well-motivated: using game to simulate LLMs behaviors and as a tool to assess their intelligence level is definitely a very promising direction. The presentation is clear.\", \"Weaknesses\": \"The way that it construct a strategy from an LLM is questionable. The conclusion of the evaluation does not provide too much insights to me. Please see the questions section below.\", \"Questions\": \"While I like the idea of using games to assess and evaluate the intelligence of LLMs, I am concerned about the following points.\\n\\n1. The way that you construct a strategy from LLMs. Equation (2) is not entirely clear to me. If I understand correctly, this approach always determinstically pick the action with maximum probability output by an LLM? What would be its difference between just sample according to the raw probability? There also could be a variety of other ways to construct a strategy. Actually the results in your coordination game study suggest one: first predict the opponent's next action, and then play a best response against that. This type of LLM strategy has been found impressive in repeated rock-paper-scissors [1]. However, no justification for your approach and comparison can be found in the paper.\\n\\n2. The evaluation methodologies in the experimental section. These include:\\n\\n      i. The scores in Table 1. I am not entirely following the definition \\\"if both players had played such that the player we are analyzing would have gained the maximum possible outcomes on every round\\\". Does it mean the maximum payoff of the current player in the game matrix? If it is then I think it is a really bad metric as it optimistically assumes the opponent always help you, which almost never happen. A more reasonable metric could be regret-based notions. For example, the maximum deviation you can gain assuming your opponent's strategy is fixed.\\n\\n     ii Also, I am not sure how is the scores against different types opponents are aggregated into one score for each entry of Table 1. Is it an average over the scores against all opponents? If it is, then probably it also need justification.\\n\\n    iii For the results in the prisoner's dilliemma section. First I think an important baseline strategy, tit-for-tat is missing. And according to the head-to-head payoff matrix, looks like GPT-4, text-davinci-003, claude and defect are all pretty strong? Which one is the more intelligent one according to this result? Also is the right-hand figure of Figure 3 depict only one trajectory? How many trajectories do you run for each pair of strategies?\\n\\n    iv. In your opinions, why different LLMs behaviors qualitively different?\\n\\n[1] \\\"Population-based Evaluation in Repeated Rock-Paper-Scissors as a Benchmark for Multiagent Reinforcement Learning\\\", Lanctot et. al. TMLR\"}"}]}
{"paper_id": "gwbQ2YwLhD", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Structure learning aims to uncover the underlying graphical structure from data, often represented by a Directed Acyclic Graph (DAG). DAGs are prevalent in applications like Bayesian Networks and causal research. Various methods exist to learn DAGs from data, with many relying on optimizing least square losses or log-likelihood losses. Previous research has indicated that these optimization processes can be influenced by the scale of data, particularly noted in low-dimensional settings.\nGap: While it is known that least square losses and performance of structure learning algorithms can be influenced by data scale in low-dimensional, linear settings, there is a lack of understanding and exploration of these issues in higher-dimensional and non-linear cases.\nHypothesis: The authors hypothesize that scale affects performance of structure learners, and that least square losses and log-likelihood losses can be minimal for incorrect DAGs in higher dimensions and non-linear cases. They aim to generalize previous findings to d-dimensional settings and show the scale impact on various loss functions used in structure learning.\nMethod: The authors provide theoretical proofs and conduct experiments to show conditions under which MMSE and other loss functions become minimal for incorrect DAGs. They utilize synthetic data and real-world datasets to test their theoretical findings across different graph structures and dependency types (linear and non-linear). The structure learning algorithms explored include NT, DAG-GNN, GraN-DAG, GES, and others optimized on MMSE, BIC, and ELBO losses.\nConclusion: The paper concludes that the scale of the variables significantly affects the prediction performance of structure learning algorithms. Specifically, MMSE and log-likelihood losses become minimal for incorrect DAGs due to scale, influencing both linear and non-linear relationships. The study emphasizes the need for caution when interpreting results from scale-sensitive losses and advocates for normalization and exclusion of free variances in loss calculations to improve robustness.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"the authors provide conditions under which using square-based losses will lead a wrong DAG result in structure learning. The conclusions drawn in the paper are reasonable.\", \"Weaknesses\": \"I think that the findings in the paper aren't unexpected. Learning causal structures should go beyond just predictive effects (MMSE). I don't see the paper as particularly significant.\", \"Questions\": \"1.\\tIn this article, MMSE seems to be used for prediction, rather than for learning causal graphs. The definition of MMSE is the sum of squares of predicted residuals, which characterizes the statistical correlation of variables rather than causal relationships. It is an obvious conclusion that only using statistical correlation for causal graph learning will lead to errors.\\n2.\\tIn structure learning algorithms, the score function usually contains penalty terms, and the variable that needs to be optimized in the score function is the adjacency matrix W, rather than data X. In addition, the adjacency matrix W will also be included in the penalty term. So, why does the score function directly degenerate into a square based loss function? Or, why is optimizing the score function equivalent to optimizing MMSE?\\n3.\\tIn practical applications, most of the score functions we choose are not affected by data normalization, just like the SRL function mentioned in this article. Thus, this article does not have a clear critical objective, or rather, the critical target seems to be rare in practical applications, which greatly reduces the significance of this study.\"}"}]}
{"paper_id": "eIYDKNqXuV", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Unsupervised clustering is essential for understanding the inherent patterns within datasets, especially when dealing with unlabeled or partially labeled data. Traditional methods like K-Means, DBScan, and fuzzy C-Means often face challenges with non-linear and complex datasets. Advanced methods like t-SNE, UMAP, and spectral clustering tackle such challenges but at high computational costs. There's a need for methods that are efficient and scalable for large and complex datasets.\nGap: Existing advanced clustering methods for non-convex or complex manifold data structures are often computationally intensive and require extensive hyperparameter tuning. This limits their scalability and efficiency with large-scale datasets or those with complex structures.\nHypothesis: The authors propose 'Village-Net Clustering,' a two-phase clustering algorithm that combines the simplicity and speed of K-Means with a novel community detection method to effectively cluster complex datasets. The method autonomously determines the number of clusters, promising both computational efficiency and scalability.\nMethod: Village-Net Clustering starts with K-Means to divide the dataset into partial clusters called 'villages.' These villages then form nodes in a weighted network indicating proximity relationships. The Walk-Likelihood Community Finder (WLCF) algorithm is used to detect communities within this network. The method can determine the number of clusters automatically and has a time complexity of O(N*k*d), making it efficient for large datasets. The experimental setup involved benchmarking against several real-world datasets (such as MNIST and Letters) to evaluate performance using metrics like NMI.\nConclusion: Village-Net Clustering demonstrates competitive performance with state-of-the-art methods, particularly excelling in speed and efficiency. While it performs well on real datasets, its accuracy can be limited by the initial K-Means step. The method's scalability and runtime efficiency provide a significant advantage, though future work may focus on increasing accuracy through iterative refinement of cluster representations.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The proposed method seems to be accurate and efficient in practice.\", \"Weaknesses\": \"1. The contribution of the paper is not so clear. The idea of over-clustering + merging is not new, which can be find in many previous methods, like BIRCH.\\n2. The proposed method is a combination of different techniques, but it is not clear why this combination is unique and which specific unsolved problem it can handle.\\n3. The comparison methods are weak, only K-means and DBSCAN.\\n4. The performance of the proposed method strongly depend on K-means and hyper-parameters. \\n5. The authors violated the double blind rule.\", \"Questions\": \"See above weakness.\"}"}]}
{"paper_id": "bAMPOUF227", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large Language Models (LLMs) have shown significant abilities in in-context learning through prompt engineering, and recent advancements in generative models have expanded their applications. However, LLMs face challenges like resource demands, slow inference, and susceptibility to hallucinations. In contrast, Supervised Language Models (SLMs) are more cost-efficient and can rapidly adapt to specific tasks, although they lack the broad capabilities of LLMs.\nGap: Existing research has primarily treated LLMs and SLMs as separate paradigms and hasn't thoroughly explored the use of task-specific SLMs to systematically enhance LLMs' in-context learning, particularly in out-of-distribution (OOD) scenarios.\nHypothesis: The authors hypothesize that SLMs, which are fine-tuned on specific tasks, can enhance LLMs' performance during inference by providing task-specific supervised knowledge. This can help LLMs generalize better to OOD data and reduce hallucinations.\nMethod: The paper introduces a framework called SuperContext. It integrates the predictions and confidence scores from task-specific SLMs into the in-context learning prompts of LLMs. This method is tested using Llama 2 and ChatGPT, with experiments conducted on datasets for OOD tasks and question answering. The approach uses 16 curated datasets across 9 tasks, with both zero-shot and few-shot experiments to evaluate performance compared to baseline methods.\nConclusion: SuperContext is shown to enhance the performance of LLMs in handling OOD tasks, improving generalizability and factuality compared to baseline LLM and SLM models. SuperContext with integrated SLM knowledge improves performance significantly, particularly in reducing hallucinations in generative tasks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"The proposed approach serves as a plug-in method, resulting in enhanced versions of Llama 2 and ChatGPT that outperform their original counterparts in terms of generalizability and factuality. SuperContext can bring decent performance benefit compared to few-shot in-context learning and outperform original SLMs and LLMs with both zero-shot and few-shot settings.\", \"Weaknesses\": \"1. I disagree with the statement in the introduction that says, \\\"However, since our goal is to allow reliable task adaptation rather than knowledge acquisition, the consulting agent becomes SLMs rather than search engines.\\\" It seems that the approach in this paper is quite similar to what Hugging Face's [1] GPT does, with the only difference being that it uses its own trained API instead of Hugging Face's SOTA small model API.\\n\\n2. The paper lacks significant novelty, essentially enhancing the output of small models with larger models. However, it extensively validates the effectiveness of this approach (even though Hugging Face's GPT has also done similar validations).\\n\\n3. The paper intentionally incorporates confidence when using the output of small models, but it lacks a detailed ablation study on the role of confidence. I am particularly interested in understanding the significance of confidence in this context.\\n\\n[1] HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face. NeurIPS 2023\", \"Questions\": \"None\"}"}]}
{"paper_id": "eNoiRal5xi", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Domain Generalization (DG) aims to enhance the transferability of models from a source domain to unobserved domains. Traditional methods include alignment-based, augmentation-based, and regularization-based approaches, but they often underperform when domain information is limited. Sharpness-aware optimizers such as SAM focus on flattening the loss landscape in parameter space to improve generalization across domains, backed by PAC-Bayesian theories.\nGap: While SAM and its variants have improved DG, they primarily focus on parameter space perturbation, lacking theoretical guarantees for generalization to unknown domains that are not theoretically accessible. These methods do not address unexplored potential in data space perturbation for generalizing to unknown domains.\nHypothesis: The paper proposes that combining parameter space perturbation with data space perturbation can improve model generalization to unknown domains. By aligning the loss landscape between a source domain and perturbed (simulated unknown) domains, grounded in flat minima, the model should generalize better across unseen domains.\nMethod: The authors introduce Unknown Domain Inconsistency Minimization (UDIM), which aims to minimize the discrepancy in loss landscapes between the source domain and unknown domains using parameter and data perturbations. Unknown domains are simulated by perturbing instances from the source dataset. The UDIM framework combines SAM optimization with an objective that empirically bounds the true objective of the DG task using both parameter perturbation and data space exploration. UDIM's effectiveness is evaluated on several DG benchmark datasets, demonstrating statistically significant improvements, especially in scenarios with more restrictive domain information.\nConclusion: UDIM consistently outperforms SAM variants across multiple DG benchmark datasets, highlighting its superior generalization capability in unseen domains. By reducing the loss landscape inconsistency between source and unknown domains through combined parameter and data perturbation, UDIM provides a robust solution to the domain generalization problem.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1.This work extends the parameter perturbation in existing SAM optimization to data perturbation, achieving loss landscape alignment between source domains and unknown domains. Experiments show the validity of the proposed objective.  \\n\\n2.This work establishes an upper bound for DG by merging SAM optimization with the proposed objective.\\n\\n3.The proposed objective can be combined with multiple SAM optimizers and further enhance their performance, demonstrating the necessity of the loss landscape consistency between source and unknown domains.\", \"Weaknesses\": \"1.I believe that the proposed data perturbation method is consistent in both ideology and essence with traditional domain augmentation and adversarial attack techniques. So, what is the main difference and advantage of the proposed objective? And what if combining some domain augmentation techniques with the SAM optimizers?\\n\\n2.How to guarantee that the perturbed data is still meaningful, rather than generating some noisy samples? If so, will enforced the loss landscape alignment across domains bring negative impact? Besides, the unknown distribution may not necessarily be within the scope of perturbed data region.\\n\\n3.What is the sampling strategy for sampling data from $D_s$ to generate perturbed data?\\n\\n4.Is the optimization only conducted on the perturbed samples during the formal training phase? Would it be more stable to train on both the source and perturbed samples simultaneously?\\n\\n5.Since the training procedure involves gradient calculation, what is the time complexity after applying the BackPACK technique?\", \"Questions\": \"See weakness.\"}"}]}
{"paper_id": "uU0Adp7Sfo", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Generative adversarial networks (GANs) are used to generate synthetic data by creating a competition between a generator and a discriminator. However, the traditional competition-only setup of GANs is insufficient when broader insights into data distributions are required, such as in multi-modal data generation and image super-resolution.\nGap: Traditional GANs suffer from a limitation in addressing the need for applications that require the incorporation of additional knowledge beyond just replicating the authentic data distribution. This includes challenges in multi-modal generation and the ability to replicate images accurately, as the standard competitive nature lacks incentive for generator and discriminator to collectively meet these broader requirements.\nHypothesis: The paper hypothesizes that by developing a collaborative-competitive framework (CCGAN), which blends competition with collaboration, it is possible to enable more informed data generation that respects additional requirements beyond the authentic data distribution, while theoretically preserving equilibrium and avoiding training collapse.\nMethod: CCGAN introduces a new collaboration-competitive scheme by incorporating regularization as a divergence metric that allows for a theoretically well-defined game-theoretic setup. This approach harmonizes competition and collaboration losses, thereby reducing the complexity of solving optimality. CCGAN is designed with a novel regularization term that breaks the mutual dependency between generator and discriminator, allowing for exploration of a broader parameter space. Comprehensive experiments were performed using 4 datasets to validate CCGAN's performance compared to baseline models, specifically focusing on generating images of regularly shaped objects.\nConclusion: CCGAN, by providing a unified framework and ensuring the presence of an equilibrium point, overcomes the limitations of pure-competition GANs. It leads to consistently high-quality data generation, providing both theoretical and empirical improvements. CCGAN shows better performance, especially in regular-shaped object generation, compared to baseline GAN models and is robust to training collapse. It facilitates stable data generation that addresses broader requirements, as demonstrated by several experiments.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 5, \"Strengths\": \"- Promising idea support by theory proofs \\n- The newly introduced method does not cause additional significant computational cost. \\n- Nice application demo on multiple vision tasks.\", \"Weaknesses\": \"1. Formatting is not quite right. \\n\\n2. There are existing multiplayer methods to improve the stability of GAN training, which changes the original GAN competition setting by introducing additional players. Would like to hear some words from the authors about the advantages or disadvantages compared methods in this setting. \\n- https://arxiv.org/abs/2101.07524\\n- https://arxiv.org/abs/1907.02690\\n- https://arxiv.org/abs/1709.03831\\n\\n3. Over claim of the visual results\\n- All the visual results are too small. I can’t really see the difference unless I zoom hard on a giant monitor, which is not possible from the printed version. \\n- Figure 3 column 3,4: Not really better than baseline\\n- Figure 4b: I can’t see what the original mask is, again too small to show the details.\", \"Questions\": \"1. It sounds like the proposed method should work with the original GAN method. Why not a direct comparison with it on an image generation task with FID / IS scores? \\n\\n2. “We prove that the generator and discriminator is not mutually dependent in CCGAN” Can you combine it with SOTA methods like StyleGAN3?\\n\\n3. Training details are missing from the experiment section. Resolution? Sample size?\"}"}]}
{"paper_id": "dKju7tbe6D", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Visual reasoning and question answering require models to execute and combine multiple visual operations to make inferences. Iterative computation has been shown to be effective for this, as demonstrated by models like the MAC architecture. However, purely iterative approaches may require more steps and lead to poor generalization. On the other hand, parallel computation can efficiently handle mutually independent operations.\nGap: Current visual reasoning mechanisms are typically either purely iterative or parallel, which can lead to inefficiency and poor generalization in scenarios where both computation methods could be beneficial.\nHypothesis: Integrating both iterative and parallel computation into a single neural reasoning architecture could enhance the efficiency and generalizability of visual reasoning models compared to using iterative or parallel methods alone.\nMethod: The authors propose the Iterative and Parallel Reasoning Mechanism (IPRM), which models memory as sets of operation and result states. Given visual and language inputs, IPRM iteratively performs three main computations at each reasoning step: Operation Formation, Operation Execution, and Operation Composition. This approach dynamically composes and executes multiple operations simultaneously while also allowing step-by-step iterative processing. The method was tested on visual reasoning benchmarks such as GQA, CLEVR, CLEVR-Humans, NLVR, and CLEVR-CoGen, and compared against established models like MAC and transformer-based attention mechanisms.\nConclusion: IPRM outperforms existing reasoning mechanisms on several benchmarks, demonstrating stronger generalization and efficiency while using fewer parameters and computation steps. It achieves state-of-the-art zero-shot performance on the CLEVR-Humans dataset and excels at transfer learning tasks, notably outperforming prior methods on the NLVR benchmark. The combination of iterative and parallel processing aids the learning of more general reasoning strategies.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"1. The motivation for the architecture is interesting and reasonable.\\n2. The modular nature of the reasoning pipeline makes the overall steps seem intuitive. \\n3. The experiments show improved performance on several benchmarks over other methods that do not use explicit parallel / iterative reasoning.\", \"Weaknesses\": \"One of the main things that needs to be addressed is the part of the architecture that does the parallel reasoning. From the architecture description, this seems to arise of the first dimension in the some of the internal state matrices of dimension $N_m$. Is it due to the masks in the Operation Composition stage that make these operations \\\"parallel\\\" so that these internal states attend to different parts of the input parallelly? I am trying to understand the difference between having a larger internal state versus explicit parallel reasoning. Where is the structure enforced in the architecture that requires parallel processing when possible?\\n\\nRelated to the above, there is one line that says identity attention is used to \\\"enable a higher degree of operation composition\\\". I don't follow this and needs clarity. Overall, I am not sure how this operation composition step is different and not just implicitly subsumed into earlier architectures, where is the parallel processing explicitly arising from?\\n\\nI think the iterative nature, on the other hand, is immediately due to the recurrent nature of the algorithm.\\n\\nThere seem to be many design decisions that are not immediately clear as being intuitive, and need further explanation. This is true of the Operation Composition stage. It seems to me that this stage is supposed keep a record of the operations performed up to a time step. It is said that operation states are composed with states from multiple time steps in the past. I don't follow this, isn't the M matrix already encoding memory of states in some way, why look back multiple time steps? This stage of the architecture is difficult to follow.\\n\\nThe visualizations in the supplement for one example seem to be pointing to some interesting aspects of the algorithm. But I don't see where the parallel reasoning is being utilized or even the iterative aspects. I think a clear explanation of the figure is needed, I don't understand how to interpret the memory states, the parallel processing, or how the prediction \\\"cylinder\\\" came into being. Perhaps visualizing other matrices and attention masks is also important here.\\n\\nMany more visualizations should be added to illustrate these important parts of the algorithm. Otherwise, it is hard to say whether it is indeed the parallel and iterative structure that is helping or a different issue.\\n\\nSome of these questions also arise because the numbers themselves are also hard to interpret. I don't see why test samples for \\\"logic\\\" type in Table 1 need parallel computation. The reasoning the authors provide are circular and seem to be based on the numbers themselves.\", \"Questions\": \"A minor question, is $N_m$ the same as M in Table 4?\"}"}]}
{"paper_id": "DL7JWbdGr3", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Forecasting epidemic trends is crucial for public health decision-making. Deep learning methods have shown promise in outperforming traditional mechanistic models by learning from past epidemic data. Projects like Tycho have aggregated large datasets from past diseases, presenting an opportunity for leveraging this rich source of historical data.\nGap: Traditional epidemic forecasting methods often only focus on the specific epidemic of interest and fail to leverage the wealth of historical data from different diseases due to challenges in data heterogeneity and lack of methodologies to incorporate such diverse data. Moreover, previous models do not benefit from pre-training paradigms effectively used in other domains like language and vision.\nHypothesis: Pre-trained Epidemic Time-Series Models (PEMS) can effectively learn useful priors from heterogeneous epidemic data using self-supervised learning tasks, allowing them to be efficiently fine-tuned for various downstream epidemic analysis tasks, resulting in improved performance over current state-of-the-art models.\nMethod: The authors introduce PEMs, which involve pre-training transformer-based models on multiple datasets from different diseases using self-supervised learning tasks, designed to capture epidemic dynamics. They utilize tasks such as random masking (RANDMASK), last segment masking (LASTMASK), peak masking (PEAKMASK), and seasonal detection (SEASONDETECT) to learn valuable priors. Data is sourced from diverse datasets including Project Tycho and influenza datasets from the US and Japan. The models are evaluated against existing forecasting benchmarks, focusing on efficiency and robustness to new epidemics.\nConclusion: PEMs significantly outperform existing state-of-the-art models in various epidemic forecasting tasks by providing 11-24% better forecasting accuracy and improved peak prediction. The pre-training scheme enhances training efficiency and allows generalization to new diseases not seen in the pre-training phase, showcasing the model's adaptability and utility in real-world epidemic monitoring.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"- originality: This is a new application domain for SSL as far as I know, but the methodology is not particularly new.\\n\\n- quality: The submission is well written and formatted, with strong experimental results and ablations.\\n\\n- clarity: most of the training details are there, though much is relegated to the appendix.\\n\\n- significance: the work is significant as an application of SSL to a specific domain with greater performance, and finds relevance in the current post-pandemic context.\", \"Weaknesses\": \"- sounds like your segments of time series is basically patching, similar to (Nie et al. 2022). If so, why do you achieve so much better results than PatchTST? Is it just the finetuning?\\n\\n- I'm not sure I understand why inputting each time step as a single token would lack \\\"semantic meaning\\\", seeing as the output representations of a transformer for a given token are influenced by all the other tokens in the input sequence (i.e. \\\"contextual embeddings\\\")\\n\\n- How do you normalize the training process of your method wrt baselines? How can you disentangle the architecture's influence from the influence of the training process and the influence of using additional compute?\\n\\n- unclear what is being evaluated in appendix D table 6 since it's not known how the baselines are trained. For example, it is clearly remarked that no pre-training is performed for the PT baseline, but not why.\\n\\n- why are the results around one of the contributions (Significant improvement in data and training efficiency and adaptability to novel epidemics) only in the appendix?\", \"Questions\": \"- There might be bleed between the influenza data from 2001 to 2010 and the crypto data from 2006 to 2012 given the temporal alignment, but I'm not knowledgeable enough to judge how strong the overlap could be. Is the argument that, since influenza and crypto have different inherent dynamics, they shouldn't influence each other?\\n\\n- How do you choose the value of T, i.e. the length of the input time series?\\n\\n- How do you justify the default choice of gamma = 0.1 for lastmask ? Seems like 0.2 performs well too.\\n\\nOverall, I am rating this a 5 (marginally below acceptance) before discussion, since it is hard to disentangle the impact of the supervised pre-training from the architectural design choices and the increased compute/data. There are no details on how the baselines are trained, so these comparisons do not properly motivate which components of PEM lead to its superior performance. I am very amenable to changing this rating given discussions around how the baselines were implemented for fair comparisons.\"}"}]}
{"paper_id": "1PaDPHDhwe", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Machine learning models often perform well on average but struggle with minority groups due to spurious correlations and dataset biases. Group distributionally robust optimization (GDRO) has been introduced to mitigate algorithmic biases, but it typically improves robust accuracy at the cost of average accuracy.\nGap: Existing approaches to improve robust accuracy often sacrifice average accuracy, and there has been little exploration of methods that consider both metrics comprehensively. Current methods have limited ability to evaluate performance across the trade-off between robust and average accuracy.\nHypothesis: The authors propose that a simple post-processing technique called class-specific scaling can effectively manage the trade-off between robust and average accuracies by optimizing both metrics simultaneously, without requiring additional training.\nMethod: The method introduces a class-specific scaling strategy that adjusts prediction scores of pre-trained models to identify Pareto optimal points on the trade-off curve between robust and average accuracies. They propose an instance-wise adaptive scaling method, which uses clustering to further optimize scaling factors for individual test examples. A novel evaluation metric, robust coverage, measures the completeness of debiasing algorithms by capturing the trade-off.\nConclusion: The proposed scaling methods improve both robust and average accuracies across various datasets, demonstrating that a post-processing approach can be effective without additional training. The approach allows for a comprehensive evaluation and understanding of debiasing algorithms by revealing the trade-offs they entail.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 1, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"The paper tackles an important problem. It has a detailed discussion of related works. Performance improvements are consistent.\", \"Weaknesses\": \"1. The proposed method is too trivial. It simply searches a classwise scaling factor on the validation set. There is also no insight provided, e.g., why does the proposed classwise scaling bring improvements in the target metric? How does the optimal scaling factor differ for various metrics?\\n\\n2. The comparison with baselines is potentially unfair. Since the scaling factors are found by greedy search on the validation set, they can be regarded as trainable parameters. Hence the proposed method effectively leverages the validation set as additional training data. Hence direct comparison with baselines is unfair.\\n\\n3. The paper is poorly written. For example, in the first paragraph of the introduction, the concept of \\\"group\\\" and \\\"minority group\\\" is discussed without proper introduction. The claim that \\\"there has been little exploration for a comprehensive evaluation that considers both metrics jointly\\\" is not convincing, as there are many works studying the tradeoff between in-distribution and out-of-distribution accuracy [1*]. Despite the simplicity of the method, it is poorly explained, e.g., what is the detailed procedure of greedy search? Is it by gradient descent or brute-force trial from a pre-determined list? Section 3.3 is especially confusing: what are the cluster centroids used for? How to perform step 2? How to determine the cluster membership?\\n\\n[1*] Calibrated ensembles can mitigate accuracy tradeoffs under distribution shift. UAI 2022.\\n\\nOther suggestions & questions\\n\\n1. It is recommended to include the dataset statistics (train/val/test set), especially because the proposed method leverages the val set.\\n\\n2. Beyond numerical results, it is more interesting to see some qualitative examples of how the proposed approach improves baselines.\", \"Questions\": \"See weaknesses.\"}"}]}
{"paper_id": "iI7hZSczxE", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Disentangled representation learning has been vital in fields such as computer vision and natural language processing due to its benefits in interpretability, fairness, and generalization. Previous efforts on time series have faced challenges in generalization, especially regarding energy consumption data like Non-Intrusive Load Monitoring (NILM). Standard disentanglement methods often assume statistical independence of latent attributes, which can lead to non-identifiability when real-world data exhibits correlated attributes.\nGap: Existing methods for disentangled representation learning in time series assume attribute independence, which is often not valid due to real-world correlations between attributes. This leads to issues in identifying disentangled representations and impacts the ability to handle real-world scenarios like energy consumption disaggregation.\nHypothesis: The hypothesis is that by relaxing the assumption of statistical independence to independence-of-support via contrastive learning, it is possible to achieve more robust disentangled representations for time series data with correlated attributes. This approach should enable improved identification and generalization of representations across diverse and correlated scenarios.\nMethod: The authors propose the Disentanglement under Independence-Of-Support via Contrastive Learning (DIOSC), which minimizes support overlaps of latent variables using contrastive learning. This is combined with l-variational inference layers incorporating self-attention mechanisms to address temporal dependencies and achieve robust representation learning. The method involves a single encoder/decoder for efficient disentanglement, using contrastive objectives to maintain support independence. A new metric, the Time Disentangling Score (TDS), is proposed to measure disentanglement quality.\nConclusion: DIOSC demonstrates improved disentanglement and robustness in handling correlated attributes in time series data, outperforming traditional methods by up to 61.4% in specific metrics. The approach enhances generalization on downstream tasks like NILM, highlighting the potential of contrastive learning combined with attentive l-variational inference in addressing real-world generalization challenges.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 1, \"Confidence\": 3, \"Strengths\": \"The main strength of the paper is its approach to tackling the important problem of disentangled representation learning, which may contribute to reducing carbon footprint.\", \"Weaknesses\": \"The paper lacks proper organization and has a tendency to include some unproven (or wrong) claims. For instance, in the introduction, the authors mention that \\\"Disentanglement via VAE **can be achieved** by a regularization term of the Kullback-Leibler divergence[],\\\" which is not necessarily true without certain strong assumptions and underlying conditions. The Beta-VAE paper has some qualitative evidence showing how the images are more disentangled compared to VAE. The authors also claim, \\\"Rather than training separate auto-encoders for individual appliances,\\\" which requires empirical validation with proper citations.\\n\\nIn addition to these issues, the notations used are very confusing and are not defined before they are referenced. For example, in the proposed method section, the notation $z_m^+$ is used without description it. It is also unclear how it differs from $\\\\bf{z}$ or $z$.\\n\\nThe main goal of this paper remains unclear to me. For example, the authors mentioned, \\\"The primary goals of this work are twofold: to effectively address the NILM problem and to obtain a disentangled representation of input data.\\\" However, it is unclear what the NILM problem is, what the nature of the input data is, and how the authors plan to achieve a disentangled representation that distinguishes itself from previous works. One issue might be that the problem statement and preliminaries are somewhat intertwined.\\n\\nThe color meaning used in the tables of result section is not clear. Even it is not clear how TDS (as a metric) has been compared with VAE and Beta-VAE in Table 1.\", \"Questions\": \"- I strongly suggest the authors make their main contributions clear at the end of the introduction. \\n\\n- There is no related work section.\"}"}]}
{"paper_id": "qrGjFJVl3m", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large Language Models (LLMs) have achieved remarkable capabilities in text generation and comprehension but are limited to dealing with pure text, lacking the ability to process other modalities like images and speech. Recently, there has been interest in Large Vision-Language Models (LVLMs) which aim to incorporate visual perception into language models to solve real-world problems involving both text and images. However, many open-source LVLMs suffer from inadequate training and optimization compared to proprietary models. Fine-grained visual understanding is essential for LVLMs, yet most existing models only offer coarse-grained understanding or lack capabilities such as object grounding and text reading.\nGap: Open-source LVLMs currently lag behind proprietary models in terms of training and optimization, limiting their application. Fine-grained visual understanding, including tasks like object grounding and text reading, remains challenging, and most models only achieve coarse-grained perception. Additionally, multilingual support in LVLMs is underexplored.\nHypothesis: The authors hypothesize that by endowing the Qwen-LM with visual capabilities through a meticulous design, it is possible to develop a series of vision-language models (Qwen-VL) that achieve superior performance on visual-language benchmarks, particularly in fine-grained visual understanding and multilingual support. These models could set new records for generalist models on various tasks and improve the state of the art in vision-language dialogs.\nMethod: The Qwen-VL series is built on the Qwen-7B language model foundation and enhanced with visual capabilities through a new visual receptor, input-output interface, a three-stage training pipeline, and a multilingual multimodal cleaned corpus. The network architecture consists of a large language model, a visual encoder (using Vision Transformer), and a position-aware vision-language adapter to compress image features efficiently. The training process consists of three stages: large-scale pre-training on weakly labeled image-text pairs, multi-task pre-training on fine-grained annotated data across several tasks (e.g., VQA, grounding), and supervised fine-tuning for dialog capabilities. The models are trained on a multilingual corpus to support multiple languages and include fine-grained visual tasks for enhanced comprehension. Evaluation is extensive, covering benchmarks in image captioning, general and text-oriented VQA, and referring expression comprehension, and measuring effectiveness in real-world scenarios through dialog benchmarks like TouchStone.\nConclusion: The Qwen-VL series, including Qwen-VL-Chat, significantly outperforms previous models across a range of visual-language tasks and languages, showing superior capability in multilingual dialog, fine-grained understanding, and grounding. These open-source models set new benchmarks and represent a significant step forward for LVLMs. The comprehensive training on diverse datasets allows Qwen-VL models to excel in image captions, VQA, and real-world applications, thereby providing a strong foundation for future research. The authors plan to further enhance Qwen-VL by integrating additional modalities and increasing the model scale.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"- Open sourcing the model is going to help the research community\\n- The model shows decent multimodal capability, especially in terms of bounding box reasoning\", \"Weaknesses\": \"My concerns are regarding the scientific and technical contributions from this paper.\\n- The claim in the related work section, \\\"Despite achieving significant progress, previous vision-language models still have several limitations such as poor robustness in instruction following, limited generalization capabilities in unseen tasks, and a lack of in-context abilities.\\\" lacks justification. For example many models are not instruction-tuned (yet). That does not mean they have a fundamental difficulty in instruction following.\\n- There is limited innovation on the model architecture and training recipe. For example the use of interleaved data and multi-stage, multi-resolution training has been proposed in previous works. Also there is limited novelty in showing that supervised finetuning with interleaved chat data can lead to chatting capability.\\n- The ablation study is not written clearly (also see questions below)\", \"Questions\": \"In the ablation study of Figure 7, which stage is that, lower-res or higher-res? If it is lower-res (224) stage it makes sense to use 256 tokens as the native number of patches is just (224/14)^2 = 256. If it is the higher-res (448) stage, then it is counterintuitive that using more tokens, i.e., 400, with more degrees of freedom, will lead to worse performance.\"}"}]}
{"paper_id": "qup9xD8mW4", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Dataset distillation involves synthesizing a small number of data points that can effectively replace large datasets for model training, and has shown success in supervised learning tasks. However, its adaptation to reinforcement learning (RL) has been limited due to the absence of a fixed dataset in RL settings.\nGap: Most existing dataset distillation methods cannot be directly used in reinforcement learning because there is no fixed dataset to sample from, which limits their application in RL tasks.\nHypothesis: The paper hypothesizes that it is possible to discover and condense the information required for training an expert policy in reinforcement learning into a small synthetic dataset of state-action pairs without access to prior expert data, thus enabling the concept of 'behaviour distillation'.\nMethod: The authors introduce HaDES (Hallucinating Datasets with Evolution Strategies), a novel method for behaviour distillation. HaDES uses a meta-evolutionary approach, applying evolutionary strategies (ES) in the outer loop to update a synthetic dataset, while employing supervised learning techniques in the inner loop to train policies on these datasets. By utilizing this bi-level optimization process, HaDES is able to distill RL information into as few as four state-action pairs that are effective across various tasks and architectures. The method was tested extensively across multiple environments, including continuous control tasks in the Brax suite and discrete tasks in MinAtar.\nConclusion: The study concludes that HaDES effectively distills RL tasks into small datasets, enabling efficient training of policies without exploration. It also shows that these datasets generalize well to a variety of architectures and hyperparameters and can be applied to multi-task learning without additional environment interaction, thus expanding the scope and utility of distilled datasets in RL. Moreover, HaDES also achieves competitive performance in traditional dataset distillation for supervised learning tasks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"+ The introduction of behaviour distillation can enrich the literature and direction of Dataset Distillation. Different from standard DD, behaviour distillation does not require access to the expert datasets. This is quite close to a standard /basic RL setting and could be a good starting point.\\n\\n+ The author's writing is easy to follow and pretty clear on the technical details\\n\\n+ The experimental section show promising results using the proposed algorithm HADES.\", \"Weaknesses\": \"- Although it is interesting, the proposed behaviour distillation seems to not have a clear motivation on why it can be useful (what's the motivation for proposing this problem and it's potential application, besides DD hasn't been applied to RL), and why not directly formulating the problem on expert dataset. Distilling directly from scratch can make the problem a lot harder.\\n- It would be great if the authors can discuss the behaviour difference of a standard RL algorithm and a synthetic data-driven RL algorithm (HADES). What's the potential benefits?\\n- One missing citation on BPTT [2]. It would be nice to add some discussion on optimization algorithm in DD, for example BPTT (the original one[1] and momentum-based[2]).\\n\\nOverall the paper is quite interesting. Looking forward to authors' response on the above questions.\\n\\n[1] Dataset Distillation\\n\\n[2] Remember the Past: Distilling Datasets into Addressable Memories for Neural Networks\", \"Questions\": \"See above.\"}"}]}
{"paper_id": "Zw8YxUWL4R", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Text-to-image generation models have significantly advanced image synthesis, allowing the creation of diverse images from textual descriptions. These models often use diffusion processes that involve multiple cross-attention layers in a U-Net architecture, conditioning on a single text prompt embedding across all layers.\nGap: Current text-to-image models typically condition image generation on a single text prompt embedding, which limits the expressiveness and control over specific aspects of the synthesized image's structure and appearance.\nHypothesis: The introduction of an Extended Textual Conditioning space, referred to as P+, that uses multiple per-layer tokens for conditioning in the cross-attention layers of a text-to-image diffusion model can provide greater control and expressiveness in the synthesis process.\nMethod: The authors propose a new conditioning space, P+, where each cross-attention layer in the U-Net is conditioned on its specific textual token. They also introduce Extended Textual Inversion (XTI), which extends Textual Inversion by using a set of per-layer tokens to represent a subject. The authors conducted a series of experiments using a publicly available Stable Diffusion model, analyzing cross-attention mechanisms and evaluating XTI against traditional Textual Inversion (TI) and DreamBooth methods.\nConclusion: The P+ space was found to be more expressive and provided better control over the synthesized images by disentangling shape and appearance in different layers. XTI showed improved reconstruction quality and editability, converging faster than TI. The method also enabled novel results in object-style mixing, suggesting potential for broad applications in personalized and creative image synthesis.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"- The paper is well written and easy to follow, with sufficient literature references and reasonable and intuitive design in terms of extended controlling space.\\n\\n- The findings illustrated in Fig. 3 in terms of per-layer prompting is interesting.\", \"Weaknesses\": \"- The XTI section is a bit confusing, the proposed XTI reconstruction loss (also, this equation has no numbering which makes it difficult to refer to) seems to be a stepwise loss, which means the extended space is constructed/optimized at every diffusion step of T2I models?\\n\\n- Following the first point, while this operation is intuitive as many existing editing methods do follow the step-wise iterative paradigm, it is worth doing some ablations/analytical experiments on this particular operation, as recent works [a] have shown that single-step operation is sufficient to achieve semantic modification for diffusion models with more theoretical justifications. \\n\\n- Following the previous two points, for the simple experiments illustrated in Fig.3, does the textual conditioning are introduced at every diffusion step? \\n\\n- While the work seems to be technically solid and shows interesting results in downstream applications, most of the findings are empirical-driven, and I find it relatively difficult to interpret the proposed findings from theoretical perspectives. Do the authors have any insights on potential rationales for the layer-wise differences in U-Net on the denoising results? \\n\\n[a] Boundary Guided Learning-Free Semantic Control with Diffusion Models, NeurIPS 2023.\", \"Questions\": \"Please see the weaknesses for details.\\n\\nOverall, I feel the step-wise operation is worth further investigating. To be more specific, my concern is that the statement/the proposed empirical finding “the coarse inner layers of U-Net affecting the shape of the generated image, and the outer layers affecting the style and appearance” may be inaccurate, which is more likely to be a fact of diffusion steps rather than the U-Net architecture.\"}"}]}
{"paper_id": "3NXhwkZGjz", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The field of domain adaptation, specifically Unsupervised Domain Adaptation (UDA), strives to address the challenge when the distribution of training and testing data varies, which is known as domain shift. UDA enables transferring knowledge from labeled source domains to unlabeled target domains, thereby reducing the dependency on annotated data in the target domain. Recent approaches have focused on source-free UDA, where only a pre-trained model from the source domain and unlabeled target domain data are used, addressing privacy concerns related to accessing labeled source data.\nGap: The primary challenge in Source-Free Unsupervised Domain Adaptation (SFUDA) is generating reliable supervision from unlabeled target data alone. The accuracy of initial label predictions can be poor due to the domain shift, and using these inaccuracies for model adaptation can yield misleading outcomes.\nHypothesis: The authors propose that considering multiple prediction hypotheses for each sample and analyzing the rationale behind each can help identify the most likely correct hypotheses. This reliable set of hypotheses can serve as pseudo-labeled data to enhance a semi-supervised learning process for domain adaptation without relying on source data or target labels.\nMethod: The authors introduce a three-step adaptation process: 1) Model Pre-Adaptation: To reduce domain shift by promoting smooth predictions using a newly constructed memory of image features and their nearest/furthest neighbors. 2) Hypothesis Consolidation: Multiple hypotheses per sample are created, with rationale representations derived from GradCAM. By comparing these to class-wise rationale centroids, the most reliable hypothesis (based on close proximity to rationale centroids and lack of conflicting rationales) is selected. 3) Semi-Supervised Learning: Utilizing the reliable pseudo-labels obtained previously in the standard FixMatch semi-supervised learning algorithm for model adaptation, which uses consistency regularization and pseudo-labeling.\nConclusion: The proposed method demonstrates state-of-the-art performance in SFUDA tasks, effectively transforming this domain adaptation problem into a semi-supervised learning problem. By consolidating hypothesis rationales, the approach enhances the pseudo-label reliability, leading to improved training outcomes. The method's flexibility allows it to be integrated into existing approaches, thereby bolstering their adaptation performance without requiring access to source data.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"- To the best of my own knowledge, this paper is the first attempt to use multiple source hypotheses in SFUDA and to transform SFUDA into a semi-supervised learning task. They provide a novel view and framework for SFUDA.\", \"Weaknesses\": \"- The motivation of the multiple hypotheses setting is not clear. These hypotheses are pre-trained on the same domain instead of multiple domains like previous works. Is this setting common in practice? I think that multiple high-quality hypotheses on one specific domain are not easy to access. It seems that repeatedly training these hypotheses is too wasteful. Overall, the motivation should be further clarified and providing several practical cases will be better.\\n- Assume the above issue can be explained, then how to choose the best $\\\\tilde{k}$ for another task? $\\\\tilde{k}=4$ seems to be acquired from DomainNet-126 dataset, but this hyper-parameter may be not suitable for others. This issue also exists for the choice of $\\\\tau_1$ and $\\\\tau_2$.\\n- The Eq. (4) seems to be wrong. For the latter item, $\\\\hat{y}_u$ is an integer but $p(\\\\mathcal{A}_s(x_u))$ is a vector.\\n- The performance of the proposed method is not SOTA on some sub-tasks.\", \"Questions\": \"- According to Table 5, more hypotheses will degrade the method's performance. Intuitively, more hypotheses should lead to better performance, because more candidates have a greater chance of getting the true pseudo-labels. Can you explain this strange phenomenon?\"}"}]}
{"paper_id": "Fn655mJ4bv", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Structured output models are used in many applications to map input vectors to high-dimensional output spaces, like image segmentation and multi-label classification. These models leverage dependencies among output variables for better prediction accuracy. Methods like graphical models use energy functions for inference, but interpreting these models is complex due to their non-linear and intricate nature.\nGap: Existing interpretation methods for structured output models, such as model-based and post-hoc techniques, often overlook correlations among output variables, leading to potentially suboptimal explanations. Interpretations are challenged by the complex interactions present in the computational paths of these models.\nHypothesis: Incorporating correlations among output variables when interpreting a structured output model can lead to higher-quality explanations. A globally-trained local interpreter can effectively identify important features affecting a target output variable by considering structural dependencies during its training process.\nMethod: The SOInter method introduces a deep energy-based interpreter trained to explain structured output models by focusing on a single output variable as a target. The interpreter function is trained using an energy-based approach that accounts for correlations between output variables. The method relies on a two-phase energy network training: pre-training using existing techniques and fine-tuning alongside interpreter training. The interpreter uses the Gumbel-Softmax trick to select important features and evaluates performance via a custom loss function comparing energy values of predicted outputs.\nConclusion: SOInter demonstrated effectiveness in interpreting structured output models, outperforming existing techniques like Lime and Kernel-Shap in experiments with synthetic and real datasets. It integrates structural information during training, resulting in improved explanation quality, especially when correlations between output variables are significant. The method maintains computational efficiency and scalability, making it suitable for applications with large input features.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"Strengths: \\nOverall, the paper is well written and reads easily. \\nThis work presents one of the first approaches to post-hoc interpretation of structured output prediction. The proposed approach applies when the output to be predicted is a binary vector which includes a broad variety of tasks like multi-label classification, semantic segmentation... Any structured output method that predicts a bag of items for instance (bag of substructure) will be also eligible (even if not considered in this work, except for text).\\nThe optimization problem solved to learn the interpreter is appealing with a nice way to rely on the difference of energies associated to the perturbation of inputs by the interpreter. This is really the strong novelty of the paper, for me far beyond its application to structured output prediction.\", \"Weaknesses\": \"*The paper takes a specific angle to intepretability of structured output prediciton, by considering the input features as tabular data. When dealing with images at least, identifying \\\"independently\\\" the important pixels involved in the prediction is not what I expect from explainability.  I would be interesting for raw data like images by identifying a region in the image or a concept as a function of the input space as an explanation. I think a discussion here is expected. \\n\\n*The learning algorithm is not sufficiently well documented and I have questions about its robustness against the choice of hyperpameter : is it robust to tau ? how does the learning algorithm react if we change k ? do we obtain close results if change k by k-1 or k+1 for instance ? What the impact of these parameters on the final \\\"explainability\\\"\", \"Questions\": \"Please see questions above as well.\\n\\n1) Behaviour of the learning algorithm (see previous remark).\\nI would like to have more insights about the behaviour of the learning algorithm - I would like to see a study about the robustness of leanring when varying k. \\n\\n2)   Is it possible to incorporate in SOInter a way to encourage the identification of correlated input features for instance by taking into account the relationship between features ?\\n3) Did you study the robustness of your approach against noise in test images ?\\n4) on text multi-label classification you proposed as an evaluation metrics the post-hoc F1 and the relative F1\\nPlease re-formulate more clearly the relative F1 (I think there is a typo).\\nThe deceptive results may be due to the nature of  input text representation: even the words that are not considered as important by the interpreter can help to give some context and improve the performance. Can you comment on that ?\"}"}]}
{"paper_id": "VyMW4YZfw7", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses semi-supervised node classification (SSNC), which is a central task in graph-based learning where predictions are made on network data using features and network structure. Current solutions often involve complex Graph Neural Networks (GNNs) to exploit this structure.\nGap: Many modern GNN architectures are complex and may be more sophisticated than necessary for tasks like SSNC. Recent advances in GNNs might be partially attributed to changes in evaluation standards rather than genuinely improved models.\nHypothesis: Simpler, traditional nonparametric techniques applied in the spectral domain can replace many existing complex GNN designs while maintaining or surpassing state-of-the-art (SOTA) performance on SSNC benchmarks.\nMethod: The authors propose using classical nonparametric methods in the spectral domain to construct simpler GNN models. They implement a one-layer linear GNN model using different kernel functions and low-rank approximations of the adjacency matrix to create learnable propagation structures. The experiments evaluate these simplified models on common SSNC benchmarks like Cora, Citeseer, Pubmed, and others, comparing performance against existing SOTA models.\nConclusion: The proposed simpler spectral methods can achieve competitive performance compared to more complex GNN architectures. The absence of post-model augmentations in their method allows for cleaner theoretical analysis. The study also highlights how changes in evaluation conventions, such as dataset splits, can impact perceived model performance, suggesting that some improvements may not be purely due to innovative models.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 1, \"Confidence\": 4, \"Strengths\": \"Addressing the use of simpler methods to match and outperform complex DNN-based architectures.\", \"Weaknesses\": \"Lack of Novelty: The paper does not present significant novelty. While it challenges the use of complex GNNs with spectral approaches, it falls short of providing a strong theoretical analysis or thorough experimental validation to support its claims. To enhance the paper's contribution, it should either include a rigorous theoretical analysis or conduct additional comprehensive experiments, preferably in supplementary material.\\n\\nLimited Experiments: The paper contains a limited number of experiments, which is insufficient given the broad claims made. A supplementary material section should be added to provide a more thorough evaluation, including comparisons with existing methods, diverse datasets, and various hyperparameter settings.\", \"Questions\": \"- What motivated your choice of the spectral truncation factor and the choice of kernel functions? Did you conduct any sensitivity analysis on these choices?\\n\\n- Have you considered comparing your proposed approach with existing GNN architectures on a broader set of benchmark datasets to ensure the generality of your claims? The paper lacks an extensive experimental investigation on multiple benchmarks and algorithms with several ablation studies regarding the claim of the paper? Or rather a theoretical study of the phenomenon observed.\\n\\n- Could you elaborate on the limitations and assumptions of your method? Are there scenarios where your approach may not be as effective, and if so, what are they?\\n\\n- There are multiple typos in the paper. Could you please proofread?\"}"}]}
{"paper_id": "09xFexjhqE", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Fine-tuning pre-trained model feature extractors is an effective strategy to achieve higher performance in downstream tasks, evidenced by advances such as GPT-3 achieving high GLUE benchmark scores. Robust Fine-Tuning (RFT) specifically aims at enhancing adversarial robustness, crucial for applications in safety-critical areas like medicine and autonomous vehicles.\nGap: Current RFT methods face significant issues with optimizing both adversarial and natural objectives through the same feature extractor, leading to divergent gradient directions. This divergence results in instability and high sensitivity to hyperparameters, limiting the effectiveness of RFT in achieving adversarial robustness.\nHypothesis: Introducing a low-rank branch to separate the optimization of natural and adversarial objectives can address the issue of gradient divergence in RFT, leading to more stable optimization and enhanced adversarial robustness without the need for extensive hyperparameter tuning.\nMethod: The proposed method, AutoLoRa, uses a low-rank (LoRa) branch to partition RFT into two pathways: the natural objectives are optimized via the LoRa branch, while the adversarial objectives are optimized using the feature extractor. The paper also introduces heuristic strategies for automatically scheduling learning rates and loss term scalars to enhance convergence and reduce hyperparameter sensitivity. Extensive experiments using ResNet models and various datasets demonstrated the effectiveness of this approach.\nConclusion: AutoLoRa effectively disentangles adversarial and natural optimization objectives in RFT, achieving state-of-the-art adversarial robustness across various tasks while alleviating sensitivity to hyperparameters. This approach provides a practical, automated solution for converting pre-trained feature extractors to adversarially robust models, validated by empirical results showing improvements over existing methods.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"**Strength:**\\n\\n-   The motivation that optimizing both adversarial and natural objectives through feature extractor yields divergent gradient directions make sense. The proposed disentangling of the training objective by introducing the LoRA branch is consistent with the motivation.\\n-   Extensive empirical evaluation (including the P-test) demonstrates the improvements in Robust Fine-Tuning on various tasks.\", \"Weaknesses\": \"**Weakness:**\\n\\n-   The reason for automating scheduling hyper-parameters is not well illustrated, and the ablation study in Table 5 can not show its superiority, especially for the *RA* metric.\\n-   In Formula 7, the constant factor $6$ is not well-explained, and it could be considered as a hyper-parameter with further ablation study.\\n-   Table 4 is confusing. Specifically, when the adversarial budget is set to $8$ which is the default configuration, the resulting metric is supposed to be that in Table 2. However, this is not true.\\n-   Typo in Section 5.2 Ablation Study, the end of adversarial budgets paragraph says \\\"consistently achieves consistently\\\".\\n-   Diverse network backbone architectures are encouraged to be considered beyond ResNet.\", \"Questions\": \"Refer to the weakness section.\"}"}]}
{"paper_id": "nmBjBZoySX", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Graph Neural Networks (GNNs) have gained prominence for representation learning on graph-structured data due to their superior performance in various tasks. Advancements in GNNs focus on simplifying graph structures and compressing GNN models to mitigate high computational costs associated with large graphs. A recent trend inspired by the Lottery Ticket Hypothesis posits that within GNNs, there exist sparse subnetworks that can perform as well as full networks, termed Graph Lottery Tickets (GLT).\nGap: Existing GLT methods rely excessively on trial-and-error for pruning rate tuning, lack flexibility due to irreversible pruning paradigms, and face scalability issues when applied to deep GNNs, as they assume static topology across layers, limiting their applicability in larger and deeper GNN scenarios.\nHypothesis: The paper proposes an Adaptive, Dynamic, and Automated framework for identifying Graph Lottery Tickets ($\\text{AdaGLT}$) that aims to address existing limitations by introducing layer-adaptive sparse structures, integrating pruning with training for dynamic workflows, and automatically capturing graph lottery tickets across different sparsity levels without extensive tuning.\nMethod: $\\text{AdaGLT}$ method includes: 1) Layer-adaptive sparsification, allowing different sparse structures for each layer to facilitate deeper GNNs. 2) Integrating pruning and training processes for dynamic workflows that include both pruning and potential restoration. 3) Automated pruning scheduling which obviates the need for manual configuration of pruning ratios. Theoretical proofs ensure that AdaGLT mitigates over-smoothing and achieves improved sparse structures in deep GNN settings. Extensive experiments demonstrate AdaGLT's superiority over competitors across multiple datasets and settings, particularly in deep GNN scenarios.\nConclusion: AdaGLT effectively mitigates the limitations of current GLT approaches by providing a scalable, automated, and dynamic method for identifying sparse structures within GNNs. It outperforms state-of-the-art methods in efficiency and adaptability, demonstrating the ability to uncover GLT with significant graph and weight sparsity in variety of contexts including deeper GNNs.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"1. The paper is well-written and organized. As compared to previous UGS, the proposed AdaGLT jointly sparsifies both graphs and GNN weights and adopts an adaptive layer sparsification process. In addition, it offers the practitioner the chance to adopt automatic pruning and dynamic restoration for extracting the graph lottery tickets.\\n\\n2. The effective combination of automation, layer adaptivity, and dynamic pruning/restoration provides better properties as compared to previous methods.\", \"Weaknesses\": \"Given the contribution of UGS and other series of graph lottery tickets work, this automated tool sounds like incremental work. However, the analysis of scalability issues when applied to deep GNNs is worth reading.\\n\\nMost experiments are conducted on small and transductive graph/tasks. The ogbn-arxiv is a small graph and ogbn-protein is a medium graph actually but are claimed as large graphs, more reasonablely large graphs should be considered. Also, how about inductive settings like GraphSAGE or other SOTA ones?\\n\\nOther than the above aspects, I think the other perspectives of this paper are clear.\", \"Questions\": \"See weaknesses. I wonder whether the contribution applied to an inductive setting or evolving graphs with increasing nodes or edges.\"}"}]}
{"paper_id": "di52zR8xgf", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Generative models have witnessed significant advancements across multiple data domains, especially text-to-image synthesis with the development of models like Stable Diffusion, which has been foundational for applications like image personalization, controllable editing, and others. However, these developments are often outpaced by black-box models whose opacity raises challenges for reproducibility and validation.\nGap: Despite the widespread adoption of models like Stable Diffusion, they are often not competitive with state-of-the-art but opaque black-box models. There is a need for an open model that can offer competitive performance with these state-of-the-art models, providing a transparent alternative for the community.\nHypothesis: Enhancing the Stable Diffusion model with a larger architecture, novel conditioning schemes, and a refinement stage can significantly improve its performance, making it competitive with state-of-the-art black-box models while maintaining openness and transparency.\nMethod: The paper introduces Stable Diffusion XL (SDXL), which includes a three times larger UNet backbone than previous versions, enhancements in the form of multiple attention blocks, and a second text encoder. Novel conditioning schemes, such as size and crop micro-conditioning, are introduced to improve training on varying aspect ratios. A separate refinement model is used for improving the visual fidelity post-processing of generated images while maintaining cohesion with the original model’s architecture. SDXL was trained on a diverse dataset including multiple aspect ratios with an advanced autoencoder to enhance semantic composition and high-frequency details.\nConclusion: SDXL significantly outperforms previous versions of Stable Diffusion and is competitive with state-of-the-art black-box models like Midjourney. It offers an open model alternative, enhancing reproducibility and transparency. The model shows improvements in image quality and adherence to text prompts, although it still suggests future work in areas like single-stage generation and incorporation of additional text synthesis improvements.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 3, \"Strengths\": \"1. Very sound elaboration and experiments. The proposed improvement is reasonable and effective.\\n2. The method is open sourced.\", \"Weaknesses\": \"1. Lacks some comparison with SoTA pixel-space diffusion model such as Simple Diffusion from Google.\\n2.  Also, SDXL still relied on two-stage training where a high-quality encoder is required. I wonder whether the two stage can be combined and lead to further performance boost?\", \"Questions\": \"1. As discussed in your future work section, can SD be combined into single-stage training and further boost the performance?\\n2. Can similar training pipeline introduced in this paper be applied to 3D diffusion training, and what are the challenges?\"}"}]}
{"paper_id": "0b328CMwn1", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses visual prompting (VP), a method developed to repurpose large pre-trained models for downstream vision tasks without altering the model parameters. VP introduces a universal perturbation into the input data to facilitate task-specific tuning. It is positioned among parameter-efficient fine-tuning (PEFT) techniques, which aim to reduce the computational cost and parameters required to adapt pretrained models for new tasks.\nGap: Although VP provides a novel way to use pre-trained models, there is a performance gap between VP and conventional fine-tuning methods. This gap reveals an unexplored area in both theory and practice that requires understanding and advancement to improve the performance and efficiency of VP beyond its current limitations.\nHypothesis: The authors propose that activation prompts (AP), which extend VP by introducing perturbations at the intermediate activation map layers instead of solely at the input layer, can enhance the performance and efficiency of VP. AP connects naturally to normalization tuning and can potentially outperform VP and other PEFT methods.\nMethod: The authors introduce AP, which applies task-specific perturbations to activation maps at intermediate layers of pre-trained models. This approach is analyzed theoretically in terms of its connection to normalization tuning techniques like batch normalization in CNNs and layer normalization in ViTs. The paper conducts extensive experiments over 29 datasets and compares AP with existing VP and PEFT techniques, analyzing the performance across various datasets and architectures. The experiments explore how AP affects efficiency metrics such as memory usage, time consumption, and accuracy.\nConclusion: The study concludes that AP surpasses VP in accuracy and efficiency and that the choice of the layer depth for AP implementation significantly affects performance in CNNs and ViTs. This new prompting strategy demonstrates reduced memory, time requirements, and improved accuracy compared to state-of-the-art PEFT methods, supporting its potential to close the performance gap previously exhibited by VP.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"1. The study of applying AP to different layers provides interest insights related to model interworks such as attention distance.\\n2. Experimental results are presented with many datasets, settings, and compared with various benchmarks.\", \"Weaknesses\": \"1. Novelty given extensive studies in the VPT paper. The VPT paper studies various different ways of applying the prompt. For example, in Figure 5, they studied the option of adding tokens instead of prepending tokens and observed slightly worse results, which is almost verified in Table 4 of this paper. In Fig. 14 of VPT, they also studied adding prompts in different depths of the network. These previous work almost makes this paper a special case in the search space covered by VPT.\\n2. Fair comparison with existing works such as VPT. The main results are reported all in different settings (e.g. different backbone, patch size, pretraining, etc.) that prevent direct comparison with numbers reported in the VPT paper, although this work outperforms basic baselines such as VP and norm-tune.\\n3. Soundness of the parameter counts in tables. For example, in Table 2 with ViT-L and CIFAR-10, how does LINEAR-PROBE have 1M parameters instead of 10K (1024 * 10)? In addition, if other methods (e.g. norm-tune and AP) also train a linear layer on top of the features, then this should be made clear as well. The current reported counts are significantly higher than expected (also see table 5 of VPT).\\n4. Limited gain of efficiency-accuracy trade-off compared with other methods in Table 4 such as LORA etc. The main advantage of AP is the lower cost but other methods are configured to be more expensive than they should be. e.g. this paper experiments with VPT-DEEP with 50 tokens, but (1) VPT also presents VPT-shallow as a more efficient option, (2) VPT by default searches over a set of prompt lengths and in their table 13, a much smaller average token length is needed, e.g. 10 or 1, (3) VPT applied to a deep ViT-L will put VPT in a disadvantage but this could be easily fixed by prompting a subset of layers. As a result, it is not clear where this paper stands in the extensive ablations of VPT.\", \"Questions\": \"As suggested in the weakness section, numbers directly comparable with those reported in VPT paper will facilitate fair comparison. Please see the weakness section for more details.\"}"}]}
{"paper_id": "AMDKqZcZbi", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper addresses the challenge in machine learning where models struggle to adapt to new tasks quickly while retaining proficiency on previously learned tasks. This issue, known as catastrophic forgetting, is notably absent in biological systems where animals display remarkable abilities to learn and remember through specialized neural architectures such as the entorhinal-hippocampal circuit. The authors draw parallels to these biological systems to propose a solution for rapid, continual learning in machine learning models.\nGap: Existing machine learning models often excel at either quick adaptation (rapid learning) or maintaining knowledge over time (continual learning) but rarely achieve both. This limitation constrains their ability to tackle tasks that require the combination of these abilities, leading to a phenomenon called catastrophic forgetting.\nHypothesis: The hypothesis is that a biologically inspired neural architecture, combining a content-addressable heteroassociative memory system reminiscent of the entorhinal-hippocampal circuit with a spatially invariant convolutional network architecture, can achieve both rapid learning and retention of previously acquired knowledge, thus avoiding catastrophic forgetting.\nMethod: The authors propose a task, the sequential Morris Water Maze (sWM), to evaluate their model's performance. The model comprises a structured neural architecture called MESH, which includes a content-addressable memory system mimicking the hippocampal circuit. Observations are mapped to grid cell patterns, processed by a fixed convolutional neural network (CNN) to create spatially invariant representations, and associated with actions using an attention mechanism. The system is tested for its ability to learn rapidly in new environments and retain knowledge across multiple settings, addressing both intra-environment adaptation and inter-environment retention.\nConclusion: The proposed model outperforms several baseline methods in both continual and few-shot learning contexts. It successfully learns new environments quickly without forgetting past environments, showcasing the potential advantage of incorporating biologically inspired architectures into machine learning models for tasks requiring both rapid learning and memory retention.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"The new benchmark is a valuable contribution to the continual learning community. The method has a strong neuroscientific grounding and it brings together existing components in an original way. The paper is well presented and nicely structured. The writing is clear and the figures are very helpful in conveying the main points of the argument. The ablation study provides sufficient justification for the individual design choices.\", \"Weaknesses\": \"The empirical evaluation is the main weakness of the paper. The authors compare their method to only two continual learning baselines, both of which are quite old. In addition, the replay buffer sizes that are used in the experiments are rather small (200-1800) The proposed method seems to be custom-designed for the navigation task, so while it can serve as a model of how rodents learn to navigate in new environments, it is not a practical continual learning method that could be applied to an arbitrary task. To give existing baselines a fighting chance, I would recommend having two separate networks: one mapping observations and goal position into some latent representation and another mapping these to actions. In the first environment, train both networks. For each new environment, re-train only the first network.\", \"Questions\": \"Why does replay buffer exhibit such poor performance on the last task?\\n\\nWill the benchmark be made available? Is it a framework to produce random environments or just a fixed dataset?\\n\\nAre the associations between displacement representations and actions simply memorised by the attention module?\\n\\nIs the grid code of the goal location available to your method straight away?\\n\\nHow is the goal location provided to the network for replay and naive methods?\\n\\nWhat exactly is stored in the rehearsal buffer? Observation-action pairs?\\n\\nHave you tried increasing the grid size of the environment?\"}"}]}
{"paper_id": "am7BPV3Cwo", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Out-of-distribution (OOD) detection is crucial for neural networks to avoid unreliable predictions by identifying and rejecting unknown samples during deployment. Traditional OOD detection methods are usually evaluated with category-balanced in-distribution (ID) data, but in real-world applications, data often show imbalanced distribution which affects the performance of such detectors.\nGap: Existing OOD detectors struggle to maintain performance on imbalanced ID data, often misidentifying tail class ID samples as OOD and mistaking OOD samples as head class ID samples. This is due to the inherent bias in data distribution, which has not been thoroughly addressed by current methods.\nHypothesis: The authors propose that the performance gap between balanced and imbalanced OOD detection is due to a class-aware bias. By understanding this bias, it is possible to improve OOD detection methods on imbalanced data distributions through a combination of post-hoc normalization and training-time regularization techniques.\nMethod: The authors introduce a statistical framework, ImOOD, which extends OOD detection to imbalanced data distributions by using post-hoc normalization and training-time regularization. Post-hoc normalization adjusts detection probabilities using ID classification information, while training-time regularization involves a loss function that encourages the separation of tail ID classes and OOD samples and prevents OOD samples from being classified as head ID classes. This method is applied and tested on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT benchmarks, showing significant improvements over state-of-the-art methods.\nConclusion: The ImOOD framework was shown to significantly enhance OOD detection performance on imbalanced datasets by consistently outperforming existing methods on benchmarks. By providing a unified perspective on handling class-aware biases, the method effectively reduces errors associated with imbalanced data, offering a robust solution for practical OOD detection challenges.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"- The statistical analysis performed in the paper is sound and written in a manner that is easy to follow. The authors include a helpful intuitive understanding of the bias term that connects it back to the empirical observations about the behavior of OOD detection models under the class-imbalanced setting.\\n- ImOOD is well-motivated and theoretically-grounded on the the above analysis. The general thinking behind the method is sound, and for the most part, it’s described clearly enough to understand and re-implement the design.\", \"Weaknesses\": \"- I believe the major drawback of this paper lies in the limited scope of the studied problem. It appears that the authors assume that the class-imbalance problem only exists on the original ID dataset. I am not too convinced with the practicality of the proposed scenario; it’s hard to imagine in what deployment scenarios only the ID dataset will suffer from this problem, while the OOD dataset(s) used for training/testing is void of it. Wouldn’t it be more natural to assume that the ID training dataset can be refined in advance to make it class-balanced, while the OOD data the detector must be able to identify arise in various forms, and thus is more likely to be class-imbalanced?\\n    - What if the auxiliary OOD dataset used for training and/or the target OOD dataset exhibits class-imbalance as well? I think it is important to study various combinations of class-balanced and -imbalanced ID/auxiliary OOD/target OOD datasets. Does the analysis performed in the paper extend to and hold in such settings? Can ImOOD still outperform other baselines?\\n- Please correct me if I am wrong, but it appears that the verification of post-hoc normalization (minus the train-time regularization) on larger datasets appears to be missing. Even on CIFAR10-LT, the improvement from post-hoc normalization seems marginal, but this concern could be alleviated, as long as the improvement is consistent across various datasets. If post-hoc normalization is not quite as effective on datasets, it would signify that ImOOD is heavily reliant on the train-time regularization.\\n- More details on how the label distribution is learned (during test-time regularization) would be appreciated. Also, do you use the same learned label distribution when performing post-hoc normalization? Or is it used only for training-time regularization? If you discard it after training, one would have to know the label distribution anyways for post-hoc normalization, so do we really need to use the learned distribution in the first place?\\n- The empirical validation is limited to one target OOD dataset per InD dataset. Validation on more challenging OOD datasets (e.g., near OOD data, data with spurious correlation) would be helpful to gauge the effectiveness of the proposed method.\", \"Questions\": \"Please refer to the Weaknesses section.\"}"}]}
{"paper_id": "R1crLHQ4kf", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Automatic Speech Recognition (ASR) systems are vulnerable to adversarial attacks which can mislead them into producing incorrect outputs by adding imperceptible noise to the audio input. These adversarial examples pose significant security threats, such as over-the-air attacks on commercial devices, where malicious inputs can lead to incorrect system actions. Current ASR systems, often based on deep learning, do not possess inherent robust defenses against these attacks.\nGap: Existing methods to counter adversarial attacks either focus on improving model robustness or detecting the presence of adversarial examples. Many of these approaches rely on modifying input data or adversarial training, which can be computationally expensive and sometimes ineffective when attackers adapt their methods. There is a need for an efficient detection mechanism that can work across varying ASR systems and is not susceptible to adaptive attacks.\nHypothesis: The authors propose that by analyzing the probability distribution of output tokens generated by an ASR system, it is possible to detect adversarial examples efficiently. The distributions at each time step are expected to exhibit certain statistical characteristics that can help distinguish between benign and adversarial inputs.\nMethod: The method involves measuring characteristics of the probability distribution over output tokens in each time step. These characteristics include the median, maximum, and minimum of output probabilities, entropy, and divergences such as Kullback-Leibler and Jensen-Shannon divergence between time steps. Binary classifiers, including Gaussian classifiers, ensembles, and neural networks, are then used to differentiate between benign and adversarial examples. The method is tested across multiple ASR systems and datasets to ensure generality, and adaptive attacks are formulated to test the robustness of the detection strategy.\nConclusion: The proposed detection approach achieves high accuracy, with AUROCs exceeding 99% for distinguishing adversarial from benign data in various ASR models. Although adaptive attacks can reduce detection accuracy slightly, they become more apparent and easier to filter due to increased noise levels. This approach provides a robust means of leveraging the statistical properties of output distributions to effectively differentiate between normal and adversarial inputs, offering a new defensive layer for ASR systems.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"1. This paper proposes a simple and effective method for detection adversarial examples for ASR systems.\\n2. The paper is presented with comprehensive experiments. The authors not only present the benign detection performance of adversarial attacks but also analysis the robustness under adaptive attacks with known detection method.\", \"Weaknesses\": \"1. Detection performance is only evaluated on limited adversarial attack methods. The authors only evaluate their method on C&W attack and Psychoacoustic attack. More attack methods like [1] [2] and even some black box methods like FAKEBOB [3] are still needed to be included to prove the general performance of the detection method.\\n2. Lack of comparison with other audio adversarial example detection methods like [4].\\n\\n[1] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations, 2018.\\n\\n[2] Alzantot, Moustafa, Bharathan Balaji, and Mani Srivastava. Did you hear that? adversarial examples against automatic speech recognition. arXiv preprint arXiv:1801.00554 (2018).\\n\\n[3] Guangke Chen, Zhe Zhao, Fu Song, Sen Chen, Lingling Fan, Feng Wang, and Jiashui Wang. To\\u0002wards understanding and mitigating audio adversarial examples for speaker recognition. IEEE Transactions on Dependable and Secure Computing, 2022.\\n\\n[4] Rajaratnam, Krishan, and Jugal Kalita. Noise flooding for detecting audio adversarial examples against automatic speech recognition. In 2018 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT), pp. 197-201. IEEE, 2018.\", \"Questions\": \"In Section5.1, four guiding principles are provided on selection process. Do such principles limit the adversarial attack implementation? For example, there should be an equal number of tokens in both the original and target transcriptions. Would there be other attack scenarios like tokens insertion or deletion?\"}"}]}
{"paper_id": "i7P2mK3x3o", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Flow-based models are prominent in generative tasks, especially normalizing flows, where neural networks are used to transport a data distribution to a normal distribution. These models have broad applications in density estimation, generative modeling, and likelihood inference. Typical models rely on applying a neural ODE framework to achieve continuous transformation in high-dimensional spaces.\nGap: The existing flow-based models predominantly focus on transport from a data distribution to a normal distribution. However, there is a gap in the ability to transport between arbitrary distributions P and Q, particularly when both are accessible only through finite samples, without resorting to the normal distribution as an intermediary.\nHypothesis: The paper hypothesizes that a flow-based neural ODE model can be developed to compute a dynamic optimal transport map directly between two general distributions P and Q in high-dimensional space, by minimizing transport costs and using empirical samples from both distributions.\nMethod: The authors propose Q-flow, a flow-based neural ODE model that learns an invertible optimal transport map between distributions P and Q. This is achieved by minimizing the transport cost and using two sets of samples from P and Q. The model uses a neural ODE framework to train the flow network and refine it through an iterative process. The transport map is designed to satisfy the continuity equation and minimize a combination of KL divergence and Wasserstein-2 transport cost. The model is demonstrated on tasks like density ratio estimation and image-to-image translation, capitalizing on its continuous-time formulation and invertibility.\nConclusion: The method shows strong empirical performance in high-dimensional data applications, such as mutual information estimation, energy-based generative modeling, and image-to-image translation. The Q-flow model manages to efficiently and accurately learn the optimal transport between samples of different distributions, facilitating downstream tasks and potentially serving as a new tool for generative models and distribution interpolation.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"- sufficiently clearly written paper\\n- natural idea of the algorithm\\n- detailed description of the algorithm and experimental study \\n- discussion of the features of the computational implementation of the algorithm\\n- interesting practical results\", \"Weaknesses\": \"- the title of the paper is \\\"Computing high-dimensional optimal transport by flow neural networks\\\". However, a significant part of the paper is devoted to benchmarking of the capability of the algorithm to perform density ratio estimation. So it is not clear what is the main aim of the paper - to compute OT, or to estimate log density ratio\\n\\n- If the main aim is DRE, then it is necessary to provide detailed comparison with other DRE methods, as there are many papers on this topic. E.g., what is the difference of the proposed approach with the approach https://openreview.net/forum?id=kOIaB1hzaLe\\n\\n- it is not clear why the proposed algorithm estimates optimal transport\\n\\n- experimental results to verify efficiency of computed W2 high-dimensional optimal transport are not enough to claim accuracy and efficiency of the proposed approach. E.g. the authors consider some image translation tasks, but FID score used to characterise accuracy does not guarantee that the computed W2 high-dimensional optimal transport map is accurate.\", \"Questions\": \"- it is not clear how to tune a value of gamma in (3). Any recipes for automatic tuning?\\n\\n- page 9: \\\"Meanwhile, since our Q-flow model learns a continuous transport map from source to target domains, it directly provides the gradual interpolation between the source and target samples along the dynamic OT trajectory as depicted in Figure 4b.\\\"\\n\\nAny comments on why the trajectory corresponds to OT trajectory? To construct a flow the authors optimise (3), which contains two terms, and it is not clear why such optimisation formulation guarantees any optimality or that the mapped distribution coincides with the second distribution q. \\n\\n- since the authors claim they compute W2 optimal transport, it is important to benchmark their approach on problems with ground truth solutions. There exist such benchmark, see https://github.com/iamalexkorotin/Wasserstein2Benchmark (Do Neural Optimal Transport Solvers Work? A Continuous Wasserstein-2 Benchmark, NeurIPS 2021)\\n\\n- page 18 (the second line after the displayed formula 15): Why Q = N(0,I_d) if P = N(0,Sigma)?\\n\\n- page 19: it is not clear how gamma = 0.5 was selected. Why not 0.6?\"}"}]}
{"paper_id": "YIls9HEa52", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper addresses the dimensionality reduction of neural activity and behavior using unsupervised methods, particularly focusing on the recurrent switching linear dynamical system (rSLDS). This model is known for its ability to describe the latent dynamics of neural spike train data through discrete switches between a finite number of linear dynamical systems. However, traditional Hidden Markov Models (HMM) on which these models are based often come with limitations such as a fixed number of states and lack an inherent geometry over states.\nGap: The current rSLDS models suffer from limitations including a fixed number of discrete states across trials and a lack of latent structure or organization of these states, which limits their applicability to trial-varying data in neural analysis. Additionally, existing hierarchical Dirichlet processes can handle flexible state cardinality but do not support recurrence or other dependencies between discrete states allocations.\nHypothesis: The authors propose an infinite recurrent switching linear dynamical system (irSLDS) model that integrates a distance-dependent Chinese restaurant process (dist-CRP) with latent geometry and flexible state cardinality. This model is designed to overcome the stated limitations by allowing for a semi-Markov state process and capturing recurrent dependencies in the state transitions, enriched by a PDE framework for efficient computational inference.\nMethod: The proposed irSLDS model introduces a semi-Markov discrete state process leveraging properties of the distance-dependent CRP. A partial differential equation (PDE) approach is employed to create a semi-parametric formulation for dynamical sufficient statistics for discrete states. The irSLDS model is validated on synthetic data, demonstrating its efficacy and then applied to electrophysiological data from mice during decision-making tasks. The study employs variational Laplace-EM for inference and utilizes parallel computing methods to enhance computational performance.\nConclusion: The irSLDS model effectively addresses the limitations of traditional rSLDS models by providing a flexible and efficient framework for analyzing trial-varying neural data. It is able to uncover non-stationary processes in both synthetic and real mice neural data, capturing relevant dynamics not possible with fixed-state models. The introduction of a latent geometric structure over states also facilitates better interpretability and application across various neural analysis settings.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"The paper's originality derives from improvements it makes in the class of AR-HMM and SLDS models by removing exchangeability without removing input recurrence. This enables greater efficiency with expressibility. Overall, this work nicely integrates the heat-equation generalization and recurrent switching state space models. They present overall improvements on two different experiments with greater efficiency of parameters. Visuals are strong, particularly ones pertaining to the flow fields and the switching states of the rSLDS and irSLDS.\", \"Weaknesses\": \"Most of the comparisons are done between rSLDS and irSLDS, yet the results are not convincing and substantially novel in the experiments conducted. If the authors are adamant about just comparing these two models, then it would be nice to see comparisons at greater scales instead of just testing 4-8 states for NASCAR and the IBL experiments. The results at just these scales do not appear novel.\\nFurthermore, both experiments show that the irSLDS models uncovered switching at task-relevant states, yet it isn't clear why adding the heat equation prior itself is the best choice in discovering this switching. The authors discuss their reasons for using the heat equations but don't explain further into infinite priors that possibly could have achieved the same if not more.\\nThe efficient of the parallel scan is not clear. Other works that utilize scans to efficiently compute the transition probabilities such as S4D (Gu et al., 2022), DSS (Gupta et al., 2022), S5 (Smith et al., 2023) are done on diagonalized transition matrices. The connection to the parallelization of the linear dynamics in Equation 8 is unclear since the transition matrix in this work is not diagonalized, so this work does not reap the same computational benefits that they claim, and their algorithm cannot scale effectively since it incurs cubic cost.\", \"Questions\": \"Smith et al. is cited in reference to the use of a scan operation to compute transition probabilities. However, S5 assumes a diagonal state matrix to efficiently compute the linear recurrence which takes a different form than the time-varying transition matrices in this work. How can this scan method scale efficiently?\\nWhy does the heat equation prior directly enable discovery of greater fluctuations/switching in the data? Can you attribute the performance gains to this specific infinite prior?\\nWhy not compare to other linear state space models and generalize outside of rSLDS and irSLDS that can allow for input-dependent transitions and recurrent relations without the explicit use of the influence functions as a geometric prior?\"}"}]}
{"paper_id": "koYsgfEwCQ", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Unsupervised learning of object-centric representations in dynamic visual scenes is important for understanding spatial and temporal information. Most existing approaches decompose 2D images, often leading to issues like the multi-view mismatch of 2D object segmentation due to the ignorance of the 3D nature of scenes.\nGap: The problem addressed in the paper is the unsupervised 3D dynamic scene decomposition to ensure 3D-consistent scene understanding and motivate downstream tasks such as scene editing.\nHypothesis: DynaVol, a 3D scene generative model using object-centric voxelization, can capture the 3D nature of scenes and outperform existing methods in unsupervised dynamic scene decomposition. This approach will allow not only novel view synthesis but also scene editing.\nMethod: The method involves object-centric voxelization within a differentiable volume rendering framework. DynaVol contains three network components: a dynamics module for voxel features, a volume slot attention module for refining object-level global features, and a compositional neural radiance field for view synthesis. The training involves three stages: a 3D voxel warmup stage, a 3D-to-4D voxel expansion stage, and a 4D voxel optimization stage. The model uses a cycle-consistency loss and volumetric slot attention for learning representations.\nConclusion: DynaVol outperforms existing 2D methods like SAVi and 3D methods like uORF in unsupervised dynamic scene decomposition. It can handle occlusion issues better and enables dynamic scene editing, allowing manipulation of geometric shapes and motion trajectories.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"I like this paper. It is well written, it is interesting, the method seems sensible (thought I have a few questions), and the results look good.\", \"Weaknesses\": \"I have a variety of fairly low-level questions, which I hope the authors can address. \\n\\nOverall I think the method would be easier to understand if the stages were presented in a more central way, instead of at the very end. After reading everything I am still a bit unclear on the meaning of an \\\"episode\\\". \\n\\nSection 2 emphasizes that \\\"we also consider scenarios where only one camera pose is available at the first timestamp\\\". What does it mean for only one camera pose to be available? Camera poses are relative to something. A single pose might as well be the identity matrix. \\n\\nThe beginning of Section 3.1 says the method will be trained \\\"without any further supervision\\\", without defining any initial supervision. So I suppose the question is: further to what?\\n\\nSection 3.1 introduces the number of objects N, as \\\"the assumed number of objects\\\". Is this manually chosen? Later on there is a connected components step, so I wonder if connected components could choose N instead.\\n\\n\\nIt is unclear to me what exactly is happening in the warmup stage vs. the dynamics stage. From the names I might guess that the warmup stage does not involve any dynamics, but this is wrong, because it uses all timesteps and it also trains the forward and backward dynamics modules. The section also says \\\"To obtain the dynamics information, we train an additional module f′ξ\\\" -- but dynamics information is already obtained by f_\\\\psi, so there is some mixup here, unless the authors do not see f_\\\\psi as obtaining dynamics information. Finally, it's unclear how exactly the connected components and the \\\"feature graph\\\" connect with the rest of the method.\\n\\nL_point seems like it is never defined, and only a citation is given. It would be great to describe in English what is happening there.\\n\\nSection 3.3 seems to complain that prior compositional nerfs used an MLP to learn a mapping from position+direction+feature to color+density, and then proposes to do the exact same thing. Have I missed something here? \\n\\nOverall I like the paper and I hope it can be cleaned up. The video results show that the segmentations are a little messy, and it seems like only 2 real videos are used, but still I think the paper is interesting and useful to build on.\", \"Questions\": \"This is more of a personal curiosity, but: instead of learning two deformation networks (one forward, one backward) and then asking them to be cycle-consistent, would it work to learn an invertible mapping here instead? (Like with Real NVP.)\"}"}]}
{"paper_id": "o6AN2ZoNw2", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper focusses on the task of open-vocabulary semantic segmentation, a field that assigns semantic labels to each pixel in an image utilizing arbitrary, open-vocabulary texts. Recent advancements in object detection and segmentation have been driven by frameworks like Mask R-CNN and DETR, but an enabling framework for open-vocabulary segmentation has been relatively underexplored.\nGap: Prior techniques often rely on annotated masks or large-scale pre-trained models for training, leading to complexity and potential overfitting to known categories. These techniques generally do not address the direct alignment of pixel-level features with language, which is necessary for true open-vocabulary segmentation without reliance on fixed category sets.\nHypothesis: The authors propose that a simple framework leveraging pseudo-mask generation combined with language supervision, without the need for annotated masks, can effectively train a MaskFormer model for open-vocabulary semantic segmentation, achieving state-of-the-art results compared to more complex models.\nMethod: P-Seg is built upon the MaskFormer architecture without requiring annotated image masks for training. Instead, it utilizes a pseudo-mask generator to produce artificial masks from image tokens derived using a self-supervised ViT. Semantic supervision is provided by associating language features, derived from noisy image-text datasets, with the mask embeddings. Training involves an image-text contrastive loss to align features and includes a self-training enhancement step leveraging model predictions to further refine predictions. The method is evaluated against benchmarks (Pascal VOC, Pascal Context, COCO), demonstrating scalability and improved results with increased data.\nConclusion: P-Seg demonstrates that open-vocabulary semantic segmentation can be achieved via simple, yet robust, pixel-level and language alignment without the need for extensive supervised segmentation data or large pre-trained models. It achieves state-of-the-art results, highlighting its effectiveness and scalability, and serves as a competitive alternative to prior methods that require more resource-intensive setups.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The presented method achieves competitive performance while training models from scratch (with the exception of using DINO for pseudo-supervision).\\n2. The paper investigates performance scaling with increasing dataset size, showing improvements. \\n3. The writing is clear, and the authors promise to release code and checkpoints soon.\\n4. The method obtains competitive results.\", \"Weaknesses\": \"The central weaknesses of this paper include the limited novelty of the proposal and the need for appropriate positioning and comparison within existing literature, which require further clarification of some claims.\\n\\n### Novelty\\n1. The proposed architecture closely follows the proposed scheme of ZegFormer [A] with minor modifications, such as not using a pre-trained language encoder and not using an additional image encoder during inference. The proposed method is not compared to ZegFormer, and the differences are not be discussed.\\n2. The scheme of pseudo-mask labelling self-training is similarly widely employed; most recently, CutLER [B] proposes to train an unsupervised segmenter using masks obtained from DINO clustering. \\n3. The use of pseudo-mask generation in the context of OV segmentation has also been explored in DiffuMask [C], which might require further discussion. \\n\\n### Comparisons and Claims\\n4. The paper claims the proposed method P-Seg achieves SOTA results, but the comparative baselines could be more recent. Critically, methods such as OVSegmentor [D], TCL [E] and SegCLIP [F] are not included in comparisons. The lack of such baselines is critical because, based on reported results in the paper, P-Seg would not be SOTA on Pascal VOC (OVSegmentor, TCL) and Context (TCL) and COCO (TCL), which contradicts the bolded claim in the introduction. Additionally, OVSegmentor is trained on smaller datasets and similarly does not use CLIP or ALIGN models, relying instead on DINO (like P-Seg), which offers a closely matched baseline.\\n5. Additionally, the paper compares on the COCO dataset with 80 classes and backgrounds, which matches other papers' descriptions of the COCO-Object dataset. The dataset description might need some additional clarification. CLIPpy results are reported much lower and match those of full COCO (133 classes) rather than COCO-Object (80 classes) from the paper, which might need correcting.\\n6. It might also be more reasonable to include more recent _mask+text_ methods such as OVSeg [G] or ODISE [H] in the comparison to provide a more complete picture of the models and performance attained in the field.\\n6. It should also be clarified in what sense are the methods \\\"zero-shot\\\"? Both P-Seg and P-Seg+ likely observe instance images of the target class during training. This is because the word token embeddings of the corresponding classes in the language model were trained, so training datasets should have contained target classes. P-Seg+ is trained on the training sets of the evaluation datasets, which include classes in question by design. Perhaps the paper uses more of a CLIP-like interpretation of \\\"zero-shot\\\", where the model is not trained on the target classes but on a large dataset of images and text that likely _contained_ such instances. This should be clarified. In any case, P-Seg+ should not be classed as zero-shot since it is trained on the train spits of evaluation datasets. As an alternative, observed/unseen class comparisons could be made like in other zero-shot semantic segmentation papers (e.g. [C]).\\n\\n### Experimental Evaluation\\n7. The paper lacks ablations that confirm the design choices of the method. The ablation presented centres around showing that the method improves with a larger amount of data. However, this confirms the general trend of deep learning methods and is not, arguably,  very surprising. The question of what makes the method work needs to be addressed, as they are critical to understanding and building on top of this work.\\n    - Why use simple K-Means on top of DINO features? Prior methods for extracting masks using DINO features have been developed, such as LOST [I], TokenCut [J], and MaskCut [B], which have achieved state-of-the-art results in unsupervised segmentation with mere modification of the clustering procedure. These also offer better points of comparison to the pseudo-mask extraction method. \\n    - Is setting K=8 critical? This needs to be discussed. How sensitive is the method to this choice? Nouns are extracted from the caption. Why not use the full caption? Why not set K to some function of the number of nouns in the caption?\\n    - Why are language features compared to predicted mask features? An alternative would be to compare the average features used to obtain dense masks. Is this choice critical?\\n    - Why is P-Seg+ using a different architecture and different, now pre-trained, backbone? This avoids the opportunity to initialize from large-scale training of the P-Seg. Is the large-scale VL pre-training detrimental in this case?\\n\\n### Presentation and Clarity\\n8. The paper also needs some crucial details.\\n    - The evaluation protocol for including background could be clearer. How is background class determined in P-Seg/P-Seg+? Is textual prompt used for background? What are textual prompts used for the categories? Prior works rely on templates or prompt expansion to obtain textual prompts for classes. Such details should be included.\\n    - The procedure for self-training should be discussed in detail. In particular, from the description in Sec. 3.3.4, the new model is trained using predictions of the current model, which are described as pixel-dense masks and class labels (Section 3.1, last paragraph). **This makes P-Seg+ a standard closed-set segmentation method, which is not entirely comparable to other methods used in the paper.** If this is the case, then it should be clarified.\\n9. Presentation of qualitative results should clearly indicate that **P-Seg results are obtained using a CRF**, as it is only mentioned in Appendix C and not when such comparisons are discussed. A comparison could be made without CRF to show substantial differences, as other works might similarly benefit from CRF, and CRF might obscure key differences in results.\\n10. It also needs to be clarified why specific models in Tables 3 and 4 have multiple entries with different results for the same dataset. How do these models differ? Are these different runs of the same model? This should be clarified.\\n\\n---\\n#### References\\n- [A] \\\"Decoupling Zero-Shot Semantic Segmentation\\\" Ding et al. CVPR 2022\\n- [B] \\\"Cut and learn for unsupervised object detection and instance segmentation\\\" Wang et al. CVPR 2023\\n- [C] \\\"DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models\\\" Wu et al. ICCV 2023\\n- [D] \\\"Learning Open-Vocabulary Semantic Segmentation Models From Natural Language Supervision\\\" Xu et al. CVPR 2023\\n- [E] \\\"Learning to Generate Text-grounded Mask for Open-world Semantic Segmentation from Only Image-Text Pairs\\\" Cha et al. CVPR 2023\\n- [F] \\\"Segclip: Patch aggregation with learnable centers for open-vocabulary semantic segmentation\\\" Lou et al. ICML 2023\\n- [G] \\\"Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP\\\" Liang et al. CVPR 2023\\n- [H] \\\"Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models\\\" Xu et al. CVPR 2023\\n- [I] \\\"Localizing Objects with Self-Supervised Transformers and no Labels\\\" Simeoni et al. BMVC 2021\\n- [J] \\\"Tokencut: Segmenting objects in images and videos with self-supervised transformer and normalized cut\\\" Wang et al. CVPR 2022\", \"Questions\": \"The key questions have been presented alongside the weaknesses. The following are additional questions that could be considered, though they are very minor. \\n\\n- Why were the models not pre-trained? Using a pre-trained backbone for a segmentation network is a common practice. It is also not clear why the language model needs training from scratch. It seems that using a language model pre-trained on a large corpus would be beneficial.\\n- The emphasis seems to be placed on processing time for pseudo-mask generation. However, the method is trained using 30 epochs, which would likely take a lot longer than any pre-calculation of pseudo-masks (single epoch). It is not clear why processing time is so critical.\\n- The text says that N binary masks are predicted. How are these masks converted to C-way dense masks? How are possible overlaps handled?\"}"}]}
{"paper_id": "HXZK1Z8tHa", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Image restoration is a classical inverse problem aiming to reproduce high-quality images from degraded inputs. CNN-based methods initially dominated this field, but limitations like excessive inductive bias and limited perceptual fields led to the adoption of Transformer-based methods. Transformers are advantageous for handling long-range dependencies, but they pose challenges such as high computational overhead when dealing with high-resolution images.\nGap: The primary issues with existing Transformer-based image restoration methods are their long inference times and complex training processes. The trade-offs between model latency and trainability remain unresolved, as convolutional modules can improve trainability but not reduce latency, while sparsification can decrease latency but complicates optimization.\nHypothesis: The authors propose that sharing attention maps among neighboring blocks, coupled with residual connections on the 'Value' of self-attention, can achieve a balance between latency reduction and improved trainability, leading to a more efficient and easier-to-train Transformer model for image restoration tasks.\nMethod: The proposed method, ShareFormer, introduces Shared Portion Stripe Attention (SPSA) to share attention maps between neighboring blocks, reducing redundancy and computational overhead. Residual connections are added to the 'Value' in self-attention to enhance optimization and introduce a local inductive bias. The model architecture is built with Share Attention Blocks (SAB) and combined shared attention unit (CSAU), ensuring efficiency and performance across various tasks like image super-resolution, denoising, and JPEG compression artifact reduction. The architecture is evaluated using large datasets and compared against state-of-the-art methods concerning latency, flops, and visual quality improvements.\nConclusion: ShareFormer effectively reduces network latency and improves trainability compared to existing Transformer-based methods, achieving state-of-the-art performance across multiple benchmarks. The developed model provides an optimal balance between speed and accuracy, demonstrating the potential for further applications in other image restoration tasks and possibly extending to high-level vision problems.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 5, \"Strengths\": \"[S1] The references and related works cited by the authors are very recent and relevant.\\n\\n[S2] The Lesion study presented in Sec.4.1 is comprehensive for demonstrating the proposed shared attention is highly related to ensemble behavior of the sequentially placed attention mechanism. Deleting or permutating some self-attention layers could smoothly increase the reconstruction loss. This can be the evidence that the shared attention is not the ensembles of shallow networks.\\n\\n[S3] The performances of ShareFormer for large SR and lightweight SR are notably enhanced, compared to recent SOTA methods. These results were achieved with smaller model size, fewer computations, and faster speed than the others. (But an unfair issue remain. See question Q4.)\", \"Weaknesses\": \"[W1] The presentation of text and figures in this paper should become clearer and more understandable. See Q1, Q2, and Q3.\\n\\n[W2] The paper lacks some ablation studies proving the importance of the proposed components, such as CSAU. The authors should study architecture variants if they hope to clarify that CSAU comes not from insufficient considerations but from careful construction. And the efficiency and effectiveness differences between the original GDFN and the enhanced CSAU must be provided.\\n\\n[W3] Most importantly, the core parts of ShareFormer, shared attention, seem not novel. In ELAN, one of the sota methods, shared attention map mechanisms have been already proposed. Moreover, it has been already shown how sharing attention maps in more than one layers can impact on the performance and efficiency of the model (related to your Tab.8).\\n\\n[W4] A potential unfair issue is observed with respect to SR. See Q4.\\n\\n[W5] Despite the improved inference speed, the performance gains of grayscale denoising, color denoising, and JPEG CAR are not significant.\", \"Questions\": \"[Q1] Where is the exact part that the shared attention is operated? Eq.(4), (5) apparently reveals that this operates in $(l-1)$-th and $l$-th SPSA blocks. However, Fig.2 illustrates attention map of SPSA is shared to CSAU, while Fig.3 depicts sharing attention map appears after residual connection on $\\\\textit{value}$. The reviewer thinks that the explanation of Eq.(4), (5) is the correct case the authors intended, while the phrase, \\\"**sharing attention map**\\\", in Fig.2 and Fig.3 is confusing. If it’s right, “sharing” can be omitted. Or not, please let me know what your first intention with respect to the exact parts of sharing attention map is.\\n\\n[Q2] This question is related to Sec.4.2. From ConViT, how can you draw the conclusion that the concentrated ERF implies an amplified locality bias of the network? The reviewer thinks that it is not sufficient for the authors to claim that this fact shows the locality bias of the residually connected $\\\\textit{value}$ in ShareFormer. I cannot find the acceptable evidence from ConViT paper. Don’t you have any other evidence justifying your claim, such as visualization?\\n\\n[Q3] In Appendix D, why did you compare the attention maps of layers 1 and 3? ShareFormer shares attention map in layer-order-number pairs (0, 1), (2, 3), …, (2n-2, 2n-1), respectively. Thus, comparison of attention maps in “layers 0 and 1” or “layers 2 and 3” is more compelling to demonstrate attention map redundancy. Additionally, I recommend the authors to compare attention redundancies of the cases where the shared attention is “employed” and “not employed” in the proposed ShareFormer, instead of SwinIR cases.\\n\\n[Q4] Did you apply the Progressive Learning to large and lightweight SR tasks with training patch size of from 128 to 384? However, the comparative methods, SwinIR, ELAN, and DLGSA, used smaller patch size, such as 48x48 and 64x64. I am concerned that this leads to a potential unfairness issue.\\n\\n\\n\\n**Minor issue (not necessary to be mentioned in author rebuttal, if the authors struggle to a limit on the number of characters of rebuttal.)**\\n\\n(1) Fig.2 omits (a) and (b) mark, while the caption uses (a) and (b).\\n\\n(2) In the last to second sentence of Tab.1 caption, Restormer never used window attention. Moreover, you should mention what is MHDA, which seems a typo. (In Restormer, MDTA was used, and MHDA was not shown in SwinIR.)\"}"}]}
{"paper_id": "QO3yH7X8JJ", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Diffusion-based Generative Models (DGMs) have recently shown great promise in enhancing super-resolution (SR) tasks by synthesizing high-quality visual content. Traditional SR methods are often constrained by a fixed, integer-based scaling architecture, and recent solutions have typically involved training architecture-specific DGMs from scratch, or performing extensive fine-tuning and distillation on pre-trained models, which are resource-intensive.\nGap: Existing methods for super-resolution tasks are often tied to specific integer scaling factors and rely heavily on architecture-specific models that demand significant training and hardware resources. These methods fail to address the need for arbitrary-scale super-resolution (ASSR), which can efficiently adapt to diverse upsampling scales without preparing different models for each scale.\nHypothesis: The authors propose that the super-resolution capabilities of existing pre-trained diffusion generative models can be effectively harnessed without additional fine-tuning or distillation by implementing a simple but effective noise injection methodology.\nMethod: The proposed Diff-SR method leverages pre-trained DGMs by injecting an optimal level of noise into low-resolution images before applying the DGM's backward diffusion process. This approach is built on the concept of the Perceptual Recoverable Field (PRF), which quantifies the appropriate amount of noise required to achieve a balance between maintaining low-level fidelity and reducing high-level distortion. Extensive experiments demonstrate the effectiveness of this approach across multiple datasets, using metrics such as FID, PSNR, and SSIM for evaluation.\nConclusion: Diff-SR achieves superior performance over existing methods for arbitrary-scale super-resolution tasks, showcasing significant improvements in image quality across various scales, as evidenced by lower FID scores and competitive PSNR and SSIM metrics. The method is validated through extensive experiments highlighting its flexibility and adaptability to different upsampling scenarios without retraining specific models.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 5, \"Strengths\": \"1. This paper presented a new perspective that extends a pre-trained diffusion model into an arbitrary-scale super-resolution by adding a specific amount of noise to LR images before the diffusion model backward diffusion process. The idea is interesting.\\n2. This paper proposed a metric Perceptual Recoverable Field for achieving the optimal trade-off between fidelity and realness.\", \"Weaknesses\": \"Please refer to the Questions.\", \"Questions\": \"1. Since the proposed method is for arbitrary-scale super-resolution, it is important to provide the quantity results comparison of arbitrary-scale such as 1.6X, 2.7X, 3.5X, and 4.5X. Also, the visual comparison of different scales comparison of the proposed method on the same image is also important to evaluate this work.\\n2. It mentioned that the used pre-trained diffusion model is 256*256. How does this model achieve arbitrary-scale super-resolution? For example, the low-resolution image is sized 100*100, how can we achieve 3.5X super-resolution?\\n3. Please give a reference for the value of noise level in an interval of 0.1.\\n4. Can the proposed method extend in real-world arbitrary-scale super-resolution? \\n5. Adding different noise levels when dealing with different scales, the higher noise means lower fidelity. When facing larger scale super-resolution, that means very low fidelity. Is there a big visual change between X2 and X4 results?\\n6. How does the proposed method maintain the trade-off between fidelity and realness? \\n7. In Line 22 of Page 2, this paper mentions high-level signature of generated content. What does it mean? \\n8. In Line 3 of Page 5, should the low-resolution image be high-resolution image? Otherwise, it is hard to understand this sentence.\"}"}]}
{"paper_id": "sNtDKdcI1f", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Reinforcement Learning from Human Feedback (RLHF) has been effectively used to align large language models with human-desired properties like helpfulness. This technique involves training a reward model based on human feedback and optimizing the language model to maximize the reward. This process is often reported to enhance the quality of language generation tasks such as dialogue and question answering.\nGap: One of the consistent issues observed with RLHF is that it often leads models to produce unnecessarily longer outputs. This has not been fully analyzed, leaving it unclear if increased output length is a meaningful contributor to perceived improvements in model helpfulness, or merely an artifact of reward model biases.\nHypothesis: The primary hypothesis is that optimizing for response length is a significant, and possibly overestimated, factor contributing to the improvements reported in RLHF. The authors question whether it is the main driving force behind the reward improvements seen in RLHF implementations.\nMethod: The authors conducted experiments with three open-source preference datasets: WebGPT (question answering), StackExchange (technical question answering), and RLCD (multi-turn dialogue). They analyzed output length correlations with reward and experimented with interventions at various stages of the RLHF pipeline to mitigate the effects of length. These interventions included modifying the reward models, applying length penalties, and altering preference data to balance length biases. They also compared standard RLHF to a baseline where only output length was the reward, to assess the contribution of length to model output improvements.\nConclusion: The study finds that a large portion of the reward increases achieved through RLHF can be attributed to optimizing for longer outputs, more than previously acknowledged. This suggests existing reward models are biased towards length. The experiments showed that models rewarded solely based on output length achieved performance improvements comparable to full RLHF with sophisticated reward modeling, implying that the length is a significant factor but does not fully explain the quality gains, as RLHF with typical reward models also induces qualitative changes. The study highlights the need for more robust reward model techniques to truly capture human preferences beyond shallow correlations like length.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"--comprehensive evaluations on a few different RLHF settings/datasets for helpfulness optimization\\n\\n--nice in-depth exploration confirming the extent of a phenomenon (length correlations for helpfulness optimization) that i think hasn't been comprehensively analyzed previously\", \"Weaknesses\": \"--presentation: it's quite difficult to parse what several of the abbreviations mean in the tables (and honestly i'm still a bit confused on what exactly you're showing in the first couple of figures and tables after looking at them for several minutes). it would be great if you could make the figs/tables more self-contained, e.g., define abbreviations in the captions and summarize takeaways, so i don't have to keep bouncing between the figs/tables and the text.\", \"Questions\": \"--even if we're just optimizing for length, your model still learns something nontrivial, right? an interesting point of comparison might be an extra baseline where instead of sampling 8 outputs from the base LM and picking the longest one, you just sample one output and forcibly make it keep generating past the end - i imagine this would perform a lot worse. so intuitively, i feel that optimizing for length is also somehow optimizing for some less trivial notion of comprehensiveness, maybe?\\n\\n--just to confirm, weighted reward gain i should think of as measuring how much reward improves within a particular bin of length, i.e. the reward gain that comes from factors other than length?\\n\\n--would be curious also if you have any qualitative sense of what are the non-length axes along with helpfulness improves in your experiments (i.e., any intuitions for the remaining 10-30% of helpfulness that wasn't attributed to increased length?).\\n\\n--as you wrote in the conclusion, it would be interesting to just see a quick experiment on e.g., harmlessness or honesty, if only to confirm our intuition that length is primarily a heuristic for optimizing helpfulness and not really applicable to other common tasks we RLHF for. (not necessary if it's troublesome to setup though.)\\n\\n--i'm not very clear on the concrete actionable suggestions for practitioners based on your analysis? (though they're not strictly necessary - i see that you did try a few ways to regularize length already)\"}"}]}
{"paper_id": "HFtrXBfNru", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Graph Neural Networks (GNNs) are increasingly being applied in areas such as social networks, smart cities, and drug discovery due to their capability to model complex relations. However, they face performance issues as the underlying graphs evolve over time, which leads to representation distortion and subsequent performance degradation.\nGap: Existing GNNs struggle to maintain accurate representations over time due to the evolving nature of graph data. This change occurs as nodes and edges are added or modified, or new features are introduced, making it challenging to estimate the temporal generalization performance without continuous human annotation.\nHypothesis: The authors propose that an adaptive feature extractor using self-supervised learning can effectively minimize representation distortion and improve generalization estimation in evolving graphs. They introduce 'SMART', which employs self-supervised graph reconstruction techniques to enhance temporal generalization estimation.\nMethod: The proposed method, SMART, incorporates self-supervised learning for temporal generalization estimation. It employs an adaptive feature extractor achieved through graph reconstruction losses, namely structure and feature reconstruction. This is tested on synthetic Barabási–Albert random graphs and four real-world evolving graphs. The methodology involves refining theoretical lower bounds on representation distortion, performing graph-level reconstruction post-deployment, and evaluating temporal generalization estimation using metrics such as MAPE.\nConclusion: SMART demonstrates outstanding performance in estimating temporal generalization, as evidenced by experiments on synthetic and real-world data. The adaptive feature extractor significantly reduces information loss, improving performance estimation without relying on additional human annotations post-deployment. The study also highlights the critical role of structure graph reconstruction in minimizing prediction error.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"1.\\tThe general motivation for analyzing the generalization of evolving graphs is interesting and important. It is also interesting to see the theoretical analysis with a synthetic random graph demonstration with a closed-form generalization error.\\n2.\\tThe presentation quality of this paper is in general satisfactory, in terms of the organization and figure demonstration.\\n3.\\tThe experiments and analysis on the proposed method itself are relatively sufficient including model component ablation and hyper-parameter study.\", \"Weaknesses\": \"1.\\tOne major concern is the lack of baseline methods. The proposed method is only compared with a linear regression, which turns out to be a naive baseline. The methods mentioned in the related work D.2 are not compared. Moreover, only the MAPE with standard error is evaluated. More evaluation metrics are expected.\\n2.\\tIt will be better if the methods are evaluated on a few more state-of-the-art backbones to further enhance the effectiveness of the proposed method.\\n3.\\tThe assumptions made to conduct theoretical analysis and bound derivation could be a concern as they are often too strong. Evolving graphs also contain nodes disappearing in many cases(e.g. sensor networks with possible malfunction over time), which means the assumption of an ever-growing network can be often invalid. Moreover, the zero-mean requires a non-min-max preprocessing of features. The authors need to be specific about the claims.\\n4.\\tIt would be better to sketch the connection between the information loss and the form in Equation 2, section 3.2.\\n5.\\tIt would be better to define some notations such as the function l, and generalization error beforehand for better understanding of the idea.\", \"Questions\": \"Please see the weaknesses.\"}"}]}
{"paper_id": "4u0ruVk749", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Estimating the Individual Treatment Effect (ITE) from observational data is crucial in various fields like healthcare and recommendation systems. Traditional methods often rely on the Ignorability assumption, implying no unobserved confounders exist, which is unrealistic. Generative models like VAEs and GANs have been used to simulate confounders but have limitations in model flexibility and stability.\nGap: Current methods primarily assume the absence of unobserved confounders, making the Ignorability assumption impractical in many real-world scenarios. Additionally, existing generative models for confounder simulation have stability and flexibility issues.\nHypothesis: The authors propose using a diffusion model to capture unobserved latent spaces, allowing for the estimation of causal effects under more realistic assumptions than prior models. This approach should improve ITE estimation by reconstructing meaningful unobserved confounders using a diffusion process conditioned on prior knowledge.\nMethod: The proposed method involves a two-step diffusion process to model unobserved confounders. A forward diffusion process converts observed confounders into a noise distribution while preserving useful information. A reverse process, parameterized as a Markov chain with deep neural network-estimated transitions, reconstructs the unobserved confounders. The model's performance is optimized using a variational lower bound and evaluated on both synthetic and benchmark datasets. Experimental comparisons are made with 12 state-of-the-art models using metrics like √ϵPEHE and ϵATE.\nConclusion: The diffusion model-based approach outperformed existing methods in estimating ITE by effectively handling unobserved confounders. This paper demonstrates that capturing latent confounders improves causal inference tasks, suggesting a promising direction for utilizing diffusion models in complex causal inference scenarios.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 1, \"Contribution\": 1, \"Rating\": 1, \"Confidence\": 4, \"Strengths\": \"Comprehensive numerical results are provided to support the proposed method\", \"Weaknesses\": \"1. The writing is extremely poor, making the reviewer wondering whether the draft was proofread before submission:\\n\\n- $\\\\epsilon_{PEHE}$ and $\\\\epsilon_{ATE}$ undefined before used in abstract\\n- in 2nd paragraph: \\\"(e.g.,medicine)\\\" --- missing space --- and \\\"(e.g., demographic characteristics )\\\" --- one extra space before \\\")\\\".\\n- in 2nd paragraph: \\\"DR\\\", which the reviewer assumed to be \\\"doubly robust\\\", is again used before definition\\n\\nIt is not the reviewer's responsibility to proofread the paper and it is highly recommended that a careful revision is needed before resubmission.\\n\\n2. Additionally, the review of existing work and motivation are unclear from the introduction:\\n\\n\\\"...and accordingly to infer the ITE in an unbiased settings...\\\"\\nWhat is an \\\"unbiased setting\\\"? Does it mean the estimator is unbiased?\\n\\n\\\"VAEs make substantially weaker assumptions in generating the structure of the hidden confounders Louizos et al. (2017), which could restrict the model’s flexibility.\\\"\\nWhy a weaker assumption leads to more restrictive method?\\n\\n\\\"For examples, GAN-based methods could be unstable in modeling ITE due to the adversarial losses. VAEs make substantially weaker assumptions in generating the structure of the hidden confounders Louizos et al. (2017), which could restrict the model’s flexibility. In order to address these challenges, in this paper, we propose to generate the unobserved confounders using diffusion model.\\\"\\n\\nBased on the above review of existing work, it is very, very unclear why the proposed diffusion method can handle the issues (and not to mention that those issues are not stated clearly at all).\", \"Questions\": \"See clarity questions above.\\n\\nMoreover, in table 1:\\n\\nHow is the hyperparameter selected? what is the criterion? Indeed, there is recent work (this year) showing that the criterion for hyperparameter selection is even more important than the model itself. Please look at:\\n\\nMachlanski, Damian, Spyridon Samothrakis, and Paul Clarke. \\\"Hyperparameter tuning and model evaluation in causal effect estimation.\\\" arXiv preprint arXiv:2303.01412 (2023).\\n\\nCurth, Alicia, and Mihaela van der Schaar. \\\"In search of insights, not magic bullets: Towards demystification of the model selection dilemma in heterogeneous treatment effect estimation.\\\" arXiv preprint arXiv:2302.02923 (2023).\\n\\nWei, Song, et al. \\\"Detecting Electricity Service Equity Issues with Transfer Counterfactual Learning on Large-Scale Outage Datasets.\\\" arXiv preprint arXiv:2310.03258 (2023).\\n\\nIf there were cross validation involved, please specify what error table 1 is reporting --- the current table cannot be compared to existing results in literature, e.g., table 1 in Dagonnet.\"}"}]}
{"paper_id": "Pa6SiS66p0", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Human brains have a remarkable capability for continual learning (CL), incorporating information from multiple modalities (audio, visual, etc.) to enhance perception and cognition. This multi-modal integration improves memory and perceptual learning by creating comprehensive environmental representations. However, deep neural networks (DNNs) typically suffer from catastrophic forgetting when learning new tasks in sequence, particularly in a unimodal setting, which limits their deployment in dynamic environments. Previous work in this area often emphasizes unimodal approaches, primarily within the visual domain, and multi-modal learning has primarily been explored in i.i.d. settings for specific applications, such as action and audio-visual speech recognition.\nGap: The study identifies a gap in the exploration of leveraging multiple modalities to mitigate forgetting in DNNs. Specifically, there is insufficient understanding and techniques for combining multiple modalities to enhance continual learning in DNNs, particularly in challenging scenarios like class incremental and domain incremental learning.\nHypothesis: The integration of multi-modal learning into DNNs can enhance their lifelong learning ability, allowing models to develop more accurate and robust representations less prone to modality-specific regularities and catastrophic forgetting by integrating complementary information from multiple modalities.\nMethod: The study introduces a benchmark for multi-modal continual learning based on the VGGSound dataset, which includes paired visual and audio data. It proposes a rehearsal-based multi-modal continual learning method that utilizes the relational structural similarities between data points in each modality for integration and alignment. This approach includes maintaining modality-specific representations, which are then aligned and consolidated into a unified representation space. The experiments were conducted across various challenging continual learning settings like class-incremental (Seq-VGGSound), domain-incremental (Dom-VGGSound), and generalized class-incremental learning (GCIL-VGGSound), comparing the proposed method, SAMM, against a baseline experience replay method on multi-modal datasets.\nConclusion: The study concludes that integrating multiple modalities in DNNs for continual learning significantly reduces catastrophic forgetting, enhances adaptability, and provides a better stability-plasticity trade-off. The proposed multi-modal continual learning method, SAMM, improves both multi-modal and single-modality inference capabilities and sets a strong baseline for further research using the VGGSound-based benchmark. It also highlights the potential of multi-modal learning to develop robust models that can adapt to dynamic and evolving environments.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"+ The proposed benchmark covers three CL scenarios and can be beneficial to the community. \\n+ The analysis in Section 3 makes sense and provides empirical evidence for the superiority of multiple modalities over single modality in CL.\\n+ The paper has good motivation and is well organized.\", \"Weaknesses\": \"My major concern with this paper is the lack of comparison and experiments. The evaluation seems a bit weak to me as all the experiments are conducted on VGGSound only, and the baseline Experience Replay for comparison with the proposed approach is from 2018. I wonder if the authors could apply some more recent unimodal CL approaches ([1][2] etc.) to the problem.\\n\\n[1] SS-IL: separated softmax for incremental learning   \\n[2] Class-incremental learning by knowledge distillation with adaptive feature consolidation.\\n\\n---\\nAlso, in terms of comparison with multimodal CL approaches:\\n+ (1) Could the authors further clarify why the proposed approach can not be applied to vision-language? What is the advantage of the proposed benchmark compared with [3], besides the modality difference? \\n+ (2) I understand that [4] is published after the submission ddl, but it would be good if the authors could comment a few sentences about the differences with them in the rebuttal.  \\n\\n[3] Climb: A continual learning benchmark for vision-and-language tasks.  \\n[4] Audio-Visual Class-Incremental Learning\\n\\n---\\nFor the semantic-aware feature alignment, I wonder if the authors can provide some visualization examples to demonstrate that the model indeed learns the desired modality-specific features, such as Figure 4 in [5].\\n\\n[5] The Modality Focusing Hypothesis: Towards Understanding Crossmodal Knowledge Distillation\\n\\n---\\nTypo, Figure 4 caption, \\\"leverage leverages\\\"\", \"Questions\": \"See weaknesses.\"}"}]}
{"paper_id": "rkplYfqUr0", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Language model (LM) prompting is a popular approach for solving natural language processing (NLP) tasks. These models can perform tasks through a process known as in-context learning, where they are primed with examples to adjust to new tasks without fine-tuning. Recent studies have debated the mechanics behind in-context learning and its sensitivity to prompt variations, requiring precise demonstrations for optimal performance.\nGap: Discriminative prompting, where the model predicts labels based on input, is prone to miscalibration and brittle when prompt variations occur. This setup's reliance on finding the right training examples and the high variance in performance based on prompt structure necessitate a more robust approach.\nHypothesis: The authors propose that using a generative prompting framework for zero-shot text classification can address the issues of miscalibration and brittleness. They hypothesize that estimating the likelihood of generating input conditioned on natural language descriptions of labels can lead to better performance and robustness compared to discriminative methods.\nMethod: The authors introduce GEN-Z, a generative zero-shot text classification framework. GEN-Z computes the likelihood of input text given label descriptions, which include additional context. The method aggregates results from multiple paraphrases of label descriptions to reduce variance. Experiments were conducted using six open-source language model families across 19 classification tasks. The authors manually crafted label descriptions and generated paraphrases using ChatGPT to improve task performance.\nConclusion: GEN-Z outperforms traditional zero-shot and few-shot baselines, demonstrating robustness to prompt variations and better performance overall. The approach enables personalized classification by integrating contextual information about the evaluation set, such as the author's demographics or the text's domain.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"- This paper is well-written and easy to follow.\\n- The empirical analysis is comprehensive and solid regarding datasets and backbone models.\\n- The proposed method is simple yet effective, providing a solid and lightweight framework to solve zero-shot text classification problems.\", \"Weaknesses\": \"The proposed method, presentation, and empirical analysis are well self-explained. I don't have much concern about it. \\n\\nMy main concern is whether the problem of \\\"zero-shot text classification with some human/ChatGPT labeled data\\\" is meaningful. If one would like to do some annotation, either with human annotators or ChatGPT, why not directly annotate some labeled samples for few-shot text classification? Thus, in my opinion, to show that this problem setup and the proposed framework are meaningful, the authors need to somehow show that this zero-shot framework is more effective than a few-shot text classification framework given the same amount of annotation effort.\", \"Questions\": \"Do you think the proposed framework can be used to amplify few-shot classification systems? I think it may make better sense in assisting a few-shot classification system since adding a few more training samples may not be as effective as providing these label descriptions given the same annotation efforts.\"}"}]}
{"paper_id": "pTU2X9mUBe", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Last-mile delivery is a critical area in logistics and supply chain management due to urbanization and the rise of e-commerce. It constitutes the most expensive and time-consuming part of shipping. Researchers have focused on problems like route planning and ETA predictions. However, the absence of a publicly available, comprehensive dataset hampers research progress in this domain.\nGap: There is a lack of a widely accepted, publicly available last-mile delivery dataset. Existing efforts, such as Amazon's dataset, are limited in scale, scope, and comprehensiveness, lacking data from package pick-up processes and detailed courier information.\nHypothesis: The authors propose that LaDe, a large-scale, comprehensive, and diverse dataset from real-world last-mile delivery operations, can significantly advance research in logistics by providing critical data for developing and benchmarking new algorithms.\nMethod: The authors collected a dataset named LaDe from a large logistics platform, containing 10,677k packages processed by 21k couriers over six months across various Chinese cities. The dataset captures both pick-up and delivery scenarios. They analyzed its properties and verified its utility by benchmarking it against route prediction, ETA prediction, and spatio-temporal graph forecasting tasks using several baseline models.\nConclusion: LaDe is positioned as a critical resource for research in logistics, offering comprehensive data that can enhance the development of algorithms in last-mile delivery. Its scale and diversity make it suitable for a range of tasks beyond those benchmarked, potentially furthering progress in the field.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"S1. The provision of an open-access, real-world dataset in this article has a favorable and constructive influence on research within the domain of Pickup & Delivery.\\n\\nS2. Following the release of the dataset associated with this article, it can be employed to corroborate and verify findings from research papers that had not previously made their datasets openly available.\\n\\nS3. The dataset introduced in this article incorporates measures to protect crucial user privacy information.\", \"Weaknesses\": \"W1. The data fields of the dataset proposed in the article appear relatively straightforward and, in essence, do not significantly distinguish themselves from the data fields found in other existing Pick-up & Delivery datasets. \\n\\nW2. The dataset outlined in this research paper reveals certain limitations with respect to its incorporation of road network structure, a pivotal aspect for applications pertaining to pick-up and delivery services. Predominantly relying on GPS locations(x-y coordinates), while undoubtedly valuable, might not comprehensively address the intricacies inherent in real-world logistics operations. The absence of road network information in the dataset does raise valid concerns about its suitability for applications necessitating an in-depth understanding of spatial constraints and the optimization of routing, particularly in the last-mile express domain. To augment the dataset's applicability in such scenarios, it may prove advantageous to contemplate the inclusion of coarse-grained road network data. While acknowledging the privacy concerns associated with road network information, providing a more generalized form of road network data could still facilitate researchers in achieving a more comprehensive and accurate representation of the real-world scenarios under examination.\\n\\nW3. The dataset should acknowledge the significant impact of rush hours and traffic congestion on logistics. It is important to recognize that the road network structure undergoes dynamic and temporal changes over the course of several months. Therefore, it becomes paramount to incorporate a dynamic road network structure within the dataset that can adapt to and depict the wide-ranging traffic conditions and road configurations at different times. This dynamic approach not only bolsters the dataset's precision and relevance but also furnishes researchers with a more authentic representation of the continually evolving real-world logistics and last-mile delivery scenarios.\\n\\nW4. It is worth noting that several datasets, including prominent examples like the Didi Chuxing GAIA dataset and the New York City dataset, have been extensively employed in addressing pick-up and delivery problems. These datasets have formed the foundation of numerous benchmarks and research benchmarks, rendering them well-established within the field. Consequently, it is essential to delineate the distinctive advantages that the dataset introduced in this paper brings to the table in comparison to these existing datasets and benchmarks, particularly in the context of last-mile delivery research. Clarifying the unique attributes and contributions of this new dataset is crucial in highlighting its relevance and significance within the academic and research community. To accentuate the contributions of the article, it is essential to delve deeper into a comprehensive comparison that delineates the differences between the dataset tailored for Last Mile Delivery scenarios and datasets associated with broader Pick-up & Delivery contexts, including well-known examples like the DIDI and NYC taxi datasets. \\n\\nW5. Throughout the article, the authors seek to benchmark several aspects of applications, each of which boasts a significant body of existing works. Yet, the chosen methods in the article appear to fall short of encompassing the full spectrum of relevant works and categories within these benchmarked areas. In light of this, it is imperative that the authors offer more extensive elucidation regarding their decision to select these specific methods as baselines when there are numerous alternatives at their disposal. Providing a comprehensive explanation will not only enhance the transparency and academic rigor of their research but also enable the reader to grasp the underlying reasoning behind these choices.\", \"Questions\": \"Beyond the aforementioned weak points, the additional question is as follows.\\n\\nQ1. Does the proposed benchmark potentially unveil latent insights or yield noteworthy contributions to the field? Moreover, what benefits can be discerned from conducting experiments with this dataset in contrast to other pre-existing datasets?\"}"}]}
{"paper_id": "uhtQyRrTzY", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Dense vision tasks such as depth prediction, surface normal estimation, semantic segmentation, and pose estimation typically rely on pretrained representations. Self-supervised learning has shown potential in this area, but dense prediction tasks have struggled to replicate the success seen in object recognition due to a lack of suitable datasets. Current datasets rely heavily on annotated 3D meshes and simulated environments, which limit their applicability to real-world data sources where such metadata is absent.\nGap: There is a lack of large-scale, real-world multi-view datasets without requiring 3D annotations such as meshes or camera parameters. This limitation restricts the scalability of existing self-supervised learning approaches for dense vision tasks, such as CroCo, which depend on synthetic datasets with detailed 3D annotations.\nHypothesis: The authors propose a data-curation method, MIMIC, that automatically generates large-scale multi-view datasets suitable for dense vision tasks using unannotated real-world videos and synthetic environments. They hypothesize that by leveraging real-world data in combination with computer vision techniques such as SIFT and RANSAC, they can achieve superior performance in dense prediction tasks compared to existing methods relying on annotated datasets.\nMethod: MIMIC is a dataset curation approach that automatically generates multi-view image pairs from real-world videos and synthetic environments without requiring additional annotations. The process involves identifying potential data sources, using SIFT keypoint detection and homography transformation to find correspondences, and filtering pairs based on overlap thresholds. Two datasets were created: MIMIC-1M and MIMIC-3M, which were then used to train models with masked image modeling objectives (MAE and CroCo). The models' performance was evaluated on downstream tasks like depth estimation, surface normals estimation, semantic segmentation, and others.\nConclusion: The study demonstrates that representations learned with MIMIC's automatically-generated datasets outperform those trained with expensive, annotated datasets like ImageNet-1K and synthetic dataset MULTIVIEW-HABITAT. Notably, MIMIC-3M provides substantial improvements in performance on dense geometric and dense object-related tasks even when fewer training samples are used. The method also shows promising scalability, allowing the potential creation of even larger datasets for better model training.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 3, \"Contribution\": 4, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"- Being able to mine large data for pairs is a relevant task in representation learning.\\n\\n- They outperform multiview habitat on their evaluations.\\n\\n- The method, being based on classical techniques like SIFT and RANSAC, should scale well.\\n\\n- The method is simple but effective.\", \"Weaknesses\": \"- (minor) some exposition on what the \\n\\n- This paper might be better suited for a computer vision venue\\n\\n- The paper targets dense vision tasks, but it would be interesting to see the method used to generate pairs for constrastive learning, as well as evaluations on non-dense tasks such as imagenet finetuning/linear probe accuracy.\", \"Questions\": \"Is there any reason this wouldn't be useful for general representation learning beyond dense tasks?\"}"}]}
{"paper_id": "WZfatbNdLV", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Alternative splicing (AS) of pre-mRNA is a crucial, complex process influencing transcript diversity and phenotype, including disease. It results in multiple mRNA isoforms from one gene. Significant efforts have been made to predict or manipulate AS, crucial for uncovering novel regulatory mechanisms or designing therapeutic targets. Historically, splicing code models used manually curated features or deep learning, but limitations remain in accuracy and adaptability.\nGap: Despite advancements, accurately predicting and designing RNA splicing outcomes in a tissue-specific manner remains a challenging task, particularly due to the complexity and variability of splicing mechanisms across different cellular conditions and datasets.\nHypothesis: Using generative model architectures, especially multiple transformers combined with side information as in the TrASPr model, may more accurately predict and manipulate RNA splicing specific to conditions and tissues, outperforming current models and identifying regulatory features.\nMethod: The authors introduce the TrASPr model, using transformers for predicting tissue-specific RNA splicing. The model is pretrained on human RNA splice sites using a 6-mer tokenization strategy and fine-tuned with datasets from human and mouse tissues using MAJIQ for splicing quantification. TrASPr serves as an oracle for a Bayesian Optimization (BO) algorithm designed for RNA splicing outcome design. Evaluations involved comparisons with models like Pangolin on test datasets for PSI and dPSI prediction accuracy, and regulatory elements were tested using ENCODE RBP knockout data.\nConclusion: TrASPr significantly outperforms state-of-the-art models in predicting splice site inclusion and tissue-specific splicing changes, suggests biologically plausible regulatory mechanisms, and holds potential for generating accurate RNA sequence designs. The Bayesian Optimization framework (BOS) leveraging TrASPr can effectively propose sequence alterations for specific splicing outcomes, indicating its utility in experimental and therapeutic applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 5, \"Strengths\": \"While alternative splicing modeling has been extensively studied, systematic methods for designing/altering sequences are relatively new, and the authors' Bayesian optimization method appears promising.  RBP knockdown analysis is also interesting. The authors have also undertaken a significant amount of data processing and batch correction to facilitate model training and validation.\", \"Weaknesses\": \"This manuscript attempts to address two major problems but, unfortunately, does not succeed in either case. Here are the main issues:\\n\\n1. The authors focus on differential splicing and use splicing profiles across multiple tissues and species to train their model. However, they do not present any immediate benefits. For instance, they do not provide information on how much SpliceAI (Jaganathan et al., 2019) loses in performance, or how much Traspr gains, by having a universal (tissue-agnostic) model for splicing. In Section 4.1, the authors themselves acknowledge that \\\"[...] predictions for tissue-specific splicing changes were not very accurate, and we, therefore, did not include them here.\\\"\\n\\n2. The manuscript lacks convincing performance comparisons. It is challenging to believe that Traspr outperforms Pangolin (and subsequently SpliceAI) by such a significant margin (Spearman correlation 0.81 vs. 0.17). SpliceAI and other methods have undergone extensive testing by independent researchers. Furthermore, the approach for calculating cassette exon inclusion in Pangolin, as described in the manuscript, may not be optimal, as it ignores the upstream and downstream exon junctions. A more appropriate method can be found in the Methods section of the SpliceAI paper.\\n\\n3. Traspr appears to excel at predicting constitutive and non-constitutive exons but performs poorly for exons that fall in between (the truly alternatively spliced exons). It might provide more insight if the tested exons were categorized accordingly (e.g., see Fig. 1C in the SpliceAI paper).\\n\\n4. The manuscript lacks a comparison with the AE+MLP model, which is trained only on mouse data and is not considered state of the art.\\n\\n5. The authors only examine Daam1 mutations published in (Barash et al., 2010), while numerous datasets have been published since then, e.g., in (Xiong et al. 2015) and (Jaganathan et al., 2019), among many others.\\n\\n6. When evaluating the Bayesian optimization (BO) method for sequence design, other datasets could be considered, such as the SMA dataset in (Xiong et al. 2015).\\n\\nMinor Comments:\\n- Not all figure panels are labeled, and some figure references are incorrect, e.g., Fig. 1b in subsection 3.1.1.\\n- Some tables have two values in some cells, and it is not clear what these values represent, e.g., in Tables 1 and 2.\\n- Fig. 1 is too small and challenging to read.\", \"Questions\": \"I find the sequence design aspect of this manuscript interesting and promising. I have a couple of questions on that:\\n\\n1. Is the Transformer model truly necessary here? Could the VAE be used to obtain representations and then be fed into the MLP? Essentially, could a single splice junction model be trained instead of a Transformer and a VAE?\\n2. On the same topic, it would be beneficial to understand how a model like SpliceAI, followed by an MLP, would perform as the oracle. I'm not convinced that the Transformer-based splice site detection (with a 400bp context) outperforms SpliceAI with a 10kb context.\\n3. Assuming that a tissue-specific splicing model works (which needs to be demonstrated), can the BO method be adapted to modify AS only in a specific tissue, such as the brain?\"}"}]}
{"paper_id": "YkRwadXWHd", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Sign language recognition, especially continuous sign language recognition (CSLR), is important for facilitating communication within the hearing-impaired community. CSLR is more challenging than isolated sign language recognition (ISLR) since it involves the continuous transcription of sign sequences without clearly defined boundaries. Current methods predominantly rely on video data, but struggle with dynamic hand movements, high data dimensionality, and computational costs.\nGap: Existing CSLR methods typically use dual-stream networks involving video and one other modality such as keypoints or optical flow. These approaches do not take advantage of integrating all three modalities—video, keypoints, and optical flow—together, and they also face significant computational burdens.\nHypothesis: The authors propose that a multi-modal framework combining video, keypoints, and optical flow, along with a hierarchical knowledge distillation framework, can enhance CSLR performance by extracting more representative features while reducing computational costs.\nMethod: The authors propose a multi-modal feature extractor that integrates video, keypoints, and optical flow using several fusion techniques like MLP-based, attention-based, and convolution-based methods. They introduce a hierarchical knowledge distillation (HKD) framework that transfers knowledge from the multi-modal teacher model to a single-modal student model to reduce computational load. They evaluate their approach using datasets like Phoenix-2014, Phoenix-2014T, and CSL-Daily, and use CTC loss for training.\nConclusion: The proposed SignKD approach achieves state-of-the-art performance on the Phoenix-2014, Phoenix-2014T, and CSL-Daily datasets. It demonstrates significant improvements in WER compared to previous methods, both in multi-modal and single-modal settings, validating the effectiveness of the proposed multi-modal integration and knowledge distillation framework.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"- Transferring multi-modal knowledge learned by multiple streams to a single-modality network via knowledge distillation seems to be an innovative and practical idea in the CSLR community. The advantage of this method is that while enjoying multi-modal knowledge from the teacher model, the student model itself does not need to take additional modality inputs such as optical flow and pose, where such extra modalities actually incur computational overhead during inference time.\\n\\n- The presentation for the effect of each proposed component is quite clear from both teacher-side and student-side. For the teacher side, improvements are gradually observed when more modalities are combined, and more fusion layers are applied (Table 3). From the student-side, similar to the teacher model, the performance increases when the knowledge distillation loss terms are applied in feature levels and the output-level (Table 4b). \\n\\n- Compared to the provided previous CSLR methods, the proposed scheme achieves strong results in both cases of multi-modal network and single-modal network in some standard CSLR benchmarks.\", \"Weaknesses\": \"[Major]\\n- Although the overall flow of the technical design is plausible, the proposed SignKD framework combines two existing components well-established in literature (i.e., multi-modal fusion layer and feature-level knowledge distillation), which is not an astounding result and can weaken the technical novelty of the work. As submitted to a top-tier ML conference, further theoretical or intuitive insight on the architectural design would be expected.\\n\\n- The claim for the state-of-the-art performance in CSLR tasks needs to be reconsidered. Specifically for the video-only methods, is the video-based backbone S3D pretrained on video data, or initialized from scratch? While the most performant baseline SEN (Hu et al., 2023c) is based on the architecture with frame-level 2D CNN followed by LSTM, but the proposed work adopts a strong video backbone. Such architectural differences and the following advantages should be acknowledged when comparing performances. Can the proposed work still achieve superior performances even when built upon such frame-level architecture? In addition, for the multi-modal-based experiments, comparison with baseline methods in the same modality condition is missing. For example, how much the performance drop would be observed when the same kind of modalities with baseline methods (i.e., video+pose) are used for training the final multi-modal teacher model?\\n\\n[Minor]\\n- For the additional techniques explained in Appendix C. in detail, missing citation for the sign pyramid network [Chen et al., 2022]. In addition, frame-level supervision has been also previously explored [Min et al., 2021, Hao et al., 2021, Jang et al., 2023]. Further discussions on advantages or differences of the proposed work compared to previous works would be more helpful for potential readers.\\n\\n- Computational aspects for the proposed model are missing. Batch size during training is not provided. In addition, as the training cost for a CSLR model is highly demanding, it would be beneficial to share total training hours and required VRAM memory. Moreover, how many frames the final student model can process during inference (i.e., FPS)? Considering real-world deployment, reporting such quantities and comparing with existing CSLR methods is suggested. \\n\\n- For the ablation results from Table 4(b), one additional row corresponding to the result where only CTC and O are marked is expected. This can be compared with the item where all the feature-level distillation losses are applied (the last row). \\n\\n- Related to the reproducibility, it is unclear whether the codes for training and inference would be released. If there is no plan to open the codes, could you provide a brief reason?\", \"Questions\": \"[Questions, discussions and suggestions]\\n\\n- Considering current standard CSLR benchmarks such as PHOENIX-2014, in my opinion, it would be relatively easy for a model to capture motion information of a signer since it has uniform background and the only component with motion is caused by a human. Related to this, it would be interesting to see the stress-testing result for the already trained model on challenging environments [Jang et al., 2022].   \\n\\n- Presenting sample-level qualitative visualizations such as feature activation map for a signer or gloss-level predictions on some video sequences is suggested, as usually done in CSLR literature. It would be valuable to explore whether such visualizations show different characteristics depending on the single-modal (student) network and multi-modal (teacher) network. \\n\\n- The proposed model includes a lot of loss terms with each weighting factor. Are those weight values sensitive to different datasets? \\n\\n- Inconsistent notations for Table. (i.e., ‘table 3’ in the first line of section 4.3).\\n\\n- Typo: ‘is feed’ in the first line of page 4 below the figure. \\n\\n- What is the meaning of a ‘linear transitional layer’? Is it a standard term?\\n\\n---\\n\\n[Reason for the score recommendation]\\n\\nThis paper has distinctive merit as stated in the Strengths section. However, it is quite unsurprising that the combination existing techniques such as multi-modal fusion and knowledge distillation mechanism can produce strong performance. In addition, it is not sufficient to claim as SOTA at this version. It needs to be more rigorous to compare with previous baselines beyond considering just error rates. I would be more inclined to the positive side when those two main issues are addressed during the rebuttal period. \\n\\n---\\nReferences\\n\\n[Chen et al., 2022] Two-Stream Network for Sign Language Recognition and Translation, NeurIPS 2022.\\n\\n[Min et al., 2021] Visual Alignment Constraint for Continuous Sign Language Recognition, ICCV 2021.\\n\\n[Hao et al., 2021] Self-Mutual Distillation Learning for Continuous Sign Language Recognition, ICCV 2021.\\n\\n[Jang et al., 2023] Self-Sufficient Framework for Continuous Sign Language Recognition, ICASSP 2023.\\n\\n[Jang et al., 2022] Signing Outside the Studio: Benchmarking Background Robustness for Continuous Sign Language Recognition, BMVC 2022.\"}"}]}
{"paper_id": "q4Bim1dDzb", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Inverse rendering seeks to estimate scene properties such as geometry, materials, and illumination from multi-view 2D images. Traditional methods like Neural Radiance Fields (NeRF) adapt these for inverse rendering by implicit neural representations, but require substantial computational resources due to complex mappings involved in separate modeling of each property.\nGap: Implicit learning methods for inverse rendering face low optimization efficiency as they separately model each scene property using deep neural networks, leading to long processing times. Additionally, modeling illumination with environment maps in explicit representation methods remains computationally intensive and struggles with complex real-world lighting conditions.\nHypothesis: The authors hypothesize that a unified voxelization framework, capable of modeling geometry, materials, and illumination concurrently through explicit scene representation, can significantly accelerate inverse rendering.\nMethod: The authors propose UniVoxel, which utilizes a unified voxelization framework to encode scenes into latent volumetric representations. This includes two key elements: SDF for geometry and a semantic field for materials and illumination. The approach leverages lightweight MLP networks to learn these properties efficiently, using Spherical Gaussians for modeling local incident light radiance, integrating illumination into the framework without complex ray tracing. The method was tested on benchmarks with reduced per-scene training time and favorable reconstruction quality.\nConclusion: UniVoxel demonstrates superior optimization efficiency compared to state-of-the-art inverse rendering methods, reducing training time significantly while maintaining high rendering quality. It effectively integrates illumination modeling and supports varying lighting conditions, showcasing its robustness across diverse datasets.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"+This submission proposes a unified grid-based representation that incorporates geometry and material properties. The grid-based representation has shown to be more friendly for optimization.\\n\\n+Optimization becomes more straightforward with shallower MLPs. The combination of grid-based volume representation and shallow MLPs leads to more efficient optimization. \\n\\n+Spherical Gaussian (SG) representation exhibits higher-order frequency characteristics for illumination compared to spherical harmonics (SH).\", \"Weaknesses\": \"-My major concern is that the utilization of feature grids for more efficient training (the major claim) is not a novel insight. For instance, in InstantNGP, it has been demonstrated that the combination of dense hashable grids and shallower MLPs significantly enhances efficiency. Furthermore, the memory concerns highlighted in this submission can potentially be alleviated by adopting more efficient data structures for the grid, such as hashable grids as employed in InstantNGP.\\n\\n-The observed performance improvement is incremental and does not surpass the performance achieved by previous methods in some cases. For instance, a notable limitation of the proposed method is that it often results in albedo estimates that include shading components as shown in Fig.5 first row, in contrast to TenSor RT.\\n\\n-The assertion that SH cannot effectively model higher-frequency illumination may not be conclusive, as it is based on testing only up to order-3 SH. Increasing the order of SH could potentially address this limitation. To provide a more compelling evaluation, it is advisable to ensure an equal number of parameters when comparing SG and SH representations.\\n\\n-Details missing: It is not mentioned clearly how the global illumination and self-occlusion are handled during relighting. Please see questions below for more detailed questions.\", \"Questions\": \"1. For SG-based illumination representation, how many parameters are used for representing the Gaussians? Is it comparable to the SH counterpart? \\n\\n2. How are global illumination and self-occlusion handled during relighting? During scene reconstruction, those could be baked in the SG/SH-based per-point illumination. But during relighting, the per-point environment might should be different.\"}"}]}
{"paper_id": "39cPKijBed", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Diffusion models have made significant advancements, becoming a dominant framework in generative modeling tasks such as high-fidelity image generation, image-to-image translation, and counterfactual generation. These models are typically trained on large datasets, which often contain biases that can affect the quality and fidelity of generated samples. Dataset bias in generative models can perpetuate or even amplify the biases present in the training data, leading to a distribution that does not accurately reflect the intended or unbiased distribution of target outputs.\nGap: The generative outputs of diffusion models suffer from biases present in the datasets they are trained on. Traditional methods to mitigate this include using techniques like importance reweighting, but these approaches encounter issues such as the imprecision of density ratio estimation, which leads to the propagation of errors during model training. There's a need for a more precise method to estimate the density ratio and mitigate these biases effectively.\nHypothesis: The authors propose a novel method utilizing time-dependent importance reweighting for training diffusion models that can effectively mitigate dataset bias. They hypothesize that by using a time-dependent density ratio both for reweighting and score correction, they can achieve a tractable and more precise form of the objective function, which allows the trained model to converge towards an unbiased data distribution.\nMethod: The authors introduce Time-dependent Importance reWeighting (TIW) for diffusion models. This approach involves estimating a time-dependent density ratio between the biased and unbiased distributions using a time-dependent discriminator. They use this density ratio for both weighting and score correction in the model training process. The proposed method uses a stochastic differential equation framework to learn the time-dependent changes in the data distribution, allowing for the mitigation of bias. The method's performance is validated through experiments on datasets like CIFAR-10, CIFAR-100, FFHQ, and CelebA against baselines like time-independent reweighting, demonstrating superior performance in mitigating bias with various dataset settings.\nConclusion: The proposed time-dependent importance reweighting method effectively reduces bias in diffusion model training, outperforming existing baselines. The theoretical and experimental results show that the method converges to an unbiased distribution more reliably, addressing the corruption of generated outputs due to dataset bias. The approach is substantiated by results on diverse datasets with varying bias intensities, highlighting its practical applicability and significance in enhancing the generative quality of diffusion models.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"1. The paper builds on successful recent advances in diffusion models, which excel in high-fidelity image generation within generative learning frameworks. Thus, the proposed TIW method shows promise for diverse applications, including text-to-image generation, image-to-image translation, and video generation.\\n2. The paper addresses the understudied issue of dataset bias in generative modeling, introducing a method to enhance the fairness and reliability of ML systems.\\n3.The paper establishes a theoretical equivalence between the proposed method and existing score-matching objectives from unbiased distributions, which could provide a strong foundation for the validity of their approach.\", \"Weaknesses\": \"1. The paper builds upon a large body of previous work, which while showing the paper’s relevance, also means that any limitations or issues in those previous works could potentially affect the validity and effectiveness of TIW. The actually implementation process is similar to previous work on GAN.\\n2. The paper shows that it mitigates the bias in the dataset, but I am not sure whether that method would influence the overall generation quality. It is important to see detailed qualitative and quantitative analysis.\", \"Questions\": \"N/A\"}"}]}
{"paper_id": "aG3EARrrd1", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the significance of online learning, particularly in areas with large dynamic datasets such as real-time data analysis, online advertising, and financial modeling. Online regression, an area of focus, involves transforming complex tasks into simpler sub-tasks often using RBF methods due to their dimensional independence and convergence properties. Traditional RBF approaches face challenges like determining neuron size and network growth which often lead to inefficient networks or poor generalization performance. Approaches like RAN and GAP-RBF partially address these but separate structure and parameter optimization and often involve complex growth and pruning strategies. There is a growing interest in hybrid models integrating tree structures and RBF networks to enhance capacity and simplicity.\nGap: Existing RBF-based online learning methods either lack efficiency in handling large, streaming datasets or have challenges balancing network growth and generalization capability. While recent hybrid approaches attempt to address these, they often separate structure determination from parameter optimization, leading to potential inefficiencies. Additionally, the limitations of RBF networks in fitting complex target functions highlight a need for a model that integrates tree-based structures for better adaptability.\nHypothesis: The authors propose the Online Sparse Residual Tree (OSRT), a model integrating tree decomposition and adaptive radial basis function exploration, as a more efficient and accurate method for handling streaming multivariate scattered data. They suggest that OSRT can dynamically and appropriately expand and refine network components to maintain model sparsity and accuracy, thereby providing improved efficiency over traditional RBF and hybrid methods.\nMethod: OSRT utilizes online tree decomposition and adaptive RBF exploration to manage data streams effectively. The model grows the tree and refines each child node with RBF networks in response to new data, minimizing residual errors. It employs an incremental method to adaptively determine RBF centers, ensuring a sparse model while updating its structure to incorporate trends in streaming data. The adaptive RBF exploration intelligently adds nodes when necessary and prunes them based on significance and performance evaluation metrics. The model’s efficacy was tested on datasets, comparing its performance against existing online RBF approaches, and it included methods like quasi-uniform subsequence generation and adaptive function termination.\nConclusion: The OSRT model demonstrated higher prediction accuracy and efficiency compared to existing online RBF networks. This was validated using benchmark time series datasets like Mackey-Glass and Lorenz time series. The model optimizes both tree depth and RBF parameters concurrently, showing adaptability in evolving data tasks. The paper concludes that OSRT is both a feasible and efficient technique for online learning in scenarios with large and dynamic datasets.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"The paper presents several novel ideas, by combining RBF networks, sparse regressison trees, and online updating for time series prediction.\", \"Weaknesses\": \"The paper lacks a principled approach to model design and evaluation, appearing to have little rationale in the combination of methods used beyond their adaptation from the recent literature, and their apparent heuristic value.  Typically one would expect a cross validation step as part of the algorithm when complexity parameters or thresholds are called for in a model.  \\n\\nThe exposition is hard to follow at best, and at times incomplete, or the symbols are incorrect. \\nFor instance, in Section 2: \\n\\n- Gaussian kernel is designated \\\\theta, but in the approximation the character \\\\phi is used -- are these the same thing? Note that \\\\theta is reused with a different meaning in Equation (17).\\n\\n- After equation (2) the phase \\\"Where Nχ is the number of neurons in this node, δl is the shape parameter.\\\" makes no sense since neither variables appear in the previous formula.  The rest of that paragraph has similar problems with reference to variables not introduced in the equations that it attempts to explain. \\n\\nIn general, one needs a principled method for determining the complexity of the model, e.g. the number of nodes, such as cross validation, or use of complexity penalty terms, e.g. in BIC. Is the maximum tree depth (Section 2.2) something one calculates, or is it a parameter one setd? Simply considering when \\\"the increase in the number of nodes no longer yields significant improvements in approximation quality\\\" will lead to overfitting. \\\"Significant improvement\\\" is not a principled method.\", \"Questions\": \"There are many terms introduced in the explanation of the model that are introduced but not explained:  Could you describe the tree in terms of its layers?  What is the \\\"split rule\\\" and what is the stopping condition referred in the paragraph following Equation (2)?  In what sense is it sparse? Is there a sparsification step?  What do you mean in Section 2.1 by \\\"quasi-uniform\\\"?  Are your \\\"mean points\\\" C the same as your centers? Honestly this as far as I got in the text.\"}"}]}
{"paper_id": "Ts95eXsPBc", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The current primary approach for modeling episodic memory in AI systems is through transformers that store temporally ordered experiences. This method overlooks the spatial dimension, which is significant in cognitive processes as suggested by cognitive science.\nGap: The current implementation of episodic memory using transformers does not incorporate spatial information, making it unclear how the underlying structure can be extended to include the spatial axis and what benefits this could provide.\nHypothesis: Incorporating spatial information into transformer models will enable the creation of a place-centric episodic memory that considers both temporal and spatial dimensions, potentially improving memory utilization efficiency and enhancing accuracy in place-centric tasks.\nMethod: The researchers developed Spatially-Aware Transformers (SAT) that incorporate spatial information and introduced an Adaptive Memory Allocator (AMA) based on reinforcement learning to optimize memory utilization. They conducted experiments across various environments and tasks such as prediction, generation, reasoning, and reinforcement learning to demonstrate the effectiveness of these models.\nConclusion: Spatially-Aware Transformers improved spatial reasoning tasks and memory efficiency, showing advantages across multiple machine learning tasks. The introduction of the Adaptive Memory Allocator allowed flexible memory management strategies beyond traditional approaches, confirming the potential benefits of incorporating spatial information into episodic memory models.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"- This paper studies the important problem of utilizing and managing spatial and temporal memory experienced by embodied agents that are essential to address various downstream tasks. Particularly, it considers the practical issue of memory constraint and perform experiments based on the popular transformer architectures. The research presented in this paper is very well motivated.\\n\\n- The paper is very technical solid. Almost all arguments are well-supported/justified by the highly-relevant and classic references and thorough experiments (in Appendix). It carefully compares place- and time-centric store and read, and progressively studies FIFO to the proposed Adaptive Memory Allocator based on SAT. I believe the extensive settings and results presented in this paper have the potential to inspire many future works.\\n\\n- Overall, this paper was a very enjoyable read to me. All details have been clearly presented (especially with the Appendix and all nice visualizations); The paper is nicely-structured, concise but contains massive valuable information. I believe many arguments and thoughts presented in this paper will be very constructive to relevant future research.\", \"Weaknesses\": \"- The title of this paper is very misleading\\n    - The paper does not propose any new formation of the transformer architecture to better model spatial information, but focuses on managing agents' episodic memory for addressing different downstream tasks. \\n\\n- This paper overclaims several contributions.\\n    - The paper says \\\"we are the first to motivate, conceptualize, and introduce the notion of transformers capable of utilizing explicit spatial information\\\", but as the authors mentioned that defining and managing external memory have been extensively studied in previous machine learning literatures. It simply compares place- and time-centric store and hierarchical read methods that are intuitively suitable for different downstream tasks.\\n    - One good example is the use of topological graph that stores observations of keypoints for agents in visual navigation, e.g., the widely applied DUET agent [1] for vision-and-language navigation [2], which is essentially close to the proposed SAT-PM-PH model. \\n    - The experiments of action-conditioned image generation (Exp-5) and reinforcement learning agents (3.3) are not very convincing to me. Exp-5 is not a practical setting and 3.3 is relatively simple that cannot represent other reinforcement learning agents, see more below.\\n\\n- Missing experiments.\\n    - I think this paper lacks investigation on the more recent embodied agents and their memory management approaches. \\n    - The experiments presented in this paper are relatively small-scale and simple. I am concerning how the proposed methods might impact recent research that often apply more capable networks (and massive data) to learn generic spatiotemporal priors to facilitate representing and memorizing the observations. e.g., Figure 3 - an agent might be able to create a very compact representation for all ballet rooms from a single visit to each room? \\n    - Following my previous point, I think this study emphasizes the scenario of memory constraint, but the experiments only considers small memory capacity, negelating the difference in representation (i.e., how to store an observation) and the difficulty in grounding from the queries to relevant memories. Overall, the generalization and practical influence of the proposed methods is not very clear to me.\\n    - I think a valuable baseline is missing here: without explicitly defining any memory management strategies but gives the agent a certain memory budget and asks it to learn to update the memory by itself by training the agent on a mixture of data and tasks (e.g., a more general version of DNC mentioned in Appendix D.4).\\n\\n[1] Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language Navigation. Chen et al., CVPR2022.\\n\\n[2] Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments. Anderson et al., CVPR2018.\", \"Questions\": \"I hope the authors can address some of my concerns in Weaknesses. I don't have any other questions here.\"}"}]}
{"paper_id": "LfhG5znxzR", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Understanding neural networks is difficult because their hidden states are dense and continuous, making it hard to decipher their operations. Traditional software systems have sparse and discrete states, providing clarity and interpretability which neural networks often lack. Prior efforts have struggled with the superposition of features and the complexity arising from continuous activations.\nGap: The problem addressed is the lack of interpretability and control in neural networks due to their dense and continuous hidden states, which hinder understanding and manipulation of model behaviors.\nHypothesis: Introducing sparse and discrete bottlenecks, termed codebook features, into neural networks can enhance interpretability and controllability without significantly degrading performance. These bottlenecks use vector quantization to represent activations as discrete codes from a large codebook, making features sparse and interpretable.\nMethod: The authors propose a method involving finetuning neural networks, particularly Transformers, with trainable vector quantization bottlenecks at each layer. The method refines the network to replace continuous activations with a sparse combination of code vectors from a codebook. They apply these codebooks to the hidden layers of neural networks including a finite state machine dataset and natural language datasets, examining the assignment of network states to discrete codes and the causal impact of codes on network outputs. They use cosine similarity to select the most representative codes for activation, and validate their approach by examining the interpretability and control of these codebook features across tasks.\nConclusion: The method successfully creates neural networks with hidden states that are sparse and discrete, which enhances interpretability and provides a mechanism for controlling network behavior. The proposed codebook features enable networks to operate effectively with sparse representations, allowing them to maintain performance while providing intuitive ways to manipulate and understand their operations. This approach opens pathways for further interpretability and control in broader neural network architectures.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"* This work considers the problem of interpreting the intermediary layers and controlling the output of transformer networks, which are of significant interest to the machine learning community. Furthermore, quantizing features with codebooks is a popular technique which the work demonstrates leads to a minimal degradation in model performance.\\n* Experiments with the TokFSM dataset are intuitive and clearly demonstrate the ability to intervene in the output of a transformer trained with codebook quantization. Experiments such as the JS divergence with the target token distribution in Figure 4, are used to demonstrate effective intervention. Given that one has access to the semanticity of entries in the learned codebook, the proposed method seems to be effective at steering the output of transformer network.\\n* The work contains a comprehensive related works section in the appendix that clarifies the benefits of a discrete codebook over using a dictionary approach (referred to as “features-as-directions”)\", \"Weaknesses\": \"* One shortcoming of the experiment in table 1 is that it has two dependent variables: both the quantization level k and the codebook size C. It is important to ablate changes to these two variables separately to better understand if they both independently provide benefits. There is a similar issue of two dependent variables being tested at the same time in Table 2 where both k is modified as well the features being quantized (i.e., attention vs mlp features). \\n* The benefit of sparsity in the combination of codebook entries has not been clearly articulated. In fact, in section F.1.1 of the manuscript, it is argued that the continuous combination of different atoms reduces interpretability. Selecting codebook entries via top-k cosine similarity is a naive approach that has not been compared to more recent sparse coding techniques (e.g., variational sparse coding methods like in Tonolini et al. 2020 and Fallah et al. 2022). Except for the potential increase in modeling capacity, the work does not demonstrate the benefit of quantizing with multiple codebook entries per token.\\n* Interpretation of codebook entries still seems to require manual intervention. It requires a user to find input data for which a codebook entry is often activated, which can be timely and costly. It is unclear how one would use current methods to find a codebook entry that corresponds with a certain semantic concept without manually performing a forward pass using data corresponding to that concept.\\n\\n\\nMinor:\\n* Some citations need revising in the bibliography (e.g., “On the role of scientific thought”).\", \"Questions\": \"* Can the authors clarify the benefit from increasing k? Since the authors use cosine similarity between the activations and the codebook to pick the top-k entries, what would the difference be between each of these k codebook entries? It seems that taking k entries contradicts the viewpoint of “features-as-points”, and may even lead to what the authors refer to as “smuggling of information”. I would expect that increasing k may improve performance of the model, at potential cost to interpretability.\\n* Out of curiosity, have the authors considered quantizing each vector along the feature components (i.e., divide the N features of each token into k blocks)? Is this what the authors refer to as “grouped codebooks” in E3? If so, I believe this warrants discussion and attention in the main text. This would be an alternative to quantizing each token with k codebook entries that are very close in cosine similarity and be closer to the VQ-VAE setting.\\n* The notation and presentation of the different loss terms in section 2.1 can be made more clear. In the cross-entropy loss, x is used to denote a categorical random variable corresponding to a token being selected. In the reconstruction loss, to my understanding, x is referring to a continuous random variable corresponding to the activation in an intermediary layer, even though the variable a is used in an earlier section. Furthermore, k-codes are combined to quantize each activation. Is the MSE taken with the sum of these codes or each code individually?\"}"}]}
{"paper_id": "bUGzjiUsIq", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Traditional multi-label classification relies heavily on fully annotated datasets, which are difficult and often impractical to obtain, especially in domains requiring specific expert knowledge. Incomplete annotations are common in multi-label tasks due to the large number of possible labels and complexities involved in providing full annotations.\nGap: The problem addressed in this paper is learning from partially annotated multi-label classification datasets where only a subset of positive classes is annotated, leading to issues of scarcity in positive annotations and severe label imbalance.\nHypothesis: The authors propose that by integrating reinforcement learning with partial annotation using a Policy Gradient (PAPG) approach, they can effectively overcome the limitations of traditional methods and improve performance in partially annotated multi-label classification by balancing exploration and exploitation.\nMethod: The authors introduce PAPG, a framework that combines the exploration strengths of reinforcement learning with the exploitation capabilities of supervised learning. The model is structured as a Markov Decision Process (MDP) with local and global rewards. The local reward comes from a value network trained on annotated data and the global reward is based on recall metrics. The training process involves iterative updates between a policy network and a value network, using data enhancement and an adaptive threshold to adjust training data.\nConclusion: The proposed PAPG framework effectively handles the challenges of partially annotated multi-label classification, demonstrating superior performance over traditional methods and baselines. The framework shows robustness and generalizes well across different datasets and domains, substantially improving F1 scores and other metrics compared to existing methods.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The problem, partially annotated multi-label classifcation, is novel.\\n\\n2. A novel method named PAPG is proposed, which can overcome the challenges associated with a scarcity of positive annotations and severe label imbalance. \\n\\n3. Experiments validate the effectiveness of the proposed PAPG method.\", \"Weaknesses\": \"1. The motivation might be unreasonable. As stated in abstract, \\\"This task encounters challenges associated with a scarcity of positive annotations and severe label imbalance\\\". These challenges are indeed problem to deal with, but in my opinion, the main challenges correspond to that the negative samples are unknown or we do not know which are negative samples among the remaining unlabeled samples. In other words, the main challenges are the same as PU learning.\\n\\n2. For the two challenges mentioned in this paper, i.e., a scarcity of positive annotations and severe label imbalance, it is unclear how can the proposed PAPG method overcome them.\\n\\n3. As stated in Introduction, \\\"Consequently, many advanced methods of PU learning (Su et al., 2021; Acharya et al., 2022; Luo et al., 2021) cannot readily adapt to our multi-label settings\\\". In my opinion, if we focus each label one by one, it is a PU learning problem. Thus, the partially annotated multi-label classifcation problem can be solved via binary relevance strategy where the base classifier is trained with any off-the-shelf PU learning methods.\\n\\n4. For multi-label classification, it is very important to model the correlations among labels. But it is unclear how can the proposed PAPG method model label correlations.\\n\\n5. The writtting can be greatly improved. For exampe:\\n(1) The title is inappropriate.\\n(2) Errors exist in notations (especially the first paragraph in section 3.1).\\n(3) What does RARL mean in the last sentence of this paper?\", \"Questions\": \"If author disagree with my comments in Weaknesses, please clarify them in the rebuttal phase.\"}"}]}
{"paper_id": "uMAujpVi9m", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Pocket representations are crucial in biomedical applications such as druggability estimation and ligand affinity prediction. Traditional handcrafted features and data-driven deep learning methods have shown potential but are limited by the lack of extensive protein-ligand complex data for large-scale pretraining.\nGap: The main gap is the scarcity of protein-ligand complex structures available in databases like PDB, which restricts the large-scale pretraining needed for effective modeling of pocket-ligand interactions.\nHypothesis: The authors propose that by pretraining pocket representations using segmentations of high-resolution atomic protein structures into drug-like fragments, and aligning these with pseudo-ligand representations from pretrained small molecule encoders, they can better simulate ligand-receptor interactions and overcome the data scarcity problem.\nMethod: The authors introduce ProFSA, a method that first segments protein structures into drug-like fragments and corresponding pockets to simulate ligand-receptor interactions, generating over 5 million pseudo-complexes. These are used to train a pocket encoder contrastively against pseudo-ligand representations from a fixed pretrained small molecule encoder. The process also involves careful data distribution alignment to mimic real ligand interactions.\nConclusion: ProFSA significantly outperforms existing methods on tasks like pocket druggability prediction, pocket matching, and ligand binding affinity prediction, demonstrating the potential to mitigate the protein-ligand data scarcity issues and opening up avenues for using diverse protein structure databases for pretraining.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"1. The authors present a novel pairwise data synthesis pipeline by extracting pseudo-ligand-pocket pairs from protein data\\n2. The authors develop large-scale datasets and new pretraining methods to exploit the full potential of pocket representation learning, emphasizing the interactions between pockets and ligands.\\n3. ProFSA achieves significant performance gains in a variety of downstream tasks.\", \"Weaknesses\": \"1. The evidence for the construction of the pseudo-ligand is not clear.\\n2. Ablation studies evaluating the impact of critical design choices like fragment sizes, distance thresholds for pockets would provide useful insights.\\n3. While terminal corrections are applied to address biases from breaking peptide bonds, the pseudo-ligands may still exhibit substantial discrepancies from real drug-like ligands.\", \"Questions\": \"1. Why do the authors choose peptides to replace small molecules, and is this choice reliable? Have the authors considered other potential ways to further close the gap between pseudo-ligands and real ligands, either through data processing strategies or by fine-tuning on downstream tasks?\\n2. Section 3.1, second paragraph, line 4, what do the N Terminal and C Terminal refer to?\\n3. Why fixed the molecule encoder in contrastive learning, i.e., the encoder that encodes the pseudo-ligand.\\n4. Could ProFSA be extended to other tasks like protein-protein interaction prediction? How might the pipeline and contrastive approach need to be adapted?\"}"}]}
{"paper_id": "uqxBTcWRnj", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the integration of neural networks and symbolic systems, highlighting the recent trend of incorporating symbolic interpretability, compositionality, and logistics into neural networks. This is motivated by cognitive science studies suggesting that symbolic representations in the human brain transition from neural perception and develop through interaction with the environment. Traditional neural networks rely heavily on distributed representations, while symbolic systems use explicit structural representations to illustrate inputs.\nGap: Current methods integrating neural and symbolic systems do not fully exploit symbolic power and primarily enable them to work synergistically rather than truly combining the strengths of both. There exists an essential disparity between low-level distributed representations of neural networks and the explicit symbolic representations of symbolic intelligence. Moreover, symbol grounding, which links predicate representations to visual components, remains challenging and often requires labor-intensive supervised learning.\nHypothesis: The authors propose that a transitional representation system, arising from unsupervised learning, can bridge the gap between neural and symbolic representations. This system involves creating intermediate representations that both maintain high-dimensional details and convey structural information about input semantics. By achieving this, it becomes possible to implicitly learn symbolic knowledge such as visual parts and their relations from visual observations.\nMethod: The paper introduces a Transitional Dictionary Learning (TDL) framework, which involves learning dictionaries using an Expectation Maximization (EM) algorithm through online prototype clustering. This is coupled with a game-theoretic diffusion model to decompose visual input into parts. The framework aims to learn reusable and compositional predicates. Additionally, new metrics, clustering information gain and heuristic shape score, are proposed for model evaluation. The method was tested on three compositional visual object datasets to demonstrate its capability of learning symbolic features without relying on predefined visual features like colors or textures.\nConclusion: The experiments show that the TDL framework effectively discovers compositional patterns, significantly outperforming other unsupervised part segmentation methods that depend on visual features from pre-trained models. The introduced metrics align well with human evaluations, illustrating improved interpretability. This research takes a significant step towards developing neural networks that integrate symbolic reasoning capabilities, supporting the hypothesis of a seamless transition from neural to symbolic representations.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 3, \"doi\": 10, \"Strengths\": \"The paper is well-written and easy to understand.  There are good explanations for each step of the approach.  The \\\"Transitional Representation\\\" section does a really good job of approaching the symbolic and neural representations.  \\n\\n\\nThe method is topical and will be of interest to the ICLR community and the method seems to be novel for how to produce a dictionary of neuro-symbolic part segmentation.  \\n\\nI really like the overarching goal for self-supervised part segmentation and the method seems to attack the problem directly.  The neural symbolic approach to ML has been of interest for a while and part segmentation is a good problem to apply it towards.\", \"Weaknesses\": \"The biggest disappointment was not doing this on real visual data rather than on LineWorld data.  This is still useful with just LineWorld but showing on realworld data would be much more impressive.  \\n\\nRunning human evaluations requires an IRB or something similar not mentioned here.   This needs to be stated (anonymously) that you did actually go through someone to ensure the human experiments were done properly.  \\n\\nFor the “Compositional Representation” related work, please add references to older approaches such as Bag of Words such as:\\nL. Fei-Fei and P. Perona, \\\"A Bayesian hierarchical model for learning natural scene categories,\\\" 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05), San Diego, CA, USA, 2005, pp. 524-531 vol. 2, doi: 10.1109/CVPR.2005.16.\\n\\nCsurka, Gabriella, Christopher Dance, Lixin Fan, Jutta Willamowski, and Cédric Bray. \\\"Visual categorization with bags of keypoints.\\\" In Workshop on statistical learning in computer vision, ECCV, vol. 1, no. 1-22, pp. 1-2. 2004.\\n\\nThe citations needs to reference the actual venue such as this one should not just refer to Open Review (be wary of using automated citations):\\nYann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62, 2022.\\n\\nFormular 1 -> Equation 1\", \"Questions\": \"For Figure 3, could you compare a more conventional approach to compare against to see if this approach is causing it to be separated verse just from the data?  \\n\\nHave you tried this on more complex data 2D images?\\n\\nCan you elaborate on exactly what the human criteria were that they were evaluating?\"}"}]}
{"paper_id": "o4CLLlIaaH", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Neural Radiance Fields (NeRF) have seen significant success in 3D scene representation and novel view synthesis, mainly using image-based neural rendering techniques combined with multiview stereo (MVS) methods. Recent advancements have aimed at making NeRF generalize without per-scene optimization, integrating approaches like PixelNeRF, IBRNet, and MVSNeRF.\nGap: Existing generic NeRFs face issues with feature inconsistencies from occlusions, distortions due to standalone sampled point processes, and degradations when source and target views are not aligned well. Image-based methods are limited in their capacity to interact and modify scenes.\nHypothesis: The authors propose shifting from an image-based to a point-based rendering approach for generalizable neural radiance fields to solve the issues of occlusion, poor geometric handling, and lack of interactivity in representation.\nMethod: The paper introduces the Generalizable neural Point Field (GPF), which includes several innovations: visibility-oriented feature fetching based on geometric priors; a nonuniform log sampling strategy to enhance rendering speed and quality; and a feature-augmented learnable kernel for effective local feature aggregation. The method is experimentally validated on NeRF synthetic, DTU, and BlendedMVS datasets in both generalization and finetuning settings.\nConclusion: The proposed GPF outperforms traditional image-based approaches in terms of rendering quality, geometry correctness, and view consistency. It shows potential in enabling scene interaction and achieves significant enhancement over benchmarks in both generalization and finetuning scenarios, thus supporting the validity of point-based representation for generalized neural radiance fields.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"- Originality: The Generalizable Neural Point Field (GPF) is a fresh perspective in NeRF research, emphasizing point-based over image-based rendering.\\n- Quality: The proposed methods exhibit high-quality research and innovation, from the nonuniform log sampling strategy to the feature-augmented learnable kernel.\\n- Clarity: The paper's overall structure is clear, presenting a logical flow of ideas and discussions.\\n- Significance: If the claims are validated further, this research could serve as a benchmark in the NeRF domain.\", \"Weaknesses\": \"- Lack of Detailed Explanations: Some sections, especially the technical components, could use more in-depth explanations or visual aids.\\n- Dependency on Other Technologies: The initial dependency on PatchmatchMVS might limit the paper's approach from being a standalone solution.\\n- Limited Experimentation: Testing on only three datasets might not showcase the full potential or limitations of the method.\\n- Complexity: The approach's intricate nature might pose scalability or efficiency challenges that haven't been addressed comprehensively.\", \"Questions\": \"- Could the authors expand on the visibility-oriented feature fetching, possibly with diagrams, for better clarity? Figure (b) alone is not clear enough.\\n- Given the reliance on PatchmatchMVS for the initial point scaffold, how does this affect the scalability or deployment of GPF in diverse scenarios? The authors also mention in the limitation section that they want to propose a NeRF-based initialization module that can be trained from scratch. Please comment on how you plan to achieve this.\\n- Can the authors comment on potential efficiency challenges due to the method's complexity? For example, compare the rendering speed with E-NeRF.\"}"}]}
{"paper_id": "KNzL1nglNB", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Empirical Risk Minimization (ERM) is a fundamental principle in machine learning that uses class labels as supervision information to guide the learning of labeled samples, achieving great success in many applications. However, it heavily relies on labeled data, which poses a challenge in scenarios with limited or no labeled samples but abundant unlabeled samples. This is common in real-world applications like industrial defect detection, where labeled data can be expensive and time-consuming to create. Existing methods such as entropy minimization (EntMin) and techniques in semi-supervised learning (SSL) and unsupervised domain adaptation (UDA) aim to fully utilize unlabeled data to improve generalization but have limitations in maintaining prediction diversity and avoiding bias towards dominant classes.\nGap: In scenarios where labeled data is insufficient, the reliance of ERM on labeled samples makes it suboptimal. Existing methods like EntMin tend to suffer from decreased prediction diversity, pushing unlabeled samples toward dominant categories. Thus, there's a need for an approach that effectively utilizes unlabeled samples while maintaining both prediction discriminability and diversity.\nHypothesis: The authors propose that a Label-encoding Risk Minimization (LRM) approach, inspired by the phenomenon of neural collapse, can be used to address label insufficient scenarios. By estimating and aligning label encodings for unlabeled samples, LRM can potentially enhance prediction discriminability and diversity, serving as an effective extension to ERM for scenarios with limited labeled data.\nMethod: The paper introduces LRM, which aims to generalize ERM for unlabeled samples by utilizing label encodings as guidance information. The approach involves calculating prediction means (i.e., estimating label encodings) for unlabeled samples and minimizing their discrepancy with ground-truth label encodings. Extensive experiments were conducted in various label insufficient scenarios, including SSL, UDA, and semi-supervised heterogeneous domain adaptation (SHDA). LRM was applied as a plugin to enhance the generalization performance of existing models in these scenarios. The paper also includes theoretical analysis illustrating the relationship between LRM, neural class-mean collapse, and ERM, and compares it against EntMin and other existing methods.\nConclusion: The proposed LRM significantly improves performance in scenarios with insufficient labeling compared to traditional ERM and other existing methods like EntMin and BNM. LRM was effective in maintaining prediction diversity and reducing the dominance of large categories. The theoretical analysis demonstrated a link between label encoding and neural collapse, while empirical evaluation on multiple datasets in SSL, UDA, and SHDA scenarios validated its effectiveness. The results highlight LRM's potential as an effective solution in tackling label insufficient situations, paving the way for its application in other areas with similar challenges.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"- The authors have demonstrated the cause of NCC through the use of label encoding and have further enhanced it to improve network performance. \\n- The evidence supporting NCC and LRM is explicitly presented.\\n- Experimental results show that LRM achieves good performance in various tasks such as semi-supervised learning (SSL), unsupervised domain adaptation (UDA), and semi-supervised heterogeneous domain adaptation (SHDA).\", \"Weaknesses\": \"- Although authous conducted experiments on a variety of tasks, for each task they only ran experiments on specific datasets. For example, for UDA, they only ran experiments on Office-31 and did not evaluate on many other UDA datasets.\\n\\n- Authours also do not have experimental results on large scale data. It is not confirmed that their algorithm is useful when training a model from scratch on large scale training data.\", \"Questions\": \"- Can we say that domain adaptation is a situation where there are not enough labels? A typical domain adaptation is a setting where there are no target labels at all, and the amount of labels in the source data is irrelevant. What does it mean to have insufficient labels in this situation?\\n\\n- Is there a reason why you didn't perform experiments on a large scale dataset such as ImageNet?\"}"}]}
{"paper_id": "pvhyBB86Bt", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Deep neural networks (DNNs) are powerful tools in machine learning but suffer from training difficulties due to issues like vanishing or exploding gradients, particularly as depth increases. This issue is intrinsically linked to the behavior of the network's Jacobian matrix, which represents the sensitivity of output predictions to input alterations. Prior research has highlighted the importance of initialization schemes for stabilizing the Jacobian, with works such as Pennington et al. (2017) focusing on this aspect at initialization using random matrix theory.\nGap: Existing studies primarily address the stability of the Jacobian at the initialization phase and do not explain its behavior during training or under common scenarios like network pruning, leaving a gap in understanding how Jacobian stability is maintained after initialization and during training.\nHypothesis: The hypothesis of this study is that if a neural network's Jacobian is stable at initialization, it remains stable throughout training. This suggests that proper initialization plays a crucial role in ensuring the stability of the Jacobian throughout the entire training process.\nMethod: The authors extend traditional analyses by utilizing recent results from random matrix theory, particularly those by Brailovskaya and van Handel (2022), to propose a general stability theorem for the Jacobian that covers training. They analyze the behavior of the Jacobian matrix during training using Multi-Layer Perceptrons (MLPs) as a model and investigate its behavior in sparse networks and under non-iid weight initialization.\nConclusion: The study concludes that if a network's Jacobian is stable at initialization, it remains stable through training, which aligns initiation phase with overall training performance. The paper further identifies phases of Jacobian behavior during training, providing evidence for the stability of sparse networks when weights are suitably scaled after pruning.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"The subject explored in this paper is of profound interest. It poses an essential and inherent inquiry regarding the maintenance of stability for the Jacobian matrix in neural networks from initialization throughout training. This is the inaugural study on this particular topic.\\n\\nThe manuscript is well written and has a clear structure.\\n\\nThe paper employs novel methodologies to examine the spectrum of the Jacobian matrix. The findings from Bandeira et al. (2021) are indeed aptly integrated into the analysis of neural networks and warrant recognition within the learning theory community.\", \"Weaknesses\": \"Assumption 2 appears to be quite stringent, as it presumes certain conditions throughout the entire training process. While there is empirical support provided, it lacks a solid theoretical basis, and the given intuitive reasoning doesn't resonate with me. This assumption seems crucial for this study.\\n\\nRegarding Theorem 3, I'm not entirely certain, but my interpretation is that it essentially says: \\\"In the initial stages of training, we are in the kernel regime with a stagnant feature, leading to a static norm of the Jacobian.\\\" Is the challenge in proving this mainly technical? That's how I currently see it. Furthermore, the example in Theorem 4 seems overly simplistic. Perhaps analyzing it using multi-dimensional input in deep linear networks would be more illuminating? Lastly, the assertion that gradient-based training inherently pushes the network towards smaller gradients, causing the Jacobian norm to decrease, doesn't sit right with me. In the end, even with a zero gradient, the Jacobian norm remains non-zero.\", \"Questions\": \"Why haven't you detailed the RMT tools you used in the main content? Although you mention using new tools in the abstract, there isn't sufficient explanation in the main body of the text.\"}"}]}
{"paper_id": "Mylk5iamJC", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The classic recognition problem assumes that all test classes are known during training, known as closed-set recognition (CSR). Open-set recognition (OSR) requires models to reject unknown classes not encountered during training. Discriminative models struggle with OSR due to the absence of unknown samples, leading to methods that focus exclusively on either CSR or OSR, compromising the other.\nGap: Existing methods are often optimized for CSR or OSR but not both, due to difficulties in learning decision boundaries without unknown samples in OSR. This results in poor performance trade-offs between CSR and OSR.\nHypothesis: The paper proposes that CSR and OSR share the same goal when approached with generative models, as both can be minimized using maximum likelihood estimation on known class training data. Therefore, a single latent generative model can address both problems simultaneously.\nMethod: The authors introduce the Latent Gaussian Mixture Model (L-GMM) with a collaborative training algorithm. This model uses an encoder to map inputs to a latent space and a density estimator to compute probability densities via a neural Gaussian Mixture Model. The end-to-end training algorithm separates and synchronizes the generative learning (density estimator) with maximum likelihood estimation and the discriminative learning (encoder) using a loss derived from the generative model. This framework is tested on image recognition and segmentation tasks for both CSR and OSR, demonstrating superior performance to discriminative counterparts and existing specialized methods.\nConclusion: L-GMM achieves better closed-set recognition performance than discriminative models when trained from scratch, and also excels in open-set recognition without needing modifications or prior knowledge. The proposed method effectively combines generative and discriminative modeling, offering a unified approach to address both CSR and OSR problems.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"The solution of using latent Gaussian mixture model in the encoder to tackle the close-set recognition is valid.\", \"Weaknesses\": \"(1) The novelty of the paper is pretty limited as similar ideas for using Gaussian mixture model in variational inference to tackle open-set classification has been studied and published.\\nFor instance, \\\"Open-Set Recognition with Gaussian Mixture Variational Autoencoder\\\" by A. Cao at in AAAI 2021\\n\\n(2) Compared to the previous work as mentioned, this paper did not provide any new insight and fails to clarify the distinction with the similar paper mentioned above.\\n\\n(3) The derivation of latent Gaussian mixture model is very common and similar derivation has been shown in previous published work including \\nhttps://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=5fda8f8bd7ff10db1ee3ab558fd41e552f008fb3\\n(i)\\\"Robust Estimation of Gaussian Mixtures from Noisy Input Data\\\", CVPR 2008\\nand \\n(ii) https://ieeexplore.ieee.org/abstract/document/9523628, TNNLS 2023 \\\"Robust Semisupervised Deep Generative Model Under Compound Noise\\\"\\nThese papers have clearly shown that Gaussian mixture model can effectively handle the outliers and noisy labels as well as tell samples from unknown class for recognition in generative model including VAE and GAN. In this case, it is not clear if there is really new insight and contribution that the current manuscript conveys. Modules including discrimination loss, density estimators have also been well studied in the previous work. If we look at the math, there is really very little difference except small tweaking.\", \"Questions\": \"It is important to increase the novelty by providing new insights with new approaches instead of providing small extensions with existing method.\\n\\nIt is not clear what is the main contribution of this paper as the novelty is too limited and the discussion with the existing methods is also important and necessary.\\n\\nIt is necessary to show the distinction of the current paper from the previous work shown in 3(i) and 3(ii) in order to stand on the safe ground in terms of novelty.\"}"}]}
{"paper_id": "MsOcVFzv8D", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Multi-domain text classification (MDTC) is an approach in supervised machine learning where the goal is to use available resources from various related domains to improve classification accuracy in a target domain, particularly when labeled data in the target domain is scarce compared to related domains. Techniques like adversarial training and the shared-private paradigm have been used for MDTC to achieve state-of-the-art performance. These methods focus on creating domain-invariant features that can be transferred across domains. Domain adaptation strategies, including statistical matching and discrepancy minimization, have also been effectively applied to this end.\nGap: Most current MDTC methods lack theoretical guarantees, which poses a significant obstacle to their development and improvement. The existing methods largely develop their effectiveness based on empirical observations rather than solid theoretical foundations. While techniques like f-divergence provide some theoretical backdrop, a broader generalization bound for MDTC is missing.\nHypothesis: Incorporating margin discrepancy, a metric used to measure domain divergence, into the framework of adversarial training for MDTC can provide a robust theoretical foundation and improve model performance. The extension of this theoretical analysis can bridge the existing gap between theory and practical algorithm development in the field of MDTC.\nMethod: The authors propose a method called Margin Discrepancy-based Adversarial Training (MDAT) for MDTC, guided by theoretical analysis. They first conduct a theoretical decomposition of the MDTC task into multiple domain adaptation tasks, using margin discrepancy to measure domain divergence. A new generalization bound for MDTC based on Rademacher complexity is formulated. Following this theoretical framework, the MDAT method is developed. It involves a shared feature extractor, domain-specific extractors, and two classifiers, where adversarial training is employed to use margin discrepancy for minimizing domain divergence. Empirical validation was done on two MDTC benchmarks, using the Amazon review dataset and FDU-MTL dataset.\nConclusion: The proposed MDAT method outperforms previous state-of-the-art MDTC approaches in empirical tests. The experimental results showcase its superiority in terms of average accuracy and its effectiveness across individual domains within both datasets. The method is supported by a new theoretical framework which provides guarantees about its performance, thereby addressing the noted gap and providing a more comprehensive understanding of MDTC algorithms.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 5, \"Strengths\": \"Most works in MDTC are empirical, while this paper, to this reviewer, appears to be the only one that is strongly justified theoretically.  Bringing the approach of [Zhang et al, 2019] to the context of MDTC is the key contribution of this work. Extending the development of [Zhang et al, 2019] from domain adaptation to MDTC as done in this paper is natural and sound.\\n\\nThe paper is in general well written, and an easy read.\", \"Weaknesses\": \"The theoretical development closely follows [Zhang et al, 2019]. In fact, to this reviewer, all theoretical results are merely re-stating the results of [Zhang et al, 2019] in the context of MDTC. In other words, the novelty in theoretical analysis is very thin. -- This is in fact not a weakness; I bring it up here just to mean that the theoretical analysis in this paper should not be regarded as a \\\"strength\\\" of any significant magnitude, despite the theoretical results presented.  I suggest the authors clearly state in Section 4.1 that the theoretical analysis closely follows that in [Zhang et al, 2019], so as not look over-claiming.\\n\\nI think the paper is above the acceptance threshold, although marginally.   \\n\\nIn below, I declare a confidence level of 5, which I need to explain. I did not check the proofs. But since the application of [Zhang et al, 2019] is so natural in this context, I think the presented results must be correct (that is also why I do not consider the theoretical development novel). I have not read all literature of MDTC.  So my confidence level \\\"5\\\" only refers to the correctness of the paper. If there were another work that has already exploited [Zhang et al, 2019], then my rating of the paper would drop below the acceptance threshold.\", \"Questions\": \"NA\"}"}]}
{"paper_id": "fBlHaSGKNg", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Semi-Supervised Learning (SSL) has gained prominence due to its ability to harness large amounts of unlabeled data in scenarios where labeled data is scarce and costly to obtain, such as in fields requiring expert annotation like healthcare and finance.\nGap: Most existing SSL studies do not adequately address the efficient selection of samples for annotation under budget constraints. Previously, focus has been on leveraging labeled and unlabeled data to train models, while the selection of labeled samples has often been overlooked, which can impact the performance, especially with limited annotation budgets.\nHypothesis: Minimizing a novel criterion called α-Maximum Mean Discrepancy (α-MMD) through an efficient sample selection methodology named Unleashing the Power of Annotation (UPA) can enhance the generalization ability of SSL by selecting a diverse and representative subset for annotation, even under budget constraints.\nMethod: The approach employs a modified Frank-Wolfe algorithm, Generalized Kernel Herding without Replacement (GKHR), to minimize α-MMD. This method selects samples that balance representativeness and diversity. The UPA method's effectiveness is tested through experimental integration with popular SSL frameworks like FlexMatch and FreeMatch across datasets such as CIFAR-10, CIFAR-100, and SVHN, using a Gaussian kernel with CLIP extracted features for sample selection.\nConclusion: UPA significantly improves SSL performance across various setups by efficiently selecting diverse and representative samples for annotation, surpassing existing active learning (AL) and semi-supervised active learning (SSAL) methods even under constrained budgets. This demonstrates its potential for real-world applications where annotation resources are limited.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"1.\\tAn approach the optimizes both diversity and representativeness has advantage\\n2.\\tSome theoretical results support the advantage of the method.\\n3.\\tEmpirical results show some advantage over Random baseline and for cifar 10 clear advantage to using flexMatch with UPA\", \"Weaknesses\": \"1.\\tThe soundness of the paper is poor: \\n \\na.\\tmostly the empirical validation is lacking additional datasets to support the advantage of the method\\n\\nb.\\timprovement over random baseline is too modest in my view, 0.5 precent improvement when the STD is at 0.5 is not convincing\\n\\nc.\\twe are missing a simple ablation study: if you show results of flexMatch+UPA, you have to show also results of just flexMatch, otherwise it is not clear if the advantage is due to FlexMatch or the addition of UPA.\\n\\nd.\\t3 independent runs is not enough\\n\\ne.\\tWhy are you selecting the particular m values for each data set? why is it different for each data set? I would much prefer seeing a graph over a range of values.\\n\\nf.\\tNot enough method are used in the comparison, what about BADGE by Ash et al -  it is a classical diversity and uncertainty approach that could also be compared to show that representativeness and diversity is better (if it is indeed so…)\\n\\n2.\\tThe requirement that m/n <=0.2 is quite weak and unrealistic, sometimes 0.2 can be a huge data set to annotate! How does that work with Active learning setting in which the budget is limited.\\n3.\\tClarity is also poor:\\n\\na.\\tThere is no pseudo code describing the method, or at least a set of steps\\n\\nb.\\tWhat is the input space over which the method is used? Is it the actual data? For coreset is it the penultimate layer representation, what are you using?\\n\\nc.\\tRemark 1 is rather confusing, if you present Thm 3.1 and then claim that thm 3.1 doesn’t always work\\n\\nd.\\tFigure 1 can not be deciphered with out a basic explanation\\\\legend for the colors\\n\\ne.\\tEquation 6: is f_I_p(x_i) why is it defined only for x?\\n\\nf.\\tIn equation (8) what is K in B=2K?\\n\\n4.\\tThe computational complexity of O(mn) should be better explained? Wouldn’t the kernel construction cost more?\\n\\n5.\\tOverall Im not convinced at all that the approach of diversification and representativeness is an optimal one. There are other important trade offs in active learning such as exploration-exploitation, which can outperform this UPA approach, in my view.\", \"Questions\": \"1.\\tPlease see the questions above in the ‘weaknesses’\\n2.\\tPlease explain what does the flexMatch method do , and why you chose it for your UPA approach.\\n3.\\tWhy do you think the diversification and representativeness is the best one for AL?\"}"}]}
{"paper_id": "HwQ8NVvdmm", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Continual learning allows machine learning models to adapt to new data and tasks over time without forgetting previously learned tasks. This is crucial for applications that need in-field adaptation, such as human activity recognition (HAR) on edge devices with privacy and low latency requirements. Traditional approaches to continual learning are resource-expensive, making them impractical for resource-constrained environments such as edge devices. Reducing computational precision through fully quantized training (FQT) can decrease memory usage and computational demand, making models more suitable for edge deployment.\nGap: While FQT can improve efficiency, aggressive quantization often leads to significant drops in model accuracy. There is limited research on applying FQT with continual learning (CL) techniques, particularly for resource-constrained environments. This raises the question of whether FQT can be effectively integrated with CL to maintain accuracy while leveraging the benefits of quantization.\nHypothesis: The authors propose that leveraging Hadamard transforms, combined with integer-only quantization and appropriate techniques such as tiled matrix multiplication and stochastic rounding, can enable fully quantized continual learning that maintains accuracy close to that of higher precision models while reducing resource consumption.\nMethod: The proposed method integrates Hadamard transforms with quantization, applying them to the backward pass during training to use available quantization levels more efficiently. This involves matrix multiplication in the Hadamard domain, integer quantization of weights and activations to 4-bits, and 8-bit accumulation. The authors also determine certain tensors needing stochastic rounding to mitigate quantization noise. They evaluate their method, called Hadamard Domain Quantized Training (HDQT), using CIFAR100 and several HAR datasets in a class-incremental learning setting, testing against popular CL techniques like LwF, iCaRL, and BiC.\nConclusion: The study finds that using HDQT, it is possible to achieve quantized training with minimal accuracy loss: less than 0.5% for HAR datasets and less than 3% for CIFAR100, compared to floating-point baselines. Moreover, they observe that quantized continual learning does not exacerbate catastrophic forgetting compared to non-CL training. HDQT also shows significant improvements in computational efficiency and energy consumption, making it well-suited for edge applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"* This work is clearly written and proposes a principled and efficient method of solving the non-uniformity of gradients during backward propagation.\\n* Whereas previous works have proposed log scaling of values, this is impractical to implement efficiently on hardware. Also, it is difficult to guess the necessary value range beforehand. The Hadamard transform is both efficient to implement and is ideally suited to removing gradient spikes while preserving information. As such, there is potential for Hadamard domain backpropagation in reduced-precision training. \\n* In the backpropagation process, utilizing HDQT allows setting the input and accumulation bit-width to 4 and 8 bits, respectively, showcasing improved energy efficiency compared to conventional CIL methods.\\n* The authors present promising results on image datasets (CIFAR100) and human activity recognition datasets (DSADS, PAMAP2, HAPT), demonstrating a degradation of ≤ 2.5% through quantization in vision tasks.\", \"Weaknesses\": \"* It is difficult to understand the motivation for connecting continual learning with integer valued training. While edge devices have fewer resources for training, they would not be expected to perform much training or have it completed quickly. Therefore, it would be difficult to justify the additional expense in both software and hardware when the same result can be obtained simply by waiting for a few more hours. \\n* More experiments on different domains are necessary to support the claims of the efficacy of Hadamard domain quantization. For a fairer comparison, there should be a comparison using other methods used in fully quantized training when applied to continual learning. Moreover, there should be a comparison with previous methods when training from scratch, not just during continual learning.\\n* The method proposed in the paper would be difficult to implement due to the requirement for stochastic rounding, which cannot be easily implemented in hardware, integer-based or otherwise. Even if such hardware could be implemented, this would require custom circuitry solely for that purpose.\", \"Questions\": \"* In Section 4.4, when discussing the energy estimates derived from the accelerator cost model for HDQT application, there is a concern that if the training time increases due to HDQT, it may consume more energy than conventional methods. Can the experiments include a comparison of training times when applying HDQT?\\n* When examining the results for CIFAR100 in Table 1, the accuracy degradation appears to be relatively higher compared to other datasets. It seems that the trend may vary depending on the dataset or network architecture. Have there been results applying the method to more complex network architectures or larger datasets?\\n* While there are advantages to performing matrix multiplication as integer-arithmetic units through HDQT, it is likely that the overhead for Quantization and Stochastic Quantization cannot be ignored, especially as the batch size increases. I am curious about the energy efficiency, including the overhead for Quantization and Stochastic Quantization, as the batch size becomes larger.\"}"}]}
{"paper_id": "cZo6pDtDZr", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Collision probability is a significant measure of how widely a distribution's probability mass is dispersed. It is widely used in fields like ecology (Simpson index), economics (Herfindahl-Hirschman index), and statistical mechanics. Unlike Shannon entropy, collision probability is easier to estimate as its sample complexity is independent of the support size.\nGap: Previous work on locally differentially private estimation of collision probability had a sample complexity that worsened as the privacy parameter tightened. Additionally, no sequential testing algorithms specifically for collision probability, rather than batch testing, existed prior to this study.\nHypothesis: The authors propose that it is possible to develop a locally differentially private (LDP) algorithm that estimates collision probability more sample-efficiently than previous methods and one that supports sequential testing under LDP without needing a known epsilon.\nMethod: The authors introduce two novel algorithms: (1) A private estimation algorithm using a distributed user-server protocol that computes estimates by hashing and bias correction, overcoming sample complexity issues with a thoughtful distribution of computational steps across users and the server. (2) A sequential testing algorithm that draws samples iteratively and applies threshold checks on collision frequency observations, allowing it to adapt to unknown difficulty parameters and efficiently reject hypotheses.\nConclusion: The private estimation algorithm achieves near-optimal sample complexity in terms of achieving better error bounds, roughly independent of the support size, while using significantly fewer samples than prior methods. The sequential testing algorithm efficiently determines hypothesis distinctions without pre-specified sample limits, showcasing reduced sample needs as compared to batch methods, making them suitable even when epsilon is unknown.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The writing is clear and intuitive. It was easy to follow the technical arguments, and I believe they are correct (did not look at some of the full proofs though).\\n2. Experimentally, the mechanism proposed here seems to outperform the existing approach from Bravo-Hermsdorff et al., and slightly outperform the open source implementations for locally private heavy hitters.\", \"Weaknesses\": \"1. Poor contextualization of the results in comparison to previous work. Why focus on additive approximation? What are precisely the best known results?\\n2. Limited novelty, both results are simple and do not introduce significant new tools.\", \"Questions\": \"Disclaimer: I have reviewed this paper in the past and one request I had still remains. Please contextualize your paper better against the problem of locally private heavy hitters. It seems that your problem is in some ways easier and perhaps less interesting than heavy hitters. Does a separation between collision probability and heavy hitters follow from existing results in the literature (or from your existing results)? can you show that your problem is provably easier in terms of sample complexity?\"}"}]}
{"paper_id": "F7QnIKlC1N", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The study of molecular ground-state conformation is essential because it often determines a molecule's physical, chemical, and biological properties. Traditional methods for obtaining these conformations, such as density functional theory (DFT), are resource-intensive. Deep learning (DL) methods, particularly molecular representation learning (MRL) using graph neural networks, have shown promise in molecular modeling, mastering tasks like molecular property prediction and optimization. Some recent works have integrated 3D geometric information into neural networks to enhance performance.\nGap: Existing generative models primarily focus on producing multiple stable conformations instead of targeting the ground-state conformation, which involves additional screening processes to find the optimal one. Current methodologies either rely on labor-intensive computations or fail to accurately capture the necessary molecular geometry efficiently. Hence, there is a clear need for more efficient, accurate models that can predict the ground-state conformation directly from 2D molecular graphs.\nHypothesis: The authors propose that a novel network architecture based on a Graph Transformer with a new self-attention mechanism, called Molecule Structural Residual Self-Attention (MSRSA), can efficiently predict the 3D ground-state conformation of molecules from their 2D topological architectures. This method should be competitive with or superior to existing state-of-the-art models and open-source tools like RDkit.\nMethod: The GTMGC model is presented as a Graph Transformer based network designed to predict molecular conformations end-to-end. It uses the MoleBERT Tokenizer to provide context-aware tokenized inputs, reducing representational redundancy of atoms. Positional encoding is applied using Laplacian Positional Encoding from the adjacency matrix. The core of the model is the MSRSA mechanism, which refines the self-attention process by incorporating adjacency and interatomic distance matrices to bias attention scores, preserving global and local molecular structure information. The model was trained and validated on the Molecule3D and QM9 datasets, showing significant enhancement in conformational prediction accuracy.\nConclusion: The GTMGC model demonstrated remarkable improvement over current state-of-the-art methods and exceeded the performance of established algorithms in RDkit based on benchmarks from the Molecule3D and QM9 datasets. The proposed MSRSA module not only enhanced model performance for conformational prediction but also proved effective for other molecular property predictions, illustrating its robustness and general applicability.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The paper is very clear and pleasant to read.\\n2. The integration of various existing frameworks for the task of molecules' ground state conformation is original to some extent.\\n3. The performances compared to the baselines are significant.\", \"Weaknesses\": \"I believe there are two main weaknesses:\\n\\n1. Novelty: There are plenty of previous works (not cited) implementing Transformer architectures that implement the elements of the proposed self-attention, e.g., [1] uses the adjacency matrix, [2] makes use of the distance matrix (even cited in the manuscript), while the weighted summation is ubiquitous in many fields using Transformers [3]. The initial encoding is obviously not a technical contribution.\\n\\nThus, the contribution may be summarized as the integration of existing methods/approaches, augmented with the regularization loss on the distance matrix (the \\\\beta seems to bring very minor improvement).\\n\\n2. Strange results: I may be wrong, but according to Table 3 (ablation study) a *simple Transformer* architecture without any addition to the self-attention reaches 0.4395 (MAE) which is already better than all the other baselines. This is problematic. Also, the final improvement is only ~1%. Finally, the reported results seem to take the best-ablated model results for each metric, which is wrong.\\n\\nMoreover,  we have:\\n\\n3. The advantage of MoleBert over the standard atomic encoding is extremely shallow or even worse.\\n4. Lack of comparison with other Transformer based methods.\\n\\n[1] *Molecule attention transformer.*\\n\\n[2] *Geometric transformer for end-to-end molecule properties prediction.*\\n\\n[3] *Axial-DeepLab*\", \"Questions\": \"Currently, it seems the proposed contributions don't bring any advantage over a simple (large) transformer model.\\n\\n1. One needs to know the capacity of the model in order to assess the origin of the good performance. \\nAccording to weakness 2, it seems the good performances are obtained *almost solely* from a large/powerful standard Transformer model. From Table 5, the model seems much bigger than other methods. Also, the discrepancies between the tables are disturbing (one single metric should be used for the model validation).\\n\\n2. Ablating the initial LPE. \\n\\n3. It would be beneficial to have a comparison performance with (at least one) other molecule transformer-based methods such as [4,5,6] or others at a similar capacity.\\n\\n4. It would be interesting to look at the learned weighting parameters (\\\\gamma) of the self-attention to better understand the contribution of each (maybe even adding a weighting to the global attention).\\n\\n\\n[4] Relative molecule self-attention transformer.\\n\\n[5] 3dtransformer: Molecular representation with transformer in 3d space.\\n\\n[5] Geometric transformer for end-to-end molecule properties prediction.\"}"}]}
{"paper_id": "8JCn0kmS8W", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Recent advancements in audio generation models have primarily focused on domain-specific applications such as speech transcriptions and audio captions. These systems struggle to handle the complexities involved in generating comprehensive audio containing various elements like speech, music, and sound effects, tailored to controllable conditions.\nGap: Existing audio generation systems are limited in their ability to create harmonious, compositional audio that integrates multiple audio elements controlled by text instructions. They often fail to meet the demands of real-world scenarios necessitating sophisticated audio storytelling from textual descriptions.\nHypothesis: WavJourney proposes using Large Language Models (LLMs) to generate a comprehensive audio creation framework that can orchestrate multiple audio models to produce storytelling audio content from textual descriptions. This methodology aims to harmonize speech, music, and sound effects into a coherent audio script that can be executed as a computer program.\nMethod: WavJourney employs LLMs to generate an audio script from text instructions. This script serves as a structured semantic representation of the desired audio elements, detailing their type, layout, and attributes. The script is then compiled into a computer program, where task-specific audio generation models and computational functions are invoked to produce the final audio content. This system is designed to be compositional and interpretable, leveraging different expert audio models without additional training. The framework is evaluated on text-to-audio benchmarks including AudioCaps and Clotho.\nConclusion: WavJourney successfully synthesizes realistic audio aligned with semantic, spatial, and temporal conditions described by text, achieving state-of-the-art results on benchmarks. It demonstrates potential in crafting engaging storytelling audio and facilitates human-machine co-creation in multi-round dialogues. The introduction of a multi-genre story benchmark and subjective evaluations highlights WavJourney's effectiveness, although areas like efficiency and artificial composition remain as potential limitations.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes a novel task, text-to-audio storytelling. The task opens up a huge space for AI-powered general audio creation.\\n\\n2. The proposed system outperforms or achieves on-par  performance against previous SOTA methods, i.e., AudioGen and AudioLDM, in both objective and subjective metrics.\\n\\n3. The new established benchmark is another good contribution.\\n\\n4. The developed prototype and demos look amazing.\", \"Weaknesses\": \"1. The method proposed has limited methodology contribution to the research community. The system reflects a crafted engineering pipeline by combining several off-the-shelf audio expert models through LLM. There is limited contribution in terms of machine learning algorithms.\\n\\n2. As sort of an ensemble of expert models, the paper does not include any system-level ablation studies by comparing against other design choices.\\n\\n    a. How much does the system rely on the capabilities of LLMs? How would it perform if LLM is not GPT-4, but rather those open-source alternatives like Llama.\\n\\n    b. The script writer acts like an agent. There are numerous works and existing methods working on connecting LLM to perform various tasks, e.g., AutoGPT. But the authors didn’t compare against these agent-based system design.\", \"Questions\": \"See weakness.\"}"}]}
{"paper_id": "XbLffB0T2z", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Availability data poisoning attacks involve introducing small, imperceptible changes to training data to degrade the test accuracy of machine learning models. This type of attack is particularly relevant given the prevalence of untrusted data sources used in training models and the threat they pose to model robustness.\nGap: Existing poisoning strategies typically assume that the adversary and the victim use the same learning algorithm, limiting their transferability across different learning paradigms. The effectiveness of such attacks decreases significantly when the victim employs a different training method than what was anticipated by the adversary.\nHypothesis: The authors propose that by generating high-frequency poisoning perturbations using both supervised and unsupervised contrastive learning paradigms, the transferability of availability poisoning attacks can be greatly enhanced across different learning methods and paradigms.\nMethod: The authors introduce Transferable Poisoning, an approach that iteratively uses gradient information from models trained via supervised and unsupervised contrastive learning to optimize high-frequency poisoning perturbations. The method involves alternating between supervised targets with a pretrained model and unsupervised contrastive learning to generate perturbations. The experiments utilize benchmark image datasets like CIFAR-10, CIFAR-100, and TinyImageNet, with various learning algorithms to test transferability.\nConclusion: Transferable Poisoning achieves significantly improved transferability of poisoning perturbations across different learning paradigms, reducing victim model test accuracy by over 27% compared to existing methods. The use of high-frequency perturbations ensures efficacy regardless of the training paradigm, demonstrating a more severe threat of availability attacks than previously thought.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The motivation is convincing that availability poisoning attack should be effective for different learning paradigms.\\n2. The proposed algorithm improves the transferability of poisoning attack across evaluation algorithms including SL, CL, Semi-Supervised Learning on CIFAR-10, CIFAR-100 and Tiny-ImageNet.\\n3. This paper provides plenty of investigation on alternative methods including HF, CC, SCP, and WTP.\", \"Weaknesses\": \"1. The authors should check their code regarding the CL and SupCL evaluation for poisoning attacks, especially how transforms (augmentations) are performed when preparing the poisoned dataset. It seems that the data augmentation for each image is fixed before the CL (SupCL) training starts in their code. For example, in this case, each positive pair consists of two fixed augmented images. However, the CL (SupCL) algorithm applies different data augmentations each time it reads an image. Once this part of code is not properly configured, the reliability of the algorithm is questionable. For these reasons, I have significant concerns about the CL and SupCL evaluation results reported in this paper.\\n2. The novelty of the proposed method is limited. The core of the TP algorithm is a just combination of two existing algorithms, i.e. iteratively updating poisons using TAP and CP to inherit their advantages.\\n3. The paper seems inaccurately describe the existing work. “Targeted Adversarial Poisoning (Fowl et al., 2021) can produce poisoning perturbations with high-frequency features that are not linear separable, ...” However, as discussed in Table 1 of [a], TAP perturbations are linear separable.\\n4. The main machanism of TP is kind of vague. This paper claims that “we propose Transferable Poisoning (TP) to generate poisoning perturbations with shared high-frequency characteristics that can be transferred across different learning paradigms (Section 3).” It is unclear why high-frequency perturbation should be a goal for generating transferable poisoning attacks. The proposed algorithm even does not explicitly optimize the frequency of perturbations. In Figure 4, all four attacks are high-frequency but three of them, namely CC, SCP, and WTP have poor transferability. Does it contradict the claim?\\n5. Besides the first concern about CL evaluation, I have also concerns about the comparison of attack performance.\\n(1)\\tTable 1,2,4 do not consider BYOL that was used in the paper of CP. What is the attack performance of TP against BYOL on CIFAR-10 and CIFAR-100?\\n(2)\\tIn Table 1, is SimCLR used for CP and TUE perturbation generation? However, SimCLR is not surely the best generation algorithm for transferability of CP and TUE. To be a fair comparison, it is helpful to consider CP and TUE attacks based on other CL algorithms as well as class-wise CP attacks which are effective for SL.\\n(3)\\tIt would be better to compare your TP with existing powerful poisoning methods (UE, AP, CP and TUE) on larger datasets like CIFAR-100 and TinyImageNet.\\n6. As the algorithm involves contrastive training, it possibly costs too much time to generate TP perturbations, especially on Tiny-ImageNet and ImageNet. Can you provide time consuming comparison of your TP with existing transferable methods like AP, (class-wise) CP, and TUE? Considering availability poisoning attacks are a sort of data protection means, low efficiency of generation might hinder the applications in real world scenarios.\\n\\n[a] Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Availability attacks create shortcuts.\\n\\nClarity, Quality, Novelty And Reproducibility:\\nI hope the author would make more clarificaitons about the main machecism of TP and descriptions about existing work. The novelty of proposed algorithm is hindered by the combination of two existing algorithms. For reproducibility, the first concern about CL and SupCL evaluation is vital.\", \"Questions\": \"See Weakness.\"}"}]}
{"paper_id": "4y3GDTFv70", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large language models (LLMs) have shown significant abilities in natural language processing tasks, such as language understanding, few-shot in-context learning, and instruction fine-tuning. These capabilities are often referred to as emergent abilities and are observed as LLMs increase in model size and data. Prior theories have linked these abilities to the properties of data distributions or latent concepts in language.\nGap: Despite observations of emergent abilities in LLMs, there is not a comprehensive theoretical framework explaining why and how these abilities develop, particularly given that LLMs are trained primarily through unsupervised learning to predict next tokens without explicit semantic training.\nHypothesis: The paper hypothesizes that the emergent abilities of LLMs can be attributed to Bayesian inference on sparse joint distributions of languages, distinguishing between unambiguous and ε-ambiguous languages. LLMs effectively perform Bayesian inference, examining sparse structures in joint distributions through their role as universal density approximators of marginal distributions.\nMethod: The study proposes a latent space theory for emergent abilities in LLMs. It defines languages as either unambiguous or ε-ambiguous, based on the ability to infer intentions from messages with varying degrees of precise unambiguity. The authors use a stochastic process for language generation involving the sampling of intentions and conditioning message generation on these intentions. They argue that LLMs trained with maximum likelihood estimation on large data sets approximate the marginal distribution of languages accurately, thereby supporting Bayesian inference for understanding and generating text responses. Simulation experiments using synthetic data (doubly-embedded Markov chains) provide validation for this theoretical framework.\nConclusion: The paper concludes that LLMs' emergent abilities, including language understanding and in-context learning, arise from their implicit ability to perform Bayesian inference on sparse joint distributions of languages, supported by LLMs' capacity to approximate marginal distributions. The paper also suggests a scaling behavior for these abilities, asserting that model size and data breadth enhance the realization of these emergent qualities.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 1, \"Confidence\": 4, \"Strengths\": \"The problem itself is very important.\", \"Weaknesses\": \"I agree that the emergent behaviors of LLMs need a thorough scientific\\ninvestigation, but this paper says almost nothing about these emergent\\nabilities. What is important here is that LLM seem to solve a new task that is\\nnot contained in the training data: that is called emergent. However, just \\nfinding a latent \\\"intention\\\" (quite loosely defined in this paper) cannot\\nexplain this behavior, because the new emergent intention just doesn't exist\\nso far. Therefore, in spite of elementary mathematical discussions in Section\\n4, the paragraph \\\"In other words, ...\\\" virtually adds nothing about the\\nunderstanding of the behaviors of LLMs.\\nConsidering other possible explanations, such as recently developed by Arora\\net al. (2023), the arguments of this paper is too trivial to be useful in\\npractice.\", \"Questions\": \"Nothing.\"}"}]}
{"paper_id": "ODSgo2m8aE", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Relational learning, specifically using Graph Neural Networks (GNNs), has emerged as a powerful method for analyzing graph data across various domains such as social media and drug discovery. GNNs have shown great promise due to their ability to perform deep learning operations on relational data, particularly using architectures like Graph Convolutional Networks (GCNs). However, as GNNs are often trained on data that may contain inherent biases, it poses challenges in ensuring fairness and stability in their outputs.\nGap: The gap identified is the lack of research addressing how to stabilize the outputs of GNNs when the graph data contains implicit biases. Existing studies have not applied Lipschitz bounds as a method to control and stabilize the output perturbations of GNNs due to such biases, even though Lipschitz bounds are known to be effective in stabilizing outputs in Euclidean neural networks.\nHypothesis: The hypothesis is that by formulating a Lipschitz bound specifically for GNNs, it is possible to constrain the output perturbations caused by input biases in graph data, thus enhancing fairness during the training of GNN models.\nMethod: The authors propose a method to apply Lipschitz bounds to GNNs to constrain biases and stabilize outputs. This involves defining Lipschitz bounds for GNNs and using the Jacobian matrix to efficiently calculate these bounds, facilitating practical implementation during training. The method is validated through theoretical analysis and empirical experimentation on node classification and link prediction tasks using real-world datasets. The key implementation is through a plug-and-play approach called JacoLip that integrates with existing GNN training pipelines in PyTorch, leveraging automatic differentiation for efficient computation of these bounds.\nConclusion: The study concludes that incorporating Lipschitz bounds in GNN training can effectively limit biases and stabilize model outputs, thus enhancing fairness without significant computational overhead. The proposed method, JacoLip, demonstrates superior performance in enhancing both the fairness and stability of GNN's predictions, validating the hypothesis.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 1, \"Strengths\": \"- Estimate the Lipschitz constant of GNNs via Jacobian matrix\\n- Comprehensive experiments on GNNs for the bias\", \"Weaknesses\": \"Since I’m not familiar with GNN and fairness, it appears difficult to evaluate the contribution. \\n\\nFor the estimation of the Lipchitz constant, the approach is based on the Jacobian matrix, which is loose. In this community, there are several advanced and promising approaches for the Lipschitz constant estimation on DNNs. So I’m wondering how this can be extended to the GNN setting, and what’s the difficulty issue behind this?\", \"Questions\": \"See the above.\"}"}]}
{"paper_id": "RlfD5cE1ep", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Self-supervised representation learning, particularly contrastive learning, utilizes both positive views generated via data augmentation and negative examples to align representations. Non-contrastive methods like BYOL (Bootstrap Your Own Latent) and SimSiam offer computational benefits by eliminating negative examples. Despite initial assumptions that representations might collapse without negative samples, prior work has shown non-collapse is possible with strong data augmentation balancing regularization.\nGap: Existing analyses of non-contrastive dynamics, such as those using L2 loss, do not incorporate feature normalization, which is commonly employed in practice. This oversight may lead to collapse when regularization is too strong, contradicting the observed stability in practical settings that use normalization.\nHypothesis: Feature normalization significantly influences the dynamics of non-contrastive learning, preventing collapse even under strong regularization by inducing higher-order dynamics that stabilize the representation space.\nMethod: The paper extends previous L2-loss-based dynamic analyses by incorporating cosine loss, which accounts for feature normalization. Using a high-dimensional limit, the research models the learning dynamics with synthetic data. It presents eigenmode dynamics analysis, demonstrating that cosine loss induces sixth-order dynamics, contrasting with the third-order system from L2 loss approaches. Numerical simulations are conducted to verify these dynamics using the CIFAR-10 dataset and SimSiam framework with ResNet-18.\nConclusion: Feature normalization plays a pivotal role in stabilizing non-contrastive learning dynamics, counteracting potential collapse even with strong regularization. This is attributed to the emergence of stable equilibriums due to the complex dynamics induced by the cosine loss. The findings suggest a theoretical expansion towards understanding practical mechanisms that support the robust performance of self-supervised non-contrastive learning systems.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"This article presents an improvement other than the theoretical framework of non-contrastive learning using solely the Euclidian loss.\\nThe paper is well-written and easy to follow. The assumptions taken are relatively well justified and allow for an interesting analysis.\", \"Weaknesses\": \"**Previous literature** There have been recent contributions to the literature of non-contrastive learning which do take into account the cosine loss, and which are not referenced in this article. In particular, Halvagal et al., Implicit variance regularization in non-contrastive SSL, 2023. The eigenmode dynamics seem extremely similar (after some changes in the notation) and it seems extremely important to me that the authors compare themselves to this article. The authors also do not seem to have a similar conclusion on the implicit variance regularization that Halvagal et al. focus on.\\n\\n**Regimes** The three regimes found in Section 5.2. seem to have been found solely by categorizing the regimes experimentally shown in Figure 2 while reading, which seems like a weak justification for those regimes. A clearer analysis of the equilibrium points at least in the Appendix seems necessary. \\n\\nThe authors claim that as the norms decrease, the regimes fall to the stable one. However, in the stable regime, the norms will increase as the eigenmodes increase to the saddle point $p^+$. Is there a risk of the acute and stable regimes alternating between each other? \\n\\n**Experiments** Numerical experiments on the SimSiam model remain on linear networks in Section 5.4. A similar Figure to Figure 6 for a real network such as ResNet (maybe only focusing on the linear projection head) would help confirm the theoretical findings in the linear case. Otherwise, the link with a real SimSiam network remains relatively limited, except for the weight decay argument.\\n\\n**Figure 6c** I also find Figure 6.c. hard to read. Are the intervals the theoretical intervals using the values of the norms? In this case, what is the theoretical value of the saddle point? Having the values of a single eigenmode gives very little information on the values of the spectrum of $W$. Do all the values stay relatively constant like here?\", \"Questions\": \"The notion of Thermodynamic limit is novel to me in optimization and needs to be further explained. How is it different from the Neural Tangent Kernel regime, is it the constant ratio $\\\\alpha$?\\n\\nDo the authors have more intuition on the role of the exponential moving average in BYOL with their new findings?\\n\\nSmall remarks:\\n* Intro: \\\"Folklore says\\\" is a somewhat strange way to quote an article.\\n* Sec 4. Assumption 2. $Σ = I $ seems superfluous to add after $D = N(0, I)$.\\n* After Lemma 3: $\\\\Phi x'$ is not defined. \\n* Equation (5): $\\\\hat H$ is not defined.\\n* After Equation (9): \\\"unite learning rate\\\"\"}"}]}
{"paper_id": "18TfucMNTr", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Training deep neural networks is a challenging non-convex optimization problem characterized by local minima and saddle points. Incorporating noise into training can help escape local minima and improve convergence. Moreover, optimization by continuation, which involves solving a convex approximation of the objective function before gradually transforming it into the original non-convex function, has been applied successfully in various domains including computer vision and reinforcement learning. Despite the potential of continuation methods, challenges remain due to the presence of saddle points in deep learning optimization landscapes.\nGap: The paper identifies that while optimization by Gaussian continuation shows promise, saddle points require special treatment as they may trap the optimizer in local minima during the continuation process. There is a need to address these issues to improve the training of deep architectures and handle non-convex loss functions more effectively.\nHypothesis: The paper proposes that applying a form of regularization to the continuation optimizer can address the challenge posed by saddle points during optimization by Gaussian continuation. This approach is hypothesized to improve training outcomes for deep networks by reducing variability and enabling escape from local minima.\nMethod: The paper uses a continuation method that involves Gaussian convolution of the objective function. Starting with a large continuation parameter that ensures convexity, the paper gradually decreases this parameter while optimizing. A regularization term is added to the continuation optimizer to manage the impact of saddle points and stabilize the training process. The method also adapts Monte Carlo techniques to evaluate high-dimensional optimization efficiently. The performance of this method is assessed on benchmark problems including image classification with the MNIST dataset and learning dynamics with neural ordinary differential equations.\nConclusion: The study demonstrates that Gaussian continuation reduces the risk of being trapped in local minima, achieving better training results for deep learning models. It specifically shows greater stability and reduced variability in training outcomes. The continuation approach leads to faster convergence and higher accuracy compared to standard methods, supporting the hypothesis that the regularization of the continuation parameter can mitigate saddle point issues, thereby enhancing deep neural network training.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"- The paper gives a good introduction, formal treatment and extensive background of the basic concepts and background literature on Gaussian continuation\", \"Weaknesses\": \"- The numerical experiments seem severly flawed in multiple ways. The baseline MNIST classifier without continuation is nowhere near what a simple fully connected 2-layer neural network trained with SGD can achieve on MNIST in terms of test set accuracy (their baseline training set accuracy is below 80% while it is easy to get above 90% test accuracy and should be easy to memorize the training set on MNIST). Furthermore, the variance across different training runs the authors show in their figures, especially for the baseline (without continuation) looks very alarming. Their baseline training setup is too unstable to average across and be considered anything remotely close to a fair baseline.\\n- Given the minimal numerical results it does not become clear whether the added dimension of how to decay lambda properly is really given a robust solution with the approach described in the paper\", \"Questions\": \"- If the focus is on high dimensional deep learning problems perhaps do not use so much space on 1D intuitions (e.g. Figure 1 and Figure 2). It is also not clear to me that the saddle point example in 2.4 is very meaningful to how saddle points in the loss landscape for high dimensional neural networks look like. I would instead recommend to focus more on how Gaussian continuation interplays with the noise from SGD (different noise distribution than normal Gaussian) and its well known annealing affects typically leveraged through learning rate schedules. Does injecting noise with the different distribution really help and if yes in what circumstance in a high dimensional loss landscape? Some papers in the direction of this question (like https://proceedings.mlr.press/v162/orvieto22a.html) are already cited in the paper.\\n- Nit: In the abstract it says saddle points are responsible for getting stuck in local minima, but a saddle point is not a minimum, just a zero gradient point to get stuck in?\"}"}]}
{"paper_id": "JGTC6WgO4T", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Unsupervised Domain Adaptation (UDA) is a technique aimed at training models using an unlabeled target domain by leveraging knowledge from a labeled source domain. UDA traditionally requires access to source data and models, which raises privacy and security concerns. Black-Box Domain Adaptation (BDA) was introduced to alleviate these constraints by utilizing pseudo labels derived from a single source prediction for self-training of the target model.\nGap: Existing methods in BDA primarily focus on a single source model, assuming adequate correlation between the source and target domains. However, in real-world scenarios, multiple source domains might be available, facilitating the generation of pseudo labels, yet these multi-source settings have been largely overlooked in BDA research.\nHypothesis: The authors propose that a novel framework, which efficiently refines pseudo labels by considering the statistical relationships between multiple source domains and the target domain, can fundamentally enhance multi-source black-box domain adaptation (MSBDA).\nMethod: The paper introduces a Label Space-Induced Pseudo Label Refinement (LPR) framework for MSBDA, featuring a Pseudo-label Refinery Network (PRN). This framework learns to relate each source conditioned by the target based on source predictions. The PRN is designed to improve pseudo labels' quality by incorporating prediction concentration within label spaces and reducing the distance between distinct label spaces. The approach is supported theoretically and empirically validated on four benchmark datasets, demonstrating competitive performance.\nConclusion: The proposed MSBDA framework using LPR and PRN demonstrates competitive performance compared to state-of-the-art domain adaptation methods under various domain adaptation settings. It efficiently utilizes multi-source information to refine pseudo labels, thereby facilitating effective target domain adaptation without access to source data or models.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"1. The studied problem is interesting and significant for distilling black-box models into user-accessible models. \\n\\n2. The authors provide thorough experimental details for reproducing this paper.\", \"Weaknesses\": \"1. The major concern is that the presentation is **terrible**. It's hard to understand the motivation and the details of this work. In fact, I don't think this can be easily fixed during the rebuttal phase.\\n\\n2. First, the author mentioned that existing BDA methods suffer from 'decrease empirical errors e.g. through de-noising of source predictions and label smoothing when yT is not accessible'. But, in the modified triangle inequality, there is still a term of p_c, whose definition requires the ground truth. \\n\\n3. I don't quite understand how p_c can be properly obtained. It seems this paper attempts to obtain p_c via the concentration of one source's predictions to those of other sources. What if there is only one best source while the other sources are all terrible? Also, Section 3.3 is confusing. Why the label space should be divided?\\n\\n4. The proposed label refine network also confused me. Is the network architecture derived according to the theorem? I don't think such a complex network in the label space can improve the performance.\\n\\n5. The final prediction performance improvement is marginal. Also, a recent SOTA paper is missing - https://openreview.net/forum?id=hVrXUps3LFA\", \"Questions\": \"See above.\"}"}]}
{"paper_id": "fQHb1uZzl7", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: In the field of computer vision, finding visual correspondences between images is a key problem with many applications such as SLAM, augmented reality, and structure from motion. Traditional methods focus on sparse correspondences, detecting key points and matching them across images, while dense correspondence aims to match all pixels between images. Recent research in visual correspondence has utilized either feature aggregation or cost aggregation for improving dense matching tasks, each having its own advantages and limitations.\nGap: Many existing methods either focus solely on feature aggregation or cost aggregation for dense matching tasks, without leveraging the potential strengths of combining the two. Feature aggregation aligns similar features within and across images, while cost aggregation ensures coherence in flow estimates across pixels. However, relying on only one type might not fully utilize the potential information available.\nHypothesis: The paper hypothesizes that by effectively unifying feature aggregation and cost aggregation within a Transformer-based framework, enhanced dense matching performance can be achieved due to the complementary strengths of both aggregation types.\nMethod: The authors propose a new architecture called UFC (Unified Feature and Cost Aggregation Transformers) which utilizes Transformers to unify both feature and cost aggregation through self- and cross-attention mechanisms. The method combines feature descriptors and cost volumes, using integrative self-attention to handle both types of data simultaneously. This approach is further refined in a coarse-to-fine hierarchical manner to improve robustness, culminating in multi-scale predictions to enhance the final correspondence estimation. The network produces multi-scale predictions, computes their confidence scores, and selects the most confident flow for final prediction. The method is evaluated on various benchmarks for semantic and geometric matching, demonstrating significant improvements over existing techniques.\nConclusion: The proposed UFC method effectively integrates feature and cost aggregation, leading to improved performance in dense matching tasks, as demonstrated on various benchmarks. The Transformer-based architecture capitalizes on self- and cross-attention mechanisms, allowing for robust and efficient estimation of correspondences even under challenging conditions such as large intra-class variations and geometric deformations.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"1) This paper is generally presented well;\\n\\n2) The idea of use cost score matrix for both self- and cross-attention feature updating is simple and effective;\\n\\n3) Experimental results are good.\", \"Weaknesses\": \"1)  Section 5.1. It is quite blurry for me whether previous state-of-the-art methods in Table 1 and 2 are trained on the same datasets; For example, the proposed method is trained on the DPED-CityScape-ADE and MegaDepth datasets;\\n\\n2) The performance of the proposed method on the optical flow (KITTI, Sintel) task is blurry for me;\\n\\n3) It's good to see the improved matching performance on the HPatches dataset. However, I want to see whether the improved matching would lead to better Rotation and translation estimations. \\n\\n4) Using cost score matrix for both self- and cross-attention feature updating is good. However, this contribution may be constrained to large overlapping ratio between images. If pairwise images have small overlapping ratio, the cost score matrix is noisy, and may provide wrong guidance for feature updating. Would you please check whether the proposed method works on some challenging image pairs from the MegaDepth dataset.\\n\\n5) Please show some failure cases.\", \"Questions\": \"Please refer to the weaknesses.\"}"}]}
{"paper_id": "YjG29CIOP7", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Data-free Meta-learning (DFML) focuses on enabling learning from a collection of pre-trained models without accessing their original training data. It's a contrast to traditional meta-learning that relies on data from related tasks. DFML is particularly relevant in privacy-sensitive scenarios where only pre-trained models are available, such as from repositories like GitHub. The concept was introduced by works like Wang et al., 2022, and Hu et al., 2023, highlighting its capability in creating generalizing meta-learners.\nGap: Existing DFML methods face two core vulnerabilities: Task-Distribution Shift (TDS), where the distribution of synthetic tasks changes during continuous learning leading to overfitting, and Task-Distribution Corruption (TDC), where task distributions can be corrupted by model poisoning attacks via deceptive pre-trained models. These issues hamper the practical applicability of current DFML techniques.\nHypothesis: The authors propose a new approach to develop robust DFML methods that can effectively address TDS and TDC. TEAPOT leverages a memory-based strategy to handle TDS by maintaining old task memories and diversifying pseudo task distributions. ROSY, a model selection policy, is introduced to tackle TDC, ensuring reliable model selections using reinforcement learning.\nMethod: To address TDS, TEAPOT introduces a memory-based approach that maintains old tasks' memory to prevent over-reliance on new tasks. It uses interpolation with classes from different tasks to diversify pseudo task distributions. To counter TDC, the authors introduce ROSY, which uses a learnable reliability score to rank and select models, optimized via policy gradients due to the non-differentiable nature of model selection. The approach's effectiveness was validated using extensive experiments on multiple datasets, including CIFAR-FS, MiniImageNet, VGG-Flower, and CUB, demonstrating superior performance compared to previous baselines.\nConclusion: The authors' approach, TEAPOT combined with ROSY, successfully addresses the vulnerabilities of TDS and TDC in DFML. TEAPOT significantly enhances the stability and robustness of learning processes against task-distribution shifts, achieving higher accuracy and generalization. ROSY further improves robustness against task-distribution corruption by effectively filtering deceptive models. The proposed methods demonstrate a significant empowering leap for DFML, ensuring it is applicable in broader, real-world situations.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"+ The paper discusses robustness for the first time in the relatively new meta-learning problem setting of DFML.\\n+ The paper proposes an effective method for replaying past tasks in DFML and confirms its effectiveness through experiments.\\n+ The paper proposes a reinforcement-learning-based model selection method that can maintain meta-testing performance even when unreliable models are included.\", \"Weaknesses\": \"- The risk of TDC is avoidable through sanity checks with simple data collection, and thus there is little motivation to introduce the proposed method with high computational complexity using RL. In fact, the experiments provided in the paper use pre-trained models in the medical domain for the attack, but these are not difficult to determine by simply inputting data obtained from an image search engine or a text image model such as Stable Diffusion without privacy concerns. It is unnatural for the problem setting to allow data inversion for meta-learning but not to allow such a simple preprocessing. We could not agree with the practicality of TDC's problem setting unless it shows important use cases or experimental results, indicating it cannot be solved without introducing the proposed method.\\n- The replay-based method in TEAPOT has already been well studied in the context of continual learning [a], and there is nothing new about it other than the task (i.e., DFML) to which the method is applied.\\n- Section 4 is misleading because of the mixture of existing and proposed methods. In particular, the second paragraph is almost the same as in the previous study [b].\\n- Experiments are conducted only on 4-layered convolution neural networks, and the performance is not verified in a realistic setting with heterogeneous architectures. Since the paper considers a practical problem setting for DFML, experiments using more architectures would help make the paper more convincing.\\n\\n[a] Wang, Liyuan, et al. \\\"A comprehensive survey of continual learning: Theory, method and application.\\\" arXiv preprint arXiv:2302.00487 (2023).\\n\\n[b] Hu, Zixuan, et al. \\\"Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning.\\\" CVPR 2023.\", \"Questions\": \"- Can the risk of TDS be avoided by splitting the inverted dataset for validation? In the context of generative models, methods for validation and model selection using synthetic data have been proposed [c].\\n- Can model-poisoning attacks be detected as OOD by visualizing or embedding data recovered from a pre-trained model into an existing model? The previous study [b] actually visualized the restored data, and pre-trained neural networks can be used for OOD detection [d].\\n- How much does the computational complexity of ROSY reinforcement learning increase relative to the baseline (PURER)?\\n\\n[c] Shoshan, Alon, et al. \\\"Synthetic data for model selection.\\\" ICML 2023.\\n\\n[d] Lee, Kimin, et al. \\\"A simple unified framework for detecting out-of-distribution samples and adversarial attacks.\\\" NeurIPS 2018.\"}"}]}
{"paper_id": "DmDpu3r8iU", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the advancements in Large Language Models (LLMs) and the emerging concern about their alignment with human values. Although LLMs have enhanced capabilities, their potential misalignment poses risks, which is difficult to evaluate due to their complex and adaptive nature. Some existing methods have tried evaluating LLMs' understanding of values and alignment through different cultural and ethical perspectives, but these are limited in assessing deeper understanding.\nGap: Existing methods focus on LLMs' emergence of values and ethical considerations but do not address their understanding of values—how LLMs categorize their responses within value frameworks and understand the reasons behind these categorizations. This paper identifies the need for evaluating both 'know what' and 'know why' elements of value recognition and understanding in LLMs.\nHypothesis: The authors hypothesize that a true understanding of values in LLMs requires assessment of both 'know what' (categorization of responses to value types) and 'know why' (understanding the reasoning behind these categorizations).\nMethod: The Value Understanding Measurement (VUM) framework is proposed to evaluate LLMs' understanding of values through the discriminator-critique gap (DCG) approach. Using the Schwartz Value Survey, the study develops a dataset with GPT-4 annotations to evaluate ten values. Five LLMs' responses are assessed on alignment with these values and their reasoning behind this alignment, using a combination of discriminator scores for 'know what' and critique scores for 'know why'.\nConclusion: The study finds that while 'know what' capabilities are impacted by scaling laws, 'know why' capabilities remain consistent across models, suggesting that LLMs might simulate understanding without genuinely grasping the values. This presents risks in plausible but context-driven explanations without true value alignment. Additionally, inherent understanding of risky values like 'Power' is insufficient in LLMs, influenced more by context than the models' inherent capabilities.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"rumor\": 20, \"Strengths\": \"- This paper studies a very interesting problem: what values are hold in the LLMs. Understanding the correct way to probe and measure this will be beneficial to the whole community.\\n- To me, the way the authors proposed to achieve this goal is novel.\", \"Weaknesses\": \"- I am confused by several parts when reading the paper:\\n  - In Figure 1, it's super unclear what is fed to the model in each step. Is the whole history included? It's more clear when reading through 4.3 but it would be much better if that can be reflected in the Figure. Also, it seems that the critique model only takes the two reasoning text as inputs. Then how does it deal with the contextualized phrases like \\\"the values I mentioned\\\"?\\n  - In 5.1.2 the authors mentioned they have a setting about \\\"Add prompts that make it relevant to the values\\\". I am also confused by this part. What are the prompts used here and where is the results? I would be better if the authors can \\\"dry run\\\" several examples for each setting in the appendix, which could help readers understand the whole process much easier.\\n  - I also had a hard timing parsing Figure 5. \\n\\n- The authors claimed that \\\"The scaling law (Kaplan et al., 2020) significantly impacts \\\"know what\\\" but not much on \\\"know why\\\", which has consistently maintained a high level\\\". I am not convinced by the argument:\\n  - Model parameters are not the single factor that affect the models' performance. For example, how many tokens they have been trained on, and what alignment data/methods are used heavily affect models' performance. Furthermore, there is no official statement from OpenAI about the size of GPT-3.5-Turbo (rumor: 20B model) and GPT-4 (rumor: MOE of 200B model). How is that possible to make such a claim without knowing the details of the two models?\\n  - I am also not convinced by the \\\"know why\\\" argument. Seen from Figure 3 and Figure 4, all models achieve quite similar scores. However, could it be that the GPT-4 doesn't have the capability to distinguish those models' outputs from the baseline reason? I saw the authors mentioned they did some kind of human study in Appendix B.2, but all the details about this human study is missing. I would like to see a thorough human study and analysis about this part.\", \"Questions\": \"see weakness\"}"}]}
{"paper_id": "kXHEBK9uAY", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Diffusion-based generative methods have shown effectiveness in trajectory modeling with offline datasets, but they face challenges in computational efficiency and generalizing temporal abstractions for long-horizon tasks. Hierarchical planning, where high-level plans with subgoals guide subsequent low-level plans, offers more efficient sampling and gradient propagation.\nGap: Current diffusion-based planning methods, like the Diffuser, are dense and flat, leading to computational inefficiency. They also inadequately cover dataset distributions and struggle with long-horizon, high-dimensional tasks, lacking a hierarchical approach to improve these aspects.\nHypothesis: Combining the strengths of hierarchical planning with diffusion-based modeling can lead to more efficient and effective planning strategies for decision-making in long-horizon tasks.\nMethod: The proposed Hierarchical Diffuser uses two diffusers: a high-level 'Sparse Diffuser' for subgoal generation and a low-level diffuser for achieving these subgoals. Training involves splitting trajectories into segments, with subgoals defined as segment split points. The Sparse Diffuser is trained on trajectories of subgoals, facilitating large receptive fields with lower computation. Low-level planners train on segments between subgoals and facilitate fine-tuning. The method was tested on offline reinforcement learning benchmarks, achieving parallel planning of subgoals and improved generalization.\nConclusion: Empirical results show that the Hierarchical Diffuser outperforms non-hierarchical methods and other hierarchical planning methods on standard offline RL benchmarks, with better training and planning speeds. The method also enhances generalization capabilities for compositional out-of-distribution tasks. The hierarchical approach also aids in efficient planning for longer horizons.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"Discretizing planning into sub-goals with diffusion models is shown to be advantageous as more diverse scenarios can be solved. Low-level diffusion planning becomes task-agnostic.\\n\\nEven the sparse diffuser or SD version is better than diffuser because of increased receptive field. The authors validate this by showing that increasing the kernel-size (hence the receptive field) of diffuser leads to better performance but weaker generalization.\\n\\nSD with dense actions leads to better fitting of the sparse objective, a typical bottleneck in hierarchical RL.\\n\\nHD takes lesser time than Diffuser due to shorter high-level sequence and parallelly solving low-level plans for all the segments.\", \"Weaknesses\": \"The formulation has limited novelty. While a single diffuser is not sufficient for planning over long-horizons, the work introduces two diffusers: one sparse diffuser for planning sequence of sub-goals between initial state and goal, while a standard diffuser solves for individual sub-goal segments.\\n\\nGiven that there are also methods which perform state-only diffusion like Decision-diffuser [2], is it possible to perform relevant ablations to justify why states are not sufficient for having a good estimate of $J$? If you are following diffuser’s codebase, are you using the diffusion sampled actions or a separate controller for Maze2d tasks like diffuser?\\n\\nSolving for sub-goals independently without the knowledge about the final goal might lead to non-optimal behavior. How do you ensure that the sampled path for one segment does not overlap with the paths for the other segment? How many trajectories do you sample for individual segments and for the sub-goal sequence?\\n\\nAlso, what happens when actions do not lead to the exact sub-goal (unless you use inverse dynamics actions) for the maze case? Because there is no-feedback from the low-level diffuser to high-level diffuser, it might become challenging.\\n\\nOverall, I acknowledge that the presented method is a promising revision of Diffuser which performs better in all aspects. However, I believe that the contributions are not significant enough.\", \"Questions\": \"See weakness above.\"}"}]}
{"paper_id": "i7LCsDMcZ4", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Event cameras, which are inspired by biological visual sensors, capture changes in brightness at each pixel asynchronously, mimicking the human eye. They offer benefits like low latency and high dynamic range, making them suitable for challenging conditions. Spiking Neural Networks (SNNs), modeled on how biological neurons operate, are designed to handle such event-based data. Despite this, SNNs face overfitting issues and lack saliency map generation methods, which are crucial for effective data augmentation and generalization.\nGap: There is no existing method for extracting saliency maps and class activation maps (CAMs) from Spiking Neural Networks (SNNs), which is a critical step for effective data augmentation and reducing overfitting in event-based data classification.\nHypothesis: The authors propose the hypothesis that by using relevance propagation techniques (Spiking Layer-Time-wise Relevance Propagation - SLTRP, and Spiking Layer-wise Relevance Propagation - SLRP), they can accurately generate saliency maps and class activation maps (CAMs) for SNNs. This will enable more effective event data augmentation, improving the generalization ability and performance of SNNs in classification tasks.\nMethod: The authors introduce two relevance propagation methods: SLTRP and SLRP, adapted from Layer-wise Relevance Propagation (LRP) for SNNs. SLTRP provides temporal precision, while SLRP is computationally less intensive. These methods are used to generate stable and accurate CAMs and saliency maps. Using these maps, the authors develop two data augmentation techniques: RPGDrop, which removes events in regions of high saliency, and RPGMix, which combines events while maintaining clear texture features by avoiding overlapping the most salient object areas. The methods are integrated into a framework called EventRPG, which combines RPGDrop and RPGMix with traditional augmentation techniques like geometric augmentations.\nConclusion: EventRPG, using the newly proposed SLTRP and SLRP methods, significantly enhances the generalization capability of SNNs. The approach achieves state-of-the-art performance on several event-based object and action recognition tasks, demonstrating that SNNs can be trained more effectively with relevance-guided data augmentation. However, the application is currently limited to classification tasks, suggesting potential future work in expanding these methods to other neural network tasks beyond classification.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"- The derivation of the CAM and saliency map of SNNs itself is a clear contribution. The results in Table 1 prove the correctness of this\\n- EventRPG is able to consistently improve performance across event datasets\\n- The time cost of EventRPG is comparable to similar augmentations in conventional vision\", \"Weaknesses\": \"My main concern is regarding the experiments:\\n- The paper motivates the need for event data augmentation with the statement that \\\"the lack of huge event-based datasets similar to Imagenet prevents us from improving the model performance on relatively small datasets using a Pretrain-Finetune paradigm\\\". However, there is an event camera version of ImageNet available [1], and its paper shows that pre-training on N-ImageNet can greatly improve the accuracy on other datasets via transfer learning. Therefore, this statement in the Introduction is wrong\\n- Since N-ImageNet is available, I would like to see results on this dataset. This is similar to conventional vision research on data augmentation, where ImageNet is the best testbed. If the authors are not able to train on N-ImageNet, the mini subset can be considered, though I do not think that is a comprehensive benchmark\\n- The authors compare EventRPG with conventional vision methods such as Grad-CAM in Table 1. Thus, I wonder if it is possible to compare with Puzzle Mix and SaliencyMix in Table 3?\\n- NDA applies the method to unsupervised contrastive learning and shows promising results. Is it possible to conduct such experiments using EventRPG?\\n\\n[1] Kim, Junho, et al. \\\"N-ImageNet: Towards robust, fine-grained object recognition with event cameras.\\\" ICCV. 2021.\", \"Questions\": \"See Weaknesses\"}"}]}
{"paper_id": "jHdz0CIS2y", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Voice conversion research typically focuses on disentangling speech into separate representations for speaker characteristics and linguistic content. However, this approach can lose finer nuances like accent and emotion. Recent works leverage self-supervised learning for extracting such attributes without text.\nGap: Existing methods that disentangle speech representations often result in loss of important nuances, and reliance on heuristic voice perturbation algorithms limits the diversity of training transforms. Current methodologies struggle with effective voice conversion, especially in cross-lingual and zero-shot contexts.\nHypothesis: The authors propose that using entangled speech representations from self-supervised learning and speaker verification provides a more effective base for voice conversion models, and these models can be iteratively improved through self-synthesized examples, enhancing speaker similarity and overall results in voice conversion tasks.\nMethod: SelfVC leverages entangled speech representations from a self-supervised learning model and a speaker verification model to train a controllable voice conversion system. Key techniques include: deriving prosodic information from audio, an iterative refinement training strategy using self-synthesized examples to improve the synthesis model, and operating without text, thus ensuring applicability across languages. The method is assessed through zero-shot voice conversion tasks and compared against existing models using a range of naturalness, speaker similarity, and intelligibility metrics.\nConclusion: SelfVC shows significant improvements over baseline models and prior work, achieving state-of-the-art results in zero-shot voice conversion. The integration of self-synthesized examples during training substantially enhances speaker similarity, and the model is competent across various languages and applicable to diverse tasks such as zero-shot conversion and cross-lingual voice conversion.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"2023\": 1, \"Strengths\": \"* The paper combines strategies from voice conversion and singing voice conversion to improve zero-shot voice conversion. One noteworthy contribution is the introduction of a novel training strategy that utilizes self-synthesized examples for data augmentation and iterative improvement of the generation model. This represents a significant advancement from traditional approaches that heavily relied on heuristic transformations.\\n\\n* The paper conducts comprehensive experiments. By extensively comparing the proposed framework with several baseline models, the authors effectively demonstrate its efficacy. \\n\\n* Overall, the paper successfully presents a valuable contribution to the field of voice conversion and offers an approach for achieving zero-shot voice conversion.\", \"Weaknesses\": \"* The main framework presented in this paper appears to draw from existing voice conversion and singing voice conversion techniques, such as utilizing SSL features, speaker embeddings, and incorporating prosody information like duration and pitch. These strategies resemble prior work ([1-3]) in the field. Additionally, the synthesizer's approach showcases similarities to methods used in speech synthesis, specifically resembling techniques employed in FastSpeech 2 [4]. To support these statements and provide a comprehensive overview of the related work in the field, it would be beneficial for the authors to provide appropriate citations and conduct comparative analyses.\\n\\n* Moreover, the claim of the framework's efficiency in scaling to other languages through the introduction of SSL features is not unique to the authors' proposed model since similar approaches have been explored elsewhere.\\n\\n* Additionally, it is worth noting that the concept of data augmentation using self-synthesized examples has been discussed in the literature, particularly in the context of speaker verification [5]. \\n\\n* Furthermore, the experimental results suggest that the inclusion of self-synthesized examples for data augmentation yields only a marginal improvement in the model's performance.\\n\\n* Consequently, a deeper analysis or controlled study could be conducted to better isolate and understand the specific effects of the proposed training strategy from other contributing factors.\\n\\nReferences:\\n\\n[1] Jayashankar T, Wu J, Sari L, et al. Self-Supervised Representations for Singing Voice Conversion[C]//ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023: 1-5.\\n\\n[2] Hussain S, Neekhara P, Huang J, et al. ACE-VC: Adaptive and Controllable Voice Conversion Using Explicitly Disentangled Self-Supervised Speech Representations[C]//ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023: 1-5.\\n\\n[3] Maimon G, Adi Y. Speaking style conversion with discrete self-supervised units[C]//EMNLP 2023. EMNLP, 2023.\\n\\n[4] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, “Fastspeech 2: Fast and high-quality end-to-end text to speech,”[C]//International Conference on Learning Representations (ICLR), 2021.\\n\\n[5] Cai D, Cai Z, Li M. Identifying Source Speakers for Voice Conversion Based Spoofing Attacks on Speaker Verification Systems[C]//ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023: 1-5.\", \"Questions\": \"* The authors should clarify what specific aspects of their framework, beyond the use of SSL features, contribute to this scalability to strengthen the claim of originality in this regard.\\n\\n* Could the authors elucidate the unique characteristics or modifications they have made within their framework that differentiate it from other SSL-based voice conversion models?\\n\\n* The introduction of the self-synthesized examples as a data augmentation method is intriguing. However, its efficacy and broader applicability remain questions. Can the authors provide experimental evidence or analysis showcasing the generalizability of this data augmentation technique across various methods?\"}"}]}
{"paper_id": "NkYCuGM7E2", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The existing learning-based autonomous driving (AD) systems often struggle with understanding high-level information, generalizing to rare events, and providing interpretability. This is largely due to their reliance on predefined rules and reward functions, which limit their ability to adapt to various scenarios and make decisions that are comprehensible to humans.\nGap: The paper addresses the limitations of current learning-based AD systems, which struggle with comprehending high-level information, adapting to rare driving scenarios, and lacking interpretability in their decision-making processes.\nHypothesis: The authors propose using Large Language Models (LLMs) as the decision-making component in AD systems to mimic human-like reasoning. They hypothesize that LLMs can enhance AD systems by leveraging commonsense reasoning and interpretable decision-making capabilities, improving safety, efficiency, and adaptability in complex driving scenarios.\nMethod: The method involves integrating LLMs to conduct high-level decision-making for AD. The system uses a cognitive pathway framework that involves a chain-of-thought process, allowing LLMs to work through sub-problems systematically. The decisions made by LLMs are translated into actionable commands for low-level controllers using guided parameter matrix adaptation, focusing on Model Predictive Control (MPC) systems. Experiments were conducted to assess the capability of this integrated system in both single and multi-vehicle scenarios, including typical and complex traffic situations.\nConclusion: The paper concludes that employing LLMs as high-level decision-makers in AD systems outperforms traditional methods, especially in single and multi-vehicle tasks. The system improves traffic flow efficiency, decreases safety penalties, and adapts better to complex scenarios, such as navigating intersections or coordinating multiple vehicles. It showcases the potential of LLMs to function as interpretable, effective components in AD, suggesting promising directions for further research.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. Combining LLM and MPC is a smart way to leverage the high-level reasoning capability of LLM. Since LLM is not good at low-level control as shown in previous literature, using MPC to output low-level actions helps incorporate strong prior knowledge into the whole pipeline.\\n\\n2. Rare and long-tail events put great challenges to data-driven algorithms. Using common sense to solve such long-tail problems is a promising way. Since LLM already contains a lot of common sense, this paper is a good attempt to explore this direction.\\n\\n3. The paper is well-written and easy to follow. Figure 2 provides a clear illustration of the entire pipeline, which makes me easy to understand the method proposed in this paper.\", \"Weaknesses\": \"1. A common question for LLM-based agents is the trustworthiness. When used for safety-critical applications like autonomous driving, the requirements for safety are much higher than QA or code generation tasks. As far as I know, GPT3.5 and GPT4 have uncontrollable randomness even with temperature = 0. Does this randomness influence the running results? How to ensure that LLM always outputs reasonable results. Are the results consistent if the version of the ChatGPT model changes?\\n\\n2. How about the running frequency of the entire system? Since driving requires a high frequency to make decisions, the speed of processing the information with language may not be fast enough. This is one of the biggest challenges of using LLM for driving, which may be unaffordable and unsolvable with a general-purpose LLM. Using LLM which can only communicate with downstream modules with language wastes a lot of time on extracting information.\\n\\n3. In the first paragraph of the introduction, the authors provide an example “Human drivers intuitively know that according to traffic rules, they should slow down and yield, even if it is technically possible to speed through”. This example is confusing to me as it just shows two types of common driving styles of human drivers (aggressive and conservative styles). I am not aware of any complex rules or reward functions here since it is easy to force the driving algorithm to be conservative with some simple rules. Actually, considering the complexity of the pipeline designed in Figure 2, I am not sure which one is more complex. Maybe several simple cost functions in an MPC can already solve the scenarios in the experiment part.\\n\\n4. It seems that one important motivation for using LLM for driving is solving rare and long-tail events. However, I only find Figure 1 discusses one specific case where a breakdown vehicle is stopped in the middle of the road in a roundabout. I can’t find any evidence or experiment results showing that LLM broadly solves long-tail problems. \\n\\n5. In the evaluation part, only one RL algorithm (not mentioned which algorithm exactly) and one MPC are compared. Since modern AV algorithms are usually complex, I don’t think such a comparison is not convincing enough to demonstrate the advantage of the LLM method.\", \"Questions\": \"1.\\tCould the authors report the running time of all methods in the experiments? \\n2.\\tThere are 5 scenarios mentioned in Table 1 but no detailed explanation of them. I cannot evaluate the realism and difficulty of them.\\n3.\\tCould the authors explain more about the interpretability of the LLM method? Does this interpretability mean the reason for decision-making? Why is such interpretability useful and can we always trust such interpretability? If the interpretability is something shown in Figure 1, I think all collision avoidance algorithms can provide the reason for the decision.\"}"}]}
{"paper_id": "RtDok9eS3s", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The transformer architecture, introduced in 2017 by Vaswani et al., has been a cornerstone of various advancements in deep learning. It typically involves stacking identical transformer blocks, which are complex structures made up of attention and MLP sub-blocks, skip connections, and normalization layers. Despite their complexity, these blocks have undergone little modification since their initial design. Simplification of these blocks could help bridge the gap between theoretical studies, which often assume simpler architectures, and practical implementations. Signal propagation theory has previously been employed to study and enhance deep architectures like transformers by mitigating signal degeneracies.\nGap: Standard transformer blocks, although crucial for neural network architectures, are intricate and somewhat rigid, making them sensitive to minor architectural changes. This complexity can lead to drastic impacts on training efficiency and even render models untrainable under slight alterations. There's a compelling need to determine how much these standard blocks can be simplified without compromising training speed or performance.\nHypothesis: The authors hypothesize that the standard transformer block can be significantly simplified by removing several components without any loss in per-iteration training speed or performance. These components might include skip connections, value or projection parameters, sequential sub-blocks, and normalization layers.\nMethod: The authors propose and test simpler transformer block designs based on signal propagation theory and empirical studies. They suggest various modifications: removing skip connections, setting value and projection parameters to identities, employing shaped attention, and using parallel block structures. These changes are evaluated through experiments on autoregressive decoder-only and BERT encoder-only models. The study largely uses a 18-block 768-width model on the CodeParrot dataset, applying standard setups from existing literature for baselines. For BERT experiments, the authors build on the Crammed BERT setup, using the Pile dataset for pre-training and GLUE for fine-tuning, with a masked language model pretraining objective.\nConclusion: The authors successfully demonstrate that transformer blocks can be simplified significantly. By removing several components like skip connections, value, and projection parameters, and using simplified attention strategies, they achieve models with 16% faster training throughput and 15% fewer parameters while maintaining the performance of standard transformers. This work suggests that streamlining transformer blocks could lead to better resource efficiency in training and inference without sacrificing model capability.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"- As the work mentions, this is the first work which has simplified the transformer architecture with training throughput gains. Previous works which have tried to simplify transformer architectures have led to increase in training speeds.\\n- The paper is overall well written. The authors discuss each design choice in detail, which helps in understanding the motivation behind the simplifications proposed.\\n- The authors have appropriately discussed the limitations of their work.\", \"Weaknesses\": \"- One weakness is that the results are limited to smaller models of size ~100M. It is not clear if the results would scale to bigger models. \\n- Moreover, the authors only consider GLUE benchmark for downstream evaluation. It would be nice to include a more comprehensive evaluation across different task categories like classification, generation or reasoning.\", \"Questions\": \"- The authors mention that there is a Pre-LN and a Post-LN block and the Pre-LN work is more popular because Post-LN suffers from training instability. Is this phenomenon observed in the signal propagation literature only or all the current LM works also use this Pre-LN variant?\\n- The authors mention “our simplified transformers match the per-update training speed and performance of standard transformers, while enjoying 15% faster training throughput”. If the parameters are being reduced, why isn’t the per update speed faster? And, why is the throughput faster?\\n- On page 3, the authors mention skipless transformers are slower than those with skip connections. Why is that?\\n- Are there any settings (data or tasks) where this simplified transformer does not work as well compared to the standard transformer?\\n- What are the limitations of the proposed simplifications in terms of model expressiveness and capability?\\n- What is the size of the model which has 18 blocks and 768 hidden dimensions?\\n- In section 4,1 what is the motivation from going from SkipInit to ShapedAttention? Another question is since this work is adding identity addition to value and projection matrices, in what way this is simpler than the residual connection?\"}"}]}
{"paper_id": "YCPDFfmkFr", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The use of optimization layers in neural networks has gained popularity due to their capacity to solve diverse machine learning tasks and capture domain-specific knowledge. This involves incorporating differentiable optimization problems as layers, allowing for complex domain modeling within the architecture. Specifically, convex quadratic programming (QP) layers are of interest due to their rich modeling capabilities and computational efficiency. Differentiable optimization frameworks have been developed, but typically assume feasibility of the underlying optimization problems.\nGap: The main issue addressed is the potential infeasibility of QP layers during neural network training. Previous approaches often enforce feasibility through architectural constraints, which limits the modeling power and may require additional learning variables. These methods rely on maintaining primal feasibility to differentiate QPs, which is not always guaranteed in dynamic learning tasks like control and robotics.\nHypothesis: The authors propose that by leveraging primal-dual augmented Lagrangian techniques, they can compute derivatives for both feasible and infeasible QP solutions. This enables differentiating through infeasible QPs during training to move towards feasibility at test time, enhancing the expressiveness of QP layers.\nMethod: The method involves extending the definition of QP solutions to the closest feasible ones when infeasible, using the classical least squares sense. The authors introduce an Extended Conservative Jacobian (ECJ) framework to path-differentiate these solutions with respect to the parameters. They provide ways to compute ECJs in both forward and backward modes of differentiation. The practicality of this approach is demonstrated through the implementation of QPLayer, a C++ package for differentiating feasible and infeasible convex QPs, interfacing with common learning frameworks.\nConclusion: The proposed method shows that it is possible to handle infeasible QP layers in neural network training, thus broadening the range of layer architectures that can be effectively employed. By allowing the relaxation of primal feasibility constraints, the approach enables the training of QP layers to achieve superior predictive performance in certain tasks. Experimental results suggest that this method is numerically robust and efficient compared to existing methods, providing improved predictive power in structured tasks such as Sudoku solving.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"1. The paper studies an important problem.\\n2. The paper proposes a new approach to calculate gradients in convex QP layers.\", \"Weaknesses\": \"1. The paper is not easy to follow. See the comments below.\\n\\n2. The analysis in the paper is not sufficient. See the comments below.\", \"Questions\": \"1. The paper is not easy to follow. In some key sections or proofs, the authors suggests to read some other referecens to undertand the concepts. This makes reading the paper difficult for readers that are not working on this specific area of extended conservative Jacobian. To make the paper easier to read, the authors are suggested to add the basic concepts and ideas in the paper instead of requesting the audience to read a number of other reading materials.  \\n\\n2. To address the infeasibility issue for the QP layers, one straightfoward way is to introducing slack variables and add penalties about the constraint violation in the objective function. The authors are suggested to show the advantage of the proposed approach as compared to this straightforward approach.\\n\\n3. One application of the QP layers is to use it as a building block for existing iterative algorithms (similar to the approach in [R1] and [R2]). The authors are suggested to analyze the convergence performance of the iterative algoritms when using the propsoed conservative Jacobian as a building block.\\n\\n\\n[R1] Donti, P.L., Rolnick, D. and Kolter, J.Z., 2021. DC3: A learning method for optimization with hard constraints. arXiv preprint arXiv:2104.12225.\\n[R2] Donti, P., Agarwal, A., Bedmutha, N.V., Pileggi, L. and Kolter, J.Z., 2021. Adversarially robust learning for security-constrained optimal power flow. Advances in Neural Information Processing Systems, 34, pp.28677-28689.\"}"}]}
{"paper_id": "ueTdErd5Ib", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper addresses contextual stochastic optimization problems where optimization must handle uncertain parameters from unknown, context-dependent distributions. Historically, a two-stage process of predict-then-optimize has been used, which often fails to produce optimal and robust results. Recent methods aim for end-to-end learning but focus on minimizing average cost without incorporating robustness, which is crucial for real-world applications.\nGap: Existing methods primarily focus on minimizing expected costs without adequately addressing robustness against worst-case scenarios. The unpredictability of uncertain parameters in optimization tasks remains problematic, as traditional methods either separate prediction from optimization or overly simplify the relationship between predictions and decisions.\nHypothesis: The authors propose a method that integrates learning with the optimization task, aiming to prescribe decisions maximizing the likelihood that costs remain below a user-defined threshold, enhancing robustness against uncertainty.\nMethod: The proposed method involves discretizing the feasible region into subsets to approximate the uncertain objective function deterministically within each subset. Then, a secondary optimization problem integrates these approximations to prescribe decisions, aiming for maximized robustness and minimized regret. Theoretical guarantees bound the regret of decisions, and experiments, including a real-world electricity generation problem, demonstrate robustness and reduced worst-case costs compared to traditional methods.\nConclusion: The proposed discretization framework for contextual stochastic optimization demonstrates competitiveness in average regret and significantly improved robustness over existing methods. Theoretical and experimental validations show reduced worst-case costs, suggesting practical advantages for applying this method in real-world scenarios.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"Proposed a new framework to solve robust contextual stochastic optimization problem. Please see my comments in Summary for details.\", \"Weaknesses\": \"Not fully clear presentation/discussion of their algorithm. Please see my comments in Summary for details.\", \"Questions\": \"Please see my comments in Summary for details.\"}"}]}
{"paper_id": "sDmjlpphdB", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large Language Models (LLMs) have demonstrated a strong capacity to generalize across novel tasks when guided by effective language prompts and in-context learning demonstrations. Prompts, which comprise instructions for tasks, are critical to the models' ability to perform complex tasks, as evidenced by earlier methods that tried to automate prompt design.\nGap: Previous methods in prompt optimization constrained the search to demo-free instructions, limiting their ability. A single instruction may not suitably cover the diverse problem space of complex tasks, given that it lacks the context provided by demonstrations.\nHypothesis: The paper proposes that by adopting a Mixture-of-Experts approach, where the problem space for prompts is divided into homogeneous regions with specialized experts, and by extending prompts to include both instructions and demos, the performance of automatic prompting can be significantly enhanced.\nMethod: The paper introduces Mixture-of-Prompts (MoP), which divides the problem space into regions managed by different experts—each with a customized prompt containing specific instructions and demos. The methodology uses a two-phase process: firstly, demos are assigned to clusters based on semantic similarity using kernel regression principles; secondly, instructions are allocated through a region-based joint search to complement these demo clusters. Experiments were conducted on benchmark NLP tasks using datasets formatted for these tasks to evaluate MoP's performance over competing techniques.\nConclusion: The MoP framework outperformed previous prompt optimization methods on benchmark NLP tasks, with improvements up to 43%. The framework successfully showed that splitting tasks into smaller, similar problem spaces dedicated to specialized experts with tailored prompts can significantly improve task performance.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"S1: I like their presentation and language writing, which is clear.\\nS2: they proposed two-step search algorithm, which leverages semantical similar- ity for demo assignment and routing function and region-based joint search for instruction assignment, achieves significant performance gains on the APE Benchmark.\", \"Weaknesses\": \"W1: The idea of grouping training demos into homogeneous clusters, with each cluster corresponding to a specific expert, is disputable. This method actually assumes that all the potential demos or test queries can be all decomposed by these groups. This causes several problems: (1) how to make sure the given training demos are representative enough to cover all the cases? (2) how to make sure your clustering method is effective in making each cluster homogeneous? (3) how to make sure your generated groups are qualified to support better performance? (4) If each group corresponds to one expert, how to set an optimal number of experts for different clustering methods? How to make sure these experts are representative enough, how to achieve the balance between expert number and efficiency?   \\n\\nHere is a very simple solution to all the problems raised by W1: ignore/remove the clustering component, and treat all the demos as normal instances that can be represented by a K-dimensional latent space where each dimension corresponds to one expert. I would like to see the feedback from the authors.\\n\\n\\nW2: can I assume that the proposed INSTRUCTION ASSIGNMENT is just the combination of Independent Search and Joint Search? I thank the authors for their insights but it is too straightforward, and there exist more important problems that should be considered. For example, since each expert corresponds to one sub-instruction, there is a pressing need to make sure these instructions are heterogeneous. Achieving this goal is not easy, for example, maybe we should maximize KL divergence among pairs of sub-instructions. I didn’t find such effort in this paper.\\n\\nW3: Overall, I thank the authors for their hard work in solving an interesting problem. But I am not surprised about their novelties.\", \"Questions\": \"Q1: In Equation (3), if we let $C=1$, can I treat this equation as this: we use one region with the entire training demos to search a single instruction? So the insight of your Mixture-of-Expert for prompt optimization is: you think searching one single instruction is hard (no matter whether we have training demos), and you reduce the difficulties by splitting this problem into several sub-problems (searching the combination of multiple sub-instructions with demos)\\n\\nIf my understanding is correct, I think the following sentence in your paper is over-claimed: \\\"a limitation of existing auto-prompting methods is that they solely focus on searching for an optimal demo-free instruction\\\" (below equation 2). It seems that the above sentence should be replaced with something like: \\\"a limitation of existing auto-prompting methods is that searching an optimal instruction is usually very hard\\\"\\n\\nQ2: W1\\n\\nQ3: W3\"}"}]}
{"paper_id": "Mdk7YP52V3", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Heteroskedastic regression models allow modeling of non-constant output noise, unlike traditional homoskedastic models which assume noise is constant across inputs. Such models can be useful in scenarios where output noise varies with input, highlighting areas where a model \"knows it fails.\" However, real-world applications have shown that overparameterized heteroskedastic neural networks are prone to overfitting, either by fitting every data point too closely or by considering targets as pure noise.\nGap: There is a lack of comprehensive theoretical understanding of the failure modes of overparameterized heteroskedastic regression models, particularly in their propensity to overfit data in extreme cases. Prior studies have not provided a model-agnostic theoretical framework to explain these phenomena, which indicates a gap in understanding the foundational reasons behind the observed pathologies in model behavior.\nHypothesis: The paper hypothesizes that the observed overfitting and model instabilities in overparameterized heteroskedastic regression models can be explained through a theoretical lens using statistical physics. It posits that these issues are not unique to neural network architectures but are inherent to the field theory of an overparameterized conditional Gaussian likelihood model.\nMethod: The authors employ a field-theoretical approach derived from statistical physics to study the pathologies in heteroskedastic regression models. They propose a nonparametric free energy (NFE) framework to model the behaviors of overparameterized models, suggesting that solutions to the differential equations derived (from this field theory) can explain the behaviors observed in practice. They also present a phase diagram to describe the model's behavior under varying regularization strengths and empirically validate the framework against real-world data using neural networks and numerical solutions to the derived equations.\nConclusion: The study finds that overparameterized heteroskedastic models exhibit phase transitions—sharp changes in behavior under varying regularization strengths—a finding that aligns with the NFE solutions. This highlights the necessity for appropriate regularization in these models. Furthermore, the authors suggest a more efficient regularization tuning approach aligning with numerical insights. Their theoretical framework helps explain issues with these models, generic across architectures and datasets, potentially leading to improved regularization strategies in practical applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"1. (main) Conceptually interesting: a richer understanding of regularization for both mean and deviation.\\n2. Uncertain quantification: an important topic, this paper provides some ideas about how to tame overparametrization\", \"Weaknesses\": \"1. (main) Is there a sharp phase transition? This paper lacks quantitative measurement and results. It would be interesting to calculate, at least under certain simple generative models, the free energy and check if there is a first-order/second-order phase transition. \\n2. (main) Technically speaking, not sure which part of the paper is innovative---for example, Eqns 7--10, are they new or semi-new (i.e., similar derivations are obtained in a different context)?  I would be skeptical that Eqns 7--10 are entirely new. Also, the proposed regularization is well-studied in the literature.\\n3. Data experiments are simple, but I am mostly fine with that, since this paper is mainly proof-of-concept.\\n4. It is a bit handwaving when transitioning from parameter norm regularization to gradient regularization\\n$$\\n\\\\int \\\\alpha || \\\\nabla \\\\hat \\\\mu(x) ||_2^2 dx, \\\\qquad \\\\int \\\\beta || \\\\nabla \\\\hat \\\\Lambda(x) ||_2^2 dx.\\n$$\\nI feel that there are missing gaps between parametric models vs nonparametric models, though the idea can be understood intuitively.\", \"Questions\": \"See the above section\"}"}]}
{"paper_id": "YGTSLDAPqb", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Domain adaptation is critical in scenarios where machine learning models trained on labeled source domains encounter significant performance drops when deployed on out-of-distribution (OOD) target domains. This issue is prominent in fields like astronomy, where labeled data is limited to bright, nearby objects, leaving datasets of faint, distant objects largely unlabeled. Existing strategies leverage self-supervised pretraining on unlabeled data, but these approaches may not always yield the desired improvements in adapting models to OOD data.\nGap: The existing approach of standard fine-tuning following self-supervised pretraining does not consistently enhance OOD performance over purely supervised training on labeled source data. There is a disconnect because generic data augmentations used in pretraining may not effectively bridge the input space between source and target domains.\nHypothesis: To resolve the limitations of conventional fine-tuning techniques, the hypothesis is that self-supervised pretraining should be followed by fine-tuning with targeted augmentations specifically designed to account for known distribution shifts between the source and target domains, thereby improving OOD performance.\nMethod: The proposed 'Connect Later' framework entails two phases: first, self-supervised pretraining with generic augmentations to develop robust representations within both source and target domains. Second, a fine-tuning phase employs targeted augmentations, crafted based on the understanding of distribution shifts to better align the domains. This methodology is validated across four real-world datasets, involving different tasks such as astronomical object classification, redshift regression, wildlife species identification, and tumor detection.\nConclusion: Connect Later demonstrated superior performance, achieving state-of-the-art results in multiple benchmarks. The framework notably improved OOD performance by 12% in the AstroClassification task, reduced RMSE in Redshifts by 11%, and provided similar improvements in iWildCam-WILDS and CAMELYON17-WILDS datasets. The study validates that using targeted augmentations post pretraining can significantly bolster OOD error metrics, addressing previous inefficacies in fine-tuning alone.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"This paper looks at an interesting problem: Unsupervised domain adaptation is a very relevant problem (e.g., in many cases practitioners have labelled datasets available from a curated/lab setting, but need to generalize to actual deployment conditions). At the same time, self-supervised learning has become very popular, but its performance in the context of unsupervised domain adaptation (UDA) is still unclear.\\n\\nThis paper introduces a new dataset (RedShifts), and contains some encouraging results on two different domains (camera trips and astronomical observations) while performing several relevant ablations (model scale, whether to train the last layer before fine-tuning, strength of pre-training augmentations).\\n\\nThe paper is relatively easy to read, with good writing and a clear structure.\", \"Weaknesses\": \"Overall, I find this paper lacking in a variety of areas:\\n\\nThe authors' refer several times to Shen et al. (2022). This paper argues that contrastive pre-training is beneficial as long as data augmentations ensure that augmented examples must be more likely to change class or domain than changing both. The authors of this manuscript seem to argue in section 3 that the bad results they observe are explained by a violation of this assumption from Shen et al. (2022). However, this isn't tested rigorously (see section 6 of Shen et al., 2022, for examples on how to evaluate connectivity on real world datasets).\\n\\nA second shortcoming is that different self-supervised methods were used for the iWildCam-WILDS dataset (SWaV contrastive learning) and the astronomical time-series (masked autoencoding). This makes it hard to draw conclusions from table 1, since it is unclear which differences can be attributed to the use of different pre-training objectives.\\n\\nThe results on the iWildCam dataset are not very strong (given the means and standard deviations listed, there's a 20% chance that ERM + targeted augs outperforms Connect Later for any run). This leaves the improvements in the astronomical time-series datasets as the stronger proof of Connect Later's performance. But these results are a bit confusing to me: How is it that standard fine-tuning improves in-domain performance (tables 1 and 2)? This suggests to me that these datasets are possibly very small and benefit strongly from the regularization that pre-training provides. In that case, is Connect Later really addressing a domain shift issue? Or is it just addressing a regular overfitting issue? The latter seems plausible given the very small size of the dataset (6,74 objects). I would argue that in order to draw conclusions, the method should be tested on a wider variety of larger datasets.\\n\\nA baseline that seems to be missing as well is the use of general augmentations during fine-tuning (my understanding is that standard fine-tuning uses no augmentations at all). This would clarify whether it is important to use targeted augmentations during fine-tuning, or whether any form of augmentations would be fine.\\n\\nOverall, I find the insights that this paper provides limited: It is not surprising to me that targeted augmentations would help with fine-tuning, given that they were shown to help with regular ERM. The bigger question to me is the one raised in section 3: Why does pre-training not always lead to increased OOD performance? This question remains largely unanswered. I think this paper would be better if it provided, for example, (1) clear evidence that the assumptions from Shen et al. (2022) are routinely violated in real-world datasets and commonly used augmentations, (2) a thorough evaluation of how different pre-training techniques (MAEs, contrastive learning) perform in the UDA setting, or (3) strong evidence that targeted augmentations are particularly important compared to general augmentations.\", \"Questions\": \"See weaknesses.\"}"}]}
{"paper_id": "JTcaziw7G1", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large language models (LLMs) have shown impressive abilities in answering queries based on pre-trained knowledge. However, for information not included in pretraining data, information retrieval can augment the generation capabilities of LLMs. Private information, which needs to be retrieved, is increasingly stored in distributed settings by organizations and individuals.\nGap: The problem addressed is the leakage of information about queries and database content during information retrieval using neural embeddings unless both are stored locally. The existing methods do not ensure privacy when the database is distributed across multiple servers.\nHypothesis: The authors propose that it is possible to achieve private retrieval augmented generation (PRAG) for LLMs by using multi-party computation (MPC) techniques that ensure secure query transmission and retrieval from distributed servers without any server observing the client’s query or the database content.\nMethod: The proposed approach, PRAG, uses a combination of secret sharing and MPC to securely transmit queries to a distributed set of servers containing a private database. It leverages a novel MPC-friendly protocol for inverted file approximate search (IVF) that allows for efficient document searches. The method is designed to achieve sublinear communication complexity, enabling private and scalable information retrieval useful for LLMs without centralizing data or exposing sensitive information. The experiments involve synthetic and real data to evaluate retrieval accuracy and speed across various parameters, demonstrating the method's efficacy.\nConclusion: PRAG successfully provides a privacy-preserving solution for dense information retrieval that remains efficient in terms of communication complexity. The approach allows for the secure augmentation of LLMs with external information while maintaining the confidentiality of both queries and database content. This lays a foundation for more secure LLM applications, though the computational costs suggest a need for further optimization.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1.\\tAddress the privacy issue in the LLM augment generation process.\\n2.\\tThe design provides security guarantees for both users and external information supplier.\", \"Weaknesses\": \"1.\\tThe design is ad-hoc, just combining existing protocols together without design efforts.\\n2.\\tThe approximate search design in Sec 2.5 may lead to the leakage of centroids, which is not well-elaborated.\\n3.\\tThe design is very slow and impractical.\", \"Questions\": \"1.\\tIt seems that such design can also be implemented with other linear secret sharing schemes, why do you choose to use the Shamir’s shares (instead of arithmetic secret shares, replicated secret shares, etc.)?\\n\\n2.\\tThe authors claim that it is possible to use perturbations to mitigate the leakage in the approximate search design, but it is unclear what the suitable parameters is for this noise.\\n\\n3.\\tAlso, it is unclear the security level after adding the noise, a proof sketch is required to understand whether the countermeasure is effectively addressing the leakage.\\n\\n4.\\tThe proposed scheme becomes very slow (at least 50s needed) when the database size goes to a million scale, but such database is quite common in the real-world. This renders the impracticality of proposed design.\\n\\n5.\\tEven if we take the approximate design into consideration, it is still slow with a large k, I am wondering whether a large k is also common in the real-world systems because the LLM may want to embed more external resources to produce a general response for queries.\"}"}]}
{"paper_id": "lEkFq4RUCX", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: 3D point clouds, used in computer vision, 3D modeling, and robotics, represent geometric shapes of objects in an unstructured form. Traditional methods for measuring differences between 3D point clouds, like Earth Mover's Distance (EMD) and Chamfer Distance (CD), focus on point-wise correspondence. These methods are often memory and computationally heavy (EMD) or prone to reaching local minima (CD).\nGap: Current metrics for assessing the dissimilarity between 3D point clouds focus on point-wise differences and are often inefficient or ineffective, particularly when different sampling might represent the same surface. These metrics miss evaluating the geometric surface differences and are computationally intensive or lack robustness in practical scenarios.\nHypothesis: The authors propose a new metric, Directional Distance Field (DDF), which measures the difference between the underlying 3D surfaces of point clouds through directional distances from reference points. This metric is expected to be more efficient and accurate for tasks involving 3D point cloud processing, like shape reconstruction, registration, and scene flow estimation.\nMethod: The DDF metric involves generating reference points that induce local surface geometry of 3D point clouds. Each reference point calculates its directional distance to both point clouds. The difference in these distances characterizes the geometric difference. The final DDF is the average of these differences across all reference points. This method transforms and evaluates multiple 3D tasks - including shape reconstruction using normalized point clouds from ShapeNet, rigid registration with the Human dataset, scene flow estimation with Flyingthings3D, and feature representation using auto-encoding.\nConclusion: The proposed DDF metric significantly improves accuracy in analyzing 3D point clouds across various tasks, offering a more memory-efficient and computationally effective solution compared to existing metrics like EMD and CD. By focusing on underlying surface differences rather than point-wise correspondences, DDF enhances the potential of 3D point cloud processing frameworks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 3, \"Strengths\": \"1. It is reasonable to utilize the surface where the point clouds should be to compute the distance of two 3D point clouds.\\n2.The experiments show the proposed method improves the performance of all the downstream tasks.\\n3. The implementation of methods of all the downstream tasks are explained in detail.\", \"Weaknesses\": \"The generation of reference points are not explained very clearly. The reviewer is confused by the shared identical weight operation and the reference point generation process.\", \"Questions\": \"The reference points are generated from one of the two point clouds, and the weights in Equ.1 are computed for each kNN points and reference points. The reviewer wants to ask how to share the weights in g(q_m, P1) and g(q_m, P2)?\"}"}]}
{"paper_id": "Fq8tKtjACC", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The training of large artificial neural networks, especially with the advent of Transformer architectures, has seen significant progress, where scaling laws have shown that performance typically improves with increased compute and model size. Recent work has explored alternative approaches focusing on improving the quality of training data to enhance model performance, potentially matching larger models while reducing dataset size and computational cost.\nGap: Current approaches for training large language models for code generation heavily rely on large-scale data and computational resources. However, there is a need to explore if high-quality data can allow achieving state-of-the-art performance with smaller models and resources.\nHypothesis: The authors propose that using 'textbook quality' high-quality data in training large language models can significantly improve performance, matching or surpassing larger models while reducing the required data size and computational resources.\nMethod: The authors introduce phi-1, a Transformer-based model with 1.3B parameters, trained on 7B tokens of high-quality data. This includes 6B tokens from filtered web sources and 1B synthetically generated tokens using GPT-3.5. The initial phase involves pretraining on a code-language dataset and further finetuning on a set of synthetic coding exercises. The training was executed on 8 A100 GPUs for 4 days, focusing on tasks like writing Python functions from docstrings, evaluated using the HumanEval benchmark. Key distinctions include data filtering using a classifier trained on GPT-4 annotations and generating synthetic datasets designed to enhance diversity and instructiveness.\nConclusion: Despite its smaller size, phi-1 achieves 50.6% pass@1 accuracy on HumanEval and 55.5% on the MBPP benchmark, outperforming several larger models. The findings show that leveraging high-quality data can break the existing scale-performance relationship, achieving high performance with smaller models. However, there are limitations, such as the model's specialization in Python and sensitivity to prompt variations, but these can potentially be addressed with further refinements in data and methods.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 4, \"Rating\": 8, \"Confidence\": 5, \"Strengths\": \"This paper proposes a relatively high efficiency approach to training LLMs. It is impressive to get such results from a model trained on 8 A100s in 4 days. This makes this work approachable from academic research labs, which takes a step towards reversing the trend of pursuing ever larger datasets with larger models and computational requirements. This also has implications for energy efficiency and sustainability.\\n\\nThe paper itself is well-written and quite clear.\\n\\nThe authors have commited to the release of the \\\"model for usage and evaluation by the broader community\\\". This is important for open science and a big strength.\", \"Weaknesses\": \"Key details on the data generation process are not shared for \\\"proprietory reasons\\\", yet this is central to the paper, its proposition and its results. \\n\\nThe lack of a broader impacts section weakens this paper.\\n\\nIf this weakness is addressed, I am happy to improve my score.\", \"Questions\": \"Would you argue that the structure and compositionality of programming code is critical for achieving such high levels of performance from small models and datasets. I wonder if a takeway is to always seed training from well structured and commented code? If so, would this translate to other programming/instruction like language tasks? e.g. recipes, etc..\\n\\nDo you think that the code excercises factor into improving performance by encouraging the model to self-critic whatever it generates, given that it seems to append excercises to generations?\\n\\nWhere do you think this approach is most likely to fail? With what kinds of tasks/data?\\n\\nDoes phi-1 have ICL abilities? And are there other emergent capabilities that can be forseen and easily tested?\\n\\n\\nUpdate: Authors have addressed my questions.\"}"}]}
{"paper_id": "8fQlGQkj0S", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large language models have shown improved predictive performance when provided with in-context samples. In-context learning (ICL) has been connected with theories relating to Bayesian inference and gradient descent. However, there exist two distinct modes: task learning and task retrieval, which have not been thoroughly examined together in prior analyses.\nGap: Existing analyses largely overlook the impact of pre-training distributions on ICL and lack explanations covering both task retrieval and learning within the same framework.\nHypothesis: The hypothesis is that by modeling both pretraining data and in-context samples using generative models, researchers can derive risk upper bounds for task retrieval and learning, providing clear separations and understandings between these two modes.\nMethod: The authors propose generative models for pretraining data and in-context samples, using a Gaussian mixture model to reflect task diversity. They assume the pretraining data is drawn from noisy task distributions centered around clean tasks. They then explore in-context prediction using a Bayes-optimal next-token predictor as equivalent to the posterior mean of the label conditioned on in-context samples, deriving risk upper bounds for ICL. Numerical computations and experiments on Transformers validate their findings.\nConclusion: The paper concludes that the task learning risk decreases quadratically with more in-context samples, while task retrieval risk follows a U-shaped pattern, suggesting an optimal number of in-context samples exists for task retrieval. These findings provide a new theoretical framework distinguishing between in-context task learning and retrieval modes.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"- The authors highlight the task retrieval and task learning aspects of ICL. \\n- The authors provide risk bounds for both the setup under a mixture of linear Gaussian generative model.\", \"Weaknesses\": \"- The generative model is simplistic as it is limited to linear Gaussian mixture. In contrast HMM based ICL is already studied in other works already mentioned in the paper. The authors should explain the importance of studying this model. \\n\\n- The authors leave out some area of works that are related, e.g. meta-learning and retrieval augmented learning. A few recent examples of the latter are -  'A Statistical Perspective on Retrieval-Based Models' by Basu et al., 'Generalization and stability in in-context learning' Li  et al. \\n\\n- The dependence of U shaped risk bound on component reweighting and shifting is not discussed properly. Furtheremore, U shape is also observed through the bias-variance tradeoff in optimization literature. The authors don't connect the U shaped mention here with the bias-variance tradeoff. \\n\\n- The novelty in deriving the posterior distributions for the mixture of Gaussian distribution is unclear to me.\", \"Questions\": \"Please look at the weakness section.\"}"}]}
{"paper_id": "hRos9WldRK", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Deep learning in computer vision has made significant advancements, with its success largely attributed to large, high-quality datasets. However, label noise, prevalent in real-world datasets due to factors like annotation errors, hinders model performance. Existing methods for learning with noisy labels primarily involve loss correction by estimating noise and adjusting loss functions, which often require assumptions or can lead to neglecting noisy yet informative data.\nGap: Traditional methods struggle to handle high levels of noise effectively and often depend on assumptions about the noise model or require a clean validation set, which might limit their applicability and effectiveness, especially when relying on assumptions or potentially discarding valuable noisy data.\nHypothesis: The authors propose that a joint reweighting mechanism, which dynamically adjusts the importance between real and pseudo-labels as well as between samples, can enhance model robustness against label noise without depending on explicit assumptions about noise or a clean validation set.\nMethod: The proposed Learning to Bootstrap (L2B) method exploits a meta-learning framework, adjusting the weights of instances and labels using a validation set feedback. The model adjusts weights in training using its own predictions while minimizing loss influenced both by real and pseudo-labels. The method is validated on datasets like CIFAR-10, CIFAR-100, ISIC2019, and Clothing 1M through empirical testing and is shown to integrate well with existing noisy label learning techniques.\nConclusion: The method delivers robust results against label noise across several datasets, demonstrating improved accuracy over baseline approaches. It eliminates the dependency on a clean validation set, leverages pseudo-labels more effectively, and generalizes well to other tasks like segmentation, suggesting its potential as a practical complement to existing noisy label learning methods.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 2, \"Strengths\": \"- The authors perform extensive experimentation on synthetic datasets, one dataset with real label noise, and compare to a large number of methods. \\n- The convergence analysis is a great contribution to the paper. \\n- The paper is well-written. Before getting to the experiments section, I was curious about whether or not the means of simplex projection for \\\\alpha and \\\\beta mattered in the method, since the authors perform an approximation. The authors seem to have pre-empted this potential question of mine, as they include an ablation that evaluates using the softmax of \\\\alpha and \\\\beta.\", \"Weaknesses\": \"While the authors evaluate on one dataset with real label noise, Clothing-1M, there are a variety of other real-world settings where coping with (potentially systematic) label noise is important. It would be great to include additional real-world settings that may have systematic label noise. For example, label noise is a problem that is often inherent to the family of programmatic weak supervision techniques.\", \"Questions\": \"- Given Euqation 9, it's unclear to me if \\\\alpha and \\\\beta actually sum up to 1 in the final algorithm. Can the authors clarify this? In earlier discussion of the algorithm, it seems as though allowing \\\\alpha and \\\\beta to not sum to 1 is crucial for its final performance. \\n- The ablation studies on \\\\alpha and \\\\beta are interesting and a useful contribution to the paper. While I imagine that performing an exact simplex projection using one of the algorithms for doing so would be slow (mainly because it must be done separately for each sample), have the authors tried other methods besides the softmax or the sigmoid? \\n- In the current formulation that does not use the sigmoid or softmax, parameters are allowed to be zero. Is it common for either or both of them to be zero? Could this by why projecting to the interior of the simplex via softmax or sigmoid improves the stability of training? \\n- It is not clear to me whether or not a validation set is required. It is mentioned in several places in the paper that a validation set is not required, however, in 3.1, \\\"In this work, we also assume that there is a small unbiased and clean validation set.\\\" Can the authors clarify this point?\"}"}]}
{"paper_id": "ttRSBZiCiu", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Recent advancements in training algorithms and network architectures have enabled the production of high-fidelity audio by deep generative models, particularly in the field of speech synthesis. Previous approaches include autoregressive models that produce audio waveforms from mel-spectrograms, flow-based models that require invertibility and are constrained by design, and Generative Adversarial Networks (GANs) that efficiently generate high-quality waveforms. Diffusion probabilistic models have been popular due to their ability to transform prior distributions into complex audio distributions without the need for extensive parameterization, although they are often computationally expensive during inference.\nGap: Despite improvements in generative model architectures and inference speed, there is a need for more efficient and flexible models that can produce high-quality audio in fewer steps without the typical constraints of invertibility (as in flow-based models) or the extensive computational requirements of diffusion models.\nHypothesis: The authors propose that a new model, WaveFluid, which learns a velocity field through adversarial training rather than relying on solving a pre-defined partial differential equation, can efficiently synthesize high-fidelity audio from mel-spectrograms. By leveraging a two-stage model and reparameterization techniques, WaveFluid aims to improve memory efficiency and training speed.\nMethod: WaveFluid uses a two-stage adversarial model conditioned on mel-spectrograms for speech synthesis. The first stage is a deterministic function that up-samples mel-spectrograms, and the second stage is a probabilistic refiner that uses a learned velocity field to generate waveforms. Adversarial training is employed to directly learn the velocity field, and reparameterization techniques are used to reduce the memory footprint and improve training efficiency. Modular design features like attention blocks, up-sampling, and down-sampling shortcuts are introduced to enhance model performance.\nConclusion: The WaveFluid model demonstrates competitive performance in generating high-fidelity audio compared to previous diffusion and GAN-based vocoders, notably achieving comparable quality with only one inference step. This model shows promise in efficiently generating high-quality audio with reduced computational overhead, presenting an effective alternative to existing methods in speech synthesis.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 5, \"Strengths\": \"1.The physical perspective of using fluid equations to elucidate the modeling of probability distributions in generative models is highly innovative. It provides a theoretical foundation for better analyzing existing generative models and designing more optimal ones.\\n\\n2.The article offers a comprehensive mathematical analysis of using fluid equations to explain diffusion models and Poisson flow models, providing strong evidence for this analytical approach.\\n\\n3.The proposed model achieves a good balance between audio quality in generation and generation speed. The provided samples sound of good quality.\", \"Weaknesses\": \"1.The experimental part is relatively limited. More advanced GAN-based vocoders, such as BigVGAN, may be compared as baselines. And additional metrics like MCD may be calculated. For results unseen speakers, evaluating speaker similarity with metrics like SMOS would also be valuable.\\n\\n2.There's a lack of analysis concerning the model's size, including an analysis of the impact of different parameter counts on the model's performance and an analysis of the effect of varying parameter sizes of different baselines.\\n\\n3.As described in the article, this method is only applicable to specific tasks with small information gaps between conditions and data (such as mel-conditioned audio generation) and is not suitable for general-purpose generation. This limitation restricts its broader application.\", \"Questions\": \"Why are different datasets used for subjective and objective evaluations of unseen speaker results? Both results on LibriSpeech and Speech Command may be used to calculate MOS and objective metrics.\"}"}]}
{"paper_id": "WYsLU5TEEo", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Neural Networks (NNs) are widely used for binary classification in various fields like medical diagnosis and defect detection due to their precision and efficiency. However, they are difficult to interpret and vulnerable to adversarial attacks. Existing efforts to improve interpretability include generating counterfactual examples and techniques like Grad-CAM, but these often lack precision. Recent research explores GANs for counterfactuals to enhance interpretability. On the robustness front, projects enhance models by adversarial training methods such as PGD. Most current methods handle interpretability or robustness separately, without integrating both.\nGap: Existing strategies primarily address either improving interpretability or enhancing robustness of neural image classifiers, but not both simultaneously. Post-hoc interpretability approaches do not alter the classifier during the counterfactual training process, possibly missing opportunities to increase robustness.\nHypothesis: The authors propose that a unified framework using image-to-image translation GANs can generate counterfactual samples which both highlight salient regions for interpretability and act as adversarial samples to enhance dataset robustness.\nMethod: The paper introduces a framework combining a classifier and discriminator within a single model to produce and identify counterfactuals as 'fake'. The generator performs image-to-image translation to create samples attributed to opposite classes, augmenting robustness and interpretability. They tested on two datasets: a CASC IFW database for fruit defects and a Concrete Crack Segmentation Dataset, measuring performance against a semantic segmentation task and resilience to PGD attacks. Key components include specialized architectures for the GAN components, cycle-consistent loss term, and gradient update strategies to improve GAN stability and performance.\nConclusion: The framework effectively combines interpretability and adversarial robustness, achieving competitive IoU scores for saliency maps compared to traditional methods and resisting adversarial attacks better than conventional classifiers. The discriminator's output could further serve as a measure of prediction uncertainty, demonstrating both methodological and practical advances in image classification tasks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"- **Noteworthy Contribution**: The paper introduces a novel framework that has the potential to address the challenges of achieving robust and interpretable classifiers. This contribution is particularly relevant in the context of existing classifiers that often struggle with the trade-off between robustness and interpretability.\\n\\n- **Varied Evaluation**: The paper's assesses the proposed method using different model architectures, including convolutional networks and transformer models. This varied evaluation demonstrates the adaptability of the approach across different scenarios and model types, highlighting its potential for broader applicability.\", \"Weaknesses\": \"- **Limited to Binary Tasks**: A major limitation of the paper is that it only addresses binary classification tasks. It would be interesting to expand its applicability to multiclass problems to demonstrate broader utility, as mentioned in the discussion section.\\n\\n- **Single Seed Experiments**: The experiments in the paper are limited to training on a single seed, making it difficult to assess the significance of performance differences and the true impact of the proposed cycle consistency loss on convergence. Multiple seed experiments would provide a more robust evaluation.\\n\\n- **Experiment Clarity**: The presentation of experiments can be confusing and should be more detailed. For instance, the \\\"Hybrid D\\\" model is never introduced in the paper. The explanation of the computation of performance when using D is also presented *after* showing results. The description of Table 2 is also unclear, making it challenging for readers to understand the methodology and the comparison.\\n\\n- **Misleading Introduction**: The paper introduces the approach as \\\"combining classifier and discriminator in a single model\\\" (in the abstract), which is incorrect since the generator and discriminator are fundamentally different.\\n\\n- **Lack of Comparative Analysis**: The paper lacks a comparison with other counterfactual approaches, which could provide insights into the quality of the counterfactuals produced and help position the proposed method within the broader context of counterfactual research.\", \"Questions\": \"- It could be interesting to have a rule of thumb in which model to use, G or D ? Both seems to be strong for classification, but do they have their own advantages ?\\n- Can we do more than 1 cycle in the \\\"cycle consistency loss\\\" ? The loss is described with $c \\\\geq 1$, but experiments only show $c=1$ or $c=0$.\\n- I'm not sure about the conclusion of section 4.5 on the robustness of the classifiers. From Figure 3, we can see that the models trained with the proposed approaches both have *lower* F1 scores than the baselines when increasing the perturbation size. I assumed that the labels of the approaches are inverted in the plot. Can the authors clarify this ? Otherwise, the conclusion on robustness would be completely different.\"}"}]}
{"paper_id": "KKZaj2QS3G", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Time series analysis is crucial for many practical applications, such as finance, medicine, and engineering. Labeling time series data is often costly and complex due to intricate patterns and privacy concerns. Unsupervised learning, particularly contrastive self-supervised learning, offers a promising avenue for developing informative representations without labeled data.\nGap: The existing research on time series representation learning has not explicitly addressed the impact of noise, which is a significant factor affecting task accuracy. Moreover, there is a lack of efficient and lightweight encoder architectures that can capture robust and informative representations for various downstream tasks.\nHypothesis: The authors propose that employing a noise-resilient sampling strategy, together with a scalable and robust encoder design, will enable effective learning of time series representations that are consistent and informative even in the presence of noise.\nMethod: The proposed method, CoInception, integrates a noise-resilient sampling strategy that utilizes a spectrum-based low-pass filter to create perturbed series for robust representation learning. This strategy preserves essential time series characteristics while filtering out noise. The method uses an encoder architecture combining Dilated Convolution with Inception blocks to effectively capture wide receptive fields while maintaining model scalability and robustness. The learning is guided by a hierarchical triplet loss to enforce noise resilience and ensure alignment and uniformity in the embeddings for tasks like forecasting, classification, and anomaly detection.\nConclusion: The CoInception framework consistently outperforms existing state-of-the-art methods across multiple benchmarks for forecasting, classification, and anomaly detection in time series data. It achieves this while being more parameter-efficient, highlighting its scalability and robustness in handling noise-afflicted time series tasks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"The paper certainly represents a significant amount of work by the authors, especially in the experimental section. The related work section is also extensive.\", \"Weaknesses\": \"The paper does not do a good job positioning itself in the vast literature of timeseries modeling. It is not clear how the paper extends the state of the art in this area, and whether/how the proposed method is new compared to some fairly basic signal processing concepts, such as low-pass filtering to reduce noise.\", \"Questions\": \"The paper does not do a good job positioning itself in the vast literature of timeseries modeling. Some key statements in the paper show this issue more clearly. For example:\\n\\n\\\"A common shortcoming emerges in existing methods: none explicitly address noise in time series data alongside an appropriate strategy for handling this unwanted signal. Unlike other data types, real-world time series often harbor substantial noise, severely impacting task accuracy\\\"\\n\\nThis statement is hard to accept. Of course all or most prior work in this area considers that the time series will be noisy. This is one of the main reasons we use neural networks, as opposed to other models that are much less capable to deal with noisy data. \\n\\nThen, there are some sentences that are hard to understand. For example: \\n\\n\\\"we devise a sampling strategy based on the insight that the noisy signal combined with the original series shouldn’t disrupt time series frameworks. In essence, frameworks should produce consistent representations given noise-free or raw series (noise-resiliency characteristics).\\\"\\n\\nWhat \\\"Frameworks\\\" are the authors talking about? \\n\\nAdditionally, there are some aspects of the model, or claims about the model, that are very basic but they are presented as an important technical contribution. For example:\\n\\n\\\"To achieve this, we shift our focus from the temporal domain to the spectral domain. Here, we employ a spectrum-based low-pass filter to create correlated yet distinct views of each input time series. These augmented views serve as positive samples of the raw series, effectively capturing the desired noise invariance. The advantages of this low-pass filter-based augmentation are twofold: (1) the filter preserves key characteristics such as trend and seasonality, ensuring deterministic and interpretable representations; (2) it eliminates noise-prone high-frequency components, improving noise resilience and enhancing downstream task performance by aligning the raw signal representation with the augmented view\\\"\\n\\nOf course there is nothing particularly new in the previous paragraph. Working in the spectral domain and performing LPF to remove high-frequency noise are elementary operations that are typically taught at the level of undergraduate signal processing courses. \\n \\nConsidering the previous weaknesses, it is hard to provide specific technical comments to the authors, given that (at least to this reviewer) it is not clear if the paper has something really new to contribute in the area of timeseries modeling.\"}"}]}
{"paper_id": "LWDRiFzbHQ", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper addresses the need to evaluate the generalization capacity of classification models on out-of-distribution (OOD) test data without relying on ground truth labels, which is important due to the cost of obtaining test ground truths in real-world scenarios. Existing works propose unsupervised risk measurements to capture model properties such as confidence and invariance, which correlate with OOD accuracy. However, these indicators are often computed individually for each test sample, which can lead to unreliable results due to spurious model responses.\nGap: Existing OOD generalization assessment methods use isolated test samples to compute risk proxies like confidence and invariance, which can result in spurious model responses (i.e., excessively high or low confidence), thus compromising the reliability of the proxy as an indicator of generalization ability.\nHypothesis: The hypothesis proposed is that by integrating model responses of neighboring test samples into the correctness indicator of each test sample, the predicted generalization ability can be enhanced. This is because samples with high correctness scores among nearby samples are likely also to be predicted correctly, thus providing a more holistic measure of model accuracy.\nMethod: The proposed method introduces the Vicinal Risk Proxy (VRP), which incorporates neighboring samples' responses into the risk proxy calculation for each test sample, using a similarity-weighted sum approach. This vicinal assessment smooths out spurious scores and provides a more robust measure of model accuracy without relying on labels. The VRP can be applied to various generalization indicators like average confidence or effective invariance, showing consistent improvements over these baselines across multiple datasets and test scenarios.\nConclusion: The vicinal risk proxy provides a stronger correlation with model accuracy, especially on severe out-of-distribution test sets, by leveraging neighboring sample responses. This result demonstrates the effectiveness of VRP in addressing spurious responses, as corroborated by experiments showing improved correlations over existing methodologies. The approach broadens the applicability of risk estimation techniques while increasing reliability in evaluating model generalization.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"- The author provides very clear examples to introduce their method, which allows me to quickly understand the work done in this paper. In particular, Figure 1 helps me to understand the work done in this paper and how it is implemented as soon as I see it.\\n\\n- The author's formulas are very clear, especially the repeated declarations of symbols in each formula, which prevents me from having to flip back to the beginning of the paper to find the definition of a symbol when I encounter it later in the paper.\", \"Weaknesses\": \"The innovation of the paper is insufficient. In fact, as early as the beginning of 2020, I had tried a similar idea, which is almost identical to the approach described in this paper, and the paper only describes my first version of the attempt. Here is my experience and why I did not continue this research:\\n\\n\\n- Calculating dµ involves a significant amount of additional time, so I abandoned this approach since the performance of ensemble methods can surpass many calibration methods, and their biggest drawback is the substantial inference cost. If a method cannot outperform an ensemble in terms of time efficiency, its performance needs to surpass that of the ensemble, but I found that the performance of this method cannot exceed the ensemble method.\\n\\n- To address this issue, I used a neural network to fit the similarity of samples, i.e., training an additional branch on the original network to fit the vicinal score, which transfers the additional time cost to the training process and significantly reduces the time spent during the inference process. However, this method is still unsatisfactory for two reasons: \\n \\n  - 1. Confidence suffers from severe overfitting, as the confidence of the vast majority of samples in the training set is 1. \\n\\n  - 2. The proxy estimation of confidence depends on the surrounding samples, which means that it requires good confidence estimation of the surrounding samples, so this estimation does not fundamentally solve the problem.\\n\\n- Finally, I changed my strategy and used true class probability (TCP[1]) as the confidence of the vicinal sample, which can effectively alleviate the problem of poor confidence estimation of the vicinal sample, to some extent solving the issues.\\n\\nHowever, I ultimately did not continue to study this work because the method is not elegant enough.\\n\\n_[1] Corbière C, Thome N, Bar-Hen A, et al. Addressing failure prediction by learning model confidence[J]. Advances in Neural Information Processing Systems, 2019, 32._\", \"Questions\": \"I have no other questions about this paper. I hope the author can continue to conduct more in-depth research, and the previous research I just described can provide useful information for the author.\"}"}]}
{"paper_id": "Yp01vcQSNl", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Directed graphs are crucial for representing various real-world relationships such as social networks and paper citations. Traditionally, graph neural networks (GNNs) have been applied to these datasets to learn node and edge encodings. However, conventional graph transformers, which can potentially outperform GNNs due to their flexible attention mechanism, often fail to incorporate edge directionality effectively. Existing methods frequently rely on the static adjacency matrix or local computations, which do not fully exploit the potential of dynamic attentional relations that can be developed with transformers.\nGap: Current graph transformer architectures largely ignore or inadequately handle edge directionality, undermining their applicability to directed graphs. The core limitation is their reliance on structural assumptions like static adjacency matrices, which restrict their ability to leverage edge directions flexibly in learning tasks.\nHypothesis: The main hypothesis is that a graph transformer architecture that explicitly accounts for edge directionality using dual encodings and dynamic attention mechanisms can significantly enhance performance on learning tasks for directed graphs.\nMethod: The proposed method, Directed Graph Transformer (DiGT), introduces dual encodings representing each node's source and target role in directed edges. These are learned dynamically using a multi-head directional attention mechanism without relying on explicit graph structure. The DiGT leverages a novel attention matrix interpretable as a latent adjacency matrix, incorporating edge channels as a bias and focusing on k-hop neighborhood localization. DiGT also includes techniques to mask attention matrices based on k-hops and combines multiple heads to achieve final predictions. Additionally, the paper explores alternative ways to incorporate directionality into existing graph transformer architectures.\nConclusion: DiGT outperforms state-of-the-art models in graph learning when directionality is a key component, demonstrating that capturing the directional aspect of edges is crucial for certain directed graph datasets. The experiments on new directed graph datasets designed specifically where edge directionality is important further validate the effectiveness and promise of DiGT.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"Strong Points:\\n- Dual node encodings in DiGT elegantly capture directionality throughout the model.\\n- Directional attention neatly exploits query/key asymmetry for modeling direction.\\n- DiGT significantly outperforms baselines on directional graphs.\\n- Quantifies dataset directionality via entropy measure.\\n- New directional graph datasets enable better evaluation, although it may be biased in the context of this work.\", \"Weaknesses\": \"Weaknesses and\", \"Questions\": \"included together with weaknesses\"}"}]}
{"paper_id": "LndMyiBl3n", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Below are example prompts & responses:\n\nEXAMPLE 1 (USER):\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the concept of domain shift in machine learning, particularly focusing on sub-population shifts where the data distribution changes within specific sub-groups or populations during training vs. testing. Domain shifts, including sub-population shifts, are identified as significant sources of algorithmic bias. The vision-language model CLIP is highlighted for its inherent robustness to distributional changes, although this robustness can be compromised through fine-tuning.\nGap: Despite the robustness of models like CLIP to sub-population shifts, fine-tuning such models often compromises this robustness. Furthermore, existing universal distributionally robust optimization (DRO) methods combined with CLIP may result in performance instability. There is a need to effectively handle sub-population shifts without compromising robustness or requiring domain-aware validation datasets.\nHypothesis: The authors propose that leveraging natural language inputs to debias image feature representations in multi-modality models like CLIP can improve performance under sub-population shifts, particularly improving worst-case performance without requiring instance-wise label information.\nMethod: The proposed method, L-DRO, utilizes natural language to mitigate sub-population shifts in CLIP by debiasing the image feature representations. It employs a feature adapter that optimizes between the entropy of sub-population predictions and consistency with original features. This approach uses a two-phase training where DRO-like risk is addressed by oversampling or re-weighting significant data points. Experiments are conducted on datasets like CelebA and Waterbirds to compare its performance against baseline methods, using various configurations of text prompts for classification and debiasing.\nConclusion: The proposed L-DRO method significantly improves worst-case performance and stability across sub-population shifts compared to baseline methods. L-DRO can achieve such improvements without requiring domain-aware validation datasets during training and occasionally enhances average performance. However, the paper notes limitations in the necessary careful selection of text prompts and potentially challenging sub-populations to describe.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON.\n\nEXAMPLE 1 (ASSISTANT):\n{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The paper proposes an effective framework for debiasing subgroup information using aligned representations from Vision Transformers (ViT), which improves the subgroup robustness. This work could inspire future research in this domain.\\n2. The evaluation is comprehensive and systematic, showcasing effectiveness across multiple datasets and metrics, and providing a comparison with several state-of-the-art baselines. The Table 4 analysis of (dis)alignment within subgroups between the source and target is both innovative and inspiring.\\n3. The paper is well-organized and clearly presented.\", \"Weaknesses\": \"1. I’m concerned about the innovation w.r.t. proposing a new strategy for subgroup robustness. The concept of gaining robustness from language modality to enhance another is not novel [1], and the idea of learning debiased representations—where domain predictability is removed and task predictability is emphasized—is fairly common. \\n2.  While the method is motivated by the general notion that aligned representations from multimodal models can share robustness, the algorithm is restrictive — It primarily facilitates the use of text prompts to learn the debiased representation adapter for improved classification. Extending the algorithm to incorporate other combinations of modalities may be challenging, particularly for modalities where designing debiasing prompts (e.g., in vision) might be difficult.\", \"Questions\": \"1. Can L-DRO be adapted for combinations of modalities other than vision and language, or does the methodology intrinsically require assistance from the language modality?\\n2. Table 4 offers interesting insights. In the rows where the source and target are misaligned, the average accuracy is comparable to that of a zero-shot scenario, however, there is a significant divergence in worst-case accuracy. Does this suggest other subgroups are either adversely affected or disproportionately benefited (so that the average scores can remain similar)? Does this imply that the algorithm still works with other influential attributes even if they are not directly related to the target domain?\"}\n\n-----\n\nNOW PLEASE DO THE SAME FOR THIS NEW PAPER BELOW.\n\nPlease read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Graph Neural Networks (GNNs) have become popular across various applications. Despite their success, GNNs are vulnerable to adversarial attacks that involve structural modifications, i.e., adding or deleting edges. This vulnerability has led to the development of different types of attacks, categorized based on the attacker's knowledge (white-box, grey-box, black-box). The most strict setting is the restricted black-box attack (RBA), where attackers know only the node features and graph structure, not labels or model parameters.\nGap: Most previous studies on graph adversarial attacks assume conditions like low-rank node embeddings or graph homophily, which limits their applicability, especially for heterophilic graphs. Heterophilic settings, where nodes of different classes are more likely connected, pose a unique challenge for existing models since they often rely on the homophily assumption.\nHypothesis: The authors propose that evaluating the quality of graphs using a modified silhouette score (MSS) can effectively guide attacks under the RBA setting. MSS aims to serve as a more generalizable approach across graphs with varying levels of homophily and heterophily.\nMethod: The authors introduce SheAttack, guided by a Modified Silhouette Score (MSS) that evaluates the quality of node embeddings within graphs. MSS is calculated based on intra-class and inter-class distances, generalized by replacing the minimal inter-class distance with an average, thus widening the objective's scope. SheAttack employs Greedy Randomized Block Coordinate Descent to optimize edge perturbations guided by MSS, providing scalable computations and effectiveness across different graph settings without prior label or model knowledge.\nConclusion: SheAttack demonstrates strong performance in degrading GNNs across homophilic and heterophilic graphs, comparable to white-box attacks without relying on additional split information. It is efficient and scalable, achieving success without prior knowledge of node labels or victim models, and adapts to both poisoned and evasion settings effectively.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1–4):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1–10):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1–5):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. The studied restricted black-box attack is a practical scenario and an important problem.\\n2. The motivation of this paper is clear and the idea of introducing the Silhouette Score to measure the quality of a graph is interesting.\\n3. Extensive experiments are conducted.\", \"Weaknesses\": \"1. The proposed SheAttack depends on various hyper-parameters, which may significantly affect the performance of the proposed method and lack detailed theoretical support or empirical analysis. For example, the number of clusters $k$, the propagation layer/time on the adjacency matrix, and the $\\\\lambda$ that balance the Shift Loss and Silhouette Score-based Loss. \\n2. The proposed method does not seem to be consistently effective in all scenarios in Tables 1&3.\\n3. The writing of this paper needs to be further improved. For example, the citation format of references in Introduction and Preliminaries seems strange. 'Aggreation function' should be 'Aggregation function’. The definition of poison attacks and evasion attacks on page 3 is confusing.\", \"Questions\": \"1. In Figure 3, why do GRBCD and RandAttack show different Modified Silhouette Score at epoch 0?\\n2. How would the proposed method perform when the victim model is not a two-layer GNN? In other words, when the propagation layers/times in the proposed method and victim model are not the same, would the proposed method still be effective?\"}"}]}
