[
  {
    "paper_id": "J2FyEVg8HR",
    "summary": "Background: Large language models (LLMs) have revolutionized various natural language processing tasks but are limited by their high computational cost, especially during inference. This computational demand imposes significant challenges for real-time applications and necessitates efficient inference strategies. Prior solutions have attempted to address this by routing entire queries to either small or large models based on complexity, which may not always be optimal.\nGap: Existing approaches to reduce inference costs often lack flexibility as they typically route entire queries rather than token-level content, which may not efficiently utilize model capacities and could lead to unnecessary computational overhead or loss of precision.\nHypothesis: The authors propose the CITER framework, which enables efficient inference by dynamically routing tokens rather than entire queries to either small language models (SLMs) or large language models (LLMs) based on their individual criticality, thereby reducing computational costs without compromising the quality of generation.\nMethod: CITER introduces a token-level routing mechanism, employing a router trained via reinforcement learning. The router predicts routing scores for each token to decide whether it should be processed by an SLM or an LLM. The training involves estimating a reward function efficiently through a shortcut process that reduces training costs. The method is validated through extensive experiments on four benchmark datasets and involves an innovative iterative training process that uses collected routing preferences to refine routing decisions.\nConclusion: CITER effectively reduces the inference cost of LLMs while maintaining high output quality, showing improved performance compared to existing query-level and token-level routing methods. These improvements are substantiated by up to 30% reduction in LLM calls or up to 25% increase in accuracy in comparison to baseline methods. The framework provides a significant advance in making LLMs more practical for real-time and resource-constrained environments.\n",
    "full text": "\nCITER: COLLABORATIVE INFERENCE FOR EFFICIENT\nLARGE LANGUAGE MODEL DECODING WITH TOKEN-\nLEVEL ROUTING\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nLarge language models (LLMs) have achieved remarkable success in natural lan-\nguage processing tasks but suffer from high computational costs during infer-\nence, limiting their deployment in latency-constrained applications. To address\nthis issue, we propose a novel Collaborative Inference with Token-lEvel Routing\n(CITER) framework that introduces a token-level routing mechanism, enabling ef-\nficient collaboration between small and large language models (SLMs & LLMs).\nSpecifically, CITER enables routing non-critical tokens to an SLM to reduce com-\nputational overhead, while critical tokens are processed by an LLM to maintain\ngeneration quality. We formulate the training of the router as a reinforcement\nlearning task, where the router receives rewards based on both the quality of pre-\ndictions and the inference cost of generation. This allows the router to learn to pre-\ndict token-level routing scores and make routing decisions based on both the cur-\nrent token and the future impact of its decisions. To further accelerate the reward\nevaluation process, we introduce a shortcut for reward function estimation, signif-\nicantly reducing the cost of the reward estimation and improving the practicality\nof our approach. Extensive experiments across four benchmark datasets demon-\nstrate that CITER reduces inference cost while preserving high-quality generation,\noffering a promising solution for real-time and resource-constrained applications.\n\n1\n\nINTRODUCTION\n\nLarge language models (LLMs) have revolutionized a wide range of natural language processing\ntasks, from machine translation to context summarization and question answering (Coleman et al.,\n2024; Kamalloo et al., 2024; Eniser et al., 2024; He et al., 2024). However, their impressive perfor-\nmance comes with a substantial computational cost, particularly during inference. As these models\ngrow in size, the cost of inference becomes a significant barrier to their practical deployment, espe-\ncially in real-time applications. Thus, there is a growing need for accelerating the inference process\nwithout compromising the quality of the generated outputs.\n\nAmong the strategies (Dao et al., 2022; Sanh et al., 2020; Kou et al., 2024; Anagnostidis et al.,\n2024) to reduce inference costs, particularly the number of arithmetic operations, routing tasks to\nmodels of different sizes is a promising approach to accelerating LLM inference while maintaining\noutput quality. In this approach, small language models (SLMs) handle simpler tasks with lower\ncomputational overhead, while more complex cases are routed to LLMs to ensure response accuracy.\nHowever, while promising, existing works largely focus on routing entire user queries to different\nmodels for generation (Ong et al., 2024; Mohammadshahi et al., 2024). This binary “either-or”\napproach limits routing flexibility and may reduce efficiency, as many responses are often routed to\nLLM to ensure the highest possible accuracy, even though SLM can often provide reasonably good\nanswers.\n\nTo address this challenge, we present a novel framework, namely Collaborative Inference with\nToken-lEvel Routing(CITER). CITER introduces a token-level router that predicts whether a to-\nken is important by estimating the token-level routing score, and routes it to the appropriate model\nto balance the efficiency and accuracy of generation. Specifically, many tokens in the response that\nare not important to the final prediction, can be generated by SLM to reduce inference cost, while\n\n1\n\n000\n001\n\n002\n003\n004\n005\n006\n007\n\n008\n009\n010\n011\n012\n013\n\n014\n015\n016\n017\n018\n\n019\n020\n021\n022\n023\n024\n\n025\n026\n027\n028\n029\n030\n\n031\n032\n033\n034\n035\n036\n\n037\n038\n039\n040\n041\n\n042\n043\n044\n045\n046\n047\n\n048\n049\n050\n051\n052\n053\n\n\fUnder review as a conference paper at ICLR 2025\n\n054\n055\n\n056\n057\n058\n059\n060\n061\n\n062\n063\n064\n065\n066\n067\n\n068\n069\n070\n071\n072\n\n073\n074\n075\n076\n077\n078\n\n079\n080\n081\n082\n083\n084\n\n085\n086\n087\n088\n089\n090\n\n091\n092\n093\n094\n095\n\n096\n097\n098\n099\n100\n101\n\n102\n103\n104\n105\n106\n107\n\nFigure 1: An overview of the CITER framework. A router is leveraged to perform collaborative in-\nference between the SLM and LLM. The router is trained using routing preference collected through\nthree cases. Case 1: The SLM generates the correct token, the routing preference is assigned to the\nSLM. Case 2: The SLM generates an incorrect token, while the LLM generates the correct token,\nthe routing preference is assigned to the LLM. Case 3: None of the SLM or the LLM generates\nthe correct token, then the collaborative inference is conducted to obtain the completed response for\nassigning the routing preference.\n\nthe LLM can be reserved to generate important tokens only. We formulate a reinforcement learning\n(RL) problem to train the router, with the objective of minimizing inference cost while preserving\noutput quality. By employing this formulation, the router learns to predict token-level routing scores\nand make routing decisions not only based on the current token but also considering the impact of\nthese decisions on future tokens. However, training the router using RL can be computationally\nexpensive, as it requires estimating the reward function, which is defined as the accuracy of the\nresponses generated via collaborative inference starting from each initial state. To make this formu-\nlation more practical, we present a shortcut to the reward function, where the prediction from the\nSLM and LLM is leveraged to estimate the final reward without completing the whole generation\nprocess, which significantly accelerates its training process. Through this RL framework, the router\nlearns an optimal token-level decision-making strategy, enabling an SLM and an LLM to collaborate\nfor effective and efficient autoregressive generation.\n\nOur primary contribution is CITER, which accelerates LLM inference by employing a token-level\nrouter to select the appropriate model for generating each token. Experiments on four benchmark\ndatasets demonstrate the effectiveness of our approach, achieving a reduction in LLM inference\ncost while maintaining high output accuracy, with comparable accuracy while up to 30% fewer\ncalls to LLM or delivering up to a 25% improvement in accuracy with the same ratio of calls to\nLLM compared to co-LLM (Shen et al., 2024). Additionally, our ablation study experiments also\ndemonstrate that token-level routing offers more flexibility for achieving more promising results\ncompared to query-level routing and that considering the long-term impact of routing decisions\nboosts the acceleration performance significantly.\n\n2 COLLABORATIVE INFERENCE WITH TOKEN-LEVEL ROUTING(CITER)\n\nIn this section, we describe our Collaborative Inference with Token-lEvel Routing (CITER) frame-\nwork that leverages token-level routing to accelerate LLM inference. As illustrated in Figure 1, in\nCITER, we introduce a router to facilitate collaborative inference between a powerful but computa-\ntionally expensive LLM and a fast but potentially inaccurate SLM. Specifically, the router is lever-\naged to predict the token-level routing score for each token, and a pre-defined threshold τ is used\nto determine which model should generate this token. The key challenge of our framework is the\ntraining process of the router. To feed the router with the knowledge on making the global optimal\n\n2\n\nIteration 2Iteration 1VincentQ: \"Who is the authorof The Starry Night? \"A: \"It's ...ClaudeMonetVincentvan GoghClaudeVincentCollaborative InferenceSLMLLMRouterLLMSLMSLMSLMSLMLLMLLMRouterTrainingRoutingPreferenceSLMLLMVincent ItisClaund van Monet.GoghRouter\fUnder review as a conference paper at ICLR 2025\n\nrouting decisions not only based on the accuracy of the current token but also the long-term impact\nof its decision, we formulate the training process of the router as a reinforcement learning problem,\naiming to minimize the inference cost while maintaining the generation quality. To be more specific,\nwe first formulate the RL problem and derive the reward function as token-wise routing preference,\nwhich should be computed to collect during the router training process. Subsequently, we intro-\nduce a shortcut for the reward function estimation, leveraging both the SLM and LLM’s prediction\nto estimate the reward, to significantly accelerate the collection process of the token-wise routing\npreference. Finally, we propose an iterative training process for the router to mitigate the potential\ninconsistencies of the routing decisions in the preference collection phase and deployment. In the\nrest of this section, we will outline the router training and collaborative inference processes in detail.\n\n2.1 ROUTER TRAINING\n\n2.1.1 PROBLEM FORMULATION\n\nTo equip the router with the ability to predict token-level routing scores and decide which model\nto use based not only on the accuracy of the current token but also on the impact of future\ntoken generation, we formulate the training process of the router as a reinforcement learning\nagent. Let the current state Th be the historical context input to the LLM before h-th token, i.e.,\nTh = {x, y1, · · · , yh−1}, where x is the input prompts. At each step h ∈ [H], the RL agent\nselect either the LLM πL and the SLM πS with action a = {L, S}. Then the state Th+1 is com-\nposed by first sampling the next token yh ∼ πa(·|Th) and then concatenating it to previous state\nas Th+1 = Th + {yh}. In addition, we assume the reward is defined by the whole trajectory by\nR(TH , AH ) where AH is the sequence of a routing policy, i.e. AH = {a1, · · · aH }, ah = {L, S}.\nReward R comprehensively encodes the performance of the routing strategy, including the quality\nof generated response TH and the computational cost required by performing AH . We further define\nthe (meta) policy for the routing agent as Π = {Πh(·|Th)}h and the state-action value function as\nh (Th, a) = E[R(TH , Ah:H )|Th, a, Π], where the expectation is taken over TH and Ah:H given\nQΠ\nhistory context Th, action a ∈ {L, S} and (meta) policy Π. The objective of the routing agent is to\noptimize the policy by\n\nΠh = arg max\n\nΠh\n\nEa∼Πh(·|Th)QΠ\n\nh (Th, a) − KL(Πh ∥ µ),\n\n(1)\n\nwhere µ is the pre-defined policy with µ(L) = µ(S) = 0.5, meaning that the SLM and\nLLM are randomly selected. The closed-form solution for equation 1 is therefore Πh(a|Th) ∝\nµ(a) exp(QΠ\n\nh (Th, a)).\n\n2.1.2 EFFICIENT COLLECTION OF TOKEN-WISE ROUTING PREFERENCES\n\nGenerally, it is hard to evaluate the quantity of R(TH , AH ) because it contains both the quality of TH\nand the efficiency of AH . We inject the pairwise preference 1h[S ≻ L] following the Bradley–Terry\nmodel (Bradley & Terry, 1952) as:\n\n(S ≻ L|Th, Π) =\n\nPr\nh\n\n1\n1 + exp(QΠ(Th, L) − QΠ(Th, S))\n\nFollowing (Rafailov et al., 2024), we have that\n\nQΠ\n\nh (Th, L) − QΠ\n\nh (Th, S) = log\n\nΠh(L|Th)\nµ(L)\n\n− log\n\nΠh(S|Th)\nµ(S)\n\n= log\n\nΠh(L|Th)\nΠh(S|Th)\n\n,\n\nwhere the equation 3 is due to log(µ(L)/µ(S)) = 0. Plugging equation 3 into equation 2 yields\n\nPr(S ≻ L|Th, Π)h =\n\n1\n1 + Πh(L|Th)/Πh(S|Th)\n\n= Πh(S|Th),\n\n(2)\n\n(3)\n\n(4)\n\nwhere the last equation is due to the fact that Πh(S|Th) + Πh(L|Th) = 1. Therefore, given a context\nTh, once we have labeled the preference 1h[S ≻ L|Th, Π], Πh(S|Th), the routing agent Π can be\nlearned by minimizing the cross-entropy loss\n\nL =\n\n(cid:88)\n\nTh\n\n1h[S ≻ L|Th, Π] log Πh(S|Th) + 1h[L ≻ S|Th, Π] log Πh(L|Th),\n\n(5)\n\n3\n\n108\n109\n\n110\n111\n112\n113\n114\n115\n\n116\n117\n118\n119\n120\n121\n\n122\n123\n124\n125\n126\n\n127\n128\n129\n130\n131\n132\n\n133\n134\n135\n136\n137\n138\n\n139\n140\n141\n142\n143\n144\n\n145\n146\n147\n148\n149\n\n150\n151\n152\n153\n154\n155\n\n156\n157\n158\n159\n160\n161\n\n\fUnder review as a conference paper at ICLR 2025\n\nwhere the 1h[L ≻ S|Th, Π] is defined similarly to 1h[S ≻ L|Th, Π], but it takes action L (selecting\nthe LLM) instead of S (selecting the SLM) at step h, conditioned on the state Th.\nTo optimize equation 5, the routing preference 1h[S ≻ L|Th, Π] is determined by whether the\nfully generated response, starting from state Th+1, is correct. The state Th+1 is reached by taking\naction S (selecting the SLM) from state Th. Specifically, if the state Th+1 is a completed state\nTH (e.g., ending with an <EOS> token), the routing preference is assigned based on whether the\ngenerated response TH is correct. If the state Th+1 is an intermediate state, the routing agent Π will\nbe leveraged to process the collaborative inference between the SLM and the LLM, obtaining the\ncompleted state TH . Subsequently, the routing preference 1h[S ≻ L|Th, Π] will be determined as\nwhether the generated response TH is correct. The whole process can be formulated as follows:\n\n1h[S ≻ L|Th, Π] = 1(Correct(TH )),\n1h[S ≻ L|Th, Π] = 1(Πh+1(S|Th+1) > τ ) 1h+1[S ≻ L|Th+1, Π]+\n1(Πh+1(S|Th+1) ≤ τ ) 1h+1[L ≻ S|Th+1, Π],\n\nif Th+1 is completed,\n\n(6)\n\notherwise,\n\nwhere the Correct(·) is used to determine whether the generated response is correct. The threshold\nτ is a hyperparameter that determines the routing score required to select the SLM. If the routing\nscore exceeds τ , the token is considered non-critical and the SLM’s output will be committed to the\nfinal response. Otherwise, the token is marked as critical and routed to the LLM for re-generation.\n\nHowever, the second case in equation 6 requires generating the full response TH and calculating\nthe reward from each starting state Th, which is computationally expensive. To mitigate this, we\nintroduce a shortcut for estimating the reward function, significantly reducing the cost of reward\ncomputation. We hypothesize that if the SLM cannot generate a token correctly when provided with\nthe previous ground truth contextual history, then this token is particularly challenging to generate.\nBased on this hypothesis, we first provide both the SLM and LLM with the ground truth historical\ncontext and their output tokens are then compared with the ground truth. Then, we assign the routing\npreference 1h[S ≻ L|Th, Π] directly without generating the full response TH if at least one of them\ngenerates the correct token. Specifically, we define the ground truth context before h-th token is\nT ∗\nh = {x, y∗\nh is defined as the ground truth h-th token. Subsequently, we feed\nT ∗\nh to the SLM to generate the next token. If the SLM can generate the correct token y∗\nh based\non the ground truth context, the SLM will be selected, and we will assign the routing preference\n1h[S ≻ L|Th, Π] = 1. Otherwise, we then feed T ∗\nh to the LLM, and if LLM can generate the correct\nh, we will choose the LLM and assign 1h[S ≻ L|Th, Π] = 0. Only when both models fail to\ntoken y∗\ngenerate the correct token y∗\nh based on ground truth context, the full response generation is required\nto compute the reward. This shortcut allows us to obtain routing preferences for most tokens without\ngenerating the full response. Empirically, we find that about 80% ∼ 90% of tokens can be correctly\npredicted by either the SLM or LLM, making the shortcut significantly reduce the computational\ncost of the reward function estimation. After collecting the preference, it will be used to train the\nrouter using equation 5.\n\nh−1}, and y∗\n\n1, · · · , y∗\n\n2.1.3\n\nITERATIVE TRAINING PROCESS\n\nIdeally, we would expect the router Π used during preference data collection to make the same\nrouting decision as the one used during deployment. However, as the router is updated throughout\ntraining using the collected data in Section 2.1.2, its behavior is likely to change, leading to potential\ninconsistencies. To address this issue, we propose a multi-iteration router training process. In each\niteration k, the router Πk−1 from the previous iteration k − 1 is used to collect routing preferences.\nThese newly collected routing preferences are then utilized to train a new router Πk. With each\niteration, the router’s behavior becomes more consistent, eventually reaching convergence. This\niterative process continues until the collected routing preferences match those from the previous\niteration or until a predefined number of iterations K is reached. The only exception occurs in the\nfirst iteration, where no trained router exists.\nIn this case, a simple routing policy is employed,\nrouting all tokens to the SLM to collect the initial preference data. By following this approach, we\ncan train the router in a practical and efficient manner. The full process is outlined in Algorithm 1.\n\n4\n\n162\n163\n\n164\n165\n166\n167\n168\n169\n\n170\n171\n172\n173\n174\n175\n\n176\n177\n178\n179\n180\n\n181\n182\n183\n184\n185\n186\n\n187\n188\n189\n190\n191\n192\n\n193\n194\n195\n196\n197\n198\n\n199\n200\n201\n202\n203\n\n204\n205\n206\n207\n208\n209\n\n210\n211\n212\n213\n214\n215\n\n\fUnder review as a conference paper at ICLR 2025\n\nAlgorithm 1: The Iterative Training Process of our Router\nInput: The training data D = {(Th, y∗\n\nthe pre-defined threshold τ and the number of iterations K\n\nh)}i, the initial router Π0, the SLM MS and LLM ML,\n\nOutput: The trained router Π\n\n4\n\n5\n\n6\n\n1 Initialization: iter k ← 0, previous routing preference S0 ← ∅\n2 for k = 1 to K do\nSk ← ∅\n3\nfor i = 1 to |D| do\nyS\nh ← MS(Th)\nh == y∗\nif yS\nSk ← Sk ∪ {1}\nContinue\nyL\nh ← ML(Th)\nh == y∗\nif yL\nSk ← Sk ∪ {0}\nContinue\n\nh then\n\nh then\n\n9\n\n7\n\n8\n\n10\n\n11\n\n12\n\nGenerate the full response TH starting from Th ∪ yS\nSk ← Sk ∪ {1(Correct(TH ))}\n\nh with router Πk−i and threshold τ\n\nif Sk == Sk−1 then\n\nbreak\n\nTrain the router Πk with the routing preference Sk and dataset D as the loss\nfunction equation 5\nΠ ← Πk\n\n13\n\n14\n\n15\n\n16\n\n17\n\n18\n19 Return Π\n\n2.2 COLLABORATIVE INFERENCE\n\nAfter router training, during inference, we aim to leverage the SLM to collaboratively generate\ntokens to reduce the number of tokens generated by the LLM, thereby improving the efficiency of\nthe inference process. Specifically, we start by feeding both the input prompt and the previously\ngenerated tokens into the SLM, obtaining the output token and corresponding hidden states. Then,\nthe router Π trained in Section 2.1 is leveraged to predict the token-level routing score based on\nthe hidden states from the SLM. During this process, the router considers both the current token\nand the historical context to make routing decisions based not only on the accuracy of the current\ntoken but also on the long-term influence of its decision on future token generation. Subsequently, a\npre-defined threshold τ is used to determine whether the SLM or the LLM should handle the current\ntoken. If the routing score exceeds the threshold τ , indicating that the SLM is confident with its\noutput. The output token from the SLM will be committed to the final response and the generation\nprocess will go on. Otherwise, the token will be routed to the LLM for re-generation and the SLM’s\noutput will be discarded. During the preference data collection process for router training, most\ntokens are assigned to the SLM, with only a few routed to the LLM through our shortcut. As a\nresult, the SLM efficiently generates the majority of tokens. This process continues until an <EOS>\ntoken is produced by either the SLM or LLM. In this way, the router dynamically routes each token\nbetween the SLM and LLM, offloading non-critical tokens to the SLM to reduce computational\noverhead while utilizing the LLM’s capabilities to maintain response quality.\n\n3 EXPERIMENTS\n\nIn this section, we evaluate the performance of CITER aiming to answer the following questions: (1)\nCompared with the previous works on speeding up the inference of LLM, how does our framework\nperform in terms of the computational cost and the quality of the generated response? (2) Does\nthe components we proposed in our framework boost the performance of the router? (3) Does the\niterative training process of the router improve the performance of our framework? (4) How does\nthe performance of our framework change with the size of the LLM? (5) Can the router distinguish\nthe critical and non-critical tokens correctly?\n\n5\n\n216\n217\n\n218\n219\n220\n221\n222\n223\n\n224\n225\n226\n227\n228\n229\n\n230\n231\n232\n233\n234\n\n235\n236\n237\n238\n239\n240\n\n241\n242\n243\n244\n245\n246\n\n247\n248\n249\n250\n251\n252\n\n253\n254\n255\n256\n257\n\n258\n259\n260\n261\n262\n263\n\n264\n265\n266\n267\n268\n269\n\n\fUnder review as a conference paper at ICLR 2025\n\nTable 1: The statistics of our evaluation datasets.\n\nDataset\n\nDomain\n\nTask\n\n# choices Train size Test size\n\nCommonsense QA\nARC-Challenge\nGSM8k\nMMLU-Professional Psychology\n\nGeneral\nReasoning\nMath\n\nCoT + Multi-choice\nCoT + Multi-choice\nQuestion answering\nPsychology CoT + Multi-choice\n\n5\n4\nN/A\n4\n\n9,741\n1,119\n7,473\n612\n\n1,221\n299\n1,319\n69\n\n(a) Commonsense QA\n\n(b) ARC-Challenge\n\n(c) GSM8k\n\n(d) MMLU-PP\n\nFigure 2: The accuracy vs % calls to LLM curve of CITER and the baselines. Points closer to the\ntop-left corner indicate better acceleration performance.\n\n3.1 EXPERIMENTAL SETUP\n\nDataset Description. We evaluate CITER and our baselines on four widely-used academic bench-\nmark datasets: the commonsense QA dataset (Talmor et al., 2019) contains 12,102 questions requir-\ning different types of commonsense knowledge to answer; the ARC-Challenge dataset (Clark et al.,\n2018), including 1,418 genuine grade-school level, multiple-choice science questions; the GSM8k\ndataset (Cobbe et al., 2021) with 8.5K high quality linguistically diverse grade school math word\nproblems; and the MMLU-Professional Psychology dataset (Hendrycks et al., 2021), consisting of\n874 multiple-choice questions on psychology, respectively. The statistics of the datasets are in Ta-\nble 1.\n\nEvaluation. We evaluate the performance of CITER and the baseline methods using the test sets\nand corresponding evaluation metrics for each dataset. Specifically, a threshold τ is applied in each\nmethod to balance the trade-off between response accuracy and the % calls to LLM, which is defined\nas the ratio of calls routed to the LLM. We then plot the accuracy vs. the % calls to LLM curve to\nillustrate the acceleration performance of both CITER and the baselines. The optimal point is located\nin the top-left corner of the curve, corresponding to the highest accuracy with the lowest inference\ncost.\n\nBaselines. We compare CITER with both a representative query-level\nrouting method\n(RouteLLM (Ong et al., 2024)) and a token-level routing method (co-LLM (Shen et al., 2024)).\n\n6\n\n270\n271\n\n272\n273\n274\n275\n276\n277\n\n278\n279\n280\n281\n282\n283\n\n284\n285\n286\n287\n288\n\n289\n290\n291\n292\n293\n294\n\n295\n296\n297\n298\n299\n300\n\n301\n302\n303\n304\n305\n306\n\n307\n308\n309\n310\n311\n\n312\n313\n314\n315\n316\n317\n\n318\n319\n320\n321\n322\n323\n\n0.00.20.40.60.81.0%CalltoLLM4050607080AccuracyQwen2-1.5BQwen2-72BRandomCITERco-LLMRouteLLM0.00.20.40.60.81.0%CalltoLLM556065707580859095AccuracyQwen2-1.5BQwen2-72BRandomCITERco-LLMRouteLLM0.00.20.40.60.81.0%CalltoLLM304050607080AccuracyQwen2-1.5BQwen2-72BRandomCITERco-LLMRouteLLM0.00.20.40.60.81.0%CalltoLLM3040506070AccuracyQwen2-1.5BQwen2-72BRandomCITERco-LLMRouteLLM\fUnder review as a conference paper at ICLR 2025\n\n324\n325\n\n326\n327\n328\n329\n330\n331\n\n332\n333\n334\n335\n336\n337\n\n338\n339\n340\n341\n342\n\n343\n344\n345\n346\n347\n348\n\n349\n350\n351\n352\n353\n354\n\n355\n356\n357\n358\n359\n360\n\n361\n362\n363\n364\n365\n\n366\n367\n368\n369\n370\n371\n\n372\n373\n374\n375\n376\n377\n\n(a) Commonsense QA\n\n(b) ARC-Challenge\n\n(c) GSM8k\n\n(d) MMLU-PP\n\nFigure 3: The accuracy vs % calls to LLM curve of CITER and the varient CITER-S. Points closer\nto the top-left corner indicate better acceleration performance.\n\n(a) Commonsense QA\n\n(b) ARC-Challenge\n\n(c) GSM8k\n\n(d) MMLU-PP\n\nFigure 4: The accuracy vs % calls to LLM curve of CITER with 1.5B SLM and CITER with 7B\nSLM. Points closer to the top-left corner indicate better acceleration performance.\n\nRouteLLM makes routing decisions for entire queries, directing them to different models for gener-\nation, while co-LLM breaks down the generation process, routing each token to different models.\n\nImplementation Details We implement our framework using the Hugging Face Transformers li-\nbrary (Wolf et al., 2020). For the SLM and LLM, we utilize Qwen2-1.5b and Qwen2-72b, respec-\ntively. The router is implemented as a multilayer perceptron (MLP) network with three hidden\nlayers, ReLU activation (Agarap, 2019), BatchNorm normalization (Ioffe & Szegedy, 2015), and a\n0.1 dropout rate. It is trained using the Adam optimizer (Kingma & Ba, 2017) with a learning rate\nof 1 × 10−7, betas of (0.9, 0.99), and no weight decay. Training is performed on a single NVIDIA\nH100 GPU with a batch size of 80. The iterative training process runs for 2 rounds.\n\n3.2 OVERALL PERFORMANCE\n\nWe conduct extensive experiments to assess the performance of CITER across all benchmark\ndatasets, comparing it against baseline methods. The results are presented in Figure 2. Notably,\nall token-level routing methods, including CITER and co-LLM, significantly outperform the query-\nlevel routing method, RouteLLM, across all datasets, particularly on the Commonsense QA and\nGSM8k datasets, reducing up to 30% calls to LLM while maintaining the same accuracy or achiev-\ning up to 12% higher accuracy with the same ratio of calls to LLM. This emphasizes the effec-\ntiveness of token-level routing, which provides enhanced flexibility in reducing computational costs\nwhile preserving response quality. Furthermore, CITER consistently surpasses co-LLM, achieving\ncomparable accuracy with up to 27% fewer calls to LLM or delivering up to a 17% improvement\nin accuracy with the same ratio of calls to LLM. These findings demonstrate the success of our\nframework in accelerating LLM inference. This outcome is expected, as co-LLM does not consider\nlong-term information during the router training phase, which is crucial for token-level routing. In\nthe following section, we present experiments to further demonstrate the importance of incorporat-\ning long-term information in router training.\n\n3.3 ANALYSIS OF LONG-TERM INFLUENCE\n\nIn this section, we conduct an ablation study on a key component of our framework:\nthe long-\nterm influence of routing decisions, to evaluate its effectiveness. For this purpose, we design an\nablation variant, CITER-S, where the SLM is selected if both the SLM and LLM provide incorrect\npredictions during the routing preference collection, disregarding the long-term impact of routing\n\n7\n\n0.00.20.40.60.81.0%CalltoLLM4050607080AccuracyQwen2-1.5BQwen2-72BRandomCITER-SCITER0.00.20.40.60.81.0%CalltoLLM60708090AccuracyQwen2-1.5BQwen2-72BRandomCITER-SCITER0.00.20.40.60.81.0%CalltoLLM304050607080AccuracyQwen2-1.5BQwen2-72BRandomCITER-SCITER0.00.20.40.60.81.0%CalltoLLM3040506070AccuracyQwen2-1.5BQwen2-72BRandomCITER-SCITER0.00.20.40.60.81.0%CalltoLLM4050607080AccuracyQwen2-1.5BQwen2-7BQwen2-72BRandom1.5B7B0.00.20.40.60.81.0%CalltoLLM60708090AccuracyQwen2-1.5BQwen2-7BQwen2-72BRandom1.5B7B0.00.20.40.60.81.0%CalltoLLM304050607080AccuracyQwen2-1.5BQwen2-7BQwen2-72BRandom1.5B7B0.00.20.40.60.81.0%CalltoLLM3040506070AccuracyQwen2-1.5BQwen2-7BQwen2-72BRandom1.5B7B\fUnder review as a conference paper at ICLR 2025\n\ndecisions. The results are shown in Figure 3. Clearly, CITER significantly outperforms the ablation\nvariant CITER-S across all datasets, reducing calls to LLM by up to 42% while maintaining the\nsame accuracy, or achieving up to a 23% accuracy improvement with the same ratio of calls to\nLLM. These findings highlight the critical role of accounting for the long-term influence of routing\ndecisions.\n\n3.4 ANALYSIS OF ITERATIVE TRAINING PROCESS\n\nTo highlight the importance of the iterative training pro-\ncess, we present the performance curve of CITER with\nthe router over the first three iterations on the Common-\nsense QA dataset. As shown in Figure 5, the results\ndemonstrate a clear improvement in performance from\nthe first to the second iteration. In the second iteration,\nCITER reduces ∼ 5% calls to LLM while maintaining\nthe same accuracy or achieves 2 ∼ 3% higher accuracy\nwith the same ratio of calls to LLM compared to the first.\nThis improvement underscores the effectiveness of our\nproposed iterative training process. Moreover, the per-\nformance curve of the third iteration closely follows that\nof the second, indicating that the router has already con-\nverged by the second iteration. The rapid convergence of\nthe router emphasizes the robustness of our training strat-\negy, suggesting that optimal performance can be achieved\nwithout excessive computational costs or extended train-\ning periods.\n\n3.5 ANALYSIS OF THE IMPACT OF SLM MODEL SIZE\n\nFigure 5: Accuracy vs. % calls to\nLLM curve of CITER with router over\nthe first three iterations on the common-\nsense QA datasets. Points closer to the\ntop-left corner indicate better accelera-\ntion performance.\n\nWe further scale up the SLM size from Qwen2-1.5B to Qwen2-7B, while keeping the LLM fixed\nto Qwen2-72B, to understand the scalability of our framework. As shown in Figure 4, the results\nclearly demonstrate that CITER reduces calls to LLM by up to 10% while maintaining the same level\nof accuracy or achieves up to 11% higher accuracy with the same ratio of calls to LLM when using\nQwen2-7B as the SLM compared to Qwen2-1.5B, particularly on the commonsense QA and GSM8k\ndatasets, underscoring our framework’s scalability with larger SLMs. However, the performance\ngap is most noticeable when only a few calls to LLM are made, and it gradually diminishes or even\ndisappears as the ratio of calls to LLM increases. This is expected, as the SLM’s capacity limits its\nperformance, and the quality of responses increasingly depends on the LLM as more calls are routed\nto it.\n\n3.6 CASE STUDY ANALYSIS ON THE ROUTER\n\nFinally, we perform a case study to further analyze the decision-making process of the router in our\nframework. A selection of examples, along with their corresponding routing decisions, is shown\nin Figure 6. In the left example, it is clear that our router accurately identifies the critical tokens,\nincluding the first occurrence of the answer ”Midwest” and the word ”fertile,” which describes the\nfarmland in the Midwest, both crucial to the final answer. Moreover, most non-critical tokens are\nefficiently offloaded to the SLM, effectively reducing computational costs.\n\nIn the right example, we compare CITER with the token-level routing method co-LLM. It is evident\nthat our router outperforms co-LLM by correctly identifying potential critical tokens, particularly\ntime-related words. In co-LLM’s response, at the first red-marked word ”morning,” co-LLM incor-\nrectly routes the word ”the” to the LLM while assigning the contextually important word ”morning”\nto the SLM, leading to an initial error in the response. Similarly, co-LLM routes the critical phrase\n”afternoon meal” to the SLM, which results in the final incorrect prediction. In contrast, our router\ncorrectly identifies the critical word ”day” and routes it to the LLM, followed by routing the phrase\n”has already” to accurately capture the reasoning process, ultimately leading to the correct predic-\ntion.\n\n8\n\n378\n379\n\n380\n381\n382\n383\n384\n385\n\n386\n387\n388\n389\n390\n391\n\n392\n393\n394\n395\n396\n\n397\n398\n399\n400\n401\n402\n\n403\n404\n405\n406\n407\n408\n\n409\n410\n411\n412\n413\n414\n\n415\n416\n417\n418\n419\n\n420\n421\n422\n423\n424\n425\n\n426\n427\n428\n429\n430\n431\n\n0.00.20.40.60.81.0%CalltoLLM4050607080AccuracyQwen2-1.5BQwen2-72BRandomIteration1Iteration2Iteration3\fUnder review as a conference paper at ICLR 2025\n\n432\n433\n\n434\n435\n436\n437\n438\n439\n\n440\n441\n442\n443\n444\n445\n\n446\n447\n448\n449\n450\n\n451\n452\n453\n454\n455\n456\n\n457\n458\n459\n460\n461\n462\n\n463\n464\n465\n466\n467\n468\n\n469\n470\n471\n472\n473\n\n474\n475\n476\n477\n478\n479\n\n480\n481\n482\n483\n484\n485\n\nFigure 6: The case study analysis of our router. The words highlighted with yellow background are\ngenerated by the LLM, while other words are generated by the SLM. The red-marked words are the\nmistakes in co-LLM’s response.\n\nThese examples illustrate that the router in CITER effectively distinguishes between critical and\nnon-critical tokens, offloading non-critical tokens to the SLM to minimize computational overhead,\nwhile leveraging the LLM to ensure the quality of the generated response.\n\n4 RELATED WORK\n\nIn this section, we conduct a literature review that mainly focuses on prior LLM inference acceler-\nation methods, especially those that involve using routing mechanisms and collaborative inference\nbetween LLMs for inference acceleration.\n\nQuery-Level Routing Mechanisms. Previous routing methods (Jang et al., 2023; Chronopoulou\net al., 2023; Diao et al., 2023; Lu et al., 2023; Cheng et al., 2024; Lu et al., 2024; Chen et al., 2023b;\nWang et al., 2024b) for efficient inference mainly focus on routing entire user queries to different\nmodels for generation. For example, Routoo (Mohammadshahi et al., 2024) proposes a performance\npredictor and a cost-aware decoder to route between LLMs, considering both performance and re-\nsource constraints; RouteLLM (Ong et al., 2024) formulates the routing problem as a classification\nproblem and employs a data augmentation framework to significantly expand the dataset used for\ntraining the router. FrugalGPT Chen et al. (2023b) formulates the routing problem as a constrained\noptimization problem, where the final generated quality is maximized under a budget or inference\ncost constraint. However, as highlighted in Section 1, routing at the query-level granularity may lead\nto suboptimal performance, as non-critical tokens in complex queries may be generated inefficiently,\nwhile critical tokens in simple queries may suffer from inaccuracy. In contrast, token-level routing\nmethods offer more fine-grained control over the routing process, improving both inference costs\nand the quality of the generated response.\n\nToken-Level Routing Mechanisms. Unlike query-level routing methods, previous token-level\nrouting methods (Pfeiffer et al., 2021; Belofsky, 2023; Muqeeth et al., 2024; Wang et al., 2024a;\nWu et al., 2024; Xu et al., 2024) mainly focus on routing input tokens to different specialized\nexperts to enhance performance without considering the computational cost. For example, Ar-\nrow (Ostapenko et al., 2024) reuses a library of expert LoRAs to build a mixture-of-experts (MoE)\n\n9\n\nco-LLMWendi feeds each of her 20 chickens 3 cups of feed daily, splitinto three meals. In the morning, she gives them 15 cups, and inthe afternoon, another 25 cups. How many cups of feed does sheneed to give in the final meal?James was looking for agood place to buyfarmland. Where mighthe look?CITERWendi needs 20 * 3 = 60cups of feed for the morningmeal. Wendi needs 60 cupsof feed for the afternoonmeal, so she needs 60 + 60= \\boxed{120} cups of feedfor the final meal.Wendi needs 20 * 3 = 60 cupsof feed for the day. Wendi hasalready given her chickens 15 +25 = 40 cups of feed. Thus,Wendi needs to give herchickens 60 - 40 = \\boxed{20}cups of feed in the final meal.CITERThe Midwest is a region of theUnited States that is knownfor its fertile farmland.Therefore, the answer isMidwest.Q1Q2\fUnder review as a conference paper at ICLR 2025\n\n486\n487\n\n488\n489\n490\n491\n492\n493\n\n494\n495\n496\n497\n498\n499\n\n500\n501\n502\n503\n504\n\n505\n506\n507\n508\n509\n510\n\n511\n512\n513\n514\n515\n516\n\n517\n518\n519\n520\n521\n522\n\n523\n524\n525\n526\n527\n\n528\n529\n530\n531\n532\n533\n\n534\n535\n536\n537\n538\n539\n\narchitecture, dynamically routing inputs to different LoRAs during inference. Similarly, Branch-\nTrain-MiX (Sukhbaatar et al., 2024) fine-tunes LLMs on different domains from a seed LLM, cre-\nating specialized experts to form an MoE framework. Among these methods, co-LLM (Shen et al.,\n2024) is the most relevant to our framework CITER, introducing a router to route tokens to mod-\nels of different sizes. However, co-LLM only considers the current outputs from SLM and LLM\nwhen generating ground truth labels to train the router. This may lead to suboptimal performance\nsince the influence of current decisions on future tokens is not considered. Moreover, similar to\nother token-level routing methods, co-LLM focuses on enhanced response quality without taking\nthe computational cost of the inference process into account. In contrast, our CITER framework\nconsiders both the current token and the future impact of each decision, enabling more accurate and\nefficient routing.\n\nOther Methods for LLM Inference Acceleration.\nIn addition to routing methods, several ap-\nproaches ranging from algorithmic to system optimizations (Leviathan et al., 2023; Miao et al.,\n2023; Kwon et al., 2023; Bhendawade et al., 2024; Cai et al., 2024; Chen et al., 2024b;a) have been\nproposed to accelerate LLM inference. Speculative Decoding (Leviathan et al., 2023; Chen et al.,\n2023a) employs a small draft model to generate potential next tokens, which are concatenated with\npreviously generated tokens. These guesses are then processed by the target LLM in parallel to\nverify their correctness. Tokens are only committed to the final output if confirmed by the target\nLLM. Although this approach reduces inference time by generating multiple tokens in a single for-\nward pass, it does not lower the overall computational complexity (e.g., the total amount of FLOPs).\nSpeculative Streaming (Bhendawade et al., 2024) addresses the computational overhead of Specu-\nlative Decoding by predicting n-grams instead of individual tokens in each forward pass. However,\nit requires redesigning the LLM architecture, necessitating re-pretraining, which is computation-\nally prohibitive for many use cases. Medusa (Cai et al., 2024) mitigates the re-pretraining issue\nby adding auxiliary heads to the original LLM, allowing n-gram predictions without modifying the\ncore model. These heads can be trained while keeping the original LLM frozen, thereby avoiding\nthe need for re-pretraining. SpecInfer and Sequoia (Miao et al., 2023; Chen et al., 2024b) leverage\ntree-based parallelism for decoding and verification to further accelerate inference.\n\n5 CONCLUSION AND FUTURE WORK\n\nIn this paper, we propose a novel Collaborative Inference with Token-lEvel Routing(CITER) frame-\nwork to address the challenge of high computational costs associated with large language models.\nBy introducing a token-level routing mechanism, CITER enables efficient collaboration between\nsmaller and larger models, achieving a balance between computational efficiency and generation\nquality. The key challenge lies in training the router, which we formulate as a reinforcement learn-\ning task. The router receives rewards based on both prediction quality and inference cost, allowing\nit to learn to predict token-level routing scores and make routing decisions by considering both the\ncurrent token and the future impact of its decision. Additionally, we introduce an iterative training\nprocess to mitigate inconsistencies during the dataset collection phase. To further accelerate the\nlearning process, we propose a shortcut where the reward is estimated based on the predictions from\nboth the small and large models at each step. Extensive experiments demonstrate that CITER sig-\nnificantly reduces inference costs while maintaining high-quality generation, offering a promising\nsolution for real-time and resource-constrained applications.\n\nFuture work can explore several directions to further enhance the CITER framework. One potential\navenue is the application of CITER to other modalities, such as vision or multimodal tasks, to assess\nits generalizability and effectiveness beyond language models. Additionally, investigating the use of\nself-supervised learning to pretrain the router and reduce reliance on costly iterative training could\nalso be a promising direction. To be more specific, other estimation methods could be introduced\nto further accelerate the routing preference collection for the case where both the SLM and the\nLLM make the incorrect next token prediction. Future research could also focus on improving the\ninterpretability of the routing mechanism, providing more transparency on how the model makes\ntoken-level routing decisions. Finally, exploring the integration of CITER with model compression\ntechniques or dynamic architectures may yield further reductions in computational costs, making it\neven more suitable for deployment in real-time, edge-based applications.\n\n10\n\n\fUnder review as a conference paper at ICLR 2025\n\n540\n541\n\n542\n543\n544\n545\n546\n547\n\n548\n549\n550\n551\n552\n553\n\n554\n555\n556\n557\n558\n\n559\n560\n561\n562\n563\n564\n\n565\n566\n567\n568\n569\n570\n\n571\n572\n573\n574\n575\n576\n\n577\n578\n579\n580\n581\n\n582\n583\n584\n585\n586\n587\n\n588\n589\n590\n591\n592\n593",
    "abstract": "Large language models (LLMs) have achieved remarkable success in natural language processing tasks but suffer from high computational costs during inference, limiting their deployment in latency-constrained applications. To address this issue, we propose a novel \\textbf{C}ollaborative \\textbf{I}nference with \\textbf{T}oken-l\\textbf{E}vel \\textbf{R}outing (CITER) framework that introduces a token-level routing mechanism, enabling efficient collaboration between small and large language models (SLMs \\& LLMs). Specifically, CITER enables routing non-critical tokens to an SLM to reduce computational overhead, while critical tokens are processed by an LLM to maintain generation quality. We formulate the training of the router as a reinforcement learning task, where the router receives rewards based on both the quality of predictions and the inference cost of generation. This allows the router to learn to predict token-level routing scores and make routing decisions based on both the current token and the future impact of its decisions. To further accelerate reward evaluation process, we introduce a shortcut for reward function estimation, significantly reducing the cost of the reward estimation and improving the practicality of our approach. Extensive experiments across four benchmark datasets demonstrate that CITER reduces inference cost while preserving high-quality generation, offering a promising solution for real-time and resource-constrained applications.",
    "response": {
      "Soundness": 2,
      "Presentation": 3,
      "Contribution": 2,
      "Rating": 5,
      "Confidence": 3,
      "Strengths": "The study addresses the critical practical issue of reducing LLM inference costs, which is vital for real-world deployment. CITER achieves equivalent performance with 30% fewer LLM calls and improves performance by up to 25% with the same number of LLM calls. Technical innovation is notable, featuring fine-grained control through token-level routing, systematic router training via reinforcement learning, and enhanced training efficiency using shortcut reward estimation. The validation is thorough, with extensive experiments conducted across four benchmarks, a detailed ablation study, and verification across various model sizes.",
      "Weaknesses": "The paper lacks sufficient overhead analysis, with no evaluation of the router's computation and memory costs, potential latency from model switching, or end-to-end performance assessment. Especially, the overhead analysis is missing in terms of inference and training as well. \n\nIts generalizability remains uncertain, as evaluations are limited to QA tasks and a single model series (Qwen2), without verification in multilingual or long-form generation contexts. \n\nThe theoretical justification is also limited, with insufficient rationale for the router's structural choices, no convergence analysis for iterative training, and inadequate verification of shortcut methods' accuracy.",
      "Questions": "What is the additional latency introduced by the router?\nI strongly doubt that single large model call can be more efficient than your approach in case of including training and fine-tuning. \n\nDo similar benefits hold for larger models?\nHow does the framework perform with inputs of varying lengths?\nHow does it perform on tasks with high token dependency?\n\nWhat are the anticipated challenges in real-world deployment?"
    }
  },
  {
    "paper_id": "SM1guXel3E",
    "summary": "Background: Mixup augmentation is a technique used to enhance the generalization ability of deep neural networks (DNNs). It involves mixing data examples to help models learn smoother decision boundaries and reduce overfitting. Historically, Mixup and its variants like ManifoldMix, CutMix, and others have been applied primarily in visual classification tasks. However, mixup augmentation lacks standardized implementations and benchmarks, which makes it hard to achieve reproducibility and consistency across studies.\nGap: The gap addressed in this paper is the absence of a standardized, comprehensive mixup augmentation codebase and benchmark in visual representation learning. This absence leads to challenges in reproducibility, fairness in comparison, and deriving consistent insights across different efforts in applying and extending mixup techniques.\nHypothesis: The authors propose that creating a standardized mixup augmentation codebase and benchmark, called OpenMixup, will address the issues of reproducibility and fairness in the evaluation of mixup techniques. This will also facilitate broader applications of mixup beyond classification, including self-supervised learning and regression tasks.\nMethod: The authors introduce OpenMixup, a comprehensive open-source codebase and benchmark for mixup augmentation in vision tasks. They implement and evaluate 18 mixup methods across 11 image datasets with varying characteristics, such as fine-grained and non-iconic scene datasets. OpenMixup includes a modular framework that supports data preprocessing, mixup development, architecture selection, model training, and evaluation. The authors also conduct empirical analyses of the mixup methods using metrics like GPU memory usage, loss landscape, and robustness to gain insights into trade-offs between performance and efficiency.\nConclusion: OpenMixup enables fair comparisons and assessments of mixup methods by providing a standardized platform. The experiments show that dynamic mixup methods generally outperform static ones in generalizability and performance. Furthermore, OpenMixup's extensible framework supports diverse tasks and promotes further research and application development in mixup augmentation. The observations derived from this benchmark are expected to guide future advancements in mixup techniques and their applications.\n",
    "full text": "\nOPENMIXUP: OPEN MIXUP TOOLBOX AND BENCH-\nMARK FOR VISUAL REPRESENTATION LEARNING\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nMixup augmentation has emerged as a widely used technique for improving the\ngeneralization ability of deep neural networks (DNNs). However, the lack of stan-\ndardized implementations and benchmarks has impeded recent progress, resulting\nin poor reproducibility, unfair comparisons, and conflicting insights. In this paper,\nwe introduce OpenMixup, the first mixup augmentation codebase and benchmark\nfor visual representation learning. Specifically, we train 18 representative mixup\nbaselines from scratch and rigorously evaluate them across 11 image datasets of\nvarying scales and granularity, ranging from fine-grained scenarios to complex non-\niconic scenes. We also open-source our modular codebase including a collection\nof popular vision backbones, optimization strategies, and analysis toolkits, which\nnot only supports the benchmarking but enables broader mixup applications be-\nyond classification, such as self-supervised learning and regression tasks. Through\nexperiments and empirical analysis, we gain observations and insights on mixup\nperformance-efficiency trade-offs, generalization, and optimization behaviors, and\nthereby identify preferred choices for different needs. To the best of our knowledge,\nOpenMixup has facilitated several recent studies. We believe this work can further\nadvance reproducible mixup augmentation research and thereby lay a solid ground\nfor future progress in the community. The source code will be publicly available.\n\n1\n\nINTRODUCTION\n\nData mixing, or mixup, has proven effective in\nenhancing the generalization ability of DNNs,\nwith notable success in visual classification\ntasks. The pioneering Mixup (Zhang et al.,\n2018) proposes to generate mixed training ex-\namples through the convex combination of two\ninput samples and their corresponding one-\nhot labels. By encouraging models to learn\nsmoother decision boundaries, mixup effec-\ntively reduces overfitting and thus improves\nthe overall performance. ManifoldMix (Verma\net al., 2019) and PatchUp (Faramarzi et al.,\n2020) extend this operation to the hidden space.\nCutMix (Yun et al., 2019) presents an alter-\nnative approach, where an input rectangular\nregion is randomly cut and pasted onto the\ntarget in the identical location. Subsequent\nworks (Harris et al., 2020; ha Lee et al., 2020;\nBaek et al., 2021) have focused on designing\nmore complex hand-crafted policies to gener-\nate diverse and informative mixed samples, which can all be categorized as static mixing methods.\n\nFigure 1: Radar plot of top-1 accuracy for represen-\ntative mixup baselines on 11 classification datasets.\n\nDespite efforts to incorporate saliency information into static mixing framework (Walawalkar et al.,\n2020; Uddin et al., 2020; Qin et al., 2023), they still struggle to ensure the inclusion of desired\ntargets in the mixed samples, which may result in the issue of label mismatches. To address this\nproblem, a new class of optimization-based methods, termed dynamic mixing, has been proposed, as\nillustrated in the second row of Figure 2. PuzzleMix (Kim et al., 2020) and Co-Mixup (Kim et al.,\n2021) are two notable studies that leverage optimal transport to improve offline mask determination.\n\n1\n\n000\n001\n\n002\n003\n004\n005\n006\n007\n\n008\n009\n010\n011\n012\n013\n\n014\n015\n016\n017\n018\n\n019\n020\n021\n022\n023\n024\n\n025\n026\n027\n028\n029\n030\n\n031\n032\n033\n034\n035\n036\n\n037\n038\n039\n040\n041\n\n042\n043\n044\n045\n046\n047\n\n048\n049\n050\n051\n052\n053\n\nImageNet(ResNet-50)ImageNet(ConvNeXt-T)CIFAR-10(ResNet-18)CIFAR-100(WRN-28-8)Tiny-ImageNet(ResNeXt-50)FashionMNIST(ResNet-18)STL-10(ResNet-18)CUB-200(ResNeXt-50)FGVC-Aircraft(ResNeXt-50)iNatualist-2017(ResNeXt-101)iNatualist-2018(ResNeXt-101)Place205(ResNet-50)77.778.379.079.7VanillaMixupCutMixManifoldMixSaliencyMixFMixPuzzleMixResizeMixAutoMixSAMix80.080.981.882.895.796.497.197.882.283.584.886.066.268.570.873.095.195.696.196.677.880.583.286.083.785.086.287.484.485.386.387.264.666.067.468.867.668.869.971.063.363.664.064.4\fUnder review as a conference paper at ICLR 2025\n\n054\n055\n\n056\n057\n058\n059\n060\n061\n\n062\n063\n064\n065\n066\n067\n\n068\n069\n070\n071\n072\n\n073\n074\n075\n076\n077\n078\n\n079\n080\n081\n082\n083\n084\n\n085\n086\n087\n088\n089\n090\n\n091\n092\n093\n094\n095\n\n096\n097\n098\n099\n100\n101\n\n102\n103\n104\n105\n106\n107\n\nFigure 2: Visualization of mixed samples from representative static and dynamic mixup augmentation\nmethods on ImageNet-1K. We employ a mixing ratio of λ = 0.5 for a comprehensive comparison.\nNote that mixed samples are more precisely in dynamic mixing policies than these static ones.\n\nMore recently, TransMix (Chen et al., 2022), TokenMix (Liu et al., 2022a), MixPro (Zhao et al.,\n2023), and SMMix (Chen et al., 2023) are specifically tailored for Vision Transformers (Dosovitskiy\net al., 2021). The AutoMix series (Liu et al., 2022d; Qin et al., 2024) introduces a brand-new mixup\nlearning paradigm, where mixed samples are computed by an online-optimizable generator in an\nend-to-end manner. These emerging dynamic approaches represent a promising avenue for generating\nsemantically richer training samples that align with the underlying structure of input data.\n\nWhy do we call for a mixup augmentation benchmark? While dynamic methods have shown signs\nof surpassing the static ones, their indirect optimization process incurs significant computational\noverhead, which limits their efficiency and applicability. Therefore, without a systematic understand-\ning, it is uncertain if dynamic mixup serves as the superior alternative in vision tasks. Moreover, a\nthorough and standardized evaluation of different dynamic methods is also missing in the community.\nBenchmark is exactly the way to establish such an understanding, which plays a pivotal role in driving\nresearch progress by integrating an agreed-upon set of tasks, impartial comparisons, and assessment\ncriteria. To the best of our knowledge, however, there have been no such comprehensive benchmarks\nfor mixup augmentation to facilitate unbiased comparisons and practical use in visual recognition.\n\nWhy do we need an open-source mixup codebase? Notably, most existing mixup techniques are\ncrafted with diverse settings, tricks, and implementations, each with its own coding style. This lack\nof standardization not only hinders user-friendly reproduction and deployment but impedes further\ndevelopment, thus imposing costly trial-and-error on practitioners to determine the most appropriate\nmixup strategy for their specific needs in real-world applications. Hence, it is essential to develop a\nunified mixup visual representation learning codebase for standardized data pre-processing, mixup\ndevelopment, network architecture selection, model training, evaluation, and empirical analysis.\n\nIn this paper, we present OpenMixup, the first comprehensive benchmark for mixup augmentation in\nvision tasks. Unlike previous work (Naveed, 2021; Lewy & Ma´ndziuk, 2023), we train and evaluate\n18 methods that represent the foremost strands on 11 diverse image datasets, as illustrated in Figure 1.\nWe also open-source a standardized mixup codebase for visual representation learning, where the\noverall framework is built up with modular components for data pre-processing, mixup augmentation,\nnetwork backbone selection, optimization, and evaluations. The codebase not only powers our bench-\nmarking but supports broader relatively under-explored mixup applications beyond classification,\nsuch as semi-supervised learning (Berthelot et al., 2019), self-supervised learning (Kalantidis et al.,\n2020; Shen et al., 2022), and dense prediction tasks (He et al., 2017; Bochkovskiy et al., 2020).\n\nFurthermore, insightful observations are obtained by incorporating multiple evaluation metrics and\nanalysis toolkits in our codebase, including GPU memory usage (Figure 4), loss landscape (Fig-\nure 5(c)), Power Law (PL) exponent alpha metrics (Figure 6), robustness and calibration (Table A8),\netc. For instance, despite the key role static mixing plays in today’s deep learning systems, we surpris-\ningly find that its generalizability over diverse datasets and backbones is significantly inferior to that\nof dynamic algorithms. By ranking the performance and efficiency trade-offs, we reveal that recent\ndynamic methods have already outperformed the static ones. This may suggest a promising break-\nthrough for mixup augmentation, provided that the dynamic computational overhead can be further\nreduced. Overall, we believe these insights can facilitate better evaluation and comparisons of mixup\nmethods, enabling a systematic understanding and thus paving the way for further advancements.\n\nSince such a first-of-its benchmark can be rather time- and resource-consuming and most current\nadvances have focused on and stemmed from visual classification tasks, we centralize our benchmark-\ning scope on classification while extending it to broader mixup applications with transfer learning.\n\n2\n\nImage AAttentiveMixAutoMixCutMixImage BMixupPuzzleMixResizeMixFMixSaliencyMixSmoothMixSAMixGridMixCo-Mixup\fUnder review as a conference paper at ICLR 2025\n\n108\n109\n\n110\n111\n112\n113\n114\n115\n\n116\n117\n118\n119\n120\n121\n\n122\n123\n124\n125\n126\n\n127\n128\n129\n130\n131\n132\n\n133\n134\n135\n136\n137\n138\n\n139\n140\n141\n142\n143\n144\n\n145\n146\n147\n148\n149\n\n150\n151\n152\n153\n154\n155\n\n156\n157\n158\n159\n160\n161\n\nMeanwhile, we have already supported these downstream tasks and datasets in our open-source\ncodebase, allowing practitioners to customize their mixup algorithms, models, and training setups in\nthese relatively under-explored scenarios. Our key contributions can be summarized as follows:\n\n• We introduce OpenMixup, the first comprehensive benchmarking study for mixup augmentation,\nwhere 18 representative baselines are trained from scratch and rigorously evaluated on 11 visual\nclassification datasets, ranging from non-iconic scenes to gray-scale, fine-grained, and long tail\nscenarios. By providing a standard testbed and a rich set of evaluation protocols, OpenMixup\nenables fair comparisons, thorough assessment, and analysis of different mixup strategies.\n\n• To support reproducible mixup research and user-friendly method deployment, we provide an\nopen-source codebase for visual representation learning. The codebase incorporates standardized\nmodules for data pre-processing, mixup augmentation, backbone selection, optimization policies,\nand distributed training functionalities. Beyond the benchmark itself, our OpenMixup codebase\nis readily extensible and has supported semi- and self-supervised learning and visual attribute\nregression tasks, which further enhances its utility and potential benefits to the community.\n\n• Observations and insights are obtained through extensive analysis. We investigate the general-\nization ability of all evaluated mixup baselines across diverse datasets and backbones, compare\ntheir GPU memory footprint and computational cost, visualize the loss landscape and PL expo-\nnent alpha metrics to understand optimization behavior, and evaluate robustness against input\ncorruptions and calibration performance. Furthermore, we establish comprehensive rankings in\nterms of their performance and applicability (efficiency and versatility), offering clear method\nguidelines for specific requirements. These findings not only present a firm grasp of the current\nmixup augmentation landscape but shed light on promising avenues for future advancements.\n\n2 BACKGROUND AND RELATED WORK\n\n2.1 PROBLEM DEFINITION\n\nMixup Training. We first consider the general image classification tasks with k different classes:\ni=1 ∈ Rn×W ×H×C and their corresponding ground-\ngiven a finite set of n image samples X = [xi]n\ni=1 ∈ Rn×k, encoded by a one-hot vector yi ∈ Rk. We attempt to seek the\ntruth class labels Y = [yi]n\nmapping from input data xi to its class label yi modeled through a deep neural network fθ : x (cid:55)−→ y\nwith parameters θ by optimizing a classification loss ℓ(.), say the cross entropy (CE) loss,\n\nℓCE(fθ(x), y) = −y log fθ(x).\nThen we consider the mixup classification task: given a sample mixing function h, a label mixing\nfunction g, and a mixing ratio λ sampled from Beta(α, α) distribution, we can generate the mixed\ndata Xmix with xmix = h(xi, xj, λ) and the mixed label Ymix with ymix = g(yi, yj, λ), where α is\na hyper-parameter. Similarly, we learn fθ : xmix (cid:55)−→ ymix by the mixup cross-entropy (MCE) loss,\n\n(1)\n\nℓM CE = λℓCE(fθ(xmix), yi) + (1 − λ)ℓCE(fθ(xmix), yj).\n\n(2)\n\nMixup Reformulation. Comparing Eq. (1) and Eq. (2), the mixup training has the following\nfeatures: (1) extra mixup policies, g and h, are required to generate Xmix and Ymix.\n(2) the\nclassification performance of fθ depends on the generation policy of mixup. Naturally, we can\nsplit the mixup task into two complementary sub-tasks: (i) mixed sample generation and (ii) mixup\nclassification (learning objective). Notice that the sub-task (i) is subordinate to (ii) because the final\ngoal is to obtain a stronger classifier. Therefore, from this perspective, we regard the mixup generation\nas an auxiliary task for the classification task. Since g is generally designed as a linear interpolation,\ni.e., g(yi, yj, λ) = λyi + (1 − λ)yj, h becomes the key function to determine the performance of\nthe model. Generalizing previous offline methods, we define a parametric mixup policy hϕ as the\nsub-task with another set of parameters ϕ. The final goal is to optimize ℓM CE given θ and ϕ as:\n\nmin\nθ, ϕ\n\nℓM CE\n\n(cid:16)\n\n(cid:17)\n(cid:0)hϕ(xi, xj, λ)(cid:1), g(yi, yj, λ)\n\n.\n\nfθ\n\n(3)\n\n2.2 SAMPLE MIXING\n\nWithin the realm of visual classification, prior research has primarily concentrated on refining the\nsample mixing strategies rather than the label mixing ones. In this context, most sample mixing\nmethods are categorized into two groups: static policies and dynamic policies, as presented in Table 1.\n\n3\n\n\fUnder review as a conference paper at ICLR 2025\n\n162\n163\n\n164\n165\n166\n167\n168\n169\n\n170\n171\n172\n173\n174\n175\n\n176\n177\n178\n179\n180\n\n181\n182\n183\n184\n185\n186\n\n187\n188\n189\n190\n191\n192\n\n193\n194\n195\n196\n197\n198\n\n199\n200\n201\n202\n203\n\n204\n205\n206\n207\n208\n209\n\n210\n211\n212\n213\n214\n215\n\nCategory Publication\nICLR’2018\nICCV’2019\nICML’2021\n\nTable 1: Overview of all supported vision Mixup augmentation methods in OpenMixup. Note that\nMixup and CutMix in label mixing indicate mixing the labels of two samples by linear interpolation\nor computing cut squares. The Perf., App., and Overall denote the performance, applicability, and\noverall rankings of all methods, which are derived from average rankings across baselines (view B.5).\nMethod\nLabel Mixing Extra Cost ViT only Perf. App. Overall\nMixup (Zhang et al., 2018)\nStatic\nStatic\nCutMix (Yun et al., 2019)\nDeiT (CutMix+Mixup) (Touvron et al., 2021) Static\nSmoothMix (ha Lee et al., 2020)\nGridMix (Baek et al., 2021)\nResizeMix (Qin et al., 2023)\nManifoldMix (Verma et al., 2019)\nFMix (Harris et al., 2020)\nAttentiveMix (Walawalkar et al., 2020)\nSaliencyMix (Uddin et al., 2020)\nPuzzleMix (Kim et al., 2020)\nAlignMix (Venkataramanan et al., 2022)\nAutoMix (Liu et al., 2022d)\nSAMix (Li et al., 2021)\nAdAutoMix (Qin et al., 2024)\nTransMix (Chen et al., 2022)\nSMMix (Chen et al., 2023)\nDecoupledMix (Liu et al., 2022c)\n\nICLR’2021\nDynamic ICML’2020\nDynamic CVPR’2022 Optimal-transported Interpolation\nDynamic ECCV’2022\nDynamic arXiv’2021\nDynamic ICLR’2024\nDynamic CVPR’2022\nDynamic ICCV’2023\n\nSample Mixing\nHand-crafted Interpolation\nHand-crafted Cutting\nCutMix+Mixup\nHand-crafted Cutting\nHand-crafted Cutting\nHand-crafted Cutting\nLatent-space Mixup\nFourier-guided Cutting\nPretraining-guided Cutting\nSaliency-guided Cutting\nOptimal-transported Cutting\n\nMixup\nCutMix\nCutMix+Mixup\nCutMix\nCutMix\nCutMix\nMixup\nCutMix\nCutMix\nCutMix\nCutMix\nCutMix\nCutMix\nCutMix\nCutMix\nAttention-guided\nAttention-guided\nDecoupledMix\n\nStatic CVPRW’2020\nStatic\nStatic CVMJ’2023\nICML’2019\nStatic\nStatic\narXiv’2020\nStatic ICASSP’2020\nStatic\n\nEnd-to-end-learned Cutting\nEnd-to-end-learned Cutting\nEnd-to-end-learned Cutting\nCutMix+Mixup\nCutMix+Mixup\n\nStatic NeurIPS’2023 Any Sample Mixing Policies\n\n15\n13\n7\n18\n17\n10\n14\n16\n9\n11\n8\n12\n3\n1\n2\n5\n4\n6\n\n10\n8\n3\n13\n12\n5\n9\n11\n6\n6\n6\n8\n4\n1\n4\n7\n6\n2\n\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✓\n✗\n✓\n✓\n✓\n✓\n✓\n✗\n✗\n✗\n\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✗\n✓\n✓\n✗\n\n1\n1\n1\n1\n1\n1\n1\n1\n3\n1\n4\n2\n6\n5\n7\n8\n8\n1\n\nPR’2021\n\nStatic Policies. The sample mixing procedure in all static policies is conducted in a hand-crafted\nmanner. Mixup (Zhang et al., 2018) first generates artificially mixed data through the convex combina-\ntion of two selected input samples and their associated one-hot labels. ManifoldMix variants (Verma\net al., 2019; Faramarzi et al., 2020) extend the same technique to latent space for smoother feature\nmixing. Subsequently, CutMix (Yun et al., 2019) involves the random replacement of a certain\nrectangular region inside the input sample while concurrently employing Drop Patch throughout the\nmixing process. Inspired by CutMix, several researchers in the community have explored the use of\nsaliency information (Uddin et al., 2020) to pilot mixing patches, while others have developed more\ncomplex hand-crafted sample mixing strategies (Harris et al., 2020; Baek et al., 2021).\n\nIn contrast to static mixing, dynamic strategies are proposed to incorporate\nDynamic Policies.\nsample mixing into an adaptive optimization-based framework. PuzzleMix variants (Kim et al.,\n2020; 2021) introduce combinatorial optimization-based mixing policies in accordance with saliency\nmaximization. SuperMix variants (Dabouei et al., 2021; Walawalkar et al., 2020) utilize pre-trained\nteacher models to compute smooth and optimized samples. Distinctively, AutoMix variants (Liu et al.,\n2022d; Li et al., 2021) reformulate the overall framework of sample mixing into an online-optimizable\nfashion where the model learns to generate the mixed samples in an end-to-end manner.\n\n2.3 LABEL MIXING\n\nMixup (Zhang et al., 2018) and CutMix (Yun et al., 2019) are two widely-recognized label mixing\ntechniques, both of which are static. Recently, there has been a notable emphasis among researchers\non advancing label mixing approaches, which attain more favorable performance upon certain sample\nmixing policies. Based on Transformers, TransMix variants (Chen et al., 2022; Liu et al., 2022a;\nChoi et al., 2022; Chen et al., 2023) are proposed to utilize class tokens and attention maps to adjust\nthe mixing ratio. A decoupled mixup objective (Liu et al., 2022c) is introduced to force models to\nfocus on those hard mixed samples, which can be plugged into different sample mixing policies.\nHolistically, most existing studies strive for advanced sample mixing designs rather than label mixing.\n\n2.4 OTHER APPLICATIONS\n\nRecently, mixup augmentation also has shown promise in more vision applications, such as semi-\nsupervised learning (Berthelot et al., 2019; Liu et al., 2022c), self-supervised pre-training (Kalantidis\net al., 2020; Shen et al., 2022), and visual attribute regression (Wu et al., 2022; Bochkovskiy et al.,\n2020). Although these fields are not as extensively studied as classification, our OpenMixup codebase\nhas been designed to support them by including the necessary task settings and datasets. Its modular\nand extensible architecture allows researchers and practitioners in the community to effortlessly adapt\nand extend their models to accommodate the specific requirements of these tasks, enabling them to\nquickly set up experiments without building the entire pipeline from scratch. Moreover, our codebase\nwill be well-positioned to accelerate the development of future benchmarks, ultimately contributing\nto the advancement of mixup augmentation across a diversity of visual representation learning tasks.\n\n3 OPENMIXUP\n\nThis section introduces our OpenMixup codebase framework and benchmark from four key aspects:\nsupported methods and tasks, evaluation metrics, and experimental pipeline. OpenMixup provides a\n\n4\n\n\fUnder review as a conference paper at ICLR 2025\n\n216\n217\n\n218\n219\n220\n221\n222\n223\n\n224\n225\n226\n227\n228\n229\n\n230\n231\n232\n233\n234\n\n235\n236\n237\n238\n239\n240\n\n241\n242\n243\n244\n245\n246\n\n247\n248\n249\n250\n251\n252\n\n253\n254\n255\n256\n257\n\n258\n259\n260\n261\n262\n263\n\n264\n265\n266\n267\n268\n269\n\nFigure 3: Overview of codebase framework of OpenMixup. (1) benchmarks provide benchmarking\nresults and corresponding config files for mixup classification and transfer learning. (2) openmixup\ncontains implementations of all supported methods. (3) configs is responsible for customizing\nsetups of different mixup methods, networks, datasets, and training pipelines. (4) docs & tools\ncontains paper lists of popular mixup methods, user documentation, and useful tools.\n\nunified framework implemented in PyTorch (Paszke et al., 2019) for mixup model design, training,\nand evaluation. The framework references MMClassification (Contributors, 2020a) and follows\nthe OpenMMLab coding style. We start with an overview of its composition. As shown in Fig-\nure 3, the whole training process here is fragmented into multiple components, including model\narchitecture (.openmixup.models), data pre-processing (.openmixup.datasets), mixup\npolicies (.openmixup.models.utils.augments), script tools (.tools) etc. For instance,\nvision models are summarized into modular building blocks (e.g., backbone, neck, head etc.) in\n.openmixup.models. This modular architecture enables practitioners to easily craft models\nby incorporating different components through configuration files in .configs. As such, users\ncan readily customize their specified vision models and training strategies. In addition, benchmark-\ning configuration (.benchmarks) and results (.tools.model zoos) are also provided in the\ncodebase. Additional benchmarking configurations and details are discussed below.\n\n3.1 BENCHMARKED METHODS\n\nOpenMixup has implemented 17 representative mixup augmentation algorithms and 19 convolutional\nneural network and Transformer model architectures (gathered in .openmixup.models) across\n12 diverse image datasets for supervised visual classification. We summarize these mixup methods\nin Table 1, along with their corresponding conference/journal, the types of employed sample, and\nlabel mixing policies, properties, and rankings. For sample mixing, Mixup (Zhang et al., 2018) and\nManifoldMix (Verma et al., 2019) perform hand-crafted convex interpolation. CutMix (Yun et al.,\n2019), SmoothMix (ha Lee et al., 2020), GridMix (Baek et al., 2021) and ResizeMix (Qin et al., 2023)\nimplement hand-crafted cutting policy. FMix (Harris et al., 2020) utilizes Fourier-guided cutting.\nAttentiveMix (Walawalkar et al., 2020) and SaliencyMix (Uddin et al., 2020) apply pretraining-\nguided and saliency-guided cutting, respectively. Some dynamic approaches like PuzzleMix (Kim\net al., 2020) and AlignMix (Venkataramanan et al., 2022) utilize optimal transport-based cutting and\ninterpolation. AutoMix (Liu et al., 2022d) and SAMix (Li et al., 2021) perform end-to-end online-\noptimizable cutting-based approaches. As for the label mixing, most methods apply Mixup (Zhang\net al., 2018) or CutMix (Yun et al., 2019), while the latest mixup methods for visual transformers\n(TransMix (Chen et al., 2022), TokenMix (Liu et al., 2022a), and SMMix (Chen et al., 2023)), as well\nas DecoupledMix (Liu et al., 2022c) exploit attention maps and a decoupled framework respectfully\ninstead, which incorporate CutMix variants as its sample mixing strategy. Such a wide scope of\nsupported methods enables a comprehensive benchmarking analysis on visual classification.\n\n3.2 BENCHMARKING TASKS\n\nWe provide detailed descriptions of the 12 open-source datasets as shown in Table 2. These\ndatasets can be classified into four categories below: (1) Small-scale classification: We conduct\nbenchmarking studies on small-scale datasets to provide an accessible benchmarking reference.\nCIFAR-10/100 (Krizhevsky et al., 2009) consists of 60,000 color images in 32×32 resolutions.\nTiny-ImageNet (Tiny) (Chrabaszcz et al., 2017) and STL-10 (Coates et al., 2011) are two re-scale\nversions of ImageNet-1K in the size of 64×64 and 96×96. FashionMNIST (Xiao et al., 2017) is\nthe advanced version of MNIST, which contains gray-scale images of clothing. (2) Large-scale\nclassification: The large-scale dataset is employed to evaluate mixup algorithms against the most\n\n5\n\ndocs & toolsopenmixupconfigsbenchmarks.awesome_mixups.model_zoos.tutorials...OpenMixupmodels.backbonesTrain & TestResult SummaryModel AnalysisBackbonesHeadsLossesExternal BlocksClassificationMixupDetectionutils.augmentsmodels.headsmodels.lossesMixup Methodsutils.layers...Self-supervised.visualizationsSegmentationGuidelines.classificationmodels.selfsup.selfsup.semisup....classification.detectron2.mmdetection.mmsegmentation.analysis_tools.summary.dist_train.pyData PipelineModel CustomizeMixup PoliciesOptimizationMixed PrecisionAuto Train\fUnder review as a conference paper at ICLR 2025\n\n270\n271\n\n272\n273\n274\n275\n276\n277\n\n278\n279\n280\n281\n282\n283\n\n284\n285\n286\n287\n288\n\n289\n290\n291\n292\n293\n294\n\n295\n296\n297\n298\n299\n300\n\n301\n302\n303\n304\n305\n306\n\n307\n308\n309\n310\n311\n\n312\n313\n314\n315\n316\n317\n\n318\n319\n320\n321\n322\n323\n\nTable 2: The detailed information of supported visual classification datasets in OpenMixup.\nDatasets\nSource Classes Resolution Train images Test images\nCIFAR-10 (Krizhevsky et al., 2009)\nCIFAR-100 (Krizhevsky et al., 2009)\nFashionMNIST (Xiao et al., 2017)\nSTL-10 (Coates et al., 2011)\nTiny-ImageNet (Chrabaszcz et al., 2017)\nImageNet-1K (Russakovsky et al., 2015)\nCUB-200-2011 (Wah et al., 2011)\nFGVC-Aircraft (Maji et al., 2013)\niNaturalist2017 Horn et al. (2018)\niNaturalist2018 Horn et al. (2018)\nPlaces205 (Zhou et al., 2014)\n\nCategory\nIconic\nIconic\nGray-scale\nIconic\nIconic\nIconic\nFine-grained\nFine-grained\nFine-grained & longtail\nFine-grained & longtail\nScenic\n\n50,000\n50,000\n50,000\n50,00\n10,000\n1,281,167\n5,994\n6,667\n579,184\n437,512\n2,448,873\n\n32×32\n32×32\n28×28\n96×96\n64×64\n469×387\n224×224\n224×224\n224×224\n224×224\n224×224\n\n10,000\n10,000\n10,000\n8,000\n10,000\n50,000\n5,794\n3,333\n95,986\n24,426\n41,000\n\n10\n100\n10\n10\n200\n1000\n200\n100\n5089\n8142\n205\n\nlink\nlink\nlink\nlink\nlink\nlink\nlink\nlink\nlink\nlink\nlink\n\nstandardized procedure, which can also support the prevailing ViT architecture. ImageNet-1K (IN-\n1K) (Russakovsky et al., 2015) is a well-known challenging dataset for image classification with\n1000 classes. (3) Fine-grained classification: To investigate the effectiveness of mixup methods in\ncomplex inter-class relationships and long-tail scenarios, we conduct a comprehensive evaluation\nof fine-grained classification datasets, which can also be classified into small-scale and large-scale\nscenarios. (i) Small-scale scenarios: The datasets for small-scale fine-grained evaluation scenario are\nCUB-200-2011 (CUB) (Wah et al., 2011) and FGVC-Aircraft (Aircraft) (Maji et al., 2013), which\ncontains a total of 200 wild bird species and 100 classes of airplanes. (ii) Large-scale scenarios: The\ndatasets for large-scale fine-grained evaluation scenarios are iNaturalist2017 (iNat2017) (Horn et al.,\n2018) and iNaturalist2018 (iNat2018) (Horn et al., 2018), which contain 5,089 and 8,142 natural\ncategories. Both the iNat2017 and iNat2018 own 7 major categories and are also long-tail datasets\nwith scenic images (i.e., the fore-ground target is within large backgrounds). (4) Scenic classification:\nScenic classification evaluations are also conducted to investigate the performance of different mixup\naugmentation methods in complex non-iconic scenarios on Places205 (Zhou et al., 2014).\n\n3.3 EVALUATION METRICS AND TOOLS\n\nWe comprehensively evaluate the beneficial properties of mixup augmentation algorithms on the\naforementioned vision tasks through the use of various metrics and visualization analysis tools in a\nrigorous manner. Overall, the evaluation methodologies can be classified into two distinct divisions,\nnamely performance metric and empirical analysis. For the performance metrics, classification\naccuracy and robustness against corruption are two performance indicators examined. As for empirical\nanalysis, experiments on calibrations, CAM visualization, loss landscape, the plotting of training loss,\nand validation accuracy curves are conducted. The utilization of these approaches is contingent upon\ntheir distinct properties, enabling user-friendly deployment for designated purposes and demands.\n\nPerformance Metric.\n(1) Accuracy and training costs: We adopt top-1 accuracy, total training\nhours, and GPU memory to evaluate all mixup methods’ classification performance and training\ncosts. (2) Robustness: We evaluate the robustness against corruptions of the methods on CIFAR-\n100-C and ImageNet-C (Russakovsky et al., 2015), which is designed for evaluating the corruption\nrobustness and provides 19 different corruptions, e.g., noise and blur etc. (3) Transferability to\ndownstream tasks: We evaluate the transferability of existing methods to object detection based on\nFaster R-CNN (Ren et al., 2015) and Mask R-CNN (He et al., 2017) on COCO train2017 (Lin et al.,\n2014), initializing with trained models on ImageNet. We also transfer these methods to semantic\nsegmentation on ADE20K (Zhou et al., 2018). Please refer to Appendix B.4 for details.\n\nEmpirical Analysis.\n(1) Calibrations: To verify the calibration of existing methods, we evaluate\nthem by the expected calibration error (ECE) on CIFAR-100 (Krizhevsky et al., 2009), i.e., the\nabsolute discrepancy between accuracy and confidence. (2) CAM visualization: We utilize mixed\nsample visualization, a series of CAM variants (Chattopadhyay et al., 2018; Muhammad & Yeasin,\n2020) (e.g., Grad-CAM (Selvaraju et al., 2019)) to directly analyze the classification accuracy\nand especially the localization capabilities of mixup augmentation algorithms through top-1 top-2\naccuracy predicted targets. (3) Loss landscape: We apply loss landscape evaluation (Li et al., 2018)\nto further analyze the degree of loss smoothness of different mixup augmentation methods. (4)\nTraining loss and accuracy curve: We plot the training losses and validation accuracy curves\nof various mixup methods to analyze the training stability, the ability to prevent over-fitting, and\nconvergence speed. (5) Quality metric of learned weights: Employing WeightWatch (Martin\net al., 2021), we plot the Power Law (PL) exponent alpha metric of learned parameters with mixup\nalgorithms to study their properties on different scenarios, e.g., acting as the regularizer to prevent\noverfitting or expanding more data as the augmentation technique to learn better representations.\n\n6\n\n\fUnder review as a conference paper at ICLR 2025\n\n324\n325\n\n326\n327\n328\n329\n330\n331\n\n332\n333\n334\n335\n336\n337\n\n338\n339\n340\n341\n342\n\n343\n344\n345\n346\n347\n348\n\n349\n350\n351\n352\n353\n354\n\n355\n356\n357\n358\n359\n360\n\n361\n362\n363\n364\n365\n\n366\n367\n368\n369\n370\n371\n\n372\n373\n374\n375\n376\n377\n\nTable 3: Top-1 accuracy (%) on CIFAR-\n10/100 and Tiny-ImageNet (Tiny) based\non ResNet (R), Wide-ResNet (WRN),\nand ResNeXt (RX) backbones.\nDatasets\nBackbones\nEpochs\nVanilla\nMixup\nCutMix\nManifoldMix\nSmoothMix\nAttentiveMix\nSaliencyMix\nFMix\nGridMix\nResizeMix\nPuzzleMix\nCo-Mixup\nAlignMix\nAutoMix\nSAMix\nAdAutoMix\nDecoupled\n\nCIFAR-10 CIFAR-100 Tiny\nWRN-28-8 RX-50\n400 ep\n65.04\n66.36\n66.47\n67.30\n68.61\n67.42\n66.55\n65.08\n69.12\n65.87\n67.83\n68.02\n68.74\n70.72\n72.18\n72.89\n67.46\n\nR-18\n800 ep\n95.50\n96.62\n96.68\n96.71\n96.17\n96.63\n96.20\n96.18\n96.56\n96.76\n97.10\n97.15\n97.05\n97.34\n97.50\n97.55\n96.95\n\n800 ep\n81.63\n82.82\n84.45\n83.24\n82.09\n84.34\n84.35\n84.21\n84.24\n84.87\n85.02\n85.05\n84.87\n85.18\n85.50\n85.32\n84.88\n\nTable 4: Top-1 accuracy (%) on ImageNet-1K using\nPyTorch-style, RSB A2/A3, and DeiT settings based on\nCNN and Transformer architectures, including ResNet (R),\nMobileNet.V2 (Mob.V2), DeiT-S, and Swin-T.\n\nBackbones\nEpochs\nSettings\nVanilla\nMixup\nCutMix\nDeiT / RSB\nManifoldMix\nAttentiveMix\nSaliencyMix\nFMix\nResizeMix\nPuzzleMix\nAutoMix\nSAMix\nAdAutoMix\nTransMix\nSMMix\n\nR-50 Mob.V2 1x DeiT-S Swin-T\nR-50\n300 ep 300 ep\n100 ep\n100 ep\nDeiT\nDeiT\nPyTorch RSB A3\n80.21\n75.66\n81.01\n77.72\n81.23\n80.13\n81.20\n79.80\n81.15\n78.03\n81.29\n80.32\n81.37\n79.88\n81.47\n80.45\n81.36\n78.61\n79.60\n77.37\n81.80\n80.78\n81.87\n80.94\n81.75\n80.81\n81.80\n80.68\n81.10\n81.80\n\n300 ep\nRSB A2\n71.05\n72.78\n72.23\n72.87\n72.34\n70.30\n72.07\n72.79\n72.50\n72.85\n73.19\n73.42\n-\n-\n-\n\n77.27\n77.66\n77.62\n78.08\n77.78\n77.46\n77.93\n77.76\n77.85\n78.02\n78.44\n78.64\n78.54\n-\n-\n\n76.83\n77.12\n77.17\n77.35\n77.01\n77.28\n77.14\n77.19\n77.42\n77.54\n77.91\n78.06\n78.04\n-\n-\n\n3.4 EXPERIMENTAL PIPELINE OF OPENMIXUP CODEBASE\n\nOpenMixup provides a unified training pipeline that offers a consistent workflow across various\ncomputer vision tasks, as illustrated in Figure A1. Taking image classification as an example,\nwe can outline the overall training process as follows. (i) Data preparation: Users first select the\nappropriate dataset and pre-processing techniques from our supported data pipeline. (ii) Model\narchitecture: The openmixup.models module serves as a component library for building desired\nmodel architectures. (iii) Configuration: Users can easily customize their experimental settings\nusing Python configuration files under .configs.classification. These files allow for the\nspecification of datasets, mixup strategies, neural networks, and schedulers. (iv) Execution: The\n.tools directory not only provides hardware support for distributed training but offers utility\nfunctionalities, such as feature visualization, model analysis, and result summarization, which can\nfurther facilitate empirical analysis. We also provide comprehensive online user documents, including\ndetailed guidelines for installation and getting started instructions, all the benchmarking results, and\nawesome lists of related works in mixup augmentation, etc., which ensures that both researchers and\npractitioners in the community can effectively leverage our OpenMixup for their specific needs.\n\n4 EXPERIMENT AND ANALYSIS\n\n4.1\n\nIMPLEMENTATION DETAILS\n\nWe conduct essential benchmarking experiments of image classification on various scenarios with\ndiverse evaluation metrics. For a fair comparison, grid search is performed for the shared hyper-\nparameter α ∈ {0.1, 0.2, 0.5, 1, 2, 4} of supported mixup variants while the rest of the hyper-\nparameters follow the original papers. Vanilla denotes the classification baseline without any mixup\naugmentations. All experiments are conducted on Ubuntu workstations with Tesla V100 or NVIDIA\nA100 GPUs and report the mean results of three trials. Appendix B provides full visual classifica-\ntion results, Appendix B.4 presents our transfer learning results for object detection and semantic\nsegmentation, and Appendix C conduct verification of the reproduction guarantee in OpenMixup.\n\nSmall-scale Benchmarks. We first provide standard mixup image classification benchmarks on\nfive small datasets with two settings. (a) The classical settings with the CIFAR version of ResNet\nvariants (He et al., 2016; Xie et al., 2017), i.e., replacing the 7 × 7 convolution and MaxPooling by\na 3 × 3 convolution. We use 32 × 32, 64 × 64, and 28 × 28 input resolutions for CIFAR-10/100,\nTiny-ImageNet, and FashionMNIST, while using the normal ResNet for STL-10. We train vision\nmodels for multiple epochs from the stretch with SGD optimizer and a batch size of 100, as shown in\nTable 3 and Appendix B.2. (b) The modern training settings following DeiT (Touvron et al., 2021)\non CIFAR-100, using 224 × 224 and 32 × 32 resolutions for Transformers (DeiT-S (Touvron et al.,\n2021) and Swin-T (Liu et al., 2021)) and ConvNeXt-T (Liu et al., 2022b) as shown in Table A7.\n\n7\n\n\fUnder review as a conference paper at ICLR 2025\n\n378\n379\n\n380\n381\n382\n383\n384\n385\n\n386\n387\n388\n389\n390\n391\n\n392\n393\n394\n395\n396\n\n397\n398\n399\n400\n401\n402\n\n403\n404\n405\n406\n407\n408\n\n409\n410\n411\n412\n413\n414\n\n415\n416\n417\n418\n419\n\n420\n421\n422\n423\n424\n425\n\n426\n427\n428\n429\n430\n431\n\n(a) DeiT-S on ImageNet-1K\n\n(b) DeiT-S on CIFAR-100\nFigure 4: Trade-off evaluation with respect to accuracy performance, total training time (hours), and\nGPU memory (G). The results in (a) are based on DeiT-S architecture on ImageNet-1K. The results\nin (b) and (c) are based on DeiT-S and ConvNeXt-T backbones on CIFAR-100, respectively.\n\n(c) ConvNeXt-T on CIFAR-100\n\nTable 5: Rankings of various mixup augmentations as take-home messages for practical usage.\n\nMixup\n\n13\n1\n8\n\nCutMix\n\n11\n1\n6\n\nDeiT\n\n5\n1\n1\n\nSmoothMix\n\nGridMix\n\nResizeMix\n\nManifoldMix\n\n16\n1\n11\n\n15\n1\n10\n\n8\n1\n4\n\n12\n1\n7\n\nF Mix\n\n14\n1\n9\n\nAttentiveMix\n\nSaliencyMix\n\nPuzzleMix\n\nAlignMix\n\nAutoMix\n\nSA Mix\n\nTransMix\n\n7\n3\n5\n\n9\n1\n5\n\n6\n4\n5\n\n10\n2\n6\n\n2\n7\n4\n\n1\n6\n2\n\n4\n5\n4\n\nMix\n\nS M\n3\n5\n3\n\nPerformance\nApplicability\nOverall\n\nStandard ImageNet-1K Benchmarks. For visual augmentation and network architecture commu-\nnities, ImageNet-1K is a well-known standard dataset. We support three popular training recipes:\n(a) PyTorch-style (He et al., 2016) setting for classifical CNNs; (b) timm RSB A2/A3 (Wightman\net al., 2021) settings; (c) DeiT (Touvron et al., 2021) setting for ViT-based models. Evaluation is\nperformed on 224×224 resolutions with CenterCrop. Popular network architectures are consid-\nered: ResNet (He et al., 2016), Wide-ResNet (Zagoruyko & Komodakis, 2016), ResNeXt (Xie et al.,\n2017), MobileNet.V2 (Sandler et al., 2018), EfficientNet (Tan & Le, 2019), DeiT (Touvron et al.,\n2021), Swin (Liu et al., 2021), ConvNeXt (Liu et al., 2022b), and MogaNet (Li et al., 2024). Refer to\nAppendix A for implementation details. In Table 4 and Table A2, we report the mean performance of\nthree trials where the median of top-1 test accuracy in the last 10 epochs is recorded for each trial.\n\nBenchmarks on Fine-grained and Scenic Scenarios. We further provide benchmarking results on\nthree downstream classification scenarios in 224×224 resolutions with ResNet backbone architectures:\n(a) Transfer learning on CUB-200 and FGVC-Aircraft. (b) Fine-grained classification on iNat2017\nand iNat2018. (c) Scenic classification on Places205, as illustrated in Appendix B.3 and Table A10.\n\n4.2 OBSERVATIONS AND INSIGHTS\n\nEmpirical analysis is conducted to gain insightful observations and a systematic understanding of the\nproperties of different mixup augmentation techniques. Our key findings are summarized as follows:\n\n(A) Which mixup method should I choose? Integrating benchmarking results from various perspec-\ntives, we provide practical mixup rankings (detailed in Appendix B.5) as a take-home message for\nreal-world applications, which regards performance, applicability, and overall capacity. As shown in\nTable 1, as for the performance, the online-optimizable SAMix and AutoMix stand out as the top two\nchoices. SMMix and TransMix follow closely behind. However, regarding applicability that involves\nboth the concerns of efficiency and versatility, hand-crafted methods significantly outperform the\nlearning-based ones. Overall, the DeiT (Mixup+CutMix), SAMix, and SMMix are selected as the\nthree most preferable mixup methods, each with its own emphasis. Table 5 shows ranking results.\n\n(B) Generalization over datasets. The intuitive performance radar chart presented in Figure 1,\ncombined with the trade-off results in Figure 4, reveals that dynamic mixup methods consistently yield\nbetter performance compared to static ones, showcasing their impressive generalizability. However,\ndynamic approaches necessitate meticulous tuning, which incurs considerable training costs. In\ncontrast, static mixup exhibits significant performance fluctuation across different datasets, indicating\npoor generalizability with application scenarios. For instance, Mixup and CutMix as the static\nrepresentatives perform even worse than the baseline on Place205 and FGVC-Aircraft, respectively.\nMoreover, we analyze how mixup methods improve on different datasets in Figure 6 and Figure A4.\nOn small-scale datasets, mixup methods (dynamic ones) tend to prevent the over-parameterized\n\n8\n\n\u0000\u0017\u0000\u0013\u0000\u0019\u0000\u0013\u0000\u001b\u0000\u0013\u0000\u0014\u0000\u0013\u0000\u0013TotalTrainingHours(h)\u0000\u001a\u0000\u0018\u0000\u0011\u0000\u0013\u0000\u001a\u0000\u0019\u0000\u0011\u0000\u0013\u0000\u001a\u0000\u001a\u0000\u0011\u0000\u0013\u0000\u001a\u0000\u001b\u0000\u0011\u0000\u0013\u0000\u001a\u0000\u001c\u0000\u0011\u0000\u0013\u0000\u001b\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u001b\u0000\u0014\u0000\u0011\u0000\u0013Top1Accuracy(%)14G15G14G27GPerformancevs.Timevs.GPUMemory\u00009\u0000D\u0000Q\u0000L\u0000O\u0000O\u0000D\u00000\u0000L\u0000[\u0000X\u0000S\u0000\u000e\u0000&\u0000X\u0000W\u00000\u0000L\u0000[\u00000\u0000L\u0000[\u0000X\u0000S\u0000&\u0000X\u0000W\u00000\u0000L\u0000[\u0000$\u0000W\u0000W\u0000H\u0000Q\u0000W\u0000L\u0000Y\u0000H\u00000\u0000L\u0000[\u0000\u000e\u00006\u0000D\u0000O\u0000L\u0000H\u0000Q\u0000F\u0000\\\u00000\u0000L\u0000[\u00003\u0000X\u0000]\u0000]\u0000O\u0000H\u00000\u0000L\u0000[\u0000)\u00000\u0000L\u0000[\u00005\u0000H\u0000V\u0000L\u0000]\u0000H\u00000\u0000L\u0000[\u00007\u0000U\u0000D\u0000Q\u0000V\u00000\u0000L\u0000[\u00006\u00000\u00000\u0000L\u0000[\u0000$\u0000X\u0000W\u0000R\u00000\u0000L\u0000[\u00006\u0000$\u00000\u0000L\u0000[\u0000\u0016\u0000\u0013\u0000\u0017\u0000\u0013\u0000\u0018\u0000\u0013\u0000\u0019\u0000\u0013(d)TotalTrainingHours(h)\u0000\u0019\u0000\u001b\u0000\u0011\u0000\u0013\u0000\u001a\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u001a\u0000\u0015\u0000\u0011\u0000\u0013\u0000\u001a\u0000\u0017\u0000\u0011\u0000\u0013\u0000\u001a\u0000\u0019\u0000\u0011\u0000\u0013\u0000\u001a\u0000\u001b\u0000\u0011\u0000\u0013\u0000\u001b\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u001b\u0000\u0015\u0000\u0011\u0000\u0013Top1Accuracy(%)8.1G8.3G8.3G18.2G21.3G23.2GPerformancevs.Timevs.GPUMemory\u00009\u0000D\u0000Q\u0000L\u0000O\u0000O\u0000D\u00000\u0000L\u0000[\u0000X\u0000S\u0000&\u0000X\u0000W\u00000\u0000L\u0000[\u00000\u0000L\u0000[\u0000X\u0000S\u0000\u000e\u0000&\u0000X\u0000W\u00000\u0000L\u0000[\u00006\u0000P\u0000R\u0000R\u0000W\u0000K\u00000\u0000L\u0000[\u00006\u0000D\u0000O\u0000L\u0000H\u0000Q\u0000F\u0000\\\u00000\u0000L\u0000[\u0000$\u0000W\u0000W\u0000H\u0000Q\u0000W\u0000L\u0000Y\u0000H\u00000\u0000L\u0000[\u0000\u000e\u0000)\u00000\u0000L\u0000[\u0000*\u0000U\u0000L\u0000G\u00000\u0000L\u0000[\u00003\u0000X\u0000]\u0000]\u0000O\u0000H\u00000\u0000L\u0000[\u00005\u0000H\u0000V\u0000L\u0000]\u0000H\u00000\u0000L\u0000[\u00007\u0000U\u0000D\u0000Q\u0000V\u00000\u0000L\u0000[\u00006\u00000\u00000\u0000L\u0000[\u0000$\u0000X\u0000W\u0000R\u00000\u0000L\u0000[\u00006\u0000$\u00000\u0000L\u0000[\u0000$\u0000G\u0000$\u0000X\u0000W\u0000R\u00000\u0000L\u0000[\u0000\u0014\u0000\u0013\u0000\u0015\u0000\u0013\u0000\u0016\u0000\u0013\u0000\u0017\u0000\u0013\u0000\u0018\u0000\u0013\u0000\u0019\u0000\u0013TotalTrainingHours(h)\u0000\u001b\u0000\u0013\u0000\u0011\u0000\u0013\u0000\u001b\u0000\u0014\u0000\u0011\u0000\u0013\u0000\u001b\u0000\u0015\u0000\u0011\u0000\u0013\u0000\u001b\u0000\u0016\u0000\u0011\u0000\u0013\u0000\u001b\u0000\u0017\u0000\u0011\u0000\u0013\u0000\u001b\u0000\u0018\u0000\u0011\u0000\u0013Top1Accuracy(%)4.2G4.3G4.2G10.3G15.1GPerformancevs.Timevs.GPUMemory\u00009\u0000D\u0000Q\u0000L\u0000O\u0000O\u0000D\u00000\u0000L\u0000[\u0000X\u0000S\u0000&\u0000X\u0000W\u00000\u0000L\u0000[\u00000\u0000L\u0000[\u0000X\u0000S\u0000\u000e\u0000&\u0000X\u0000W\u00000\u0000L\u0000[\u0000'\u0000H\u0000F\u0000R\u0000X\u0000S\u0000O\u0000H\u00000\u0000L\u0000[\u00000\u0000D\u0000Q\u0000L\u0000I\u0000R\u0000O\u0000G\u00000\u0000L\u0000[\u00006\u0000P\u0000R\u0000R\u0000W\u0000K\u00000\u0000L\u0000[\u00006\u0000D\u0000O\u0000L\u0000H\u0000Q\u0000F\u0000\\\u00000\u0000L\u0000[\u0000$\u0000W\u0000W\u0000H\u0000Q\u0000W\u0000L\u0000Y\u0000H\u00000\u0000L\u0000[\u0000\u000e\u0000)\u00000\u0000L\u0000[\u0000*\u0000U\u0000L\u0000G\u00000\u0000L\u0000[\u00003\u0000X\u0000]\u0000]\u0000O\u0000H\u00000\u0000L\u0000[\u00005\u0000H\u0000V\u0000L\u0000]\u0000H\u00000\u0000L\u0000[\u0000$\u0000O\u0000L\u0000J\u0000Q\u00000\u0000L\u0000[\u0000$\u0000X\u0000W\u0000R\u00000\u0000L\u0000[\u00006\u0000$\u00000\u0000L\u0000[\u0000$\u0000G\u0000$\u0000X\u0000W\u0000R\u00000\u0000L\u0000[\fUnder review as a conference paper at ICLR 2025\n\n(a) DeiT-S on CIFAR-100\n\n(b) Swin-T on CIFAR-100\n\n(c) ResNet-50 on ImageNet\n\nFigure 5: (a)(b) Training epoch vs. top-1 accuracy (%) plots of different mixup methods on CIFAR-\n100 to analyze training stability and convergence speed. (c) 1-D loss landscapes for mixup methods\nwith ResNet-50 (300 epochs) on ImageNet-1K. The results show that dynamic approaches achieve\ndeeper and wider loss landscapes than static ones, which may indicate better optimization behavior.\n\n(a) DeiT-S on CIFAR-100\n\n(b) Swin-T on CIFAR-100\nFigure 6: Visualization of PL exponent alpha metrics (Martin et al., 2021) of learned models by\ndifferent mixup based on DeiT-S or Swin-T on (a)(b) CIFAR-100 and (c) ImageNet-1K. In each\nfigure, the bars are sorted with the top-1 accuracy from left to right. Holistically, the alpha metric\nmeasures the fitting degree of the learned model to a certain task. A smaller alpha indicates better\ntask fitting. Empirically, values less than 2 or larger than 6 run the risk of overfitting and underfitting.\nTherefore, this could serve as a favorable toolkit to evaluate the impact of different mixups on models.\n\n(c) DeiT-S on ImageNet-1K\n\nbackbones (Vanilla or with some static ones) from overfitting. On the contrary, mixup techniques are\nserved as data augmentations to encourage the model to fit hard tasks on large-scale datasets.\n\n(C) Generalization over backbones. As shown in Figure 4 and Figure 5(c), we provide extensive\nevaluations on ImageNet-1K based on different types of backbones and mixup methods. As a\nresult, dynamic mixup achieves better performance in general and shows more favorable generaliz-\nability against backbone selection compared to static methods. Noticeably, the online-optimizable\nSAMix and AutoMix exhibit impressive generalization ability over different vision backbones, which\npotentially reveals the superiority of their online training framework compared to the others.\n\n(D) Applicability. Figure A2 shows that ViT-specific methods (e.g., TransMix (Chen et al., 2022)\nand TokenMix (Liu et al., 2022a)) yield exceptional performance with DeiT-S and PVT-S yet exhibit\nintense sensitivity to different model scales (e.g., with PVT-T). Moreover, they are limited to ViTs,\nwhich largely restricts their applicability. Surprisingly, static Mixup (Zhang et al., 2018) exhibits\nfavorable applicability with new efficient networks like MogaNet (Li et al., 2024). CutMix (Yun\net al., 2019) fits well with popular backbones, such as modern CNNs (e.g., ConvNeXt and ResNeXt)\nand DeiT, which increases its applicability. As shown in Figure 4, although AutoMix and SAMix are\navailable in both CNNs and ViTs with consistent superiority, they have limitations in GPU memory\nand training time, which may limit their applicability in certain cases. This also provides a promising\navenue for reducing the cost of well-performed online learnable mixup augmentation algorithms.\n\n(E) Robustness & Calibration. We evaluate the robustness with accuracy on the corrupted version\nof CIFAR-100 and FGSM attack (Goodfellow et al., 2015) and the prediction calibration. Table A8\nshows that all the benchmarked methods can improve model robustness against corruptions. However,\nonly four recent dynamic approaches exhibit improved robustness compared to the baseline with\nFGSM attacks. We thus hypothesize that the online-optimizable mixup methods are robust against\nhuman interference, while the hand-crafted ones adapt to natural disruptions like corruption but are\nsusceptible to attacks. Overall, AutoMix and SAMix achieve the optimal robustness and calibration\nresults. For scenarios where these properties are required, practitioners can prioritize these methods.\n\n(F) Convergence & Training Stability. As shown in Figure 5, wider bump curves indicate smoother\nloss landscapes (e.g., Mixup), while higher warm color bump tips are associated with better conver-\n\n9\n\n432\n433\n\n434\n435\n436\n437\n438\n439\n\n440\n441\n442\n443\n444\n445\n\n446\n447\n448\n449\n450\n\n451\n452\n453\n454\n455\n456\n\n457\n458\n459\n460\n461\n462\n\n463\n464\n465\n466\n467\n468\n\n469\n470\n471\n472\n473\n\n474\n475\n476\n477\n478\n479\n\n480\n481\n482\n483\n484\n485\n\n255075100125150175200Epoch203040506070AccuracyEpoch vs AccuracyVanillaMixupCutMixMixup+CutMixSmoothMixAttentiveMix+SaliencyMixFMixGridMixPuzzleMixResizeMixTransMixAutoMixSAMix255075100125150175200Epoch20304050607080AccuracyEpoch vs AccuracyVanillaMixupCutMixMixup+CutMixSmoothMixAttentiveMix+SaliencyMixFMixGridMixPuzzleMixResizeMixAlignMixAutoMixSAMix0.40.20.00.20.4012345678LossMixupManifoldMixCutMixFMixResizeMixPuzzleMixAutoMix020406080100AccuracyMixupManifoldMixCutMixFMixResizeMixPuzzleMixAutoMixVanillaSmoothMixResizeMixGridMixSaliencyMixMixUpFMixPuzzleMixCutMixDeiTAttentiveMix+TransMixAutoMixSAMix1234Alpha3.162.883.953.373.633.273.673.653.223.543.654.084.444.5Alpha Values for Mixup Methods (Top-1 Accuracy Sorted)SmoothMixMixUpVanillaGridMixAlignMixResizeMixSaliencyMixCutMixFMixAtteniveMix+DeiTAutoMixSAMix123Alpha1.542.042.191.932.092.082.192.552.413.192.542.892.6Alpha Values for Mixup Methods (Top-1 Accuracy Sorted)VanillaFMixMixUpResizeMixDeiTSaliencyMixCutMixAttentiveMix+PuzzleMixTransMixAutoMixSAMixSMMix123456Alpha6.074.745.065.344.825.014.984.754.794.844.935.014.4Alpha Values for Mixup Methods (Top-1 Accuracy Sorted)\fUnder review as a conference paper at ICLR 2025\n\n486\n487\n\n488\n489\n490\n491\n492\n493\n\n494\n495\n496\n497\n498\n499\n\n500\n501\n502\n503\n504\n\n505\n506\n507\n508\n509\n510\n\n511\n512\n513\n514\n515\n516\n\n517\n518\n519\n520\n521\n522\n\n523\n524\n525\n526\n527\n\n528\n529\n530\n531\n532\n533\n\n534\n535\n536\n537\n538\n539\n\nFigure 7: Visualization of class activation mapping (CAM) (Selvaraju et al., 2019) for top-1 and top-2\npredicted classes of supported mixup methods with ResNet-50 on ImageNet-1K. Comparing the first\nand second rows, we observe that saliency-guided or dynamic mixup approaches (e.g., PuzzleMix\nand SAMix) localize the target regions better than the static methods (e.g., Mixup and ResizeMix).\n\ngence and performance (e.g., AutoMix). Evidently, dynamic mixup algorithms own better training\nstability and convergence than static mixup in general while obtaining sharp loss landscapes. They\nare likely to improve performances through exploring hard mixup samples. Nevertheless, the static\nmixup variants with convex interpolation, especially vanilla Mixup, exhibit smoother loss landscape\nand stable training than some static cutting-based methods. Based on the observations, we assume\nthis arises from its interpolation that prioritizes training stability but may lead to sub-optimal results.\n\n(G) Downstream Transferability & CAM Visualization. To further evaluate the downstream\nperformance and transferability of different mixup methods, we conduct transfer learning experiments\non object detection (Ren et al., 2015), semantic segmentation (Kirillov et al., 2019), and weakly\nsupervised object localization (Choe et al., 2020) with details in Appendix B.4. Notably, Table A11,\nTable A12, and Table A13 suggest that dynamic sampling mixing methods like AutoMix indeed\nexhibit competitive results, while recently proposed ViT-specific label mixing methods like TransMix\nperform even better, showcasing their superior transferability. The results also show the potential for\nimproved online training mixup design. Moreover, it is commonly conjectured that vision models\nwith better CAM localization could potentially be better transferred to fine-grained downstream\nprediction tasks. As such, to gain intuitive insights, we also provide tools for class activation mapping\n(CAM) visualization with predicted classes in our codebase. As shown in Figure 7 and Table A13,\ndynamic mixup like SAMix and AutoMix shows exceptional CAM localization, combined with\ntheir aforementioned accuracy, generalization ability, and robustness, may indicate their practical\nsuperiority compared to the static ones in object detection and even borader downstream tasks.\n\n5 CONCLUSION AND DISCUSSION\n\nContributions. This paper presents OpenMixup, the first comprehensive mixup augmentation\nbenchmark and open-source codebase for visual representation learning, where 18 mixup algorithms\nare trained and thoroughly evaluated on 11 diverse vision datasets. The released codebase not only\nbolsters the entire benchmark but can facilitate broader under-explored mixup applications and\ndownstream tasks. Furthermore, observations and insights are obtained through different aspects of\nempirical analysis that are previously under-explored, such as GPU memory usage, loss landscapes,\nPL exponent alpha metrics, and more, contributing to a deeper and more systematic comprehension\nof mixup augmentation. We anticipate that our OpenMixup benchmark and codebase can further\ncontribute to fair and reproducible mixup research and we also encourage researchers and practitioners\nin the community to extend their valuable feedback to us and contribute to OpenMixup for building a\nmore constructive mixup-based visual representation learning codebase together through GitHub.\n\nLimitations and Future Works. The benchmarking scope of this work mainly focuses on visual\nclassification, albeit we have supported a broader range of tasks in the proposed codebase and have\nconducted transfer learning experiments to object detection and semantic segmentation tasks to draw\npreliminary conclusions. We are aware of this and have prepared it upfront for future work. For\nexample, our codebase can be easily extended to other computer vision tasks and datasets for further\nmixup benchmarking experiments and evaluations if necessary. Moreover, our observations and\ninsights can also be of great value to the community for a more comprehensive understanding of\nmixup augmentation techniques. We believe this work as the first mixup benchmarking study is\nenough to serve as a kick-start, and we plan to extend our work in these directions in the future.\n\n10\n\nAttentiveMixCutMixMixupGridMixFMixPuzzleMixAutoMixSAMixSaliencyMixResizeMix\fUnder review as a conference paper at ICLR 2025\n\n540\n541\n\n542\n543\n544\n545\n546\n547\n\n548\n549\n550\n551\n552\n553\n\n554\n555\n556\n557\n558\n\n559\n560\n561\n562\n563\n564\n\n565\n566\n567\n568\n569\n570\n\n571\n572\n573\n574\n575\n576\n\n577\n578\n579\n580\n581\n\n582\n583\n584\n585\n586\n587\n\n588\n589\n590\n591\n592\n593",
    "abstract": "Mixup augmentation has emerged as a widely used technique for improving the generalization ability of deep neural networks (DNNs). However, the lack of standardized implementations and benchmarks has impeded recent progress, resulting in poor reproducibility, unfair comparisons, and conflicting insights. In this paper, we introduce OpenMixup, the first mixup augmentation codebase and benchmark for visual representation learning. Specifically, we train 18 representative mixup baselines from scratch and rigorously evaluate them across 11 image datasets of varying scales and granularity, ranging from fine-grained scenarios to complex non-iconic scenes. We also open-source our modular codebase including a collection of popular vision backbones, optimization strategies, and analysis toolkits, which not only supports the benchmarking but enables broader mixup applications beyond classification, such as self-supervised learning and regression tasks. Through experiments and empirical analysis, we gain observations and insights on mixup performance-efficiency trade-offs, generalization, and optimization behaviors, and thereby identify preferred choices for different needs. To the best of our knowledge, OpenMixup has facilitated several recent studies. We believe this work can further advance reproducible mixup augmentation research and thereby lay a solid ground for future progress in the community. The source code will be publicly available.",
    "response": {
      "Soundness": 3,
      "Presentation": 3,
      "Contribution": 3,
      "Rating": 6,
      "Confidence": 2,
      "Strengths": "- This paper is clearly written and provides a good background of existing mixup methods.\n\n- It compares 18 different methods in 12 vision tasks trained from scratch in the same codebase. This is a significant and fair comparison with potential benefits to the community by bringing clear comparison between all the methods.\n\n- The code will be available, allowing others to try ideas on top of it.\n\n- It clearly passes comparison messages such as: dynamic policy performing better than static ones, a ranking of methods based on performance and how applicable they are (e.g. depend on specific architecture), compute and memory cost of the methods.",
      "Weaknesses": "- The paper spends a significant amount of space describing the OpenMixup software, it is indeed described as a main contribution of the paper, but overall software description and how its structure to be reused by others seems better left for software documentation and not as main contribution. In particular no significant design decisions seem novel to the mixup problem that deserves attention. (Which is good, software should not be forced to be novel and unexpected). Instead of that I would have preferred to see further exploration/guidance of when to use mixup.\n\n- Figure 4 is really hard to parse, sharing legend and using more space might make it better at conveying information.\n\n- Although the information is there, I found it hard to parse when/whether there is an absolute significant gain on going from vanilla->static->dynamic, that and some other general conclusions when it is worth to use it would be useful information for a practitioner.",
      "Questions": "- The comparisons in this work are when training from scratch, is mixup relevant for practitioners who initialise from pretrained networks trained on larger datasets than ImageNet-1k?\n\n- How do mixup benefits behave when the number of training examples gets reduced/increased?"
    }
  },
  {
    "paper_id": "eNjXcP6C0H",
    "summary": "Background: The study of physical dynamics is crucial for a variety of scientific and engineering applications, such as fluid dynamics, climate modeling, and materials science. These systems are governed by Partial Differential Equations (PDEs) and involve complex and high-dimensional data, making data-driven modeling methodologies, like generative models, highly valuable. Even though discriminative models (finite elements, finite difference, PINNs) effectively handle dynamics, generative models offer the advantage of capturing full data distributions, enhancing robustness and interpretability. Generative models, particularly diffusion-based ones, have shown potential in simulating high-dimensional data distributions and offer advantages for data synthesis, anomaly detection, and semi-supervised learning.\nGap: Current generative modeling techniques often overlook the integration of physical priors, which are essential for the produced dynamics to adhere to basic physical laws such as energy and momentum conservation. This oversight results in violations of these fundamental laws and leads to suboptimal performance in generating physically realistic dynamics.\nHypothesis: Integrating physical priors into diffusion-based generative models by leveraging distributional and physical feasibility priors, such as roto-translational invariance and basic conservation laws, could significantly improve the quality and physical realism of the generated dynamics. This new framework could advance data-driven studies in AI4Physics by producing more accurate and consistent dynamics with fundamental physical principles.\nMethod: The proposed framework embeds two categories of physical priors into diffusion-based generative models: Distributional priors (e.g., roto-translational invariance) and Physical feasibility priors (e.g., energy and momentum conservation laws, PDE constraints). The method redefines the generative process by incorporating these priors, handling complex mathematical and physical principles, and ensuring the generated dynamics adhere to fundamental physical laws. The framework uses noise matching and data matching objectives to approximate the targeted score functions, considering both linear and multilinear cases, Jensen's gap considerations, and convexity in constraining the output of the models, ensuring robust integration of the specified physical priors.\nConclusion: The novel framework successfully incorporates physical priors into diffusion-based generative models, producing high-quality, physically realistic dynamics. Empirical evaluations demonstrate the method's robustness and generalizability across various physical phenomena, indicating significant potential for future data-driven AI4Physics research. It highlights the importance of embedding domain knowledge into learning systems and bridges physics and machine learning by leveraging advances in diffusion models to ensure consistency with fundamental physical laws.\n",
    "full text": "\nGENERATING PHYSICAL DYNAMICS UNDER PRIORS\n\nZihan Zhou1, Xiaoxue Wang2, Tianshu Yu1∗\n1School of Data Science, The Chinese University of Hong Kong\n2ChemLex Technology Co., Ltd.\nzihanzhou1@link.cuhk.edu.cn, wxx@chemlex.tech\nyutianshu@cuhk.edu.cn\n\nABSTRACT\n\nGenerating physically feasible dynamics in a data-driven context is challenging,\nespecially when adhering to physical priors expressed in specific equations or\nformulas. Existing methodologies often overlook the integration of “physical pri-\nors”, resulting in violation of basic physical laws and suboptimal performance. In\nthis paper, we introduce a novel framework that seamlessly incorporates physi-\ncal priors into diffusion-based generative models to address this limitation. Our\napproach leverages two categories of priors: 1) distributional priors, such as roto-\ntranslational invariance, and 2) physical feasibility priors, including energy and\nmomentum conservation laws and PDE constraints. By embedding these priors\ninto the generative process, our method can efficiently generate physically realis-\ntic dynamics, encompassing trajectories and flows. Empirical evaluations demon-\nstrate that our method produces high-quality dynamics across a diverse array of\nphysical phenomena with remarkable robustness, underscoring its potential to ad-\nvance data-driven studies in AI4Physics. Our contributions signify a substantial\nadvancement in the field of generative modeling, offering a robust solution to gen-\nerate accurate and physically consistent dynamics.\n\n1\n\nINTRODUCTION\n\nThe generation of physically feasible dynamics is a fundamental challenge in the realm of data-\ndriven modeling and AI4Physics. These dynamics, driven by Partial Differential Equations (PDEs),\nare ubiquitous in various scientific and engineering domains, including fluid dynamics (Kutz, 2017),\nclimate modeling (Rasp et al., 2018), and materials science (Choudhary et al., 2022). Accurately\ngenerating such dynamics is crucial for advancing our understanding and predictive capabilities in\nthese fields (Bzdok & Ioannidis, 2019). Recently, generative models have revolutionized the study\nof physics by providing powerful tools to simulate and predict complex systems.\n\nGenerative v.s. discriminative models. Even when high-performing discriminative models for\ndynamics are available such as finite elements (Zhang et al., 2021; Uriarte et al., 2022), finite differ-\nence (Lu et al., 2021; Salman et al., 2022), finite volume (Ranade et al., 2021) or physics-informed\nneural networks (PINNs) (Raissi et al., 2019), generative models are crucial in machine learning for\ntheir ability to capture the full data distribution, enabling more effective data synthesis (de Oliveira\net al., 2017), anomaly detection (Finke et al., 2021), and semi-supervised learning (Ma et al., 2019).\nThey enhance robustness and interpretability by modeling the joint distribution of data and labels,\noffering insights into unseen scenarios (Takeishi & Kalousis, 2021). Generative models are also piv-\notal in creative domains, such as drug discovery (Lavecchia, 2019), where they enable the creation\nof novel data samples.\n\nChallenge. However, the intrinsic complexity and high-dimensional nature of physical dynamics\npose significant challenges for traditional learning systems. Recent advancements in generative\nmodeling, particularly diffusion-based generative models (Song et al., 2020), have shown promise\nin capturing complex data distributions. These models iteratively refine noisy samples to match\nthe target distribution, making them well-suited for high-dimensional data generation. Despite their\nsuccess, existing approaches often overlook the incorporation of “physical priors” expressed in spe-\n\n∗corresponding author\n\n1\n\n\fPublished as a conference paper at ICLR 2025\n\ncific equations or formulas, which are essential for ensuring that the generated dynamics adhere to\nfundamental physical laws.\n\nSolution. In this work, we propose a novel framework that integrates priors into diffusion-based\ngenerative models to generate physically feasible dynamics. Our approach leverages two types of\npriors: Distributional priors, including roto-translational invariance and equivariance, ensure that\nmodels capture the intrinsic properties of the data rather than their specific representations; Physical\nfeasibility priors, including energy and momentum conservation laws and PDE constraints, enforce\nthe adherence to fundamental physical principles, thus improving the quality of generated dynamics.\n\nThe integration of priors into the generative process is a\ncomplex task that necessitates a deep understanding of the\nrelevant mathematical and physical principles. Unlike pre-\ndictive tasks, where the objective is to estimate a specific\nground-truth value x0, diffusion generative models aim to\ncharacterize a full ground-truth distribution ∇x log qt(xt)\nor E[x0 | xt] (notation in Equation 1). This fundamental\ndifference complicates the direct application of priors based\non ground-truth values to the output of generative models.\nIn this work, we propose a framework to address this chal-\nlenge by effectively embedding priors within the generative\nmodel’s output distribution. By incorporating these priors\ninto a diffusion-based generation framework, our approach\ncan efficiently produce physically plausible dynamics. This\ncapability is particularly useful for studying physical phe-\nnomena where the governing equations are too complex to\nbe learned purely from data.\n\nFigure 1: Animated visualization of\ngenerated samples of shallow water\ndynamics, showcasing the variations\nover time. Use the latest version of\nAdobe Acrobat Reader to view.\n\nResults. Empirical evaluations of our method demonstrate its effectiveness in producing high-\nquality dynamics across a range of physical phenomena. Our approach exhibits high robustness\nand generalizability, making it a promising tool for the data-driven study of AI4Physics. In Fig. 1,\nwe provide a generated sample of the shallow water dataset (Mart´ınez-Aranda et al., 2018). The gen-\nerated dynamics not only capture the intricate details of the physical processes but also adhere to the\nfundamental physical laws, offering an accurate and reliable representation of underlying systems.\n\nContribution. In conclusion, our work presents a significant advancement in the field of data-driven\ngenerative modeling by introducing a novel framework that integrates physical priors into diffusion-\nbased generative models. In all, our method 1) improves the feasibility of generated dynamics, mak-\ning them more aligned with physical principles compared to baseline methods; 2) poses the solution\nto the longstanding challenge of generating physically feasible dynamics; 3) paves the way for more\naccurate and reliable data-driven studies in various scientific and engineering domains, highlighting\nthe potential of AI4Physics in advancing our understanding of complex physical systems.\n\n2 PRELIMINARIES\n\nIn Appendix A, we present a comprehensive review of Related Work, specifically focusing on three\nkey areas: generative methods for physics, score-based diffusion models, and physics-informed\nneural networks. This section aims to provide foundational knowledge for readers who may not be\nfamiliar with these topics. We recommend that those seeking to deepen their understanding of these\nareas consult this appendix.\n\n2.1 DIFFUSION MODELS\n\nDiffusion models generate samples following an underlying distribution. Consider a random vari-\nable x0 ∈ Rn drawn from an unknown distribution q0. Denoising diffusion probabilistic mod-\nels (Song & Ermon, 2019; Song et al., 2020; Ho et al., 2020) describe a forward process xt, t ∈ [0, T ]\ngoverned by an Ito stochastic differential equation (SDE)\n\ndxt = f (t)xtdt + g(t)dwt, x0 ∼ q0,\n\nf (t) =\n\nd log αt\ndt\n\n, g2(t) =\n\ndσ2\nt\ndt\n\n− 2\n\nd log αt\ndt\n\nσ2\nt ,\n\n(1)\n\n2\n\n\fPublished as a conference paper at ICLR 2025\n\nwhere wt ∈ Rn denotes the standard Brownian motion, and αt and σt are predetermined functions\nof t. This forward process has a closed-form solution of qt (xt | x0) = N (cid:0)xt | αtx0, σ2\nt I(cid:1) and\nhas a corresponding reverse process of the probability flow ordinary differential equation (ODE),\nrunning from time T to 0, defined as (Song et al., 2020)\n\ndxt\ndt\n\n= f (t)xt −\n\n1\n2\n\ng2(t)∇x log qt(xt), xT ∼ qT (xT | x0) ≈ qT (xT ).\n\n(2)\n\nThe marginal probability densities {qt(xt)}T\nt=0 of the forward SDE align with the reverse\nODE (Song et al., 2020). This indicates that if we can sample from qT (xT ) and solve Equation 2,\nthen the resulting x0 will follow the distribution q0. By choosing αt → 0 and σt → 1, the dis-\ntribution qT (xT ) can be approximated as a normal distribution. The score ∇x log qt(xt) can be\napproximated by a deep learning model. The quality of the generated samples is contingent upon\nthe models’ ability to accurately approximate the score functions (Kwon et al., 2022; Gao & Zhu,\n2024). A more precise approximation results in a distribution that closely aligns with the distribu-\ntion of the training set. To enhance model fit, incorporating priors of the distributions and physical\nfeasibility into the models is advisable. Section 3 will elaborate on our methods for integrating\ndistributional priors and physical feasibility priors, as well as the objectives for score matching.\n\n2.2\n\nINVARIANT DISTRIBUTIONS\n\nAn invariant distribution refers to a probability distribution that remains unchanged under the action\nof a specified group of transformations. These transformations can include operations such as trans-\nlations, rotations, or other symmetries, depending on the problem domain. Formally, let G be a group\nof transformations. A distribution q is said to be G-invariant under the group G if for all transfor-\nmations G ∈ G, we have q(G(x)) = q(x). Invariance under group transformations is particularly\nsignificant in modeling distributions that exhibit symmetries. For instance, in the case of 3D coordi-\nnates, invariance under rigid transformations—such as translations and rotations (SE(3) group)—is\nessential for spatial understanding (Zhou et al., 2024). Equivariant models are usually required to\nembed invariance. A function (or model) f : Rn → Rn is said to be (G, L)-equivariant where G is\nthe group actions and L is a function operator, if for any G ∈ G, f (G(x)) = L(G)(f (x)).\n\n3 METHOD\n\nIn this study, we aim to investigate methodologies for enhancing the capability of diffusion models\nto approximate the targeted score functions. We have two primary objectives: 1) To incorporate\ndistributional priors, such as translational and rotational invariance, which will aid in selecting the\nappropriate model for training objective functions; 2) To impose physical feasibility priors on the\ndiffusion model, necessitating injection of priors to model’s output of a distribution related to the\nground-truth samples (specifically, ∇x log qt(xt) or E[x0 | xt]). In this section, we consider the\nforward diffusion process given by Equation 1, where xt = αtx0 + σtϵ, with ϵ ∼ N (0, I).\n\n3.1\n\nINCORPORATING DISTRIBUTIONAL PRIORS\n\nIn this section, we study the score function ∇x log qt(xt) for G-invariant distributions. Understand-\ning its corresponding properties can guide the selection of models with the desired equivariance,\nfacilitating sampling from the G-invariant distribution. In the following, we will assume that the\nsufficient conditions of Theorem 1 hold so that the marginal distributions qt are G-invariant. The\ndefinitions of the terminologies and proof of the theorem can be found in Appendix F.1.\nTheorem 1 (Sufficient conditions for the invariance of q0 to imply the invariance of qt). Let q0 be\na G-invariant distribution. If for all G ∈ G, G is volume-preserving diffeomorphism and isometry,\nand for all 0 < a < 1, there exists H ∈ G such that H(ax) = aG(x), then qt is also G-invariant.\n\nProperty of score functions. Let qt be a G-invariant distribution. By the chain rule, we have\n∇x log qt(xt) = ∇x log qt(G(xt)) = ∂G(xt)\n\n∂x ∇G(x) log qt(G(xt)), for all G ∈ G. Hence,\n\n∇G(x) log qt(G(xt)) =\n\n(cid:19)−1\n\n(cid:18) ∂G(xt)\n∂x\n\n∇x log qt(xt).\n\n(3)\n\n3\n\n\fPublished as a conference paper at ICLR 2025\n\nThis implies that the score function of G-invariant distribution is (G, ∇−1)-equivariant. We should\nuse a (G, ∇−1)-equivariant model to predict the score function. The loss objective is given by\n\nJscore (θ) = Et,x0,ϵ\n\nw(t) ∥sθ (xt, t) − ∇x log qt(xt)∥2(cid:105)\n(cid:104)\n\n,\n\n(4)\n\nwhere w(t) is a positive weight function and sθ is a (G, ∇−1)-equivariant model. We will discuss\nthe handling of the intractable score function ∇x log qt(xt) subsequently in Equation 6.\n\nIn the context of simulating physical dynamics, two distributional priors are commonly considered:\nSE(n)-invariance and permutation-invariance. They ensure that the learned representations are con-\nsistent with the fundamental symmetries of physical laws, including rigid body transformations and\nindistinguishability of particles, thereby enhancing the model’s ability to generalize across different\nphysical scenarios. The derivations for the following examples can be found in Appendix F.2.\nExample 1. (SE(n)-invariant distribution) If q0 is an SE(n)-invariant distribution, then qt is also\nSE(n)-invariant. The score function of an SE(n)-invariant distribution is SO(n)-equivariant and\ntranslational-invariant.\nExample 2. (Permutation-invariant distribution) If q0 is a permutation-invariant, then qt is also\npermutation-invariant. The score function of a permutation-invariant distribution is permutation-\nequivariant.\n\nIn the following, we will show that using such a (G, ∇−1)-equivariant model, we are essentially\ntraining a model that focuses on the intrinsic structure of data instead of their representation form.\n\nEquivalence class manifold for invariant distributions. An equivalence class manifold (ECM)\nrefers to the minimum subset of samples where all the rest elements are considered equivalent to\none of the samples in this manifold (informal). For example, in three-dimensional space, coordi-\nnates that have undergone rotation and translation maintain their pairwise distances, which allows\nthe use of a set of coordinates to represent all other coordinates with the same distance matrices,\nthereby forming an equivalence class manifold (see Appendix B for the formal definition and ex-\namples). By incorporating the invariance prior to the training set, we can construct ECM from the\ntraining set or a mini-batch of samples. The utilization of ECM enables the models to concentrate\non the intrinsic structure of the data, thereby enhancing generalization and robustness to irrelevant\nvariations. We assume that the distribution of x follows an G-invariant distribution qt. Let φ map\nxt to the corresponding point having the same intrinsic structure in ECM. Then there exists G ∈ G\nsuch that G(φ(xt)) = xt . Since qt is G-invariant, we have qt(xt) = qECM (φ(xt))·pUniform(G)(G),\nwhere qECM is defined on the domain of ECM. Taking the logarithm and derivative, we have\n∇φ(x) log qt(xt) = ∇φ(x) log qECM(φ(xt)). Note that ∇x log qt(xt) = ∂φ(xt)\n∂x ∇φ(x) log qt(xt).\nHence,\n\n∇x log qt(xt) =\n\n∂φ(xt)\n∂x\n\n∇φ(x) log qECM (φ(xt)).\n\n(5)\n\nThis implies that the score function of the G-invariant distribution is closely related to the score\nfunction of the distribution in ECM. Such a result indicates that if we have a (G, ∇−1)-equivariant\nmodel that can predict the score functions in ECM, then, this model predicts the score functions for\nall other points closed under the group operation. We summarize this result in the following theorem\nwhose proofs can be found in Appendix F.3.\nTheorem 2 (Equivalence class manifold representation). If we have a (G, ∇−1)-equivariant model\nsuch that sθ(xt, t) = ∇x log qECM (xt) almost surely on xt ∈ ECM, then we have sθ(xt, t) =\n∇x log qt(xt) almost surely.\n\nObjective for fitting the score function. The score function ∇x log qt(xt) is generally intractable\nand we consider the objective for noise matching and data matching (Vincent, 2011; Song et al.,\n2020; Zheng et al., 2023), where objectives and optimal values are given by\n\nJnoise (θ) = Et,x0,ϵ\n\nJdata(θ) = Et,x0,ϵ\n\nw(t) ∥ϵθ (xt, t) − ϵ∥2(cid:105)\n(cid:104)\n,\nw(t) ∥xθ (xt, t) − x0∥2(cid:105)\n(cid:104)\n\nϵ∗\nθ (xt, t) = −σt∇x log qt (xt) ;\n\n(6a)\n\n, x∗\n\nθ (xt, t) =\n\n1\nαt\n\nxt +\n\nσ2\nt\nαt\n\n∇x log qt (xt) .\n\n(6b)\n\n4\n\n\fPublished as a conference paper at ICLR 2025\n\nThe diffusion objectives for both the noise predictor ϵθ and the data predictor xθ are intrinsically\nlinked to the score function, thereby inheriting its characteristics and properties. However, the data\npredictor incorporates a term, 1\nxt, whose numerical range exhibits instability. This instability\nαt\ncomplicates the predictor’s ability to inherit the straightforward properties of the score function.\nTherefore, to incorporate G-invariance, it is advisable to employ noise matching, which is given by\nEquation 6a and ϵθ is (G, ∇−1)-equivariant, which is the property of the score function.\n\nA specific instance of a distributional prior is defined by samples that conform to the constraints\nimposed by PDEs. In this context, the dynamics at any given spatial location depend solely on the\ncharacteristics of the system within its local vicinity, rather than on absolute spatial coordinates. Un-\nder these conditions, it is appropriate to employ translation-invariant models for both noise matching\nand data matching. Nevertheless, the samples in question exhibit significant smoothness. As a re-\nsult, utilizing the noise matching objective necessitates that the model’s output be accurate at every\nindividual pixel. In contrast, applying the data matching objective only requires the model to pro-\nduce smooth output values. Therefore, it is recommended to adopt the data matching objective\nfor this purpose. The selection between data matching and noise matching plays a critical role in\ndetermining the quality of the generated samples. For detailed experimental results, refer to Sec. 4.3.\n\nRemark 1. In this section, we primarily explore the principle for incorporating distributional priors\nby selecting models with particular characteristics. Specifically:\n\n1. When the distribution exhibits G-invariance, a (G, ∇−1)-equivariant model should be em-\n\nployed alongside the noise matching objective (Equation 6a).\n\n2. For samples that are subject to PDE constraints and exhibit high smoothness, the data\n\nmatching objective (Equation 6b) is recommended.\n\n3.2\n\nINCORPORATING PHYSICAL FEASIBILITY PRIORS\n\nIn this section, we explore how to incorporate physical feasibility priors such as physics laws\nand explicit PDE constraints into noise and data matching objectives in diffusion models. By\nTweedie’s formula (Efron, 2011; Kim & Ye, 2021; Chung et al., 2022), we have E[x0 | xt] =\n1\nαt\n\nt ∇x log qt (xt)(cid:1). Hence,\n\n(cid:0)xt + σ2\n\nE[x0 | xt] =\n\n1\nαt\n\n(xt − σtϵ∗\n\nθ (xt, t)) , E[x0 | xt] = x∗\n\nθ (xt, t) .\n\n(7)\n\nFor both noise and data matching objectives, we are essentially training a model to approximate\nE[x0 | xt]. A purely data-driven approach is often insufficient to capture the underlying physical\nconstraints accurately. Therefore, similar to PINNs (Leiteritz & Pfl¨uger, 2021), we incorporate an\nadditional penalty loss JR into the objective function to enforce physical feasibility priors R (x0) =\n0 and set the loss objective to be J (θ) = Jscore (θ) + λJR(θ), where Jscore is the data matching\nor noise matching objectives and λ is a hyperparameter to balance the diffusion loss and physical\nfeasibility loss. We consider the data matching objective where xθ (xt, t) approximates E[x0 | xt].\nFor noise matching models, we can transform the model’s output by Equation 7. For general cases,\nwe cannot directly add the constraints R (x0) = 0 to the output of the diffusion model E[x0 | xt]\ndue to the presence of Jensen’s gap (Bastek et al., 2024), i.e., R (E[x0 | xt]) ̸= E[R (x0) | xt] = 0.\nHowever, in some special cases, we can avoid dealing with this gap.\n\nLinear cases. When the constraints are linear/affine functions, Jensen’s gap equals 0. Hence, we\nw(t) ∥R (xθ (xt, t))∥2(cid:105)\n(cid:104)\ncan directly add the constraints to xθ (xt, t). We have JR (θ) = Et,x0,ϵ\n.\n\nMultilinear cases. A function is called multilinear if it is linear in several arguments when the\n∈ Rm+n, u0 ∈ Rm, v0 ∈ Rn. When the constraints\n\nother arguments are fixed. Denote x0 =\n\n(cid:21)\n\n(cid:20)u0\nv0\n\nfunction is multilinear w.r.t. u0, we can write the constraints in the form of R (x0) = W0u0 +b0 =\n0, where W0 and b0 are functions of v0. In this case, we can use the penalty loss as JR (θ) =\nEt,x0,ϵ[w(t)∥W0uθ (xt, t) + b0∥2]. Such a design is supported by the following theorem whose\nproof can be found in Appendx F.4.\n\n5\n\n\fPublished as a conference paper at ICLR 2025\n\nTheorem 3 (Multilinear Jensen’s gap). The optimizer for Et,x0,ϵ[w(t)∥uθ1 (xt, t) − u0∥2] is the\nreweighted optimizer of Et,x0,ϵ[w(t)∥W0uθ2 (xt, t) + b0∥2] with reweighted variable W⊤\n\n0 W0.\n\nIf the constraints R is convex, by Jensen’s inequality, R (E[x0 | xt]) ≤ E[R (x0) |\nConvex cases.\nxt] = 0. Hence, 0 = ∥E[R (x0) | xt]∥2 ≤ ∥R (E[x0 | xt]) ∥2. When a data matching model is\napproximately optimized, directly applying constraints to the model’s output minimizes the up-\nper bound of the constraints on x0. The upper bound of the Jensen’s gap is related the absolute\ncentered moment σp = p(cid:112)E[|x0 − µ|xt|p], where µ = E[x0|xt]. If the constraints function R\napproach R(µ) no slower than |x0 − µ|η and grow as x0 → ±∞ no faster than ±|x0|n for\nn ≥ η, then the Jensen’s gap E[R (x0) | xt] − R (E[x0 | xt]) approaches to 0 no slower than\nση\nn as σn → 0 (Gao et al., 2017). Usually, in the reverse diffusion process, σn → 0 as t → 0\nsince the generated noisy samples converge to a clean one. In this case, we use the penalty loss of\nJR (θ) = Et,x0,ϵ\n\nw(t) ∥R (xθ (xt, t))∥2(cid:105)\n\n(cid:104)\n\n.\n\nIn the aforementioned three scenarios, at the implementation level, the model’s output may be\ndirectly considered as the ground-truth sample x0 itself, rather than the conditional expectation\nE[x0 | xt]. These scenarios are referred to as “elementary cases”. In the following, we will discuss\nhow to deal with nonlinear cases using the above elementary cases.\n\nReducible nonlinear cases. For nonlinear constraints, mathematically speaking, we cannot di-\nrectly apply the constraints to E[x0 | xt]. However, we may recursively use multilinear functions\nto decompose the nonlinear constraints into elementary ones as: R (x0) = g1 ◦ · · · gm (x0) = 0,\nwhere all gi are elementary. Using elementary functions for decomposition, we may 1) reduce non-\nlinear constraints into elementary ones by treating terms causing nonlinearity as constants, and 2)\nreduce the complex constraints into several simpler ones. In this case, the penalty loss is set to\nJR (θ) = Et,x0,ϵ\n. See Sec. 4.2 for concrete examples of nonlin-\near formulas for the conservation of energy.\n\nw(t) ∥g1 ◦ · · · gm (xθ (xt, t))∥2(cid:105)\n\n(cid:104)\n\nGeneral nonlinear cases. For general nonlinear cases, if it is not feasible to decompose the non-\nlinear constraints into their elementary components, it may be necessary to consider alternative\napproaches where we may reparameterize the constraints variable into elementary cases. Given\nthe nonlinear constraints, we reparameterize it as R (x0) = g (h(x0)) = 0, where g is elemen-\ntary and h is non-necessarily elementary functions. Subsequently, another diffusion model, de-\nnoted as ˜xθ (xt, t), is trained to predict h(x0), utilizing the same hidden states as model xθ (xt, t).\nThis training process employs the methods applicable to elementary cases. The objective is for\nmodel ˜xθ to learn the underlying physical constraints and encode these constraints into its hid-\nden states. Consequently, when model xθ predicts, it inherently incorporates the learned phys-\nical constraints g parameterized by h(x0). To train model ˜xθ, we set the penalty loss to be\nJR (θ) = Et,x0,ϵ\n\nw(t) ∥ ˜xθ (xt, t) − h(x0)∥2(cid:105)\n(cid:104)\n\n. See Appendix E.1 for implementation details.\n\nNotably, in our proposed methods for integrating constraints, the explicit form of prior knowledge,\nsuch as the physics constants required for energy calculations, is not necessary. Instead, it suffices\nto determine whether the model’s output parameters are elementary w.r.t.\nthe constraints. This\napproach enhances the applicability of our methods to a broader spectrum of constraints.\nRemark 2. In conclusion, incorporating the physics constraints can be achieved in different ways\ndepending on their complexity. For elementary constraints, one can directly omit Jensen’s gap and\nimpose the penalty loss on the model’s output. In the case of nonlinear constraints, decomposition\nor reparameterization techniques are utilized to transform constraints into elementary ones.\n\n4 EXPERIMENTS\n\nIn this section, we assess the enhancement achieved by incorporating physics constraints into the\nfundamental diffusion model across various synthetic physics datasets. We conduct a grid search\nto identify an equivalent set of suitable hyperparameters for the network to perform the data/noise\nmatching, ensuring a fair comparison between the baseline method (diffusion objectives without\npenalty loss) and our proposed approach of incorporating physics constraints. Appendix E provides\n\n6\n\n\fPublished as a conference paper at ICLR 2025\n\na detailed account of the selection of backbones and the training strategies employed for each dataset.\nWe also provide ablation studies in Sec. 4.3 of 1) data matching and noise matching techniques for\ndifferent datasets, revealing that incorporating a distributional prior enhances model performance;\n2) the effect of omitting Jensen’s gap, finding that nonlinear constraints can hinder performance if\nnot properly handled. However, appropriately managing these priors using our proposed methods\ncan lead to significant performance improvements.\n\n4.1 PDE DATASETS\n\nPDE datasets, including advection (Zang, 1991), Darcy flow (Li et al., 2024), Burgers (Rudy et al.,\n2017), and shallow water (Kl¨ower et al., 2018), are fundamental resources for studying and modeling\nvarious physical phenomena. These datasets enable the simulation of complex systems, demonstrat-\ning the capability of models for broader application across a wide range of PDE datasets. Through\nthis, they facilitate advances in understanding diverse natural and engineered processes.\n\nExperiment settings. The PDE constraints for the above datasets are given by:\n\nAdvection: ∂tu(t, x) + β∂xu(t, x)\nDarcy flow: ∂tu(x, t) − ∇(a(x)∇u(x, t)) = f (x),\nBurger: ∂tu(x, t) + u(x, t)∂xu(x, t) = 0,\n\n= 0,\n\nShallow water: ∂tu = −∂xh,\n\n∂vt = −∂hy, ∂th = −c2 (∂xu + ∂yv) .\n\n(8a)\n(8b)\n(8c)\n\n(8d)\n\nA detailed introduction and visualization of the datasets can be found in Appendix C.1.\nIn this\nstudy, we investigate the predictive capabilities of generative models applied to advection and Darcy\nflow datasets. Our experiments focus on evaluating the models’ accuracy in forecasting future states\ngiven initial conditions. Additionally, we examine the models’ ability to generate physically feasible\nsamples that align with the distribution of the training set on advection, Burger, and shallow water\ndatasets. The evaluation metrics are designed to assess to what extent the solutions adhere to the\nphysical feasibility constraints imposed by the corresponding PDEs.\n\nInjecting physical feasibility priors. We train the models that apply the data matching objective\nas suggested in Remark 1. We employ finite difference methods to approximate the differential\nequations. This approach renders the PDE constraints linear for the advection, Darcy flow, and\nshallow water datasets. However, PDE constraints become multilinear for the Burgers’ equation\ndataset (see Appendix C.2 for the proof). Thus, the first set of datasets: advection, Darcy flow, and\nshallow water—correspond to the linear case, while the Burgers’ equation dataset corresponds to the\nmultilinear case. We can directly apply the physical feasibility constraints on the model’s output.\n\nExperimental results. Results can be seen in Tab. 1, 2. In Tab. 1, we analyze the performance\nof diffusion models in predicting physical dynamics, given initial conditions, within a generative\nframework that produces a Dirac distribution. The accuracy of these models is evaluated using\nthe RMSE metric. The observed loss magnitude is comparable to the prediction loss using with\nFNO, U-Net, and PINN models (Takamoto et al., 2022) (refer to Appendix E.4 for further details).\nOur results indicate that the incorporation of constraints consistently enhances the accuracy of the\nprediction. In Tab. 2, the feasibility of the generated samples is evaluated by calculating the RMSE\nof the PDE constraints, which determine the impact of incorporating physical feasibility priors on\ndiffusion models. We also provide visualization of the generated samples in Fig. 11, 12, 13.\n\nMethod\nw/o prior\nw/ prior\n\nAdvection (×10−2) Darcy flow (×10−2)\n\n1.7263±0.0491\n1.6536±0.0677\n\n2.0648±0.0600\n1.9678±0.0651\n\nMethod\nw/o prior\nw/ prior\n\nAdvection\n2.398±0.024\n2.305±0.001\n\nBurger\n6.862±0.060\n6.610±0.012\n\nShallow water\n8.0153±0.0960\n7.7618±0.0645\n\nTable 1: Performance comparison of diffu-\nsion models with/without priors for predict-\ning physical dynamics. The models’ accu-\nracy is measured using the RMSE metric,\nhighlighting the impact of incorporating con-\nstraints on improving prediction accuracy.\n\nTable 2: Comparative analysis of diffusion\nmodels, assessing the feasibility of generated\nsamples with/without physical feasibility pri-\nors. We evaluate the RMSE of PDE con-\nstraints, demonstrating the effect of physical\nfeasibility priors on the adherence to PDEs.\n\n7\n\n\fPublished as a conference paper at ICLR 2025\n\n4.2 PARTICLE DYNAMICS DATASETS\n\nWe train diffusion models to simulate the dynamics of chaotic three-body systems in 3D (Zhou &\nYu, 2023) and five-spring systems in 2D (Kuramoto, 1975; Kipf et al., 2018) (see Appenidx. D.1\nfor visualizations of datasets). In the case of the three-body, we unconditionally generate the posi-\ntions and velocities of three particles, where gravitational interactions govern their dynamics. The\nstochastic nature of this dataset arises from the random distribution of the initial positions and ve-\nlocities. In five-spring systems, each pair of particles has a probability 50% of being connected by\na spring. The movements of the particles are influenced by the spring forces, which cause stretch-\ning or compression interactions. We conditionally generate the positions and velocities of the five\nparticles based on their spring connectivity.\n\nNotations. The features of the datasets are represented as X(0) = [C(0) V(0)] ∈ RL×K×2D,\nwhere C(0), V(0) ∈ RL×K×D. Here, the matrix C(0) encapsulates the coordinate features, while\nV(0) encapsulates the velocity features. The superscript denotes the time for the diffusion process\nand the subscripts denote the matrix index. L represents the temporal length of the physical dynam-\nics, K denotes the number of particles, and D corresponds to the spatial dimensionality. We use the\nsubscript l to indicate time, while the subscripts i, j, and k are used to denote the indices of particles.\nThe subscript d represents the index corresponding to the spatial axis. We also use the subscript of\nθ to denote the corresponding values of the model’s prediction of E[X(0) | X(t)] with inputs X(t)\nand t, and Xθ = [Cθ Vθ].\n\nInjecting SE(n)-invariance and permutation invariance. Two physical dynamic systems are\ngoverned by the interactions between each pair of particles, resulting in a distribution that is SE(n)\nand permutation invariant. Our objective is to develop models that are SO(n)-equivariant, translation\ninvariant, and permutation equivariant. We intend to apply a noise matching objective to achieve the\ndesired invariant distribution. However, to the best of our knowledge, no such architecture satisfying\nthe above properties has been established within the context of diffusion generative models. There-\nfore, we opt to utilize a data augmentation method to ensure the model’s equivariance and invariance\nproperties (Chen et al., 2019; Botev et al., 2022), i.e. we apply these group operations in the training\nprocess, which enforces models to be equivariant and invariant.\n\nConservation of momentum. For both datasets, the momentum conservation is given by:\n\nK\n(cid:88)\n\nk=1\n\nmkV(0)\n\nl,k,d = constantd,\n\n∀l = 1, . . . , L, d = 1, . . . , D.\n\n(9)\n\nHere, mk represents the mass of k-th particle, and V(0)\nl,k,d denotes the velocity along axis d of the k-\nth particle at time l. The total momentum in each axis remains constant, as indicated by the equality.\nThis constraint is linear w.r.t. V(0)\nl,k,d, corresponding to the linear case. Let f : RL×K×D×RK → RD\ncalculate the mean of the total momentum over time and set the penalty loss as\n\n\n\nJR (θ) = Et,x0,ϵ\n\nw(t)\n\nL\n(cid:88)\n\nD\n(cid:88)\n\nl=1\n\nd=1\n\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n\nk=1\n\n(cid:32) K\n(cid:88)\n\n(cid:33)\n\nmk (Vθ)l,k,d\n\n− fd(Vθ, {mk}K\n\nk=1)\n\n2\n\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n\n .\n\n(10)\n\nConservation of energy for the three-body dataset. The total of gravitational potential energy\nand kinetic energy remains constant over time. The energy conservation equation is given by:\n\n−\n\nK\n(cid:88)\n\ni̸=j\n\nGmimj\nR(0)\nl,ij\n\nK\n(cid:88)\n\nD\n(cid:88)\n\n+\n\nk=1\n\nd=1\n\n1\n2\n\nmk(V(0)\n\nl,k,d)2 = constant,\n\n∀l = 1, . . . , L,\n\n(11)\n\nwhere G denotes the gravitational constant. R(0)\nl,j ∥ denotes the Euclidean distance\nbetween the i-th and j-th particle at time l. This constraint is nonlinear with X(0) but can be de-\ncomposed into elementary cases. Note that the constraint is multilinear w.r.t. 1/R(0)\nl,k,d)2.\nHence, from the results of the general nonlinear cases, we can train another model sharing the same\n\nl,ij and (V(0)\n\nl,ij = ∥C(0)\n\nl,i − C(0)\n\n8\n\n\fPublished as a conference paper at ICLR 2025\n\nFigure 2: Visualization of generated samples from the three-body (first row) and five-spring (second\nrow) datasets. The leftmost figures in each row represent methods without priors, the middle figures\ncorrespond to our proposed methods, and the rightmost figures illustrate the physical properties as\nthey evolve over time. Both total momentum and total energy should remain conserved. The samples\ngenerated by our methods demonstrate stronger adherence to physical feasibility.\n\nhidden size as the model for noise matching to predict these variables related to the conservation of\nenergy. Furthermore, since these variables are convex w.r.t. X(0), by the results of the convex case,\nwe can directly apply the penalty loss as:\n\n\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n\n1\n(Rθ)l,ij\n\n−\n\n(cid:13)\n2\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n\n1\nR(0)\nl,ij\n\n+ w2(t)\n\n(cid:88)\n\nl,k,d\n\n(cid:13)\n(cid:13)\n(Vθ)2\n(cid:13)\n(cid:13)\n\nl,k,d −\n\n(cid:16)\n\nV(0)\nl,k,d\n\n2\n(cid:17)2(cid:13)\n(cid:13)\n, (12)\n(cid:13)\n(cid:13)\n\nJR (θ) = Et,x0,ϵ\n\nw1(t)\n\n(cid:88)\n\nl;i̸=j\n\nwhere (Rθ)l,ij = ∥Cθ\nl,j ∥, i.e. model’s prediction of the Euclidean\ndistance between two particles calculated from its prediction of coordinates. This penalty loss can\nalso be derived from the reducible case. The detailed derivation can be found in Appendix D.2.\n\nl,i − Cθ\n\n(cid:0)X(t), t(cid:1)\n\n(cid:0)X(t), t(cid:1)\n\nConservation of energy for the five-spring dataset. The combined elastic potential energy and\nthe kinetic energy are conserved throughout time. The equation for the conservation of energy is\nrepresented by:\n\n(cid:88)\n\n(i,j)∈E\n\n(cid:16)\n\n1\n2\n\nκ\n\nR(0)\nl,ij\n\n(cid:17)2\n\n+\n\nK\n(cid:88)\n\nD\n(cid:88)\n\nk=1\n\nd=1\n\n(cid:16)\n\nV(0)\nl,k,d\n\n(cid:17)2\n\nmk\n\n1\n2\n\n= constant,\n\n∀l = 1, . . . , L,\n\n(13)\n\nwhere κ denotes the elastic constant, R(0)\nl,j ∥ denotes the distance between the i-th\nand j-th particle at time l, and E denotes the edge set of springs connecting particles. mk represents\nthe mass of the k-th particle. Analogue to the conservation of energy for the three-body dataset, we\ncan reduce the nonlinear constraints into elementary cases.\n\nl,ij = ∥C(0)\n\nl,i − C(0)\n\nExperimental results. The results can be seen in Tab. 3 and Fig. 2, 14, 15, and we refer readers\nto Appendix E.2 for a detailed account of the experimental settings, as well as a more extensive\ncomparison of the effects of hyperparameters and various methods for injecting constraints across\nthe discussed cases. Our analysis indicates that for the three-body dataset, the incorporation of\nthe conservation of energy prior, via the reducible case method, substantially enhances the model’s\nperformance across all evaluated metrics. Similarly, applying the conservation of momentum prior to\nthe five-spring dataset significantly reduces the momentum error in the generated samples. This also\n\n9\n\n024681.51.00.50.00.5baseline gravity energybaseline kinetic energybaseline total energyours gravity energyours kinetic energyours total energy010203040501.51.00.50.00.51.01.52.0baseline momentum along xbaseline momentum along your momentum along xour momentum along y\fPublished as a conference paper at ICLR 2025\n\nMethod\n\nw/o prior\nw/ prior\n\nTraj error\n2.4132±0.1208\n1.9880±0.3418\n\nThree-body\nVel error\n2.5745±0.0790\n0.8328±0.1042\n\nEnergy error\n4.3292±0.7235\n0.5465±0.0705\n\nDynamic error Momentum error\n5.3699±0.0462\n5.1754±0.0286\n0.3898±0.0118\n5.0731±0.0406\n\nEnergy error\n1.0618±0.0243\n0.7418±0.0129\n\nFive-spring\n\nTable 3: Sample quality of the three-body and five-spring datasets. For both datasets, we simulate\nthe ground-truth future motion based on the current states of the generated samples and report the\nMSE error between the ground-truth motion and the generated ones. We also calculate the error\nof physical feasibility such as conservation of the energy and momentum, which should remain\nunchanged along the evolution of the systems.\n\ncontributes to a reduction in the errors associated with dynamics and energies. Fig. 2 demonstrates\nthat the total momentum and energy of samples generated with the incorporation of priors exhibit\ngreater stability compared to those without priors. We also provide sampling results using the DPM-\nsolvers (Lu et al., 2022) in Appendix E.7, which significantly lower computational expenses.\n\n4.3 ABLATION STUDIES\n\nDistributional priors through match-\ning objective. We employ data match-\ning and noise matching techniques for the\nPDE and particle dynamics datasets, re-\nspectively. An ablation study is conducted\nto investigate the effects of applying the\nalternative matching objective on the par-\nticle dynamics and PDE datasets, both\nwithout physical feasibility priors. The\nresults, presented in Tab. 4, demonstrate\nthat\nincorporating a distributional prior\ncan significantly improve the model’s per-\nformance.\n\nMethod\ndistributional prior\nalternative\n\nThree-body\n2.6084\n4.7241\n\nFive-spring Darcy flow Shallow water\n\n5.1929\n5.3120\n\n2.016\n7.268\n\n8.150\n27.40\n\nTable 4: Results of an ablation study comparing the ef-\nfects of data matching and noise matching techniques.\nThe findings show that incorporating a distributional\nprior improves model performance. We use the mean\nof trajectory error and velocity error as the metric for\nthe three-body dataset.\n\nMethod\nw/o prior\nprior by PINN\nprior by ours\n\nTraj error Vel error\n2.6555\n2.6437\n0.7307\n\n2.5613\n2.6048\n1.6072\n\nEnergy error\n3.8941\n4.2219\n0.5062\n\nTable 5: Results show the impact of enforcing energy\nconservation constraints on the three-body dataset.\nDirect application of nonlinear constraints (prior by\nPINN) can degrade performance, while proper han-\ndling (prior by ours) improves accuracy.\n\nOmitting Jensen’s gap.\nIn the three-\nbody dataset, we employ a multilinear\nfunction to simplify constraints into con-\nvex scenarios. We now conduct an abla-\ntion study in which the output of a diffu-\nsion model is considered the ground-truth,\nand the constraint of energy conservation\nis imposed similarly to the injection of constraints by penalty loss in the prediction tasks of PINNs.\nThis configuration is referred to as “prior by PINN”. We define a penalty loss based on the variation\nof energy over time, analogous to the penalty loss used to enforce momentum conservation con-\nstraints. However, unlike the conservation of momentum, which is governed by a linear constraint\nand can thus be applied directly, the conservation of energy involves a nonlinear constraint. This\nintroduces Jensen’s gap, preventing the direct application of the constraint. The results, presented in\nTab. 5, indicate that directly applying nonlinear constraints can degrade the model’s performance.\nHowever, appropriately handling these constraints can significantly improve the sample quality.\n\n5 CONCLUSION\n\nIn conclusion, this paper presents a novel method for generating physically feasible dynamics using\ndiffusion models by integrating distributional and physical feasibility priors. We inject distribu-\ntional priors through equivariant models and noising matching, while incorporating physical fea-\nsibility priors through constraint decomposition. Empirical results demonstrate the robustness of\nour method across various physical phenomena, highlighting its promise for data-driven AI4Physics\nresearch. This work emphasizes the importance of embedding domain knowledge into learning\nsystems, bridging physics and machine learning through innovative use of physical priors.\n\n10\n\n\fPublished as a conference paper at ICLR 2025\n\nACKNOWLEDGMENTS\n\nThis work was supported by the National Science and Technology Major Project of China under\nGrant 2022ZD0116408. This work is also supported by a grant from ChemLex Technology Co.,\nLtd..",
    "abstract": "Generating physically feasible dynamics in a data-driven context is challenging, especially when adhering to physical priors expressed in specific equations or formulas. Existing methodologies often overlook the integration of ''physical priors'', resulting in violation of basic physical laws and suboptimal performance. In this paper, we introduce a novel framework that seamlessly incorporates physical priors into diffusion-based generative models to address this limitation. Our approach leverages two categories of priors: 1) distributional priors, such as roto-translational invariance, and 2) physical feasibility priors, including energy and momentum conservation laws and PDE constraints. By embedding these priors into the generative process, our method can efficiently generate physically realistic dynamics, encompassing trajectories and flows. Empirical evaluations demonstrate that our method produces high-quality dynamics across a diverse array of physical phenomena with remarkable robustness, underscoring its potential to advance data-driven studies in AI4Physics. Our contributions signify a substantial advancement in the field of generative modeling, offering a robust solution to generate accurate and physically consistent dynamics.",
    "response": {
      "Soundness": 3,
      "Presentation": 3,
      "Contribution": 2,
      "Rating": 6,
      "Confidence": 2,
      "Strengths": "1. The manuscript is well-written and easy to follow.\n2. Thorough derivations are provided in the main content and appendix.\n3. Sufficient experiments and ablation studies are conducted to validate the effectiveness of the proposed method.",
      "Weaknesses": "Please see the Question part.",
      "Questions": "- The three contributions listed in Section 1 appear similar and convey the same meaning. Accurately summarizing the manuscript's contributions will be more helpful for the readers.\n\n- Why distribution priors (such as translational rotational invariance) are important for generating physical dynamics? As stated in [1], the invariant sampling and invariant loss functions by restricting architecture designs often sacrifice empirical performances. Are simple data augmentations that approximate probability equivariance also available in physical dynamics generation? Experiments with data augmentations that approximate probability equivariance might help analyze the effectiveness of equivariant models.\n\n- How are the physical priors selected in this work? It appears that the roto-translational invariance and the priors based on energy and momentum conservation laws are chosen arbitrarily. There seems to be no systematic analysis that explores all possible categories of physical priors or how to incorporate each type of prior into diffusion models. I believe there are many more physical priors that generative models should be constrained by. Categorizing them and discussing each category systematically would provide more insight.\n\n- In Line 498-508, the paragraph title is \"Data matching vs noise matching\". However, the ablation study is conducted for incorporating a distributional prior or not, which is expected to investigate the training objective instead.\n\n---\n[1] SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation. TMLR 2024."
    }
  },
  {
    "paper_id": "ZK4VSRzBNC",
    "summary": "Background: Reinforcement learning (RL) has achieved notable success in various domains, but high sample complexity remains a challenge. Previous methods to address sample complexity include optimization techniques, transfer learning, modular RL, hierarchical RL, and offline RL. Reward adaptation (RA) allows agents to adapt to a new reward function using previously learned behaviors in the same domain with different reward functions. Existing RA methods, like Successor Feature Q-Learning (SFQL), are limited when the target behavior significantly diverges from source behaviors.\nGap: Existing reward adaptation methods, particularly SFQL, assume a high similarity between source behaviors and the target behavior. This limits their effectiveness when the target behavior is significantly different from the source behaviors. The paper identifies the need for a more general method that does not depend on the similarity between source and target behaviors.\nHypothesis: The authors propose that a method called Q-Manipulation (Q-M) can improve reward adaptation by computing Q-function bounds and enabling action pruning, which does not compromise the optimality of the policy. This approach should lead to improved sample efficiency and applicability across varied domains.\nMethod: The proposed Q-Manipulation (Q-M) method involves manipulating Q-functions to compute bounds for the Q-function in the target domain. This process relies on the assumption that the target reward function is a known function of source reward functions. Q-M uses an iterative process to tighten these bounds before learning in the target domain, which allows for action pruning without affecting policy optimality. The method evaluates its performance against baselines in synthetic and simulation domains, illustrating its effectiveness and sample complexity improvements.\nConclusion: Q-Manipulation significantly enhances reward adaptation by enabling action pruning without compromising policy optimality. It performs better than existing baselines, particularly in scenarios where target behaviors differ from source behaviors. The method generalizes well across different domains and is shown to be beneficial even with noisy reward combinations and continuous state spaces through discretization. The study demonstrates the potential of Q-M to contribute to more efficient reinforcement learning through enhanced reward adaptation.\n",
    "full text": "\nREWARD ADAPTATION VIA Q-MANIPULATION\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nIn this paper, we propose a new solution to reward adaptation (RA), the prob-\nlem where the learning agent adapts to a target reward function based on one or\nmultiple existing behaviors learned a priori under the same domain dynamics but\ndifferent reward functions. RA has many applications, such as adapting an au-\ntonomous driving agent that can already operate either fast (if transporting goods)\nor comfortable (if carrying passengers) to operating both fast and comfortable (if\ntransporting goods with human passengers onboard). Learning the target behavior\nfrom scratch is possible but often inefficient given the available source behaviors.\nOur work represents a new approach to RA via the manipulation of Q-functions.\nAssuming that the target reward function is a known function of the source reward\nfunctions, our approach to RA computes bounds of the Q function. We introduce\nan iterative process to tighten the bounds, similar to value iteration. This enables\naction pruning in the target domain before learning even starts. We refer to such a\nmethod as “Q-Manipulation” (Q-M). We formally prove that our pruning strategy\ndoes not affect the optimality of the returned policy while empirically show that\nit improves the sample complexity. Comparison with baselines is performed in a\nvariety of synthetic and simulation domains to demonstrate its effectiveness and\ngeneralizability.\n\n1\n\nINTRODUCTION\n\nReinforcement Learning (RL) as described by Watkins (1989); Sutton and Barto (2018) represents\na class of learning methods that allow agents to learn from interacting with the environment. RL\nhas demonstrated great successes in various domains such as games like Chess in Campbell et al.\n(2002), Go in Silver et al. (2016), and Atari games in Mnih et al. (2015), logistics in Yan et al. (2022),\nbiology in Angermueller et al. (2019), and robotics in Kober et al. (2013). However, applying RL to\nmany real-world problems still suffers from the issue of high sample complexity. Prior approaches\nhave been proposed to alleviate the issue from different perspectives, such as learning optimization,\ntransfer learning, modular and hierarchical RL, and offline RL.\n\nThe problem of reward adaptation (RA) was first introduced and addressed by Barreto et al. (2018;\n2020), where the learning agent adapts to a target reward function given one or multiple existing\nbehaviors learned a priori (referred to as the source behaviors) under the same actions and transition\ndynamics but different reward functions. RA has many useful applications, such as enabling a\nvehicle’s driving behavior from two known behaviors (comfortable driving with passengers and fast\ndriving for goods delivery) to a new target behavior that combines comfort and speed, accommodating\nboth passengers and goods. Featuring such a special type of transfer learning, RA methods can benefit\nfrom an ever-growing repertoire of source behaviors to create new and potentially more complex\ntarget behaviors. Learning the target behavior from scratch is possible but often inefficient given the\navailable source behaviors. In this paper, we present a new approach that offers its unique benefits\ncompared to the previous work on RA.\n\nTo better conceptualize the RA problem, consider a grid-world as shown in Fig. 1, which is an\nexpansion of the Dollar-Euro domain described by Russell and Zimdars (2003). In this domain, the\nagent can move to any of its adjacent locations at any step. The agent’s initial location is colored in\nyellow and the terminal locations are colored pink or green, which correspond to the source reward\nfunctions (i.e., collecting dollars and euros), respectively. Visiting the terminal location with a single\ncolor returns a reward of 1.0 under the corresponding reward function, and visiting the terminal\nlocation with split colors returns a reward of 0.6 under both reward functions. In RA, the assumption\n\n1\n\n000\n001\n\n002\n003\n004\n005\n006\n007\n\n008\n009\n010\n011\n012\n013\n\n014\n015\n016\n017\n018\n\n019\n020\n021\n022\n023\n024\n\n025\n026\n027\n028\n029\n030\n\n031\n032\n033\n034\n035\n036\n\n037\n038\n039\n040\n041\n\n042\n043\n044\n045\n046\n047\n\n048\n049\n050\n051\n052\n053\n\n\fUnder review as a conference paper at ICLR 2025\n\nis that the optimal behaviors under the source reward functions are given, referred to as the source\nbehaviors. A target domain may correspond to a reward function that awards both dollars and euros.\n\nUnder the assumption that the reward function is expressed in the form of feature weights such that\nthe source behaviors can be evaluated easily under the target domain, prior work for addressing RA\ncan be viewed as combining the best parts of the source behaviors to initialize learning, referred to as\nSuccessor Feature Q-Learning (SFQL) by Barreto et al. (2018; 2020). Consequently, SFQL may not\nwork well for situations where the target behavior differs substantially from the source behaviors, such\nas in the Dollar-Euro domain. Our approach, instead, reasons about the best/worse-case scenarios\nunder each source domain and combines such knowledge to compute upper/lower bounds of the\ntarget Q-function to enable action pruning. It results in a more general knowledge transfer method\nwhose efficacy does not rely on the similarity between the source and target behaviors.\n\nOur new approach to RA is referred to as “Q-Manipulation”\n(Q-M). We assume the existence of a function, referred to\nas the combination function, that relates the source reward\nfunctions to the target reward function. In practice, we often\nhave a good idea about the functional relationship between\nthe source and target reward functions (e.g., linear in the\nDollar-Euro domain). Based on such a relationship, Q-M\ncomputes an upper and lower bound of Q-function in the\ntarget domain to identify actions that cannot contribute to\nthe optimal behavior via an iterative process similar to value\niteration. It enables us to prune those actions before learning\nthe target behavior without affecting its optimality. In our\nevaluation, we empirically show that the effectiveness of\nQ-M across simulated and randomly generated domains and analyze its limitations, focusing on\nconditions under which its efficacy is negatively impacted. Furthermore, we demonstrate that Q-M\ncan still be effective in domains with continuous state spaces via discretization, even though the\noptimality guarantee would be lost there. In general, Q-M requires additional computing resources\n(i.e., CPU time and storage) to implement but its benefits outweigh the costs in practical applications,\nespecially in situations where accessing the target domain for samples is expensive.\n\nFigure 1: Dollar-Euro domain.\n\nOur core contributions are: We address the problem of reward adaptation (RA) via Q-Manipulation\n(Q-M), which represents a new approach to RA that supports more general knowledge transfer than\nthe previous work. In domains with discrete state spaces, we formally prove the correctness of the\naction pruning process under certain initialization conditions; otherwise, we suggest how Q-M may\nbe applied to expedite learning at the cost of guaranteed optimality. We extensively evaluate Q-M\nwith respect to baselines to validate its efficacy and analyze its limitations.\n\n2 METHODOLOGY\nIn this section, we start with a brief introduction to reinforcement learning (RL) before discussing\nreward adaptation (RA) and our approach. In RL, the task environment is modeled as an MDP\nM = (S, A, T, R, γ), where S is the state space, A is the action space, T : S × A × S → [0, 1]\nis the transition function, R : S × A × S → R is the reward function, and γ is the dis-\ncount factor. At every step t, the RL agent observes state st and takes an action at ∈ A.\nAs a result, the agent progresses to state st+1 according to the transition dynamics T (·|st, at),\nand receives a reward rt. The goal is to search for a policy that maximizes the expected\ncumulative reward or expected return. We use π to denote a policy as a mapping from S\nto A. The Q function of the optimal policy π∗ is denoted by Q∗ and defined in Eq.\n1.\nTo prepare us for later discussion, we\nalso introduce Qµ (Eq. 2) to repre-\nsent the Q function of the “worst” pol-\nicy that minimizes the expected return.\nThe following lemma establishes the\nconnection between Qµ and a variant\nof Q∗:\nLemma 1.\n\nQ∗(s, a) = max\n\nQµ(s, a) = min\n\nt=0\n(cid:34) ∞\n(cid:88)\n\nγtrt|s0, π\n\nγtrt|s0, π\n\n(cid:34) ∞\n(cid:88)\n\n(cid:35)(cid:35)\n\n(cid:35)(cid:35)\n\n(1)\n\n(2)\n\nt=0\n\nE\n\nE\n\n(cid:34)\n\n(cid:34)\n\nπ\n\nπ\n\nR(s, a) = −Q∗\n−R(s, a) denotes the Q function of the optimal policy under negative R or −R.\n\n−R(s, a)\n\nQµ\n\nwhere Q∗\n\n(3)\n\n2\n\n054\n055\n\n056\n057\n058\n059\n060\n061\n\n062\n063\n064\n065\n066\n067\n\n068\n069\n070\n071\n072\n\n073\n074\n075\n076\n077\n078\n\n079\n080\n081\n082\n083\n084\n\n085\n086\n087\n088\n089\n090\n\n091\n092\n093\n094\n095\n\n096\n097\n098\n099\n100\n101\n\n102\n103\n104\n105\n106\n107\n\n\fUnder review as a conference paper at ICLR 2025\n\nIn this paper, we consider RL with discrete state and action spaces and deterministic policies.\nExtending the discussion to the continuous cases and stochastic policies will be future work. Proofs\nthroughout the paper are included in the appendix.\n\n2.1 REWARD ADAPTATION (RA)\n\nDefinition 1 (Reward Adaptation (RA)). Under M \\ R, denoting an MDP without the specification\nof a reward function, RA is to determine the optimal policy for a target reward function R, given a\nset of source behaviors trained under their respective source reward functions R1, R2 . . . Rn.\n\nIn RA, we assume the same transition dynamics, state and action spaces for the source and target\nbehaviors. Note that the source domains are no longer accessible while learning the target behavior.\nNext, we provide the problem statement of RA under Q-M as follows:\n\nProblem Statement [Reward Adaptation with Q-Variants]: Given an RA problem where variants of\nthe Q functions are accessible for the source domains (e.g., Q∗’s and Qµ’s under the source reward\nfunctions), determine the optimal policy under a target reward function R that is a known function of\nthe source reward functions specified as follows:\n\nR = f (R1, R2, . . . Rn)\nf above is also referred to as the combination function. When f is not known exactly but can be\nmodeled with an additional noise component, we will discuss later how Q-M can be adapted to handle\nsuch situations at the cost of reduced efficacy.\n\n(4)\n\nTo derive a solution to RA with Q-variants, we propose Q-M, an action-pruning strategy that ensures\nthat only unnecessary actions are pruned. To achieve this, we aim to compute an upper and lower\nbound of Q∗ under the target reward function based on the Q variants from the source behaviors.\nIntuitively, if the lower bound of an action a is higher than the upper bound of action (cid:98)a under a state s,\n(cid:98)a can be pruned. In Q-M, we derive these bounds based on an iterative process that we describe next.\n\n2.2 Q-MANIPULATION\n\nIn Q-M, we first initialize an upper and lower bound of Q∗\nR and then iteratively refine them. To avoid\nnotation cluttering to improve clarity, we omit the subscript of Q for indicating the reward function\nused. These two steps are formalized below (Note that we do not assume any knowledge of Q∗):\n\nUpper Bound (UB)\n\nQU B\n\nk+1(s, a) = min\n\n(cid:18)\n\nQU B\n0\n\n(s, a) > Q∗\n\nQU B\nk\n\n(s, a), max\n\ns′∈ ˆT (·|s,a)\n\n[Initialization]\n(cid:104)\n\nR(s, a, s′) + γ max\n\na′\n\n(cid:105) (cid:19)\n\n(s′, a′)\n\nQU B\nk\n\nLower Bound (LB)\n\nQLB\n\nk+1(s, a) = max\n\n(cid:18)\n\nQLB\n\n0 < Q∗\n\nQLB\n\nk (s, a), min\n\ns′∈ ˆT (·|s,a)\n\n[Initialization]\n(cid:104)\nR(s, a, s′) + γ max\n\na′\n\n(cid:105) (cid:19)\n\nQLB\n\nk (s′, a′)\n\n(5)\n\n(6)\n\n(7)\n\n(8)\n\nˆT (·|s, a) denotes reachable states (or neighbouring states) from s, a. This information is assumed to\nbe available in Q-M or can be obtained while training the source behaviors. Similarly, the source\nreward functions or Ri’s are also assumed to be available so that R(s, a, s′) in the equations above\ncan be computed based on its known relationship with them (Eq. 4). The outermost max/min ensures\nQU B ≥ Q∗ ≥ QLB throughout the iterative processes via simple induction. It is worth noting that\nthe updates above ensure that the upper and lower bounds are always decreasing and increasing,\nrespectively, as desired such that the bounds are tightening. When the source reward functions are\nnoisy, it requires their means to be used in the updates. Next, before discussing the initializations, we\nshow that such processes converge to a fixed point in Q-M, respectively.\nDefinition 2. The min and max Bellman operator for UB and LB in Q-M are mappings T : R|S×A| →\nR|S×A| that satisfy, respectively:\n(cid:32)\n\n(cid:104)\nR(s, a, s′) + γ max\n\nQU B\nk\n\n(s′, a′)\n\na′\n\n(cid:33)\n(cid:105)\n\n(TminQU B\n\nk\n\n)(s, a) = min\n\nQU B\nk\n\n(s, a), max\n\ns′∈ ˆT (·|s,a)\n\n3\n\n108\n\n109\n110\n111\n112\n113\n\n114\n115\n116\n117\n118\n119\n\n120\n121\n122\n123\n124\n125\n\n126\n127\n128\n129\n130\n131\n\n132\n133\n134\n135\n136\n\n137\n138\n139\n140\n141\n142\n\n143\n144\n145\n146\n147\n148\n\n149\n150\n151\n152\n153\n154\n\n155\n156\n157\n158\n159\n\n160\n161\n\n\fUnder review as a conference paper at ICLR 2025\n\n(cid:32)\n\n(TmaxQLB\n\nk )(s, a) = max\n\nQLB\n\nk (s, a), min\n\ns′∈ ˆT (·|s,a)\n\n(cid:104)\n\nR(s, a, s′) + γ max\n\na′\n\n(cid:33)\n\n(cid:105)\nk (s′, a′)\n\nQLB\n\nSince the theoretical results for the min and max operator are similar, we do not distinguish between\nthem below but provide separate proofs for them in the appendix.\nTheorem 1 (Convergence). The iteration process introduced by the Bellman operator in Q-M satisfies\n∥T Qk − T Qk+1∥∞ ≤ γ∥Qk − Qk+1∥∞, ∀Qk, Qk+1 ∈ R|S×A|.\n\nsuch that the Q function converges to a fixed point.\n\nFormally, ∥f ∥∞ = supx |f (x)| and it returns the maximum absolute difference between Qk(s, a)\nand Qk+1(s, a) under any s, a above. The process converges to a fixed point, since the difference\nbetween two consecutive iterations always decreases. However, it turns out that the fixed point may\nnot necessarily be unique as with value iteration.\nTheorem 2. The Bellman operator in Q-M specifies only a non-strict contraction in general:\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)T Q − T (cid:98)Q\n(cid:13)∞\n\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)\n(cid:13)Q − (cid:98)Q\n(cid:13)∞\n\n≤\n\nThis result is interesting since it identifies another case where non-strict contraction results in a fixed\npoint other than the identity map.\nCorollary 1 (Non-uniqueness). The fixed point of the iteration process in Q-M may not be unique.\n\nIn our evaluation, we observe that the fixed point found by the iteration process depends on the\ninitialization. Another observation is that the Bellman operator in Q-M appears almost identical to\nthat in value iteration when the MDP is deterministic. In such cases, we observe that Q-M often\nresults in zero-shot learning when the upper and lower bounds converge to Q∗\nR.\n\n2.3\n\nINITIALIZING THE BOUNDS\n\nA simple way to initialize the bounds would be to identify the most positive and negative rewards\nand compute the sums of their geometric sequences via the discount factor, respectively. However,\nthese bounds are likely to be too conservative to be useful since the iteration processes may converge\nundesirably due to non-unique fixed points. Intuitively, we would like the bounds to be tight initially\nto yield the best results. However, computing bounds for the target behavior based on information\nfrom the source behaviors only is not a trivial task. Next, we show situations where additional\nassumptions hold such that we can provide more desirable initializations. In particular, we will show\nnext how different forms of the combination function f in Eq. 4 can affect the initializations.\n\ni ’s while learning the source behaviors, we propose the initializations as follows. Note that Qµ\n\nLinear Combination Function: First, we consider the case when the target reward function is a\nlinear function of the source reward functions. In such cases, if the agent maintains both Qµ\ni ’s and\nQ∗\ni can\nbe obtained conveniently while learning the source behaviors based on Lemma 1.\nLemma 2. When R = (cid:80) ciRi where ci ≥ 0 , an upper and lower bound of Q∗\nrespectively, by:\n\nR are given,\n\nQU B\n\n0 =\n\nn\n(cid:88)\n\ni=1\n\nciQ∗\ni\n\nQLB\n\n0 = max\n\n[ciQ∗\n\ni +\n\ni\n\ncjQµ\n\nj ] where j ∈ {1 : n} \\ i\n\n(cid:88)\n\nj\n\n(9)\n\nNonlinear Combination Function: Handling nonlinear combination is more complicated and\nderiving tight bounds that are guaranteed to be correct is difficult. Instead, we propose approximate\nbounds for monotonically increasing and positive function f as follows:\n|R1|, Q∗\n\n0 = −f (Q∗\n\n0 = f (Q∗\n\n|R2|, . . . Q∗\n\n|R2|, . . . Q∗\n\n|R1|, Q∗\n\n|Rn|)\n\n|Rn|)\n\nQU B\n\nQLB\n\n(10)\n\nUsing the bounds above requires the agent to maintain Q∗\n|Ri|’s. Since these bounds are approximate,\nthey do not guarantee correctness in general, meaning that actions belonging to the optimal policy\nmay be pruned. However, we show that they work well in practice in our evaluation.\n\n4\n\n162\n163\n\n164\n165\n166\n167\n168\n169\n\n170\n171\n172\n173\n174\n175\n\n176\n177\n178\n179\n180\n\n181\n182\n183\n184\n185\n186\n\n187\n188\n189\n190\n191\n192\n\n193\n194\n195\n196\n197\n198\n\n199\n200\n201\n202\n203\n\n204\n205\n206\n207\n208\n209\n\n210\n211\n212\n213\n214\n215\n\n\fUnder review as a conference paper at ICLR 2025\n\n2.4 NOISY COMBINATION FUNCTION AND CONTINUOUS STATE SPACES\n\nNoisy Combination Function: When the combination function is not known exactly but can be\nmodeled with an additional noise component such that R = f (R1 . . . Rn) + N , and we know the\nrange of the noise (i.e., Nmin and Nmax). We can consider such situations by augmenting the\nR(s, a, s′) in Eqs. 6 and 8 with Nmax and Nmin, respectively. We must also update the initialization\nof the bounds using QU B = QU B + Nmax × 1−γtmax\n, where\n1−γ\ntmax is the maximum steps in an episode. Note however that such modifications will likely reduce\nthe efficacy of Q-M.\n\nand QLB = QLB + Nmin × 1−γtmax\n\n1−γ\n\nHandling Continuous State Spaces: For domains with continuous state spaces, we resort to using\nfeatures (e.g., tile-coding) to discretize the state space and then apply the process of Q-M on such a\nspace to prune actions. We can then run any RL method that can handle continuous state spaces (such\nas Deep Q-Learning) under the reduced action space per each discrete state. Although the optimality\nguarantee is obviously lost due to the discretization, we aim to show how effective such a simple\nadaption can be. The implementation details are discussed in Sec. 3. We will extend Q-M to natively\nhandle continuous state and action spaces in future work.\n\nAction Pruning in Q-M: Intuitively, if an action a’s lower bound is higher than some other action\nˆa’s upper bound under a state s, then ˆa can be pruned for that state. This allows us to reduce the\naction space per each different state, which contributes to faster convergence. When the upper and\nlower bounds are sound, the optimal policies are preserved.\nTheorem 3. [Optimality] For reward adaptation with Q variants, the optimal policies in the target\ndomain remain invariant under Q-M when the upper and lower bounds are initialized correctly.\n\n3 EVALUATION\nThe primary objective here is to evaluate the performance of Q-M to analyze its benefits and\nlimitations. We compare Q-M with SFQL described by Barreto et al. (2018), the state-of-the-art\napproach to reward adaptation. Q-M and SFQL initialize learning in different ways to transfer\nprior knowledge from the source domains but otherwise both implement Q-Learning (QL) to learn\nthe target behavior. Hence, we also use QL without any knowledge transfer as a baseline. More\nspecifically, to initialize learning for SFQL, we evaluate the given source behaviors on the target\ndomain to compute a bootstrap Q-function as described in the generalized policy improvement\ntheorem in Barreto et al. (2018). Additional results analyzing Q-M (including where actions are\npruned) and running time comparisons are reported in Sec. A.3. We keep the hyperparameters for\nQ-Learning (or DQN) the same across the different methods.\n\nSince we are interested in demonstrating Q-M as a more general knowledge transfer method than\nSFQL, we design the evaluation domains such that the target behaviors are substantially different\nfrom the source behaviors in most of them (similar to the situation in Dollar-Euro). In such cases,\nSFQL, initializing learning by combining the best parts of the source behaviors, is expected to not\nperform well unless the target behavior happens to be characterized by some combination of the\nsource behaviors. Details on how the source and target behaviors are designed are in the appendix.\n\nFor Q-M, we use the initializations described in Sec. 2.3. One observation about Q-M is that the\ncomputation of UB and LB is affected substantially by the stochastic branching factor (SBF) of a\ndomain, as evident in Eqs. 6 and 8. SBF here is defined as the maximum number of next states\nreachable (or with a nonzero transition probability) from any state and action pair. Intuitively, the\nless stochastic the domain is, the more the Bellman updates in Q-M resemble that in value iteration\n(except for the outermost max/min). To demonstrate the influence of SBF, for each evaluation domain,\nwe gradually increase its SBF. At the same time, the number of reachable states from a given state\nand action pair is allowed to vary and randomly chosen between 1 and a set SBF. We first evaluate\nwith simulation and randomly generated domains under linear combination functions and then move\non to the more challenging cases of nonlinear and noisy functions. To showcase the generality of\nQ-M, we also consider randomizing the domains so that we evaluate with 1) given MDP \\R and\ndesigned rewards, 2) randomized MDP \\R and designed rewards, and 3) randomized MDP \\R and\nrandomized rewards. All evaluations are averaged over 30 runs. More details about the evaluation\nsettings along with a detailed description of all the domains, including the design of source and target\nbehaviors, are reported in the appendix.\n\n5\n\n216\n217\n\n218\n219\n220\n221\n222\n223\n\n224\n225\n226\n227\n228\n229\n\n230\n231\n232\n233\n234\n\n235\n236\n237\n238\n239\n240\n\n241\n242\n243\n244\n245\n246\n\n247\n248\n249\n250\n251\n252\n\n253\n254\n255\n256\n257\n\n258\n259\n260\n261\n262\n263\n\n264\n265\n266\n267\n268\n269\n\n\fUnder review as a conference paper at ICLR 2025\n\n3.1 LINEAR COMBINATION FUNCTION\n\nGiven MDP \\R and Designed Rewards:\n\nFigure 2: Convergence plots for Dollar Euro (top) and Racetrack (bottom).\n\nIn this evaluation, we compared Q-M with the baselines in simulation domains that include Racetrack\nand Dollar-Euro. The convergence plots are shown in Fig. 2. In each subfigure, we show the SBF\nused (labeled at the top). We observe that Q-M converges substantially faster than the baselines\nin both domains. However, as expected, the performance of Q-M is negatively impacted as SBF\nincreases. An interesting observation is the performance of SFQL. SFQL seems to struggle with these\ndomains, especially Racetrack. Since the sources behaviors differ much from the target behavior,\nknowledge transfer in SFQL based on combining the source behaviors can actually misguide the\nlearning process. It is worth mentioning that SFQL eventually converged to the optimal policy after\nwe allowed it to train with more episodes. In addition, we also observe that Q-M in deterministic\nscenarios (left most subfigures when SBF = 1) results in zero-shot learning: its iterative processes for\ncomputing UB and LB both converge to Q∗\nR. This result demonstrates that Q-M is indeed a more\ngeneral knowledge transfer method that does not depend on the similarity between the source and\ntarget behaviors.\n\nRandomized MDP \\R and Designed Rewards: First, we evaluated with the Frozen Lake domain\n\nFigure 3: Convergence plots for auto-generated domains (top) and Frozen Lake (bottom).\n\n6\n\n270\n271\n\n272\n273\n274\n275\n276\n277\n\n278\n279\n280\n281\n282\n283\n\n284\n285\n286\n287\n288\n\n289\n290\n291\n292\n293\n294\n\n295\n296\n297\n298\n299\n300\n\n301\n302\n303\n304\n305\n306\n\n307\n308\n309\n310\n311\n\n312\n313\n314\n315\n316\n317\n\n318\n319\n320\n321\n322\n323\n\n\fUnder review as a conference paper at ICLR 2025\n\nwhile randomizing the hole locations (4 holes) in each run. Additionally, we evaluated with auto-\ngenerated MDP\\R’s where the numbers of states and actions are randomly generated, and terminal\nstates were randomly selected. The number of terminal states in both domains was held fixed as well\nas their terminal rewards. The convergence plots are presented in Fig. 3. Similarly, we can observe\nthat Q-M performs the best in both domains. It demonstrates that Q-M can generalize to different\nconfigurations of MDP\\R.\n\nRandomized MDP \\R and Randomized Rewards: In this evaluation, we aim to push the results\n\nFigure 4: Convergence plots for auto-generated domains.\nfrom the previous evaluation further by analyzing the generality of Q-M with both randomized\nMDP \\R and rewards. Randomizing all of these factors simultaneously can introduce very different\nbehaviors, which represent more challenging situations to generalize. In this evaluation, MDP\\R’s\nwith fixed numbers of states and actions were auto-generated in each run. A fixed number of terminal\nstates were selected randomly. Rewards for each transition, including terminal states, were generated\nrandomly. The convergence plots are presented in Fig. 4. Q-M still consistently performs better than\nthe baselines. However, we can also observe that SFQL performs better than QL, which is in contrast\nto the previous evaluations. This is likely due to the fact that a high level randomization here results\nin more similarities between the source and target behaviors that are taken advantage of by SFQL.\n\n3.2 NONLINEAR COMBINATION FUNCTION\n\nFigure 5: Convergence plots for auto-generated domains with a nonlinear f : R = (R1 + R2)3.\n\nWe now extend our evaluation to nonlinear combination functions. The main aim here is to evaluate\nthe effectiveness of the initializations proposed even though the optimality guarantee is lost. In\nthis evaluation, we use the same setting as in Randomized MDP\\R and Designed Rewards above.\nThe convergence plots are presented in Fig. 5. We observe that Q-M is still more efficient than the\nbaselines although the performance gain is not as obvious as in the previous evaluations, especially\nas shown in the last subfigure. As expected, RA with nonlinear combination functions is to more\nchallenging than with linear functions, resulting in reduced action pruning. This is due in part to the\ndifficulty in establishing bounds that are tight while being sound.\n\n3.3 NOISY COMBINATION FUNCTION\nWe aim to evaluate how Q-M would perform under noisy combination functions and how noise\naffects its performance. We used the same setting as in Randomized MDP\\R and Randomized\nRewards above. We consider a situation where the combination function is not exactly known but can\nbe modeled by using a noise component: R = R1 + R2 + N . Assuming the knowledge of Nmin\n\n7\n\n324\n325\n\n326\n327\n328\n329\n330\n331\n\n332\n333\n334\n335\n336\n337\n\n338\n339\n340\n341\n342\n\n343\n344\n345\n346\n347\n348\n\n349\n350\n351\n352\n353\n354\n\n355\n356\n357\n358\n359\n360\n\n361\n362\n363\n364\n365\n\n366\n367\n368\n369\n370\n371\n\n372\n373\n374\n375\n376\n377\n\n\fUnder review as a conference paper at ICLR 2025\n\n378\n379\n\n380\n381\n382\n383\n384\n385\n\n386\n387\n388\n389\n390\n391\n\n392\n393\n394\n395\n396\n\n397\n398\n399\n400\n401\n402\n\n403\n404\n405\n406\n407\n408\n\n409\n410\n411\n412\n413\n414\n\n415\n416\n417\n418\n419\n\n420\n421\n422\n423\n424\n425\n\n426\n427\n428\n429\n430\n431\n\nFigure 6: Convergence plots with auto-generated domains and a noisy combination function.\n\nand Nmax, we updated the initializations and Bellman updates for Q-M. The convergence plots are\npresented in Fig. 6 where the noise levels with respect to the mean of the rewards was labeled at the\ntop. As expected, we observe that noise has an impact on the efficacy of Q-M: the more noise, the\nsmaller the performance gain with respect to the baselines. However, it is promising to observe that\nQ-M can still be effective under such noisy situations since it can greatly expand the applicability of\nQ-M. For instance, when the functional relationship is unknown, we can apply regression to fit the\nsource reward functions to the observed target rewards under an assumed functional form based on\ndomain expertise; noise can be incorporated to handle regression error.\n\n3.4 DOMAINS WITH CONTINUOUS STATE SPACES\n\nFigure 7: Convergence plots for domains with continuous state spaces.\n\nIn environments with continuous state spaces, we applied both Q-M and SFQL with discretized state\nspaces based on tile-coding, where each feature is discretized to produce the state space. The source\nQ-functions are also discretized with values determined according to the midpoint of each discrete\nstate. For Q-M, we also maintained a fixed number of reachable states from any state and action pair\n(assumed to be given or learned from training source behaviors) to compute the Bellman updates.\nWe used Deep Q-Network (DQN) as the underlying learning method after initializing learning for\nboth Q-M and SFQL. During learning in Q-M, pruned actions in a discrete state are not considered\nfor any state belonging to that state. Convergence plots are presented in Fig. 7. We observe that\nQ-M (QM-DQN) performs only marginally better than the baselines in Cartpole and Lunar Lander,\nsuggesting that discretization has a significant negative impact on the performance of Q-M. This is\nexpected since discretization has the effect of adding substantial “noise” to the Q functions. It is\nhowever encouraging to see that Q-M in such cases seems to have avoided pruning out the optimal\nactions. In Pong, SF-DQN outperformed both QM-DQN and DQN. This was due to the choice of\nsource behaviors that are either keeping left or right. The target behavior requires the agent to move\nto the left and right to catch the ball, which shares strong similarity with the source behaviors.\n\n4 RELATED WORK\nReward and Q-Decomposition: The combination function in Q-M can be viewed in general as\nspecifying a structure of the target reward function based on the source functions. Reward structure\ncan significantly influence the effectiveness of an RL agent as discussed in Silver et al. (2021). Prior\napproaches such as Lin et al. (2019); Marthi (2007); Ciardo and Trivedi (1993) have suggested novel\nways to exploit reward structure and decompose the reward function to better learn. For example,\nQ-Decomposition as described by Russell and Zimdars (2003) involves a similar setting to ours\n\n8\n\n\fUnder review as a conference paper at ICLR 2025\n\n432\n433\n\n434\n435\n436\n437\n438\n439\n\n440\n441\n442\n443\n444\n445\n\n446\n447\n448\n449\n450\n\n451\n452\n453\n454\n455\n456\n\n457\n458\n459\n460\n461\n462\n\n463\n464\n465\n466\n467\n468\n\n469\n470\n471\n472\n473\n\n474\n475\n476\n477\n478\n479\n\n480\n481\n482\n483\n484\n485\n\nwhere it aims to learn a behavior under a reward function that is the linear sum of multiple sub-reward\nfunctions. Each sub-agent for such a sub-reward function undergoes its own learning process and\nsupplies its Q values to an aggregator. The idea has also been extended to work with Deep Q Networks\n(DQN) by Van Seijen et al. (2017). There, it is argued that reward decomposition enables faster\nlearning as separate value functions only depend on a subset of input features, resulting in simpler\ndomains. Similar ideas are developed in Sutton et al. (2011); Sprague and Ballard (2003). While these\nideas are inspirational to ours, they are mostly for learning from scratch. No transfer is considered.\n\nMulti-Objective Reinforcement Learning: Multi-Objective Reinforcement Learning (MORL)\nas described in Liu et al. (2014); Sprague and Ballard (2003); Roijers et al. (2013); Vamplew\net al. (2011) is a branch of RL that deals with learning trade-offs between multiple objectives. A\ncommon approach to MORL is to search for the Pareto frontier, which is generally infeasible. A\nmore practical way to combine the objectives uses linear scalarization as discussed by Van Moffaert\net al. (2013). Often, the domain expert decides the weights for the objectives. Limitations have been\nreported by Vamplew et al. (2008) and solutions to counter them are proposed such as using the\nChebyshev function. Our problem setting can be considered as a special case of MORL where the\ndifferent objectives must be combined in complex ways. However, our focus is on improving sample\ncomplexity during learning by utilizing the existing behaviors for the individual objectives.\n\nHierarchical Reinforcement Learning: Hierarchical RL (HRL) as discussed in Dietterich (1998);\nVezhnevets et al. (2017); Barreto et al. (2020); Bacon et al. (2017); Barto and Mahadevan (2003);\nXiaoqin et al. (2009); Cai et al. (2013); Doroodgar and Nejat (2010) is the process of learning based\non a hierarchy of behaviors that is often assumed to be known or learned. A hierarchical structure\nmakes it possible to divide a learning problem into sub-problems, sometimes in a recursive manner.\nAt any point in time, a hierarchy of behaviors may be activated and the behavior at the lowest level\ndetermines the output behavior. In HRL, the interaction between the behaviors is often assumed to\nbe simple, i.e., sequential execution, since they are considered to address different parts of the state\nspace. In contrast, the source and target behaviors in our work share the same state and action spaces\nand their interactions can be arbitrarily complex via the correlations between their reward functions.\n\nTransfer Learning and Multi-Task Learning: Transfer learning, with various applications such as\nthose described in Andreas et al. (2016); Bahdanau et al. (2016); Chang et al. (2015), is the process\nof learning a target task by leveraging experiences from source tasks. As a transfer learning method\nfor reinforcement learning, multi-task reinforcement learning surveyed in Vithayathil Varghese and\nMahmoud (2020) deals with learning from multiple related tasks simultaneously to expedite learning.\nIn D’Eramo et al. (2019), for instance, individual learning agents learn from a related task and share\ntheir weights with the global network at regular intervals. The global network also periodically\nshares its parameters with individual learning agents. Our approach also deals with knowledge\ntransfer from the source to the target domains. However, it represents the class of indirect transfer\nmethods where the agent must “infer” useful information from the given information (i.e., source\nbehaviors) before using it. Furthermore, in contrast to domain adaptation discussed in Peng et al.\n(2018); Eysenbach et al. (2020) for addressing the sim-to-real gap, reward adaptation is more about\ntransferring knowledge between different tasks (i.e., reward functions).\n\n5 CONCLUSIONS\nIn this paper, we introduced reward adaptation, the problem where the learning agent adapted to a\ntarget reward function based on the existing source behaviors under the same MDP \\R. We proposed\nan approach to reward adaptation, referred as Q-Manipulation (Q-M). The key was to maintain Q\nvariants for each of the source behaviors and apply Q-M iterations to compute bounds of the target Q\nfunction and their initializations for action pruning before learning the target behavior. We formally\nproved that our approach converged and retained optimality under correct initializations. Empirically,\nwe showed that Q-M was substantially more efficient than the baselines in domains where the source\nand target behaviors differ, and generalizable under different randomizations. We also applied Q-M\nto noisy combination functions and continuous state spaces to extend its applicability. As such, Q-M\nrepresents a valuable contribution to advancing transfer learning for reinforcement learning. It is\nworth mentioning that, given its unique way of knowledge transfer, Q-M can be combined with other\napproaches (such as SFQL) to further improve learning. Our work also opens up many future research\nopportunities, such as addressing continuous state and action spaces and handling different domain\ndynamics (in addition to difference in reward functions) as in domain adaptation.\n\n9\n\n\fUnder review as a conference paper at ICLR 2025\n\n486\n487\n\n488\n489\n490\n491\n492\n493\n\n494\n495\n496\n497\n498\n499\n\n500\n501\n502\n503\n504\n\n505\n506\n507\n508\n509\n510\n\n511\n512\n513\n514\n515\n516\n\n517\n518\n519\n520\n521\n522\n\n523\n524\n525\n526\n527\n\n528\n529\n530\n531\n532\n533\n\n534\n535\n536\n537\n538\n539",
    "abstract": "In this paper, we propose a new solution to reward adaptation (RA), the problem where the learning agent adapts to a target reward function based on one or multiple existing behaviors learned a priori under the same domain dynamics but different reward functions. \nRA has many applications, such as adapting an autonomous driving agent that can already operate either fast  (if transporting goods) or comfortable  (if carrying passengers) to operating both fast and comfortable (if transporting goods with human passengers onboard). Learning the target behavior from scratch is possible but often inefficient given the available source behaviors. Our work represents a new approach to RA\nvia the manipulation of Q-functions.  Assuming that the target reward function is a known function of the source reward functions, our approach to RA  computes bounds of the Q function. We introduce an iterative process to tighten the bounds, similar to value iteration. This enables action pruning in the target domain before learning even starts. We refer to such a method as Q-Manipulation (Q-M). We formally prove that our pruning strategy does not affect the optimality of the returned policy while empirically show that it improves the sample complexity. Comparison with baselines is performed in a variety of synthetic and simulation domains to demonstrate its effectiveness and generalizability.",
    "response": {
      "Soundness": 2,
      "Presentation": 2,
      "Contribution": 2,
      "Rating": 5,
      "Confidence": 3,
      "Strengths": "+ Theoretical analysis on the Q bound",
      "Weaknesses": "+ Problem Definition: The problem the authors aim to address is not clearly described. In Definition 1, reward adaptation is defined as the task of learning an optimal policy for a target reward function, given a set of behaviors trained under source reward functions. However, it is unclear whether the agent still has access to the underlying MDP when learning the target policy. Is the proposed method an offline algorithm where the target policy is learned only based on the source data?\n\n+ Assumption. The assumption that the target reward function is a known function of the source rewards (e.g., a linear combination) is not well-motivated and seems overly restrictive. Additionally, the experimental domains are not well-described, making it difficult for readers to interpret this assumption. For instance, in Race Track, $R_3$ assigns a positive reward (+3) for remaining at the initial location.. This is a relatively large reward compared to others in $R_1$ and $R_2$. In the target domain when $\\mathcal{R}=R_1+R_2+R_3$, factors like the distance between the goal and initial location, maximum episode length, and discount factor significantly influence policy behavior. In extreme cases, such as with a short maximum episode length and low discount factor, the optimal policy could be to remain at the initial location.\n\n+ Plots. For the Q-M method, pre-training and pre-computation of Q-functions under the source reward are required. However, in the visualizations, it appears that the computational costs associated with this stage are not included in the plots. This omission raises concerns about the fairness of the comparisons.",
      "Questions": "When the target reward is a linear combination of the source rewards, this setup appears quite similar to multi-objective reinforcement learning (MORL), where each objective is summed linearly. Could you clarify if there is any relationship between your method and multi-objective RL?"
    }
  },
  {
    "paper_id": "yj9lLwMjnE",
    "summary": "Background: In recent years, pre-training and representation learning have become critical in speech processing, primarily focusing on either discriminative tasks (e.g., speech recognition) or generative tasks (e.g., speech synthesis), with each requiring different foundational models. Predominantly, existing techniques focus on self-supervised methods for masked audio modeling, targeting specific tasks and leaving gaps for unified models.\nGap: The gap identified by the authors is the lack of a unified pre-training model that can effectively serve both discriminative and generative speech processing tasks. Existing models are typically specialized for one task type, leading to inefficiencies and increased pre-training costs.\nHypothesis: The authors propose that it is feasible to create a single pre-trained model, which they call UniWav, that can learn both representation encoding and generative audio decoding. This unified model would serve both discriminative and generative tasks effectively, potentially replacing numerous task-specific models.\nMethod: The proposed method, UniWav, utilizes an encoder-decoder framework. The encoder is trained using masked audio modeling with a pseudo-labeling approach, where the teacher model is an exponential moving average of the encoder to provide pseudo-labels. The decoder is trained using Flow Matching conditioned on a weighted sum of encoder representations. UniWav is trained on the LibriLight dataset with both components being trained jointly from scratch, using a Transformer architecture.\nConclusion: UniWav demonstrates that a unified model for both speech recognition and generation is possible. It achieves performance on par with existing task-specific models, reducing the need for separate foundation models for each task. This approach shows potential in lowering the computational cost and complexity in deploying speech processing systems.\n",
    "full text": "\nUNIWAV: TOWARDS UNIFIED PRE-TRAINING FOR\nSPEECH REPRESENTATION LEARNING AND GENERA-\nTION\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nPre-training and representation learning have been playing an increasingly im-\nportant role in modern speech processing. Nevertheless, different applications\nhave been relying on different foundation models, since predominant pre-training\ntechniques are either designed for discriminative tasks or generative tasks. In\nthis work, we make the first attempt at building a unified pre-training framework\nfor both types of tasks in speech. We show that with the appropriate design\nchoices for pre-training, one can jointly learn a representation encoder and gen-\nerative audio decoder that can be applied to both types of tasks. We propose\nUniWav, an encoder-decoder framework designed to unify pre-training represen-\ntation learning and generative tasks. On speech recognition, text-to-speech, and\nspeech tokenization, UniWav achieves comparable performance to different ex-\nisting foundation models, each trained on a specific task. Our findings suggest\nthat a single general-purpose foundation model for speech can be built to replace\ndifferent foundation models, reducing the overhead and cost of pre-training. Audio\ndemo page: https://uniwav-demo.github.io/\n\n1\n\nINTRODUCTION\n\nRepresentation learning and generative modeling have seen rapid growth in speech processing in\nrecent years. Researchers have been consistently setting new records on discriminative tasks with\nrepresentation learning (Baevski et al., 2020b; Hsu et al., 2021; Baevski et al., 2022; Liu et al.,\n2024). Existing speech representations focus on excelling at a specific task. Mainstream pre-training\nmethods (Schneider et al., 2019; Baevski et al., 2020a;b; Hsu et al., 2021), also known as self-\nsupervised learning, learn speech representation from massive unlabeled speech. While there are\nmany different prior works in the field, they all shared the same spirit in masked audio modeling, i.e.,\nmatching the output extracted from masked audio to that from unmasked audio. These representations\ntend to capture the phonetic or semantic content of speech, leaving out acoustic information such as\nspeaker’s timbre and the acoustic environment. Consequentially, self-supervised representations have\ndominated classification and recognition tasks (Yang et al., 2021) in speech but not for generation\ntasks (Tsai et al., 2022). In fact, recent research (Wang et al., 2023) argues that self-supervised\nrepresentations (Hsu et al., 2021) are sub-optimal for speech synthesis.\n\nOn the other hand, generative models have also gained more attention in recent years. State-of-the-art\nmodels are capable of generating realistic speech with arbitrary voice and text (Wang et al., 2023;\nLe et al., 2023), also supporting a wild range to generation tasks beyond text-to-speech (Wang et al.,\n2024). Similar to representation learning, pre-training methods for speech generative models have\nalso been developed (Liu et al., 2023; Vyas et al., 2023) based on masked audio modeling. These\nmodels also are pre-trained on unlabeled data and can be easily transferred to different downstream\ntasks with minimal supervision.\n\nInterestingly, while the two families of models are similar in many ways — e.g., training transformers,\napplying masked audio modeling, pre-training with unlabeled data, etc. — they are normally designed\nand trained separately.\n\nIdeally, speech representation learning and generation should be complementary, as representation\nlearning can be used to guide generation, and generation can be used to ensure that the learned repre-\n\n1\n\n000\n001\n\n002\n003\n004\n005\n006\n007\n\n008\n009\n010\n011\n012\n013\n\n014\n015\n016\n017\n018\n\n019\n020\n021\n022\n023\n024\n\n025\n026\n027\n028\n029\n030\n\n031\n032\n033\n034\n035\n036\n\n037\n038\n039\n040\n041\n\n042\n043\n044\n045\n046\n047\n\n048\n049\n050\n051\n052\n053\n\n\fUnder review as a conference paper at ICLR 2025\n\n054\n055\n\n056\n057\n058\n059\n060\n061\n\n062\n063\n064\n065\n066\n067\n\n068\n069\n070\n071\n072\n\n073\n074\n075\n076\n077\n078\n\n079\n080\n081\n082\n083\n084\n\n085\n086\n087\n088\n089\n090\n\n091\n092\n093\n094\n095\n\n096\n097\n098\n099\n100\n101\n\n102\n103\n104\n105\n106\n107\n\nFigure 1: An overview of UniWav. The encoder is trained with masked audio modeling and pseudo-\nlabel obtained through a teacher model. The teacher model is the exponential moving average (EMA)\nof the encoder. The decoder is trained with Flow Matching conditioned on z the weighted sum of\nrepresentations of different encoder layers. All modules are trained jointly from scratch.\n\nsentation encodes sufficient information. However, combining them is challenging due to conflicting\nmethods and arguably opposing requirements for each task (Baevski et al., 2022). For example,\nwhile speech recognition tasks can benefit from a representation that is invariant to speaker and\nenvironment (Schneider et al., 2019; Hsu et al., 2021), speech synthesis tasks require a representation\nthat provides information about speaker and environment (Wang et al., 2023; Zhang et al., 2023b).\n\nNevertheless, there are successful examples on text (Devlin et al., 2018; Radford et al., 2019) and\nimage (Li et al., 2023; Xiang et al., 2023; Chen et al., 2024) that described unified foundation models\nfor representation learning and generation. These prior works showed that a unified pre-training\nframework for representation learning and generation can reduce the overhead of training, while\naccommodating both discriminative and generative downstream applications with strong performance.\n\nOur proposed framework, UniWav, is the first to present a unified pre-training framework for speech\nthat is efficient on both discriminative and generative tasks. It is an encoder-decoder framework,\nintegrating a representation encoder and a Flow Matching (Lipman et al., 2023) decoder condi-\ntioning on the learned representations. The representation encoder is guided simultaneously by\nself-distillation (Baevski et al., 2022; Liu et al., 2024) and the generative objective through the\ndecoder. By training the encoder and decoder jointly in an end-to-end manner from scratch, we build\nthe first foundation model that excels in both speech recognition and generation. In our experiments,\nwe fine-tune the pre-trained model for speech recognition and in-context text-to-speech synthe-\nsis, achieving results comparable to state-of-the-art methods in each task. Additionally, we adapt\nUniWav for speech tokenization, demonstrating the significant benefits of joint representation and\ngeneration learning by achieving high audio quality at a low bitrate. Finally, we provide observations\nand insights on unified pre-training through an ablation study and analysis, aiming to illuminate the\npath toward a single foundation model for general speech processing.\n\nOur contribution can be highlighted as follows:\n\n• We introduce, UniWav, the first unified pre-training framework for speech representation\n\nlearning and generation.\n\n• On speech recognition and in-context text-to-speech generation, we show UniWav can\n\ncompete with different foundation models within each task through fine-tuning.\n\n• By introducing a discrete bottleneck between the encoder and decoder, UniWav achieves low-\nbitrate speech tokenization and high-quality resynthesis, outperforming existing methods.\n\nIn summary, UniWav marks the first step toward unified pre-training for both representation learning\nand generation, laying the foundation for more versatile pre-trained models in speech processing.\n\n2 METHOD\n\nFigure 1 provides an overview of UniWav, an encoder-decoder framework designed to unify pre-\ntraining for representation learning and generative tasks. The model takes the surface feature (e.g.,\n\n2\n\nDecoder(§2.2)Random maskingFlow MatchingCross EntropyEncoder(§2.1)Teacher(EMA)𝜙𝑡𝑂𝑇𝑧𝑥Pseudo-label\fUnder review as a conference paper at ICLR 2025\n\nEncodec (D´efossez et al., 2022) latent or mel spectrogram) of speech as input. Through masked audio\nmodeling, the model jointly learns to encode input into latent representations and generate speech.\nBelow, we detail encoder design in §2.1, decoder in §2.2, and summarize the pre-training framework\nin §2.3.\n\n2.1 REPRESENTATION ENCODER\n\nThe key to learning representations from unlabeled data is to derive good learning targets from\nthe input itself, commonly referred to as self-supervised learning. While there are different self-\nsupervised learning methods in speech (see related works §4), we adopt self-distillation and online\nclustering from DinoSR (Liu et al., 2024) in this work.\nGiven the sequence of input audio frames x = (x1, x2, . . . , xL) where xi ∈ Rdx is the input surface\nfeature frame of speech, we randomly mask a subset of the frames, denoted M , and feed them into the\nTransformer encoder (Vaswani et al., 2017) to derive output latent embeddings z = (z1, z2, ..., zL)\nwhere zi ∈ Rd. The goal of the encoder is to predict the pseudo label yi provided by a teacher model\nby maximizing\n\ni:xi∈M\nwhere ϕ is the pseudo-label prediction head (linear layer followed by softmax activatiton).\n\n1\n|M |\n\n(cid:88)\n\nlog pϕ(yi|zi),\n\n(1)\n\nThe teacher model for deriving pseudo-label is the exponential moving average (EMA) of the encoder\nitself, i.e., θteacher ←− γteacher θteacher + (1 − γteacher) θencoder where γteacher is the decay factor. While\nboth networks are randomly initialized and set to be identical at the beginning of training, only the\nencoder takes masked audio as input. In contrast, the teacher model takes complete audio (without\nmasking) as the input to derive the pseudo-label\n\nyi = argmin\n\nv∈V\n\n∥˜zi − ev∥2\n\n(2)\n\nfor input frame xi, where ˜zi is the output of the teacher model and ev is a codeword from a codebook\nof size V . All codewords within the codebook are also updated by taking the EMA of the teacher\noutput embeddings that share the same pseudo-label, i.e.,\n\nsv ←− γcode sv + (1 − γcode)\n\nnv ←− γcode nv + (1 − γcode)\n\nev ←−\n\nsv\nnv\n\n,\n\n(cid:88)\n\ni:yi=v\n\n(cid:88)\n\ni:yi=v\n\n˜zi,\n\n1,\n\n(3)\n\nwhere γcode is the decay factor, the cluster sum sv ∈ Rd is randomly initialized, and the cluster size\nnv is initialized to 1.\n\nFollowing DinoSR, we consider pseudo-labels from all the top K layers of the teacher model by\nrepeating Eq.(1),(2), and (3) for each layer. The complete loss function of the encoder is therefore\n(cid:88)\n\n(cid:88)\n\nlog pϕk (y(k)\n\ni\n\n|zi),\n\n(4)\n\nLencoder = −\n\n1\n|K| |M |\n\ni:xi∈M\nwhere K is the set of indices of the teacher model layer to be considered, each with an independent\ncodebook. Note that the prediction is made at the final layer, using zi, of the encoder regardless of\nthe target layer k.\n\nk∈K\n\n2.2 FLOW MATCHING DECODER\n\nWe choose to train our decoder with Flow Matching (Lipman et al., 2023), which has been shown\nto be highly effective and generalizable in speech generation tasks (Le et al., 2023; Liu et al., 2023;\nKim et al., 2024; Mehta et al., 2024).\n\n1 of an unknown\nFormally, Flow Matching considers the problem of learning to generate samples x1\ndata distribution q(x) by predicting the flow of x along the path between a prior distribution p0 and\n\n1In this subsection, we denote the sequence of audio frames as x1, i.e., x1 := (x1, x2, . . . , xL), to follow\n\nthe convention in the literature.\n\n3\n\n108\n109\n\n110\n111\n112\n113\n114\n115\n\n116\n117\n118\n119\n120\n121\n\n122\n123\n124\n125\n126\n\n127\n128\n129\n130\n131\n132\n\n133\n134\n135\n136\n137\n138\n\n139\n140\n141\n142\n143\n144\n\n145\n146\n147\n148\n149\n\n150\n151\n152\n153\n154\n155\n\n156\n157\n158\n159\n160\n161\n\n\fUnder review as a conference paper at ICLR 2025\n\nthe target distribution p1 ≈ q. The flow is defined using ordinary differential equation (ODE):\n\nd\ndt\n\nϕt(x) = vt(ϕt(x)); ϕ0(x) = x;\n\n(5)\n\nwith the time-dependent vector field vt : [0, 1] × Rd → Rd. Through the change of variable formula,\nwe can generate the distribution pt at any timestep t along the path, allowing us to sample p1 by\nsolving Eq.(5) iteratively from t = 0 to t = 1.\n\nUnder the framework, the goal of our decoder is to predict the time-dependent vector field\nvt(x; θdecoder) that matches the target vector field ut that generates the path. While the frame-\nwork is simple, na¨ıve Flow Matching - regressing decoder output to ut - is intractable since the\ntarget vector field ut is generally unknown in practice. Fortunately, Lipman et al. (2023) showed\nthat we can leverage real data sample x1 to construct the conditional flow of x with tractable pt\nand ut. Specifically, the Optimal Transport (OT) conditional path is constructed by assuming the\nmean µt(x) = tx1 and standard deviation σt(x) = 1 − (1 − σmin)t to change linearly in time, with a\nsufficiently small σmin such that p1(x|x1) is centered around x1. This results in the OT conditional\nflow\n\nϕOT\nt\n\n(x) = (1 − (1 − σmin)t)x0 + tx1\n\n(6)\n\nwith tractable pt(x|x1) = N (x | µt(x1), σt(x1)2I) and ut(x|x1) = (x1−(1−σmin)x)\nConditional Flow Matching (CFM) objective\n\n(1−(1−σmin)t) , deriving the\n\nEt,q(x1),p0(x0)\n\n(cid:13)\n(cid:13)vt(ϕOT\n(cid:13)\n\nt\n\n(x0); θdecoder) −\n\n(cid:16)\n\nx1 − (1 − σmin)x0\n\n(cid:17)(cid:13)\n2\n(cid:13)\n(cid:13)\n\n,\n\n(7)\n\nwhere p0 is the standard normal distribution and t is sampled uniformly from [0, 1]. It is worth\nmentioning that Lipman et al. (2023) proved the CFM objective and the na¨ıve Flow Matching share\nidentical gradient w.r.t. the model, therefore Eq.(5) can be solved with the model trained with CFM.\n\nWe additionally condition the decoder on the weighted sum of the encoder layer features z, yielding\nthe complete loss function for the decoder:\n\nLdecoder = Et,q(x1),p0(x0)\n\n(cid:13)\n(cid:13)vt(ϕOT\n(cid:13)\n\nt\n\n(x0), z; θdecoder) −\n\n(cid:16)\n\nx1 − (1 − σmin)x0\n\n(cid:17)(cid:13)\n2\n(cid:13)\n(cid:13)\n\n.\n\n(8)\n\nSimilar to the representation encoder, the generative decoder is also implemented with a Transformer\nencoder with d-dimensional latent. The input of the decoder is\n\nW0vt(ϕOT\n\nt\n\n(x0)) + z, where z =\n\nWiz(i),\n\n(cid:88)\n\ni\n\n(9)\n\nW0 ∈ Rdx×d is the noisy data projection layer, and Wi ∈ Rd×d is the linear projection layer for\nthe representation z(i) from the i-th layer of the encoder. In other words, the decoder takes the\nnoise-injected audio feature x and a learnable combination of representations from all encoder layers\nz as input.\n\n2.3 UNIWAV FRAMEWORK\n\nCombining the encoder objective Eq.(4) and the decoder objective Eq.(8), UniWav is trained from\nscratch in an end-to-end manner with the overall objective\n\nLencoder + λ Ldecoder,\n\n(10)\n\nwhere λ is the weighting scalar. In practice, we found downstream performance less sensitive to λ\nbut a sufficiently small value stabilizes pre-training. We use λ = 0.25 throughout this work. Note\nthat for both encoder and decoder, we only compute loss at the masked positions.\n\nUnlike prior works that rely on iteratively learning representation from teacher models (Hsu et al.,\n2021; Chen et al., 2022a), adopting self-distillation and online clustering allows training the encoder\nfrom scratch together with the decoder. In practice, we also tested self-distillation without cluster-\ning (Baevski et al., 2022) and distilling from a pre-trained HuBERT (Hsu et al., 2021), but found the\noverall encoder-decoder training less stable. For the decoder, we tested reconstructing discrete code\nfrom Encodec (D´efossez et al., 2022) and found Flow Matching better. We leave exploring more\ncombinations of the encoder and decoder as an important future work.\n\n4\n\n162\n163\n\n164\n165\n166\n167\n168\n169\n\n170\n171\n172\n173\n174\n175\n\n176\n177\n178\n179\n180\n\n181\n182\n183\n184\n185\n186\n\n187\n188\n189\n190\n191\n192\n\n193\n194\n195\n196\n197\n198\n\n199\n200\n201\n202\n203\n\n204\n205\n206\n207\n208\n209\n\n210\n211\n212\n213\n214\n215\n\n\fUnder review as a conference paper at ICLR 2025\n\n216\n217\n\n218\n219\n220\n221\n222\n223\n\n224\n225\n226\n227\n228\n229\n\n230\n231\n232\n233\n234\n\n235\n236\n237\n238\n239\n240\n\n241\n242\n243\n244\n245\n246\n\n247\n248\n249\n250\n251\n252\n\n253\n254\n255\n256\n257\n\n258\n259\n260\n261\n262\n263\n\n264\n265\n266\n267\n268\n269\n\nAfter pre-training, the encoder is a stand-alone module that can be used to extract representations\nsimilar to those of prior works. We evaluate the representation encoder with speech recognition in\n§3.2 and speech tokenization in §3.4. Additional representation analysis is presented in §3.5. For\ngeneration, the decoder can be fine-tuned using desired conditions such as text and audio prompt\n(§3.3) and discretized representations (§3.4).\n\n3 EXPERIMENTS\n\n3.1 PRE-TRAINING SETUP\n\nModel Both the encoder and the decoder follow the Transformer architecture (Vaswani et al., 2017),\nwith 16 attention heads, hidden size d = 1024, and feed-forward networks with 4096 dimensions. For\nthe encoder, positional information is encoded using convolutional positional embedding (Baevski\net al., 2020a). We add skip connections between decoder layers to mimic the U-Net (Ronneberger\net al., 2015) architecture, following prior works (Le et al., 2023; Liu et al., 2023). Both the encoder\nand the decoder use the ALiBi self-attention bias (Press et al., 2021). We stack 24 layers for the\nencoder and 12 layers for the decoder. To condition the Flow Matching decoder on ODE timestep t, a\nsinusoidal embedding of t is prepended to the input sequence of the decoder. The total number of\nlearnable parameters is around 500M, where the encoder accounts for 60% of the size. Studies on the\nsize of the encoder and the decoder are presented in §3.5.\n\nData & Input Feature UniWav is trained on LibriLight (Kahn et al., 2019), an audiobook corpus\nwith around 60k hours of untranscribed English speech sampled at 16kHz. We extract the pre-\nquantized latent feature from a 16kHz EnCodec (D´efossez et al., 2022) encoder, pre-trained on\nthe same dataset, to serve as the input for our model. The input feature is comprised of 50Hz\n128-dimensional vectors, normalized to zero mean and unit variance overall. For generation tasks,\nthe features sampled by the decoder can be projected back to waveform using the EnCodec decoder.\nChoice of surface feature is studied in §A.1 by replacing EnCodec latent with mel spectrogram.\n\nHyper-parameters & Training details For the encoder teacher model, γteacher is set to increase\nfrom 0.9997 to 1.0 in the first 400k updates, and γcode is set to 0.9. The top K = 10 layers of the\nteacher model are considered a learning target for the encoder, and the representation from each\nlayer is clustered to a codebook of size V = 256, following Liu et al. (2024). For masking, each\ninput frame has an 8% chance of being replaced with a learnable mask embedding. We mask 10\nconsecutive frames if a frame is sampled to be masked. We set the decoder loss weight λ to 0.25.\nUniWav is trained using Adam optimizer (Kingma & Ba, 2014) with a cosine learning rate schedule\npeaking at 2e-4 for a total of 600k updates, including 10k warmup steps. The batch size is 312.5\nseconds per GPU, and samples are randomly cropped to cap at 20 seconds. The model is trained with\nbf16 and gradient clipping of 1.0. Pre-training is done on 16 H100 GPUs taking around 9 days.\n\n3.2 SPEECH RECOGNITION\n\nSetup We follow the prior works in self-supervised representation learning (Baevski et al., 2020a;\nHsu et al., 2021; Chen et al., 2022a; Baevski et al., 2022) to evaluate the UniWav encoder on speech\nrecognition. A linear layer for letter prediction is attached to the final output layer of encoder, and the\nwhole encoder is fine-tuned with CTC (Graves et al., 2006). In alignment with the prior works, we\nfine-tune our encoder using either 960 hours of transcribed speech from LibriSpeech (Panayotov et al.,\n2015) or 100 hours from its train-clean-100 subset. We use the open-sourced fine-tuning and\ndecoding script from Baevski et al. (2020a), and perform a hyperparameter search over learning rate,\nmask probability, mask length, and input channel dropout probability. For decoding, the LibriSpeech\n4-gram language model is used.\n\nResults We report word error rate (WER) on the test-other subset in Table 1. The clear gap\nbetween random initialized and pre-trained weights demonstrated the effectiveness of pre-training.\nComparing UniWav with prior works on representation learning, we note that our model falls short\nof state-to-the-art by around 0.5% WER. Nevertheless, we note that prior works in this direction\nfocused exclusively on discriminative tasks, and none of the prior works supported generation tasks.\nThe speech recognition experiment confirmed that the encoder of UniWav can effectively function\n\n5\n\n\fUnder review as a conference paper at ICLR 2025\n\nTable 1: Comparison of speech foundation models. For speech recognition, we report word error rate\n(WER) on the test-other subset of the Librispeech (Panayotov et al., 2015) dataset when fine-tuning\nwith 960 hours or 100 hours of labeled data. For speech synthesis, we report ASR-measured WER\n(ASR-WER) and speaker similarity (Sim.) for in-context text-to-speech.\n\nPre-training\n\nSpeech Recognition\n\nSpeech Synthesis\n\ndata (hr)\n\n100hr\n\n960hr\n\ndata ASR-WER\n\nSim.\n\nSelf-supervised Representation Models\n\nHuBERT (Hsu et al., 2021)\nWavLM (Chen et al., 2022a)\ndata2vec (Baevski et al., 2022)\ndata2vec 2.0 (Baevski et al., 2023)\n\n60k\n94k\n60k\n60k\n\nGenerative Models\n\n4.5\n4.6\n4.6\n4.3\n\n3.7\n-\n3.7\n3.5\n\n-\n-\n-\n-\n\n-\n-\n-\n-\n\n-\n-\n-\n-\n\nVALL-E (Wang et al., 2023)\nVoicebox† (Le et al., 2023)\nSpeechFlow† (Liu et al., 2023)\nUniWav† w/o pre-training\nUniWav†\n† Inference with time-aligned phone sequence using force alignment (McAuliffe et al., 2017); 32 function evaluations.\n\n0.508\n0.616\n0.629\n\n60k\n60k\n960\n\n0\n0\n60k\n\n0.532\n0.635\n\n3.8\n2.3\n2.4\n\n17.5\n4.8\n\n960\n960\n\n0\n60k\n\n5.8\n4.0\n\n2.6\n2.5\n\n-\n-\n-\n\n-\n-\n-\n\nas the backbone for discriminative tasks, even when pre-trained with the additional demands of a\ngenerative objective.\n\n3.3 SPEECH SYNTHESIS\n\nSetup Next, we evaluate the speech generation ability of UniWav against existing foundation\nmodels on in-context text-to-speech (Wang et al., 2023; Le et al., 2023; Liu et al., 2023). We fine-tune\nUniWav on LibriSpeech to synthesize speech, conditioned on text and audio prompts that provide\nspeaker information. For text encoding, we use input-frame-aligned phone labels with position\npostfixes (Le et al., 2023). Phone embeddings are randomly initialized and added to the model input.\nFor audio prompts, we directly use the encoder representation (z in Eq.(8)), where 70% to 100%\nof the sequence is randomly masked with a single mask. We also drop both audio prompt and text\ncondition with a 20% chance for classifier-free guidance (CFG; Dhariwal & Nichol, 2021; Le et al.,\n2023).\n\nWe fine-tune our model with 1e-5 learning rate for 150k steps with logit-normal time sampling (Esser\net al., 2024) on 8 A100 GPUs. Note that since our goal is to evaluate acoustic modeling, we use\nMontreal Force Aligner (McAuliffe et al., 2017) to obtain alignment for both training and evaluation\n(and apply the same for prior works that require alignment). For solving the initial value problems in\nEq.(5), we use the midpoint method from torchdiffeq (Chen, 2018). With a step size of 0.0625\nand CFG α = 1.9, the number of function evaluations (NFEs) is 32 per sample.\n\nFor evaluation, we adapt the protocol introduced by Wang et al. (2023) to perform speaker-conditioned\ntext-to-speech on the test-clean subset with 3-second enrollment. We follow Liu et al. (2023)\nto use fine-tuned HuBERT-L (Hsu et al., 2021) to measure word error rate for audio intelligibility\n(ASR-WER) and WavLM-TDCNN speaker embedding model (Chen et al., 2022a) to measure speaker\nsimilarity (Sim) for speaker consistency between the audio prompt and the result.\n\nResults We compared UniWav against prior works using the same setup in Table 1. Compared to\nVall-E (Wang et al., 2023) and Voicebox (Le et al., 2023) that relied on speech data with transcriptions,\nUniWav achieved better or similar results with significantly less data. Compared to the generative pre-\ntraining method SpeechFlow (Liu et al., 2023), our method shows equal performance in fine-tuning.\nThis finding is important because, unlike SpeechFlow, UniWav can also be applied for discriminative\ntasks like speech recognition. To ablate the effectiveness of our pre-training method, we report\nbaseline numbers without pre-training. Our result suggest that pre-training has more impact on the\nmodel’s acoustic modeling ability (measured by SIM) and less on the language modeling ability\n(measured by ASR-WER).\n\n6\n\n270\n271\n\n272\n273\n274\n275\n276\n277\n\n278\n279\n280\n281\n282\n283\n\n284\n285\n286\n287\n288\n\n289\n290\n291\n292\n293\n294\n\n295\n296\n297\n298\n299\n300\n\n301\n302\n303\n304\n305\n306\n\n307\n308\n309\n310\n311\n\n312\n313\n314\n315\n316\n317\n\n318\n319\n320\n321\n322\n323\n\n\fUnder review as a conference paper at ICLR 2025\n\nTable 2: Speech tokenization and resynthesis results. All models have a frame rate of 50Hz. Methods\nsharing the same bitrate are highlighted with the same color. Visualized audio samples are provided\nin §Appendix A.2 and audible samples are available on demo page.\n\nquantization\n\nbitrate\n\nASR-WER↓\n\nUTMOS↑\n\nSpkr. Sim.↑\n\nViSQOL↑\n\nGround Truth\n\n-\n\n256k\n\nEncodec a,f\n\nDAC b,f\n\nHuBERTBASE\nHuBERTLARGE\n\nc,g\n\nc,g\n\nSpeechTokenizerd,g\n\nUniWavg\n\nVQ1024\nRVQ2×1024\nRVQ8×1024\nVQ1024\nRVQ2×1024\nRVQ8×1024\nKM1024\nKM1024\nVQ1024\nRVQ2×1024\nRVQ8×1024\nKM1024\nKM2×1024\n\ne\n\n500\n1k\n4k\n\n500\n1k\n4k\n\n500\n500\n\n500\n1k\n4k\n\n500\n1k\n\ndev\n\n3.2\n\n41.7\n9.6\n3.8\n\n80.9\n22.8\n3.8\n\n9.8\n7.6\n\n10.5\n8.5\n4.5\n\n7.2\n5.6\n\ntest\n\n3.4\n\n41.0\n10.3\n4.0\n\n80.6\n24.2\n4.1\n\n10.2\n7.9\n\n11.0\n9.1\n4.9\n\n7.1\n5.6\n\ndev\n\ntest\n\ndev\n\ntest\n\ndev\n\ntest\n\n3.87\n\n1.24\n1.52\n2.94\n\n1.24\n1.28\n3.09\n\n2.36\n2.66\n\n1.25\n2.20\n3.65\n\n3.64\n3.79\n\n3.77\n\n1.24\n1.50\n2.82\n\n1.24\n1.28\n3.03\n\n2.37\n2.65\n\n1.26\n2.08\n3.56\n\n3.59\n3.72\n\n-\n\n0.25\n0.60\n0.89\n\n0.11\n0.33\n0.87\n\n0.14\n0.14\n\n0.16\n0.34\n0.83\n\n0.25\n0.49\n\n-\n\n0.25\n0.60\n0.89\n\n0.10\n0.31\n0.87\n\n0.15\n0.15\n\n0.16\n0.34\n0.83\n\n0.26\n0.50\n\n-\n\n2.96\n3.59\n4.25\n\n1.88\n2.95\n4.42\n\n2.14\n2.00\n\n1.97\n3.04\n4.22\n\n2.14\n2.96\n\n-\n\n2.88\n3.47\n4.21\n\n1.73\n2.70\n4.39\n\n2.15\n2.03\n\n2.03\n3.03\n4.20\n\n2.09\n2.92\n\naD´efossez et al. (2022); bKumar et al. (2024); c With unit-HifiGAN Polyak et al. (2021); Nguyen et al. (2023); dZhang et al.\n\n(2023b); e Residual quantization, see Eq.(11); f Trained on general audio; g Trained on LibriSpeech only;\n\n3.4 SPEECH TOKENIZATION AND RESYNTHESIS\n\nTask introduction In addition to purely discriminative and generative tasks, there are tasks that\nbridge the gap between the two, such as speech tokenization and resynthesis. With the recent\nadvancements in large language models, there has been growing interest in converting speech into\nsequences of discrete tokens (Zhang et al., 2023b; Zhu et al., 2023; Huang et al., 2023; Bai et al.,\n2024), allowing speech to be processed similarly to text. These tokens must also be capable of being\ndecoded back into audio signals, enabling token-based language models to generate speech.\n\nPrior works We consider four prior works Encodec (D´efossez et al., 2022), DAC (Kumar et al.,\n2024), unit-HiFiGAN based on HuBERT units (Hsu et al., 2021; Polyak et al., 2021; Nguyen\net al., 2023), and SpeechTokenizer (Zhang et al., 2023b). Encodec and DAC are general neural\naudio codec models that are not designed specifically for speech tokenization. HuBERT units and\nUnit-HiFiGAN (Polyak et al., 2021) is a cascaded system for speech tokenization and resynthesis.\nTokenization is done by running kmeans clustering on the representation extracted from a pre-\nselected layer of HuBERT. Resynthesis is done by training HiFiGAN (Kong et al., 2020) that takes\nspeech tokens as input. For Unit-HiFiGAN, we train the model using the codebase released by\nthe authors (Nguyen et al., 2023). In addition to HuBERTBASE used in the original work, we also\nreport results using HuBERTLARGE that matches the size of UniWav encoder. To ensure a consistent\nsetup, we used 1024 kmeans clusters and did not include speaker and emotion conditions to match\nother models. SpeechTokenizer is a neural audio codec model based on Encodec. In addition\nto audio reconstruction and adversarial training used in Encodec, SpeechTokenizer introduced an\nadditional loss to distill representation from a pre-trained HuBERT into the first vector quantization\nlayer. For SpeechTokenizer, we used the official model released by the authors for evaluation. Both\nmethods required hand-selecting a layer from HuBERT for quantization or distillation, which is done\nby identifying the layer that matches the underlying phone label the most. Prior works therefore\nreferred representations from the selected layer to as semantic representation. As a reference, both\nunit-HiFiGAN and SpeechTokenizer use layer 9 of HuBERTBASE to extract semantic representation,\nand we found layer 22 works best for HuBERTLARGE. While unit-HifiGAN, SpeechTokenizer, and\nour model are trained on LibriSpeech, Encodec and DAC are trained on a mixture of speech, sound,\nand music.\n\nFine-tuning UniWav UniWav can be naturally applied to the task by introducing a discrete\nbottleneck between the encoder and decoder. We follow the prior works to first identify the semantic\n\n7\n\n324\n325\n\n326\n327\n328\n329\n330\n331\n\n332\n333\n334\n335\n336\n337\n\n338\n339\n340\n341\n342\n\n343\n344\n345\n346\n347\n348\n\n349\n350\n351\n352\n353\n354\n\n355\n356\n357\n358\n359\n360\n\n361\n362\n363\n364\n365\n\n366\n367\n368\n369\n370\n371\n\n372\n373\n374\n375\n376\n377\n\n\fUnder review as a conference paper at ICLR 2025\n\nTable 3: Recognition WER of different\nmodel sizes. Results are on dev-other\nwith LM after 200k pre-training steps.\n\nTable 4: In-context TTS ASR-WER and speaker simi-\nlarity of different model sizes after 200k pre-training\nsteps. The default encoder/decoder depth is 24/12.\n\nencoder depth\n\n12\n24\n\ndecoder depth\n\n0\n\n7.6\n5.8\n\n12\n\n8.1\n5.5\n\n24\n\n8.2\n5.2\n\ndecoder depth\n\nencoder depth\n\n0\n\n12\n\n24\n\n12\n24\n\n3.4 / 0.45\n2.5 / 0.61\n\n2.5 / 0.60\n2.5 / 0.63\n\n2.5 / 0.63\n2.5 / 0.63\n\nlayer to run kmeans. More information can be passed to the decoder by quantizing the remaining layers\nat a cost of a higher bitrate for tokenization. More precisely, we replace the encoder representations z\nin Eq.(9) with\n\n¯z = Wi argmin\n\n(cid:13)\n(cid:13)z(i) − q\n(cid:13)\n(cid:123)(cid:122)\nsemantic quantization\n\nq∈Qi\n\n(cid:13)\n(cid:13)\n(cid:13)2\n(cid:125)\n\n(cid:124)\n\n(cid:88)\n\n(cid:13)\n(cid:13)\n(cid:13)\n\n+ argmin\nq∈Qresidual\n(cid:124)\n\nj̸=i\n(cid:123)(cid:122)\nresidual quantization (optional)\n\nWjz(j) − q\n\n,\n\n(cid:13)\n(cid:13)\n(cid:13)2\n(cid:125)\n\n(11)\n\nwhere i = 10 is the index of layer used for extracting semantic representation, Qi and Qresidual are the\nsets of centroids obtained through k-means clustering. We fine-tune the decoder with the quantized\nencoder representation on 8 A100 GPUs for 150k steps with a 1e-4 learning rate. For decoding, we\nuse a step size of 0.25 and CFG α = 1.0, resulting in 8 NFEs.\n\nData & Metrics All models considered are trained on LibriSpeech and tested on the dev and test\nsubsets including both clean and other splits. For audio intelligibility, we report ASR-WER (see\n§3.3) on the resynthesized speech. For the overall audio quality, we use UTMOS (Saeki et al., 2022)\nfollowing the discrete vocoder training track of the Interspeech 2024 Challenge on Speech Processing\nUsing Discrete Units (Chang et al., 2024). UTMOS is a reference-free, machine perception score\non audio naturalness on a five-point scale that aligns well with human preference. We also measure\nand speaker similarity (see §3.3) between the original audio and the resynthesized version. For audio\nreconstruction quality, we measure ViSQOL (Chinen et al., 2020) following Encodec and DAC.\n\nResults Results are presented in Table 2. At 500 bps, UniWav leads in all metrics by a margin,\noutperforming all other methods. The lowest ASR-WER reflected that UniWav carried the most\nsemantic information under the same bitrate. Outstanding UTMOS suggested UniWav is able to\ngenerate realistic audio with limited information provided, which emerges from the generative pre-\ntrained decoder. We note that speaker information is still dropped by UniWav, much like all other\nlow-bitrate methods, due to the choice of semantic representation. At 1k bitrate, UniWav maintained\na clear advantage over SpeechTokenizer, with a significantly lower word error rate (5.6% v.s 9.1%)\nand higher UTMOS (3.72 v.s. 2.08). In fact, 1kbps UniWav is comparable to 4kbps SpeechTokenizer\nin terms of ASR-WER and UTMOS. UniWav only falls short on speaker similarity, which requires\ninput speaker information that cannot be filled in by the decoder. Nevertheless, the similarity score is\nstill significantly higher (0.499 v.s. 0.340) than SpeechTokenizer at the same 1kbps.\n\nResults on speech tokenization and resynthesis provided strong evidence for the usefulness of jointly\nlearning speech representation and generation. UniWav is able to perform low-bitrate speech encoding\nwhile decoding audio of substantially better quality than the state-of-the-art SpeechTokenizer.\n\n3.5 ANALYSIS\n\nScaling Encoder and Decoder Since the encoder and the decoder are trained jointly in UniWav, it\nis interesting to investigate how the scale of the encoder and the decoder changes the overall behavior.\nIn Table 3 and Table 4, we report speech recognition and synthesis results when varying the size of\nthe encoder and the decoder. Zero depth indicates the corresponding training objective was ignored.\nFor speech recognition with a shallow encoder, we found introducing the decoder degrades WER\nregardless of the size. Interestingly, an opposite trend is observed when the encoder size is doubled,\ni.e., the encoder benefits from the decoder and the generative objective. For speech generation, we\nfound that encoder depth was critical when working with a shallow decoder, and the gain became less\nsignificant with a deeper decoder. More importantly, the size of the decoder became less important as\nthe encoder depth increased.\n\n8\n\n378\n379\n\n380\n381\n382\n383\n384\n385\n\n386\n387\n388\n389\n390\n391\n\n392\n393\n394\n395\n396\n\n397\n398\n399\n400\n401\n402\n\n403\n404\n405\n406\n407\n408\n\n409\n410\n411\n412\n413\n414\n\n415\n416\n417\n418\n419\n\n420\n421\n422\n423\n424\n425\n\n426\n427\n428\n429\n430\n431\n\n\fUnder review as a conference paper at ICLR 2025\n\n432\n433\n\n434\n435\n436\n437\n438\n439\n\n440\n441\n442\n443\n444\n445\n\n446\n447\n448\n449\n450\n\n451\n452\n453\n454\n455\n456\n\n457\n458\n459\n460\n461\n462\n\n463\n464\n465\n466\n467\n468\n\n469\n470\n471\n472\n473\n\n474\n475\n476\n477\n478\n479\n\n480\n481\n482\n483\n484\n485\n\nFigure 2: Mutual information between quantized representation and phone/speaker (left/right) at\ndifferent layers. Results are computed on the dev set of LibriSpeech. For quantization, 1024 clusters\nare used for k-means.\n\nTo summarize, we highlight two important observations regarding unified pre-training:\n\n• Discriminative representation learning benefits from generative pre-training only when the\n\nencoder has enough capacity.\n\n• Representation encoder plays a more important role in unified pre-training, offering a better\n\ntrade-off between compute and performance.\n\nMutual Information Between Representation and Labels For speech representation learning,\nlayer-wise analysis has been used to examine what is encoded by the model and its dynamics\nat different layers (Pasad et al., 2023). In this study, we introduce a discrete bottleneck to each\nlayer of UniWav’s encoder, using the method described in §3.4 and Eq.(11). For each layer, we\napproximate the mutual information (MI) between quantized representations and the underlying\nspeech label — either phone or speaker — by computing the empirical distribution P (unit), P (label),\nand P (unit, label), and visualize them in Figure 2. The results provide explanations for UniWav’s\nbehavior in different applications: (1) In the latter layers, UniWav achieved lower MI in relation to\nphones compared to HuBERT, potentially making fine-tuning for speech recognition more difficult;\n(2) For most of the layers, UniWav retained a higher MI in relation to speaker label, which explained\nthe strong performance on audio quality and speaker similarity metrics in speech generation and\nresynthesis.\n\nThrough the analysis, we conclude some observations and future directions: (1) Due to the existence\nof the generative decoder, UniWav captured information beyond language from speech. Such property\nmight overturn the belief that self-supervised speech representations are sub-optimal (Wang et al.,\n2023) for generation tasks. Our speech tokenization experiment is a good evidence. (2) Compared\nto the pure self-supervised learning algorithm, UniWav learns a more entangled representation.\nFuture works in unified pre-training can potentially benefit from representation disentanglement\nmethods (Qian et al., 2022; Chang et al., 2023) to further improve controllability and downstream\nperformance.\n\n4 RELATED WORKS\n\nThe success of self-supervised representation learning and generative models has emerged as a\npromising direction for enhancing model generalization, leading to general-purpose speech and audio-\nintelligent systems. Our work contributes to this growing body of research by rethinking the potential\nof jointly modeling both speech understanding and generation, aiming for a unified framework that\ncan seamlessly integrate these tasks. We would like to acknowledge several previous efforts in this\nspace, particularly the intersection between speech representation learning and generative modeling.\n\nSpeech Representation Learning In recent years, the research community has witnessed signif-\nicant advancements in self-supervised learning techniques across speech and other domains, with\nnotable developments ranging from vector-quantized methods (Van Den Oord et al., 2017; Dhariwal\net al., 2020), contrastive pre-training methods (Schneider et al., 2019; Baevski et al., 2020b), to more\nrecent random-projection quantizer methods (Chiu et al., 2022; Yang et al., 2024) or self-labeling\nmethods (Hsu et al., 2021; Chen et al., 2022a; Baevski et al., 2022; Liu et al., 2024). These self-\nsupervised representation learning methods have a wide impact in speech processing, but their focus\nremained primarily on discriminative tasks. Applying these techniques to generative tasks is more\nchallenging (Tsai et al., 2022), often relying on a pipelined generative model (Polyak et al., 2021;\nZhang et al., 2023b). While UniWav’s encoder follows this line of work, our experiments and analysis\n\n9\n\n123456789101112131415161718192021222324Layer1.501.752.002.252.50MIHuBERTUniWav123456789101112131415161718192021222324Layer0.20.30.40.50.6MIHuBERTUniWav\fUnder review as a conference paper at ICLR 2025\n\n486\n487\n\n488\n489\n490\n491\n492\n493\n\n494\n495\n496\n497\n498\n499\n\n500\n501\n502\n503\n504\n\n505\n506\n507\n508\n509\n510\n\n511\n512\n513\n514\n515\n516\n\n517\n518\n519\n520\n521\n522\n\n523\n524\n525\n526\n527\n\n528\n529\n530\n531\n532\n533\n\n534\n535\n536\n537\n538\n539\n\nsuggested that it is significantly different from prior work, due to the generative components involved\nin pre-training. UniWav can be viewed as a more general representation framework aiming to support\na more diverse collection of applications.\n\nBesides speech-only models, it is also worth noting that text-injection-based pre-training could\nimprove speech and text representation on cross-modality matching (Chung et al., 2019; Chen et al.,\n2021; Ao et al., 2021) and feature disentanglement (Chen et al., 2022b). Unified pre-training can\npotentially be further improved by cross-modality alignment in future studies.\n\nGenerative Models in Speech Similar to representation learning, generative models have also made\nsignificant strides in the field of speech. Methods like adversarial training (Kong et al., 2020; Lee\net al., 2022), normalizing flows (Prenger et al., 2019; Valle et al., 2020), auto-regressive models (Wang\net al., 2023; 2024), and diffusion-style framework (Koizumi et al., 2022; Le et al., 2023; Kim et al.,\n2024) have all pushed the edge of speech generation. In this line of work, generative pre-training\nwith Flow Matching (SpeechFlow; Liu et al. 2023) is the most relevant work. Our UniWav decoder\nis built upon SpeechFlow’s design and follows a similar fine-tuning paradigm. Our decoder can be\nviewed as a representation-conditioned variant of SpeechFlow. Nevertheless, we note that the jointly\noptimized representation encoder and generative decoder are substantially different from training\ngenerative models on fixed embeddings, and our contribution in this direction is novel.\n\nSpeech Foundation Models and Large Langauge Models (LLM). In alignment with the recent\ntrend in audio-enabled LLMs (Gong et al., 2023; Tang et al., 2023; Zhang et al., 2023a; Kong\net al., 2024; Huang et al., 2024), representation learning and generative models have gained further\npopularity. For such models, we often desire them to possess the capability of both perceiving\n(discriminative tasks) and producing (generative tasks) audio content, e.g., GPT-4o (OpenAI, 2024),\nand Moshi (D´efossez et al., 2024). Furthermore, modern LLMs are typically trained in an autore-\ngressive manner, and thus require the model’s input and output to be in a unified representation\nspace. However, previous discriminative representations, such as HuBERT (Hsu et al., 2021), are\nnot easily adaptable to generative tasks. Conversely, representations with more detailed acoustic\nfeatures, such as EnCodec codes (D´efossez et al., 2022), exhibit limitations in discriminative per-\nformance and degraded performance in phonetic classification tasks (Yang et al., 2023). As a result,\nprior models either do not support sound generation (Gong et al., 2023; Tang et al., 2023; Kong\net al., 2024), rely on additional components like a vocoder (Zhang et al., 2023a) or text-to-speech\nmodel (Huang et al., 2024), or has a higher word error rate than models trained with discriminative\nrepresentations (D´efossez et al., 2024). This highlights the urgent need for a speech representation\nthat excels in both discriminative and generative tasks. UniWav addresses this gap by introducing a\nunified representation, which is essential for building future foundation models that integrate both\nunderstanding and generation capabilities.\n\n5 CONCLUSION\n\nIn this work, we introduced UniWav, a unified framework for speech representation learning and\ngeneration. By integrating both discriminative and generative capabilities, UniWav demonstrates that\na single model can effectively bridge the gap between tasks with competing requirements. Through\nexperiments across speech recognition, synthesis, and tokenization tasks, we show that UniWav\nachieves competitive performance compared to task-specific models, reducing the need for distinct\npre-trained models and simplifying the speech processing pipeline. The compactness of the learned\nrepresentation and the ability to generate high-quality audio underscore the potential of unified\nfoundational models for future Speech and Audio intelligent systems.\n\nLimitations The most significant limitation we noticed is the performance drop on recognition tasks\ncompared to self-supervised learning models at similar scales. To the extent of our experimentation\nthus far, the cost of enabling generation appears to make this result inevitable. In computer vision,\nresearchers have made similar observations on the small gap between a unified model and a single-\nfunction state-of-the-art representation model (Li et al., 2023). Whether the gap can be closed or\nnot remains an open research problem. We also note that this work focused exclusively on English\nSpeech, from audiobooks in particular. Applying UniWav to more generic speech, more languages,\nand different audio domains (such as sound and music) are left as important future works.\n\n10\n\n\fUnder review as a conference paper at ICLR 2025\n\n540\n541\n\n542\n543\n544\n545\n546\n547\n\n548\n549\n550\n551\n552\n553\n\n554\n555\n556\n557\n558\n\n559\n560\n561\n562\n563\n564\n\n565\n566\n567\n568\n569\n570\n\n571\n572\n573\n574\n575\n576\n\n577\n578\n579\n580\n581\n\n582\n583\n584\n585\n586\n587\n\n588\n589\n590\n591\n592\n593",
    "abstract": "Pre-training and representation learning have been playing an increasingly important role in modern speech processing.  Nevertheless, different applications have been relying on different foundation models, since predominant pre-training techniques are either designed for discriminative tasks or generative tasks. In this work, we make the first attempt at building a unified pre-training framework for both types of tasks in speech. We show that with the appropriate design choices for pre-training, one can jointly learn a representation encoder and generative audio decoder that can be applied to both types of tasks. We propose UniWav, an encoder-decoder framework designed to unify pre-training representation learning and generative tasks. On speech recognition, text-to-speech, and speech tokenization, UniWav achieves comparable performance to different existing foundation models, each trained on a specific task. Our findings suggest that a single general-purpose foundation model for speech can be built to replace different foundation models, reducing the overhead and cost of pre-training.",
    "response": {
      "Soundness": 3,
      "Presentation": 3,
      "Contribution": 2,
      "Rating": 6,
      "Confidence": 5,
      "Strengths": "The main contribution of this work is the combination of DinoSR and a flow matching model to train a unified representation model. While not considered significant, this special setup is somewhat novel.\n\nThe paper contains sufficient experimental results to support their claims although some setups are questionable (in detail below). \n\nThe paper clearly indicated the limitations of the work.",
      "Weaknesses": "The claim their work is \"the first unified pre-training framework for speech representation learning and generation\" is arguable. For example, the paper \"Speechgptgen: Scaling chain-of-information speech generation.\" introduced an approach to train a discrete representation that works for both understanding and generation. The paper \"Moshi: a speech-text foundation model for real-time dialogue\" further improved the approach. \n\nThe presentation needs to be improved. \n1. the DinoSR part in Figure 1 is confusing since the softmax layer is not clearly shown since the encoder, according to the text description, does not include that layer. In contrast, the figure in the original DinoSR paper is very clear. \n2. The majority of Section 2.1 is describing DinoSR. However, notation is not clearly explained sometimes. for example, what is \"k\" in s^k_v right below eq. 3? it's unclear why Euclidean distance (instead of COS distance) is used given that Cos distance is usually more preferred in higher-dim spaces. \n3. notation \"z\" is overloaded in eq 9.\n4. the footnote under Table 1 is not clear. alignment is only used for generation tasks?\n5. eq 11 is wrong? argmin returns an index instead of a representation.\n\nWhen researchers train a representation model they usually keep the encoder fixed when using them in downstream tasks such as ASR. however, in this work the encoder is finetuned. This causes the claims weaker. Similarlly if flow matching is used in the generation part the quality of the generated speech will of course become better. However, this gain comes from the flow matching model instead of the way the representation is learned. Some clarification here is needed.",
      "Questions": "why \"For speech recognition with a shallow encoder, we found introducing the decoder degrades WER regardless of the size. Interestingly, an opposite trend is observed when the encoder size is doubled\"? This is something requires more analysis and explanation.\n\nIn section 3.1, \"We extract the prequantized latent feature from a 16kHz EnCodec (D´efossez et al., 2022) encoder, pre-trained on the same dataset, to serve as the input for our model.\" This is contradictory to the claim in early sections that the input to your model is mel spectrum?  In addition, this makes the comparison with other approaches unfair since you used another model as the pre-processing module (i.e., total model size is actually increased significantly)."
    }
  },
  {
    "paper_id": "INyi7qUdjZ",
    "summary": "Background: Transformers exhibit in-context learning (ICL), the ability to utilize novel information presented in context without further weight updates. This emerges when models are trained on a diverse set of tasks, with a sharp transition from memorization to generalization as task diversity increases. ICL is thought to arise from either capacity limits of the network or specific dynamic learning mechanisms.\nGap: The exact mechanisms governing the transition from memorization to generalization in transformers during ICL are not well understood. Prior work suggests network capacity limits may favor generalization, but this hypothesis lacks detailed examination.\nHypothesis: The transition from memorization to generalization during ICL is driven by the differential learning rates of independent sub-circuits responsible for these activities, rather than the limited capacity of the network.\nMethod: The study uses a small transformer model on a synthetic ICL task. Through theoretical analysis and experiments, it examines the distinct learning kinetics of memorization and generalization sub-circuits. The model includes a one-layer transformer with an MLP for a controlled setup, allowing for analysis of dynamics between ICL and in-weights learning (IWL). The approach analyzes learning rates, establishes a memorization scaling law, and explores the effects of task diversity and regularization.\nConclusion: The study concludes that the transition from memorization to generalization in transformers during ICL is governed by the differential learning dynamics of sub-circuits, which can be largely independent. It uncovers a task diversity threshold determined by these dynamics rather than capacity limits, providing a framework that explains phenomena like the bimodal solutions near the threshold and the ephemeral nature of ICL. The findings challenge the capacity-based hypothesis and suggest that dynamics, rather than limits, play a critical role.\n",
    "full text": "\nDIFFERENTIAL LEARNING KINETICS GOVERN THE\nTRANSITION FROM MEMORIZATION TO GENERALIZA-\nTION DURING IN-CONTEXT LEARNING\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nTransformers exhibit in-context learning (ICL): the ability to use novel informa-\ntion presented in the context without additional weight updates. Recent work\nshows that ICL emerges when models are trained on a sufficiently diverse set\nof tasks and the transition from memorization to generalization is sharp with in-\ncreasing task diversity. One interpretation is that a network’s limited capacity to\nmemorize favors generalization. Here, we examine the mechanistic underpinnings\nof this transition using a small transformer applied to a synthetic ICL task. Using\ntheory and experiment, we show that the sub-circuits that memorize and general-\nize can be viewed as largely independent. The relative rates at which these sub-\ncircuits learn explains the transition from memorization to generalization, rather\nthan capacity constraints. We uncover a memorization scaling law, which deter-\nmines the task diversity threshold at which the network generalizes. The theory\nquantitatively explains a variety of other ICL-related phenomena, including the\nlong-tailed distribution of when ICL is acquired, the bimodal behavior of solu-\ntions close to the task diversity threshold, the influence of contextual and data\ndistributional statistics on ICL, and the transient nature of ICL.\n\n1\n\nINTRODUCTION\n\nLarge transformer models trained to predict the next token exhibit powerful generalization capabil-\nities. One signature of such generalization capabilities is in-context learning (ICL): the ability to\nsolve a task based on new information presented in the context without additional weight updates\n(Brown et al. (2020); Dai et al. (2022); Dong et al. (2022); Garg et al. (2022); Xie et al. (2021);\nOlsson et al. (2022)). Arguably, the ability to interpret novel inputs on-the-fly is a core feature of\nany intelligent system. However, updating synaptic weights on rapid behavioral timescales is chal-\nlenging, both for natural and artificial systems. The emergence of ICL in large language models\n(LLMs) shows that finding network states that learn on-the-fly is indeed possible. Understanding\nhow ICL emerges in LLMs promises insights into how such algorithms may be implemented in the\nbrain and how the data distribution, training objective and network architecture interact to enable\nICL acquisition at scale.\n\nVarious methods have been used to probe the ICL capabilities of LLMs (Brown et al. (2020); Dong\net al. (2022); Pan (2023); Min et al. (2022); Olsson et al. (2022)). A common ICL paradigm is\nto present exemplars as a sequence of item-label pairs, and measure the network’s response to a\ntarget item (Chan et al. (2022); Kirsch et al. (2022); Garg et al. (2022); Aky¨urek et al. (2022);\nVon Oswald et al. (2023); Ravent´os et al. (2023); Bai et al. (2023)). While LLMs display remarkable\ncapabilities on such ICL tasks, interpreting the underlying network mechanisms that give rise to\nthese capabilities remains challenging (but see Wang et al. (2022)). Recent work has approached\nthis challenge by examining how small transformer models solve synthetic ICL tasks (Reddy (2023);\nBietti et al. (2024); Aky¨urek et al. (2022); Ahn et al. (2023); Von Oswald et al. (2023); Edelman et al.\n(2024)). We highlight two notable aspects of ICL phenomenology relevant for our current work: the\ninfluence of task diversity on whether the network memorizes a finite dataset or acquires ICL (i.e.,\ngeneralizes), and how ICL is acquired (and lost) during training.\n\n1\n\n000\n001\n\n002\n003\n004\n005\n006\n007\n\n008\n009\n010\n011\n012\n013\n\n014\n015\n016\n017\n018\n\n019\n020\n021\n022\n023\n024\n\n025\n026\n027\n028\n029\n030\n\n031\n032\n033\n034\n035\n036\n\n037\n038\n039\n040\n041\n\n042\n043\n044\n045\n046\n047\n\n048\n049\n050\n051\n052\n053\n\n\fUnder review as a conference paper at ICLR 2025\n\nFirst, data distributional properties (such as task diversity and their rank-frequency distribution) in-\nfluence whether the network acquires ICL or encodes the response to queries seen during training\nwithin its weights (Kirsch et al. (2022); Chan et al. (2022); Ravent´os et al. (2023). Following pre-\nvious work, we refer to such memorization as in-weights learning (IWL). Notably, the transition\nfrom memorization (IWL) to generalization (ICL) is sharp with respect to task diversity. A curious\nfeature of this transition is that solutions close to the task diversity threshold are bimodal (Kirsch\net al. (2022)). That is, across different random number seeds, solutions either show IWL or acquire\nICL but intermediate solutions are unlikely.\n\nSecond, ICL is often implemented by multi-layer computations involving nonlinear attention heads\nand MLPs (Olsson et al. (2022); Von Oswald et al. (2023); Reddy (2023); Bietti et al. (2024)).\nThe rugged loss landscape induced by such multi-layer, nonlinear operations leads to long plateaus\nfollowed by a sharp drop in loss (Reddy (2023)). Finally, ICL is seemingly transient, i.e., the network\ngradually loses the ICL capability if it is trained for sufficiently long (Singh et al. (2023)).\n\nIt is unclear what leads to the transition from\nIWL to ICL acquisition with increasing task di-\nversity. The loss is minimized when a finite\ntraining dataset is perfectly memorized. Thus,\na naive hypothesis would suggest that as the task\ndiversity increases, the network’s limited capac-\nity to memorize favors the ICL solution (Figure\n1a). Our goal is to test this hypothesis by deriv-\ning a precise quantitative description of the tran-\nsition, and thus we set out to identify a minimal\nsetting that captures the phenomenon.\n\nContributions and outline. We first identify a\none-layer transformer model trained on an in-\ncontext classification task that recapitulates the\nsharp transition from memorization to general-\nization. Despite its simplicity,\nthe one-layer\nmodel displays surprisingly rich phenomenol-\nogy, including abrupt ICL learning dynamics,\ntransience and bimodal solutions close to the\ntask diversity threshold.\n\nFigure 1: (a) In the capacity-constrained model,\nthe network’s limited capacity to memorize fa-\nvors ICL acquisition with increasing task di-\n(b) In the differential learning kinetics\nversity.\nmodel, independent sub-circuits contribute to-\nwards IWL and ICL. IWL is slower for greater\ntask diversity. The network acquires ICL before\nthe network can significantly memorize the train-\ning set. IWL is significantly slowed down as ICL\nexplains most of the loss, but does eventually\nmemorize the training set. The network subse-\nquently loses the ICL capability due to regular-\nization.\n\nNext, we derive an analytical framework that\nquantitatively characterizes ICL acquisition and\nits competition with IWL. We show that the\ntransition from IWL to ICL for our network\nis governed by a dynamical competition be-\ntween memorization and generalization (Figure\n1b). However, whether the network is capacity-\nconstrained or rate-determined depends on the\nnetwork architecture, and we derive a quantita-\ntive measure to determine which of these constraints is at play. The theory predicts that the number\nof iterations before ICL is acquired is exponentially sensitive to the initial parameters, which in\nturn explains the bimodal behavior of solutions close to the task diversity threshold. The task di-\nversity threshold follows a power law whose exponent has a non-trivial relationship with another\nnovel memorization scaling law. ICL transience naturally follows from the theory under standard\nL2 regularization. Finally, we validate our theory by empirically verifying these predictions using\nour transformer model.\n\n2 TASK FORMULATION\n\nWe consider a simplified version of an ICL task proposed by Chan et al. (2022), which allows for\ndisentangling ICL and IWL performance (Figure 2a). Before training, we generate a dataset\nthat contains K item-label pairs,\n\n(x1, ℓ1), (x2, ℓ2), . . . , (xK, ℓK)\n\n=\n\nD\n. Each item xi is a D-\n}\n\nD\n\n{\n\n2\n\n054\n055\n\n056\n057\n058\n059\n060\n061\n\n062\n063\n064\n065\n066\n067\n\n068\n069\n070\n071\n072\n\n073\n074\n075\n076\n077\n078\n\n079\n080\n081\n082\n083\n084\n\n085\n086\n087\n088\n089\n090\n\n091\n092\n093\n094\n095\n\n096\n097\n098\n099\n100\n101\n\n102\n103\n104\n105\n106\n107\n\nICLIWLICLIWLIWLIWLICLICLdata diversity(a) Capacity-constrained(b) Differential learning kinetics \fUnder review as a conference paper at ICLR 2025\n\nD\n\nD\n\n∼ N\n\n1, +1\n}\n\n(0, 1/D) and ℓi is randomly sampled from\n\nconsisting of K item-label (xi, ℓi) pairs\nFigure 2: (a) Data generation process: We create a dataset\n. The network receives a\nwhere each xi\n{−\nℓi of an (xi, ℓi)\nsequence of N + 1 tokens. Each of the first N tokens is a concatenation xi\npair sampled uniformly from\n(details in main-text). The final N + 1 target token consists of\nonly the item xi as its label component is zero-ed out. The network is trained to correctly predict\nthe label of the last item. Model architecture: The input is normalized using LayerNorm, then\nfed to our network, which consists of a one-layer attention network followed by a 3-layer MLP.\n(b) ICL performance demonstrates a sharp transition as a function of data diversity K. Further, at\n104, we observe bimodality where the model either memorizes or\nthe transition threshold K ∗\ngeneralizes. (c) IWL accuracy vs K. IWL and ICL accuracies follow opposite trends. (d) ICL\naccuracy curves show that ICL performance plateaus at the beginning of training but undergoes a\nrapid transition as ICL is acquired. (e) ICL is transient, i.e., ICL accuracy gradually decreases to\nchance levels when the parameters in the attention head are heavily regularized.\n\n⊕\n\n≈\n\ndimensional random vector with components drawn i.i.d from\none of two labels, ℓi\n\n.\n\n(0, 1/D) and is randomly assigned\n\nN\n\n∈ {−\n\n1, +1\n}\n\nD\n\nThe data is presented to the network as a sequence of N + 1 tokens, where each token tj is the item\nconcatenated with its label, tj = xj\nℓj. The first N tokens in the sequence are drawn uniformly\nfrom\n. The N + 1th token (the target token) has an empty label vector. The target token is chosen\nuniformly randomly from the N items in the context, so that there is an exemplar always present\nin the context. Given an input sequence, the network is trained to predict the label of the target\ntoken using a binary cross-entropy loss. Since the total number of item-label pairs (K) is finite, the\nnetwork can either memorize each item’s label (IWL), or it can learn to use the exemplar within the\ncontext to predict the correct label (ICL).\n\n⊕\n\nD\n\n) and evaluate the network’s accuracy on sequences sampled from\n\nTo measure ICL, we construct a test dataset\ntest consisting of novel item-label pairs (sampled\nD\nlike\ntest using the previously\ndescribed procedure. To measure IWL, we evaluate the network on sequences sampled from\n,\nD\nexcept that the target has no corresponding exemplar in the sequence.\nIn this case, the context\nhas no useful information, and the network must rely on the label’s information encoded within its\nweights.\n\nD\n\n3 RESULTS\n\n3.1 A ONE-LAYER TRANSFORMER MODEL RECAPITULATES ICL PHENOMENOLOGY\n\nWe begin with a one-layer attention-based network followed by a multi-layer perceptron (MLP).\nGiven tokens t1, t2, . . . , tN +1, we first apply a LayerNorm operation to obtain t′\ni = LayerNorm(ti).\n\n3\n\n108\n109\n\n110\n111\n112\n113\n114\n115\n\n116\n117\n118\n119\n120\n121\n\n122\n123\n124\n125\n126\n\n127\n128\n129\n130\n131\n132\n\n133\n134\n135\n136\n137\n138\n\n139\n140\n141\n142\n143\n144\n\n145\n146\n147\n148\n149\n\n150\n151\n152\n153\n154\n155\n\n156\n157\n158\n159\n160\n161\n\n02040Trainingiterations(×105)0.50.60.70.80.91.0AccuracyICLIWL012Trainingiterations(×105)0.50.60.70.80.91.0AccuracyICLIWL103104105K0.50.60.70.80.91.0ICLaccuracy103104105K0.50.60.70.80.91.0IWLaccuracy(b)(c)(d)(e)(a)Training input sequence?ICL eval sequence (novel item-labels)?DatasetIWL eval sequence (no exemplar)?Model+3-layer MLPattentionheadlogitinput\fUnder review as a conference paper at ICLR 2025\n\nFigure 3: Phenomenology of the minimal model. (a) ICL performance in the minimal model demon-\nstrates a sharp transition as a function of data diversity K. (b) ICL acquisition is abrupt during\ntraining. (c) ICL is transient when w is exclusively regularized.\n\nThe attention operation then produces the output\n\nu = t′\n\nN +1 +\n\nN +1\n(cid:88)\n\nj=1\n\n′T\nj KT Qt′\net\nk et′ T\n(cid:80)\n\nk KT Qt′\n\nN +1\n\nN +1\n\nV t′\nj,\n\n(1)\n\n×\n\n(D + 2) query, key and value matrices (ℓj’s are one-hot vectors for\nwhere Q, K, V are (D + 2)\n1 elsewhere). The MLP ϕ takes input u of dimension D + 2\nthe one-layer model, and we use ℓj =\nlog σ(ϕ(u)), and if the\nand produces logits ϕ(u). If the target’s true label is (1, 0), the loss is\n−\ntarget’s true label is (0, 1), the loss is\nϕ(u)), where σ is the logistic function. The MLP\nis a three-layer ReLU network with hidden dimension d. We use stochastic gradient descent (SGD)\nwith batch size 128, learning rate 0.01, weight-decay 10−10, D = 63, N = 100 and d = 512, unless\notherwise specified.\n\nlog σ(\n\n±\n\n−\n\n−\n\n≫\n\nK ∗, where\nWe examine how ICL accuracy scales with the number of item-label pairs, K. For K\n104, we find that the network shows IWL but fails to acquire ICL (Figure 2b,c). Conversely,\nK ∗\n∼\nK ∗, the network consistently acquires ICL but does not show IWL. The transition from\nfor K\nIWL to ICL is sharp. Solutions in the vicinity of K = K ∗ are bimodal (visible in Figure 2b\nand quantified later in Figure 6c). Examining the ICL accuracy during learning shows an abrupt\ntransition from chance-level to perfect accuracy (Figure 2d). Previous work has also shown that ICL\naccuracy gradually decays to zero if training is continued for a sufficiently large number of iterations\n(Singh et al. (2023)). Transience is recapitulated when we significantly extend the number of training\niterations (Figure 2e), provided the parameters within the attention head are regularized more heavily\nthan those in the MLP. This skewed regularization scheme hints at a potential explanation for what\ncauses transience (as noted in Singh et al. (2023)), which we will later explain quantitatively using\nour theoretical framework.\n\n≪\n\n3.2 DISENTANGLING ICL AND IWL IN A MINIMAL MODEL\n\nFigure 2(c-f) shows that, despite its simplicity, the one-layer transformer model captures the core\nfeatures of the memorization to generalization transition and ICL training dynamics observed in\nmore complex models. However, a mechanistic analysis is still challenging due to the nonlinearities\nin the attention head, the MLP and how these two operations interact. To make progress, we further\nreduce our one-layer transformer model into a disentangled model (which we refer to as the “mini-\nmal model” hereafter) by proposing two ansatz. We will show empirically that the minimal model\nalso reproduces the phenomena in Figure 2(c-f). This minimal model is amenable to a theoretical\nanalysis and leads to specific quantitative predictions. We then validate our ansatz by empirically\ntesting these predictions using our original transformer model (Section 4).\n\nTo motivate the ansatz, we observe that ICL in this task involves a simple match-to-sample operation\nimplemented by the attention head. The attention paid by the target token is determined by its dot-\nproduct similarity with the content (the first D dimensions) of the tokens in the context. The value\nmatrix reads the labels of the tokens weighted by the attention paid to those tokens and passes it on\n\n4\n\n162\n163\n\n164\n165\n166\n167\n168\n169\n\n170\n171\n172\n173\n174\n175\n\n176\n177\n178\n179\n180\n\n181\n182\n183\n184\n185\n186\n\n187\n188\n189\n190\n191\n192\n\n193\n194\n195\n196\n197\n198\n\n199\n200\n201\n202\n203\n\n204\n205\n206\n207\n208\n209\n\n210\n211\n212\n213\n214\n215\n\n103104105K0.50.60.70.80.91.0ICLAccuracy0500100015002000Trainingiterations(×50)0.50.60.70.80.91.0AccuracyICLIWL05000100001500020000Trainingiterations(×50)0.50.60.70.80.91.0AccuracyICLIWL(a)(b)(c)\fUnder review as a conference paper at ICLR 2025\n\nto the MLP. Put together, these observations suggest that the relevant operations performed by the\nquery-key product and the value matrix are captured by\n\nK T Q =\n\n(cid:18)βID×D 0D×1\n01×1\n\n01×D\n\n(cid:19)\n\n,\n\nV =\n\n(cid:18)0D×D 0\nw\n0\n\n(cid:19)\n\n(2)\n\nwhere ID×D is a D-dimensional identity matrix, β, w are (learnable) scalars and the rest of the\ncomponents are zeroes.\n\nFrom equation 1, the MLP receives the sum of the target token tN +1 and the output of the attention\nhead as its input. Note that all the information required by the MLP to memorize the label of the\ntarget is present in tN +1. That is, IWL does not require the output of the attention head. Similarly,\nall the information required to predict the target’s label using ICL is contained in the output of the\nattention head. Based on these observations, we posit that the final logit used to predict the target’s\nlabel is the sum of logits generated independently by an MLP (which takes xN +1 as input) and the\nattention head (using the simplified K T Q and V matrices in equation 2). Specifically, given an\ninput sequence t1, t2, . . . , tN +1, we assume the MLP ϕ produces a logit zMLP = ϕ(xN +1) and the\nattention head produces the logit\n\nzATT =\n\nN\n(cid:88)\n\nj=1\n\neβxT\nj xN +1\nk=1 eβxT\n\n(cid:80)N\n\nk xN +1\n\nwℓj,\n\n(3)\n\nPut together, the predicted probability that the target’s label is +1 is given by σ(zMLP + zATT).\nThe binary cross-entropy loss for a given input sequence is then\nlog σ(ℓc(zMLP + zATT)), where\nℓc =\n\n1 is the true label of the target.\n\n−\n\nIn summary, we assume an independence ansatz, where the attention head and MLP perform ICL\nand IWL respectively, and additively contribute towards the prediction of the target’s label. Further,\nwe assume that the majority of the ICL learning dynamics is captured by reducing the K T Q and V\nmatrices to two “order parameters”, β and w. That the strengths of the relevant attention operations\ndetermine ICL acquisition is our second ansatz.\n\n±\n\nThe minimal model is parameterized by β, w and the parameters of the MLP (a three-layer ReLU\nnetwork of hidden dimension 512). The model is trained and evaluated using the same procedures\nused on the transformer model. Figure 3(a-c) show that the three phenomena of interest, that is,\nthe transition from memorization to generalization, abrupt ICL learning and ICL transience, are\nreproduced by the minimal model. We now use the minimal model to develop an analytical theory.\nWe outline the main results here and present more detailed derivations in the Appendix.\n\n3.3 THE LOSS LANDSCAPE OF THE MINIMAL MODEL\n\n(recall,\nWe consider the asymptotic limit K\nK > 103, N = 102, D = 63 in our experiments). From equation 3, ICL is acquired when w, β\n1.\nOur goal is to compute the time taken for the network to acquire ICL starting from w = w0, β = β0\nwith\n\n1 and the infinite-dimensional limit D\n\n→ ∞\n\n≫\n\n≫\n\n≫\n\nN\n\n1.\n\nβ0\n\n,\n\nw0\n|\n\n|\n\n|\n\n| ≪\n\nIn the limit D\nis unlikely there is more than one copy of the target in the context when K\nindex of this copy. From equation 3, we have\n\nj xN +1 is 1 if xj is a copy of the target and 0 otherwise. It\nN . Let c denote the\n\n, the dot product xT\n\n→ ∞\n\n≫\n\nzATT\n\nw\n\n≈\n\n(cid:18)\n\neβ\neβ + N\n\nℓc +\n\n1\n\n1\neβ + N\n\n(2n+\n\nN\n\n−\n\n−\n\n1\n\n(cid:19)\n\nℓc)\n\n,\n\n(4)\n\n−\n\n−\n\nwhere n+ is the (binomally distributed) number of tokens with label +1 amongst the N tokens in\n1/2 + η/2√N , where η\nthe context. When N\n\n(0, 1).\n\n1, n+/N\n\n≫\n\n≈\n\n∼ N\n\nNext, the MLP’s contribution to the average loss appears only through the distribution of logits ob-\n. In particular, denote P + as the distribution\ntained by applying the MLP to each of the K items in\nof logits obtained when the MLP is applied to the items in\nwith a +1 label. We use the fact\nthat the two labels are symmetric, and average over n+ and P + to show that the average binary\ncross-entropy loss\n\nD\n\nD\n\n(cid:42)(cid:18)\n\nL ≈ −\n\n1 +\n\nL\nη\n√N\n\nis (see Appendix)\n(cid:32)\n(cid:19)\n\nlog σ\n\nϕ+ + w\n\n(cid:32)\n\neβ\n−\neβ + N\n\n+\n\n1\n\n1\n\n−\n\nη√N\neβ + N\n\n−\n\n1\n\n5\n\n(cid:33)(cid:33)(cid:43)\n\n.\n\n(5)\n\nη∼N (0,1),ϕ+∼P +\n\n216\n217\n\n218\n219\n220\n221\n222\n223\n\n224\n225\n226\n227\n228\n229\n\n230\n231\n232\n233\n234\n\n235\n236\n237\n238\n239\n240\n\n241\n242\n243\n244\n245\n246\n\n247\n248\n249\n250\n251\n252\n\n253\n254\n255\n256\n257\n\n258\n259\n260\n261\n262\n263\n\n264\n265\n266\n267\n268\n269\n\n\fUnder review as a conference paper at ICLR 2025\n\nThe loss landscape throughout training is thus specified by P +, w and β. Retaining the fluctuations\nin η is necessary to accurately describe ICL acquisition.\n\n3.4 THE DYNAMICS OF ICL ACQUISITION\n\nL ≈\n⟩ϕ+ and c2\n\nTo examine the dynamics of ICL acquisition, we find an expression for the loss when\neβ\nN (for arbitrary P +). Both these conditions are satisfied at initialization (\nFrom equation 5, a few steps of simplification leads to (Appendix)\n\n≪\n\n−\n\n1\n\nw\n|\nw0\n|\n\n√N and\n| ≪\n1).\nβ0\n,\n|\n|\n\n| ≪\n\n(cid:68)\nlog(1 + e−ϕ+\n\n(cid:69)\n)\n\nϕ+ −\n\nc1\nN\n\n(cid:18)\n\neβw\n\nc2w2\n2\n\n−\n\n(cid:19)\n\n,\n\n(6)\n\nϕ+)\n\nσ(\n\nwhere c1\n≪\nN to Taylor expand equation 5 and retained terms to order 1/N (terms of order 1/√N vanish in\nexpectation). The distribution from which ϕ+ is drawn has been dropped for notational convenience.\n\n⟩ϕ+/c1. Here, we used\n\n| ≪\n\n≡ ⟨\n\n− ⟨\n\nσ(\n\n−\n\n−\n\n−\n\n≡\n\nw\n\n1\n\n1\n\n|\n\n√N and eβ\n\nϕ+)2\n\nEquation 6 allows us to make several important in-\nferences. The first term in the r.h.s of equation 6\nis the loss incurred by the MLP. It does not involve\nw, β and thus does not affect ICL learning dynam-\nics. Since the second term in the r.h.s of equation 6\nis small at initialization, the rate at which the MLP\nmemorizes is not affected by ICL learning. That\nis, IWL proceeds without any competition from ICL\nuntil ICL is acquired (which happens abruptly).\nThe scalar variables c1 and c2 depend on P + and\nthus depend on the time t since training began. Their\nevolution in general depends on multiple factors, in-\ncluding MLP architecture, initialization scheme and\nthe number of tasks K to be memorized.\nImpor-\ntantly, IWL influences ICL acquisition only through\nc1(t) and c2(t), which in turn depend only on how\nthe MLP memorizes class labels. We proceed with\nour analysis by retaining c1(t) and c2(t) as yet-to-\nbe-determined MLP-specific dynamical “order pa-\nrameters”, keeping in mind that their dependence on\nt and K will play an important role in our analysis\nfurther below.\n\nGradient descent dynamics over the loss in equa-\ntion 6 gives\n\nL\n\nFigure 4: The approximate ICL loss land-\nscape\n(fixing zMLP = 0) in the minimal\nmodel as a function of the key parameters\nβ, w exhibits a nearly flat region close to ini-\ntialization, but the dynamics always leads to\nICL acquisition (w, β\n\n1).\n\n≫\n\ndw\ndt\ndβ\ndt\n\n=\n\n=\n\nc1\nN\nc1\nN\n\nc2w(cid:1) ,\n\n(cid:0)eβ\n\n−\n(cid:0)weβ(cid:1) .\n\n(7)\n\n(8)\n\n1 and 0 < c1 < 1). Since ϕ+ on average\nLearning initially proceeds at a slow rate c1/N (since N\nincreases as the MLP memorizes, c1 decreases and slows down ICL acquisition. If the MLP (near)\nperfectly memorizes the K item-label pairs before ICL is acquired, then ICL is never acquired. In\nother words, the loss “explained away” due to MLP memorization creates an effective competition\nbetween IWL and ICL acquisition despite the additive contributions of the MLP and the attention\nhead to the logit. Since 0 < c2 < 1, equation 7 shows that w eventually converges from its initial\nvalue to a positive value w = eβ/c2. β increases monotonically when w is positive until ICL is\nacquired. Thus, equation 7 and equation 8 imply that ICL will always be acquired, however slowly,\nif the MLP is unable to perfectly memorize the K item-label pairs (i.e., c1(\n\n) > 0).\n\n≫\n\nHowever, the choice of label statistics in the context matters. For example, consider the case when\n1 labels in the context. To compute the\nN is even and there are exactly N/2 tokens with +1 and\n1)/N to get\nmean loss\n\n′ in this scenario, we set η = 0 in equation 5 and Taylor expand w.r.t (eβ\n\nL\n\n(cid:68)\nlog(1 + e−ϕ+\n\n(cid:69)\n)\n\n′\nL\n\n≈\n\nϕ+ −\n\n−\nc1\nw (cid:0)eβ\nN\n\n1(cid:1) .\n\n−\n\n−\n\n(9)\n\n∞\n\n6\n\n270\n271\n\n272\n273\n274\n275\n276\n277\n\n278\n279\n280\n281\n282\n283\n\n284\n285\n286\n287\n288\n\n289\n290\n291\n292\n293\n294\n\n295\n296\n297\n298\n299\n300\n\n301\n302\n303\n304\n305\n306\n\n307\n308\n309\n310\n311\n\n312\n313\n314\n315\n316\n317\n\n318\n319\n320\n321\n322\n323\n\n−20246b−2.50.02.55.07.5w0.51.01.52.0\fUnder review as a conference paper at ICLR 2025\n\nThe origin is a saddle point. The parameters either flow to the ICL solution (w, β > 0) or to a\nsuboptimal solution (w, β < 0) depending on the initial values w0, β0. That is, ICL acquisition is\nnot guaranteed. Intuitively, when n+ is binomially distributed, the network learns that the target’s\nlabel is more likely to be a +1 if there are more +1 labels than\n1 labels in the context. This bias,\nhowever small, pushes w to a positive value and leads the network into the ICL basin. When the\nnumbers of +1 and\n1 labels are forced to be equal, w could flow into the basin that leads to ICL or\nflow to an alternative potentially suboptimal solution. We revisit the n+ = N/2 case in Section 4.\n\n−\n\n−\n\n3.5 EXPONENTIAL DEPENDENCE OF tICL ON INITIAL CONDITIONS\n\nEquation 7 and equation 8 allow us to estimate the number of iterations it takes to acquire ICL\n(Appendix). Note that “time” t here is a proxy for the number of iterations, which we can only\ndetermine up to a constant pre-factor. We fix w0 = 0 hereafter, though an expression for tICL when\n1 can be derived (Appendix). Exact integration of equations 7 and 8 is infeasible,\nw0\nbut an approximate expression can be obtained when\n1. We show that the number of\nβ0\n|\niterations it takes for ICL acquisition (denoted τK) satisfies\n\n= 0,\n\n| ≪\n\n| ≪\n\nw0\n\n|\n\nN √2πe−β0\n\n≈\n\nIK(τK), where IK(t)\n\n(cid:90) t\n\n2\n\n≡\n\n0\n\nc1(t′)dt′.\n\n(10)\n\nThe subscript K is introduced to highlight that c1 depends on K.\n\nWe first consider the case K =\nϕ+)\nis distributed symmetrically around 0, in which case c1(t) =\nSolving for τ∞ (which we call tICL hereafter) using equation 10, we get\n\nso that the MLP is unable to memorize\n⟩ϕ+\n\nσ(\n⟨\n\n∞\n\n−\n\n. The MLP logit ϕ+\n1/2 and I∞(t) = t.\n\nD\n≈\n\ntICL\n\nN √2πe−β0.\n\n(11)\n\n≈\nThe dynamics are qualitatively different when\n1. In this case, we obtain\n−\nN e−2β0 (Appendix). We numerically verify the exponential dependence of tICL on the initial\ntICL\nvalues of β0 (Supplementary Figure A.1a). A consequence of this exponential dependence is that\nnormal-distributed values of β0 will lead to a long-tailed distribution of tICL. In pictorial terms,\ndue to the nearly flat loss landscape close to initialization (Figure 4), small variation in the initial\nparameters w0, β0 leads to large variation in when ICL is acquired.\n\nβ0 is large and eβ0\n\n≪\n\n≈\n\n3.6 MEMORIZATION SCALING LAWS AND THE TRANSITION FROM MEMORIZATION TO\n\nGENERALIZATION\n\nEquation 10 shows that the behavior of an MLP-specific quantity, c1(t) (via IK), determines when\nICL is acquired for different values of K. It is useful to introduce the quantity IK(\n), which can\nbe interpreted as the time taken for the MLP to memorize a dataset of size K. Equations 10 and 11\ntogether with the monotonicity of IK(t) imply that ICL is acquired if\n\n∞\n\nWe delineate two distinct mechanisms depending on whether IK(\n\n) is finite or not:\n\n∞\n\n∞\n\ntICL < IK(\n\n).\n\n(12)\n\n1. Capacity-constrained: We call the network capacity-constrained if IK(t) diverges as t\n\n→\n, i.e., the network never fully memorizes the dataset. Equation 12 then implies that the\n∞\nnetwork generalizes when K > Kcc, where Kcc is the smallest K at which the network is\ncapacity-constrained.\n\n2. Differential learning kinetics: It is possible that IK(\n\n) is finite. In this case, the network\ntransitions from memorization to generalization at K = K ∗ such that tICL\n). In\nother words, when K > K ∗, it takes longer for the network to memorize the dataset (even\nthough it has the capacity to do so) than it takes for the network to generalize. We call\nthis case the differential learning kinetics regime as the relative rates at which the network\nmemorizes and generalizes determine when the transition occurs.\n\nIK∗ (\n\n∞\n\n∞\n\n≈\n\nmay occur either because the network has limited capacity to\nThe divergence of IK(t) as t\nmemorize the K samples or because of the data distribution. For example, if the rank-frequency\ndistribution of item-label pairs follows a Zipf’s law p(f )\n1, then the\nnetwork’s loss is dominated by rare item-label pairs that are not memorized. Previous work has\nshown that such skewed data distributions indeed favor ICL acquisition (Chan et al. (2022)).\n\nf −α with exponent α\n\n→ ∞\n\n∼\n\n≤\n\n7\n\n324\n325\n\n326\n327\n328\n329\n330\n331\n\n332\n333\n334\n335\n336\n337\n\n338\n339\n340\n341\n342\n\n343\n344\n345\n346\n347\n348\n\n349\n350\n351\n352\n353\n354\n\n355\n356\n357\n358\n359\n360\n\n361\n362\n363\n364\n365\n\n366\n367\n368\n369\n370\n371\n\n372\n373\n374\n375\n376\n377\n\n̸\n\fUnder review as a conference paper at ICLR 2025\n\n378\n379\n\n380\n381\n382\n383\n384\n385\n\n386\n387\n388\n389\n390\n391\n\n392\n393\n394\n395\n396\n\n397\n398\n399\n400\n401\n402\n\n403\n404\n405\n406\n407\n408\n\n409\n410\n411\n412\n413\n414\n\n415\n416\n417\n418\n419\n\n420\n421\n422\n423\n424\n425\n\n426\n427\n428\n429\n430\n431\n\nWe examine the behavior of IK for a uniform distribu-\ntion over item-label pairs. To our knowledge, current\ndeep learning theory does not inform how the distribu-\ntion of logits (or summary statistics such as c1) scales\nwith t and K for typical MLP architectures. To make\nprogress, we empirically measure c1(t), c2(t) for dif-\nferent K using an independent set of MLP experiments\n(Supplementary Figure A.2). We find that c1(t) for K\nbetween 500 and 50000 decays fast enough such that its\nintegral IK(\n) is finite (Supplementary Figure A.3a).\n∞\nOur MLP is thus not capacity-constrained. Further, we\nuncover a scaling law, IK(\n0.7\n∞\n(Figure 5). From equation 12, using the expression for\nK ν\ntICL in equation 11 and the scaling law IK(\nleads to an estimate for the task diversity threshold K ∗\nfor the transition from memorization to generalization:\n\nK ν, where ν\n\n)\n∞\n\n∼\n\n≈\n\n∼\n\n)\n\nK ∗\n\n∼\n\nN 1/νe−β0/ν,\n\n(13)\n\nFigure 5: IK(\n∞\ning with K with exponent ν\n\n) shows a power-law scal-\n\n0.7.\n\n≈\n\nup to a constant prefactor. Note the exponential dependence on β0, whose random initialization\nimplies that there is a range of K for which the network will either show memorization or gen-\neralization. The theory further predicts a non-trivial power-law relation between the task diversity\nthreshold and the context length.\n\n3.7 SLOW IWL EXPLAINS TRANSIENT ICL\n\nWe now explain why transience appears in our minimal model (Figure 3c) when the attention head\nis regularized more heavily compared to the MLP. For simplicity, we impose L2 regularization with\nparameter λw only on w. We return to equation 5 for the loss, which applies throughout training.\n1 after ICL is acquired, we can simplify equation 5 to obtain (Appendix)\nSince w, β\n\n≫\n\ne−ϕ+(cid:69)\n(cid:68)\n\nϕ+\n\ne−w +\n\nλww2\n2\n\n,\n\nL ≈\n\n(14)\n\nOnce ICL is acquired, memorization slows down dramatically due to the small factor e−w. Without\nL2 regularization on w, w continues to increase (at a decreasing rate) and ICL is not transient.\nHowever, when w is regularized, w after ICL acquisition tracks wtr, where\n\nwtr(t)\n\n≈\n\nW (c3(t)/λw) ,\n\nc3(t)\n\n(cid:68)\ne−ϕ+(cid:69)\n\nϕ+\n\n.\n\n≡\n\n(15)\n\nThe Lambert W function W (x) is monotonic in x when x is positive. c3 decreases as the network\nmemorizes the dataset (Figure A.2c). Thus, wtr decreases as c3 decreases. wtr decays to zero (and\nICL fades away) when the dataset is sufficiently memorized, i.e., when c3\nλw. Thus, the analysis\nsuggests that extremely slow memorization coupled with regularization leads to ICL transience.\nWe note however that in more complex models the effects of a global regularization parameter on\ndifferent sub-circuits are hard to disentangle, which may explain the puzzling observations in Singh\net al. (2023).\n\n≈\n\nEquation 15 hints at a relationship between the loss on ICL sequences (\nsequences (\n\nIWL) after ICL is acquired. We use a heuristic argument (Appendix) to show that\n\nICL) and the loss of IWL\n\nL\n\nL\n\nIWL\n\nL\n\n≈ −\n\nICL\n\nL\n\n≈ −\n\n1\n2\n1\n2\n\nlog(\n\nL\n\nlog(\n\nL\n\nICL), when\n\nICL\n\nL\n\n≪\n\n1,\n\nIWL), when\n\nIWL\n\nL\n\n≪\n\n1.\n\n(16)\n\nThese approximate relations between\nIWL are consequences of our two ansatz. If our\nL\nansatz are valid, the theory predicts that these relations should hold from the moment ICL is acquired\nuntil it fades due to gradual IWL.\n\nICL and\n\nL\n\n8\n\n103104105K100101IK(∞)Slope=0.70\fUnder review as a conference paper at ICLR 2025\n\n≈\n\nFigure 6: (a) The critical task diversity threshold K ∗ exhibits a power law relationship with respect\nto N . The experimentally determined critical exponent (linear fit) closely matches our predicted\ncritical exponent 1/ν where ν\n0.7 (predicted fit). (b) The median time taken to acquire ICL (tICL)\nscales linearly as a function of N and the distribution of tICL is long-tailed. For clearer visualization\nof the full distribution, see Supplementary Figure A.4. (c) Close to K = K ∗, depending on the\ninitialization seed (red and green dots), the network will either generalize (green) or not (red). Dif-\nferent seeds are predicted to show either small tICL or do not acquire ICL, with few intermediates.\nA histogram of tICL for K = 9666 confirms this prediction at the critical task diversity threshold\nK ∗. Note that the maximum number of iterations is\n105. (d) In runs that exhibit transience,\nwe observe a relationship between ICL loss and IWL loss after the model has acquired ICL, closely\nmatching our predicted functional relationship. (e) We measured the fraction of solutions that ac-\nquired ICL as we vary K, by training at least 20 models with different seeds for each K, on a new\n′ where every sequence is constrained to contain exactly N/2 items of each label. We ob-\ndataset\nserve that the critical task diversity threshold K ∗ is greatly increased and that many more solutions\nfail to achieve ICL compared to models trained on our standard dataset\n\n4.5\n\n×\n\n≈\n\nD\n\n.\n\nD\n\n4 EMPIRICAL VALIDATION\n\nThe theory makes a number of new quantitative predictions related to ICL acquisition. We em-\npirically test six nontrivial predictions that span various aspects of ICL phenomenology using the\noriginal transformer model in equation 1.\n\nPower-law scaling of the task diversity threshold with context length. Equation 13 predicts a\nhighly non-trivial power law relationship between the task diversity threshold K ∗ and N . To test\nthis prediction, we train our original transformer model (equation 1) at varying N and K. At each N ,\nwe observe a sharp transition from memorization to generalization as K increases (Supplementary\nFigure A.7). For each N , we determine K ∗ by fitting a sigmoidal curve to ICL performance as a\nfunction of K. As ν\n1.43, closely matching our\nmeasured exponent\n\n0.7, Equation 13 predicts an exponent of 1/ν\n1.41 (Figure 6a).\n\n≈\n\n≈\n≈\n\nLinear scaling of the time taken to acquire ICL with context length. Equation 11 predicts that\ntICL (time taken to acquire ICL) scales linearly with N . To test this, we train our original transformer\nmodel (equation 1) at varying N and take the limit K\nat every\ntraining iteration. We then determine tICL as the epoch at which ICL accuracy exceeds 95%. We\n100 seeds for each N to obtain the full distribution of tICL. Figure 6b confirms a linear\ntrain\n\nby resampling our dataset\n\n→ ∞\n\nD\n\n≈\n\n9\n\n432\n433\n\n434\n435\n436\n437\n438\n439\n\n440\n441\n442\n443\n444\n445\n\n446\n447\n448\n449\n450\n\n451\n452\n453\n454\n455\n456\n\n457\n458\n459\n460\n461\n462\n\n463\n464\n465\n466\n467\n468\n\n469\n470\n471\n472\n473\n\n474\n475\n476\n477\n478\n479\n\n480\n481\n482\n483\n484\n485\n\n1026×1012×102N104K∗DataLinearFitPredictedFit104K−0.20.00.20.40.60.81.0FractionofrunsthatacquireICLEqual0sand1sStandard50100150200250N01234tICL(initerations×105)LinearFitMedians0123ICLloss0.00.51.01.52.02.5IWLlossK=10000LIWL=−12log(LICL)LIWL=e−2LICL024tICL(initerations×105)0255075100CountK∗=9666(b)(a)(c)(d)(e)Task diversity threshold vs context lengthTime to acquire ICLBimodality at the thresholdICL & IWL lossesContextual statistics101102103Iterations(t)100101102IK(t)\fUnder review as a conference paper at ICLR 2025\n\nrelationship between the median tICL as a function of N . We verified that the linear relationship also\nholds for a two-layer transformer (Supplementary Figure A.5).\n\nK ∗, equation 13 predicts\nBimodal solutions near the transition. For K near the transition, K\nthat independent runs will either have tICL > IK∗ (\n) depending on the net-\nwork’s initial parameters. In the former case, ICL is never acquired. The nonlinear form of IK(t)\n(illustrated in Figure 6c, top) suggests that in the latter case, tICL will be short, with few intermedi-\nates in between. We verify this bimodal behavior of the solutions around the transition by generating\nruns with\n\n≈\n) or tICL < IK∗ (\n\nK ∗ (Figure 6c, bottom).\n\n250 seeds near K\n\n∞\n\n∞\n\n≈\n\n≈\n\nLong-tailed distribution of the time taken to acquire ICL. Equation 37 predicts a long-tailed\ndistribution of tICL. We verify this prediction in histograms of tICL for each N , using the same data\nas in Figure 6b (Supplementary Figure A.4).\n\nInterdependence of the ICL loss and IWL loss after ICL acquisition. Equation 16 predicts a\nnon-trivial relationship between the ICL loss\nIWL after ICL is acquired. In\nruns in which we observed transience, we plot IWL performance as a function of ICL performance\nand observe a close match between equation 16 and data (Figure 6d).\n\nICL and the IWL loss\n\nL\n\nL\n\n−\n\n1 labels in the context. We test this prediction in a dataset\n\nFailure of ICL acquisition when every sequence contains N/2 tokens of each label. Equation 9\npredicts that the network is more likely to fail to acquire ICL when there are exactly N/2 tokens\n′ with N = 100, where\nwith +1 and\neach training sequence (including the target token) viewed by the one-layer transformer has N/2\ntokens of each label in its context. For each K, we trained the model using\n20 seeds. We measure\n≈\nthe probability of acquiring ICL as the fraction of seeds whose final ICL performance exceeds 75%.\n′ compared to those\nWe observe much lower probability of acquiring ICL for models trained on\ntrained on our standard\n(Figure 6e). Moreover, the loss curves across seeds are diverse, with many\nseeds not acquiring ICL and some saturating at sub-optimal solutions (See Supplementary Figure\nA.8).\n\nD\n\nD\n\nD\n\n5 CONCLUSION\n\nHere, we propose a theory based on the ansatz that the network contains sub-circuits that are inde-\npendently involved in memorization and generalization. A trade-off arises simply because the rate\nat which one sub-circuit is optimized depends on how much loss is already explained by the other\nsub-circuit(s). Building on this theory, we show that the transition from memorization to general-\nization in our model is determined by the relative rates at which these sub-circuits memorize and\ngeneralize. However, the theory does not rule out the possibility that capacity constraints play a role\nin other scenarios.\n\nThis ansatz, despite being cast in the context of a simplified one-layer model, explains a surprising\nvariety of ICL-related phenomena observed with much larger models. These include a long-tailed\ndistribution in when ICL is acquired, an MLP memorization scaling law, the bimodality of solutions\nat the task diversity threshold, the transient nature of ICL, amongst other novel quantitative relations\nthat our theory identifies. The two most striking predictions are (1) the non-trivial relationship\nK ν) and a task diversity threshold\nbetween an MLP-specific memorization scaling law (IK(\nN 1/ν), and (2) the long-tailed distribution of when ICL is\nscaling law w.r.t context length (K ∗\nacquired and its linear scaling with context length (tICL\nN ). Both these predictions have been\nvalidated in our experiments.\n\n)\n∞\n\n∼\n\n∼\n\n∼\n\nOur results offer some hope that seemingly intractable phenomena observed in large models can be\nreproduced and analyzed using simpler, tractable models through careful experimental design. How-\never, further work is necessary to examine to what extent such insights provided by small models\nremain valid for larger models (that potentially contain many sub-circuits) and for more naturalistic\ntasks (where a clear distinction between memorization and generalization cannot be made) (Min\net al. (2022); Wei et al. (2023); Pan (2023); Shi et al. (2024)).",
    "abstract": "Transformers exhibit in-context learning (ICL): the ability to use novel information presented in the context without additional weight updates. Recent work shows that ICL emerges when models are trained on a sufficiently diverse set of tasks and the transition from memorization to generalization is sharp with increasing task diversity. One interpretation is that a network's limited capacity to memorize favors generalization. Here, we examine the mechanistic underpinnings of this transition using a small transformer applied to a synthetic ICL task. Using theory and experiment, we show that the sub-circuits that memorize and generalize can be viewed as largely independent. The relative *rates* at which these sub-circuits learn explains the transition from memorization to generalization, rather than capacity constraints. We uncover a memorization scaling law, which determines the task diversity threshold at which the network generalizes. The theory quantitatively explains a variety of other ICL-related phenomena, including the long-tailed distribution of when ICL is acquired, the bimodal behavior of solutions close to the task diversity threshold, the influence of contextual and data distributional statistics on ICL, and the transient nature of ICL.",
    "response": {
      "Soundness": 3,
      "Presentation": 3,
      "Contribution": 3,
      "Rating": 8,
      "Confidence": 3,
      "Strengths": "The way models acquire generalization during training is an interesting topic and the paper provides an insightful exploration of the mechanisms involved. \nThe authors convincingly support the premise of the paper with theory and empirical studies.",
      "Weaknesses": "Because the model used for the empirical validation is stripped down to a one layer transformer model the authors note that they cannot be sure that the dynamics are the same for much larger models.\nThe authors could consider supplementing the one-layer analysis with tests on a deeper transformer model. \nEven if constrained by computational resources, experiments on a 2 or 3 layer transformer would help assess the applicability of their findings to larger networks.\n\nThe paper does not recommend methods to facilitate generalization. If possible, a discussion of how the ideas in the paper may be used to develop initialization and regularization techniques would make the submission even stronger.",
      "Questions": "Can the authors use this work to recommend methods to improve generalization? For example, can we choose an initialization method that might better support ICL?\n\nThe theory hints that ICL decays due to the use of regularization. Though using $L_2$ regularization makes the results more comparable with typical training, would it be possible to provide empirical results without regularization?"
    }
  },
  {
    "paper_id": "z1Jq1PLQWs",
    "summary": "Background: The paper focuses on Reinforcement Learning from Human Feedback (RLHF), where relative human preference feedback, often modeled using sigmoid utility models, is employed to refine AI systems. This form of feedback is cost-effective and less biased, providing a nuanced understanding of human values. Despite its popularity, existing RLHF algorithms lack rigorous theoretical foundations and face computational inefficiencies, hindering scalability to complex real-world scenarios.\nGap: Existing RLHF methods lack performance guarantees and face computational limitations, especially for large-scale problems or when adapting to dynamic environments. Current algorithms are often based on principles like UCB or Thompson Sampling, which can be computationally intractable and inefficient for real-world applications, limiting their practicality and scalability.\nHypothesis: The paper hypothesizes that by adopting a gradient descent-based approach, particularly a mirror descent method, it is possible to develop a no-regret learning algorithm that efficiently handles adversarial preferences and scales well to RLHF settings. This approach aims to provide optimal regret guarantees.\nMethod: The authors propose an online gradient descent algorithm utilizing a mirror descent approach to achieve optimal regret in the presence of adversarial preferences. The method relies on a weaker form of feedback—binary preferences—rather than traditional value feedback. They introduce 'Double-Scrible', an algorithm that estimates the loss function from preference feedback. The method is extended to handle batched pairwise preferences and ranking feedback. The algorithm's performance is analyzed against both upper and lower bounds for regret, demonstrating its optimality. The theoretical contributions are supported by empirical validations.\nConclusion: The paper presents the first efficient gradient descent-based algorithm for RLHF with adversarial preferences that guarantees near-optimal regret of O(sqrt(T)). It addresses critical computational challenges, enhancing the feasibility of preference-based learning in high-dimensional settings. The algorithm is extendable, accommodating various feedback models with improved performance guarantees, making it suitable for large-scale applications in AI alignment and LLM fine-tuning.\n",
    "full text": "\nDUELING IN THE DARK: AN EFFICIENT AND OPTIMAL\nMIRROR DESCENT APPROACH FOR ONLINE CONVEX\nOPTIMIZATION WITH ADVERSARIAL PREFERENCES\n\nAnonymous authors\nPaper under double-blind review\n\nABSTRACT\n\nRecent developments in Large Language Models (LLMs) have sparked significant\nattention in Reinforcement Learning from Human Feedback (RLHF). A simple,\nwidely used, and cost-effective method for gathering human feedback is through\nrelative queries based on human preferences, where the pairwise preference of\ntwo alternatives is often modeled as the sigmoid of their respective utility scores.\nDespite the popularity of these sigmoid-based RLHF frameworks, their theoreti-\ncal foundations remain underdeveloped as existing algorithms often lack the de-\nsired performance guarantees, or are limited to small-scale problems due to com-\nputationally intractable steps. We address this challenge by developing the first\nefficient online gradient descent-based algorithm for the problem with provably\noptimal performance guarantees. In fact, our proposed methods work even for ad-\nversarially changing preferences, unlike existing attempts, which assume a fixed\nunderlying stochastic preference model. Formally, we consider the adversarial on-\nline convex (linear) optimization (OLO) problem in d-dimensions, but unlike the\nexisting OLO framework, we assume only that the learner can observe a (weaker)\npreference feedback upon choosing a few alternatives at each round. With the\nobjective of identifying the best arm, we propose an efficient online mirror de-\nscent (OMD) based approach for the problem with regret and sample complexity\nguarantees. The main challenge lies in finding a suitable gradient approximation\nof the underlying (adversarially changing) utility functions solely from the weak\npreference feedback, as opposed to the conventional gradient or value feedback\nused in OLO. We also extend our methods beyond pairwise preferences to multi-\nway preference (B-sized batched pairwise) and partial ranking feedback with im-\nproved performance guarantees. Additionally, our algorithms are optimal as we\nproved by matching lower bounds closing the potential of any better algorithms\nfor the settings. Our contribution lays the groundwork for a practical gradi-\nent descent-based algorithm in RLHF. Supported by robust theoretical guarantees,\nour approach holds promise in the current landscape of developing efficient algo-\nrithms for LLMs and addressing human-AI alignment challenges.\n\n1\n\nINTRODUCTION\n\nThe rapidly advancing field of AI has sparked interest in Reinforcement Learning from Human\nFeedback (RLHF), which incorporates human input to refine AI systems, mitigating risks in au-\ntonomous decision-making and fostering systems that act aligned with users’ best interests. This\npaper explores the theoretical aspects of RLHF with preference feedback, emphasizing its potential\nto enhance AI alignment.\n\nHuman preference feedback is a critical form of feedback within the field of machine learning (ML).\nUnlike conventional feedback models used in ML optimization literature for designing predictive\nAI models, which includes demonstration Hussein et al. (2017); Swamy et al. (2023); Torabi et al.\n(2018), gradient-based Zinkevich (2003); Boyd et al. (2004); Fletcher (2013), value-based feedback\nFlaxman et al. (2005); Cesa-Bianchi & Lugosi (2006b); Shamir (2015); Saha (2021a), preference\nfeedback is a weaker form of feedback that receives only relative desirability (a.k.a. preference) of\ndifferent outcomes/actions for a given task. However, on the positive end, preference feedback can\n\n1\n\n000\n001\n\n002\n003\n004\n005\n006\n007\n\n008\n009\n010\n011\n012\n013\n\n014\n015\n016\n017\n018\n\n019\n020\n021\n022\n023\n024\n\n025\n026\n027\n028\n029\n030\n\n031\n032\n033\n034\n035\n036\n\n037\n038\n039\n040\n041\n\n042\n043\n044\n045\n046\n047\n\n048\n049\n050\n051\n052\n053\n\n\fUnder review as a conference paper at ICLR 2025\n\n054\n055\n\n056\n057\n058\n059\n060\n061\n\n062\n063\n064\n065\n066\n067\n\n068\n069\n070\n071\n072\n\n073\n074\n075\n076\n077\n078\n\n079\n080\n081\n082\n083\n084\n\n085\n086\n087\n088\n089\n090\n\n091\n092\n093\n094\n095\n\n096\n097\n098\n099\n100\n101\n\n102\n103\n104\n105\n106\n107\n\ncapture a more nuanced understanding of human values and priorities by tallying the relative desir-\nability of different outcomes. Studies in psychology and cognitive neuroscience also corroborate the\nfact that humans are often naturally more comfortable providing relative feedback compared to the\nother modes (Musallam et al., 2004; Kahneman & Tversky, 1982), hence the training data tend to be\nless biased and resource-efficient. Consequently, this form of feedback enables AI systems to learn\nmore complex and subtle aspects of human intentions, which are often difficult to encode through\ndemonstrations or reward feedback. This also makes RLHF with preference feedback a powerful\ntool for improving the reliability and safety of AI systems in practice.\n\nTo understand the RLHF with preference feedback problem Wu & Sun (2024); Xie et al. (2024);\nXiong et al. (2024); Rafailov et al. (2024) more formally: In the simple online/active exploration\nRLHF with preference feedback setting, the learner can sequentially query a pair of actions and\nreceive binary 0–1 preference feedback indicating the preferred item. The objective for these classes\nof problems is usually to find a good (value-maximizing) policy π : C (cid:55)→ D, a mapping from the\ncontext space C to decision space D, as efficiently as possible. The decision space D represents\nthe set of actions/alternatives to learn from, e.g., for language models D could be the class of all\nwords (or tokens), the set of trajectories for autonomous car driving, or the set of movies for a\nmovie-recommender system, etc.\n\nExisting work on preference-based learning for online (exploratory) RLHF, whether empirical or\ntheoretical, is limited by computationally inefficient algorithms (Xie et al., 2024; Xiong et al., 2024).\nMany current approaches struggle to scale effectively with the complexity of real-world scenarios,\noften requiring extensive computational resources and time to process human feedback and update\nAI models accordingly. This inefficiency not only hampers the practical deployment of preference-\nbased learning systems but also restricts their ability to quickly adapt to dynamic environments and\nevolving human preferences.\n\nLimitations of Existing Online RLHF with Preference Feedback Algorithms. The well-cited\nwork of Rafailov et al. (2024); Ouyang et al. (2022); Chen et al. (2024) use offline data in nature\nwhich does not allow active exploration and also lack convergence guarantees. Recently the liter-\nature saw a surge of papers on online RLHF with preference feedback (Xu et al., 2020; Chatterji\net al., 2021; Saha et al., 2023; Saha, 2021a; Kausik et al., 2024; Das et al., 2024), however, these\nalgorithms are based on the optimism in the face of the uncertainty (UCB based) principle which\nrequires maintaining confidence sets and optimizing over the policy space which could be compu-\ntationally intractable. Few studies (Efroni et al., 2021; Li et al., 2024; Wu & Sun, 2024) have also\nconsidered Thompson Sampling (TS) approaches as an alternative but again updating and sampling\nfrom the posterior distribution could be computationally hard as well, making them impractical for\nreal-world applications. Quoting from Xie et al. (2024), “However, the most powerful approaches in\nthis space are computationally intractable in the general reinforcement learning (RL) setting (Jiang\net al., 2017; Jin et al., 2021; Foster et al., 2021), and prior attempts to adapt them to RLHF either\nmake unrealistic modeling assumptions (i.e., do not allow for general function approximation) (Xu\net al., 2020; Novoseller et al., 2020; Saha et al., 2023; Wu & Sun, 2024; Zhan et al., 2023; Du et al.,\n2024; Das et al., 2024), or are computationally inefficient and not feasible to faithfully implement\n(Chen et al., 2022; Wang et al., 2023; Ye et al., 2024),” which nicely summarizes the state of the\nliterature. In fact, the computational efficiency of Xie et al. (2024) itself is in question since they\nrequire to optimize in the policy space using methods like PPO (Schulman et al., 2017) which might\nnot be runtime efficient unless the policy space is finite or parameterized under some restrictive\nassumptions.\n\nConsequently, there is a pressing need for the development of more computationally efficient al-\ngorithms that can harness preference feedback in a timely and resource-effective manner, thereby\nenhancing the feasibility and responsiveness of AI alignment strategies. In this work, we present\nthe first mirror (gradient) descent-based algorithm for the problem with an optimal performance\nguarantee.\n\n1.1 CONTRIBUTIONS\n\nFor simplicity, we frame the RLHF problem as a best-arm identification problem in the online linear\noptimization (OLO) framework Shamir (2015); Hazan (2019) in dimension d, with pairwise prefer-\nence feedback (see Sec. 2). Note this is the first step in designing gradient descent-based approaches\n\n2\n\n\fUnder review as a conference paper at ICLR 2025\n\nfor RLHF which certainly will play a critical role in extending these methods for more complex\npolicy optimization settings.\n\n√\n\nKey Contribution. Our contributions are multifaceted. At a technical level, we are the first to\naddress the problem of adversarial online linear optimization with preference feedback, which has\ncritical implications in the RLHF with preference feedback literature. Our specific contributions\nare: •(1) Algorithmic Contribution. Our key contribution is to design an online mirror-descent\nbased (OMD) algorithm to obtain a near-optimal ˜O(d\nT ) regret algorithm for online optimization\nwith adversarial preferences (Alg. 1,Thm. 1). Our algorithm is motivated by the Scrible algorithm\nof Hazan (2019). However, Scrible operates under ‘value feedback’, as opposed to preference feed-\nback of our setting, which allowed them to use the standard ‘one-point gradient estimation’ tech-\nnique (Flaxman et al., 2005) to estimate the loss functions. One of our primary contributions in\nour proposed algorithm Double-Scribleis to estimate the loss function per round from the weaker\npreference feedback. Consequently, we had to adapt to a different proof analysis to incorporate\nthe changes which finally led to near-optimal (upto log factors) ˜O(d\nT ) regret bounds of Double-\nScrible (Thm. 1). •(2) Performance Limit Analysis. To understand the effectiveness of our anal-\nysis, we further prove a matching lower bound of Ω(d\nT ) to show that our algorithm is within a\nlogarithmic factor of the optimal performance limit (Thm. 3). Deriving the lower bound for this\nproblem was non-trivial, we derive this from the first principle lifting tools from the classical litera-\nture of information theory.\n\n√\n\n√\n\nAdditional Contributions. We enriched and extended our above result with pairwise preference in\nmultiple ways:\n\n1. In Sec. 4, we first generalize the above algorithm to multiwise (batched) preference feedback,\nwhere the learner can query a set of B pairwise preferences in one go. In many settings, it is\nnot feasible to actively update the model’s prediction after every round, perhaps due to commu-\nnication delays, parallel processing or time/ cost overhead. Instead, the system may prefer to\ncollect a bunch of comparison queries in a batch and then update its model to generate the next\nset of queries. In such settings, batched RLHF is a natural model to consider. Our improved\nanalysis of Alg. 2, the batched variant of Double-Scrible shows that one can achieve a faster\n˜O(\n\nT ) regret learning rate for this case (Thm. 4).\n\nd√\n\n√\n\nmin{B,d}\n\n2. Next we consider another interesting feedback model in Sec. 5 which generalizes pairwise pref-\nerence feedback to subsets of size k (for any k ≥ 2), and allows the learners to query partial\nrank ordered feedback of length m ∈ {1, . . . k}. The objective was to understand if the learning\nalgorithm is allowed to query from a larger set of k alternatives and obtain a richer m-length\nranking feedback, can it learn faster? What is the optimal trade-off of the learning rate with m\nand k? Our proposed algorithm MNL-Scrible addresses this setting with a regret guarantee of\n˜O(\nT ). The k (subsetsize) independence of the result could be surprising to many\n\nd√\n\n√\n\nmin{m,d}\n\nas one may expect that larger subsets may lead to faster convergence! However, this is not the\ncase we explained in Rem. 6. On the other hand, as expected the regret indeed improves with the\nincreasing length of the rank-ordered feedback m. Our algorithm MNL-Scrible actually exploits\nthe key ideas of our batched algorithm Alg. 2 by cleverly extracting m-batched pairwise prefer-\nence information from the Top-m ranking feedback σm,t. We describe the algorithm in Sec. 5\nand its regret performance follows almost immediately from Thm. 4.\n\n3. Same as Thm. 3, we corroborate the performance analysis of all our algorithms with their cor-\nresponding lower bound analysis to understand the tightness of our algorithmic guarantees. Pre-\ncisely, for the B-batched feedback setting we show that our regret guarantee of BaBle-Scrible is\noptimal (up to log factors) with a matching lower bound analysis Thm. 6. Similarly, our Top-m\nranking feedback lower bound of Thm. 9 justifies the tightness of regret performance of MNL-\nScrible. All our lower bounds are derived from the first principles of information theory.\n\nAdvantage of Gradient Descent Methods: Gradient-based methods have multiple advantages\ncompared to confidence-based methods:\n(1) GD/OMD handle high-dimensional problems effi-\nciently due to their reliance on gradient information: (2) They are suitable for both stochastic and\nadversarial environments, making the gradient-based methods robust to changing data distributions\nor the underlying loss/reward functions which is often more practical for modeling real-world prob-\n\n3\n\n108\n109\n\n110\n111\n112\n113\n114\n115\n\n116\n117\n118\n119\n120\n121\n\n122\n123\n124\n125\n126\n\n127\n128\n129\n130\n131\n132\n\n133\n134\n135\n136\n137\n138\n\n139\n140\n141\n142\n143\n144\n\n145\n146\n147\n148\n149\n\n150\n151\n152\n153\n154\n155\n\n156\n157\n158\n159\n160\n161\n\n\fUnder review as a conference paper at ICLR 2025\n\nlems, (3) These methods can optimize a wide range of objective functions, including non-linear,\nnon-convex, and constrained problems, (4) Gradient descent algorithms are simple to implement,\neven seamlessly integrate with modern deep learning frameworks, making these methods computa-\ntionally efficient, unlike many UCB and TS based methods which often do not have a closed form\nsolution Saha et al. (2023); Das et al. (2024) or sampling from the posteriors could be complicated\nNovoseller et al. (2020), and (5) Gradient descent techniques are inherently robust to model mis-\nspecification and smoothly integrate with differential privacy techniques.\n\n2 PROBLEM SETUP\n\nNotation. Let [n] = {1, . . . n}, for any n ∈ N. Given a set S and two items x, y ∈ S, we denote\nby x ≻ y the event x is preferred over y. For any r > 0, let Bd(r) and Sd(r) denote the ball and the\nsurface of the sphere of radius r in d dimensions respectively. Id denotes the d × d identity matrix.\nFor any vector x ∈ Rd, ∥x∥2 denotes the ℓ2 norm of vector x. 1(φ) is generically used to denote an\nindicator variable that takes the value 1 if the predicate φ is true and 0 otherwise. Unif(S) denotes\na uniform distribution over any set S. We write ˜O for the big O notation up to logarithmic factors.\nFor any set Ω ⊂ Rd, int(Ω) denotes the interior of the set Ω. Ber(p) defines Bernoulli distribution\nwith parameter p ∈ [0, 1].\n\n2.1 PROBLEM: ADVERSARIAL LOGISTIC DUELING BANDITS (LO G I T-DB):\n\nWe consider an online T round sequential decision-making game on a decision space D ⊂ Rd\nin the Adversarial Online Linear Optimization (Bandits) Hazan (2019); Abernethy et al. (2008)\nframework. At every round, the algorithm plays xt, yt ∈ D and observes a binary feedback ot s.t.\nσ(cid:0)θ∗\n\n⊤(xt − yt)(cid:1)(cid:17)\n\not ∼ Ber\n\n(cid:16)\n\n.\n\nt\n\nWe denote the probability of arm x being preferred over arm y as:\n\nPt(x, y) = σ(cid:0)θ∗\n\nt\n\n⊤(x − y)(cid:1) =\n\nexp(θ∗\nt\nNote we call the problem Logit-DB since the preference relation Pt follows a logistic model, as\nσ : R (cid:55)→ [0, 1] is the logistic link function, i.e. σ(x) = (1 + e−x)−1.\n\n⊤y)\n\n⊤x)\n\nexp(θ∗\nt\n⊤x) + exp(θ∗\nt\n\n, ∀x, y ∈ D.\n\nObjective-I: Regret Minimization w.r.t. the Best Choice. The goal of the algorithm is to mini-\nmize the cumulative regret, defined as:\n\nRegLogit-DB\nT\n\n:=\n\nT\n(cid:88)\n\nt=1\n\n(cid:20) (Pt(x∗, xt) − 1/2) + (Pt(x∗, yt) − 1/2)\n2\n\n(cid:21)\n,\n\nassuming x∗ ← arg maxx∈D\n\n(cid:80)T\n\nt=1 θ∗\n\nt\n\n⊤x the best (highest scoring) arm in the hindsight.\n\n.\nRemark 1. For any x ∈ D, note then θ∗\n⊤(x∗ − x), when\nD ⊆ Bd(1). We prove this in App. A. Consequently, in the rest of the paper, we will address the\nregret\n\n≤ Pt(x∗, x) − 1/2 ≤ θ∗\nt\n\n⊤(x∗−x)\n\n4\n\nt\n\n(cid:100)Reg\n\nLogit-DB\nT\n\n:=\n\nT\n(cid:88)\n\n(cid:20) θ∗\nt\n\n⊤(x∗ − xt) + θ∗\nt\n\n⊤(x∗ − yt)\n\nt=1\n\n2\n\n(cid:21)\n,\n\nLogit-DB\nnoting RegLogit-DB\nT\nsuffice to bound RegLogit-DB\n\n≤ (cid:100)Reg\n\nT\n\n.\n\nT\n\nfrom Rem. 1, thus designing algorithm to bound (cid:100)Reg\n\nLogit-DB\nT\n\nwould\n\nObjective-II: Sample Complexity. One can also consider a different learning objective where\ninstead of regret minimization the goal is to find ‘ϵ-best arm’. Precisely, we define an arm ˆx ∈ D\n⊤(x∗ − ˆx) < ϵ. The goal could be find any such ˆx ∈ D with the\nto be an ϵ-best arm if 1\nT\nleast number of samples.\n\nt=1 θ∗\n\n(cid:80)T\n\nt\n\n4\n\n162\n\n163\n164\n165\n166\n167\n\n168\n169\n170\n171\n172\n173\n\n174\n175\n176\n177\n178\n179\n\n180\n181\n182\n183\n184\n185\n\n186\n187\n188\n189\n190\n\n191\n192\n193\n194\n195\n196\n\n197\n198\n199\n200\n201\n202\n\n203\n204\n205\n206\n207\n208\n\n209\n210\n211\n212\n213\n\n214\n215\n\n\fUnder review as a conference paper at ICLR 2025\n\nRemark 2. An attentive reader might have already noticed that for our problem setting, regret\nminimization is a stronger objective than the latter as a regret guarantee of a learning algorithm\nimmediately yields a valid sample complexity bound for the same setting ˆx = 1\nT\n\n(xt+yt)\n2\n\n(cid:80)T\n\nt=1\n\n.\n\nTo understand Rem. 2 more formally, consider any algorithm A with regret bound (cid:100)Reg\nRA(T ). Then\nT\n(cid:88)\n\nT\n(cid:88)\n\nT\n(cid:88)\n\n(cid:21)\n\n⊤\n\n1\nT\n\nt=1\n\nθ∗\nt\n\n⊤(x∗ − ˆx) =\n\n1\nT\n\n(cid:20)\nx∗ −\n\nθ∗\nt\n\n(xt + yt)\n2\n\nt=1\n\n1\nT\n\n(cid:21)\n\nt=1\n\nT\n(cid:88)\n\nT\n(cid:88)\n\n(cid:20)\nT θ∗\nt\n\n=\n\n1\nT 2\n\n1\nT\nLet use define g(a) = RA(a)/a for any a ∈ N, then equating g(T ) ≤ ϵ, the desired sample\ncomplexity T = g−1(ϵ).\n\n(xt + yt)\n2\n\n(xt + yt)\n2\n\nRA(T )\nT\n\n⊤x∗ −\n\n⊤x∗ −\n\nt=1\n\nt=1\n\nt=1\n\nt=1\n\n≤\n\n=\n\n.\n\nT\n(cid:88)\n\n(cid:20)\nθ∗\nt\n\nT\n(cid:88)\n\n(cid:21)\n\nLogit-DB\nT\n\n(A) ≤\n\n2.2 PROBLEM SETUP: ADVERSARIAL BATCHED LO G I T-DB\n\nAs motivated in the introduction, from a practical viewpoint it might be hard to actively up-\ndate the parameters of the learning algorithm at each round t due to time or cost overhead.\nDistributed deployment of the system might also hinder an active sequential adaptation of the\nlearning algorithm due to parallel processing. Thus a natural variant of Logit-DB problem\nis to consider a batched feedback model where the learner gets to (actively) query B-pairwise\nqueries in a batched fashion: Precisely, at each round t,\nthe learner gets to query B-pairs\nt , yB\nt , y1\n{(x1\nt )} together and observes the corresponding B-pairwise prefer-\nt ∼ Ber(cid:0)σ(cid:0)θ∗\nences o1\n\nt ), . . . , (xB\nt , where oi\n\nt , y2\nt , . . . oB\n\nt ), (x2\nt , o2\n\nt − yi\n\nt)(cid:1)(cid:1).\n\n⊤(xi\n\nt\n\nRegret Objective. Same as (cid:100)Reg\nminimize the regret over T rounds, defined as:\n\nBatched-LogitDB\nT\n\n, the objective of the learner, in this case, is to\n\n(cid:100)Reg\n\nBatched-LogitDB\nT\n\n:=\n\nT\n(cid:88)\n\nt=1\n\n1\nB\n\n(cid:20) B\n(cid:88)\n\nθ∗\nt\n\ni=1\n\n⊤(x∗\n\nt − xi\n\nt) + θ∗\nt\n2\n\n⊤(x∗\n\nt − yi\nt)\n\n(cid:21)\n.\n\nNote we can also consider the exact same Sample complexity objective in this setting as well, as\ndefined above for Logit-DB in Sec. 2.1.\n\n2.3 PROBLEM SETUP: ADVERSARIAL MULTINOMIAL LOGIT BANDITS (MNL-BA N D I T S):\n\nIn this setting, we generalize the Logit-DB problem (Sec. 3) to the subsetwise case, where\nthe learner can observe preference feedback over a subset of items. More formally, as be-\nfore, we consider a decision space D ⊂ Rd and at every round the algorithm plays a k-subset\nSt := {x1\nt } ⊆ D and gets to see Categorial feedback ot ∈ [k] indicating the index of the\n(cid:1), where pt\nwinning arm in St s.t. ot ∼ Categorial(cid:0)pt\n\nt , . . . xk\n\ni = exp(θ∗\n\n2, . . . , pt\nk\n\nt , x2\n\n1, pt\n\n(cid:80)k\n\n.\n\nt\n\n⊤xi\nt)\n⊤xj\nj=1 exp(θ∗\nt )\nt\n\nIn fact, one could even consider a top-m ranking generalization of the above feedback model. Let for\nany subset S, ΣS = {σ | σ is a permutation over items of S}, where for any permutation σ ∈ ΣS,\nσ(i) denotes the element at the i-th position in σ, i ∈ [|S|]. Then one can further define Σm\nS :=\n{σm := (σ(1), . . . , σ(m)) | σ ∈ ΣS} and formally define the top-m ranking feedback as follows:\n\nGeneralized Top-m ranking of items (TR-m): In this case at every round t the environment\nreturns a ranking of the top m items from St (|St| = k) by drawing a full ranking σt ∈ ΣSt over St\naccording to Plackett-Luce (PL) model without replacement, and returns the first m ranked elements\nof σ, i.e., (σ(1), . . . , σ(m)). More precisely, if σm,t is any random top-m ranking on a k-subset St\ndrawn according to the multinomial (MNL) model (Saha & Gopalan, 2019), then for every position\ni ∈ [m], 1 ≤ m ≤ k − 1,\n\nσm,t(i) ∼ Categorial(cid:0)pt\n\n1, pt\n\n2, . . . , pt\n\nk\\{pt\n\nσm,t(1), pt\n\nσm,t(2), . . . pt\n\nσm,t(i−1)}(cid:1), s.t. pt\n\ni =\n\n⊤xi\nexp(θ∗\nt)\nt\n⊤xj\nj=1 exp(θ∗\nt )\n\nt\n\n(cid:80)k\n\n,\n\n5\n\n216\n217\n\n218\n219\n220\n221\n222\n223\n\n224\n225\n226\n227\n228\n229\n\n230\n231\n232\n233\n234\n\n235\n236\n237\n238\n239\n240\n\n241\n242\n243\n244\n245\n246\n\n247\n248\n249\n250\n251\n252\n\n253\n254\n255\n256\n257\n\n258\n259\n260\n261\n262\n263\n\n264\n265\n266\n267\n268\n269\n\n\fUnder review as a conference paper at ICLR 2025\n\nis drawn by successively sampling m winners from S according to the PL model, without replace-\nment. Thus (equivalently) we have:\n\nPt(σm,t|S) =\n\nm\n(cid:89)\n\ni=1\n\n(cid:80)m\n\nj=i exp(θ∗\n\nt\n\n⊤xσm,t(i)\nexp(θ∗\nt\nt\n) + (cid:80)\n\n)\n\nj∈St\\m\n\n⊤xσm,t(j)\nt\n\nexp(θ∗\nt\n\n⊤xσm,t(j)\nt\n\n)\n\n,\n\nNote for k = 2, the top-m ranking feedback is equivalent to the dueling feedback discussed in\nSec. 2.1.\n\nRegret Objective. Following the objective from Logit-DB from Sec. 2.1 and Rem. 1, one can\nsimilarly define the regret for this setup as:\n\nsuch that St\\m = St \\ {σm,t(i)}m\n\ni=1.\n\n(cid:100)Reg\n\nMNL\nT\n\n:=\n\nT\n(cid:88)\n\nt=1\n\n(cid:18) 1\nk\n\n(cid:88)\n\nθ∗\nt\n\nx∈St\n\n(cid:19)\n\n⊤(x∗ − x)\n\n,\n\nagain assuming x∗ := arg maxx∈D\n\n(cid:80)T\n\nt=1 θ∗\n\nt\n\n⊤x the best arm in the hindsight.\n\n3 DUELING CASE: ALGORITHM FOR LO G I T-DB PROBLEM\n\nIn this section, we investigate the Logit-DB problem (Sec. 2.1) for the pairwise preference (duel-\ning) feedback.\n\n(cid:111)\n\n1√\n\n:= {x |\n\nτ =1 g⊤\n\nτ w + ψ(w)\n\nAlgorithm description: Our algorithm is motivated by the Scrible algorithm from Abernethy\net al. (2008); Hazan (2019), which is a variant of the online mirror descent algorithm with a\nself-concordant barrier Boyd et al. (2004) as the regularizer.1 The algorithm iteratively updates\nthe decision variable wt by minimizing the sum of the ψ-regularized linearized loss within the\n1\nδ-contracted decision set Dδ\n1−δ x ∈ D}. Precisely at each step t, we compute\n(cid:110)\nη (cid:80)t−1\nwt = arg minw∈Dδ\n. We then perform eigendecomposition of the Hes-\nsian ∇2ψ(wt), sample an index it uniformly at random from [d], and generate perturbed solu-\ntions xt = wt + γt\nvt,it. It is important here to note that\nxt, yt ∈ D owing to the properties of self-concordant barrier functions, as argued in Lem. 10.\nBy playing the pair (xt, yt) and observing the outcome ot, we construct the gradient estimator\n2 )(cid:112)λt,itvt,it for the next iteration and continue to the step iteration. Thm. 1 analyze\ngt = d\nγt\n√\nthe regret performance of Alg. 1 yielding an optimal O(\nT ) regret for the problem, as justified in\nRem. 4. Due to space limitations, the algorithm pseudocode is given in App. B.1 and the detailed\nregret analysis of Double-Scrible (Alg. 1) is given in App. B.3.\nTheorem 1 (Regret Analysis of Alg. 1). Consider the decision space D, such that ∇2ψ(w) ≥\n√\nH 2\nT and γt ≤ 0.7HD,ψ, the Double-\n√\nScrible (Alg. 1) guarantees a regret bound:\n\nD,ψId, ∀w ∈ D. Then for the choice of η =\n\nvt,it and yt = wt − γt\n\nνHD,ψ\nT log T\n\n(ot − 1\n\n, δ = 1\n\n1√\n\nλt,it\n\nλt,it\n\nd\n\n(cid:100)Reg\n\nLogit-DB\nT\n\n≤ O\n\n(cid:18) d\n\n√\n\nνT log T\nHD,ψ\n\n(cid:19)\n.\n\nIt is worth noting that HD,ψ is generally a problem-dependent constant for bounded decision sets\nand most choices of ψ, as we explain in Rem. 7.\nRemark 3 (Minimal Eigenvalue Assumption). Thm. 1 holds assuming the minimal eigenvalue of\n∇2ψ(w) is larger than H 2\nD,ψ. This assumption was not required in the analysis of Scrible Abernethy\net al. (2008), however, we could not circumvent it. The reason we are required to make this assump-\ntion lies in the fact the reward model we optimize is a non-linear model, whereas the reward model\nin Abernethy et al. (2008) is linear in w. This assumption is equivalent for assuming ψ(w) is strongly\nconvex and may hold for different choices of decision sets. For example, for a decision set which\n\n1Interested readers may check Luo (2017); Boyd et al. (2004); Hazan (2019) for the properties and examples\n\nof self-concordant barrier functions.\n\n6\n\n270\n\n271\n272\n273\n274\n275\n\n276\n277\n278\n279\n280\n281\n\n282\n283\n284\n285\n286\n287\n\n288\n289\n290\n291\n292\n293\n\n294\n295\n296\n297\n298\n\n299\n300\n301\n302\n303\n304\n\n305\n306\n307\n308\n309\n310\n\n311\n312\n313\n314\n315\n316\n\n317\n318\n319\n320\n321\n\n322\n323\n\n\fUnder review as a conference paper at ICLR 2025\n\nis the interior of the unit ball Bd(1) and choosing ψ(w) = − ln(1 − ∥w∥2\n2) is a 1-self concordant\nbarrier and it is easy to check that H 2\ni=1 ln wi,\nwhich is a d-self-concordant barrier in the unit ball Bd(1) and it is straightforward to verify that in\nthis case H 2\n\nD,ψ = 2. Another example could be ψ(w) = − (cid:80)d\n\nD,ψ = d.\n\nRemark 4 (Optimality of Thm. 1). The rate depicted in Thm. 1 is optimal (up to logarithmic fac-\ntors), as follows from the existing lower bound of the Logit-DB problem (Saha (2021b)).\nRemark 5 (Advantage of Our Approach over Existing Algorithms for Logit-DB). (1) Prior works\nthat considered online learning in the generalized linear bandit setting Li et al. (2017; 2024) are\nrequired to assume a lower bound on the derivative of the sigmoid link function, which results in\nt ∥≤1 σ′(θ⊤(x − y)) in the Logit-DB\na multiplicative dependency on κ = mint∈[T ] arg inf ∥θ−θ∗\nproblem Saha et al. (2023); Das et al. (2024). Interestingly, we do not need to make this assumption,\nowing to the nice trick of exploiting the pairwise preference of symmetrically opposite points xt\nand yt, as shown in Lem. 11. This is a clear advantage of our approach over the existing GLM-\nbandits based approach for Logit-DB which relies on UCB estimation based confidence bounding\ntechnique. (2) Further, since our approach relies on gradient-based techniques, they are extremely\ncomputationally efficient–the runtime requirement of our method is just O(dT ), compared to the\nprior methods which are computationally infeasible and not implementable in practice Saha et al.\n(2023); Kausik et al. (2024); Das et al. (2024).\n\nDue to page limitations, the complete proof of Thm. 1 is moved to App. B.\n\nWe also note that the sample complexity bound of Double-Scrible (Alg. 1) directly follows from\nRem. 2, leading to the following result.\nCorollary 2 (Sample Complexity Bound of Double-Scrible (Alg. 1)). Under the parameter settings\n\nof Thm. 1, the ϵ-sample complexity of Double-Scrible (Alg. 1) is roughly O\n\n(cid:18)\n\n(cid:19)\n\n.\n\nd2ν\nD,ψϵ2\n\nH 2\n\nTheorem 3 (Regret Lower Bound for Logit-DB Problem). Consider any fixed time step T . Then\nfor any algorithm A for the Logit-DB problem, there exists a decision space D ⊂ Rd and a\nsequence of unknown linear functionals θ∗\nT ∈ Rd, such that the regret of algorithm A in T\n≥ d\nrounds (cid:100)Reg\n\n1, . . . θ∗\n\nLogit-DB\nT\n\n√\n\n.\n\nT log T\n256\n\n4 BATCHED FEEDBACK: ALGORITHM FOR B-BATCHED LO G I T-DB\n\nPROBLEM\n\nIn this section, we will analyze the batched variant of Logit-DB problem as described in Sec. 2.2.\nRecall that in this the learner can actively query B-pairwise preferences in a batched fashion.\n\nThis section can be considered a primer to our algorithm for the rank-ordered feedback setting (see\nSec. 2.3) that we discussed in Sec. 5, as we will use a nice reduction of ranked-feedback setting to\nthe batched-feedback setting.\n\n4.1 ALGORITHM FOR BA T C H E D-LO G I TDB\n\nOur proposed algorithm for this case is Batched-DouBle-Scrible (BaBle-Scrible) which is a variant\nof Double-Scrible we detailed in the previous section for the Logit-DB. Same as algorithm, it\ntakes input parameters η, δ, and γt, and a ν-self concordant barrier function ψ.\nSimilar to Alg. 1, in this case too the idea is to build an estimate of θ∗\nt from the pairwise observations.\nHowever, due to the batched feedback of size B, we can build an estimate with better variance\nT ). However, one would\nleading to\n\nB-factor improvement in the final learning rate of O( d√\nB\nneed B ≤ d since it is impossible to obtain a regret rate better than Ω(\nobtains in the full information setting Zinkevich (2003).\n\nT ), which is the rate one\n\n√\n\n√\n\n√\n\nMore precisely, at any round t, assuming wt is the running estimate of the optimizer over the de-\ncision set Dδ, our proposed algorithm BaBle-Scrible first computes the eigendecomposition of the\nHessian ∇2ψ(wt) = (cid:80)d\nt , uniformly from [d].\n\nt,i, and samples B indices i1\n\ni=1 λt,ivt,iv⊤\n\nt , . . . , iB\n\nt , i2\n\n7\n\n324\n325\n\n326\n327\n328\n329\n330\n331\n\n332\n333\n334\n335\n336\n337\n\n338\n339\n340\n341\n342\n\n343\n344\n345\n346\n347\n348\n\n349\n350\n351\n352\n353\n354\n\n355\n356\n357\n358\n359\n360\n\n361\n362\n363\n364\n365\n\n366\n367\n368\n369\n370\n371\n\n372\n373\n374\n375\n376\n377\n\n\fUnder review as a conference paper at ICLR 2025\n\nvt,iℓ\n\nand yℓ\n\nt = wt + γt\n\nUpon this it assigns xℓ\n\n1\n2(cid:113)λt,iℓ\nt ), . . . , (xB\n\n1\n2(cid:113)λt,iℓ\nt , y2\nt , yB\nbatch of B-pairs {(x1\nt )}. Upon this it receives the corresponding B\nt and computes a gradient (θ∗\npairwise preferences o1\nt, where\nt − 1\nt = d\ngℓ\n. The process is then repeated for a T rounds, iteratively, refining the\nλt,iℓ\n2\nγt\nrunning estimate wt+1 by minimizing the sum of the ψ-regularized linearized loss over Dδ. Due to\nspace limitations, the pseudocode of BaBle-Scrible is given in App. C.1.\n\nt , y1\nt ), (x2\nt , . . . , oB\nvt,iℓ\n\nt ) estimate gt = 1\nk\n\nt = wt − γt\n\n. and plays the\n\nℓ=1 gℓ\n\nvt,iℓ\n\n(cid:0)oℓ\n\n(cid:1)(cid:113)\n\n(cid:80)k\n\nt\n\nt\n\nt\n\nt\n\nt\n\nt\n\n√\n\nd\nmin{d,B}\n\nT (cid:1) regret for the the problem.\n\nThm. 4 analyzes the regret performance of Alg. 2 which is shown to yield an optimal\nO(cid:0)\nTheorem 4 (Regret Analysis of Alg. 2). Consider the decision space D, such that ∇2ψ(w) ≥\n√\nH 2\nD,ψI, ∀w ∈ D. Then for the choice of η =\nBaBle-Scrible (Alg. 1) guarantees a regret bound:\n(cid:18)\n\nT and γt ≤ 0.7HD,ψ, the\n\nν min{B,d}HD,ψ\nT log T\n\n, δ = 1\n\n√\n\n(cid:19)\n\n√\n\nd\n\n(cid:100)Reg\n\nBatched-LogitDB\nT\n\n≤ O\n\nd\n\nνT log T\n(cid:112)min{B, d}HD,ψ\n\n.\n\nThe regret analysis of BaBle-Scrible (Alg. 2) is given in App. C.2.\n\nSimilar to Corollary 2, one can derive the sample complexity bounds for BaBle-Scrible (Alg. 2)\nusing Rem. 2:\nCorollary 5 (Sample Complexity Bound of BaBle-Scrible (Alg. 2)). Under the parameter settings\n\nof Thm. 4, the ϵ-sample complexity of BaBle-Scrible (Alg. 2) is roughly O\n\n(cid:18)\n\nd2ν\n\nmin{d,B}H 2\n\nD,ψϵ2\n\n(cid:19)\n.\n\nTheorem 6 (Regret Lower Bound for Batched Logit-DB Problem). Consider any fixed time step\nT and batched size B. Then for any algorithm A for the B-Batched Logit-DB problem, there\n1, . . . θ∗\nexists a decision space D ⊂ Rd and a sequence of unknown linear functionals θ∗\nT ∈ Rd,\nsuch that the regret of algorithm A in T rounds (cid:100)Reg\n.\n\nBatched-Logit-DB\nT\n\n√\n√\n\n≥ d\n256\n\nT log T\nmin{B,d}\n\n5 RANKING FEEDBACK: ALGORITHM FOR MNL-BA N D I T S PROBLEM\n\nIn this section, we investigate the MNL-Bandits problem (Sec. 2.3) for the general top-m rank-\ning feedback. We first analyze the fundamental performance limit proving a lower bound for the\nproblem. Following this we design an optimal algorithm matching the lower bound.\n\n5.1 PROPOSED ALGORITHM: MNL-Scrible\n\nUseful Notations. We will find it useful to define some notations before describing our main\nalgorithm MNL-Scrible.\n\nWe denote by Vn = {(±1)n}, for any n ∈ N+.\nClearly |Vn| = 2n. Let G(Vn) be the graph with\nvertex set Vn ⊆ {±1}n and there exists an (undi-\nrected) edge between two nodes v and ˜v iff v and\n˜v only differs sign in one of the n coordinates, i.e.\n∃k ∈ [n], v(k) = ˜v(k) and v(k′) = ˜v(k′) for any\nk′ ̸= k. Clearly the number of neighboring nodes\nof any vertex v ∈ Vn in graph G is |N (v, G)| = n.\nIn other words, the degree of any node in graph G\nis n. We show an example for n = 3 in the right\nfigure. Also, let us define ℓk = ⌊log k⌋.\n\nAlgorithm Description: MNL-Scrible As before (in Alg. 1 and Alg. 2), this algorithm too maintains\na running estimate of the minimizer wt (initialized to w1 ∈ D), and find the eigen decomposition\nof the hessian at wt, say ∇2ψ(wt) = (cid:80)d\n\ni=1 λitvt,iv⊤\nt,i.\n\n8\n\n378\n379\n\n380\n381\n382\n383\n384\n385\n\n386\n387\n388\n389\n390\n391\n\n392\n393\n394\n395\n396\n\n397\n398\n399\n400\n401\n402\n\n403\n404\n405\n406\n407\n408\n\n409\n410\n411\n412\n413\n414\n\n415\n416\n417\n418\n419\n\n420\n421\n422\n423\n424\n425\n\n426\n427\n428\n429\n430\n431\n\n\fUnder review as a conference paper at ICLR 2025\n\nt\n\nℓk\n\n(cid:105)\n\n, . . . ,\n\nv\nt,i\nt√\nℓk\n\n(cid:104) vt,i1\nt√\nℓk\n\ni = wt + γUtv′\n\ni ∈ St, for all v′\n\nt ∼ [d] for j ∈ [ℓk] and let Ut =\n\n(i). Structured Query Sets St: At each time t, it queries a set St of k points around wt such that\nfor every point x ∈ St, there exists exactly ℓk neighboring points which are symmetrically opposite\nto x in exactly one of the realization of vt,is: More precisely, at each time t, the algorithm first\nsamples ℓk directions ij\n∈ Rd×ℓk . We then\ndefine St = {wt + γtUtv | v ∈ Vℓk }.\nNote that by construction indeed St = 2ℓk ≤ k. Further, note for any point x = wt + γUtv ∈ St\nthere exists exactly ℓk symmetrically opposing points x′\ni ∈ N (v, G)\nsuch that (x−x′\ni)\n= vt,ij\n, j ∈ [ℓk].Given any such point xv := wt + γUtv, let us denote by the set\n2γvi\nN (xv) = {wt + γUtv′\ni | v′\ni ∈ N (v, G)} of all symmetrically opposing points of x in St around\nwt which differs in exactly one of the realization of vt,is. This property will play a very crucial role\nin our analysis.\n(ii). Inferring Pairwise Preferences from Top-m Ranking σm,t ∈ Σm\nst : One of our critical ob-\nservations is Lem. 14. Thanks to this result, we actually break the top-m ranking feedback σm,t\n:= xσm,t(i)\nto m pairwise comparisons.\n,\nt\nSt\\i := St \\ {x(j)\nj=1, and x ≻σ y denotes x is preferred over y in the ranking σ ∈ ΣS, for all\nx, y ∈ S ⊆ D. Now note that for any i ∈ [m], we can always find at least one z ∈ N (x(i)\nt ) ∩ St\\i\nsuch that x(i)\n, we denote the corresponding rank-broken\npair z by ˜x(i)\nt\n(iii). Extracting m Batched-LogitDB (batched pairwise preference) Feedback to Obtain\nAggregated θ∗\nt Estimate: Following the notations from #(ii) above, we extract all the pairwise\ncomparisons (x(ℓ)\nt ), let us denote\n(x(ℓ)\nt = vt,i for some i ∈ [d] and construct an aggregated gradient\nestimate:\n\nt ), for all ℓ ∈ [m]. Further since by definition ˜x(ℓ)\n, where note v(ℓ)\n\nIn particular for any i ∈ [m], let us denote by ˜x(i)\nt\n\nt ≻ z. For ease of notation, for any such x(i)\n\n, ˜x(ℓ)\nt ) = γtv(ℓ)\n\n. Thus by definition x(i)\n\nt\n, ∀i ∈ [m].\n\nt ≻σm,t ˜x(i)\n\nt ∈ N (x(ℓ)\n\nt − ˜x(ℓ)\n\nt }i\n\nt\n\nt\n\nt\n\ngt =\n\n1\nB\n\nm\n(cid:88)\n\nℓ=1\n\nt, where gℓ\ngℓ\n\nt :=\n\n(cid:113)\n\nd\n2γt\n\nλt,i(ℓ)\n\nt\n\nvt,i(ℓ)\n\nt\n\n(iv). FTRL update of wt: Upon finding the gradient estimate gt, the rest of the algorithm proceeds\nexactly same as Alg. 1 (or Alg. 2). More precisely, The algorithm iteratively updates the decision\nvariable wt by minimizing the sum of the ψ-regularized linearized loss within the δ-contracted\ndecision set Dδ such that wt = arg minw∈Dδ\n\n(cid:110)\nη (cid:80)t−1\n\nτ w + ψ(w)\n\nτ =1 g⊤\n\n(cid:111)\n.\n\nThe complete pseudocode of MNL-Scrible is given in App. D.1.\nTheorem 7 (Regret Analysis of MNL-Scrible(Alg. 3)). Consider the decision space D, such that\n∇2ψ(w) ≥ H 2\n\nD,ψI, ∀w ∈ D.\n√\n\nThen for the choice of η =\nguarantees a regret bound:\n\nν min{m,d}HD,ψ\nT log T\n\n√\n\nd\n\n, δ = 1\n\n(cid:100)Reg\n\nMNL\nT ≤ O\n\n(cid:18) d\n\nℓk\n\nT and γt ≤ 0.7HD,ψ\n√\n√\n\nνT log T\nmHD,ψ\n\n(cid:19)\n\n.\n\n, the MNL-Scrible (Alg. 1)\n\nThe proof of Thm. 7 follows from the simple observation that upon receiving σm,t at each round\nt, MNL-Scrible actually extracts m independent pairwise preference feedback from σm,t. This\nreduces the problem to our B-batched pairwise feedback setting with batchsize B = m. The result\nof Thm. 7 hence follows immediately from Thm. 4.\n\nFollowing the argument of Rem. 2, the sample complexity bounds of MNL-Scrible (Alg. 3 is as\nfollows:\nCorollary 8 (Sample Complexity Bound of MNL-Scrible (Alg. 3)). Under the parameter settings\n\nof Thm. 7, the ϵ-sample complexity of MNL-Scrible (Alg. 3) is roughly O\n\n(cid:18)\n\nd2ν\n\nmin{d,m}H 2\n\nD,ψϵ2\n\n(cid:19)\n.\n\nRemark 6. One may expect to see an improved regret rate as the number of items being simul-\ntaneously tested in each round (i.e. k) gets larger and larger. On the other hand, the learning\n\n9\n\n432\n\n433\n434\n435\n436\n437\n\n438\n439\n440\n441\n442\n443\n\n444\n445\n446\n447\n448\n449\n\n450\n451\n452\n453\n454\n455\n\n456\n457\n458\n459\n460\n\n461\n462\n463\n464\n465\n466\n\n467\n468\n469\n470\n471\n472\n\n473\n474\n475\n476\n477\n478\n\n479\n480\n481\n482\n483\n\n484\n485\n\n\fUnder review as a conference paper at ICLR 2025\n\nrate worsens since (in the worst case) it is intuitively ‘harder’ for the ‘best-item’ of a k-subset to\nprove its supremacy against the k − 1-competitors due to higher outcome variance. The result, in\na sense, formally establishes that the former advantage is nullified by the latter drawback yielding\na k-independent guarantee. One really needs to consider a worst-case problem instance for this\ninterplay to happen, as we carefully construct it in our lower bound derivation of Thm. 9.\n\nTheorem 9 (Regret Lower Bound for Top-m Ranking MNL-Bandits Problem). Consider any\nfixed time step T , subsetsize k and length of rank-ordered feedback m ∈ {1, . . . , k}. Then for any\nalgorithm A for the Top-m MNL-Bandits problem, there exists a decision space D ⊂ Rd and a\nsequence of unknown linear functionals θ∗\nT ∈ Rd, such that the regret of algorithm A in T\n√\nMNL\nT ≥ d\n√\nrounds (cid:100)Reg\n256\n\nT log T\nmin{m,d}\n\n1, . . . θ∗\n\n.\n\n6 EXPERIMENTS\n\nWe run synthetic experiments to report the performance of our methods, Double-Scrible (Sec. 3) and\nMNL-Scrible (Sec. 5) respectively, for dueling and top-m ranking feedback. All results are averaged\nacross 100 runs. We run our experiments on different environments (adversarial loss sequences):\nPrecisely, for a fixed d, we construct θ∗\n\nt = 1 + ε\n\nAdversarial (θ) Environments. We report our experiment results on problem instances with vary-\ning dimension d generated as follows: (1) Inst-1: For a given d and round t, we choose θt (2) Inst-2:\nFor a given d and round t, we choose θt\nThe decision space is given by D = {x | Ax ≤ b}, for some A ∈ Rc×d and b ∈ Rc, for some\nc ∈ N+.\nChoice of the self concordant barrier ψ: We use the following self concordant barrier ψ(w) =\n− (cid:80)m\nj w), which is known to be an c-self-concordant barrier for D, aj being the j-th\nrow of A.\n\nj=1 ln(bj − a⊤\n\n6.1 WITH VARYING d\n\nexperi-\nOur first\nments are reported\nthe Adver-\nfor\nLogistic\nsarial\nBandits\nDueling\n(Logit-DB)\nsetting on Inst-1\nand\nInst-2. We run the experiments for d = 2, 5, 10, 20, 50 to examine the scalability and runtime\nefficiency of our proposed methods which are provably shown to scale as O(d\n\nT ).\n\n√\n\n6.2 RUNTIME COMPARISON.\n\nWe now report the (averaged) run-\ntimes of the above executions in sec-\nonds. Note the first experiment is run\nfor T = 50, 000 while the second\nexperiment is run for T = 80, 000\nrounds.\n\nd\n\n2\n6\n10\n20\n50\n\nruntime (sec)\n\n1.459960\n1.773056\n2.149256\n3.150031\n11.567123\n\nd\n\n2\n6\n10\n20\n40\n\nruntime (sec)\n\n2.339139\n3.035577\n3.731622\n5.036075\n17.398917\n\nT = 50,000 (Inst-1)\n\nT = 80,000 (Inst-2)\n\n10\n\n486\n487\n\n488\n489\n490\n491\n492\n493\n\n494\n495\n496\n497\n498\n499\n\n500\n501\n502\n503\n504\n\n505\n506\n507\n508\n509\n510\n\n511\n512\n513\n514\n515\n516\n\n517\n518\n519\n520\n521\n522\n\n523\n524\n525\n526\n527\n\n528\n529\n530\n531\n532\n533\n\n534\n535\n536\n537\n538\n539\n\n\fUnder review as a conference paper at ICLR 2025\n\n6.3 PERFORMANCE OF MNL-Scrible: WITH VARYING m\n\nFor top-m ranking setting, we used m = 2, 4, 8, 16, 32 for d =\n50 on Inst-1.\n\n7 CONCLUSION\n\n√\n\nIn this paper, we introduced an efficient gradient descent-based approach for regret minimization\nfor online linear optimization with adversarial preferences. Our results have critical implications in\nlearning problems of RLHF which has wide applications in fields of AI-alignment, fine-tuning lan-\nguage models, etc. Our proposed novel online mirror descent (OMD) algorithm achieves an optimal\nregret bound of O(\nT ) while only relying on binary preference feedback. This advancement im-\nproves upon existing methods by addressing key computational challenges, particularly in handling\nhigh-dimensional and adversarial environments while still respecting optimal performance guaran-\ntees. We also extended our algorithm to accommodate B-batched preference feedback and m-partial\nranking on k-subsets, which is shown to yield improved performance guarantees depending on the\nbatch size B or the length of the rank-ordered feedback m. The computational efficiency of our\nalgorithms makes them suitable for large-scale real-world applications.\n\nFuture Work. Building on this work, several promising avenues for future exploration emerge: One\npotential extension is to generalize the setting beyond linear scores which is certainly not straight-\nforward even for value-feedback based convex optimization setting . Extending to partially observ-\nable preferences or partial ranking feedback over a subset of alternatives is also an interesting open\nproblem. Another direction is to explore hybrid approaches that combine gradient descent with\nother optimization techniques like Thompson sampling or Bayesian methods, to reduce variance\nin feedback-based learning. Finally, investigating how this algorithm can be adapted for different\nAI alignment challenges, such as incorporating fairness or ethical constraints in decision-making,\npresents an exciting opportunity for future research.",
    "abstract": "Recent developments in Large Language Models (LLMs) have sparked significant attention in Reinforcement Learning from Human Feedback (RLHF), which uses reinforcement learning techniques to optimize a model's performance through human-provided feedback. A simple, widely used, and cost-effective method for gathering human feedback is through relative queries based on human preferences, often modeled using sigmoid utility models. Despite the popularity of sigmoid model-based RLHF algorithms, their theoretical foundations remain underdeveloped as existing algorithms often lack performance guarantees or are limited to small-scale problems due to computationally intractable steps. We address the challenge of developing no-regret learning algorithms for training optimal policy RLHF, and develop the first efficient gradient descent-based algorithm with near-optimal regret guarantees. More technically, we consider the adversarial online convex optimization problem with preference feedback and propose a mirror descent method to obtain a regret of $O(\\sqrt{T})$ over $T$ rounds. The main challenge we are required to solve lies in finding a suitable `gradient-approximation' of the underlying utility functions solely from a binary preference feedback. Following this we extend our results to policy optimization in the RLHF framework with trajectory preferences and design no-regret RL policies using a variant of mirror descent. We also extend our methods beyond pairwise preferences --- to multi-way (batched pairwise) feedback and ranking feedback --- and analyze the trade-off between learning rate with increasing subset size. Our contribution lays the groundwork for a practical gradient descent-based algorithm in RLHF with human preferences. Supported by robust theoretical guarantees, our approach holds promise in the current landscape of developing efficient algorithms for LLMs and addressing human-AI alignment challenges. Empirical evaluations validate our theoretical findings.",
    "response": {
      "Soundness": 3,
      "Presentation": 3,
      "Contribution": 3,
      "Rating": 6,
      "Confidence": 4,
      "Strengths": "1. Novelty and Impact: This paper studies the online convex optimization problem with preference feedback, the problem is of importance and directly related to solving RLHF problems in practice. The insight of the proposed algorithm has potential in many RLHF application areas.\n\n2. Theoretical Soundness: For all three feedback settings, i.e., pair-wise preference feedback, batched feedback, and ranking feedback settings, the paper propose gradient based online optimization algorithms which has theoretical regret guarantees matching the lower bound. Overall it is a solid theoretical paper with good contribution.\n\n3. Algorithm Practicality: the proposed algorithm is computationally tractable with gradient based approach, the major computation burden in each iteration seems to come from eigen decomposition.",
      "Weaknesses": "1. Motivation: the motivation of study adversarial human feedback (the linear parameter \\theta_t adversarially change over time) seems weak from the presentation of paper.\n\n2. Comparison: the regret definitions are different for each feedback settings, which makes comparing the bounds across settings unfair. For example, the bound in Theorem 3 accounts for the case where the sample size and the number of human queries are B times larger than the case in Theorem 1 (although the definition of regret in batched setting is averaged). Therefore, the claim that batched comparison and ranking feedback improves algorithm performance is not completely solid, although correct.\n\n3. Computation Efficiency: one of this paper's claim is the proposed algorithm is more computationally efficient compared to confidence based algorithms (UCB or TS based) in the literature, but the evidence is not clear from the paper. Given computation tractability is a major strength of the proposed algorithm, the reviewer feels that numerical evidence should be provided, at least for a simple setup.\n\nTypos: \n\n1. equation 1 seems to be a typo\n\n2. Theorem 3, superscript of regret should be Batched-LogitDB\n\n3. line 458, missing reference.",
      "Questions": "1. Can you provide more examples on why adversarial human preference is of importance in RLHF applications, to motivate your theoretical framework?\n\n2. Can you provide numerical evidence showing the computation efficiency of your proposed algorithm? \n\n3. Can you present the regret bound of the three settings in a way that can be fairly compared?\n\n4. In line 347, you claim your runtime requirement is O(dT), it seems does not count the computation complexity of eigen decomposition?"
    }
  }
]