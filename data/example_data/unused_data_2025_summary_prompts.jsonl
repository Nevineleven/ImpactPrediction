{"paper_id": "J2FyEVg8HR", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large language models (LLMs) have revolutionized various natural language processing tasks but are limited by their high computational cost, especially during inference. This computational demand imposes significant challenges for real-time applications and necessitates efficient inference strategies. Prior solutions have attempted to address this by routing entire queries to either small or large models based on complexity, which may not always be optimal.\nGap: Existing approaches to reduce inference costs often lack flexibility as they typically route entire queries rather than token-level content, which may not efficiently utilize model capacities and could lead to unnecessary computational overhead or loss of precision.\nHypothesis: The authors propose the CITER framework, which enables efficient inference by dynamically routing tokens rather than entire queries to either small language models (SLMs) or large language models (LLMs) based on their individual criticality, thereby reducing computational costs without compromising the quality of generation.\nMethod: CITER introduces a token-level routing mechanism, employing a router trained via reinforcement learning. The router predicts routing scores for each token to decide whether it should be processed by an SLM or an LLM. The training involves estimating a reward function efficiently through a shortcut process that reduces training costs. The method is validated through extensive experiments on four benchmark datasets and involves an innovative iterative training process that uses collected routing preferences to refine routing decisions.\nConclusion: CITER effectively reduces the inference cost of LLMs while maintaining high output quality, showing improved performance compared to existing query-level and token-level routing methods. These improvements are substantiated by up to 30% reduction in LLM calls or up to 25% increase in accuracy in comparison to baseline methods. The framework provides a significant advance in making LLMs more practical for real-time and resource-constrained environments.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"The study addresses the critical practical issue of reducing LLM inference costs, which is vital for real-world deployment. CITER achieves equivalent performance with 30% fewer LLM calls and improves performance by up to 25% with the same number of LLM calls. Technical innovation is notable, featuring fine-grained control through token-level routing, systematic router training via reinforcement learning, and enhanced training efficiency using shortcut reward estimation. The validation is thorough, with extensive experiments conducted across four benchmarks, a detailed ablation study, and verification across various model sizes.\", \"Weaknesses\": \"The paper lacks sufficient overhead analysis, with no evaluation of the router's computation and memory costs, potential latency from model switching, or end-to-end performance assessment. Especially, the overhead analysis is missing in terms of inference and training as well. \\n\\nIts generalizability remains uncertain, as evaluations are limited to QA tasks and a single model series (Qwen2), without verification in multilingual or long-form generation contexts. \\n\\nThe theoretical justification is also limited, with insufficient rationale for the router's structural choices, no convergence analysis for iterative training, and inadequate verification of shortcut methods' accuracy.\", \"Questions\": \"What is the additional latency introduced by the router?\\nI strongly doubt that single large model call can be more efficient than your approach in case of including training and fine-tuning. \\n\\nDo similar benefits hold for larger models?\\nHow does the framework perform with inputs of varying lengths?\\nHow does it perform on tasks with high token dependency?\\n\\nWhat are the anticipated challenges in real-world deployment?\"}"}]}
{"paper_id": "SM1guXel3E", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Mixup augmentation is a technique used to enhance the generalization ability of deep neural networks (DNNs). It involves mixing data examples to help models learn smoother decision boundaries and reduce overfitting. Historically, Mixup and its variants like ManifoldMix, CutMix, and others have been applied primarily in visual classification tasks. However, mixup augmentation lacks standardized implementations and benchmarks, which makes it hard to achieve reproducibility and consistency across studies.\nGap: The gap addressed in this paper is the absence of a standardized, comprehensive mixup augmentation codebase and benchmark in visual representation learning. This absence leads to challenges in reproducibility, fairness in comparison, and deriving consistent insights across different efforts in applying and extending mixup techniques.\nHypothesis: The authors propose that creating a standardized mixup augmentation codebase and benchmark, called OpenMixup, will address the issues of reproducibility and fairness in the evaluation of mixup techniques. This will also facilitate broader applications of mixup beyond classification, including self-supervised learning and regression tasks.\nMethod: The authors introduce OpenMixup, a comprehensive open-source codebase and benchmark for mixup augmentation in vision tasks. They implement and evaluate 18 mixup methods across 11 image datasets with varying characteristics, such as fine-grained and non-iconic scene datasets. OpenMixup includes a modular framework that supports data preprocessing, mixup development, architecture selection, model training, and evaluation. The authors also conduct empirical analyses of the mixup methods using metrics like GPU memory usage, loss landscape, and robustness to gain insights into trade-offs between performance and efficiency.\nConclusion: OpenMixup enables fair comparisons and assessments of mixup methods by providing a standardized platform. The experiments show that dynamic mixup methods generally outperform static ones in generalizability and performance. Furthermore, OpenMixup's extensible framework supports diverse tasks and promotes further research and application development in mixup augmentation. The observations derived from this benchmark are expected to guide future advancements in mixup techniques and their applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 2, \"Strengths\": \"- This paper is clearly written and provides a good background of existing mixup methods.\\n\\n- It compares 18 different methods in 12 vision tasks trained from scratch in the same codebase. This is a significant and fair comparison with potential benefits to the community by bringing clear comparison between all the methods.\\n\\n- The code will be available, allowing others to try ideas on top of it.\\n\\n- It clearly passes comparison messages such as: dynamic policy performing better than static ones, a ranking of methods based on performance and how applicable they are (e.g. depend on specific architecture), compute and memory cost of the methods.\", \"Weaknesses\": \"- The paper spends a significant amount of space describing the OpenMixup software, it is indeed described as a main contribution of the paper, but overall software description and how its structure to be reused by others seems better left for software documentation and not as main contribution. In particular no significant design decisions seem novel to the mixup problem that deserves attention. (Which is good, software should not be forced to be novel and unexpected). Instead of that I would have preferred to see further exploration/guidance of when to use mixup.\\n\\n- Figure 4 is really hard to parse, sharing legend and using more space might make it better at conveying information.\\n\\n- Although the information is there, I found it hard to parse when/whether there is an absolute significant gain on going from vanilla->static->dynamic, that and some other general conclusions when it is worth to use it would be useful information for a practitioner.\", \"Questions\": \"- The comparisons in this work are when training from scratch, is mixup relevant for practitioners who initialise from pretrained networks trained on larger datasets than ImageNet-1k?\\n\\n- How do mixup benefits behave when the number of training examples gets reduced/increased?\"}"}]}
{"paper_id": "eNjXcP6C0H", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The study of physical dynamics is crucial for a variety of scientific and engineering applications, such as fluid dynamics, climate modeling, and materials science. These systems are governed by Partial Differential Equations (PDEs) and involve complex and high-dimensional data, making data-driven modeling methodologies, like generative models, highly valuable. Even though discriminative models (finite elements, finite difference, PINNs) effectively handle dynamics, generative models offer the advantage of capturing full data distributions, enhancing robustness and interpretability. Generative models, particularly diffusion-based ones, have shown potential in simulating high-dimensional data distributions and offer advantages for data synthesis, anomaly detection, and semi-supervised learning.\nGap: Current generative modeling techniques often overlook the integration of physical priors, which are essential for the produced dynamics to adhere to basic physical laws such as energy and momentum conservation. This oversight results in violations of these fundamental laws and leads to suboptimal performance in generating physically realistic dynamics.\nHypothesis: Integrating physical priors into diffusion-based generative models by leveraging distributional and physical feasibility priors, such as roto-translational invariance and basic conservation laws, could significantly improve the quality and physical realism of the generated dynamics. This new framework could advance data-driven studies in AI4Physics by producing more accurate and consistent dynamics with fundamental physical principles.\nMethod: The proposed framework embeds two categories of physical priors into diffusion-based generative models: Distributional priors (e.g., roto-translational invariance) and Physical feasibility priors (e.g., energy and momentum conservation laws, PDE constraints). The method redefines the generative process by incorporating these priors, handling complex mathematical and physical principles, and ensuring the generated dynamics adhere to fundamental physical laws. The framework uses noise matching and data matching objectives to approximate the targeted score functions, considering both linear and multilinear cases, Jensen's gap considerations, and convexity in constraining the output of the models, ensuring robust integration of the specified physical priors.\nConclusion: The novel framework successfully incorporates physical priors into diffusion-based generative models, producing high-quality, physically realistic dynamics. Empirical evaluations demonstrate the method's robustness and generalizability across various physical phenomena, indicating significant potential for future data-driven AI4Physics research. It highlights the importance of embedding domain knowledge into learning systems and bridges physics and machine learning by leveraging advances in diffusion models to ensure consistency with fundamental physical laws.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 2, \"Strengths\": \"1. The manuscript is well-written and easy to follow.\\n2. Thorough derivations are provided in the main content and appendix.\\n3. Sufficient experiments and ablation studies are conducted to validate the effectiveness of the proposed method.\", \"Weaknesses\": \"Please see the Question part.\", \"Questions\": \"- The three contributions listed in Section 1 appear similar and convey the same meaning. Accurately summarizing the manuscript's contributions will be more helpful for the readers.\\n\\n- Why distribution priors (such as translational rotational invariance) are important for generating physical dynamics? As stated in [1], the invariant sampling and invariant loss functions by restricting architecture designs often sacrifice empirical performances. Are simple data augmentations that approximate probability equivariance also available in physical dynamics generation? Experiments with data augmentations that approximate probability equivariance might help analyze the effectiveness of equivariant models.\\n\\n- How are the physical priors selected in this work? It appears that the roto-translational invariance and the priors based on energy and momentum conservation laws are chosen arbitrarily. There seems to be no systematic analysis that explores all possible categories of physical priors or how to incorporate each type of prior into diffusion models. I believe there are many more physical priors that generative models should be constrained by. Categorizing them and discussing each category systematically would provide more insight.\\n\\n- In Line 498-508, the paragraph title is \\\"Data matching vs noise matching\\\". However, the ablation study is conducted for incorporating a distributional prior or not, which is expected to investigate the training objective instead.\\n\\n---\\n[1] SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation. TMLR 2024.\"}"}]}
{"paper_id": "ZK4VSRzBNC", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Reinforcement learning (RL) has achieved notable success in various domains, but high sample complexity remains a challenge. Previous methods to address sample complexity include optimization techniques, transfer learning, modular RL, hierarchical RL, and offline RL. Reward adaptation (RA) allows agents to adapt to a new reward function using previously learned behaviors in the same domain with different reward functions. Existing RA methods, like Successor Feature Q-Learning (SFQL), are limited when the target behavior significantly diverges from source behaviors.\nGap: Existing reward adaptation methods, particularly SFQL, assume a high similarity between source behaviors and the target behavior. This limits their effectiveness when the target behavior is significantly different from the source behaviors. The paper identifies the need for a more general method that does not depend on the similarity between source and target behaviors.\nHypothesis: The authors propose that a method called Q-Manipulation (Q-M) can improve reward adaptation by computing Q-function bounds and enabling action pruning, which does not compromise the optimality of the policy. This approach should lead to improved sample efficiency and applicability across varied domains.\nMethod: The proposed Q-Manipulation (Q-M) method involves manipulating Q-functions to compute bounds for the Q-function in the target domain. This process relies on the assumption that the target reward function is a known function of source reward functions. Q-M uses an iterative process to tighten these bounds before learning in the target domain, which allows for action pruning without affecting policy optimality. The method evaluates its performance against baselines in synthetic and simulation domains, illustrating its effectiveness and sample complexity improvements.\nConclusion: Q-Manipulation significantly enhances reward adaptation by enabling action pruning without compromising policy optimality. It performs better than existing baselines, particularly in scenarios where target behaviors differ from source behaviors. The method generalizes well across different domains and is shown to be beneficial even with noisy reward combinations and continuous state spaces through discretization. The study demonstrates the potential of Q-M to contribute to more efficient reinforcement learning through enhanced reward adaptation.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"+ Theoretical analysis on the Q bound\", \"Weaknesses\": \"+ Problem Definition: The problem the authors aim to address is not clearly described. In Definition 1, reward adaptation is defined as the task of learning an optimal policy for a target reward function, given a set of behaviors trained under source reward functions. However, it is unclear whether the agent still has access to the underlying MDP when learning the target policy. Is the proposed method an offline algorithm where the target policy is learned only based on the source data?\\n\\n+ Assumption. The assumption that the target reward function is a known function of the source rewards (e.g., a linear combination) is not well-motivated and seems overly restrictive. Additionally, the experimental domains are not well-described, making it difficult for readers to interpret this assumption. For instance, in Race Track, $R_3$ assigns a positive reward (+3) for remaining at the initial location.. This is a relatively large reward compared to others in $R_1$ and $R_2$. In the target domain when $\\\\mathcal{R}=R_1+R_2+R_3$, factors like the distance between the goal and initial location, maximum episode length, and discount factor significantly influence policy behavior. In extreme cases, such as with a short maximum episode length and low discount factor, the optimal policy could be to remain at the initial location.\\n\\n+ Plots. For the Q-M method, pre-training and pre-computation of Q-functions under the source reward are required. However, in the visualizations, it appears that the computational costs associated with this stage are not included in the plots. This omission raises concerns about the fairness of the comparisons.\", \"Questions\": \"When the target reward is a linear combination of the source rewards, this setup appears quite similar to multi-objective reinforcement learning (MORL), where each objective is summed linearly. Could you clarify if there is any relationship between your method and multi-objective RL?\"}"}]}
{"paper_id": "yj9lLwMjnE", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: In recent years, pre-training and representation learning have become critical in speech processing, primarily focusing on either discriminative tasks (e.g., speech recognition) or generative tasks (e.g., speech synthesis), with each requiring different foundational models. Predominantly, existing techniques focus on self-supervised methods for masked audio modeling, targeting specific tasks and leaving gaps for unified models.\nGap: The gap identified by the authors is the lack of a unified pre-training model that can effectively serve both discriminative and generative speech processing tasks. Existing models are typically specialized for one task type, leading to inefficiencies and increased pre-training costs.\nHypothesis: The authors propose that it is feasible to create a single pre-trained model, which they call UniWav, that can learn both representation encoding and generative audio decoding. This unified model would serve both discriminative and generative tasks effectively, potentially replacing numerous task-specific models.\nMethod: The proposed method, UniWav, utilizes an encoder-decoder framework. The encoder is trained using masked audio modeling with a pseudo-labeling approach, where the teacher model is an exponential moving average of the encoder to provide pseudo-labels. The decoder is trained using Flow Matching conditioned on a weighted sum of encoder representations. UniWav is trained on the LibriLight dataset with both components being trained jointly from scratch, using a Transformer architecture.\nConclusion: UniWav demonstrates that a unified model for both speech recognition and generation is possible. It achieves performance on par with existing task-specific models, reducing the need for separate foundation models for each task. This approach shows potential in lowering the computational cost and complexity in deploying speech processing systems.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 5, \"Strengths\": \"The main contribution of this work is the combination of DinoSR and a flow matching model to train a unified representation model. While not considered significant, this special setup is somewhat novel.\\n\\nThe paper contains sufficient experimental results to support their claims although some setups are questionable (in detail below). \\n\\nThe paper clearly indicated the limitations of the work.\", \"Weaknesses\": \"The claim their work is \\\"the first unified pre-training framework for speech representation learning and generation\\\" is arguable. For example, the paper \\\"Speechgptgen: Scaling chain-of-information speech generation.\\\" introduced an approach to train a discrete representation that works for both understanding and generation. The paper \\\"Moshi: a speech-text foundation model for real-time dialogue\\\" further improved the approach. \\n\\nThe presentation needs to be improved. \\n1. the DinoSR part in Figure 1 is confusing since the softmax layer is not clearly shown since the encoder, according to the text description, does not include that layer. In contrast, the figure in the original DinoSR paper is very clear. \\n2. The majority of Section 2.1 is describing DinoSR. However, notation is not clearly explained sometimes. for example, what is \\\"k\\\" in s^k_v right below eq. 3? it's unclear why Euclidean distance (instead of COS distance) is used given that Cos distance is usually more preferred in higher-dim spaces. \\n3. notation \\\"z\\\" is overloaded in eq 9.\\n4. the footnote under Table 1 is not clear. alignment is only used for generation tasks?\\n5. eq 11 is wrong? argmin returns an index instead of a representation.\\n\\nWhen researchers train a representation model they usually keep the encoder fixed when using them in downstream tasks such as ASR. however, in this work the encoder is finetuned. This causes the claims weaker. Similarlly if flow matching is used in the generation part the quality of the generated speech will of course become better. However, this gain comes from the flow matching model instead of the way the representation is learned. Some clarification here is needed.\", \"Questions\": \"why \\\"For speech recognition with a shallow encoder, we found introducing the decoder degrades WER regardless of the size. Interestingly, an opposite trend is observed when the encoder size is doubled\\\"? This is something requires more analysis and explanation.\\n\\nIn section 3.1, \\\"We extract the prequantized latent feature from a 16kHz EnCodec (D\u00b4efossez et al., 2022) encoder, pre-trained on the same dataset, to serve as the input for our model.\\\" This is contradictory to the claim in early sections that the input to your model is mel spectrum?  In addition, this makes the comparison with other approaches unfair since you used another model as the pre-processing module (i.e., total model size is actually increased significantly).\"}"}]}
{"paper_id": "INyi7qUdjZ", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Transformers exhibit in-context learning (ICL), the ability to utilize novel information presented in context without further weight updates. This emerges when models are trained on a diverse set of tasks, with a sharp transition from memorization to generalization as task diversity increases. ICL is thought to arise from either capacity limits of the network or specific dynamic learning mechanisms.\nGap: The exact mechanisms governing the transition from memorization to generalization in transformers during ICL are not well understood. Prior work suggests network capacity limits may favor generalization, but this hypothesis lacks detailed examination.\nHypothesis: The transition from memorization to generalization during ICL is driven by the differential learning rates of independent sub-circuits responsible for these activities, rather than the limited capacity of the network.\nMethod: The study uses a small transformer model on a synthetic ICL task. Through theoretical analysis and experiments, it examines the distinct learning kinetics of memorization and generalization sub-circuits. The model includes a one-layer transformer with an MLP for a controlled setup, allowing for analysis of dynamics between ICL and in-weights learning (IWL). The approach analyzes learning rates, establishes a memorization scaling law, and explores the effects of task diversity and regularization.\nConclusion: The study concludes that the transition from memorization to generalization in transformers during ICL is governed by the differential learning dynamics of sub-circuits, which can be largely independent. It uncovers a task diversity threshold determined by these dynamics rather than capacity limits, providing a framework that explains phenomena like the bimodal solutions near the threshold and the ephemeral nature of ICL. The findings challenge the capacity-based hypothesis and suggest that dynamics, rather than limits, play a critical role.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 3, \"Strengths\": \"The way models acquire generalization during training is an interesting topic and the paper provides an insightful exploration of the mechanisms involved. \\nThe authors convincingly support the premise of the paper with theory and empirical studies.\", \"Weaknesses\": \"Because the model used for the empirical validation is stripped down to a one layer transformer model the authors note that they cannot be sure that the dynamics are the same for much larger models.\\nThe authors could consider supplementing the one-layer analysis with tests on a deeper transformer model. \\nEven if constrained by computational resources, experiments on a 2 or 3 layer transformer would help assess the applicability of their findings to larger networks.\\n\\nThe paper does not recommend methods to facilitate generalization. If possible, a discussion of how the ideas in the paper may be used to develop initialization and regularization techniques would make the submission even stronger.\", \"Questions\": \"Can the authors use this work to recommend methods to improve generalization? For example, can we choose an initialization method that might better support ICL?\\n\\nThe theory hints that ICL decays due to the use of regularization. Though using $L_2$ regularization makes the results more comparable with typical training, would it be possible to provide empirical results without regularization?\"}"}]}
{"paper_id": "z1Jq1PLQWs", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper focuses on Reinforcement Learning from Human Feedback (RLHF), where relative human preference feedback, often modeled using sigmoid utility models, is employed to refine AI systems. This form of feedback is cost-effective and less biased, providing a nuanced understanding of human values. Despite its popularity, existing RLHF algorithms lack rigorous theoretical foundations and face computational inefficiencies, hindering scalability to complex real-world scenarios.\nGap: Existing RLHF methods lack performance guarantees and face computational limitations, especially for large-scale problems or when adapting to dynamic environments. Current algorithms are often based on principles like UCB or Thompson Sampling, which can be computationally intractable and inefficient for real-world applications, limiting their practicality and scalability.\nHypothesis: The paper hypothesizes that by adopting a gradient descent-based approach, particularly a mirror descent method, it is possible to develop a no-regret learning algorithm that efficiently handles adversarial preferences and scales well to RLHF settings. This approach aims to provide optimal regret guarantees.\nMethod: The authors propose an online gradient descent algorithm utilizing a mirror descent approach to achieve optimal regret in the presence of adversarial preferences. The method relies on a weaker form of feedback\u2014binary preferences\u2014rather than traditional value feedback. They introduce 'Double-Scrible', an algorithm that estimates the loss function from preference feedback. The method is extended to handle batched pairwise preferences and ranking feedback. The algorithm's performance is analyzed against both upper and lower bounds for regret, demonstrating its optimality. The theoretical contributions are supported by empirical validations.\nConclusion: The paper presents the first efficient gradient descent-based algorithm for RLHF with adversarial preferences that guarantees near-optimal regret of O(sqrt(T)). It addresses critical computational challenges, enhancing the feasibility of preference-based learning in high-dimensional settings. The algorithm is extendable, accommodating various feedback models with improved performance guarantees, making it suitable for large-scale applications in AI alignment and LLM fine-tuning.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"1. Novelty and Impact: This paper studies the online convex optimization problem with preference feedback, the problem is of importance and directly related to solving RLHF problems in practice. The insight of the proposed algorithm has potential in many RLHF application areas.\\n\\n2. Theoretical Soundness: For all three feedback settings, i.e., pair-wise preference feedback, batched feedback, and ranking feedback settings, the paper propose gradient based online optimization algorithms which has theoretical regret guarantees matching the lower bound. Overall it is a solid theoretical paper with good contribution.\\n\\n3. Algorithm Practicality: the proposed algorithm is computationally tractable with gradient based approach, the major computation burden in each iteration seems to come from eigen decomposition.\", \"Weaknesses\": \"1. Motivation: the motivation of study adversarial human feedback (the linear parameter \\\\theta_t adversarially change over time) seems weak from the presentation of paper.\\n\\n2. Comparison: the regret definitions are different for each feedback settings, which makes comparing the bounds across settings unfair. For example, the bound in Theorem 3 accounts for the case where the sample size and the number of human queries are B times larger than the case in Theorem 1 (although the definition of regret in batched setting is averaged). Therefore, the claim that batched comparison and ranking feedback improves algorithm performance is not completely solid, although correct.\\n\\n3. Computation Efficiency: one of this paper's claim is the proposed algorithm is more computationally efficient compared to confidence based algorithms (UCB or TS based) in the literature, but the evidence is not clear from the paper. Given computation tractability is a major strength of the proposed algorithm, the reviewer feels that numerical evidence should be provided, at least for a simple setup.\\n\\nTypos: \\n\\n1. equation 1 seems to be a typo\\n\\n2. Theorem 3, superscript of regret should be Batched-LogitDB\\n\\n3. line 458, missing reference.\", \"Questions\": \"1. Can you provide more examples on why adversarial human preference is of importance in RLHF applications, to motivate your theoretical framework?\\n\\n2. Can you provide numerical evidence showing the computation efficiency of your proposed algorithm? \\n\\n3. Can you present the regret bound of the three settings in a way that can be fairly compared?\\n\\n4. In line 347, you claim your runtime requirement is O(dT), it seems does not count the computation complexity of eigen decomposition?\"}"}]}
