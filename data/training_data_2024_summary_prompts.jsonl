{"paper_id": "XTwwtlEfTF", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Multimodal learning uses data from multiple sources (e.g., text, images, audio) to improve performance in tasks like image segmentation and sentiment analysis. Robustness against missing or corrupt modalities at test time is a critical challenge as performance often degrades significantly without all modalities.\nGap: Existing multimodal networks face significant performance drops when one or more modalities are missing during testing, and current solutions either require specific training strategies or perform poorly on untrained modality combinations.\nHypothesis: Parameter-efficient adaptation of pretrained multimodal networks using low-rank adaptation and modulation of intermediate features can enhance robustness to missing modalities, partially mitigating performance drops and possibly outperforming dedicated networks for available modality combinations.\nMethod: The authors propose a parameter-efficient adaptation for pretrained multimodal networks where low-rank matrix adjustments and modulation of intermediate features (e.g., through scale and shift layers) account for missing modalities. They conducted experiments across diverse datasets for tasks including RGB-thermal and RGB-depth semantic segmentation, material segmentation, and sentiment analysis, highlighting the adaptability of their method. The adaptations require less than 0.7% additional parameters compared to the total model parameters.\nConclusion: The proposed adaptation method successfully improves model robustness to missing modalities, showing better performance than baseline and existing methods across several datasets and tasks. The method is versatile across different multimodal challenges and efficiently adapts to varying modality combinations while requiring minimal additional parameters.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"- Development of multimodal learning algorithms robust to missing modalities is crucial since observations from some modality could go often be missing due to sensor/hardware failure.\\n\\n- The proposed method is effective and importantly efficient as it introduces a very small set of parameters towards making a pre-trained model robust to missing modalities.\\n\\n- The paper is mostly well-written and easier to read.\\n\\n- Experiments have been performed on two different tasks of multimodal semantic segmentation and multimodal sentiment analyses. Results claim to surpass the existing methods and (often) dedicated methods when the modalities are missing at test time.\", \"Weaknesses\": \"- Although the idea is simple and effective for achieving parameter-efficient adaptation of models, the application of existing techniques (i.e. low rank adaptation, scale and shift transformations, and BitFit) to adapt a pre-trained model on a subset of modalities seems low from novelty aspect.\\n\\n\\n- There is no insight and/or analyses into how the proposed paramater-efficient is helpful towards making the model robust to missing modalities. It is important to develop the grounding of the proposed method since only showing improvements over the baseline with empirical results doesn't help much towards this.\\n\\n- The paper doesn\u2019t mention any clear justification on why SSA was chosen over the LoRA and BitFit to report the results in the paper.\\n\\n- Comparison is missing with any of the existing method for the task of multimodal sentiment analyses. \\n\\n- Why CMU-MOSI and CMU-MSEI datasets were chosen to evaluate and report the performance of the method?\", \"Questions\": \"- There is no study that fairly compares the parameter efficiency of the proposed method with competing methods?\\n\\n- What are the possible reasons for relatively lower performance of other adaptation strategies, compared to SSA, in Table 4? Some explanation would be helpful to better understand their comparison in missing modalities scenario.\"}"}]}
{"paper_id": "EEqEyyYUwc", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Diffusion models are powerful generative models that can convert noise into complex data distributions. Conditional generation using diffusion models involves using additional inputs like class labels or text to guide the model. Traditional approaches include Classifier Guidance, which uses pretrained classifiers for guidance, and Classifier-Free Guidance, which introduces the condition during training. Both methods rely on a weighting parameter to balance fidelity and condition adherence.\nGap: Existing methods use a constant weighting parameter during diffusion guidance, which can result in conflicts between image quality and adherence to conditions, especially in early stages with high noise. There is a lack of systematic investigation into how guidance behavior varies across different timesteps in diffusion processes.\nHypothesis: The paper hypothesizes that conflicts in generation quality and condition adherence arise due to conflicting gradients from classifiers at different timesteps, especially early on. A timestep-dependent weighting scheme can improve the balance between fidelity, diversity, and condition adherence in diffusion guidance.\nMethod: The authors propose Progressive-Guidance, a timestep-dependent weighting scheme for guidance in diffusion models. They introduce two approaches: a heuristic-based method using linear and cosine functions, and a gradient-norm-based method that adjusts weighting based on the condition's gradient norm with respect to the timestep. These methods can be implemented without retraining models and are tested across class-conditioned image, text-to-image, and text-to-motion generation tasks.\nConclusion: The proposed Progressive-Guidance scheme improves the performance of diffusion models on several benchmark tasks by achieving better trade-offs between fidelity and condition adherence. The methods outperform static guidance in generating more diverse and higher quality outputs. The approach requires minimal implementation effort and no retraining, making it applicable to various diffusion model applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"1.\\tThe empirical analysis in section 3 is interesting and reasonable.\\n2.\\tThe motivation of this paper is clear.\", \"Weaknesses\": \"1.\\tThe main idea of the proposed methods is to attenuate the influence of guidance in the initial stages. However, based on my experiences, with small guidance weight in the initial stages, the structural integrity and layout of the generated images by diffusion models would be disrupted, particularly for some complex structures. The authors should provide more evaluations of the samples with complex structures, such as instances with human bodies or longer prompts.  Actually, I believe that the trade-off between sample fidelity and conditional adherence still exists, even with the proposed methods.\\n2.\\tI have read the appendix carefully. It is weird to find that the proposed methods cannot outperform the baselines in terms of CLIP-Score with a small guidance weight w.  If I focus on the experimental results with w<10, which is actually used in practice, what are the corresponding trade-off curves in Figure 5?  \\n3.\\tThe paper does not conduct extensive experiments on different model architectures, and samplers to validate the robustness and generality of their method. \\na)\\tModels: some more powerful diffusion models should be involved for comparisons, e.g., SD XL[1], and DeepFloyd IF[2], which can both be accessed publicly. Based on my understanding, those models can provide a more accurate estimation for classifications at the initial stage of the generation. As a result, the performance improvement of the proposed methods may decrease in those models. \\nb)\\tSamplers: DDIM sampler is the main selection for this paper to sample images during generation. More samplers, such as DPMSlover[3], should also be considered.  \\nc)\\tHuman-level metric: It is well-known that some quantitative metrics like FID, and IS, may be problematic in some cases. Human evaluation should be involved to provide more clear comparisons. \\n[1] https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0\\n[2] https://huggingface.co/DeepFloyd/IF-I-XL-v1.0\\n[3] Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps\", \"Questions\": \"1.\\tWhat are the corresponding trade-off curves in Figure 5 if we focus on the cases with guidance weight w<10.0?\\n2.\\tProvide more experimental analysis mentioned above to validate the proposed methods.\"}"}]}
{"paper_id": "lkIRFglmTp", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Reinforcement learning tasks are often modeled as Markov decision processes (MDPs), which assume that the state observed contains all necessary information. However, real-world decision processes often do not satisfy the Markov property, leading to non-Markovian observations in partially observable environments. This challenge is traditionally addressed by augmenting observation spaces with history data to approximate the Markov property. However, current methods for this augmentation lack generality and often require task-specific hand-designed features. The partially observable Markov decision process (POMDP) framework accommodates such non-Markovian observations but optimal behavior typically depends on the history of states, which is difficult to manage without sophisticated mechanisms.\nGap: Existing approaches for handling partial observability in reinforcement learning often rely on manual augmentation of state information or assumptions that may not hold broadly across different tasks. Specifically, there is a lack of practical methods to autonomously detect and resolve partial observability in a general and scalable way without predefining state features or heavily relying on expert intervention.\nHypothesis: The authors hypothesize that the \u03bb-discrepancy, defined as the difference between TD(\u03bb) value functions for different lambda values, is a reliable measure to detect partial observability in decision processes. Furthermore, minimizing this \u03bb-discrepancy through optimized memory functions can resolve partial observability and improve decision-making in reinforcement learning.\nMethod: The paper introduces \u03bb-discrepancy, a measure to quantify the non-Markovianity in decision processes based on the difference between temporal difference (TD) learning value functions for different lambda parameters. The authors conducted theoretical analyses proving that the \u03bb-discrepancy reliably indicates partial observability and empirically demonstrated strategies to minimize it. The method involves using these discrepancy measures as an optimization target by implementing memory functions that summarize historical data to enhance the agent's observational inputs. Two main optimization strategies were developed: one using closed-form solution techniques in a planning setting and another using hill-climbing in online learning where only estimates are available.\nConclusion: The authors conclude that \u03bb-discrepancy is a robust measure for identifying and resolving partial observability in reinforcement learning environments. By targeting the reduction of \u03bb-discrepancy, they demonstrate that agents can use augmented memory functions to approximate the Markov property effectively, thereby enhancing decision-making capabilities. The study's findings suggest that \u03bb-discrepancy can be practically applied without the need for full state observability, offering generalizable solutions across various POMDPs.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"- The paper is generally well written\\n- The paper is well motivated -- a metric denoting the Markovness of a POMDP is a very useful tool\\n- The approach is novel\", \"Weaknesses\": \"- The experiment section is lacking, only comparing against a memory-free baseline\\n- The tested environments and models are very low dimensional, and it's not clear such a method would scale to more interesting problems\\n- If I am understanding this correctly, the proposed metric is flawed in that errors in the Q function (not the state estimator) will result in a $\\\\lambda$-discrepancy. For example, given a perfect state estimator $M^*(o_1, o_2, ..., o_n) = s_n$ and two imperfect Q functions $Q_\\\\theta, Q_\\\\phi$: $ \\\\lVert Q_{\\\\theta}^{\\\\lambda_1}(s_n) - Q_{\\\\phi}^{\\\\lambda_2}(s_n) \\\\rVert > 0$. In this case, I do not see how $\\\\lambda$-discrepancy is a better metric of Markovness than the traditional TD(0) Q learning error.\", \"Questions\": \"- What makes the $\\\\lambda$-discrepancy specific to partial observability? A policy trained on a fully-observable MDP will also have a $\\\\lambda$-discrepancy if the Q function is even slightly less-than-optimal.\"}"}]}
{"paper_id": "Gsn43rbDwp", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the challenges faced by quantum machine learning in the Noisy Intermediate-Scale Quantum (NISQ) era due to limited quantum resources. It highlights previous work on quantum computing's potential for exponential acceleration in high-dimensional data processing and its limitations in current devices. Graph-structured data, a focus area in machine learning, is notoriously difficult for classical neural networks, which struggles with complex graph data structures. Quantum computing, with its high-dimensional state representation, offers a promising avenue for addressing this challenge.\nGap: Despite advances in quantum computing, there is uncertainty regarding the application of quantum computing methodologies, specifically single qubit techniques, to complex neural network architectures like graph neural networks (GNNs). Current methods struggle to efficiently process intricate graph data due to resource constraints in quantum circuits, particularly in the NISQ regime.\nHypothesis: The paper proposes that a single qubit architecture can be effectively used for graph neural networks by employing a novel graph embedding technique based on quantum walks to reduce the parameter demands usually required by graph data processing.\nMethod: The paper introduces a Single-Qubit Quantum Graph Neural Network (sQGNN) approach. The method comprises three steps: (1) a graph embedding method using discrete-time quantum walks to convert graph data into vectors; (2) encoding the graph data into quantum circuits using a single qubit, thereby minimizing parameter use; and (3) constructing a single-qubit quantum graph neural network using quantum rotation gates for training. The evaluation involved experiments on publicly available datasets like MUTAG and PTC, deployed on both simulated and real quantum environments (IBM quantum computers). The method employs variational quantum circuits optimized using Quantum Natural Gradient, ensuring robustness against noise and efficient learning through reduced parameters and avoidance of the 'barren plateau' phenomenon.\nConclusion: The sQGNN effectively utilizes limited quantum resources by performing graph neural network computations on a single qubit, achieving strong performance on both simulated and real quantum devices. It is robust to noise and circumvents common issues faced by variational quantum algorithms like the 'barren plateau.' The approach significantly reduces model parameters and is easily adaptable to hybrid systems, making it a promising candidate for scalable quantum machine learning applications in the current NISQ era.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"The paper introduces new method using quantum neural networks by combining the existing ideas to efficiently classify graph datasets.  The method uses only single qubit thereby reducing the quantum resources required as well as mitigating the barren plateau issues. The method is also demonstrated to show robustness against noises. The quantum features also enable the method to reduce the parameters required from thousand to twelve.\", \"Weaknesses\": \"Section 2 which describes the method is not written clearly and is hard to follow the steps leading to embedding of graph. Also, there is lack of continuity and proper explanation of variables used in equations in section 2.3. Also, from the experiment results shown in figure 3, the method is doubtful as the success probability is marginal for Class 1. There are a few other issues that is raised in the questions section below.\", \"Questions\": \"1.\\tWhy there is a difference in success probability for Class 0 and 1? It seems that the QGNN is not able to change the initial state.\\n2.\\tWhy the simulations in ideal simulated environment as shown in table 1 has lesser fidelity than using noisy real quantum computers? Providing a figure similar to figure 3 for ideal simulated environment is good for comparison.\\n3.\\tThe error bars for the results seems very high (in table 1 and 2). Is there a way to confine the errors?\\n4.\\tUsing two qubits for encoding seems to worsen the training as shown in table 1. Also, the explanation given for the reduction in standard deviation of dual model is not clear as increasing the number of qubits usually tend to increase the error rates. \\n5.\\tThe literature about existing methods and its pros and cons compared to the new method is missing.\"}"}]}
{"paper_id": "DOerIFfUbs", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Contrastive pre-training on image-text pairs, such as CLIP, has become the standard for learning multi-modal visual-language representations. CLIP allows for applications like zero-shot image classification and retrieval. However, it requires extensive computational resources to train from scratch on noisy web datasets. Meanwhile, Masked Image Modeling (MIM) offers an efficient alternative for pre-training single-modal representations but lacks the multi-modal capability without additional steps.\nGap: There is a need to enhance the efficiency and effectiveness of training methods like CLIP while avoiding the drawbacks of MIM's training-finetuning inconsistencies and significant computational demands.\nHypothesis: The authors propose that Unmasked Token Alignment (UTA) can efficiently enhance CLIP's vision-language representations by training a Vision Transformer (ViT) to align unmasked visual tokens with a frozen CLIP vision encoder, effectively aligning it with the CLIP text encoder without the need for [MASK] tokens.\nMethod: UTA trains a Vision Transformer (ViT) without using [MASK] tokens by aligning unmasked visual tokens with a frozen CLIP vision encoder. The process involves masking a portion of image tokens and using the unmasked ones to align with CLIP features, maximizing cosine similarity for alignment. The ViT model can then be applied in zero-shot settings. UTA maintains training-finetuning consistency by only processing unmasked tokens, reducing training FLOPs by up to 50%. Experiments were conducted on ImageNet-21K and other datasets to assess performance in various zero-shot and fine-tuned tasks.\nConclusion: UTA effectively enhances the alignment of vision-language representations by leveraging pre-trained CLIP models, achieving strong zero-shot evaluation performance and outperforming existing methods on various benchmarks. It is training-efficient and maintains training-finetuning consistency, but still benefits from further fine-tuning on large datasets.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"-It is a good idea to perform unmask token alignment with the pretrained CLIP model due to its efficiency. \\n\\n-The experiments are quite extensive including many downstream tasks. The method also achieves the SOTA on most of the datasets.\", \"Weaknesses\": \"-Motivation of reducing pretraining costs is not convincing. In particular, the abstract claims that \\\"training CLIP from scratch on noisy web-scale datasets is computationally demanding\\\". Although this is true, this paper does not solve this issue at all because it still relies on a pretrained CLIP model at the first place.\\n\\n-The performance gains seem to come from the contrastive finetuning rather than the proposed unmasked alignment pretraining. In Table 1, comparing with the CLIP teacher i.e., EVA-CLIP, there is always a performance drop for UTA without finetuning. This is concerning because this paper claims the unmasked alignment pretraining as one of the main contributions. \\n\\n-One important baseline is missing. Since finetuning is very effective for zero-shot image classification, this paper should also compare with the CLIP teacher i.e., EVA-CLIP,  that is also finetuned on the same dataset. \\n\\n-Improving the VIT efficiency by dropping masked tokens has been done in [A].  This paper fails to cite this important reference paper and claims it to be something new. \\n\\n[A] Li et al., Scaling Language-Image Pre-training via Masking. CVPR 2023\\n\\n-The improvement in Table 4 & 5 (detection and segmentation) is marginal (< 1%). This is also concerning. The method seems to be limited to image-level prediction tasks.\", \"Questions\": \"The authors are highly encouraged to address my questions mentioned in the weakness. In addition, I have the following questions. \\n\\n-Is the CLIP teacher-model always the giant EVA-CLIP? \\n\\n-It would be good to provide the CLIP teach results in Table 3, 4, & 5.\\n\\n-This paper says that it is following previous works to perform the second-stage contrastive finetuning without proving the reference. Please provide the reference. \\n\\n-Is finetuning helpful in Table 4 & 5?\\n\\n-Is masking strategy applied in the finetuning stage?\"}"}]}
{"paper_id": "yIEKq72cTE", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Federated Learning (FL) has become an effective framework for distributed machine learning, allowing multiple clients to collaboratively train a model while keeping their data decentralized. It has applications in various fields such as healthcare, personalized services, and more. However, FL's decentralized nature makes it susceptible to adversarial attacks, particularly data poisoning attacks, where malicious clients inject harmful data to manipulate the learning process.\nGap: Existing defense mechanisms for FL against data poisoning attacks have significant limitations, including a trade-off between precision and robustness, assumptions on i.i.d. data distributions, and strongly convex optimization problems. These approaches often fail in non-i.i.d. data scenarios, which do not align with real-world applications. There is a need for an effective method to mitigate such attacks without these stringent assumptions.\nHypothesis: The hypothesis is that a dynamic weighted aggregation method using Optimal Transport (OT) principles can effectively combat data poisoning attacks in Federated Learning by distinguishing and discarding malicious updates based on validation loss differences at the server, thereby improving resilience against such attacks.\nMethod: The proposed method, Federated Learning Optimal Transport (FLOT), introduces a dynamic weighting scheme and a loss function-based rejection mechanism to suppress malicious updates. It uses the Wasserstein barycentric technique to aggregate updates from client devices into a global model. The approach involves rejecting updates with significant loss deviations and applying OT to compute a barycenter for robust aggregation. Experiments were conducted on four datasets: GTSRB, KBTS, CIFAR10, and EMNIST, under various attack scenarios. Theoretical proofs for the method's Byzantine resilience and convergence were provided, and the approach was compared against several baseline methods.\nConclusion: FLOT demonstrates superior performance compared to existing methods in mitigating data poisoning attacks across different datasets and attack scenarios. It maintains high accuracy and scalability while effectively discarding malicious updates, evidenced by low validation loss dispersion in benign clients. The method shows significant improvements in time complexity and is robust under non-i.i.d. data conditions, highlighting its practical application value for secure FL.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. This paper proposes an optimal transport-based dynamic weighted federated aggregation method designed to mitigate untargeted data poisoning attacks within the FL framework. \\n2. The complexity of the proposed defense method has a substantial improvement compared to Krum.\\n3. The theoretical convergence results are provided.\\n4. Eextensive experimental results demonstrate that the proposed method consistently outperform other methods, including Byzantine robust aggregation rules, in terms of classification performance under both single-client and 50% Byzantine worker scenarios.\", \"Weaknesses\": \"1. The proposed method relies on validation data at the server. This validation data will affect the performance of the proposed defense method much. If the server cannot obtain the training data, the performance of the proposed method should be shown. \\n2. Some advanced defense methods are missing in the introduction and experiments, e.g., [1] and [2]. \\n3. The datasets used in this paper are too simple and limited to image classification tasks. Some real-world datasets, e.g., imagenet and cifar100, and other tasks should be considered, e.g., sentiment classification and multi-modal tasks.\\n4. The federated self-supervised learning framework should be considered in the experiments.\\n5. The convergence analysis is not meaningful, due to some impractical assumptions, e.g., Polyak-\u0141ojasiewicz (PL) inequality.\\n6. The attacker setting in this paper is also simple. There are some advanced model poisoning attack methods, e.g., [3] and etc. \\n\\nThe references mentioned are as follows:\\n[1] Xiaoyu Cao, Jinyuan Jia, Zaixi Zhang, and Neil Zhenqiang Gong. \\\"FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information,\\\" In IEEE Symposium on Security and Privacy, 2023.\\n[2] Sungwon Park, Sungwon Han, Fangzhao Wu, Sundong Kim, Bin Zhu, Xing Xie, and Meeyoung Cha. \\\"FedDefender: Client-Side Attack-Tolerant Federated Learning,\\\" In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2023.\\n[3] V Shejwalkar, A Houmansadr, \\\"Manipulating the byzantine: Optimizing model poisoning attacks and defenses for federated learning, \\\" In NDSS, 2021.\", \"Questions\": \"1. Please show the performance of the proposed method when the server cannot obtain the training data and enough training data. \\n2. Please add some advanced defense methods in the experiments, e.g., [1] and [2]. \\n3. The datasets used in this paper are too simple and limited to image classification tasks. Some real-world datasets, e.g., imagenet and cifar100, and other tasks should be considered, e.g., sentiment classification and multi-modal tasks.\\n4. The federated self-supervised learning framework should be considered in the experiments.\\n5. The convergence analysis is not meaningful, due to some impractical assumptions, e.g., Polyak-\u0141ojasiewicz (PL) inequality.\\n6. The attacker setting in this paper is also simple. There are some advanced model poisoning attack methods, e.g., [3] and etc.\"}"}]}
{"paper_id": "4fH5ELoWFu", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Neural networks are prevalent in critical systems such as self-driving cars and face recognition. However, these systems are susceptible to adversarial examples, which are inputs that are subtly modified to mislead the model into incorrect predictions. To counter this, methods like adversarial training (which enhances empirical robustness but lacks certification) and certified training (which provides robustness certification but reduces accuracy) have been developed. Randomized smoothing is another method that provides certain robustness certifications but also sacrifices accuracy.\nGap: The existing challenge is to develop neural network models that are both certifiably robust and maintain high accuracy. Current approaches either compromise on the robustness certification (adversarial training) or sacrifice significant model accuracy (certified training). Randomized smoothing introduces noise, impacting accuracy as well. Therefore, a solution that concurrently achieves high accuracy and certified robustness without these drawbacks remains an open problem.\nHypothesis: The authors propose that achieving both high accuracy and certified probabilistic robustness is feasible. They suggest that deterministic robustness, which demands consistent predictions around an input, is restrictive and unnecessary. Instead, probabilistic robustness, where a model makes the same prediction most of the time within a certain vicinity, could be a practical and sufficient objective.\nMethod: The approach consists of two main components: 1) A novel training method that minimizes variance in predictions within a defined vicinity of an input, aimed at enhancing probabilistic robustness without overfitting to specific attack types. This is combined with empirical risk minimization to balance accuracy and robustness. 2) An inference method that certifies the probabilistic robustness of a model's predictions based on statistical testing. During inference, the method uses majority voting over samples from an input's vicinity to certify the prediction\u2019s robustness against adversarial attacks. The effectiveness of this method is demonstrated on multiple datasets with comparisons to state-of-the-art methods.\nConclusion: The proposed method achieves a high level of certified probabilistic robustness without sacrificing accuracy. It outperforms existing adversarial and certified training methods in both certification rate and accuracy across various datasets and perturbations. The method is also shown to defend effectively against adversarial attacks, providing a significant improvement over baseline methods in terms of defence success rates. Thus, the method successfully bridges the gap between robustness and accuracy for neural network models.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"There are a few main concerns with the paper (see Questions below) that need to be addressed. They may be based on a misunderstanding of the approach, and so appropriate answers from the authors would lead me to raise my score.\", \"Weaknesses\": \"My main concerns are raised in the Questions section.\\n\\nBut some minor details:\\n- in section 2 it was stated initially that h outputed a label. And yet later on in the same paragraph this was changed to logits. Precision/consistency is required\\n- writing G_x = arg max h(x) for example needs to make it very clear what the argument is they're searching over (vector index of logit output here)\\n- It is claimed in the introduction that randomised smoothing suffers from \\\"significant accuracy loss\\\". This is a broad statement to make as for example one can vary the variance of the added noise in order to vary the certified robust radius achieved (and also the impact on accuracy)\\n- In equation (2) it is not clear what the random variable (more importantly its distribution) is for the inner Probability\\n- near end of section 2 ... \\\"solution expects efficiency\\\" should be \\\"solution exhibits efficiency\\\"?\\n- in (1) it seems to be defining an adversarial example as a pair of points x1 and x2. Yet later on single points are referred to as adversarial examples. The definition (1) is a bit misleading\", \"Questions\": \"- In (3), conventionally I would have thought it should be G_x not G_t in the loss. i.e. you are trying to maximise the loss for the target label at x as t varies in the disk around x (also it is not made clear in the max specification that t is the free variable here)\\nThe same problem occurs in the discussion below equation (5). The text talks of computing the loss against the label of x and yet the expression in the bracket \\\"(i.e. E ....)\\\" has G_t used as the label\\n\\n- The distribution of t is not clear (in order to evaluate the expectation of the loss mean and variance). Is it uniform in the disk around x?\\n\\n- My main question is more fundamental. Firstly, it is not clear what classifier's output is being certified. Is it h or h^\\\\dagger\\n\\n(i) If h, then it is not sufficient to show that adversarial examples are sparse in the disk of radius epsilon around x. This was implied in the statement \\\"to establish certified probabilistic robustness, we must show that there is a guaranteed upper bound on the probability of adversarial example\\\".  But it doesn't seem to be sufficient. e.g. if there is one adversarial point x' in the disk around x and an attacker has some algorithm that allows them to find it, then the point x' is adversarial, regardless of the fact that such points are very rare.\\n\\n(ii) If h^\\\\dagger (which seems more likely what is meant), then the intent is to say that the majority class value is the classification at x, certified on the disk around x if the other classes are (statistically) sufficiently small. [Separate note - I believe there may a difference between equation (6) setting h as the expected logits value and then doing argmax for the class on that, and algorithm 2 which does the argmax at each sample point and then counts the occurrences of each class and takes the max. This is a point which is discussed in the certified robustness literature]\\nIf the claim that the vast majority of the points being of say class 1 in the disk is evidence that the classifier h\\\\dagger is always 1 on the disk, this seems wrong. If evaluating h^\\\\dagger at another point in the disk, we know nothing about the values outside the current disk. Hence the value of h^\\\\dagger at a point where say its disk has less than half its area in common with the disk around x is unknowable. Approaches to randomised smoothing are able to make assertions about the values across a disk (of a radius which depends on the logit differences at x), given their long-tailed noise distributions.\\n\\nPossibly this is a misunderstanding of what exactly is being claimed as being the certified robust classifier and what that property preciesly means. I look forward to clarifications.\"}"}]}
{"paper_id": "Aq35gl2c1k", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Critical learning periods are well-established phenomena in both biological systems, where temporary sensory deficits early in development can permanently affect behavior, and in artificial networks. Research has shown that these periods are tied to neural plasticity in biology. In artificial deep neural networks, analogous phenomena have been observed, raising questions about whether critical periods are inherent to the learning process of any flexible learning system, rather than being purely biologically driven.\nGap: While critical periods have been observed in artificial systems, the underlying causes are not well understood, particularly whether these periods depend on specific architectural or optimization details. There is a need to isolate the fundamental factors that cause critical periods to emerge in deep networks to better understand their generality across biological and artificial systems.\nHypothesis: The authors propose that critical learning periods are a fundamental feature of deep learning systems, driven by the depth of networks and data distribution structure rather than specific architectural or biochemical components. They suggest that these properties promote competitive dynamics between pathways, leading to critical periods.\nMethod: The study uses deep linear networks which lack the complexity of nonlinear activations to analytically explore the emergence of critical learning periods. The approach is two-pronged: (1) investigating the role of competition in a multi-pathway linear network model and (2) analyzing generalization in matrix completion tasks. Simulations and differential equation models reveal how depth and initial learning phases influence feature learning and generalization in networks. Experiments include analyses of singular value trajectories under various scenarios, such as temporary deprivations or pre-training relays on different tasks.\nConclusion: The analysis shows that critical periods arise from fundamental aspects such as network depth and the underlying data structures. These factors independently drive competitive learning dynamics and affect generalization for transfer learning. The simplicity of the linear models demonstrates that these phenomena are not reliant on complex neural architectures, supporting the idea that critical periods are inherent to deep learning processes, providing insights into both artificial and biological learning systems.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 10, \"Confidence\": 4, \"Strengths\": \"Well written, interesting avenue of research. Results well explained early in the text, with clear examples and clear explanations of experimental settings and technical details. Particularly nice clarity given the cross-disciplinary nature of the work.\\nRelated work is well written, reasonably thorough (to my knowledge), and well-explained.\\nNice phase portraits.\\nCaptions are mostly well-explained and self contained! (I don't know why this is so rare, but good job)\", \"Weaknesses\": \"It's a bit of a tradeoff with having a clear hypothesis, but I find the case oversimplified/misframed\u00a0 in the beginning and conclusion. Sure the literal biochemical explanation doesn't hold in an artificial setting, but analogies of this (reduced plasticity) could well occur. I don't think these are 'competing' hypotheses as they are framed; plasticity, imperfect optimization, etc. are *mechanisms* by which critical periods could arise in both artificial and biological systems; it doesn't tell us the reason\u00a0\u00a0(inherent to learning, quirks of substrate, something else, etc.). In the conclusion \\\"natural information processing constraints\\\" are contrasted with biochemical processes like plasticity as an alternative hypothesis, but what is (decreased) plasticity buyt a natural information processing constraint? Demonstrating presence in artificial systems doesn't estabilsh it as a general feature of learning, except insofar as we've defined learning narrowly to be done by biological and ANNs (does naive bayes learn? does it exhibit critical periods?). It becomes sort of a circular argument about what does \\\"learning\\\" even mean. This commonality is actually supported by your experiments in Fig 3 recreating Guillery's. Anyway, I think the key sentences/claims (e.g. last of the abs) and results stand and are extremely interesting,\u00a0 I would just like to see this framed around mechanisms or instantiations of information processing constraints, or something, rather than competing hypotheses. and maybe acknowledge\u00a0more of the ambiguity of what \\\"learning systems in general\\\" means/ (without compromising clarity and narrative, which I believe are strong points of the paper).\\n\\nTerms used in equations are not explained when introduced\u00a0 in 3.1 -- I know it's from a citation and you might be short on space, but you will lose a lot of readers here and it's a shame not to continue the clear prose you've had thus far.\\n\\nAround eqn 9, connection of matric completion to what you're trying to do is not clear -- be explicit about how imputing missing values is the same as / allows us to specify\u00a0 task relationships with flexibility.\u00a0\\n\\nWhile the relationship of experiments to showing dependence on depth is very clear and well-explained, the insights and relationship of experiments to data distribution is much less clear.\", \"Questions\": \"See \\\"weaknesses\\\" for some less concrete suggestions (feel free to tell me what you plan to write if you address them and I'm happy to provide feedback!)\\n\\nThe main other thing I'd like to see is connections made/discussed with nonlinear networks -- what should we expect to change vs. hold? maybe according to the sources you cite (e.g. Saxe), or ideally, a suite of experiments with e.g. MLPs of increasing depth. \\nAnother specific link could be made in Fig. 6 to scaling laws.\\n\\nsmall things:\\n\u00a0- first sentence shouldnt repeat abstract\\n\u00a0- 'competition between pathways affect' -> affects\\n- inconsistent use of \\\"artificial neural network\\\", \\\"deep net\\\", \\\"neural net\\\", et al. For me it's fine, but could be confusing for someone outside the field; better to be consistent\u00a0\\n- title for plot should be removed (conflicts/causes potential confusion vs caption and X axis label, nonstandard for technical works to have an embedded title)\\n\u00a0- remind us what \\\"Path Singular Value\\\" is in the caption/change caption e.g .singular value dimensions (confusing that its referred to as 1-5 but labelled 0-4 on plot and called something different). All this stuff explaining the axis and basics of the plots\u00a0should go first, before the interpretation.\u00a0- I didn't understand the comment \\\"and do not affect performance on the final gask\\\" in fig $. Does it mean they have equal performance?\"}"}]}
{"paper_id": "jH67LHVOIO", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Modern language models (LMs) demonstrate impressive abilities but struggle with hallucination, where they produce incorrect but confidently asserted facts. Calibration of LMs, which aligns model confidence with actual correctness likelihood, is vital for reducing hallucinations and enhancing trust in model outputs. Traditional calibration methods either involve post-processing that doesn't affect rank order of possibilities (and thus are limited) or full model training, which is impractical for large models due to computational costs.\nGap: Existing LM calibration methods (post-processing and full re-training) inadequately address the need for efficient and effective recalibration, particularly for large models where computational cost and flexibility are concerns. Moreover, there is a lack of focus on calibrating LMs for long-form text generation beyond short-form tasks.\nHypothesis: The authors propose that a lightweight calibration mechanism, which requires only minimal additional parameters and can adjust the relative confidence rankings of model outputs, can effectively calibrate LMs across tasks varying in response length and improve model trustworthiness.\nMethod: The authors introduce LitCab, which adds a linear layer on top of an LM's last hidden layer to predict a bias term that adjusts output logits. This lightweight mechanism involves less than 2% additional parameters of the LM and uses a contrastive max-margin objective to train. For testing, LitCab's effectiveness is benchmarked using a new suite (CaT) that comprises eight tasks covering both short- and long-form responses. These tasks include a variety of commonly used datasets for testing phrase, sentence, and paragraph level generations. Extensive evaluations are conducted on models including Llama2-7B, GPT, and LLaMA families, using ECE, Brier score, and selective classification metrics.\nConclusion: LitCab effectively improves calibration across all tasks and outperformed traditional methods and confidence estimation approaches in terms of reducing calibration errors. Notably, it demonstrated better data efficiency and was computationally more efficient. Key insights include: larger models within a family are better at short response tasks, GPT models showed superior calibration despite fewer parameters, and improper fine-tuning could degrade calibration. The findings suggest that calibration should be optimized alongside model accuracy.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 3, \"Strengths\": \"1. The model calibration is highly meaningful, especially for nowadays LLMs, and the proposed method can be used for most of the open-source models.\\n\\n2. The proposed method is simple yet effective, although it needs some training examples collected by ChatGPT and GPT-4.\\n\\n3. The collected datasets CAT can also be useful for evaluating the methods for LLMs calibration.\", \"Weaknesses\": \"1. The paper lacks discussion and comparison of some related works [1][2].\\n\\n- [1] Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. arXiv 2023.\\n\\n- [2] Eliciting Latent Predictions from Transformers with the Tuned Lens. arXiv 2023.\\n\\n2. There are still some efficient fine-tuning methods (e.g. LoRA) that can leverage additional training data, but the paper lacks results compared with such methods.\", \"Questions\": \"1. It appears that your method involves using GPT-4 to generate training data, which has also been widely used in recent research. Does this introduce potential unfairness in the comparison? If the training data cannot be generated using GPT-4 or if its quality is poor, would the method still be effective?\\n\\n2. The proposed method seems to have minimal differences compared to the original LM, and the experimental results also indicate that the original LM performs not very badly. Therefore, what are the main advantages of the proposed method? How does it improve LLMs qualitatively or quantitatively?\\n\\n3. Why do you use different thresholds (e.g., cov@50, cov@90, cov@60) for different datasets?\"}"}]}
{"paper_id": "2J25Vi9W8I", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Ordinal regression, also known as ordinal classification, is concerned with predicting labels with a natural order, such as age estimation and historical image dating. It is a problem of growing interest and has been approached using regression-based, classification-based, and ranking-based methods, each with its strengths and limitations. Pre-trained vision-language models (VLMs), like CLIP, have shown improvement in many vision tasks, but their applicability to ordinal regression tasks remains inadequately explored.\nGap: The paper identifies insufficient exploration of vision-language models, specifically CLIP, in the field of ordinal regression. Existing techniques often suffer from limited performance due to insufficient numerical captions and ineffective training objectives when applying CLIP directly to these tasks.\nHypothesis: The authors propose that by integrating a coarse-to-fine learning paradigm with language prior information, CLIP can be effectively adapted for ordinal regression tasks ensuring both semantic and ordinal alignment, thus significantly enhancing the performance over existing methods.\nMethod: The RegCLIP framework involves a two-level coarse-to-fine learning approach. Initially, a stagewise process is utilized, where intermediate classification is performed followed by refined predictions. Coarse semantic labels generated by large language models serve as these intermediate classes. This approach is further refined using a novel fine-grained cross-modal ranking-based loss that updates these labels to ensure semantic and ordinal alignment. The method is validated against three tasks: historical image dating, image aesthetics assessment, and age estimation, showing significant performance improvements over state-of-the-art baseline methods.\nConclusion: RegCLIP introduces a novel framework leveraging coarse-to-fine methods and language models for efficient ordinal regression. The framework outperforms existing methods significantly in accuracy and mean absolute error (MAE) in various tasks under different settings, demonstrating its effectiveness and potential impact in enhancing CLIP's performance in ordinal regression domains.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 5, \"Strengths\": \"- The paper is easy to follow and well organized.\\n- The proposed algorithm is simple but effective way to utilize CLIP for ordinal regression tasks.\\n- The proposed algorithm shows good results on various datasets.\", \"Weaknesses\": \"1. Even though the authors mentioned that they will release the codes, I think there are some missing details which are important for reproducibility and further researches.\\n\\n     - Section 3.5: Exactly, what is the revised FCRC loss? \\n     - How the fine-grained labels are predicted?\\n     - What is batch size M?\\n     - In Eq(2), what is the default value for $\\\\beta$?\\n     - In Eq(2), how the scaled ranking differences are normalized?\\n     - What are language templates for historical image classification and aesthetic quality assessments?\\n\\n\\n2. It would be good to have comparison of feature space visualizations for ablated methods. I think it can provide insights and intuitive understanding of each design choice for readers.  \\n\\n\\n3. MORPH II dataset has many different evaluation protocols. However, the results for only one setting are provided in the paper. So, it may be more useful to have more comparisons on MORPH II dataset. \\n\\n\\n4. There are some other popular datasets for age estimation (e.g. CACD, UTK, CLAP2015, Adience, \u2026). So, it would be better if the results on those datasets are compared in the paper.\", \"Questions\": \"The proposed algorithm is simple but makes sense for me. However, it omits some important details in the paper and does not contain many analysis of the proposed algorithm, as I mentioned in the weakness section. (Please see the weakness section for other concerns.) Nevertheless, in overall, I'm positive about the acceptance of this paper, because it outperforms all conventional methods with meaningful performance gaps and also authors described that the source codes will be released for algorithm details.\"}"}]}
{"paper_id": "gENfMmUIkT", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Smart surveillance systems are increasingly necessary for urban safety and are often implemented using the IoT paradigm, which integrates edge and cloud computing. These systems face challenges such as resource management, bandwidth, and message latency, which are critical for effective object detection using convolutional neural networks (CNNs) on resource-constrained devices.\nGap: While many smart surveillance systems employ strategic pipeline steps to enhance efficiency and accuracy, they typically rely on hybrid edge-cloud architectures. There is a lack of solutions that address computational constraints specifically focusing on multi-class object detection solely on edge devices, without the reliance on cloud-based resources or GPUs.\nHypothesis: The authors propose that a pipeline-based approach to object detection on edge devices can preserve detection accuracy while significantly improving processing speeds and reducing computational burdens, making it suitable for use in resource-constrained IoT scenarios.\nMethod: The proposed method involves a multi-step pipeline leveraging object size and correlation to optimize processing time and accuracy. The embedded pipeline involves steps for detection of people, cars, guns, and license plates, selectively zooming and processing based on detected context. YOLO-based CNNs (specifically YOLOv3 and YOLOv4 Tiny versions) are used, trained on a dataset of 135,000 images. The pipeline was evaluated using real hardware consisting of a Labrador 641 single-board computer in a field-proven environment. The setup included a USB camera and additional support from a microprocessor-based subsystem for feature collection and communication.\nConclusion: The pipeline approach outperformed traditional setups by reducing execution time by a factor of three and enhancing mean average precision of object detection. The system was effectively used in a surveillance context, confirming its suitability for IoT devices and edge computing applications, providing significant improvements in practical testing scenarios.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 1, \"Confidence\": 5, \"Strengths\": \"-\\tThe paper addresses a significant issue concerning object detection on resource-constrained IoT devices in the context of smart surveillance systems. Ensuring safety in metropolitan areas is a pressing concern, and the proposed approach aligns with the needs of smart cities and law enforcement.\\n-\\tCollaborative testing with the police department in real-world scenarios adds credibility to the research and demonstrates the practical applicability of the proposed approach.\", \"Weaknesses\": \"-\\tThe work seems to be an implementation of already existing and popular YOLO architecture on custom dataset.\\n-\\tThe work does not compare the work with other works that have been developed for surveillance.\\n-\\tThe pipeline-based approach uses multiple runs of the YOLO model to detect objects which adds more time delay to detection missing other important potential events while processing one event.\\n-\\tWhile the paper provides some quantitative data, a more detailed analysis of the results, especially in the pilot testing, would enhance the paper's completeness.\", \"Questions\": \"-\\tHow many classes are present in the custom dataset being used in this work? \\n-\\tIs the model trained on edge device (Labrador)? Or was it just used for Inference?\\n-\\tWhat is the frame rate used in this methodology for detection? \\n-\\tWhat measures have been taken to detect occluded objects?\\n-\\tSince the use case is a mobile surveillance and involves testing in real world scenario, what are the processes in place to identify false events transmitted?\\n-\\tHow do you keep up with the event generation when subsequent runs are needed? E.g. For generating an event from S3, you would need 8 seconds(1.5s+ 0.5s +6s). Does the system not detect any other events while it is processing one event?\"}"}]}
{"paper_id": "oKn2eMAdfc", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Capsule Networks (CapsNet) use capsule vectors to represent entities in images, with redundancy being a persistent issue leading to high computational costs and parameter counts. Traditional dynamic routing involves iterative processing and full connection between capsules, which contributes to redundancy and inefficiencies.\nGap: Despite pruning existing in previous works for CapsNets, redundancy and inefficiencies persist in deeper layers due to the reintroduction by dynamic routing. The iterative and connected nature of dynamic routing maintains high similarity among capsules leading to feature overlap.\nHypothesis: Introducing orthogonality and sparse attention routing to CapsNets can reduce redundancy, improve routing efficiency, and decrease the parameter count, overcoming the limitations of dynamic routing and traditional pruning methods.\nMethod: The OrthCaps model introduces a pruned capsule layer to reduce redundancy using L2 norms and cosine similarity. It replaces dynamic routing with orthogonal sparse attention routing, using a Householder orthogonalization approach to maintain capsule direction and diversity. Attention routing employs \u03b1-Entmax for sparsity, and a Capsule ReLU activation function is proposed to address the vanishing gradient issue prevalent in deep networks. Experiments were conducted on MNIST, SVHN, CIFAR10, CIFAR100, and FashionMNIST datasets.\nConclusion: OrthCaps significantly reduces parameters and computational costs while improving classification accuracy, outperforming state-of-the-art benchmarks on various datasets. The introduction of orthogonality and pruning strategies addresses the redundancy and inefficiencies of dynamic routing, and the Capsule ReLU activation function mitigates gradient vanishing issues. OrthCaps provides a pathway for more efficient and robust capsule networks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 1, \"Confidence\": 2, \"Strengths\": \"1. The idea of utilizing orthogonality is interesting.\\n2. Usage of Householder parameterization is natural and easy to understand.\", \"Weaknesses\": \"1. This paper is very hard to follow. I got completely lost even reading the Introduction. I would suggest the authors completely rewrite the paper to make it readable.\\n1.1 It is mentioned that attention routing is used. But how does it eliminate the need for iteration? \\n1.2 What is the fully-connected problem and how is that solved by sparsemax-based self-attention?\\n1.3 Why is a simplified attention-routing mechanism used with OrthCaps-S and what is it?\\n1.4 What are capsule vector direction shifts and how are they solved with orthogonality? Why does preserving the direction of capsule vectors mitigate feature interference?\\n1.5 What is the motivation of  Capsule ReLU? Why is that used with OrthCaps-D but not OrthCaps-S?\\n\\n2. Why is capsule ReLU designed so? What is the motivation for using BN on a scalar (||s_j||^2)? It is very unnatural, so I would expect a clear motivation.\\n\\nI rate a strong reject mostly because the writing quality of this paper makes it difficult for me to accurately understand the essence of the proposed method, thus preventing me from evaluating its technical correctness and significance.\\n\\nOf course, as I am not an expert in capsule networks, it may be due to my lack of relevant knowledge that I cannot understand this paper. If the writing of this paper is not the issue, that is, if AC and other reviewers can understand this paper without difficulty, please let me know, and I am willing to modify my rating.\", \"Questions\": \"See the weakness part.\"}"}]}
{"paper_id": "KrWuDiW4Qm", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Physics-informed machine learning (PIML) seeks to integrate physical knowledge with machine learning to enhance predictions across diverse fields like climate science and biological sciences. Traditional PIML methods work well for in-distribution tasks but struggle with out-of-distribution (OOD) scenarios where the test data has different parameters or initial conditions from the training data.\nGap: Existing PIML methods perform poorly on OOD tasks because they rely on spurious associations learned by standard machine learning components and lack robustness to changes in initial conditions and ODE parameters.\nHypothesis: The authors propose that by using a meta-learning procedure to discover causal structures, PIML can be enhanced to improve robustness to OOD scenarios.\nMethod: The proposed approach, MetaPhysiCa, builds on meta-learning to improve forecasts across OOD settings. The method involves learning a shared causal structure across different tasks with varying initial conditions and ODE parameters. It combines causal structure discovery via continuous optimization with an invariant risk minimization-type penalty, and applies a two-layer architecture to model the underlying system dynamics. The approach was tested using simulated data for damped pendulum systems, predator-prey models, and epidemic modeling, using tasks with varying initial conditions and parameters.\nConclusion: MetaPhysiCa outperformed state-of-the-art PIML and deep learning methods on three different OOD tasks, achieving 2\u00d7 to 28\u00d7 lower errors. The method successfully identified the true causal structures, showing improved robustness in OOD forecasting settings. However, the approach may require better optimization to avoid learning stiff ODEs when extended to more complex models.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 5, \"Strengths\": \"1. Develop a meta-learning technique for identifying the underlying governing equations of dynamical systems.\\n\\n2. Formulate it as a bi-level optimization problem\\n\\n3. The evaluation results have demonstrated the good performance of the proposed method over baselines.\", \"Weaknesses\": \"1. How many basis functions are used in the experiments? If there is no prior knowledge, the number of basis functions will be very large.\\n\\n2. The proposed method still adopts the SINDy-like approach in which all key terms should be included in a set of basis functions. It may not work on complex equations like $dx/dt = 1/(x_1^2 + sin (x_2^3+x_3))$\\n\\n3. Conduct experiments on more complex systems in the CoDA paper, such as reaction-diffusion system and Navier-Stokes system\\n\\n4. I am curious about the causal structure in Fig. 4. Which two terms in each of the three dynamical systems have causal relationships? Please give an example to explain it. It seems that the authors did not discuss this in the Evaluation section.\\n\\n5. Please conduct experiments on noisy data.\", \"Questions\": \"Please see the comments above.\"}"}]}
{"paper_id": "RmQAKu1wCe", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Spiking Neural Networks (SNNs) are inspired by biological neural mechanisms and are valued for their energy efficiency, particularly when implemented on neuromorphic hardware. Traditional methods for training SNNs often involve surrogate gradients to handle the non-differentiable spiking behavior, which allows them to function similarly to Recurrent Neural Networks (RNNs). However, these methods are typically optimized for a specific time step, necessitating separate and computationally expensive fine-tuning for other time steps.\nGap: Current training methodologies only optimize SNNs for specific time steps, which leads to computational inefficiency and the need for fine-tuning each time a new time step is introduced. This lack of temporal flexibility limits the generalization capabilities of SNNs across different time steps.\nHypothesis: The authors propose that using a mixed time step training (MTT) method will create a temporally flexible SNN (TFSNN) that can generalize across different time steps without requiring fine-tuning. This method could improve performance, efficiency, and adaptability of SNNs.\nMethod: The proposed method involves mixed time step training (MTT), where different time steps are randomly assigned to various stages of SNNs during training. This creates temporal flexibility by training a single TFSNN capable of performing at any time step without additional fine-tuning. The method includes developing inter-stage communication protocols and testing the model on various datasets. The approach was validated experimentally using static datasets like CIFAR10, CIFAR100, and ImageNet as well as event-based datasets such as CIFAR10-DVS and N-Caltech101. The paper also explores TFSNN's compatibility with dynamic inference time steps and its potential for deployment on asynchronous neuromorphic chips.\nConclusion: The TFSNN achieved superior accuracy compared to traditional approaches, such as a 96.84% accuracy on CIFAR10, 81.98% on CIFAR100, and 68.34% on ImageNet with time step T = 6. It showed robust generalization across different time steps and proved more efficient in computation by eliminating fine-tuning. The work demonstrates improved generalization, flexibility, and potential reduction in energy consumption, positioning the TFSNN as a suitable candidate for advanced neuromorphic applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"The MTT performs a wider search along the timestep setting by sampling different. Compared to naive NMT, MTT urges each layer of SNNs to learn feature that is weakly correlated to input length from the previous layer. Such a strong regularization causes an issue in assessing mean and variance within BN layers but is immediately resolved by calibration. Overall, I believe MTT marks a significant step forward in the right direction.\", \"Weaknesses\": \"However, all datasets used in MTT are directly retrieved or derived from static images (CIFAR10-DVS & CIFAR10, N-Caltech101 & Caltech101). I'm worried that these datasets contain essentially time-invariant features along the time dimension. Even for CIFAR10-DVS or N-Caltech101, only simple movement of static images is recorded using the event cameras. The authors should validate their methods on those datasets with affluent temporal dynamics, like from DVS-Gesture to audio such as GSC (Google SpeechCommands) or SHD (Spiking Heidelberg Dataset) to strengthen their results.\\n\\nThe former concern raises the other issue that, mixing different timesteps is partially empirically based on the finding that the temporal gradients resemble each other when timestep stretches or shrinks, and we expect the training will be stable to some extent since gradients are relatively similar. If the input has invariant length and a different amount of temporal information in essence, will the MTT work as it is now?\", \"Questions\": \"Could the authors demonstrate results on datasets with rich temporal information? This could bring some real challenges to the TFSNN.\"}"}]}
{"paper_id": "1P92J25hdf", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the integration of inductive bias (IB) in neural networks, which enhances modeling capabilities by incorporating preferences that improve generalization ability. In the context of computer vision, general IB is associated with CNNs and Transformers, leveraging scale invariance and long-range dependencies. Stereo matching, a geometric vision task, typically does not effectively utilize general IB due to volume-level scale invariance issues and the need for real-time processing.\nGap: Stereo matching often neglects volume-level scale invariance and struggles with capturing long-range dependencies without utilizing time-consuming Transformers, which limits its real-time applicability. Additionally, the potential of specific IBs associated with volume structures in stereo matching is underexplored.\nHypothesis: The authors hypothesize that introducing general IB into stereo matching can be achieved through a novel model, UStereo, which incorporates inter-layer fusion and specific volume structures, addressing both scale invariance and long-range dependencies without sacrificing real-time performance.\nMethod: The authors propose UStereo, a model that applies inter-layer fusion to address scale invariance issues and a strategy to capture long-range dependencies through shallow convolutional stacks without relying on Transformers. The model uses a Dense Scale-Aware Fusion (DSF) to connect volumes across resolutions and introduces a Mixed Direct Long-Range Compensation (MDLC) block to enhance long-range dependency capturing. Additionally, the paper presents a study on the roles of specific IB in volume structures through deep supervision techniques and original statistical indicators to evaluate volume distributions during inference.\nConclusion: The experiments demonstrate that UStereo achieves competitive performance with improved speed and robustness compared to existing volume-based methods. The model efficiently incorporates general IB into stereo matching. The study of specific IB reveals that volumes used in stereo matching can indeed function as confidence volumes and that a concentrated disparity distribution within volumes enhances model performance.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. This paper builds a new stereo matching network UStereo that can achieve real-time inference and great accuracy.\\n2. They propose to think about the specific IB in cost volume representation, which is interesting.\", \"Weaknesses\": \"1. The writing needs improving. Introducing the IB concept makes it hard to understand what the authors claim. I wonder why it needs to introduce IB instead of simply exploring scale invariance and long-range dependencies.\\n2. The results of the KITTI benchmarks are not impressive. Among real-time methods, we find CGI-Stereo, HITNet, and Fast-ACVNet+ get better performances.\\n3. There is a lack of evaluating the generalization ability in common settings. This paper claims UStereo has robust generalization, however, the common setting for generalization evaluation is on the KITTI, Middlebury, and ETH3D datasets.\", \"Questions\": \"1.  I still wonder what is the specific inductive bias in the volume representation. I think using the cost volume as confidence representation has been explored, especially in the are of Muli-view Stereo. So I want to know if there is any other specific IB proposed in this paper.\"}"}]}
{"paper_id": "lOwkOIUJtx", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The efficient use of visual information is a significant challenge in image classification and detection due to conflicting requirements such as high accuracy, low latency, and high energy efficiency. Biological visual systems are often more effective than artificial ones, utilizing mechanisms like foveal-peripheral sampling and saccades to enhance precision and energy efficiency. Foveal vision captures high-resolution details only in a small part of the visual field, while peripheral vision captures less-detailed information over a wider area. This approach reduces data input needs and increases robustness in visual tasks.\nGap: Despite advances in artificial visual systems inspired by biological mechanisms, existing approaches often do not fully capitalize on foveation and saccades to enhance data efficiency and accuracy. Most current systems do not integrate sequential saccades or perform comprehensive scene reconstruction, but instead require retraining classifiers with collected foveal-peripheral views, which is data-intensive and lacks efficiency.\nHypothesis: The authors propose that an artificial vision system that effectively integrates foveal-peripheral sampling, saccade control, and continuous scene reconstruction can significantly enhance image recognition performance while reducing data input requirements. This system could potentially achieve state-of-the-art performance with higher data efficiency and lower training complexity compared to existing methods.\nMethod: The proposed model processes visual information through three core components: a predictive reconstruction module, an evaluation module, and a policy module. The predictive reconstruction uses a three-layer ConvLSTM and self-supervised learning to reconstruct high-resolution images from foveal-peripheral views. The policy module employs reinforcement learning to optimize saccade decisions, thereby distributing visual focus intelligently across a scene. The evaluation module appraises the reconstruction quality using metrics such as SSIM or classification accuracy. The system was tested on the GTSRB and ImageNet datasets, demonstrating improved classification accuracy and reduced pixel usage.\nConclusion: The study introduces a novel framework combining foveal-peripheral sampling, saccade control, and scene reconstruction to enhance image recognition with reduced data input. Results show significant improvements in classification accuracy (e.g., 5% higher on ImageNet) with 70% less pixel usage compared to traditional methods. The framework is data-efficient, requiring less retraining effort, and can integrate with existing back-end models. The saccade model indicates high transferability, effectively extending its use to diverse visual tasks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 3, \"Strengths\": \"I find the method simple and useful, with interesting potential application. \\nIt is appealing that the method seems to be suitable for existing classification models (no retraining).\", \"Weaknesses\": \"## Major\\n\\nI am confused how the image information from the sequential glimpses is passed and integrated in the predictive reconstruction model. Much more space is spent on the background to the hybrid loss function than actually making explicit how the sequential image information is used to improve reconstruction.\\n\\nIn addition, the abstract states \\\"our model reduces the required input pixels by over 90% per frame while maintaining the same level of performance in image recognition as with the original images.\\\" I don't understand where to find support for this claim in the results. For example, in Figure 3, all subsampled models perform worse than the original. The data in Figure 4 are coming closest to the original; is this what is meant?\\n\\nAlso, please clarify whether the experiments in Figure 3 are conducted with the trained saccade control model (which one?). \\n\\n\\n## Minor\\n\\n- Instead of \\\"continuous saccades\\\" a better terminology would be \\\"sequential saccades\\\" or \\\"scanpaths\\\". See e.g. [2, 3, 4]\\n- There are now known to be three types of photosensitive cells: rods, cones and intrinsically-photosensitive ganglion cells [1, 8]\\n- You use SSIM but the relevant paper(s) are not cited (e.g. [7]).\\n- Heading 3.1 \\\"Periphrl\\\"\\n\\n## Literature\\n\\n1. Do, M. T. H., & Yau, K.-W. (2010). Intrinsically Photosensitive Retinal Ganglion Cells. Physiol Rev, 90.\\n\\n1. Hoppe, D., & Rothkopf, C. A. (2019). Multi-step planning of eye movements in visual search. Scientific Reports, 9(1), 144. https://doi.org/10.1038/s41598-018-37536-0\\n\\n1. K\u00fcmmerer, M., & Bethge, M. (2021). State-of-the-Art in Human Scanpath Prediction (arXiv:2102.12239). arXiv. http://arxiv.org/abs/2102.12239\\n\\n1. K\u00fcmmerer, M., Bethge, M., & Wallis, T. S. A. (2022). DeepGaze III: Modeling free-viewing human scanpaths with deep learning. Journal of Vision, 22(5), 7. https://doi.org/10.1167/jov.22.5.7\\n\\n1. Rosenholtz, R. (2016). Capabilities and Limitations of Peripheral Vision. Annual Review of Vision Science, 2(1), 437\u2013457. https://doi.org/10.1146/annurev-vision-082114-035733\\n\\n1. Watson, A. B. (2014). A formula for human retinal ganglion cell receptive field density as a function of visual field location. Journal of Vision, 14(7), 15. https://doi.org/10.1167/14.7.15\\n\\n1. Wang, Z., Simoncelli, E. P., & Bovik, A. C. (2003). Multiscale structural similarity for image quality assessment. The Thirty-Seventh Asilomar Conference on Signals, Systems & Computers, 2003, 1398\u20131402. https://doi.org/10.1109/ACSSC.2003.1292216\\n\\n1. Zele, A. J., Feigl, B., Adhikari, P., Maynard, M. L., & Cao, D. (2018). Melanopsin photoreception contributes to human visual detection, temporal and colour processing. Scientific Reports, 8(1), 3842. https://doi.org/10.1038/s41598-018-22197-w\", \"Questions\": \"- I would like to see how the hybrid reconstruction loss changes over timestep, and not just classification accuracy.\\n- The sampling of the periphery of individual pixels with small probability is not very like human vision. Effectively this is providing low pass information. Have the authors considered how the sampling density could be approximated more plausibly (e.g. [6])?\\n- Have the authors considered comparing scanpath strategies learned in this model to human scanpaths (e.g. [3, 4])?\"}"}]}
{"paper_id": "532tcx7IHF", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Reinforcement Learning (RL) is a prominent research area with applications in smart manufacturing, autonomous driving, among others. Existing open-source RL projects like Stable-baselines3 and Tianshou offer algorithm implementations but face challenges such as long-term sustainability, comprehensive ecosystem development, and resource-intensive nature.\nGap: Existing RL libraries and frameworks often lack a comprehensive and modular approach, fail to decouple RL algorithms fully, and do not support long-term evolution plans. Many libraries focus primarily on model training, neglecting essential components such as model evaluation and deployment, and lack exhaustive benchmark testing data.\nHypothesis: RLLTE proposes a long-term evolution, extremely modular, and open-source framework for RL. It aims to set a new standard for RL engineering practices by offering a complete ecosystem that includes model training, evaluation, deployment, and a benchmark hub, along with a LLM-empowered copilot.\nMethod: RLLTE decouples RL algorithms into primitives from the exploitation-exploration perspective, offering modular components such as encoders, storage, policy, and distribution. It supports data augmentation for improved sample efficiency and generalization. RLLTE also includes an evaluation toolkit, deployment tools, and integrates LLM to create a copilot for reduced learning curves. The framework is designed to be flexible, accommodating various hardware for enhanced scalability.\nConclusion: RLLTE is poised to advance RL research and application by providing a comprehensive and modular framework that adapts to evolving needs. It fills the gaps left by existing RL projects by offering a long-term evolution plan, complete algorithm decoupling, and a robust ecosystem for academia and industry.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"* Modular and customizable design, allowing for easy algorithm development and improvement.\\n* Long-term evolution plan, ensuring the framework stays up-to-date with the latest research.\\n* Comprehensive ecosystem, covering various aspects of RL research and application.\\n* Built-in support for data augmentation techniques, improving sample efficiency and generalization ability.\\n* Multi-hardware support, accommodating diverse computing hardware configurations.\", \"Weaknesses\": \"* The paper does not provide a thorough comparison of RLLTE with other existing RL frameworks.\\n* The proposed LLM-empowered copilot is in its early stages and may not be as effective as expected.\\n* The paper does not discuss potential limitations or challenges in implementing the proposed framework.\", \"Questions\": \"* How does RLLTE compare to other existing RL frameworks in terms of performance, modularity, and ease of use?\\n* What are the specific advantages of using RLLTE over other RL frameworks for different types of RL problems?\\n* How does the proposed LLM-empowered copilot improve the overall RL research and application process?\\n* Are there any potential limitations or challenges in implementing the proposed RLLTE framework that the authors have not discussed?\"}"}]}
{"paper_id": "MeB86edZ1P", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Neuromorphic computing with spiking neural networks (SNNs) offers a promising path for energy-efficient AI applications. However, unlike human brains, which can learn various tasks throughout life, these neural networks suffer from catastrophic forgetting when learning new tasks. Traditional approaches inspired by neuroscience like episodic replay or synaptic metaplasticity do not guarantee complete knowledge preservation across neuron populations, which is crucial for neuromorphic computing. Machine learning methods involving orthogonal projection are mathematically grounded but lack a neural execution mechanism.\nGap: There is a need to develop a method for spiking neural networks that prevents catastrophic forgetting and is aligned with both biological principles and mathematical rigor, particularly suitable for neuromorphic computing, which traditional methods fail to adequately address.\nHypothesis: Lateral connections and Hebbian learning can be utilized to project neural activity traces into orthogonal subspaces, thus preserving old knowledge while learning new tasks in a neuromorphic-friendly manner.\nMethod: The authors propose a method called Hebbian Learning based Orthogonal Projection (HLOP) for SNNs, which involves using lateral connections and Hebbian learning to extract principal neural activity subspaces, enabling orthogonal projection. This method updates synaptic weights without influencing old tasks by modifying presynaptic activity traces and utilizes recurrent lateral connections with skew-symmetric weights for orthogonal projection. Experiments conducted with different SNN training methods and datasets showed that HLOP effectively prevents forgetting.\nConclusion: The proposed HLOP method consistently outperforms existing approaches, achieving nearly zero forgetting in SNNs across various settings. The work demonstrates that Hebbian learning and neural circuits provide strong continual learning capabilities by enabling orthogonal projection of activity traces. This method opens a path for developing high-performance, continual learning systems in neuromorphic computing.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1.The paper introduces for the first time a method that fully utilizes neuronal operations to approximate orthogonal projection matrices, and achieves the best performance surpassing multiple baseline methods on various datasets in continual learning task.\\n2.The implementation based on Hebbian learning can seamlessly integrate with different training methods and aligns well with the parallel local learning approach of neuromorphic chips.\\n3.HLOP can be directly applied to SNNs, providing a new approach for continual learning in SNNs.\", \"Weaknesses\": \"1.The method lacks sufficient detail in the methodology section. Providing complete formulas or a specific illustrative example would enhance understanding. \\n2.The baselines compared in the study include EWC (Kirkpatrick et al., 2017), HAT (Serra et al., 2018), and GPM (Saha et al., 2021). It would be valuable to investigate if there are recent works that achieve better results than these methods.\\n3.Although the continual learning approach in this study relies solely on neuronal operations without directly utilizing past data, the increasing number of subspace neurons with each new task learned implies a form of data compression and storage to some extent.\\n4. Although the paper uses Hebbian learning to achieve a pure neuronal operation method, it seems that the newly added subspace neurons do not participate in the model's forward process. If this is the case, then despite being a pure neuronal operation approach, it is essentially just an estimation method for orthogonal projection matrices.\\n5. While the paper demonstrates the effectiveness of HLOP, it does not provide an explanation or analysis of why the weight matrices obtained through Hebbian learning can serve as a substitute for the orthogonal projection matrix.\", \"Questions\": \"See weaknesses.\"}"}]}
{"paper_id": "tVMPfEGT2w", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: In standard reinforcement learning (RL), agents learn to maximize observed numerical reward signals. However, designing appropriate numerical rewards is challenging and can impact the effectiveness of RL algorithms. Preference-based RL (PbRL), where agents rely on human feedback in the form of preferences between trajectories, has emerged as a promising alternative. This area has gained attention in applications like games and robot learning. Offline RL, which relies on pre-collected data without active interaction, is relevant to PbRL due to the high cost and latency associated with obtaining human feedback.\nGap: There are few algorithms specifically designed for offline PbRL with robust statistical guarantees. Existing methods often assume linear models and known transition dynamics, which limits their applicability in scenarios with unknown transitions and general function classes.\nHypothesis: The paper proposes that it is possible to develop an algorithm for offline PbRL with general function approximation and unknown transition dynamics, providing competitive performance guarantees under partial coverage conditions.\nMethod: The proposed algorithm involves two main steps: estimating implicit rewards using Maximum Likelihood Estimation (MLE) with general function approximation from offline data, and solving a distributionally robust planning problem over a confidence set around the MLE. The method introduces a single-policy concentrability coefficient to measure the coverage of a policy and establishes theoretical guarantees for learning target policies. The approach is extended to handle action-based comparisons and unknown transition dynamics.\nConclusion: The paper presents the first provable algorithm for trajectory-wise PbRL with general function approximation under partial coverage. It establishes lower bounds to highlight the distinction between offline PbRL and standard RL with explicit rewards. The algorithm remains effective even with unknown transitions and action-based feedback, maintaining strong guarantees.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 4, \"Rating\": 8, \"Confidence\": 3, \"Strengths\": \"The paper is clearly written.\\nThe paper provides several strong and novel theoretically results on preference-based RL in offline setting. Some of these results also generalize the results of Zhu et al for linear function approximators to general function approximators.\", \"Weaknesses\": \"Although the main contribution is theoretical, it would be nice to see an empirical case study or two  where the given algorithm works as expected.\", \"Questions\": \"No questions.\"}"}]}
{"paper_id": "lsvlvWB9vz", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: In distributed training of large models, a major challenge is the communication overhead which limits scalability. To address this, lossy communication compression methods, including error compensation (EC), have been developed. These methods are widely used in data homogeneous settings but are less understood in heterogeneous environments common in federated learning.\nGap: Existing approaches assume unrealistic conditions such as bounded data heterogeneity and large batches. Moreover, handling arbitrary compressors with existing methods has not achieved linear speedup in training time when increasing the number of devices.\nHypothesis: The authors propose that a new mechanism, EControl, can regulate error compensation by controlling the feedback signal's strength, allowing for stable and efficient convergence in heterogeneous data settings without making strong assumptions about data distribution or requiring impractical batch sizes.\nMethod: EControl adjusts the strength of error feedback and fuses error and gradient update steps, allowing it to efficiently balance error compensation. It operates under strongly convex, general convex, and nonconvex settings. The paper provides convergence theorems for these regimes and presents empirical evaluations showing linear speedup and robustness to data heterogeneity.\nConclusion: EControl offers provably fast convergence in various problem settings and effectively overcomes the limitations of existing EC methods, making it suitable for distributed optimization tasks in real-world deep learning applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"New error compensation algorithm: EControl with theoritical gaurantees for strongly convex, general convex, and nonconvex functions\\nEmpirical experiments to support the theoretical gaurantees and assess the efficacy of EControl.\", \"Weaknesses\": \"see the section below (Questions)\", \"Questions\": \"-In step 6 of EC-Ideal you don't have the extra parameter \\\\eta that you have in EControl, can you explain why for EC-Ideal this is not needed. Will EC-Ideal have the same convergence guarantees if you add \\\\eta in step 6, with the same values as for EControl?\\n-Bounded variance: This assumption contradicts strong convexity. I understand it is in several works in the literature. Take for instance F(x,y,z) = zx^2 + (1-z)y^2, z follows Bernoulli (1/2), then E_z(norm(\\\\nabla F(x,y,z) - \\\\nabla E_z(F(x,y,z)))^2) = (x-y)^2 and this is not bounded for all x and y in R.\\n-You mention in page 7 that \\\"The asymptotic complexity of EControl in this regime with stochastic gradient is tight and cannot be improved,\\\"  I understand for the depence on \\\\epsilon, however can you tell why the dependence on other parameters can not be improved.\\n-In the experiments, how you relate \\\\zeta to the noniddness, why for instance zeta equal to zero is iid and zeta >0 is non idd.\\n-Did you try \\\\eta= delta/400 suggested by the theory in the experiments? why you need to fine tune it, since it is fixed by theory. Is only fixed \\\\eta = \\\\delta/400 that works in theory or many other values can work, this is not clear in the paper.\\n-Did you test with other compressors?\"}"}]}
{"paper_id": "3s4fZTr1ce", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Reinforcement learning (RL) is effective in decision-making but struggles with sparse-reward environments. Large Language Models (LLMs) potentially enhance RL by providing policy guidance, leveraging their pre-trained knowledge. Prior work involved fine-tuning LLMs for better task alignment, but faced challenges such as inaccessible model weights and computational demands.\nGap: Despite LLMs' extensive knowledge, they often struggle to understand specific downstream tasks, reducing their efficacy in assisting RL agents. Fine-tuning LLMs presents practical challenges and can harm generalization, complicating deployment across diverse environments.\nHypothesis: Incorporating an adapter model within the RL framework can optimize the interaction between RL agents and LLMs without modifying LLM weights, enhancing their ability to provide useful guidance in complex environments.\nMethod: The RLAdapter framework introduces a lightweight adapter model that fine-tunes its understanding based on feedback from RL agents. It transforms LLM guidance into more applicable insights for the agent, using a partially observable Markov decision process and leveraging a lightweight language model. Evaluations were conducted in the Crafter environment with 22 tasks, comparing against state-of-the-art baselines.\nConclusion: RLAdapter significantly improves RL performance compared to baselines, showcasing enhanced common-sense behaviors and decision-making capabilities in agents. The framework allows for effective leveraging of LLMs without modification, presenting a promising avenue for further research. However, complex tasks still pose challenges.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"- Novelty and Clever Design: The introduction of an adapter model to bridge LLMs and RL algorithms is a novel idea. The adapter model helps in effectively summarizing and communicating the essential details from the RL agents to the LLMs.\\n- Empirical Evaluation: The empirical evaluation of RLAdapter in the Crafter environment and its comparison with several baselines demonstrate its effectiveness. The RLAdapter not only showed improved performance over state-of-the-art baselines but also exhibited agent behaviors grounded in common sense.\", \"Weaknesses\": \"- Understanding Score: The paper uses an understanding score calculated based on the semantic similarity between a trajectory and the LLM\u2019s sub-goals. However, it seems that this aspect is understudied in the paper. A more in-depth exploration and justification of the understanding score, possibly discussing its limitations, would have strengthened the paper. It\u2019s unclear whether it indeed does what the authors intended.\\n- It\u2019s unclear whether SFT is needed or even helpful. Perhaps few-shot-learning would be enough.\\n- In the ablation study, when removing the RLAdapter model, I believe the authors should have used the same prompts with the LLM. I was left thinking that perhaps the improved performance comes from a form of self-critique of the goal proposer before writing the next goal. The same could be achieved with prompting GPT.\\n- SPRING is mentioned quite few times, so I'd expect it to be better mentioned in \\\"related work\\\". In particular, examples of tight coupling with the environment.\", \"Questions\": \"- Analysis of Adapter Model's Capability: Does the adapter model have the capability of generating more general goals apart from making the LLM give more detailed goals? An exploration of this aspect would provide a more comprehensive view of the adapter model's functionalities.\\n  * If the adapter model only makes the LLM give more detailed sub-goals, we could simply prompt the LLM to analyze whether the last sub goal seems to be achieved and give more detailed sub-goals if not.\\n- Similar to the point above, it seems like the adapter model could be replaced by the LLM itself? Just querying it twice, one for analysis and another for sub-goal.\\n- Detailed Analysis: A more detailed analysis, such as an ablation study, on the understanding score, and how it influences the model's performance, could be beneficial.\\n- [Minor] The use of training steps in Table 1 as a metric might not be the most relevant. Consideration of other metrics like FLOPs could provide a more standardized comparison across different models. I understand this is probably not feasible given that OpenAI inference FLOPs are not public.\"}"}]}
{"paper_id": "mvGa1ikBD3", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Simulating nonlinear and anisotropic materials is crucial for applications in engineering, computer graphics, and robotics. Traditional mesh-based simulations are accurate but computationally expensive, which limits their use in interactive settings. Recent developments in graph neural networks (GNNs) offer a potentially more efficient alternative for such simulations.\nGap: Existing GNN-based methods for simulating material deformations are limited to isotropic materials, as they do not account for directional deformations and thus cannot model anisotropic properties.\nHypothesis: The authors propose that incorporating directional encodings in GNNs can preserve deformation directionality during message passing, thus enabling accurate modeling of anisotropic materials.\nMethod: The proposed GNN architecture involves a new edge feature decomposition where edge features are aligned with basis vectors to capture directional dependencies. The network follows an encoder-processor-decoder design and utilizes a physics-based, self-supervised training loss focusing on dynamic equilibrium. The approach involves training on a graph representing the mesh of a deformable object, where edge features include fiber orientations which are used to factor anisotropic properties during deformation simulation.\nConclusion: The authors' approach successfully models anisotropic materials by preserving directional deformation information, achieving better performance than existing GNN-based simulations in both convergence speed and accuracy. Their method also maintains volume preservation and accurately simulates stress-strain behavior in anisotropic materials, even outperforming traditional mesh-based GNN approaches.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"The paper addresses a useful open question within the emerging topic of learned simulators, equipping message-passing networks with anisotropic elasticity.  The solution is simple but effective, and experimental results show a meaningful improvement over the baseline method.  Overall, the writing is clear, and experiments seem reproducible.\", \"Weaknesses\": \"The technical contribution seems potentially incremental from prior work (MeshGraphNets) -- however, this is not necessarily an issue, as the experiments are well-designed, the results are solid and finding are conclusive.\\n\\nI would be interested to see additional experiments, beyond the cantilever (and cantilever-like) setup, such as simulation with collision/contact.  This is not a requirement though, as the existing experiments are quite informative.\", \"Questions\": \"- I am curious about the \\\"material space bases\\\" mentioned on page 4.  Could you elaborate on how the \\\"material space bases\\\" are defined exactly?  How are these defined relative to global coordinates, and are they defined in a canonical way?\\n\\n- Would defining the local bases in a different way change the three axis-aligned weights/coefficients that are computed during preprocessing?  In turn, would this make it difficult to learn?  (Presumably the x-axis is aligned with the edge, what about the others.). Thanks, look forward to your reply.\"}"}]}
{"paper_id": "bQWE2UqXmf", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large language models (LLMs) have advanced significantly in text generation, enabling tasks like question answering and code synthesis. However, this proliferation poses risks such as cybersecurity threats, misinformation, and academic dishonesty. Detecting AI-generated text is crucial to mitigate these issues. Traditional detection methods, using token log probabilities and other high-dimensional features, struggle with newer LLMs and are incompatible with black-box LLMs, which only provide word output via APIs.\nGap: Existing models for detecting AI-generated content often rely on internal LLM features like token log probabilities, which are not accessible in black-box models such as GPT-3.5 and GPT-4. Furthermore, these methods use high-dimensional features prone to overfitting and are not robust to novel content or adversarial attempts to bypass detection.\nHypothesis: The hypothesis is that LLMs consider machine-generated text as high quality, leading to fewer edits during a rewrite task than with human-written text. Consequently, detection of machine-generated text can be achieved by analyzing the rewriting behavior of LLMs, specifically by measuring the editing distance between the original and rewritten texts.\nMethod: The method, Raidar, detects AI-generated content by prompting LLMs to rewrite input text and calculating the editing distance of the output without relying on high-dimensional features. The editing changes are measured using bag-of-words edits and Levenshtein distances. Raidar operates on the output from any LLM, making it compatible with black-box models. It was empirically tested on datasets across varied domains, such as news articles, student essays, code, and Yelp reviews. The approach analyzes invariance, equivariance, and output uncertainty of text to discern AI-generated text from human-written text.\nConclusion: Raidar significantly enhances detection capabilities for AI-generated text over existing methods by leveraging LLMs' rewriting behavior. It improves F1 detection scores across multiple domains, demonstrating effectiveness even on novel content and resistant to adversarial attempts. The reliance on simple symbolic text properties and editing distances ensures robustness and compatibility with the latest LLMs.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"The paper discusses work on an interesting framing of detecting text generated by large language models by gauging how much rewrites (edit distance) it gives to a piece of text with the hypothesis that it considers machine-generated texts, regardless of specific models, as high quality ones compared to human-generated texts. The simplicity of the method is what I appreciate the most with the paper. I also appreciate that the author/s are able to construct a good research plan for the methodology of the study and the formalization of the task. There are some revisions suggested below to improve the structure and organization of the paper, particularly the uniformity and presentation of results.\", \"Weaknesses\": \"The paper starts well with the introduction, statement of motivation, discussion of hypothesis, and proposed method but went downhill with the presentation of results.\\n\\nFirst, in the discussion of datasets and model baseline used, the definition and characteristics should definitely be expanded by including the size of the dataset plus other unique characteristics of the data that\u2019s worth mentioning as well as the quantified performance of the baseline models just to give an idea to the readers of their relevance and why they are being used/compared in the study. \\n\\nSecond, the discussion on Section 4 seems to be too condensed and there are sudden insertions of models for comparison. Claude was tested but was not mentioned in the baseline models. Why is that and why is Claude a viable candidate for comparison? \\n\\nThird, there is no concrete / definitive discussion for the results presented in Table 1 and 2 and these are the bread and butter of the study. The table captions and Section 4.3 simply state the observed increments against the previous systems and nothing more. I would have appreciated it if there are more discussions on particular weaknesses in the texts by previous works which the current work addresses, and give some examples of this. Why are proposed methods get big performance jumps, say in the code dataset and Arxiv dataset, compared to natural language datasets? In the robustness experiments, why are the previous detector models left out? It is important to still consider them in the mix for better comparison of true performance. These discussions should be in order.\\n\\nThe statement \u201ctext from the Ada model is most easily detected, possibly due to its discernibly lower quality compared to human-written content\u201d is speculation. Some form of quantitative proof is required for this.\\n\\nIn terms of strengthening the paper, I strongly suggest the author/s to also experiment with second-language or non-native speaker writing corpora such as TOEFL or ESL essay corpus to gauge if the proposed method also exhibits that particular limitation of confusing AI-generated texts with non-native speaker-written texts. This has been a known limitation across all detectors of machine-generated texts. Refer to the existing works (https://arxiv.org/abs/2304.02819) and databases online (https://www.clarin.eu/resource-families/L2-corpora, https://uclouvain.be/en/research-institutes/ilc/cecl/learner-corpora-around-the-world.html) for the data to be experimented with. Regardless of the result, this would be a great addition and insight to the paper.\\n\\nLastly, the paper is littered with inconsistencies and wrong format for in-text citations which makes it very confusing to read. For example, it\u2019s better to read referenced text as \u201cGhostbuster (Verma et al., 2023) instead of Ghostbuster Verman et al (2023)\u201d. It seems like the Verma is included in the name of the model but it\u2019s not. There are also cases where the model is \u201cGPT-3.5-Trubo\u201d and then becomes \u201cGPT-3.5-turbo\u201d. Diagrams in Figure 6 should conform to uniform range of [0.0,1.0] in the y axis.\\n\\nI still see the importance and contribution of the study but major revision is required for the paper in order for readers to fully comprehend and appreciate the work.\", \"Questions\": \"1. How large is the feature set or dimension then for training the classifiers? Did the author/s conduct and hyperparameter optimization with the classifiers?\\n\\n2. How is the setup for comparison done for preliminary experiments shown in Table 1 and 2? What classifier was used for these experiments? There were two algorithms mentioned (LR and XGBoost) but it was never specified in the tables. The same way goes for Tables 4 and 5. I\u2019m confused as a reader as to which classifier was used for the presentation of results. Revising this part and enriching the details should be in order.\\n\\n3. Is the number of changes (deletions, insertions) done by the selective generative models between humans and machine-generated texts statistically significant? The visualization in Figure is nice but this would have been an additional supporting preliminary information to add to the hypothesis.\\n\\n4. There are six datasets chosen for experimentations but why are only subsets or some of it are used for results presented in Table 3, 4, and 5?\\n\\n5. The statement in Section 4.4 goes \u201cIn Table 5, while all models achieve significant detection performance\u201d. So a statistical test was conducted? Can you give the details for this? Otherwise, I suggest using the word substantial instead.\\n\\n6. Why is only the Yelp data used for special test cases such as effect of content length? Why not the other datasets as well? This seems very selective without reason.\"}"}]}
{"paper_id": "9IUZya8bCN", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Multi-label classification is critical in applications such as disease detection and image recognition, where inputs are associated with multiple ground-truth labels. Similar to multi-class classification, multi-label classifiers are vulnerable to data-poisoning attacks, where attackers can alter the training data to influence predictions. Existing defenses against such attacks have focused on multi-class classification, providing sub-optimal results when applied to multi-label classification.\nGap: Existing provable defenses against data-poisoning attacks are designed for multi-class classification and do not effectively extend to multi-label classification due to the complexity of multiple ground-truth labels per input. These methods assume a single-label per input, which leads to sub-optimal robustness guarantees when applied to multi-label scenarios.\nHypothesis: The paper hypothesizes that by jointly considering multiple labels when deriving provable robustness guarantees, it's possible to create an effective provable defense against data-poisoning attacks in multi-label classification.\nMethod: PoisoningGuard is introduced as a provable defense mechanism specifically for multi-label classification. The approach generalizes the concepts of bagging and Deep Partition Aggregation (DPA) to multi-label contexts. A key technical advancement is the ability to certify the number of labels predicted correctly (certified intersection size) despite data perturbations. This includes leveraging the law of contraposition and an extended version of the Neyman Pearson Lemma to address challenges in considering multiple labels simultaneously. The method is evaluated on three datasets where its robustness is compared against existing methods like bagging and DPA, showing superior performance.\nConclusion: PoisoningGuard significantly outperforms existing methods based on bagging and DPA when applied to multi-label classification, providing more effective robustness guarantees by jointly considering multiple labels. This demonstrates that it is possible to defend against data-poisoning attacks in multi-label settings using their generalized methods.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 5, \"Strengths\": \"Overall the quality of this paper is good.\\n1. Developing provable defenses against data poisoning attacks is an important topic.\\n2. The multi-label classification angle is interesting.\\n3. While I did not check the proofs line by line, I am familiar with these provable defenses that I think these results are reasonable.\", \"Weaknesses\": \"1.My major concern is in fact **a very specific mismatch** between the proposed defense and the setting of multi-label classifications.\\nSpecifically, the proposed defense **always predict k labels for a pre-defined constant k**, which is unnatural in multi-label classification. In typical multi-label classification, one does not really assume every sample must have exactly the same number of labels. For this reason, I am concerned **whether this is a reasonably good start for provable defenses in multi-label classification**. \\n\\nIn my opinion, the current formulation is more related to certifying top-k accuracy rather than multi-label classification. However, it is relatively trivial (I will still consider this as part of this work's contribution though) to generalize bagging/DPA for b to top-k accuracy since it is somewhat straightforward (at least for DPA) to allow each base classifiers to contribute multiple votes.\\n\\n2.There are some closely related but missing references [1-6 below]. [1-5] are closely related to provable defenses against poisoning attacks and [6] is about provable defenses for top-k predictions but for test-time attacks. Notably, the certificates of [4] essentially offer a solution for certifying top-2 predictions. Note that some of these references are in fact fairly new, thus while I suggest authors to discuss them, it is understandable that they are missing from the submitted version.\\n\\n\\n[1] Chen R, Li Z, Li J, Yan J, Wu C. On Collective Robustness of Bagging Against Data Poisoning. \\n\\n[2] Wang W, Levine A, Feizi S. Lethal Dose Conjecture on Data Poisoning. \\n\\n[3] Wang W, Feizi S. Temporal Robustness against Data Poisoning.\\n\\n[4] Rezaei K, Banihashem K, Chegini A, Feizi S. Run-Off Election: Improved Provable Defense against Data Poisoning Attacks. \\n\\n[5] Wang, W. and Feizi, S. On Practical Aspects of Aggregation Defenses against Data Poisoning Attacks. \\n\\n[6] Jia, J., Wang, B., Cao, X., Liu, H. and Gong, N.Z. Almost tight l0-norm certified robustness of top-k predictions against adversarial perturbations.\", \"Questions\": \"Please refer to Weakness. \\n\\n1. The major concern is about Weakness 1.\\nPlease clarify if you think there is anything missing.\\nIf you also find this to be reasonable, a direct fix from me will be to use either a fixed threshold or an adaptive one to decide whether each label is predicted or not. For example, instead of predicting a constant of k labels per sample, the ensemble can predict all classes with more than t votes for some constant t.\\nThis can actually simplify the computation of certificates and potentially make it much more suitable for the setting of multi-label classifications. However, this leads to critical changes to the proposed approach so revising&resubmiting to future venues may also be a good option.\"}"}]}
{"paper_id": "izdFGwDgvW", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Vision-language models such as CLIP, ALIGN, and BLIP have made significant progress in zero-shot image classification by combining contrastive and generative methods, allowing both visual and textual inputs to be projected into a joint latent space for better predictions. Despite their success in zero-shot classification, these models often underperform compared to in-distribution supervised fine-tuning and struggle with out-of-distribution (OOD) generalization.\nGap: Existing vision-language models struggle with OOD generalization in image classification. Although the correct class often appears within the Top-K predicted classes, these models frequently fail to identify the correct class at the top. Moreover, existing methods either require fine-tuning, large datasets, or rely on handcrafted prompts, which can be complex or not feasible in every scenario.\nHypothesis: The paper posits that enforcing consistency between image-conditioned and caption-conditioned predictions can improve the OOD generalization capabilities of vision-language models. By leveraging both image and caption embeddings, it is possible to guide the model towards the correct class within the Top-K predicted classes, thereby enhancing classification accuracy without the need for additional training.\nMethod: The proposed Image-Caption Encoding (ICE) method involves utilizing both image and caption embeddings during evaluation. First, images and class labels are embedded using standard methods. Then, captions are generated using the CoCa model's text decoder, attended by all image encodings. These captions are re-encoded as embeddings. During inference, probabilities for each class are computed separately from the image and caption embeddings with the cosine similarity and softmax operation. A weighted sum of these probability distributions determines the final prediction, where the caption's weight is dynamically adjusted based on the relative confidence of the image and caption probabilities, calculated from their standard deviations.\nConclusion: The ICE method improves zero-shot classification by leveraging the union of image and caption probabilities to make more accurate predictions in an OOD setting. This approach can be easily combined with other zero-shot methods, enhancing Top-1 OOD accuracies by an average of 0.5% and up to 3% on several challenging datasets, without requiring additional training. This showcases the potential of captions to provide complementary information that enhances model predictions.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"- The paper is well written and the organization of the paper is clear.\\n- Experiments are comprehensive with multiple downstream datasets under the zero-shot and few-shot setting.\", \"Weaknesses\": \"- It remains unclear why the text decoder from CoCa is used. It seems that the proposed method only requires a textual description of the input image (any off-the-self high-quality caption models may work). It would be interesting to compare with the performance when using other caption models.\\n\\n- Compared to CLIP, the approach requires an additional caption model (as the image decoder) and is dependent on the quality of the caption, which can be hard to measure. \\n\\n- Empirical improvement in the few-shot setting seems marginal compared to the baselines. It may be arguable whether the additional computational cost (of forwarding passing the image to the text decoder) is desirable.\", \"Questions\": \"- Can authors explain why CoCa is used, instead of an arbitrary caption model? If we replace CoCa with SoTA caption model, will the performance be significantly improved? \\n\\n- The performance gain in the few-shot setting seems marginal (Table 2). Does this mean that in the few-shot adaptation setting, we may just need a few images, instead of using an additional decoder?\"}"}]}
{"paper_id": "6vtGG0WMne", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Deep learning models often struggle with imbalanced datasets where the majority class dominates the data. Traditional methods aim to balance models to improve minority class performance by re-sampling or re-weighting. However, these methods usually focus on scenarios where training data is unbalanced, but the test set is balanced.\nGap: Existing balanced optimization methods cannot effectively handle real-world situations where the test data is also imbalanced, and different applications may require different optimizations between classes. It's challenging to adjust the model imbalance efficiently to meet specific application requirements.\nHypothesis: The authors propose that models should not always be balanced to be optimal for all applications. Instead, they should be optimized towards a user-specified imbalance state based on application metrics, leading to better performance in real-world settings.\nMethod: The authors propose a bias adjustment (BA) method that optimizes the model's class imbalance based on application-specific metrics. They define a Model Imbalance State (MIS) using class probability distributions and adjust the model's output layer bias to achieve the desired imbalance. The BA method uses KL divergence to align the MIS with a target distribution, making it efficient as it only modifies the model's final layer parameters. A new training strategy is introduced that uses BA during traditional training to find optimal representation parameters.\nConclusion: The proposed method effectively regulates model imbalance according to user-specified metrics, improving performance across various applications. Extensive experiments demonstrate that this approach offers significant improvements in accuracy, F1 score, and G-means compared to state-of-the-art methods. The BA method is found to be effective for different tasks, illustrating its broad applicability and efficiency.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 4, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"The proposed method is straightforward. Evaluations are however, fairly extensive, and are done across a variety of data types - tabular, image and text. A per-measure comparison is also provided. This kind of papers are persuasive with broad-based evaluations, usually on even more datasets.\", \"Weaknesses\": \"The hypothesis of just the bias on the softmax layer meant for classification making the classification swing by these amounts us astounding. In fact, a study on the values of the bias versus predictive performance is required in my opinion before bias adjustment is presented.\", \"Questions\": \"At 500:1 imbalance ratios, multiple benchmark methods show numbers decimated to zero. A previous study, MMM, by Mirza et al. '21 appears to suggest that even at such a ratio, classical resampling and even baselines report an above zero performance. Could you reason about the disparity?\"}"}]}
{"paper_id": "lf8QQ2KMgv", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Recent studies have suggested that memorization in deep learning models, the ability to associate training data with random labels, is essential for generalization, though intuitively memorization does not align with model generalization capabilities. Notably, Feldman and Zhang (2020) demonstrated that memorization is necessary for achieving optimal generalization error, claiming significant accuracy degradation when memorized points are removed from datasets.\nGap: The paper identifies several methodological errors in prior work by Feldman and Zhang (2020) that lead to an overestimation of memorization's impact on test accuracy. These include lack of model convergence, data leakage, and sub-population shift, which skew the perceived necessity of memorization for generalization.\nHypothesis: The authors hypothesize that after correcting for identified methodological errors, memorization does not significantly impact prediction accuracy. Consequently, memorization is not necessary for generalization contrary to prior assertions.\nMethod: The authors re-evaluate the impact of memorization on generalization by fixing errors in the original methodology of Feldman and Zhang (2020). They address model training convergence with weight decay, remove test data duplicates from memorization assessments to eliminate data leakage, and ensure sub-populations are appropriately represented. The study uses the CIFAR-100 dataset, training Resnet50 models and thoroughly analyzing the impact of removing so-called memorized points from the dataset after correcting for identified errors.\nConclusion: The corrected analysis reveals that the true impact of memorization on test set accuracy is significantly lower than previously reported. The original decline in accuracy after removing memorized points was largely due to training and data issues, not actual memorization necessity. The study finds no substantial impact of memorization on generalization, challenging the need for memorization in achieving high model generalization abilities.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"Despite there being ample room for further improvement of the paper, there are interesting visualizations of memorized and sub-population shifts in the paper.\", \"Weaknesses\": \"In general, the presented study appears to be inadequately explored, leaving more room for further empirical research and justification of observations based on theoretical foundations. The stated issues do not necessarily contradict the findings of the previous work, especially in the case of the \\\"Sub-population Shift\\\" argument, which I will explain further in my review. Therefore, this paper may not be suitable for this particular venue and might be better suited as a workshop paper or considered for the reproducibility track to facilitate further experimentation and validation of the results.\\n\\nI am strongly leaning towards rejecting the paper; however, I will try to provide my understanding, concerns, and suggestions for the authors so they can reevaluate their work and make a better presentation of their study.\\n\\nConcerns, understanding, and suggestions: \\n1. The introduction section does not adequately define the problem, and the background section has not provided sufficient background information.\\n2. Regarding errors stated in the paper:\\n    1. \u201cLack of Convergence: Models\u201d: Why should models be trained to their maximum test set accuracy? You don't have access to the test set during training!\\n    2. \u201cData Leakage: Training\u201d: The authors of the original work stated this in their paper in Figure 3 that there is a close match of memorized points in the test set.\\n    3. \\\"Sub-population Shift\\\": The argument does not necessarily contradict the previous work because it aligns with what they have been arguing in their paper: memorization is necessary due to the long-tail distribution. Both mislabeled and sub-population data points are part of the long tail; therefore. It does not necessarily mean that the model memorized the points associated with sub-populations. They have high influence scores because they are rare in the data distribution, and the model exhibits a similar behavior of memorization.\\n3. There is an argument in the paper that the definition of memorization from the original work is incorrect, but there is no strong justification for this claim, and it is deferred to future research. I would suggest the authors first review their observations and results and then use them to formulate a more robust definition of memorization. In that case, your work would be better supported.\\n4. There are many repetitions of information with different formats conveying the same content in the paper, such as in the Introduction, Background, Related work, Discussion, and Conclusion. The presentation format could be significantly improved.\\n5. The \\\"Data Leakage\\\" in the Background section is not sufficiently well explained. I have read the \\\"Do CIFAR-10 classifiers generalize to CIFAR-10?\\\" paper, and the context provided in your paper is what they have already explained, and it can be moved to the appendix.\\n6. Table 1: It is not necessary to provide a table for symbols in the main pages; it can be moved to the appendix.\\n7. A large portion of the paper is dedicated to understanding the original paper, and it is defined in a way that favors the introduced issues mentioned at the beginning of your paper. It can be shortened, and the detailed discussion can be left in the appendix.\\n8. Section 5.1 (SETUP): You should check what your models memorize with the modifications you have made, not just the 1015 memorized points provided by the original authors.\\n9. The implementation of fixes is not provided along with the submission.\", \"Questions\": \"Questions: \\n1. The previous work that you studied is based on a theoretical background. Could you please delve into the details of the theory part and justify your observation? I believe that if your argument is completely true, it should also be possible to find a flaw in the theory of the work.\\n2. Section 5.1 (SETUP): How did you define which points impact the same test point?\\n3. Section 4.2 (ERROR: DATASET LEAKAGE): Are you assuming that duplicate points are only found in memorized points?\"}"}]}
{"paper_id": "zFWKKYz2yn", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Recurrent Neural Networks (RNNs) are effective at modeling temporal patterns and have been explored for their capabilities in recognizing grammatical structures. To understand RNNs better, researchers have focused on extracting interpretable rules in the form of Deterministic Finite Automata (DFA) from RNNs. Two main approaches for rule extraction are quantization and equivalence query (L* algorithm).\nGap: Current studies do not adequately address the stability of these rule extraction methods, which is crucial as instability can lead to failure in extraction. Additionally, there is insufficient empirical analysis comparing these methods across various RNN architectures, especially in less-than-ideal training conditions (partially trained networks).\nHypothesis: Quantization-based rule extraction is hypothesized to be more stable than equivalence query-based methods, particularly for partially trained RNNs. Additionally, higher-order RNNs like O2RNN are expected to encapsulate more stable rules compared to first-order RNNs.\nMethod: The study conducts extensive experiments on 3600 RNN models trained on datasets from 7 Tomita grammars and 4 Dyck grammars, using 4 different RNN architectures (LSTM, GRU, O2RNN, and MIRNN). Rule extraction was performed using quantization methods (k-means and SOM) and L* equivalence query methods. The stability of DFAs was analyzed by their state counts and performance consistency across random initialization seeds.\nConclusion: Quantization-based methods generally outperform L* in stability across both Tomita and Dyck languages. O2RNN shows superior stability and performance in DFA extraction compared to other RNN types. The study demonstrates that quantization-based extraction maintains stability even in partially trained conditions, making it a promising approach for practical machine learning applications where perfect accuracy isn't reached.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 4, \"Contribution\": 4, \"Rating\": 5, \"Confidence\": 1, \"Strengths\": \"The paper is well presented, by providing substantial background information and experiments.\", \"Weaknesses\": \"See questions.\", \"Questions\": \"Honestly, my familiarity with current RNN research, including rule extraction strategies, is limited. Consequently, I find it inappropriate to assess the topic, method, or the novelty of the work. However, I do have several questions for discussion.\\n\\n- It would enhance the paper if the authors could furnish additional insights or intuition regarding the significance of exploring the stability of rule extraction strategies. Alternatively, a clearer definition of stability in this context would also be helpful. In the current work, stability appears to be synonymous with the variance of performance.\\n\\n- A more thorough justification for the conclusion would be beneficial. Instead of solely benchmarking several methods across various settings, delving into the rationale behind the conclusions drawn would strengthen the paper and provide a more comprehensive perspective.\"}"}]}
{"paper_id": "y3dqBDnPay", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the importance of self-supervised representation learning for multimodal data, which involves integrating and embedding information from various sources without additional labeling. This approach is useful in diverse applications such as clustering, text-to-video retrieval, and temporal action localization. Existing methods mostly focus on pseudo-label-based and contrastive-based approaches, often failing to capture the complex high-order relationships inherent in multimodal data such as inter- and intra-modality correlations.\nGap: Existing methods in self-supervised multimodal representation learning often overlook high-order inter- and intra-modality correlations inherent in real-world multimodal data. Additionally, these methods generally lack clear principles for effective multimodal fusion, which can lead to information loss or redundancy.\nHypothesis: HyperRep is proposed as a method to address the shortcomings of existing multimodal representation learning techniques. It combines hypergraph-based modeling to capture high-order inter- and intra-modality correlations and a self-supervised multimodal fusion information bottleneck principle to constrain the solution space, ensuring a more effective fusion of multimodal data.\nMethod: HyperRep employs a hypergraph-based representation to model high-order correlations in multimodal data. The method involves constructing two types of hypergraph structures: an instance hypergraph for inter-modality relationships and a modality hypergraph for intra-modality relationships. It uses a propagation process where information from modality-specific and instance-specific hyperedges is aggregated back to vertices. Furthermore, it implements a self-supervised Multimodal Fusion Information Bottleneck (MFB) principle, which maximizes mutual information shared across modalities while minimizing modality-specific information. The method was tested on public datasets for tasks such as clustering, text-to-video retrieval, and temporal action localization.\nConclusion: The proposed HyperRep method demonstrated superior performance compared to state-of-the-art methods across several metrics on different datasets. The results confirmed that the high-order correlations and multimodal information bottleneck principle are crucial for effective multimodal representation learning. The method achieves effective and efficient embedding of multimodal data, showing potential for broad applications in machine learning.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. This paper provides sufficient and detailed formal definitions and necessary proofs.\\n2. The application of hypergraphs in this paper may inspire the field of multimodal self-supervised learning.\\n3. The authors conducted extensive ablation experiments on multiple datasets.\", \"Weaknesses\": \"1. Motivation: The paper highlights the use of hypergraphs to capture higher-order correlations among modalities. However, I find the paper insufficient in explaining and analyzing why higher-order correlations are essential for multimodal self-supervised learning. This is especially relevant since most existing methods, such as the CLIP[1] series and ImageBind[2], rely on pair-wise multimodal self-supervision.\\n2. Method: The paper could improve the clarity and presentation of MFB. For instance, how does Eq. 12 relate the mutual information of shared and instance features? A clearer explanation of the illustration would be helpful. Also, the MFB module in Figure 2 is vague, and a more specific illustration could enhance the readers\u2019 comprehension of the method.\\n3. Experiment: The paper lacks comparisons with recent (2023) methods, as the only one mentioned has a huge performance gap with the classical methods. Could the paper also compare with more popular Foundation models, such as VideoCLIP[3] and OmniVL[4]? Moreover, Table 1 shows that graph-based methods perform poorly, and Table 2 reveals a drastic performance drop after removing higher-order correlations. However, AGC achieves high performance without using higher-order correlations. Does this imply that higher-order correlations are crucial for the proposed method, but not for the multimodal self-supervised learning task? \\n\\n[1] Learning Transferable Visual Models From Natural Language Supervision. ICML 2021.\\n[2] IMAGEBIND: One Embedding Space To Bind Them All. ICCV 2023.\\n[3] VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding. EMNLP 2021.\\n[4] OmniVL: One Foundation Model for Image-Language and Video-Language Tasks. NeurIPS 2022.\", \"Questions\": \"How come the MIL-NCE performance in Table II differs so much from the original paper? Are there any differences in the experimental settings?\"}"}]}
{"paper_id": "H6pf70GZVU", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Incremental learning (IL) is a method to enable AI systems to learn new tasks over time without catastrophic forgetting. Catastrophic forgetting occurs when a neural network forgets old tasks while learning new ones. Traditional methods address this by storing previous data in a memory buffer, but this is limited by memory size and privacy concerns. Recently, non-exemplar IL methods have emerged that store class centroids as prototypes to generate synthetic data, although these methods often have shortcomings such as proximity of class prototypes and inappropriate data distributions.\nGap: The current non-exemplar IL methods store class centroids and use Gaussian noise for data augmentation, which may lead to overlapping synthetic data and forgetting due to inappropriate representation of the data distribution. There is a need for methods that can maintain distinct, non-overlapping class representations without these limitations.\nHypothesis: Storing and optimizing a single prototype per class can significantly outperform current methods, avoiding the need for synthetic data replay and reducing catastrophic forgetting by ensuring that class prototypes are compact and distant from others in the representation space.\nMethod: The proposed method, YoooP, involves optimizing a single prototype per class using an attentional mean-shift algorithm to move the prototype to a high-density region of its class and minimizing its similarity to other classes. A variant, YoooP+, extends this by synthesizing replay data that preserves the angular distribution between the prototypes and historical data, avoiding Gaussian noise. The model is trained using a combination of prototype optimization, contrastive loss, and knowledge distillation to prevent catastrophic forgetting.\nConclusion: YoooP and its extension YoooP+ significantly outperform existing non-exemplar IL methods in terms of both accuracy and reduced forgetting. YoooP+ effectively stabilizes YoooP by using prototype-based synthetic data that mimics the original data distribution. These results demonstrate a new perspective on optimizing class prototypes for exemplar-free class-incremental learning.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"This paper proposes a novel approach to leave the class representative prototype considering the similarity with all real samples. The proposed method aims to be an efficient technique that can be easily applied to the existing prototype-based method.\\n\\nThis paper also proposes a data augmentation performed with the help of a rotation matrix. This strategy approximates real distribution better than conventional prototype augmentation techniques.\\n\\nIn base-0 experiments on CIFAR-100 and Tiny-ImageNet, the proposed methods achieve the best performance.\", \"Weaknesses\": \"-\\tThe scale of dataset (CIFAR-100, TinyImageNet) is small. Whether the proposed method works with bigger datasets is an important issue that needs to be addressed. \\n-\\tThe paper lacks the result with 20 phases which is a quite common configuration. \\n-\\tThe effect of an optimized prototype based on the attentional mean-shift method seems limited in many situations.\", \"Questions\": \"1) For verifying the strength of proposed augmentation, why don\u2019t you apply the new strategy to PASS instead of adding Gaussian noise to prototype? Would you show the performance of it?\\n\\n2) Could you show the classification confusion matrix result of your approach?\\n\\n3) In section 3, when define the probabilities over all classes C_(1:t ), isn\u2019t softmax(wG(F(x;\u03b8))) more accurate notation than softmax(wF(x;\u03b8))?\\n\\n4) The function c(,) in equation 2, equation 4 seems to be different. Is the c(,) in equation 2 cosine similarity function and one in equation 4 classifier (=G(F(p_k;\u03b8)))?\\n\\n5) How can we define the initial class-wise prototype p_k? Is it the same with the prototype used in PASS? (k-th class mean prototype)?\"}"}]}
{"paper_id": "7VPTUWkiDQ", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper addresses the challenge of learning representations that can generalize to novel compositions of known concepts, which is crucial for improving machine perception to more closely resemble human perception. Object-centric representation learning has been proposed as a way to enable this type of compositional generalization, where each object in an image is represented by a distinct subset of latent code. However, a principled understanding of when and how object-centric representations can indeed generalize compositionally has been lacking.\nGap: There is a lack of theoretical and empirical understanding of when object-centric representations can be guaranteed to enable compositional generalization. There is also uncertainty regarding the advancements needed in object-centric learning to promote such generalization, and the obstacles that remain for achieving this goal.\nHypothesis: The hypothesis is that autoencoders that satisfy specific structural assumptions on the decoder and enforce encoder-decoder consistency will learn object-centric representations that provably generalize compositionally. This hypothesis is rooted in the development of a theoretical framework using identifiability theory that shows under what conditions compositional generalization is possible.\nMethod: The paper utilizes identifiability theory to explore when compositional generalization is possible for object-centric representations. The authors propose that an additive decoder structure in autoencoders, combined with a compositional consistency regularizer for the encoder, ensures compositional generalization. They define conditions such as additivity, where latents are decoded as the sum of individual slot-wise decodings, and compositional consistency, where the encoder must invert the decoder not only in-distribution but also out-of-distribution (OOD). The theoretical framework is validated through experiments on synthetic image data to demonstrate the practical relevance of their assumptions.\nConclusion: The main conclusion is that it is theoretically and empirically possible for autoencoders with certain structured decoders and regularized encoders to achieve compositional generalization. This is significant for advancing machine perception closer to human capabilities. The work lays the foundation for further theoretical and empirical explorations into achieving robust compositional generalization in object-centric learning.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"- The paper is very well-written.\\n- The theory is sound and significant for the community.\\n- The joint encoder-decoder framework for compositional generalization in autoencoders is quite elegant.\\n- The limitations of the framework and the additivity constraint on the decoder are adequately stated.\", \"Weaknesses\": \"Although they support the theory, the experiments are quite limited. For instance, these are all with only two slots with 16 dimensions each. See the questions section for additional information that would be interesting to see from experimentation.\", \"Questions\": \"- How does the effect of the consistency loss scale with the number of slots?\\n- What is the impact of how slot-supported the training data is? i.e. in Figure 2 (1), what is the impact of the width of the blue band on empirical effectivity?\\n- How does the method hold up on non-synthetic data, especially if you slightly relax some constraints? For instance, what if you have expressive slot-wise decoding, but allow for a low-expressivity non-linear combination at the end for rendering?\"}"}]}
{"paper_id": "5JWAOLBxwp", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: 3D vision tasks, such as shape reconstruction, typically require inputs to be in a fixed canonical rotation, which limits their usage. Equivariant neural networks developed for 3D input, like Vector Neurons (VN), are capable of handling rotations but are limited by their reliance on three-dimensional feature representations, which are insufficient for capturing detailed 3D data.\nGap: Current SO(3)-equivariant networks, like Vector Neuron, struggle with capturing high-frequency details in 3D data due to their limited three-dimensional feature representation. The problem is that these networks cannot adequately map a 3D point to a high-dimensional feature space that acknowledges various frequencies present in 3D shapes.\nHypothesis: Introducing a new equivariant feature representation that maps 3D points into a high-dimensional feature space can improve the performance of equivariant networks like Vector Neurons by capturing multiple frequency details in 3D data.\nMethod: The authors propose a frequency-based equivariant feature representation (FER) that maps a 3D point to a higher-dimensional space. This involves constructing a mapping D from SO(3) to SO(n) such that the feature representation \u03c8 consists of sinusoids. The algorithm ensures rotation equivariance by representing orientation changes in a high-dimensional space, and is implemented in standard point processing networks like PointNet and DGCNN. The method involves proving mathematical conditions for D and integrating the representation into existing VN-based networks. Experiments cover shape classification, segmentation, normal estimation, point completion, and shape compression using datasets like ShapeNet and ModelNet40.\nConclusion: The proposed FER method enhances the ability of VN and VN-based networks to capture fine details by enabling them to process features with multiple frequencies. It demonstrates higher performance than traditional methods in various 3D vision tasks, addressing the limitations of prior SO(3)-equivariant models. Empirical results show that FER-equipped models outperform their counterparts in capturing high-frequency details, notably improving tasks like shape reconstruction and compression.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 2, \"Strengths\": \"*Originality:* The proposed approach appears novel. Although other approaches have used a Fourier like representation to represent 3D objects [1,2,3], this approach differs from those in that 1) the representation is intentionally high-dimensional, 2) only a subspace of the high-dimensional space is considered.\\n\\n*Quality:* In the experiments, proposed method performs on-par with or better than most baselines.\\n\\n*Clarity:* (see weakness)\\n\\n*Significance:* Designing expressive equivariant representations is a challenging and important problem in the field.\\n\\n\\n[1] Esteves, Carlos, et al. \\\"Learning so (3) equivariant representations with spherical cnns.\\\" Proceedings of the European Conference on Computer Vision (ECCV). 2018.\\n\\n[2] Cohen, Taco S., et al. \\\"Spherical cnns.\\\" arXiv preprint arXiv:1801.10130 (2018).\\n\\n[3] Kondor, Risi, Zhen Lin, and Shubhendu Trivedi. \\\"Clebsch\u2013gordan nets: a fully fourier space spherical convolutional neural network.\\\" Advances in Neural Information Processing Systems 31 (2018).\", \"Weaknesses\": \"*Quality:* \\n* The authors present a very critical view of methods that use spherical harmonics which I think is not only inappropriate but also incorrect. Spherical harmonics can be seen analogous to the Fourier basis which have been applied in signal processing problems for centuries, and the authors rely on for intuition building. Also, the authors state that spherical harmonics come from quantum mechanics, but the use of spherical harmonics predates the \\\"discovery\\\" of quantum mechanics by more than a century. \\n* There does not appear to be any discussion of the impact of choosing a higher dimensional input representation space. How does the choice of $n$ impact performance? What is the additional computational cost?\\n* The notation is inconsistent (see questions)\\n* Some proofs are missing (see questions)\\n\\n*Clarity:* I found the presentation to be difficult to follow. The paper uses advanced math concepts such as the Lie bracket without motivating or even naming them.\", \"Questions\": \"*Questions:*\\n- What is meant by \u201c$R^z(\\\\hat{u})$ is a rotation matrix defining the orientation measured from the $z$-axis to $\\\\hat{u}$.\u201d Is it the 2D rotation about the axis $z \\\\times \\\\hat{u}$ that aligns the $z$-axis and $\\\\hat{u}$?\\n- It seems like the map is from SO(3) to a subspace of SO(n) is that right\\n- Is the axis arbitrarily selected?\\n- Createsearchspace from Algo 2 is not described in the text, how does this work?\\n\\n*Possible typos:*\\n- \u201cTheorem 1. theoremIf\u201d \u2192 \u201cTheorem 1. If\u201d \\n- Are $\\\\hat{u}$ and $\\\\overrightarrow{u}$ used interchangeably in section 3? It would be clearer if the notation were consistent\\n- \u201cintuitively, just like F1, F2 and F3 represent angles\u201d \u2192 \u201cintuitively, just like F1, F2 and F3 represent axes of rotation\u201d\\n- Should \u201caxes \u03c8(\u02c6x), \u03c8(\u02c6y), and \u03c8(\u02c6z)\u201d be \u201caxes \u03c8(F_1), \u03c8(F_2), and \u03c8(F_3)\u201d? If not what does $\\\\hat{.}$ mean here?\\n- \u201c effective-yet-rotation-equivariant\u201d \u2192  expressive yet rotation equivariant\\n- In section D, readers are referred to the proof of Thm 1 for the proof of Prop 1, but the proof of Thm 1 does not prove Prop 1.\"}"}]}
{"paper_id": "l9GaXJnMJ8", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper addresses the challenge of approximating large-scale stochastic processes represented by Markov systems. Various methods for approximating these processes have been developed over the years, including scenario tree generation and the use of probability metrics like the Wasserstein distance. The Wasserstein distance has applications in fields such as GANs, clustering, and optimal transport. Previous work has focused on approximating distributions or mixtures rather than kernels, and computational methods such as the Sinkhorn algorithm have been used for efficient calculation of transportation distances.\nGap: Current methods for approximating the kernels of Markov systems are computationally expensive, especially when nonlinear transition risk mappings are involved. There is a need for efficient approximation methods to replace complex kernels with simpler, finite-support counterparts, especially in dynamic programming applications like risk evaluations and optimal stopping problems.\nHypothesis: The authors propose that a generalized metric based on the Wasserstein distance can effectively approximate Markov system kernels. This metric can be used as the basis for a computational technique that simplifies the kernels into a discrete, easier-to-handle form without significant loss of accuracy, thereby facilitating the analysis and manipulation of Markov systems.\nMethod: The authors introduce a novel kernel distance based on the Wasserstein metric and propose a particle selection method to approximate Markov system kernels. They develop a subgradient algorithm that is decomposable and parallelizable, allowing for efficient computation without matrix operations. The algorithm recursively approximates the forward system using the kernel distance, selecting representative particles to form a simplified system. This approach is validated through numerical experiments, including an optimal stopping problem.\nConclusion: The proposed method provides an effective means of approximating complex Markov systems using a kernel distance metric. The subgradient algorithm offers a computational advantage over traditional methods, handling large-scale problems more efficiently. The approach is validated through comparisons with the binomial tree method, showing its utility in diverse applications. The paper highlights potential applications in reinforcement learning and dynamic risk assessment, suggesting further exploration of these areas in future work.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"1. The problem in this paper is interesting.\", \"Weaknesses\": \"1. The paper is generally poorly written and contains numerous typos in both the explanations and equations.\\n2. The experimental section could benefit from additional elaboration and expansion.\", \"Questions\": \"1. The definition 2.1 is very confusing. More specifically, it explains $Q(\\\\dot | x)$ is a kernel that transforms $x$ to the probability measure. It seems $Q(\\\\dot | x)$ is a probability density function estimated and it does not look like a kernel functions. Could you please elaborate this part?\\n2. Equation (9) presents a variation of the Wasserstein barycenter problem. However, the manuscript lacks a comprehensive discussion of the Wasserstein barycenter, and it would be beneficial to explicitly address this concept. Additionally, a comparison of the proposed method with relevant baselines is warranted.\\n3. Numerous typographical errors are present in the manuscript. For instance, on page 1, there is a repeated \\\"the the.\\\" Equation (6) is lacking the variable $p,\\\" and these issues need prompt correction for the manuscript to maintain clarity and accuracy.\"}"}]}
{"paper_id": "i4kDKfllrz", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Deep learners often perform well under the closed set assumption, where classes are predefined during training. However, they struggle in open set conditions, where models must recognize both known trained classes and potentially infinite unknown classes. Open Set Recognition (OSR) is developed to address this challenge, allowing models to identify whether an input comes from the known classes or an unknown class from the broader set.\nGap: Existing open set recognition methods use a single function for distinguishing between known and unknown classes as well as for classifying within known classes. This dual-purpose approach leads to suboptimal performance because the function is not specialized for either task.\nHypothesis: The authors propose that embedding spaces should distinctly separate known and unknown classes, with specialized functions for each task: known/unknown discrimination and fine known class distinction. By using distinct functions, each can be optimized for their specific task, leading to improved performance.\nMethod: The authors introduce Synergistic Classification and unknown Discrimination (SCAD). SCAD deploys two separate networks: one for embedding data into spaces where known and unknown classes are distinct, and another for classifying among known classes. The embedding network uses a triplet loss to enforce separation, using a representative set of 'known unknowns.' A prototype-based distance measure helps decide if a sample is from a known or unknown class. A classifier then provides specific labels for known samples. Performance is measured using AUROC and correct classification rate across several datasets.\nConclusion: SCAD outperforms existing methods in open set recognition by using distinct functions for known/unknown discrimination and known class classification. It effectively separates knowns and unknowns in the embedding space and shows superior performance in both AUROC and classification accuracy, handling both known/unknown distinction and classification tasks more efficiently.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"The task of open set recognition is of interest to the field.\", \"Weaknesses\": \"1. The presentation of the paper can be confusing sometime. E.g. in Fig 2., please explain in the caption the 'know knowns' and 'known unknowns', which are very confusing. This makes understand the core idea inaccessible.\\n2. Large-scale experiments like original Imagenet and iNaturalist is needed. Current experiments are too small-scale to verify the proposal.\\n3. some ablation studies are missing. E.g. \\\\beta in the Eqn. 5.\", \"Questions\": \"1. Fig.1b hypothesize that the embedding space should be distinctly separable between the known classes and unknown classes. However, in Fig 4, I did not see distinc separation between the two. I also do not think the hypothesis holds in general. Can authors expalin.\"}"}]}
{"paper_id": "ISrxxvXJQO", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: In recent years, deep learning has become a dominant force in computer vision, excelling in tasks like image recognition, object detection, segmentation, and image generation. While these achievements are impressive, the community has not fully explored shared mathematical properties underlying images. This research seeks to understand images through mathematical invariance, offering insights into their representation.\nGap: The fundamental mathematical properties shared by images remain largely unexplored. Current methods have not adequately addressed this question, focusing more on performance metrics rather than underlying principles.\nHypothesis: The authors propose that images can be effectively approximated using a set of hidden one-way wave equations, referred to as the 'hidden waves phenomenon.' These equations have latent, learnable speeds that are shared across images, introducing a new mathematical perspective for understanding image data.\nMethod: The method involves leveraging a visual encoder, such as a convolutional neural network, to compute unique initial conditions for images, which correspond to solutions of one-way wave equations. The solutions manifest as collections of special solutions that demonstrate first-order autoregressive properties with shared coefficient matrices. The authors implemented this methodology using ImageNet data, achieving notable reconstruction performance by relaxing local constraints and applying linear autoregression models, termed FINOLA, to approximate initial conditions.\nConclusion: The study reveals a fundamental mathematical property where images can be represented within a compact space defined by one-way wave equations with hidden, learnable speeds. This contributes a novel understanding of shared invariances in image data, facilitating reconstruction with minimal loss of fidelity. The hidden waves phenomenon illuminates invariant properties across diverse images, potentially impacting future image analysis and representation techniques.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"- Through mathematical enhancements and experiments, they develop a deep understanding of FINOLA, which is a general wave representation of an image.\\n- The section comparing \\\"hidden wave representation\\\" with image autoregression or transformations is well-written and provides clear insights into this encoding.\\n- They conducted numerous experiments to validate the improvements they implemented.\", \"Weaknesses\": \"- Experimental design\\n\\nDrawing conclusions from the current results is uncertain due to the limited number of data points and the lack of significant differences. For instance, the interpretation of Table 3 remains unclear. Should we consider choosing initial condition positions different from the center? Overall, experiments were conducted from various perspectives, they do not yield meaningful insights or conclusions.\\n\\n- The phenomenon is undoubtedly intriguing and interesting, it remains unclear how this method can be applied to future research or practical applications. The paper would be improved by including discussions on potential future perspectives.\", \"Questions\": \"- Were all images resized to the same aspect ratio?  The aspect ratio can impact wave speed, and it may be more natural to preserve the aspect ratio from the original images.\\n- Conventional image compression methods like DCT transform the image into a linear composition of orthogonal basis. This is simple and provides a straightforward understanding of each coefficient. The paper effectively describes the differences, but what are the main strengths and weaknesses of this method?\\n- Is the selection of hyperparameters for training this method not very sensitive? How long does the training process take?\\n\\nMinor comments:\\n- Use ` for opening quotation mark in latex. Currently ' ... '.\\n- There are some missing brackets for citations. For example, ... known as FINOLA Chen et al. (2023).\"}"}]}
{"paper_id": "vsW5vJqBuv", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Empowering embodied AI, such as robots, has become crucial as these systems transition from solving specific, constrained problems to handling a variety of tasks in the physical world, such as household chores and healthcare. Traditional methods often struggle with the inherent diversity and complexity of open-ended task goals that are multifaceted and dynamic.\nGap: Current AI techniques are limited in their ability to handle open-ended tasks due to the traditional goal-conditioned frameworks that expect specific end-states. This restriction impedes the flexibility required for robots to adapt to novel tasks not encountered during training, which is necessary for more realistic and practical applications.\nHypothesis: The authors propose that combining diffusion models with training-free guidance can enhance the adaptability and competency of embodied AI systems in solving open-ended tasks. The diffusion models can generate various potential future states without the constraints of pre-defined goals, allowing for more flexible planning and control.\nMethod: The proposed method, DOG (Diffusion for Open-ended Goals), uses diffusion models to understand world dynamics from offline data without goal-conditioning and performs energy minimization for planning during inference. The method involves training with Markov decision frameworks and using classifier-guided diffusion models for conditional score functions. The approach was tested on diverse tasks such as maze navigation, robot movement control, and robotic arm manipulation, demonstrating its ability to handle unscripted objectives dynamically.\nConclusion: DOG significantly advances the flexibility and competence of embodied AI, enabling agents to plan and execute actions for open-ended task goals not defined during training. The framework allows for zero-shot transfer to new tasks and shows robust performance in handling various types of goals, representing a potential leap in achieving more human-like intelligence in embodied systems.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 2, \"Strengths\": \"Using diffusion models to perform Goal-Conditioned Reinforcement Learning (GCRL) looks like a good idea, although the authors are not the first to put it forward, and are not clear about the novelty they bring.\", \"Weaknesses\": \"- The paper is very unclear about the \\\"training-free\\\" property of the framework, which seems to be the main contribution with respect to Janner et al. (2022) and Liang et al. (2023).\\n\\n- The paper is unclear about the difference between defining an energy function by hand and defining a goal-related reward function by hand.\\n\\n- The experiments about using language-based prompts are not explained at all\\n\\n- The paper is written poorly and often in a non-scientific way with a lot of unnecessary over-statements (see below).\", \"Questions\": \"- could the authors explain the difference between defining energy functions for partial goals, negative goals, sequence goals, hybrid goals as they do in the bottom of page 3 and defining goal-related reward functions that do the same, as often done in GCRL? If they argue that this makes a difference, can they backup this claim with comparative experiments?\\n\\n- \\\"This way of defining a goal is fundamentally different from the conventional way of treating the goal as a variable\\\" -> could the authors explain why it is \\\"fundamentally different\\\"? You could have a goal represented with \\\"Clean = True\\\", this would be using a boolean variable, or \\\"clean level = X\\\", that would be using a scalar variable. Doesn't your energy function (c or g?) compute something similar?\\n\\n- p6: the list of examples should be reordered to match the order of the figure (or vice versa). Besides, the corresponding energy functions should be given. The examples themselves need to be better explained.\\n\\nFor instance \\\"our objective isn\u2019t state-specific but aims at maximizing or minimizing some certain property,\\\" this is a completely unspecific sentence, and an over-statement. Can your system do whatever you ask it to do? What are the limits?\\n\\n- in the related work, the authors connect training-free guidance to (Yu et al., 2023b): \\\"and training-free guidance (Yu et al., 2023b)\\\" But the paragraph where they speak of training-free guidance (p2) does not even mention (Yu et al., 2023b). This point must be explained more clearly, as it seems to be the central contribution of the paper.\\n\\n-could the authors extend the related work and better explain the difference betwen their work and Janner et al. (2022) and Liang et al. (2023)? Putting it before the experimental study would help, in my opinion.\\n\\n- Table 1: why is the authors' method performing worse than competitors in the medium dataset?\\n\\n- could the authors explain how they transformed language-based prompts into a goal-related energy function?\\n\\n\\n- Appendix F, HER is not an executor function, it is a method to leverage failed trajectories in GCRL. I don't understand the authors' description at all, we clearly do not see HER the same way...\\n\\n\\n## Over-statements:\\n\\n- p2: \\\"This work is a trial to approach the ultimate embodied AI that can assist people with diverse tasks such as healthcare, driving, and housework, though further endeavor is, of course, still largely demanded.\\\" So, is this work a successful trial? Clearly, it does not approach this ultimate embodied AI it describes. This is a huge over-statement, and the authors should describe in the introduction the goals they truly manage to achieve.\\n\\n- p5: \\\"By doing so, it expedites the sampling process and ensures adherence to defined objectives, proficiently fulfilling our requirements and presenting a viable resolution to our challenges\\\" -> only politicians write this way! :) Be concrete about the positive sides of your method, and do not be afraid of listing limitations, you are doing science, not selling something\\n\\n- p6: \\\"Fig. 4h exemplifies our algorithm\u2019s prowess\\\" -> sorry, I don't consider this as a prowess.\\n\\n- p7: This holds considerable significance for offline RL: what makes you believe this is so? Without properly backing up this claim with results, I see this as an over-statement.\\n\\n- \\\"Our work introduces a novel way of solving open-ended embodied tasks, which may shed light on cognitive neuroscientific research on understanding human intelligence (Taylor et al., 2016).\\\" -> This is a huge over-statement. Can the authors explain in which way they believe their work can concretely shed light on cognitive neuroscientific research on understanding human intelligence?\\n\\n\\n## Local issues and typos ##\\n\\n- p2: These works are worthy of recognition on the path to embodied AI that can truly tackle open-ended tasks.\\nDo the authors use \\\"task\\\" and \\\"goal\\\" interchangeably?\\n\\n- p2: from somewhere.). -> from somewhere).\\n\\n- Bayesian formula -> Bayes formula\\n\\n- p3, in (1) the energy function is written g, but in the examples below it is written c\\n\\n\\n- \\\"Generative models such as diffusion models for image generation can \u201cdraw\u201d the pixels with any color, whereas a real-world painter can only use the colors available in the palette\\\". -> This makes no sense. How can a generative model draw an image with pixels outside the computer's palette?\\n\\n- \\\"This is different from traditional machine learning tasks, where the focus is on finding a single optimal output for a given input.\\\" -> By traditional, do you mean supervised learning? Reinforcement learning is traditional (rooted in Bellman, 1957) but it corresponds to what you describe in the previous sentence.\\n\\n- \\\"Oftentimes we care about multiple steps rather than a single state, e.g, asking a robot to move in circles. Therefore, tackling\\nopen-ended task goals in embodied AI becomes challenging.\\\" -> this is very poorly written. Ask yourself what is your point and write what your point is, avoid being only allusive.\\n\\n- p3: dimension (Here -> here\\n\\n- discrete actions.) -> actions).\\n\\n- you don't need a \\\"nanny\\\" robot to clean or tidy a house, a anny is to take care about children.\\n\\n- p4: the task is achieve -> is to achieve\\n\\n- p5: concatate -> concatenate\\n\\n- p5: plan executing -> execution\\n\\n- p5: Several optional can be made for the plan executor -> there are several options...\\nBy the way, these options should be described in the main text\\n\\n- p5: Since our executor to multiple-step actions, -> something is missing\\n\\n- p5: to the Appendix: which one?\\n\\n- p5: the generating policy consistent with D4RL: what do you mean? I don't understand.\\n\\n- p6: we present 100 episodes both from the same start point (marked as green star). -> why \\\"both\\\"? It makes no sense.\\n\\n- p7: the the mean\\n\\n- p8: which including sub steps: -> includes\\n\\n- Appendix B, try to fit Table 2 into Appendix B.2\\n\\n- Appendix C, about Mujoco environment: \\\"using a specific algorithm and acculumating 1M samples\\\" -> which  specific algorithm do you mean?\\n\\n- Appendix F: \\n\\n\\\"by , takes\\\" -> something is missing\\n\\nwe adopt the (Ma et al. (2022) as ...\\\" -> something is missing\"}"}]}
{"paper_id": "Qufas3h0mY", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Training large neural networks involves challenges such as backward locking, which results from sequential dependency where the chain rule restricts each GPU's layer to await gradient information from other GPUs. This bottleneck limits the potential speedup from parallel processing and hinders efficient training of deep neural networks.\nGap: Current methods like backpropagation (BP) are inefficient for deep neural networks due to backward locking, preventing effective model parallelism. This sequential nature of BP's gradient flow creates significant delays and inefficiencies when deploying neural networks across multiple GPUs.\nHypothesis: The authors propose Supervised Contrastive Parallel Learning (SCPL) as a solution to enable model parallelism, which can decouple the end-to-end backpropagation into multiple shorter flows, allowing simultaneous gradient computations across different layers and thereby overcoming the backward locking issue while enhancing training efficiency.\nMethod: SCPL leverages supervised contrastive learning to decouple the gradient flow in deep neural networks. It assigns local objectives to network components, allowing parameter gradients to be computed separately on different GPUs. The method was tested on various datasets like CIFAR-10, CIFAR-100, Tiny-ImageNet, IMDB, and AG\u2019s news, using architectures such as ConvNet, VGG, ResNet, LSTM, and Transformer. The approach enables pipeline-like parallelism and, hence, model parallelism, potentially speeding up training and maintaining high accuracy.\nConclusion: SCPL significantly reduces training time while maintaining or even increasing accuracy compared to traditional BP, Early Exit, and Associated Learning methods. It addresses long gradient flow issues in deep networks, presenting a flexible alternative to state-of-the-art methods without needing additional architectural changes.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 2, \"Strengths\": \"1. The paper provides a thorough explanation, making the proposed method reproducible.\\n2. Excellent visualization techniques are employed to illustrate the algorithms, facilitating understanding.\\n3. Merging local gradient computations with self-supervised learning stands out as a novel approach in this domain.\", \"Weaknesses\": \"1. The description surrounding gradient length appears overly assertive. It might be more accurate to present it as $O(H/N)$, relative to the total layer count $H$ and the number of local losses $N$ employed. This observation can be further corroborated by the authors' implementation details.\\n2. The paper does not delve into the influence of $N$\u2014specifically, the count of local losses used and their respective placements\u2014on the system's overall performance. Investigating this could provide more comprehensive insights.\\n3. Although the paper introduces a synchronized pipeline parallel algorithm, it seems the \\\"bubble\\\" issue persists in the suggested method. For a rounded perspective, it would be beneficial if comparisons concerning accuracy with asynchronous pipeline parallel algorithms (like PipeDream) were provided. This would further highlight SCPL's potential advantages over asynchronous counterparts.\", \"Questions\": \"1. Is there a mistake in in paragraph 1 in Section 2.2, which should be 'the author of AL' ?\\n2. Also see weaknesses part.\"}"}]}
{"paper_id": "PhMrGCMIRL", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The development of AI models that generalize across tasks and domains has been a longstanding challenge in AI research. Traditional models trained using empirical risk minimization often face performance degradation when the test data distribution differs from the training data. Foundation Models have enabled the creation of expert models through fine-tuning, but single experts still struggle with generalization across diverse data distributions. Combining experts with complementary expertise to enhance generalization is a common practice in machine learning, but existing methods often assume that experts are trained and tested on the same distribution.\nGap: The paper identifies a gap in existing methods for combining expert models, specifically when these experts have complementary expertise across different data domains. Traditional ensembling and Mixture of Experts models often assume experts are trained and tested on similar data distributions, which can lead to poor performance when generalizing to out-of-distribution data where none of the experts perform well individually.\nHypothesis: The authors propose that fusing pre-trained expert models with complementary expertise into a new 'Fusion of Experts' (FoE) model can improve performance on data distributions that contain a mix of the domains for which the experts were originally trained. This approach is expected to perform better than using individual experts alone or traditional ensembling techniques.\nMethod: The Fusion of Experts (FoE) approach reformulates the problem of combining expert outputs into a supervised learning task. The method is applicable to both discriminative and generative tasks. It involves training a 'fuser' model that takes in the outputs from K expert models, each trained on different data domains, and learns to predict the best output for a given input. The approach is extended to a 'frugal' setting using a sequential algorithm to minimize the number of individual expert calls during inference by solving a graph-based shortest-path problem. Experiments were conducted on tasks such as image and text classification, text summarization, and question answering.\nConclusion: The FoE method demonstrated significant improvements over individual experts, confidence-based fusion, and traditional ensemble methods across various tasks, achieving performance close to an oracle model which always selects the best expert. The FrugalFoE variant effectively reduces the computational cost by minimizing expert calls without sacrificing accuracy, especially in settings where FoE naturally exhibits frugality by relying on a subset of experts. The proposed method shows promise for enhancing model generalization across mixed-domain data distributions.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"- The paper tackles a novel and significant issue: optimally leveraging different models for diverse tasks. While foundational models generally perform well across various tasks, they have differing strengths. Thus, an approach to effectively combine these models represents a significant advancement.\\n- The methodology, FrugalFoE, is both technically sound and innovative. It offers a clear problem formulation and further minimizes the need to query all the expert models without sacrificing accuracy.\\n- The experiments are extensive and cover a range of applications, including image classification, summarization, MMLU, and text evaluation. This demonstrates the method\u2019s versatility and effectiveness.\\n- The paper effectively situates its work within the broader context of ensemble learning, mixture of experts, and federated learning, thoughtfully introducing these methodologies and their limitations.\", \"Weaknesses\": \"- A primary limitation is the assumption that data for training the fuser are readily available. The proposed approach requires a labeled dataset to train the fuser network. In the discriminative case, we feed the input example to different individual models, take model outputs as the inputs to the fuser network, and train the fuser to predict the label of the input example. Similarly, in the generative case,  we feed the fuser network with individual model outputs and train the fuser to predict the best model index, which is obtained by feeding the labeled data to individual models and selecting the model that achieves the best performance. This prerequisite may not be realistic in practical scenarios, where there is no or only a few labeled data. If a large labeled dataset is available, it might be more efficient to fine-tune a foundational model or employ few-shot learning. A pivotal aspect of this research should be the generalization capabilities of the trained fuser across different tasks (e.g,. train the fuser on some tasks and test the fuser on unseen tasks), which would significantly enhance the paper's contribution.\\n- The paper lacks an in-depth analysis of the fuser network. Although it is described as a lightweight MLP network, there's no exploration of how different architectures (e.g., simpler networks like linear models or more complex ones like transformers) might impact performance. Additionally, details on the training configurations, such as learning rate, epochs, dataset, and stopping criteria, are missing. The influence of the $K$ parameter in the $K$-NN component of the fuser network is also unclear. What is the rationale for choosing $K=9$?\\n- The experiment sections lack in-depth descriptions (see questions). The paper would benefit from reallocating less critical sections (like the connection between FrugalFoE and the A* algorithm) to the Appendix and expanding on the experimental details.\", \"Questions\": \"- Page 8, Table 2: Could you clarify the distinction between \\\"TFN Expert Only\\\" and \\\"TFN Expert,\\\" and between \\\"Poem Expert\\\" and \\\"Poem Expert Only\\\"?\\n- Page 8, Table 2: What are the results for the \\\"confidence-based fusion\\\" and \\\"ensemble\\\" baselines? The same question applies to Tables 3 and 4. The confidence-based fusion seems less intuitive in the generative case, how about simply selecting the maximum confidence at each decoding step?\\n- Page 9, Table 4: Could you explain more details of FoE (Expert 1), FoE (Expert 2), and FoE (Expert 3)? Why does adding more experts appear to degrade performance?\\n- Page 4: The statement \\\"As long as there is a label shift among the domains, we expect $E[f(X_k)] = E[Y_k]$\\\" needs clarification. Why is this expected?\\n- Page 6: The phrase \\\"Then $\\\\lambda$ can be interpreted as the final error rate reduction we want to achieve\\\" \u2013 could you expound on the reasoning behind this?\\n- Page 1: The statement \\\"our emphasis is on generalization to test data distributions where none of the experts perform well individually\\\". Shouldn't it be \\\"a few\\\" instead of \\\"none\\\" based on the problem formulation?\\n- Page 6: The condition \\\"If ... <0 we terminate the search\\\" should be\\\">0\\\"?\"}"}]}
{"paper_id": "N2WchST43h", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Adversarial training is a strategy to build deep neural networks resistant to adversarial perturbations, where an adversary perturbs input data to maximize prediction loss and the learner updates the network to minimize this loss. Recent research focuses on analyzing the convergence of adversarial training using the neural tangent kernel (NTK) theory, particularly for large over-parameterized networks. However, adversarial training is computationally expensive, with time complexity traditional methods being \u2126(mnd) per iteration, due to the requirement to compute forward and backward passes for a network of width m, n input data of d dimensions.\nGap: The existing methods for adversarial training of neural networks are computationally intensive, with time complexity tied to the network's width, the number of samples, and the dimensionality of the input. The dominant cost per iteration is due to the activation of a large number of neurons, resulting in inefficiency and high computational expense, especially for large over-parameterized networks.\nHypothesis: The authors hypothesize that adversarial training can be made more efficient by leveraging the fact that only a sublinear number of neurons are activated in each network iteration. By using a shifted ReLU activation function and a novel data structure for neuron activation reporting, the time complexity per iteration can be reduced significantly to o(mnd).\nMethod: The authors analyze a two-layer neural network using a shifted ReLU activation function, which determines that fewer than o(m) neurons are activated per input data. They develop a high-dimensional search data structure (based on half-space reporting) to identify activated neurons efficiently. Through constructing a pseudo-network, they approximate the original neural network function and implement a gradient descent-based training algorithm (Algorithm 1) using this efficient neuron activation identification approach. Their method theoretically ensures convergence and significantly reduces the time complexity to (cid:101)O(m1\u2212\u0398(1/d)nd) per iteration.\nConclusion: The paper concludes that utilizing a shifted ReLU activation function and high-dimensional search data structures can successfully reduce the computational complexity of adversarial training. This advancement enables training of robust neural networks more efficiently, making it possible to perform adversarial training on large-scale networks with lower computational cost without compromising on convergence or robustness.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 3, \"Strengths\": \"The approach is novel where adversarial training that leverages geometric data structures at the expense of extra space. It extends the direction of first identifying the active neurons and then only taking dot product with those neurons. \\n\\nThe paper is well written and however, without empirical results or illustrative examples, the clarity of the paper's practical implications could be enhanced.  \\n\\nThe work is significant as it has a potential to further improve the direction of efficient adversarial training of robust neural networks.\\n\\nI have not conducted an exhaustive review of the convergence proofs; however, upon initial examination, they appear to be correct.\", \"Weaknesses\": \"The study lacks experiments, even on toy datasets, to determine whether its findings are applicable in practical scenarios.\\n\\nFurthermore, it has not been compared with existing works. Although it is primarily theoretical, clarity on how it measures up against established methods is missing.\\n\\nThe paper does not discuss how reducing training time might impact the robustness performance of the model. This trade-off between efficiency and robustness is crucial for practical use cases.\\n\\nLikewise, the trade-off between memory usage and efficiency warrants discussion. While the paper could comment on memory requirements, it may be that for only two-layer networks, the difference is minimal. However, it would still be beneficial to understand this aspect, perhaps by employing very wide neural networks to confirm it in a practical scenario.\\n\\nIt is also unclear whether the proposed methods can be effectively translated into actual GPU computations. If the approach results in underutilized GPUs, it might still be valuable but may not reduce the overall wall time. Some commentary on this feasibility would be constructive.\", \"Questions\": \"To translate the theoretical advantages into practical scenarios, the paper should present at least some toy experiments.\\n\\nIt is important to explore how reducing training time affects the model's robustness performance, as the trade-off between efficiency and robustness is crucial for practical use cases.\\n\\nThe same consideration should be given to the trade-off between memory usage and efficiency. It would be beneficial for the paper to comment on memory requirements. While the impact on only a two-layer network might be minor, understanding this trade-off is essential, and could be tested using neural networks with extremely high widths in practical scenarios.\\n\\nIn short, a toy experiment featuring a two-layer network that showcases practical, minimal use cases by comparing memory usage, computational efficiency, and robustness performance would be a valuable addition to the work.\"}"}]}
{"paper_id": "LxruQOI93v", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Neural networks are believed to fit training sets with at least as many samples as they have parameters, a concept linked to overparameterized and underparameterized models. Prior studies and theories, like approximation theories and generalization theories, provide insights into the capacity and generalization abilities of neural networks, often discussing their parameter count, expressiveness, and capacity to fit data.\nGap: While theoretical perspectives suggest that overparameterized models can fit large datasets, practical observations reveal limitations often imposed by the training process, optimizer, and architecture. The exact parameterization impacts the loss surface and limits the solutions accessible via common training procedures, indicating a gap between theoretical capacities and practical outcomes.\nHypothesis: The paper hypothesizes that, contrary to the general belief that neural networks can fit as many samples as they have parameters, practical constraints limit their ability to fit data due to training methods, optimizers, and architectural choices, leading to findings that models achieve fit with significantly fewer samples than parameters.\nMethod: The authors introduce a metric called Empirical Data Capacity (EDC) to quantify how many samples a model can fit in practice. They examine the effects of various model architectures (MLPs, CNNs, ViTs), optimizers (SGD, Adam, etc.), and datasets (MNIST, CIFAR, ImageNet) on EDC. The paper describes experiments varying architecture parameters (like depth and width), using randomly and semantically labeled data, and employing different regularization and compression techniques to observe their impact on EDC.\nConclusion: The study concludes that common training setups typically find suboptimal minima, leading to results where neural networks fit far fewer samples than they have parameters. CNNs demonstrate more parameter efficiency than MLPs and ViTs, even on randomly labeled data, indicating their architectural influence. The results suggest that optimization strategies and regularizers impact data fitting capacity, and that architecture choices, like those employing ReLU, can improve capacity beyond original intentions. The work highlights the inefficiency in parameter usage and the potential for improvement through smarter architectural and training strategies.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 1, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"- The paper is well-written overall and easy to follow. The breadth of the analysis is commendable. The paper additionally presents many interesting results.\", \"Weaknesses\": \"- First, the notion of EDC is identical to the Effective Model Complexity (Nakkiran, et al. 2021), which the authors cite but do not really compare with. Both correspond to the largest subset of training data that is interpolated by the model. However, the authors exercise caution when using it (e.g. by checking the norm of the gradients and the flatness of the loss curve, etc) to make sure that the models are not undertrained. I would recommend that the authors refrain from claiming novelty in the notion of EDC and simply center their contribution around the empirical investigation of the Effective Model Complexity, instead.\\n\\n- The second primary weakness in my opinion is the lack of clarity about the experimental details in a few places. \\n  * When comparing different architectures such as CNNs and ViTs, how did the authors ensure that the architectures are compared fairly? For example, there is no mention of the number of parameters used in each architecture. Even if those architectures share the same parameter count, the shape of the architecture can have a big impact as the authors themselves study for CNNs. It's not clear if one can conclude, for example, that CNNs are more parameter efficient than ViTs. There aren't enough details in the paper for the reader.\\n  * The authors mention repeatedly that ReLU improves EDC even though they \\\"were introduced to neural networks to prevent vanishing and exploding gradients.\\\" I do not see the discrepancy here. ReLU were introduced to prevent training instability issues so I would expect ReLU to improve training and improve EDC as a result. It is unclear why the authors highlight that the impact of ReLU on EDC is surprising. \\n  * When the authors compare different datasets, like CIFAR10/100, MNIST, and ImageNet, they use these results to argue for the impact of the input distribution. However, those datasets have different number of classes. One way to fix that issue is to fix the number of random labels in all datasets. There is no discussion of this in the paper and it seems that the authors kept the label space unchanged. The problem is that it is not clear (again) when the input distribution matters much when accounting for the differences in the number of classes.\\n  * In many figures, the authors average log(EDC). Is it reasonable to average them? Wouldn't the average EDC be dominated by what happens to large models? Perhaps it would be better to report the raw results without averaging.\\n \\n- Some minor comments: \\n  * The author should provide precise definitions of some of the terms they use. For example, what is \\\"parameter-efficient\\\"? My understanding is that it is EDC divided by the parameter count. Is this the case?\\n  * The figures are not presented in the same order they appear in the main text. \\n  * Figure 2(b) is very difficult to understand. Are the authors here scaling a single dimension at a time using EfficientNet scaling recipe and reporting the average results?\\n  * Page 6: typo in \\\"we three neural network\\\"\\n  * It would be interesting to see of the scaling laws derived for ViT [1] would also increase EDC, similar to the analysis that was done on EfficientNet. \\n\\n[1] Alabdulmohsin, Ibrahim, et al. \\\"Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design.\\\" NeurIPS, 2023.\", \"Questions\": \"- Can you please clarify how the comparison across different architectures was done? Did you ensure that all architectures have the same size? How were the shapes selected? Are they based on some standard shapes, such as those used in the original ViT and EfficientNet papers? What about MLP?\\n- When using random labels, did you fix the label space in all datasets (e.g. binary classification) or was the same label space used in each dataset (e.g. 10 classes in CIFAR10 but 100 classes in CIFAR100)?\\n- Can you please clarify Figure 2(b)?\"}"}]}
{"paper_id": "g6Urb6ukm7", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The study of multivariate time series (MTS) is crucial across various fields due to its complexity involving synchronous variables over time. Using MTS label information is challenging due to label sparsity. Unsupervised representation learning methods, particularly contrastive learning, have been explored to generate effective MTS representations without label information, achieving promising results in various tasks.\nGap: Traditional contrastive learning methods for time series do not adequately consider the spatial relationships between the variables in MTS and exhibit high sensitivity to positive pair construction, leading to performance degradation. These limitations hinder the generation of robust and informative MTS representations suitable for diverse tasks like classification and forecasting.\nHypothesis: The authors propose that by employing a spatio-temporal contrastive representation learning approach (STCR), it's possible to generate high-quality representations that account for both spatial relationships and temporal dependencies in MTS. This approach aims to effectively mitigate the sensitivity to positive pair construction inherent in conventional methods.\nMethod: The method involves the proposed spatio-temporal contrastive representation learning approach (STCR) that encourages spatio-temporal consistency. The STCR framework includes random cropping, temporal embedding using random masking, spatial embedding with a K-nearest neighbor graph, and a projection head to generate representations. The framework uses two contrastive loss functions\u2014instance-wise and timestamp-wise\u2014to ensure learned representations capture both temporal and spatial information. Extensive experiments on classification and forecasting tasks are conducted to validate the method's efficacy, compared against state-of-the-art methods.\nConclusion: STCR generates informative representations that significantly outperform state-of-the-art methods on multivariate time series classification and forecasting tasks. While effective at capturing spatial and temporal relationships, STCR has limitations in processing times and datasets with very few variables. Future work may focus on improving computational efficiency and adapting the approach for datasets with limited spatial information.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"Experiments are extensive and the motivations are interesting.\", \"Weaknesses\": \"1. This work lacks novelty. The overall structure is similar to TS-TCC [1] and TS2Vec [2]. For example, compared to TS2Vec, this manuscript is just adding GCN to consider spatial dependencies within MTS data.\\n\\n[1] Time-Series Representation Learning via Temporal and Contextual Contrasting\\n\\n[2] TS2Vec: Towards Universal Representation of Time Series\\n\\n2. References are too old, and the authors have done limited literature review. There are works highly related to this work. For example, GCC [3] also conducted graph neural network and explore the effects of spatial consistency. I think the authors should discuss their differences.\\n\\n[3] Graph Contextual Contrasting for Multivariate Time Series Classification\\n\\n3. Contributions have not addressed the problems that the authors pointed out. The authors pointed out the limitation in the existing augmentation methods, i.e., existing methods cannot consider the distribution changes in time-series data. To address the limitation, the authors proposed a method namely random cropping. I cannot see the difference between the random cropping and existing method subseries. Meanwhile, I don't think that the proposed method can address the mentioned problem. In the example of Figure 2, the signals within [s1, s2] are still having different distribution with [e1, e2]. Following the point of the authors (changing distribution), the two subseries cannot be treated as positive pairs.\\n\\n4. The authors mentioned the importance of achieving spatial-temporal consistency. However, how to define the spatial consistency? Why can spatial consistency be achieved after you construct and process a K-NN graph with GCN?\", \"Questions\": \"1. What is the differences between this work and existing studies, such as GCC [3].\\n\\n[3] Graph Contextual Contrasting for Multivariate Time Series Classification\\n\\n2. Please clarify the reason why random cropping can address the problem of distribution changes.\\n\\n3. How to define spatial consistency, and why can spatial consistency be achieved after you construct and process a K-NN graph with GCN?\"}"}]}
{"paper_id": "fXa9rO75dB", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Classical visual odometry methods, which use image depth and ego-motion prediction, have been accurate in favorable conditions but often fail in challenging, textureless, or dynamic environments. Deep visual odometry using deep learning has emerged as a promising alternative, although traditional approaches, typically requiring extensive labeled data, can be cumbersome. Recent methods like Zhou et al. (2017) demonstrate self-supervised learning but often rely on monocular vision, which faces difficulties without reference scales and in dynamic environments.\nGap: The existing methods for deep visual odometry either focus on monocular systems or fail to adequately integrate depth and ego-motion predictions from stereo cameras holistically. Also, the robustness of these methods in dynamic environments remains an issue, as they often rely on the assumption of a static world.\nHypothesis: The paper hypothesizes that integrating optical flow predictions with stereo camera data using a deep neural network can offer better accuracy in visual odometry. By imposing a 3D-geometric constraint to enforce realistic scene structures, more robust depth and ego-motion predictions can be achieved.\nMethod: The proposed self-supervised framework leverages stereo cameras and a neural network to predict optical flow, from which depth and ego-motion are derived. Key components include: 1) A FlowNet for optical flow prediction, 2) Depth estimation using stereo disparities, 3) A 3D-geometric constraint in the loss function to ensure realistic movement predictions of static and moving objects, and 4) A PoseNet for estimating ego-motion from optical flow between consecutive frames. The methodology involves training on the KITTI dataset with photometric consistency and embedding stability facilitated by features processed through a variational autoencoder.\nConclusion: The framework demonstrates state-of-the-art results on the KITTI dataset, showing higher accuracy in depth prediction compared to monocular-based methods and comparable visual odometry performance with existing stereo approaches. The results underscore the value of stereo cameras in overcoming the limitations imposed by monocular hardware in deep visual odometry tasks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 1, \"Contribution\": 1, \"Rating\": 1, \"Confidence\": 5, \"Strengths\": \"The proposed method is slightly different from previous methods, even though the use of optical flow and stereo cameras in this field has not been new.\\n\\nThe depth accuracy of the proposed method is slightly better than previous methods that are mentioned in the paper.\", \"Weaknesses\": \"The paper looks incomplete. I hope that the authors can significantly revise the paper and add more content for the next submission.\\n\\nThe novelty of the proposed method is very limited. Please reconsider this problem and better claim the contribution.\\n\\nThe evaluation results show that the proposed method is worse than previous methods in both visual odometry and optical flow estimation.\", \"Questions\": \"NA\"}"}]}
{"paper_id": "DUkYDXqxKp", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Autonomous driving has experienced significant growth in both academia and industry over the past decade. Traditional modular designs in autonomous vehicles use rule-based methods for perception, planning, and control. These systems can struggle with unseen scenarios, leading to failures. Learning-based methods have been developed to handle diverse situations through data-driven approaches. However, these systems often operate as black boxes, lacking interpretability, which poses ethical and legal challenges. Recent advances in multimodal large language models (LLMs) offer potential for enhancing interpretability by processing video and other non-text data into text form, although existing smaller language models have been limited in their flexibility and robustness.\nGap: Traditional end-to-end learning-based autonomous driving systems lack interpretability, functioning as black boxes that humans cannot understand. This has remained a significant challenge, hindering their trust and commercialization. Existing methods using smaller language models also fall short in providing adequate flexibility and generalization, especially when facing new or unexpected inquiries.\nHypothesis: Leveraging large language models (LLMs) in an end-to-end autonomous driving system can enhance interpretability by providing explanations for vehicle actions and reasoning, as well as answering diverse human inquiries. This approach could help demystify the black-box nature of current systems and improve user trust and interaction.\nMethod: The authors introduce DriveGPT4, a novel end-to-end autonomous driving system that utilizes a multimodal LLM framework. DriveGPT4 processes input from front-view RGB cameras and predicts vehicle control signals (speed and turning angle) in an end-to-end manner. It also provides natural language explanations and reasoning about the vehicle\u2019s actions in response to user inquiries. The system is trained using a custom visual instruction tuning dataset developed from the BDD-X dataset, which includes vehicle action descriptions, justifications, and control signals. ChatGPT is used to generate diverse conversation patterns to improve the model's conversational abilities. The model architecture includes a video tokenizer to convert video frames into text tokens, integrated with a large language model (LLaMA 2). Two training stages are used: pretraining on general video-text data and fine-tuning on the customized autonomous driving dataset.\nConclusion: DriveGPT4 demonstrates superior performance in comparison to conventional methods and video understanding models, both qualitatively and quantitatively. It provides better interpretability and control signal predictions and is capable of zero-shot generalization to unseen scenarios. The system successfully answers questions about vehicle actions and reasoning and shows potential for improving interaction and trust in autonomous driving applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"- The paper is basically clearly written and easy to understand.\\n- The authors demonstrate that the LLM-based method has the zero-shot generalization ability to other datasets. It is a good point to involve LLM or large-scale training for images and videos.\", \"Weaknesses\": \"Multiple overclaims and issues about the soundness.\\n\\n- *Important:* There are multiple sentences saying that \\\"constrained by the limited capacity of smaller language models, they can only address predefined human questions and provide inflexible answers, hindering their widespread application in real-world scenarios\\\". The reviewer admits that LLMs can answer out-of-domain questions, but is still wondering if the limitation lies in the LLM. I will explain this point based on the experiments.\\n  - The improvement of the fixed question-answering experiment is limited (82.1->82.4, especially considering this is evaluated by ChatGPT with great uncertainty). Meanwhile, LLMs take much longer inference time to output the much longer answers compared to small models and the original QAs. The computational complexity is not that important at the current stage but is still valuable for illustration.\\n  - The additional question-answering experiment demonstrates the effectiveness of DriveGPT4 which is finetuned on the generated data. Then how about fintuning ADAPT and other works on the data? The direct comparison seems unfair.\\n  - For the control prediction task, DriveGPT4 takes historical control signals as input while others do not. It can be observed from the ablation study, that the results of 'No control signals' are close to ADAPT-32. The contribution comes from the history information, rather than LLM, which is unfair.\\n  - It is an open question if flexible answers can help real-world applications of autonomous driving.\\n- It is not the **first** work on interpretable end-to-end autonomous driving. The authors have already listed several in the related works. Even language-based interpretability is not accurate as there are multiple types of interpretability. In my opinion, it is fair to say LLM-based. \\n- From the very beginning, is **interpretability** indeed the unsolved problem that hinders the commercialization and development of autonomous driving? As described in the first paragraph of the introduction, the popular modular-based methods' issue is not the interpretability.\\n\\nTechnical contribution is limited. There are contributions of instruction tuning for the dataset generation. The video tokenization part is based on Valley and the action part is very close to existing works such as RT-2.\", \"Questions\": \"- Why not use ground truth from the BDD dataset? The detection results from YOLOv8 could be inaccurate. I am wondering if there is a truck in the provided figure (Table 1).\\n- How bounding box coordinates are normalized? In which coordinates are they normalized?\\n- If the model can generalize to video games, it is worth trying generalizing to CARLA and evaluating it in a closed-loop manner.\\n- The short name 'DriveGPT4' is not very appropriate, as the core method is finetuned from LLaMA 2 and does not use GPT4.\"}"}]}
{"paper_id": "OCx7dp58H1", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Transformer models have achieved significant success in multiple domains, such as vision and natural language processing. However, without large datasets, their performance can saturate quickly when increasing model depth due to supposed oversmoothing, where the representations of features converge to the same vector as layers increase.\nGap: Recent work suggests Transformers are inherently low-pass filters leading to oversmoothing. This oversmoothing can cause performance degradation at increased depths. The problem is finding out how successful Transformers overcome this theoretical limitation without changing architectures or adding hyperparameters.\nHypothesis: Transformers are not inherently low-pass filters as previously assumed. Instead, whether they oversmooth depends on the eigenspectrum of their update equations. By understanding and controlling these eigenspectrums, oversmoothing can be avoided.\nMethod: The analysis was conducted by characterizing the eigenspectrum of attention and weight matrices in Transformers and examining their impact on oversmoothing. The study extends the understanding of oversmoothing through filtering operations and explores rank collapse. Additionally, it proposes a reparameterization method that adjusts the weights of update equations to prevent oversmoothing. The empirical evaluation involved testing different models, including a reparameterized version, across various tasks with datasets like CIFAR100 and ImageNet, measuring performance under different training data conditions and corruption levels.\nConclusion: The paper concludes that Transformers are not inherently limited to oversmoothing, and much of their successful application already exhibits properties that avoid this issue. The proposed reparameterization strategy effectively prevents oversmoothing without new architectures or hyperparameters, enhancing performance in scenarios with limited or corrupted data.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"The paper is easy to follow with necessary details given to understand the theory. It extends the available theory and proposes a new perspective on reparameterization. Experimental results support the proposed theory and preventive measures on oversmoothing.\", \"Weaknesses\": \"While the theory extends previous work in some aspects, it is not clear if this difference is significant. Other preventive measures such as AttnScale/FeatScale from Wang et al. needs comparison and more discussion. Several details are also missing from the paper.\\n\\n1. Wang et al also examines residual blocks and FFN in addition to the self-attention layer. While they mention the inevitability of high frequency components to be suppressed, their analysis suggest that using residual connections might prevent it from collapsing to zero. In fact, Eq (6) in Theorem-3 suggests that it is possible to prevent decaying of high-frequency components by constraining the $||W_V||_2$ properly so that it is a non-contractive mapping. I think a more detailed comparison to Wang et al is needed to highlight how your analysis differs from low-pass filtering aspect and how your preventive measures are different from constraining $||W_V||_2$ or AttnScale/FeatScale that Wang et al applies.\\n\\n2. While Figure-2 shows that asymmetry degrades with more epochs, is 0.7 small enough to suggest symmetry, given that it starts at 0.95? What about outliers in the off-diagonal entries? Additionally, can you connect it to the condition number?\\n\\n3. Can you clarify more on how do you update $\\\\Theta$ in $QR(\\\\Theta)=[V_H, R]$? Do you do QR decomposition after every gradient step or do you use $\\\\Theta R^{-1}=V_H$ where you backpropagate gradients directly to $\\\\Theta$?\\n\\n4. How does the HFC/LFC evolve in training steps? \\n\\n5. At the end of Section 4, page 6, why do you have $V_H^{-1}$ after clip?\\n\\n6. The same page, last paragraph, I think it should be $diag(\\\\Lambda_H)=\\\\Psi^2$ -- no negative sign.\\n\\n7. Please define Q in the main statement of Theorem-2.\\n\\n8. Page 6, \\\"||\\\" is missing in definition of asymmetry.\\n\\n9. Please describe the metrics for Table-2 and Table-3.\", \"Questions\": \"Please see above for more details.\\n\\n1. Can you provide a more detailed comparison with Wang et al? Including more baselines.\\n\\n2. How should I interpret the asymmetry metric? How does it relate to condition number?\\n\\n3. Can you give more details on updating QR decomposition?\\n\\n4. How does HFC/LFC evolve during training?\"}"}]}
{"paper_id": "eO6lXIWyxn", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Diffusion models have shown significant capabilities in generating a wide range of visual content, but they face challenges in accurately rendering text within images, often resulting in incorrect characters or poor integration with backgrounds.\nGap: Existing diffusion models struggle with text generation, specifically in producing accurate characters or words that integrate well with background images.\nHypothesis: Introducing a novel framework named ARTIST with an additional textual diffusion model focused on text structure learning can enhance the text rendering ability of diffusion models.\nMethod: The ARTIST framework involves pretraining a textual diffusion model to focus on text structure. This model is then used to fine-tune the visual model, allowing it to incorporate textual structure into images more effectively. Additionally, large-language models are utilized to better infer user intentions, enhancing generation quality. The method's performance is demonstrated on the MARIO-Eval benchmark.\nConclusion: The proposed method, ARTIST, significantly improves text rendering in diffusion models, achieving up to a 15% improvement in various metrics on empirical evaluations.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 4, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 5, \"Strengths\": \"- This paper is well-written and easy to follow.\\n- Text painting is a challenging task for current T2I models, and the idea of disentanglement between text and figure is well-motivated.\\n-  They provide lots of qualitative examples for the visual comparison.\\n- Their proposed ARTIST achieves notable improvements over the previous TextDiffuser, especially on the crucial OCR metric.\", \"Weaknesses\": \"- It seems that the ARTIST framework is the combination of LLM-Layout and TextDiffusers, which both have been proposed before. Not sure if this achieves ICLR's novelty bar.\\n- There are so many metrics used for the evaluation (FID, CLIP-S, and OCR). Which one is the most appropriate to evaluate the overall performance? Or is there any way to combine all of them as a final metric?\\n- As a generative task, a human generation should be conducted to compare the performance in a human aspect.\\n- Since ARTIST relies on LLM to derive the layout, there should be a discussion about the quality of the generated layout. Is it visually appealing or in a reasonable position?\\n- Missing reference about LLM-Layout: [NeurIPS'23] Compositional Visual Planning and Generation with Large Language Models / [arXiv'23] Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models\", \"Questions\": \"Please see the weaknesses\"}"}]}
{"paper_id": "nUGFpDCu3W", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper explores the inner mechanisms of Transformer-based language models like GPT, specifically focusing on their ability to recall information learned during training. It highlights the challenge of understanding how these models work internally due to their complex, densely connected architecture. The study is part of the field of mechanistic interpretability, which aims to decipher the model computations into human-understandable components. Previous works have emphasized the crucial role of MLP layers as key-value memory stores, with early layers handling basic information storage and retrieval while later layers manage more complex tasks.\nGap: While it's known that language models store factual information in their weights, there's a lack of understanding regarding what information is stored, where exactly it is located within the model, and how it is retrieved. Specifically, the study looks at how GPT models handle tasks such as identifying matching brackets, which involve long-range dependencies, to better understand the information storage and retrieval mechanisms.\nHypothesis: The authors hypothesize that early MLP layers in Transformer models like GPT are responsible for storing and associating paired tokens, such as matching brackets. This association is expected to support existing theories about the hierarchical role of different layers within the Transformer architecture, where early layers store and retrieve basic (lower-level) information, while later layers deal with more abstract data.\nMethod: The study uses GPT-2 to conduct circuit analysis focused on a linguistic task: identifying closing brackets. Activation patching, embedding projections, attention analysis, and linear probes are utilized to trace how individual components of the model contribute to its final outputs. Short sequences with random tokens are used to isolate relevant behaviors, and experiments involve replacing specific activations with alternative values to see their effect on model predictions. The focus is on how information about open and closed brackets is stored and retrieved, using positive and negative patching to test the necessity and sufficiency of specific model components.\nConclusion: The study concludes that the first MLP layer in GPT models is responsible for storing and retrieving low-level linguistic information, such as bracket associations. This layer contains distinct representations for each type of bracket which can be swapped to influence predictions. While this corroborates earlier theories of MLP layers as key-value systems, the results show the retrieved information initially lies dormant until activated by later layers. This finding supports a hierarchical understanding of Transformer models, where different layers capture different types of dependencies or knowledge. Nonetheless, the results suggest differences in mechanism depending on the complexity of the task.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"- The authors performed a thorough study with different tests\\n- They show results which supports their claims\\n- The work represents a step forward towards understanding more DL\\nnetworks.\", \"Weaknesses\": \"- The outcomes are not surprising as it is expected that simple\\ninformation is stored in the first layer and as the information flows\\ninto deeper layers, more complex relationships are established\\n- In this sense, it is not very clear the significance of the reported\\nresults.\", \"Questions\": \"The authors focused mainly in short sequences and it is not completely\\nclear the behavior in much larger sequences.\\n\\nIt is not clear why the plots in Figure 7 are very different with\\ndifferent type of brackets. I was expected to see similar curves for\\nall types of brackets.\"}"}]}
{"paper_id": "B9klVS7Ddk", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large Language Models (LLMs) have achieved significant advances in NLP, computer vision, and other areas but are limited by their extensive computational and memory requirements. Recent efforts to mitigate these limitations focus on network pruning and weight quantization, achieving up to 60% sparsity and reducing bit-width significantly with negligible impact on performance metrics like perplexity.\nGap: There is an over-reliance on perplexity as the primary metric for evaluating the performance of compressed LLMs, which fails to capture subtle but important changes in capabilities. There is a need for a more comprehensive evaluation protocol to accurately assess the abilities of compressed LLMs, especially in knowledge-intensive tasks.\nHypothesis: Current state-of-the-art (SoTA) compression methods, which rely heavily on perplexity for evaluation, may not effectively preserve the full capabilities of LLMs. More sophisticated and realistic benchmarks are needed to reveal the true performance impacts of these methods.\nMethod: The authors introduce the Knowledge-Intensive Compressed LLM BenchmarK (LLM-KICK), a suite of tasks designed to provide a holistic evaluation of compressed LLMs' abilities across language understanding, reasoning, generation, in-context retrieval, and summarization. They evaluate pruning and quantization methods using widely recognized LLMs like Vicuna and provide empirical results on performance degradation under different compression techniques across various tasks. The experiments include different sparsity patterns, such as unstructured and structured N:M sparsity, using open-source datasets.\nConclusion: LLM-KICK reveals several critical insights: current SoTA pruning methods cause significant performance degradation in knowledge-intensive tasks even at modest sparsity ratios; quantization methods maintain performance better than pruning; and perplexity-based evaluations overlook these impacts. The benchmark suggests that even highly compressed LLMs can be robust in in-context retrieval tasks but suffer greatly in understanding and generating enriched, factually correct content. It highlights the need for developing more nuanced compression evaluation metrics and methodologies.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"1. Compression of LLMs is very timely and important.\\n\\n2. The paper reveals new and yet widely unknown gaps in compressed LLMs in comparison to their uncompressed counterparts. \\n\\n3. The paper shows that compressed models may offer better performance in some tasks (e.g., In-Context Text Summarization) than others (e.g., Factoid-based Question Answering)\\n\\n4. The authors plan to release their code which may be help in the development of future compression techniques.\", \"Weaknesses\": \"1. It would make the conclusions more robust and convincing if the evaluations use more than a single family of LLMs (i.e., Vicuna).  Why not repeat these experiments with, e.g., Llama 2 and Falcon?\\n\\n2. Regarding the observation that even 8-bit quantization has evident gaps with respect to uncompressed models, have the authors considered evaluating LLM.int8()?  (https://arxiv.org/pdf/2208.07339.pdf) \\n\\n3. It would help the reader to have a table summarizing all the tasks' performance over the different architectures, compression techniques and the their resulting perplexity.\", \"Questions\": \"See weakness 1 and 2. Also, does the authors have insights regarding why, in the paper's evaluation, quantization works better than pruning?\"}"}]}
{"paper_id": "aiPcdCFmYy", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Distributional reinforcement learning (RL) enhances the classical RL approach by estimating the full distribution of returns, rather than just expectations. This approach has led to improvements in a wide range of applications, including risk-sensitive control and policy exploration. Key methods in distributional RL have employed techniques like quantile regression and maximum mean discrepancy (MMD) to represent and compare return distributions, each with their own strengths and computational challenges.\nGap: Current distributional RL methods often face challenges with the representation of return distributions and the computational inefficiencies of existing divergence measures. Specifically, quantile regression can struggle with high-dimensionality and non-monotonicity, while MMD-based approaches may overlook finer distribution characteristics.\nHypothesis: The authors propose that utilizing Sinkhorn divergence, which interpolates between Wasserstein distance and MMD, provides a computationally efficient and geometrically aware method for measuring discrepancies between return distributions in RL. This approach is expected to offer superior performance over existing techniques by capturing richer data geometry and enabling flexible sample-based distribution representation.\nMethod: The paper introduces the Sinkhorn distributional RL (SinkhornDRL) algorithm, which represents return distributions through unrestricted samples and minimizes their discrepancy using Sinkhorn divergence. SinkhornDRL is theoretically grounded with established convergence properties in tabular cases and offers a new form of regularized MMD. Experimentally, the algorithm is tested across 55 Atari games, showing its empirical effectiveness compared to existing distributional RL methods. The authors also conduct a sensitivity analysis to evaluate the impact of different parameters on the performance of SinkhornDRL.\nConclusion: SinkhornDRL consistently achieves competitive performance compared to state-of-the-art distributional RL methods. Theoretical and empirical results suggest that Sinkhorn divergence effectively balances computational efficiency with the ability to capture complex data geometry. This work not only contributes a novel algorithm to the distributional RL field but also enhances understanding of the relationship between different divergence measures, paving the way for future RL advancements.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"The propsed sinkhorn divergence based Bellman update loss includes the regularization loss which is the mutual information of a given distribution and the target distribution.\\n\\nThis makes the above two distributions statistically independent, and the reguarlizaiton loss with the coefficeint $\\\\epsilon$ helps to increase the interquantile mean performance of atari-55.\", \"Weaknesses\": \"The main theoretical contribution can be seen as a simple combination of the architecture of MMD DRL and Sinkorn divergence. The additional KL divergence term (mutual information) in the proposed loss is a key factor to improve the DRL algorithms except for two corner cases $\\\\epsilon=0,\\\\infty$. However, the role of this proposed element is not well explained in the main text. The detailed comments are as follows. The main strength statement in this paper is that Sinkhorn divergence can be useful in the complex environments with high dimensional action space. This discussion is not rigorous, because I cannot find any explanation why the mutual information between two return distribution and the dimension of action space is related.\\n\\nI also have the following minor concerns.\\n\\n1. It is hard to get an insight by introducing the Sinkhorn divergence. For example, the paper of MMD DRL provides the figure that explains the propsed scheme can estimate the high order moments better than the existing methods.\\n2. The main ablation study (sensitivity anlysis) is provided without any detailed discussion. The authors states taht the property of loss changes as the value of $\\\\epsilon$ changes in the main theorem, but the empricial analysis and discussion is not provided well. \\n3. The experiment protocol in this paper is not standard. Previous works almost conducted in 200M iterations or the authors could have chosen the protocol of the paper, DRL at the edge of statistical precipice to validate the performance.\\n4. In table 2 and Figure 2, the propsed method outperforms in terms of mean score but not in median score than baselines, and the proposed method show remarkable performance in venture and sequest. This means that the proposed method is not a generally better algorithm but a specialist for Venture and Sequest. In my point of view, the authors should have stated why the propsee method is better in such envs.\\n5. There exists more projection method for distributional RL such as EDRL [A], but the provided baselines is not enough.\\n\\n\\n\\n[A] Rowland, Mark, et al. \\\"Statistics and samples in distributional reinforcement learning.\\\"\u00a0International Conference on Machine Learning. PMLR, 2019.\", \"Questions\": \"I mentioned my main concern in the weakness part. Please refer the weakness section.\\n\\nIn addition, it is hard to understand that reducing the mutual information helps to build a better projection operator.\\nCan the authors provide detailed explanation why the mutual information between the return distribution and target distribution is important?\\nIn my point of view, the unresticted statistics can be constructed better, because its deterministic samples can be more uncorrelated by reducing the mutual information.\\nIf it is true, the main effect seems related to the sampling scheme, not the discrepency between two ground-truth return distribution.\"}"}]}
{"paper_id": "ZULjcYLWKe", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Offline reinforcement learning (RL) is aimed at learning from offline datasets without interacting with the environment, addressing the challenges of high costs and risks associated with exploration in online settings. However, a key challenge in offline RL is ensuring robustness against perturbations in state observations, which can arise from sensor errors or adversarial attacks. Current approaches to robust RL often involve adversarial training, which is not directly applicable in the offline setting where interactions with the environment are not possible.\nGap: The current solutions for robustness in offline RL struggle with handling observation perturbations effectively without becoming too conservative. Specifically, existing methods may suffer from performance degradation when noise scale increases, particularly in complex environments with high-dimensional state and action spaces. Ensuring robustness against state perturbations without the ability to interact and adjust during training remains a significant challenge in offline RL.\nHypothesis: The hypothesis is that a framework using diffusion models, specifically a Diffusion Model-Based Predictor (DMBP), can effectively recover true states under perturbed observations by employing conditional diffusion models. This approach is expected to enhance the robustness of state-based offline RL strategies against various types of noise and perturbations by optimizing a non-Markovian training objective.\nMethod: The proposed method, DMBP, leverages conditional diffusion models to denoise observed states, improving the robustness of offline RL policies. Unlike classical diffusion models, DMBP employs a non-Markovian training objective that minimizes the entropy of denoised states over RL trajectories, thus mitigating error accumulation. The system is implemented with a Unet-MLP neural network structure to aid noise prediction, enhancing the representation of trajectory information. For experimental validation, extensive tests are conducted on standard benchmark problems from the D4RL dataset, examining both robustness against added noise and handling of incomplete state observations. The implementation is available in an open-source repository.\nConclusion: The experiments demonstrate that DMBP significantly improves the robustness of offline RL algorithms against a variety of perturbations, including random and adversarial noise. It also effectively addresses issues related to incomplete state observations, outperforming existing offline RL methods. The findings suggest that diffusion models can serve as viable noise reduction tools, rather than purely generative models, in practical offline RL applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"1. DMBP strengthens the resilience of existing offline RL algorithms, allowing them to handle different scales of random noises and adversarial attacks effectively.\\n2. Unlike traditional approaches, DMBP leverages diffusion models primarily for noise reduction, rather than as generation models. This innovative use helps in better state prediction and recovery against observation perturbations.\\n3. The framework introduces a non-Markovian loss function, specifically designed to prevent the accumulation of estimation errors over the RL trajectory.\\n4. DMBP offers a more balanced approach than methods that train robust policies against worst-case disturbances. This ensures that policies do not become overly conservative, which can hinder performance in certain scenarios.\\n5. DMBP's inherent ability, derived from the properties of diffusion models, allows it to effectively manage situations with incomplete state observations. This is particularly valuable in real-world applications, such as when robots operate with compromised sensors.\", \"Weaknesses\": \"1. The paper has the assumption that perturbation on state space follows a Gaussian distribution. However, in practical settings, such perturbations might be biased and skewed. For example, a water drop on a camera might lead to distortion of captured images. Therefore, it might be better if the authors could elaborate on the assumption of the perturbations and how they are produced. \\n2. Introducing a diffusion model for noise reduction could increase the complexity of the model, which may lead to longer training time and resource-intensive computations. The authors might want to include more details regarding training time and computational resources.\\n3. Scaling to very large state spaces or handling very noisy environments might pose challenges, especially when using diffusion models. It would be better for the authors to consider testing on tasks with larger state spaces such as the humanoid. \\n4. Eq.(2) is wrong as the recovered state should not be a probability.\", \"Questions\": \"Same as the weakness.\"}"}]}
{"paper_id": "svSWP21tdp", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper addresses the issue of model-induced distribution shifts (MIDS), which occur when the outputs of models in one generation contaminate the training sets for the next generation, leading to effects like performative prediction, unfairness feedback loops, and model collapse. Historical examples, like the redlining maps of the Home Owner Loan Corporation, illustrate how policy decisions can create feedback loops that perpetuate discrimination. In machine learning, such loops can entrench bias, leading to data ecosystems that are increasingly biased against minoritized groups as synthetic data pollutes new training data.\nGap: Current research lacks a unified framework for understanding how MIDS contribute to long-term unfairness in machine learning models. There is also a need for systematic approaches to mitigate these effects and improve fairness in models trained on synthetic data across generations.\nHypothesis: The authors propose that recognizing MIDS as a distinct category of distribution shift can help researchers and practitioners better understand and mitigate the adverse impacts of model training on fairness. They hypothesize that by employing algorithmic reparation as an intentional MIDS, it is possible to redress historic discrimination and improve fairness outcomes for minoritized groups subjected to other forms of MIDS.\nMethod: The authors introduce a taxonomy of MIDS and explore two experimental settings: sequential classifiers and sequential generators coupled with sequential classifiers. These settings allow them to analyze the impact of MIDS on model performance and fairness over multiple generations. Key metrics used include accuracy, demographic parity, and equalized odds differences. They also propose algorithmic reparation, which employs sampling strategies to counteract bias and promote fairness, evaluated through simulation on datasets like CelebA, ColoredMNIST, and ColoredSVHN.\nConclusion: The study finds that MIDS can lead to significant performance degradation and increased unfairness, particularly for minoritized groups. Algorithmic reparation, when employed strategically, can mitigate some of these adverse effects, though it may lead to trade-offs with model accuracy. The paper concludes with a call for more attention to data curation and the implications of synthetic data, advocating for approaches that emphasize fairness and equity in the face of evolving data ecosystems.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"- The paper addresses the important issue of studying the fairness effects of repeated training in a data ecosystem. \\n- The paper presents a clear model of sequentially training classifiers and generators that can be used to simultaneously model various effects like feedback loops, performative effects and model collapse.\", \"Weaknesses\": \"- The paper does not propose a mathematical model for the proposed MIDS scheme. Although various effects of retraining are shown through experiments, there is no theoretical investigation on the root causes of these effects, or the efficacy of the presented Algorithmic Reparation (AR) scheme.\\n- The experimental results with the proposed AR scheme based on resampling are not very convincing. For example, the decrease in the equalized odds difference with reparation in Figure 4 is not monotonic. The gap closes for a few generations and then seems to rise up again. The plots on fairness metrics in Figure 5 are even less convincing.\", \"Questions\": \"- What is the reason for the decrease in the equalized odds difference with reparation in Figure 4 not being monotonic?\\n- The paper claims that \\\"Generator-side AR improves fairness and minoritized representation\\\", but plots on fairness metrics in Figure 5 do not corroborate this claim. Please explain.\\n- I do not understand the claim, \\\"Performative prediction on model collapse leads to higher utility\\\". Please explain what you mean by this.\"}"}]}
{"paper_id": "oDYXpvnv5f", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Deep ensembles have been identified as promising methods for uncertainty quantification in high-dimensional regression and classification. However, they often fail to provide reliable estimates outside the training distribution, leading to overconfident predictions in out-of-distribution (OOD) scenarios.\nGap: Deep ensembles tend to produce overconfident uncertainty estimates when predicting on OOD data. This occurs because the ensemble members lack sufficient diversity in their predictions, which is exacerbated by the regularization techniques typically used during training.\nHypothesis: An ensemble of networks that fit the training data with large weights is likely to manage the trade-off between increasing ensemble output diversity and maintaining accurate in-distribution predictions.\nMethod: The method proposed involves adding an anti-regularization term to the training objective that penalizes small weights, encouraging networks to develop large weights. This approach is combined with a control process to maintain the loss on the training data within an acceptable threshold, without the need for hyperparameter tuning or OOD training data. The optimization problem is formulated to maximize diversity corresponding to a 'water-filling' model, and both theoretical analysis and empirical experiments were conducted to demonstrate the effectiveness of the method.\nConclusion: The proposed Deep Anti-Regularized Ensembles (DARE) method significantly improves uncertainty quantification for OOD data, providing better performance than recent deep ensemble and OOD detection methods in both regression and classification tasks. The method is practical and doesn't require OOD data or cross-validation for hyperparameter setting, making significant improvements in ensemble diversity and consequent reliability of uncertainty estimates.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"The paper does a good job of identifying and conveying an insight (larger weight variance leads to larger output variance), providing theoretical motivation, and then running experiments to test out that method.\", \"Weaknesses\": \"The main weaknesses of this paper are the limited set of experiments and mixed results. ResNet-32 on CIFAR-10 with only the final layer ensembled provides limited data. It's a fine experiment to include in the paper, but a larger-scale experiment with a fully-ensembled bigger model on a bigger dataset (even on ImageNet, which is not that large anymore) would provide much more information on the viability of this approach and whether it provides enough benefit. Concretely, looking at Table 2, both in-distribution accuracy and OOD detection is mixed for DARE, including worse results for the important near-OOD case of CIFAR-100 vs CIFAR-10. As noted in the limitations section, this paper is also missing comparisons to Bayesian neural nets (rank-1 BNNs and Prior networks could be nice experimental comparisons here). In the current environment, experiments with language models would also be extremely useful. Overall, the experimental setup is limited and the results are not that compelling in their current state.\", \"Questions\": \"As noted in the weaknesses above, I would suggest running larger experiments to better allow the reader to understand the efficacy of the approach. Uncertainty quantification is an important problem, so I encourage continued work here!\"}"}]}
{"paper_id": "mFTPRV5hYw", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Location-based services (LBS) have become increasingly popular due to their ability to provide personalized assistance, such as traffic management and point-of-interest (POI) recommendations, by leveraging large volumes of human mobility data. However, the privacy of such mobility data is a growing concern, as it can potentially reveal sensitive user information, such as their home/work locations.\nGap: Existing research on mobility data privacy primarily focuses on protecting data during transmission or before release, often neglecting the evaluation of privacy risks associated with machine learning (ML) models that use this data, particularly POI recommendation models.\nHypothesis: The authors hypothesize that POI recommendation models, like other ML models, are vulnerable to specific privacy attacks, and evaluating these vulnerabilities requires specialized attack techniques to assess the risk of privacy leakage in mobility data.\nMethod: The authors developed a privacy attack suite specifically for POI recommendation models trained on mobility data. This suite includes data extraction and membership inference attacks. Data extraction attacks involve extracting common locations and full user trajectories, while membership inference attacks determine whether certain locations or trajectories were part of the training data. They tested these attacks on three POI recommendation models using two real-world mobility datasets. The research also included evaluations of various defense mechanisms against these attacks, such as differential privacy and other standard machine learning techniques.\nConclusion: The study demonstrates that current POI recommendation models are susceptible to the proposed privacy attacks, as evidenced by the high success rate of these attacks in experiments. It also reveals that existing defense mechanisms are not fully effective at mitigating these privacy risks without substantial utility drop, highlighting the need for more targeted defenses to protect sensitive information in mobility data.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 2, \"doi\": 10, \"Strengths\": \"1. The research problem is important.\\n2. The paper analyzes the factors in the data that affect the attack performance.\\n3. The proposed attacks for data extraction and membership inference attacks are simple yet effective.\", \"Weaknesses\": \"1. The appropriate baselines are missing. Can the existing data extraction or membership inference attacks be applied to the POI recommendation models, e.g., [1]?\\n2. There is no qualitative result analysis on the data extraction attacks. It would be better if the authors could conduct these analyses on the data extraction attacks.\\n3. The threat models assume that the adversaries are capable of accessing the confidence scores, which makes them impractical. In practice, the model owner only releases the final result to the users. In this case, whether the proposed attacks are still effective is unknown.\\n\\n\\n\\n\\n[1] R. Shokri, M. Stronati, C. Song and V. Shmatikov, \\\"Membership Inference Attacks Against Machine Learning Models,\\\" 2017 IEEE Symposium on Security and Privacy (SP), San Jose, CA, USA, 2017, pp. 3-18, doi: 10.1109/SP.2017.41.\", \"Questions\": \"N/A\"}"}]}
{"paper_id": "vVoWRFV5Y4", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The Quadratic Assignment Problem (QAP) is addressed in the context of solving combinatorial optimization problems using reinforcement learning. Previous works have successfully applied reinforcement learning to problems like the traveling salesman problem (TSP) and vehicle routing problems (VRP) by using both constructive and improvement designs. However, existing reinforcement learning approaches have struggled to outperform classical optimization techniques for specific problem classes due to the maturity and optimization of traditional solvers. The QAP is known to be particularly challenging because it is both NP-hard to solve exactly and to approximate, making it an appealing candidate for the application of novel methods like deep reinforcement learning (DRL).\nGap: There is no known method to exactly solve QAP instances for sizes larger than 30, whereas other combinatorial optimization problems such as the TSP can be solved for much larger instances with existing techniques. Additionally, previous attempts to use machine learning approaches for QAP are scarce and have not yet demonstrated success over classical methods.\nHypothesis: The authors propose that a deep reinforcement learning approach, specifically using a novel double pointer network with policy gradient methods, can effectively provide competitive solutions to the QAP without needing instance-specific retraining, and can do so in a computationally efficient manner compared to traditional solvers.\nMethod: The authors present a method based on deep reinforcement learning using a double pointer network architecture. This network alternates between selecting a location and a facility, leveraging reinforcement learning as a sequential decision-making process. The model is trained using the Advantage Actor-Critic (A2C) algorithm on synthetic QAP instances, where locations are embedded using one-dimensional convolutions and facilities using graph convolutional networks to capture flows. The network uses an attention mechanism to guide the selection process, and results are evaluated using a local search heuristic as a baseline.\nConclusion: The proposed deep reinforcement learning approach achieves solutions to the QAP that are on average within 7.5% of a high-quality local search heuristic, and occasionally outperform it. The method demonstrates a significant reduction in computation time compared to traditional optimization techniques like Gurobi, especially as the problem size increases. While not yet surpassing established solvers like the swap heuristic, the approach offers promising performance gains, particularly in runtime, and shows potential for scaling to larger problems.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1- This paper is well motivated as the QAP has been rarely studied using DRL;\\n2- It seems the proposed method works on the self-generated dataset.\", \"Weaknesses\": \"1- It seems that from the DRL based heuristic perspective, the objective function in QAP does not make difference compared with the ones in ILP, both of which are just captured by a reward in DRL. In this sense, the QAP in this paper is almost the same as the vehicle and customer assignment in VRPs with linear objectives, such as\\n[a] Deep Reinforcement Learning for Solving the Heterogeneous Capacitated Vehicle Routing Problem. IEEE T Cybernetics;\\n[b] Learning to Solve Vehicle Routing Problems with Time Windows through Joint Attention. Arxiv;\\n[c] Solving NP-hard Min-max Routing Problems as Sequential Generation with Equity Context. Arxiv.\\nThose works also involve sequentially or parallelly selecting vehicles and customers, while using (advanced) Transformers rather than pointer network.\\n\\n2- Besides, the Transformers in 'Learning Improvement Heuristics for Solving Routing Problems. TNNLS' outputs a probability matrix, which could also be tailored to the probability of facility and location pair with proper masking.\\n\\n3- The evaluation is quite simple, which solely focuses on one single problem with randomly generated instances. And the baselines are also insufficient and not strong enough.\\n\\nOverall, the method falls short of novelty, and the evaluation is inadequate, which is below the standard of an ICLR publication.\", \"Questions\": \"Please see the above weakness.\"}"}]}
{"paper_id": "rmA6uFEgIY", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Contrastive Language-Image Pre-training (CLIP) has achieved significant success in 2D vision tasks by leveraging vast amounts of image-text pairs, demonstrating extraordinary zero-shot recognition capabilities. This approach has inspired adaptations in the 3D domain, where integration with point clouds is emerging as a promising paradigm for open-world understanding. Existing 3D contrastive learning methods, such as ULIP and CLIP2, have begun exploring triplet alignments between image, text, and point cloud modalities, achieving substantial improvements in 3D recognition tasks.\nGap: Current methods in 3D contrastive learning often rely on simplistic correspondences between modalities (e.g., point-text, point-image) and fail to fully leverage the complementary attributes of these modalities, such as the semantic richness of images and the geometric depth of point clouds. Moreover, multi-view learning, although explored in supervised settings, remains underutilized within contrastive paradigms, and comprehensive training protocols for 3D models are sparse.\nHypothesis: The hypothesis is that by creating a joint representation that synergizes multi-view images and cross-modal contrastive learning, one can significantly enhance the accuracy and generalizability of 3D representations. This approach is expected to improve alignment with text modalities and bolster the overall depiction of real-world 3D objects.\nMethod: The authors propose MixCon3D, which incorporates a joint representation alignment mechanism between images and point clouds and integrates multi-view images. This method involves concatenating image and point cloud features to form a unified representation, which is then aligned with text representations through additional contrastive loss terms. The approach also leverages an improved training recipe, incorporating a large batch size, a cosine learning rate schedule with warmup, EMA stabilization, and separate temperature parameters for different modalities. The effectiveness of this method is evaluated on datasets such as Objaverse-LVIS and ScanObjectNN.\nConclusion: The proposed MixCon3D method significantly improves performance over existing baselines, surpassing previous state-of-the-art results on several challenging benchmarks, including a 5.7% accuracy improvement on the Objaverse-LVIS dataset. The approach enhances the generalizability of 3D representations, as evidenced by its performance on both synthetic and real-world scanned datasets. Furthermore, MixCon3D demonstrates effectiveness in cross-modal tasks, such as text-to-3D retrieval and point cloud captioning, indicating a well-aligned multi-modal representation space.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 5, \"arXiv\": 2305, \"Strengths\": \"1. The paper is well-written and easy to understand. The approaches are easy to follow.\\n2. Experiments state that the method achieves state-of-the-art performance, and the quantitative shows great robustness on the downstream tasks. \\n3. The motivation is clear and reasonable.\", \"Weaknesses\": \"1. Additional Comparisons:\\nYour zero-shot classification experiments on the ModelNet40 and other datasets (Table 2) offer valuable insights. To further substantiate your findings, it could be beneficial to include comparisons with some additional methods that have shown promise in this domain, such as JM3D [1] and ULIP-2 [2]. Such comparisons could provide a more comprehensive view of how your approach stands in the context of the existing state-of-the-art methods and highlight the unique contributions of your work more effectively.\\n\\n2. Clarification of Contribution:\\nYou have noted the introduction of multi-view images in the context of contrastive learning across multiple modalities as a key contribution to your paper. It is indeed an interesting aspect of your research. However, to strengthen your claim, it would be constructive to discuss how your approach differentiates from or builds upon prior works like JM3D [1] and ULIP-2 [2], which have also explored multi-view images within a similar framework. Clarifying this could help readers appreciate the novel aspects of your methodology and its significance in the field.\\n\\n3. Expansion of Quantitative Experiments:\\nThe paper presents intriguing insights into the arithmetic of multi-modalities. To solidify these findings, it might be valuable to expand on the quantitative experiments. Additional data or more varied testing conditions could offer deeper validation of your hypothesis and might also uncover further interesting aspects of your model's behavior. This would not only bolster your current claims but could also provide a richer resource for readers interested in the intricacies of multi-modal contrastive learning.\\n\\n[1] Beyond First Impressions: Integrating Joint Multi-modal Cues for Comprehensive 3D Representation, ACM MM 23\\n\\n[2] ULIP-2: Towards Scalable Multimodal Pre-training For 3D Understanding, arXiv: 2305.08275\", \"Questions\": \"1. Multi-View Analysis: Could you specify the number of views used during training and consider including an ablation study on this feature? This would greatly clarify the contribution of the multi-view approach.\\n\\n2. Extended Experiments: Exploring fine-grained tasks like semantic segmentation, as in ULIP and JM3D, could offer deeper insights and substantiate your model\u2019s versatility.\\n\\n3. Clarification on Table 5: Please clarify the meaning of \\\"no point cloud\\\" in rows 2 and 3, to better understand how these metrics are compared without point cloud inputs.\\n\\n4. Typographical Corrections: I've noticed a minor typographical error on the 10th row of the 8th page, where the word \\\"use\\\" is repeated.\"}"}]}
{"paper_id": "3pWSL8My6B", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: In explainable AI, one key challenge is determining whether the knowledge encoded by deep neural networks (DNNs) can be considered symbolic. Previous studies have observed some symbolic concept emergence in DNNs, but there had not been a formal definition or proof. Concepts in DNNs as symbolic primitives could bridge cognitive science, neuroscience, and mathematics, although this paper focuses purely on the mathematical and algorithmic perspectives while ignoring cognitive issues.\nGap: Despite empirical observations suggesting that DNNs encode a few salient interactions that symbolize concepts, there has been no formal proof demonstrating how and when these symbolic interaction primitives emerge in DNNs. Addressing this could profoundly impact both theoretical and practical aspects of AI.\nHypothesis: The paper hypothesizes that under specific conditions, most DNNs encode only a small number of sparse interactions between input variables, which can be considered as symbolic primitive inference patterns. These conditions include (i) zero high-order derivatives of network output, (ii) DNNs delivering higher confidence with less occlusion, and (iii) consistent confidence levels in occluded samples.\nMethod: The authors introduce a theoretical framework proving the emergence of sparse interaction primitives by setting three conditions: zero high-order derivatives (Assumption 1-\u03b2), task-driven monotonicity in strategic occlusion (Assumption 2), and bounded degradation on occlusion confidence (Assumption 3). The authors use Harsanyi interactions to define the symbolic primitives and perform experiments on DNNs, including image and natural language datasets, to illustrate interaction sparsity and evaluate p values tied to masking effects.\nConclusion: The study proves that symbolic interaction primitives are a common phenomenon across various DNNs trained for different tasks under defined conditions. It establishes the link between sparse interactions and the nature of tasks, offering a foundation for conceptualizing DNNs in explainable AI. Future work aims to tighten focus and clarify conditions for interaction emergence.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 2, \"Strengths\": \"* The paper is well-written and clear.\\n* The paper contributes novel proofs for theorems 2 onwards.\\n* The paper appears well-placed in the relevant literature.\\n* The papers theoretical results appear significant and relevant to the ICLR community.\", \"Weaknesses\": \"* The claim of to \\\"prove the emergence of symbolic concepts (or more precisely, sparse primitive inference patterns) in well-trained AI models.\\\" (Line 1), appears too broad and un-substantiated, specifically encompassing \\\"AI models\\\". It appears the evidence presented in the paper is only applicable and demonstrated for Deep Neural Networks (DNNs). For example, I cannot see how this proof or claims can be applied to other AI models that are not DNNs, such as Random Forests, which considers the outputs of all trees; k-NN's, that use all features to compute distances, or Boosting algorithms (e.g., Gradient Boosting Machines). Specifically, the empirical evidence presented in the paper in Figure 3, Figure 4, Figure 5 and Table 1, only apply to deep neural network models, and there is no other AI models that are not DNN's to support this claim.\\n* It is not clear how the input variables should be grouped to form the set of $S$ input variables. For example, in the dog image example, \\\"image regions\\\" are proposed; however, there is no indication or discussion on how large these \\\"image regions\\\" should be for the approach and proof to work. For example, does this sparse primitive approach still hold if the \\\"image regions\\\" are too small, e.g., each region is a single pixel, and does it still hold if the \\\"image regions\\\" are too large, e.g., encompassing most of the image, or even half of the image. Some discussion on this, and how to select these groupings could be informative to the reader.\\n*  Missing error bars; and large error bars. There appears to be missing error bars for Figure 4, Figure 5 (a), Table 1 (top row: Percent of samples with monotonicity). The provided error bars in Table 1 and Table 2, are so large that they overlap; leading the reader to suspect that the claim of the input samples satisfying the monotonicity assumption is not empirically verified, as the numeric results in Table 1 are not statistically significant, as the error bars overlap.\", \"Questions\": \"* Can you clarify if the core proof of the paper applies to other AI models that are not DNN's? If it does, can you provide evidence for this, and if it does not, perhaps think of refining your claims, and title to be only applicable to DNNs.\\n* Can you clarify how the reader should group input variables to form the set of $S$?\\n* Can you include error bars for the top row of Table 1, and include error bars for Figure 4 and Figure 5 (a). Also can you re-run with more random seeds to see if the error bars in Table 1 reduce.\"}"}]}
{"paper_id": "ZKnbIZefER", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Availability attacks aim to prevent unauthorized use of private data and commercial datasets by adding imperceptible noise to make the data unlearnable, thus hindering the training of usable models. Contrastive learning, which has gained performance comparable to supervised learning, can bypass these attacks when traditional supervised learning fails.\nGap: Existing availability attacks fail to achieve contrastive unlearnability, allowing malicious data collectors to use contrastive learning to effectively leverage poisoned data, undermining data protection.\nHypothesis: Employing stronger data augmentations in supervised poisoning generation can create shortcuts for contrastive learning, improving the worst-case unlearnability across multiple supervised and contrastive learning algorithms.\nMethod: The authors propose two attacks: Augmented Unlearnable Example (AUE) and Augmented Adversarial Poisoning (AAP). These attacks use enhanced data augmentations during the noise generation process to mimic the behavior of contrastive learning models. This involves optimizing a supervised loss under stronger augmentations to indirectly minimize the contrastive loss, thus creating more effective noise that undermines both supervised and contrastive learning.\nConclusion: The proposed AUE and AAP attacks improve the worst-case unlearnability across multiple supervised and contrastive algorithms compared to existing methods. These attacks show increased effectiveness in diminishing the impact of contrastive learning on protected data, verified through experiments on datasets like CIFAR-10 and ImageNet-100. The approach highlights the necessity of considering both supervised and contrastive pathways in designing data poisoning attacks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"- The authors quantitatively measure the GAP between the attack effectiveness of supervised learning and contrastive learning methods using two contrastive learning metrics.\\n- By incorporating the data augmentation techniques of contrastive learning into supervised learning, the authors develop an availability attack method that is effective under both supervised learning and contrastive learning training frameworks.\", \"Weaknesses\": \"- The paper is difficult to read and the organization of the content is not very clear.\\n- The Cross-Entropy (CE) loss and InfoNCE loss may be essentially similar, and using these two losses to reflect the relationship between the two tasks is not particularly convincing.\\n- Although the paper emphasizes sampling in data augmentation as being introduced from contrastive learning, it bears a resemblance to the Expectation Over Transformation (EOT) used in the reference paper on REM. EOT also involves sampling from data augmentation, and this technique is commonly used in adversarial settings.\\n- The assumptions made in the theoretical analysis employ a simple linear network, which presents a significant discrepancy from practical settings.\\n- The resolution of the Tiny-ImageNet dataset is not down-sampled, and the Mini-ImageNet dataset has a limited number of samples per class.\", \"Questions\": \"- Does the value of the GAP affect accuracy? According to the paper, GAP reflects the difference between clean and poison distributions. Logically, the greater the difference, the better the protection effect should be. However, the results in Table 1 do not seem to support this notion.\\n- Why does the loss of Alignment in Figure 3(c) first decrease and then increase?\"}"}]}
{"paper_id": "g9diuvxN6D", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Instruction fine-tuning has emerged as a promising approach to improving zero-shot capabilities of LLMs, enhancing the performance of smaller models to be competitive with larger counterparts. Prior work highlights the potential of instruction tuning and its success across various benchmarks, but issues regarding LLMs understanding and robustness to prompt phrasings remain under-explored.\nGap: There is a lack of comprehensive analysis on the robustness of instruction-tuned LLMs to variations in instruction phrasings, especially in zero-shot contexts. It is essential to determine if models are overly sensitive to unobserved instructions, which could limit practical utility.\nHypothesis: Instruction-tuned models are not robust to variations in instruction phrasings, leading to performance degradation with unobserved instructions. Introducing a method to align the representations of semantically equivalent instructions could improve robustness.\nMethod: The authors collected 319 novel instructions from NLP practitioners for over 80 tasks and evaluated LLMs against unseen instructions. They introduced 'soft prompt' embedding parameters that optimize the similarity between representations of equivalent instructions. Experiments were conducted using large benchmarks like MMLU and BIG-BENCH with various instruction-tuned models (Flan-T5, Alpaca, T0).\nConclusion: The study found that LLMs show significant performance variability when using unobserved instructions. The proposed method of aligning instruction representations improved robustness, indicating potential for addressing the sensitivity of instruction-tuned models to phrasing variations.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"- The experiments are clear and well-thought-out, and the results are solid and interesting, affecting many current popular models (Llama finetunes), and holding for different model families.\\n- The observed correlations between performance and instruction similarity (in representation space) are interesting (even if the correlation is somewhat weak).\\n- The proposed fix method is well-motivated, and the results seem reasonable, especially for the Alpaca-7B model.\\n- The robustness of models to different prompts is an important and interesting area of study, especially considering current difficulties with LM evaluations.\", \"Weaknesses\": \"- The novelty of the robustness experiments is somewhat limited, as there is prior work that has observed variance based on prompt formatting (discussed in the work itself). Considering this work seems to examine more modern models such as Alpaca, and considers a different set of tasks, I think this is a minor weakness - the work does add a number of interesting insights, especially around representation distance.\\n- The claim that models perform worse at novel phrasings of tasks seems a bit strong compared to the results: examining Table 3, the variance of performance on unobserved phrasings often is greater than the difference in average performance, suggesting that the difference in performance may not be significant (although the large increase in variance is certainly not desirable and interesting to see).\\n- While the proposed method seems strong, examining the results in Appendix B.3 seems to show that in many cases, prompt-tuning alone performs within one standard deviation of the PT+KL results (e.g., for BBL unobserved and average). As such, the proposed method might not be significantly different from just prompt tuning.\\n\\nOverall, I think this paper is okay, and the robustness results are interesting and useful to see, along with the representation alignment results. However, the novelty of the robustness experiments is a bit limited, and it\u2019s not clear the proposed method for improving this is significantly better than baselines.\", \"Questions\": \"- How does your proposed method compare to introducing the model paraphrases during initial instruction finetuning? Training on the paraphrased versions alone after the instruction tuning may cause forgetting in an undesirable way.\\n- Is the degradation of performance on unobserved prompts statistically significant? The variance seems very high.\\n- Doing prompt tuning alone seems to often perform very similarly to PT+KL, and within the reported standard error in Appendix B.3. Do you have statistical tests for significance for your results?\\n- It would be interesting to see the results in Table 7 broken down into the BBL QA/BC/MC categories shown in Table 6 to see how much the difference in closest distance affects the performance of these different categories.\"}"}]}
{"paper_id": "B21c9hT1D7", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: In the field of statistics, robust regression is a classical topic that deals with the estimation of vector parameters in the presence of data contamination. Traditionally, the Huber loss has been used as it provides robustness against outliers while maintaining convexity. However, most studies have been conducted under assumptions of sub-Gaussian features and bounded noise variance, which may not hold in high-dimensional settings with heavy-tailed data.\nGap: Existing work predominantly analyzes robust regression assuming sub-Gaussian features and bounded noise variance, leaving a gap in understanding the behavior of robust regression techniques when covariates and noise exhibit heavy-tailed distributions, which lack finite second moments.\nHypothesis: The authors propose that, in high-dimensional regimes with heavy-tailed data, traditional assumptions about the optimality of regression techniques (like Huber loss) may not hold, and further regularization is required to achieve optimal performance.\nMethod: The paper employs high-dimensional asymptotic analysis via the replica method to characterize the performance of robust M-estimators, particularly focusing on Huber regression, under heavy-tailed covariate and noise distributions. The authors explore various loss functions and covariate distributions to derive theoretical characterizations of error decay rates and optimal estimators. Experiments are conducted to observe the behavior of these estimators under different contamination levels and to compare them against Bayesian optimal performance.\nConclusion: The study finds that in high-dimensional settings, robust regression techniques like the Huber loss, when unregularized, are suboptimal amidst heavy-tailed data, underscoring the necessity of additional regularization. The error decay rates are affected by the presence of heavy tails, and different convergence behaviors are observed depending on the distribution of the covariates and noise. The analyses generalize to a broader family of models and data distributions, hinting at complex interactions in mixture models.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1- The motivation to study the asymptotic performance of robust classifiers in the realistic regime, where the dimension is of the same order as the sample size, is interesting. \\n\\n2- The results and intuitions derived seem interesting and new, and the theoretical and empirical predictions are in agreement with the experiments carried out.\\n\\n3- The generality of the results, which can be applied to broader distributions such as Gaussian mixture model, is also an advantage, as is the independence of the formulae from any function used for robust regression.\", \"Weaknesses\": \"1- The presentation could be improved, particularly the ratings. Given the wide range of notations, a section explaining the notational conventions would have been ideal, as would the rigour of defining all the notations and pointing them out. Although there is a section discussing the results, it would have been interesting to discuss results 2.4 and 2.6 a little better by showing particular cases where the solution is closed (such as ridge regression in a contaminated or uncontaminated case), and to derive the major changes in the theoretical formulae progressively.\\n\\n2- It is a pity that the asymptotic performance depends on the true regression parameter, which limits its practical use (for example on real data) for optimizing the regularization parameters and loss in particular. The authors could have addressed the issue of estimation.\\n\\n3- The results derived by the authors seem non-rigorous, although several attempts in the literature have made them fairly rigorous. It would be interesting to discuss the reasons why it is not possible to derive rigorous results and, above all, to explain why it is not possible to derive rigorous results. It would be interesting to discuss the reasons why it is not possible to derive rigorous results and above all to show to what extent the theoretical contribution of the paper is different and challenging from previous works.\", \"Questions\": \"1- The paper mentions that the contamination model applies to both the covariates and the responses. However, in the definition of the model there is more mention of contamination on the covariates, which obviously indirectly induces contamination on the responses. But written this way, we would expect additional contamination on the regression noise.\\n\\n2- Given the complexity of the expressions, it is important to simplify the notations as much as possible and to avoid defining unnecessary quantities and also to clearly define those which are used. Unless I'm mistaken, this is not always the case. For example in the result 2.4,  $\\\\xi \\\\sim \\\\mathcal{N}(0, I_d)$ and $z$ seem to be defined but never used. Likewise, the integration variables $\\\\eta$ and $\\\\tau$ are not properly defined. The notation $\\\\partial_w$ and $\\\\partial_\\\\mu$ are not clear since we do not know what $w$ and $\\\\mu$ refer to.\\n\\n3- The results depend on the real parameter $\\\\beta_\\\\star$, which limits the application of the bound on real data. Can the authors comment on when to estimate asymptotic performance to carry out, for example, model selection, ...\\n\\n\\n4- Section 4 could have been reorganized differently to provide more detailed explanation and experiments. In substance the result is, with the exception of one or two notations, the same as that of the main result. It is therefore in my opinion unnecessary to present it again. It can be put in an appendix by highlighting in the main paper what are the differences with the main result and deriving the main complementary intuitions.\\n\\n5- The lack of rigor of the results still remains one of the weaknesses but it is very little discussed. What else prevents you from having rigorous results? What are the blocking passages?\\n\\nAfter rebuttal, I read the answer of the authors. The authors promise to enhance the readability of the paper by providing notations section, by illustrating and discussion specific particular cases. I think this will allow to reach a wider audience. But I still think there are more to do in this sense. I will advise authors to progressively help the reader to grasp all the cumbersome notation by explaining before the theorem each main variables and giving intuitive explanation. How they behave in the asymptotic case (p fixed and n large)? What are the effects of the large dimensional corrections, and to particularly discuss notations that are specific to the heavy tailed analysis and that don't appear in the non heavy tailed analysis by raising some limiting cases. I think the problem of estimation of the signal to noise ratio limits the practical application of the paper and even though the authors claim it s a standard problem for regression problem, I think it is important to make the nice theoretical analysis practical even by providing some guidelines from the theory or from the analysis.\"}"}]}
{"paper_id": "uNrFpDPMyo", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Autoregressive language models based on the Transformer architecture have become popular due to their success across various applications, but they face challenges related to computational complexity and GPU memory consumption, especially when using the Key/Value (KV) cache mechanism for generative inference.\nGap: Despite the benefits of the KV cache mechanism in large language models for generative inference, its memory consumption increases significantly with model size and generation length, making it difficult to manage within the GPU memory without resorting to offloading, which can add overhead and affect performance.\nHypothesis: The authors propose that by utilizing the intrinsic structure of attention modules, it is possible to adaptively construct a KV cache to reduce memory consumption without impacting the quality of generative inference, potentially eliminating the need for offloading.\nMethod: The authors introduce FastGen, an adaptive KV cache compression technique that employs lightweight attention profiling to determine the structure of attention heads. Based on this profiling, the method adapts the KV cache by evicting certain contexts or employing different caching policies per head. Experiments were conducted using LLaMA models across various benchmarks to demonstrate the efficacy of this approach in reducing GPU memory usage with minimal impact on generation quality.\nConclusion: FastGen effectively reduces GPU memory consumption by adaptively compressing the KV cache based on attention head structures without the need for expensive retraining or fine-tuning. It achieves significant compression with little impact on generation quality and is a plug-and-play solution that can be integrated into existing models.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 4, \"LLMs\": 1, \"Strengths\": \"- The paper introduces valuable insights drawn from LLMs: 1. Different structure in different attention 2. The same head structures persist. These insights are well-supported with empirical data and references to existing literature. \\n\\n- The authors leverage these insights to come up with an effective compression method that adapts to the structure of each attention head. The results show consistent compression rate and model quality improvement over prior SoTA fixed compression mechanisms.\", \"Weaknesses\": \"- The paper could benefit from presenting actual GPU inference performance results using FastGen and comparing them with other compression methods. Additionally, providing a runtime breakdown would offer more insights into the overhead caused by the profiling, compression, and decompression processes.\\n- It would be nice to look into the structure of KV in the multi-query attention design.\", \"Questions\": \"-\"}"}]}
{"paper_id": "gqzknNJShg", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Resource-constrained vision tasks, like image classification on low-end devices, face challenges due to limited computational resources and restricted access to large training samples. Prior research has used data augmentation to learn lightweight models with fewer samples, but this often requires scenario-specific optimization and does not fully exploit frequency components accessible via Fourier analysis.\nGap: Conventional data augmentation methods for resource-constrained environments require optimization for specific scenarios or do not exploit frequency components from Fourier analysis, leading to models that may not learn discriminative high-frequency features effectively.\nHypothesis: The hypothesis is that by using a frequency-based image encoding method (FourierAugment), lightweight models can learn richer features from limited data by explicitly providing information from various frequency bands.\nMethod: FourierAugment is a frequency-based image encoding method that uses discrete Fourier transformation (DFT) to separate frequency components of an image. It provides these components to a lightweight model, aiming to learn both low- and high-frequency information in balance. The method does not complicate model architecture or increase computation and involves transforming images into frequency domain and back for feature extraction.\nConclusion: FourierAugment significantly enhances the performance of lightweight models under resource constraints by promoting balanced learning of frequency components. It achieves state-of-the-art performance in multiple benchmarks, is easily applicable to existing models, and supports richer feature learning without architectural complexity or computational increase.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"1. The introduction of FourierAugment as a frequency-based image encoding method represents a novel and innovative approach in addressing resource-constrained vision tasks. FourierAugment's unique utilization of frequency components sets it apart from conventional data augmentation methods.\\n2. The paper's empirical study is thorough and well-designed, ensuring the validity and reliability of the results. Extensive experiments on multiple datasets and resource-constrained vision tasks provide a strong foundation for the paper's claims.\\n3. The paper is well-structured, with a clear presentation of the problem statement, method development, empirical study, and results.\", \"Weaknesses\": \"The paper could benefit from a more comprehensive theoretical background and motivation section. It's important to provide a clear foundation for why FourierAugment was developed and the specific theoretical underpinnings. A deeper exploration of the relationship between frequency components and lightweight model learning could enhance the paper's overall coherence.\", \"Questions\": \"1. The theoretical underpinnings and motivations for proposing FourierAugment, including the specific theoretical underpinnings that guided its development, would add to the coherence of the paper if they were added in full.\\n2. The n value is chosen empirically? Is there a definite basis for setting 2 or 3?\\n3. How does the method proposed in this paper compare to the comparative baseline AugMix, RandAugment and Deep AutoAugment in terms of computational efficiency and generalizability to different visual tasks?\\n4. Does adding the image enhancement proposed in this paper to the image classification and FSCIL tasks significantly extend the processing time of the tasks?\"}"}]}
{"paper_id": "0jsfesDZDq", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Recurrent Spiking Neural Networks (RSNNs), inspired by the human brain, have emerged as efficient models for processing spatio-temporal data. They utilize spikes for efficient learning and have been shown to improve with diversity in neuronal parameters, known as heterogeneous RSNNs (HRSNNs). Previous research has demonstrated that exploiting this heterogeneity can optimize network performance, but such models also suffer from increased complexity with more neurons and dense connections, making them hard to optimize.\nGap: Traditionally, sparse RSNNs are created by first training a dense model for a specific task and then using activity-based pruning methods that focus on removing low-activity neurons. This approach often results in models that are highly customized for specific tasks and may not generalize well across different tasks, leading to poor overall efficiency and adaptability.\nHypothesis: The authors hypothesize that a task-agnostic pruning method can be devised that prunes large, untrained RSNN models to form sparse, generalizable RSNNs, without sacrificing performance. By leveraging Lyapunov exponents and graph sparsification, a universally robust network can be achieved that maintains stability and flexibility across different tasks.\nMethod: The authors introduce the Lyapunov Noise Pruning (LNP) method, which involves pruning untrained, initialized RSNN models. The LNP employs spectral graph sparsification and leverages Lyapunov exponents to determine neuron and synapse removal, optimizing for both the network topology and neuronal time constants. The method does this while maintaining stability, without initial training on specific tasks. Experiments were conducted on image classification (CIFAR10 and CIFAR100) and time-series prediction tasks using synthetic (e.g., Lorenz and Rossler systems) and real-world datasets (Google stock and wind prediction).\nConclusion: The LNP method results in sparse RSNNs that are flexible and efficient, maintaining performance across multiple tasks. Compared to traditional pruning methods, LNP enhances computational efficiency by reducing neuron and synapse count while maintaining accuracy. The method shows greater generalizability and robustness due to its task-agnostic nature, making it advantageous in scenarios with constrained computational resources.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"The LNP enables pruning the network prior to training, and effectively enhancing the sparsity while maintaining commendable performance.\", \"Weaknesses\": \"1. The authors should elucidate the motivation behind their proposed method. What significance does their method hold for the field of spiking neural networks?\\n2. The network model and task employed in the experiments are too simple and do not need pruning. I am more intrigued by the adaptability of the LNP method under more intricate situations.\\n3. The authors provided a detailed description of the latest pruning works in SNN in the second paragraph of the Introduction. However, their results are not referenced in Table 1. Meanwhile, the compared works in Table 1 lack reference links.\\n4. The equation 3 appears abrupt. I don\u2019t understand why the spike frequency of untrained SNN can be used to construct this linearized model. And how to calculate the matrix D and L?\\n5. Activity Pruning needs to provide citation or explanation.\\n6. Lorenz and Rossler are unfamiliar datasets in the field of SNN. Could you please provide the citations?\", \"Questions\": \"See the weakness please.\"}"}]}
{"paper_id": "9GviaQcGnx", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Regularization techniques, like weight decay and L2 regularization, are commonly used in deep learning to reduce overfitting by applying uniform penalties to all model parameters. However, these techniques may not be optimal for all parameters, as each may benefit from different regularization strengths. Previous works have explored adaptable weight decay and layer-wise scheduling to improve final performance, indicating the potential benefits of dynamic regularization strategies.\nGap: Current weight decay methods apply a uniform penalty to all parameters, which may not be ideal for all neural network parameter groups. Different parameters could benefit from varying regularization strengths, yet implementing such dynamic regularization typically requires setting multiple hyperparameters, complicating the regularization strategy.\nHypothesis: Constrained Parameter Regularization (CPR) can provide individualized and dynamic weight regularization by setting upper bounds on parameter group norms. This approach eliminates the need for separate penalty coefficients and can potentially outperform traditional weight decay and L2 regularization by dynamically adapting regularization strength without additional runtime overhead.\nMethod: CPR is implemented by transforming the regularization problem into a constrained optimization problem where an upper bound is enforced on a statistical measure (such as L2-norm) of different parameter groups. This is solved using an adaptation of the augmented Lagrangian method. CPR requires two hyperparameters: a regularization bound and an update rate, with an adaptive mechanism proposed to adjust bounds during training. The approach was tested against traditional weight decay in tasks involving grokking, computer vision, and language modeling. CPR\u2019s effectiveness was demonstrated using ResNet and GPT2 models on CIFAR100 and OpenWebText datasets, without incurring extra runtime costs.\nConclusion: Constrained Parameter Regularization effectively counteracts the grokking phenomenon and shows improved or on-par performance relative to traditional weight decay in various neural network tasks. The use of CPR allows for efficient and individualized adaptation of parameter regularization during training, making it a potentially more flexible alternative to existing methods while keeping hyperparameter tuning minimal.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"1\uff09 The authors introduce a constrained parameter regularization that does not uniformly \\napply a constant penalty to all parameters.\\n2\uff09 The authors provide an open-source implementation of CPR, which can be easily adjusted \\nby replacing the optimizer class.\", \"Weaknesses\": \"1\uff09 The authors should provide a clearer exposition of the problem statement and research \\nmotivation in the abstract.\\n2\uff09 In the related work section, the authors should provide a more comprehensive review of \\nprior research.\\n3\uff09 The experimental results conducted by the authors did not show a significant \\nimprovement in optimization performance, and they did not provide additional \\nexperimental results or comparisons related to existing work.\\n4\uff09 While applying different regularization strengths to each parameter group may seem \\nintriguing, the authors have not discussed the necessity and potential advantages of this \\napproach compared to traditional methods.\", \"Questions\": \"N/A\"}"}]}
{"paper_id": "v5Bb7F1Wkf", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Incremental Learning (IL) is crucial in dynamically evolving applications such as autonomous robots, where models are required to learn new tasks sequentially without accessing data from completed tasks. Traditionally, IL faces the challenge of balancing plasticity (learning new tasks) with stability (retaining old-task knowledge). Existing methods have attempted to address this through approaches like memory-based, regularization-based, and parameter-isolation techniques. Memory-based methods store data to prevent forgetting, regularization-based techniques penalize changes to maintain stability, and parameter-isolation approaches allocate specific model parts to tasks, but each comes with trade-offs.\nGap: The challenge in exemplar-free IL, where no old-task data can be retained, lies in achieving a balance between plasticity and stability without excessive memory consumption. Existing methods either use significant memory or tend to compromise either stability or plasticity.\nHypothesis: The authors propose that averaging model parameters from independently learned models for each task can preserve knowledge across tasks effectively. Based on this, they suggest a dual-learner framework with cumulative parameter averaging to manage knowledge transfer between a plastic learner (for new tasks) and a stable learner (for accumulated knowledge from all tasks).\nMethod: The Dual-Learner framework with Cumulative Parameter Averaging (DLCPA) involves a cyclic method where: (i) a plastic learner is optimized for new tasks using both self-supervised and supervised loss functions to enhance feature extraction robustness, (ii) a stable learner receives cumulative average updates from the plastic learner to maintain generalization across tasks, and (iii) task-specific classifiers are adjusted to align with the stable learner. The approach was tested on CIFAR-100 and Tiny-ImageNet datasets, where DLCPA outperformed state-of-the-art exemplar-free IL methods in both Task-IL and Class-IL settings.\nConclusion: The proposed DLCPA framework significantly enhances the balance between plasticity and stability in exemplar-free IL scenarios. By using cumulative parameter averaging, it effectively consolidates and transfers knowledge across tasks, achieving competitive results while maintaining memory efficiency. However, the method assumes clear task boundaries and equal task importance, and future work might address these limitations to expand DLCPA's applicability.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"## Presentation\\n- The paper is clearly written for the most part.\\n\\n## Contribution\\n- The core idea behind the presented method is simple.\\n- The method seems to work well on Tiny-imagenet\\n- The analysis of how the proposed stable-plastic architecture can help is nice and simple.\", \"Weaknesses\": \"## Presentation\\n- The \\\"Analysis\\\" section contains the problem definition and explanation of the baselines, which are background information. It would be better to split this information into a clear background section.\\n- The task settings \\\"Task-IL\\\" and \\\"Class-IL\\\" are never defined in the paper.\\n- The paper assumes that storing previous models or datasets is so costly that it is a big disadvantage to the memory-based methods. However, from a lay perspective, it would seem that storage is very cheap. I suggest the authors include more motivating discussion about why the memory-based IL methods have a disadvantage.\\n\\n## Contribution\\n- The self-supervised learning (SSL) objective is introduced without appropriate analysis and baselines.\\n    - From the ablation studies, we see that the SSL objective is a crucial component of the proposed method. However, I don't see how SSL is specific to the proposed method. Without applying SSL objectives to at least some of the most competitive baselines, we don't know how much of the improvement from the proposed method is actually due to the stable-plastic learner architecture.\\n    - Three different SSL losses are proposed to be used with the method. The best results in each of the datasets are split between each of the SSL objectives. On some datasets, choosing a different SSL objective would lead to the proposed method no longer beating the baselines. This seems like a weakness of the method, in that you have to train three versions of it in order to pick which one you want to deploy.\", \"Questions\": \"- Which SSL objective should a user choose?\\n- To which baselines is the SSL objective applicable?\\n    - How well would those baselines do with the SSL objective?\\n- Is there a practical task, where one of the methods considered in this paper would work well, but storing all of the data would become prohibitively expensive?\"}"}]}
{"paper_id": "jdynlBj3b0", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Multi-label learning (MLL) involves associating an instance with multiple labels and is used in applications like image annotation and text classification. Single-positive multi-label learning (SPMLL) addresses the challenge of high annotation costs by training models using only one positive label per instance. Existing SPMLL methods rely on pseudo-labels assuming identical class-priors across categories.\nGap: The assumption of identical class-priors in SPMLL methods is unrealistic for real-world applications where class-priors vary, leading to biased pseudo-labels and suboptimal model performance.\nHypothesis: The paper proposes that incorporating class-priors into SPMLL can improve model performance by providing a more realistic framework for training, thereby addressing the bias introduced by the assumption of identical class-priors.\nMethod: The authors introduce CRISP (Class-pRiors Induced Single-Positive multi-label learning), which estimates class-priors using a class-priors estimator. This estimator uses a threshold-based approach to converge to ground-truth class-priors. An unbiased risk estimator derived from the estimated class-priors approximates the optimal risk minimizer on fully supervised data. Ten benchmark datasets were used to validate the method, showing its superior performance to existing approaches.\nConclusion: The CRISP framework effectively integrates class-prior estimation into SPMLL, leading to more accurate models by removing unrealistic assumptions about class-priors. The method showed significant performance improvements across multiple datasets, confirming the importance of considering class-priors in SPMLL.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 3, \"Contribution\": 4, \"Rating\": 8, \"Confidence\": 3, \"Strengths\": \"1. Strong empirical performance\\n2. The proposed method is theoretically principled with a convergence guarantee\\n3. The unbiased risk estimator is simple and intuitive\", \"Weaknesses\": \"1. Clarity of writing. I found the problem setting to be unclear until I finished section 4.  One suggestion would be to add more details about the setup in the preliminary setting e.g. the absolute loss function, before diving into deriving the estimators. Also, changing the order by deriving the risk estimator before the class-priors, may provide a better motivation on why we need to estimate the class-priors.\", \"Questions\": \"1.  The algorithm relies on iteratively estimating the class-prior from f and then using it to update f. Is it possible if there is a failure mode ?\\n2.  Would it be possible to extend this type of estimator to a different loss than the absolute loss?\"}"}]}
{"paper_id": "EjIKerYk1O", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Digital towers are emerging as a cost-effective alternative to physical air traffic control towers, especially beneficial for small and medium-sized airports. They offer video-sensor-based surveillance, which enhances safety and efficiency when integrated with computer vision technologies. Currently, most research focuses on ground-based aircraft tracking at airports, whereas tracking aircraft during their final approach requires development. Existing distance measurement techniques using monoscopic or stereoscopic views face challenges concerning accuracy and calibration, especially over large distances.\nGap: There is a notable gap in research concerning the estimation of distance-to-touchdown (DTD) for aircraft approaching runways, a crucial parameter for final approach and departure sequencing. Current techniques such as monoscopic and stereoscopic distance estimation are ineffective for achieving the required accuracy over distances of up to 10 nautical miles.\nHypothesis: To accurately estimate the distance-to-touchdown of approaching aircraft, a multi-view video feed approach can overcome the limitations of monoscopic and stereoscopic methods. Integrating advanced object detection models with auxiliary regression and auto-calibration can provide real-time and accurate DTD estimates.\nMethod: The authors propose a real-time deep learning approach utilizing multi-view video feeds from digital towers. The approach uses Yolov7 for real-time aircraft detection and combines it with auxiliary regression auto-calibration to handle diverse camera viewpoints. The final features extracted from camera views are processed using LSTM models to ensure precise distance estimations. The approach is adaptable to various camera configurations without requiring retraining. The model was evaluated using simulated and real video data from Singapore Changi Airport, demonstrating low predictive errors and successful performance under different meteorological conditions. Data collection involved generating high-fidelity simulated data for training. A TensorRT engine was used to enhance processing efficiency, achieving high frame rates needed for real-time applications.\nConclusion: The proposed method effectively estimates aircraft distance-to-touchdown using multi-camera inputs, achieving high accuracy (Mean Absolute Percentage Error of 0.18% with simulated data up to 10 nautical miles, and 0.33% with real data under cloudy conditions up to 7 nautical miles). It outperforms monoscopic methods and demonstrates robustness across various lighting and weather conditions, thus enhancing runway controllers' ability to manage sequencing and spacing, improving overall airport operations.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"1. Distance estimation is a challenging task.  \\n2. This paper is easy to understand.\", \"Weaknesses\": \"1. The model design is not novel, which has limited technical learning.   \\n2. The dataset is not available. Then it cannot be a part of contribution.  \\n3. Calibaration network is a little strange. Why it can align two views without knowing the related position for the two cameras? If the two camera's position is changed, can this model still work?   \\n4. The number of views in the dataset is only 2.  The statement of \\\"multi-view\\\" is unsoundness. Author should increase the view number.  \\n5. The paper writing should be further improved. Besides, figure in the manuscript should be the vector figure (Most figures are blur).\", \"Questions\": \"Refer to the weakness.\"}"}]}
{"paper_id": "dyG2oLJYyX", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Neural Architecture Search (NAS) automates the design of neural architectures, reducing manual trial and error but often incurs high search costs due to extensive training requirements. Traditional NAS involves sampling a vast space of architectures and evaluating them, usually demanding significant computational resources. Previously, parameterized predictors were used to estimate an architecture's performance without full training, but this led to inefficiencies such as passive architecture filtering.\nGap: Existing NAS methods are inefficient, focusing on time-consuming sampling and training of numerous architectures that may not align with task-specific objectives. They largely rely on evaluating architectures post-generation, which results in wasted computational efforts on irrelevant architectures.\nHypothesis: By shifting from the traditional NAS approach to a conditional Neural Architecture Generation (NAG) framework using diffusion models, architectures can be generated with properties aligning more closely with specific tasks, guided by predictors, thereby increasing efficiency and performance.\nMethod: The proposed method, DiffusionNAG, employs diffusion models to generate neural architectures by treating them as directed graphs. The approach integrates predictors to guide the generation towards architectures satisfying specific conditions, like better performance or robustness. DiffusionNAG involves conditioning the generative process with predictors, allowing architectures to be generated in regions of the search space that are more likely to meet desired properties. This method was tested through experiments in Transferable NAS and Bayesian Optimization NAS scenarios, demonstrating significant performance improvements over baseline NAS techniques. The experimental setup also involved tasks on MobileNetV3 search space and NAS-Bench-201, leveraging dataset-aware predictors for validation.\nConclusion: DiffusionNAG demonstrates a paradigm shift from conventional NAS methods by efficiently generating task-specific architectures using diffusion models under the guidance of property predictors. This method significantly outperforms existing approaches in efficiency and quality, notably achieving substantial speedups in large search spaces and enhancing BO-based NAS processes by limiting early low-quality sample generation.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"This paper is well-motivated. Previous work only uses the diffusion model to model undirected graphs for neural architecture generation. Using the diffusion model for modeling the directed graph is needed. Moreover, classifier guidance is well suited for the Transferable NAS and substitute step 3) in BO-based NAS approaches, where the dataset and the pre-trained classifier are given as the setup.\", \"Weaknesses\": \"1.\\\"Whether diffusion model for neural architecture generation is more efficient than Mutation + Random\\\" is a good question. It is important to ask \\\"when they are better\\\".\\n\\n* The strength of the predictor guidance is an important hyperparameter. In this paper, a good performance can be achieved by cross-validation. Could the author provide some ablation studies of how this hyperparameter affects the performance? How could we effectively define the search range for cross-validation?\\n\\n* Will using the diffusion model for neural architecture generation have some model collapse phenomenon? Does the Diffusion model only generate some similar architecture, and achieve the better \\\"worse case accuracy\\\" by sacrificing the diversity? If that is the case, any explore and exploit framework for it?\", \"Questions\": \"Please see the Weaknesses part.\"}"}]}
{"paper_id": "aAE44ivBtx", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Machine learning models, especially deep learning ones, are vulnerable to data poisoning attacks where adversaries inject malicious samples to alter model behavior. Clean-label poisoning is a sophisticated variant where such perturbations are human-imperceptible, making them harder to detect and filter out.\nGap: Existing defenses against clean-label poisoning either lack provable guarantees or significantly impact model utility. Some defenses rely on impractical assumptions about the attacker's capabilities or specific knowledge about the test input.\nHypothesis: A pre-trained diffusion model can sanitize training data by removing adversarial perturbations, acting as a certified defense against clean-label poisoning attacks while maintaining high model utility.\nMethod: The authors propose using a denoising diffusion probabilistic model (DDPM) to pre-process training data by removing adversarial perturbations before the model training. This approach decouples the defense from the training algorithm, allowing for certification. They test their defense on CIFAR-10 against seven clean-label poisoning attacks in different scenarios, comparing clean accuracy and attack success rates against other defenses.\nConclusion: The proposed diffusion model-based defense effectively reduces the success of targeted clean-label poisoning attacks to 0\u201316% with minimal impact on test accuracy. It outperforms existing poisoning defenses by maintaining higher model utility and should be used as a baseline for developing stronger attacks and defenses.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The paper introduces a defense method based on DDPM to counter clean-label poisoning attacks, and it demonstrates strong performance against various attack methods.\", \"Weaknesses\": \"1. The primary results presented in Table 1 are obtained through empirical experiments rather than relying on certified defense methods that guarantee a specific radius where the model is proven to provide the correct predictions. I would suggest a mathematical formulation to provide a certified bound.\\n\\n2. Randomized smoothing has already been established as an effective technique for ensuring l2 or l_infinite robustness against various types of perturbations, including backdoor perturbations. Moreover, it offers a clear and established framework for calculating certified l2 or l_infinite robustness bounds. The novelty of this paper is somewhat limited, as its core idea revolves around utilizing randomized smoothing to provide certified defense specifically against backdoor perturbations.\\n\\n3. Experiments on the indiscriminate data poisoning [1] should also be included.\\n\\n[1] Yiwei Lu, Gautam Kamath, and Yaoliang Yu. 2023. Exploring the limits of model-targeted indiscriminate data poisoning attacks. In Proceedings of the 40th International Conference on Machine Learning (ICML'23), Vol. 202. JMLR.org, Article 949, 22856\u201322879.\", \"Questions\": \"See weakness above.\"}"}]}
{"paper_id": "1NHgmKqOzZ", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Dataset distillation aims to minimize the time and memory needed for training deep networks by creating small sets of synthetic images that replicate the performance of large datasets. This is particularly useful for reducing the computational costs associated with training on large datasets and is relevant to fields like continuous learning, neural architecture search, and privacy-preserving machine learning.\nGap: Current dataset distillation methods fall short in performance compared to training on original datasets. They primarily use a single synthetic subset to match early training dynamics, which doesn't capture the entire training process, especially the later stages where neural network training dynamics change significantly.\nHypothesis: The authors propose that instead of using a single synthetic subset, multiple subsets should be used, each capturing different stages of the training dynamics. This approach can yield better generalization performance by capturing the full range of training dynamics in deep networks.\nMethod: The authors introduce Progressive Dataset Distillation (PDD), a pipeline that generates multiple small sets of synthetic images, each conditioned on the previous sets. Each set captures different stages of training dynamics, and the model is cumulatively trained on these subsets. PDD does not require additional training time compared to conventional methods.\nConclusion: PDD improves performance of existing dataset distillation methods by up to 4.3%. It also enables the generation of larger synthetic datasets, significantly closing the performance gap with training on full datasets. The progressive approach of capturing training dynamics in stages allows for better generalization, and PDD achieves near full dataset accuracy with only a fraction of the data size.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 5, \"Strengths\": \"The idea of progressive dataset distillation is interesting. This can 1) capture the training dynamic of neural networks better as demonstrated by authors, 2) reduce the complexity of training the whole synthetic dataset together, and 3) serve as a strong method for slimmable dataset condensation [a].\\n\\n[a] Slimmable Dataset Condensation, CVPR 2023.\", \"Weaknesses\": \"1. Many places are unclear.\\n    * Fig. 1 needs some explanations. How do the results come in detail, like what's the IPC of each stage, and how to conduct multi-stage training? Although some of these questions are answered in the following parts, the writing is not coherent.\\n    * The networks for the next stage come from the training results with previous batches of synthetic data. In IDC, the networks are periodically re-initialized randomly. In MTT, the networks come from checkpoints of training with original datasets. We have to conduct some modifications to these baselines before using PDD. These operations are unclear.\\n2. A comment: this method makes an assumption on downstream training using synthetic datasets: models must be trained in a multi-stage way using the provided multi-stage synthetic data, which would introduce a lot of hyperparameters, especially in the cross-architecture setting and make the dataset distillation less elegant. Given that the performance gain is not significant in most cases, the practical value of the proposed method is somewhat limited.\\n3. Through the results in Tab. 6, the effect of discarding easy-to-learn examples at later stages is not significant. More evidence is necessary to demonstrate the effectiveness.\", \"Questions\": \"Please refer to the Weaknesses part. I would also like the authors to discuss more benefits of PDD as mentioned in the 1st point of Strengths in the camera-ready version or in the next revision cycle.\"}"}]}
{"paper_id": "9qtswuW5ux", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Combinatorial optimization (CO) is a crucial area in computer science characterized by selecting combinations of elements to minimize costs within problem constraints. These problems are often NP-hard, making exact solutions impractical for large instances. The rise of Graph Neural Networks (GNNs) offers promising alternatives for solving CO problems due to their ability to learn patterns within graph data. The Quadratic Unconstrained Binary Optimization (QUBO) problem formulation is a valuable approach, as it can represent numerous CO problems, including Max-Cut and graph coloring, making algorithms addressing QUBO solutions especially significant.\nGap: Although there have been various approaches leveraging GNNs for CO problems, including supervised methods requiring labeled training instances and reinforcement learning with scalability challenges, the use of unsupervised GNNs leveraging QUBO as a loss function has been debated regarding their solution accuracy and efficiency compared to classical algorithms.\nHypothesis: The paper proposes that a novel unsupervised QUBO-based Graph Neural Network with a Recurrent Feature (QRF-GNN) will improve performance for solving combinatorial optimization problems by using recurrent features and parallel convolutional layers, leading to results competitive with state-of-the-art heuristics.\nMethod: The proposed method, QRF-GNN, employs a GNN architecture with a recurrent feature that uses previous predictions to improve current predictions in an unsupervised learning setup. The architecture involves parallel convolutional layers implementing different aggregation functions, alongside using artificial node features. It optimizes a loss function based on QUBO relaxation. The method was tested on benchmark datasets for maximum cut and graph coloring problems, using various performance metrics and comparisons against existing GNN and heuristic methods.\nConclusion: QRF-GNN demonstrates superior performance compared to existing unsupervised GNN approaches and is competitive with state-of-the-art heuristics, particularly in terms of solution accuracy and computational efficiency for large graphs. This suggests that unsupervised learning with QUBO loss functions and recurrent features can provide viable solutions for CO problems, bridging a gap in current methodologies.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. Using Neural Networks to solve combinatorial optimization problems is an interesting topic.\\n2. The proposed QRF-GNN can outperform existing GNNs in solving Max-Cut and graph coloring problems.\\n3. The paper is well-written and easy to follow.\", \"Weaknesses\": \"1. The paper's novelty appears constrained, utilizing a GNN with QUBO's continuous relaxation as a loss function for combinatorial optimization. Clarification on how this differs from prior work would be beneficial.\\n\\n2. The motivation behind the specific design of the GNN, particularly the use of two different convolutional layers (SAGEConv Pool and SAGEConv mean) in the first layer, is not explained. A rationale for these choices should be provided.\\n\\n3. An ablation study is needed to substantiate the proposed GNN's superiority over existing methods. Without this, the advantage of the proposed model remains unclear.\", \"Questions\": \"Please refer to the weaknesses.\"}"}]}
{"paper_id": "lBY65YaAho", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper discusses the advancements in AI facilitated by training large transformers using next-token prediction, highlighting successes in generative AI like ChatGPT. However, these approaches rely heavily on human supervision, which poses challenges due to the need for extensive human input and expertise, especially in specialized domains.\nGap: The reliance on human supervision for fine-tuning large language models presents a substantial challenge, as it is resource-intensive and limits scalability. There is also the issue of lack of diversity in self-generated data when AI models are used in place of human supervision, leading to rapid saturation in learning processes.\nHypothesis: The authors propose a novel paradigm called Exploratory AI (EAI) designed to autonomously generate high-quality training data, reducing dependence on human supervision by leveraging large language models to explore and create diverse data in the natural language space.\nMethod: The approach employs two main components: an actor that generates novel content and a critic that evaluates and guides the content generation process. EAI draws inspiration from unsupervised reinforcement learning pretraining and adapts it to large language models, focusing on novelty evaluation. The method involves using principles for data exploration such as rephrasing, restructuring questions, and creating new scenarios. EAI is tested on mathematical reasoning benchmarks like GSM8K and MATH, comparing it against human-supervised and other AI-supervised methods.\nConclusion: EAI significantly improves model performance on complex reasoning tasks, outperforming state-of-the-art AI supervision methods. It generates diverse training data without human involvement, suggesting it as a scalable and automated alternative to traditional fine-tuning methods. The study demonstrates EAI's superior sample efficiency and diversity in data generation, offering insights into how AI can autonomously enhance learning processes.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"The problem of having less human involvement in finetuning is an important topic. The link between unsupervised RL and this problem is very interesting. The proposed method seems to work on the experiment.\", \"Weaknesses\": \"1. To start with, I think the method is still relying a lot on human insights especially on some exact ways of generating \\\"diverse\\\" methods. The argument that this is a general method is not well supported enough, and I would love to see more experiments on different kinds of benchmark.\\n\\n2. The paper is missing ablation to see how exactly the critic help the results. In other words, it needs to compare with similar methods like self-instruct.\\n\\n3. While the diversity experiment in Fig3, it would be more interesting in seeing more visualization. Since you already have the embeddings, maybe do a T-SNE plot or something to better prove the diversity\\n\\n4. Finally, the method seems to have very weak link to unsupervised RL pretraininig, and in fact the model is not trained at all but simply prompted. And I don't see any thing related \\\"learning skills\\\".\", \"Questions\": \"See above\"}"}]}
{"paper_id": "Kfpaq5CJPy", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Adversarial robustness is crucial for deep neural networks used in image classification due to their susceptibility to adversarial attacks, which are small perturbations that can cause the network to misclassify inputs. Traditional adversarial attack methods use Lp norms to bound perturbations, with methods like PGD being prominent. However, real-world distribution shifts, not bounded by any norm, such as those caused by weather conditions, also degrade performance. Image transformations such as DCT and DWT have been proposed to improve robustness by using them in adversarial training to handle common perturbations.\nGap: Standard adversarial attacks and defenses typically target the entire L\u221e space, which is often not efficient for generating visually similar adversarial examples or for generalizing against distribution shifts like common image corruptions.\nHypothesis: Attacking within a low-dimensional subspace of image representations defined by discrete transforms can yield adversarial examples with higher perceptual similarity to the original images and enhance the robustness of neural networks against a wider class of perturbations, especially when used for adversarial training.\nMethod: The authors propose a novel adversarial attack method that operates in a meaningful subspace of an image's transformed representation while constrained by the L\u221e pixel space using a non-linear programming technique called the barrier method. They instantiate this approach using three transformations: DCT, DWT, and Glow (a flow-based model). The barrier method enables efficient computation of perturbations without projections, making the attack feasible for adversarial training. They evaluated this on the CIFAR-10 and ImageNet datasets, comparing against standard PGD and other L\u221e attacks, and tested the models for robustness against common image corruptions using CIFAR-10-C.\nConclusion: The proposed method successfully produces adversarial examples that are visually and perceptually closer to the original images while maintaining a high attack success rate within the defined bounds. Training with these adversarial examples (especially with DWT) leads to better robustness against common corruptions while maintaining competitive natural accuracy compared to models trained under full L\u221e constraints. This supports the hypothesis that focusing on meaningful subspaces can bridge adversarial and corruption robustness, offering a new direction for adversarial strategies and practical robustness enhancements.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"The idea presented in the paper is very interesting and novel. Converting the adversarial attack problem into a linear programming problem and using adequate tools to solve it is a good catch. The authors have explained well the problems encountered during making such a conversion and strategies used to overcome it. E.g. using barrier method to remove need for projection, using first-order update rule instead of the usual minimization, dealing with discontinuity of $L^{\\\\infty}$ norm. Overall the problem is well formulated and explained.\\n\\nThe flow of the paper is clear and easy to understand. In terms of significance, this reformulation of adversarial attack is interesting and might be for the useful for the community to look at.\", \"Weaknesses\": \"The idea presented is novel but the experiments do not back it up well. \\n\\n1. The method finds examples that are less perturbed than other methods compared but the attack success rate is not upto par with other methods. In fact it is very low in some of the image representations used. The presented the trade-off between distance from original image and success rate does not show it to be very effective attack method. You can't really visually tell the difference between the adversarial images generated from this attack and PGD.\\n2. The attack methods are only compared on CIFAR - Imagenet would be a good addition as datapoint to the experiments.\\n3. No mention of the cost of attack, number of steps to reach the adv example and time taken by barrier method optimization.\\n4. For adversarial training, comparison has only been made with PGD. I believe other methods should be used in the comparison. \\n5. Again ImageNet not used for adversarial training, would be a good experiment to add.\\n\\nOverall the experiments do not showcase the effectiveness of method as adversarial attack or for training robust models compared to other state of the art methods. \\n\\nTypo:\\n\\nEqn 2 : $R^n$ -> $R^q$ ?\", \"Questions\": \"See weaknesses\"}"}]}
{"paper_id": "cHy00K3Och", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Modern datasets and deep learning models are growing in size and complexity, which results in extensive use of computational resources and increasing training time and effort. Additionally, this rise contributes to a larger carbon footprint for training models. There is a need for methods to reduce these computational requirements while maintaining model performance.\nGap: The main gap addressed in the paper is the need for a simple and effective method to select representative subsets of data (coresets) that can reduce the computational requirements for training deep learning models, particularly during the early stages, without compromising the model's generalization performance.\nHypothesis: The authors propose that a coreset can be effectively identified using the similarity of loss gradients. By focusing on samples whose gradients are similar to others, these samples can approximate the training set's gradients and thus represent the dataset adequately. This method would provide an efficient and effective means of coreset selection that could outperform existing methods.\nMethod: The proposed method, GradSimCore, identifies representative samples through gradient similarity. The method evaluates the gradients produced by data samples and selects those with the strongest gradient similarity to others as part of the coreset. Experiments were conducted using popular datasets, including CIFAR10, CIFAR100, and ImageNet-1K, with various CNN architectures to demonstrate efficacy. The approach employs several computational efficiency techniques, such as focusing on the last layer's gradients and using a radius-based nearest neighbor search.\nConclusion: GradSimCore is a novel, intuitive coreset selection method that achieves state-of-the-art accuracy on several benchmark datasets while being efficient in computational time. It provides a simple yet effective baseline for future coreset selection methods with a promise of reduced computational costs and carbon footprint in training deep learning models.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 1, \"Presentation\": 1, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"This article is that it provides links to the code\", \"Weaknesses\": \"See the summary.\", \"Questions\": \"No question! I suggest the author should not resubmit a manuscript like this again.\"}"}]}
{"paper_id": "98g9NdJPxm", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Federated learning (FL) is a collaborative learning paradigm designed to protect data privacy by training models locally on devices and only sharing model updates with a central server. Despite this, recent research has shown that FL is vulnerable to data reconstruction attacks, where adversaries can reconstruct training data from shared model updates.\nGap: Existing research on data reconstruction attacks in federated learning lacks a theoretical framework to understand the attacks' effectiveness and limitations. Most current work is empirical, which makes it difficult to fairly compare different attack methods and understand the extent of data reconstruction possible under these attacks.\nHypothesis: The authors propose that a theoretical framework can be developed to accurately bound the data reconstruction error for various attacks in federated learning, allowing for a fair comparison of their effectiveness. They hypothesize that an attack's effectiveness can be reflected in its reconstruction error bounds.\nMethod: The authors introduce a theoretical framework to analyze data reconstruction attacks in FL. This involves deriving a bound for the reconstruction error based on the Lipschitz constant of the reconstruction function. They detail the conditions under which their theoretical results hold. The framework is tested on various datasets using current attack methods like DLG, iDLG, and InvGrad. They employ a custom unrolled feed-forward network to compute the upper bound of the Lipschitz constant necessary for error estimation.\nConclusion: The theoretical framework allows for a systematic comparison of different data reconstruction attacks in FL. It confirms that certain methods, like iDLG, have inherent advantages over others like DLG in terms of attack effectiveness. The framework's results are consistent with empirical observations on dataset reconstructions, though there is some discrepancy in other cases. The study sets a foundation for future work to refine these bounds and extend the framework to non-convex losses.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The clarity is good. The flow of the writing is smooth.\\n2. In the introduction section, the paper points out three meaningful limitations of existing attack methods: high sensitivity to the initialization; evaluation highly depending on the choice of model snapshot; and lack of theoretical analysis.\", \"Weaknesses\": \"My main concern is about the significance. In details\\n1. Theorems on the attack upper bound need unrealistic assumptions.\\n-  The FL model is required to be strongly convex and smooth, etc. This is far away from the scenario where those attack methods are located -- those attack methods are commonly evaluated for deep neural networks as the input is the image in the evaluation.\\n- The reconstruction method is assumed to have a Lipschitz constant. However, the attack methods studied in the paper are optimization-based attacks and do not have an explicit Lipschitz constant. Although the paper proposes a method to approximately calculate the constant, there's no guarantee how large the approximation error would be. This might introduce an additional arbitrarily large noise to calculate the upper bound.\\n2. As shown in the empirical results, there is a large gap between the upper bound and the attack performance. It might be hard to indicate a useful conclusion from this bound.\", \"Questions\": \"Please see the weakness above.\"}"}]}
{"paper_id": "lWe3GBRem8", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Prior work in reinforcement learning (RL) has explored the combination of offline RL, which allows for policy training on pre-collected static datasets, and online RL, which addresses exploration challenges. Offline RL is effective for learning from existing data, but often suffers from distributional issues, while online RL facilitates exploration by encouraging new state occupancy via exploration bonuses. These exploration bonuses aim to increase state coverage, but can introduce bias, leading to suboptimal policy performance even with increased exploration. Recent studies have suggested combining the strengths of both paradigms by leveraging offline RL techniques for exploitation while using online interaction for state exploration.\nGap: The authors identify a key issue in reinforcement learning: the bias introduced by exploration bonuses can result in suboptimal policies. In addition, there is a challenge in balancing exploration and exploitation, where exploration bonuses in online RL do not decay quickly enough, leading to policies that prioritize exploration over task-specific rewards. Existing methods that combine offline and online RL often end up with a policy that is biased by exploration bonuses during fine-tuning, reducing overall policy performance.\nHypothesis: The authors hypothesize that decoupling the exploration and exploitation phases in reinforcement learning processes\u2014using offline RL to retrain policies from online interactions\u2014can mitigate the exploration bias. By using an optimistic exploration policy for state-exploration and a separate, pessimistically-trained exploitation policy for evaluation, the framework can leverage the advantages of both offline and online RL, thereby potentially recovering more performant policies from online interactions.\nMethod: The authors propose the Offline-to-Online-to-Offline (OOO) framework which employs two distinct policies: an optimistic exploration policy for interacting with the environment and gathering data, and a pessimistically trained exploitation policy for evaluation. The process begins with pre-training the exploration policy on offline data using exploration bonuses. During online interaction, the exploration policy interacts with the environment, collecting additional data. After the interaction phase, an exploitation policy is trained from scratch on all collected data, using pessimistic offline RL approaches such as Implicit Q-Learning (IQL) or Cal-QL. This division aims to reduce the inherent bias introduced by exploration bonuses during evaluation, thereby fostering the development of a more focused exploitation policy that accurately reflects task-specific rewards. Experiments were conducted using D4RL benchmarks and OpenAI gym environments, employing methods such as RND for exploration bonuses and examining various RL algorithms to substantiate the framework\u2019s efficacy.\nConclusion: The OOO framework improves state-of-the-art performance on several benchmark environments by effectively separating exploration and exploitation. By employing decoupled policy learning, the framework demonstrates substantial performance increases over previous methods, showcasing improvements in environments where systematic exploration is critical. Furthermore, it proves beneficial in fine-tuning from incomplete offline datasets, where previous approaches may struggle, by recovering performant policies without being penalized by continuous exploration bias. The study suggests decoupling policies as a viable strategy to enhance RL exploration and exploitation balance and improve policy effectiveness.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"The paper is well-written and easy to follow. The proposed method is overall reasonable.  Besides, I really like the topic raised by the author. How to use offline data for safe and efficient online interaction to find a global optimal policy is a key issue. Currently, pessimistic training in offline RL indeed enabled the recovery of performant policies from static datasets. Thus, I think the topic is timely to be proposed, as it can be the potential last step of the offlineRL paradigm to ground RL in real-world applications.\", \"Weaknesses\": \"1. some important baselines seem missed: baseline of offlineRL + exploration (without decoupling) should be added; some offlineRL + decoupled exploration methods should be added (see below);\\n2. some related works should be discussed: decoupling the exploration policies and the exploitation policies (or target policies) is not a new idea in standard exploration studies. There are indeed many works [1,2,3,4] that have discussed this problem. There should be a more formal discussion on the differences between the decoupling in offlineRL and off-policy RL beyond the difference in the gradient computation methods they used.\\n3. the evidence of exploration bonuses bias in the learned policy should be discussed more explicitly. Maybe giving some visualizations for OOO and a standard offlineRL + RND algorithm is good.\\n4. In Figure 3, why use \\\"frozen\\\" RND  and a baseline to demonstrate the effects of exploration bias?\\n5. I am a bit confused about the paragraph \\\"What explains the improved performance in OOO RL?\\\". How can we derive the conclusion that \\\"These ablations suggest that mitigating the exploration bias by removing the intrinsic reward when training the exploitation policy leads to improved performance under OOO\\\" just by excluding the two hypotheses?\\n\\n\\n[1] Decoupled Exploration and Exploitation Policies for Sample-Efficient Reinforcement Learning. 2021.\\n\\n[2] Off-policy Reinforcement Learning with Optimistic Exploration and Distribution Correction. 2021.\\n\\n[3] Curiosity-Driven Exploration for Off-Policy Reinforcement Learning Methods. 2019.\\n\\n[4] Reinforcement Learning with Derivative-Free Exploration. 2019.\", \"Questions\": \"An open question for discussion: What do you think are the essential differences, the extra challenges, and the benefits of the standard exploration problem in online RL and the exploration problem in offline-to-online RL?\"}"}]}
{"paper_id": "Cw6lk56w6z", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: In-context learning (ICL) has emerged as the default method for leveraging large language models (LLMs), where models implicitly learn tasks from given instructions and demonstrations. This paradigm, supported by alignment techniques such as instruction tuning, has become foundational for many applications, providing enhanced capabilities to serve diverse human needs.\nGap: The paper identifies that ICL struggles with specification-heavy tasks\u2014those requiring complex and extensive specifications which often demand significant time for human mastery. ICL's performance on these tasks usually does not reach half of the state-of-the-art results from fine-tuned models.\nHypothesis: The authors propose that the failure of ICL in handling specification-heavy tasks is not due to inherent flaws in LLMs but stems from inadequate alignment methods. They hypothesize that with proper fine-tuning and alignment, LLMs can perform well on these tasks.\nMethod: The study involves comprehensive experiments testing ICL across 18 specification-heavy tasks using various modern LLMs, including GPT-4 and FLAN-UL2, to identify specific performance issues. The authors then fine-tune LLMs on these tasks to assess potential performance improvements, also performing instruction tuning to evaluate if enhanced alignment could improve ICL effectiveness.\nConclusion: The study concludes that the challenges faced by ICL with specification-heavy tasks are primarily due to shortcomings in current alignment methods rather than limitations of the LLMs themselves. Fine-tuning can dramatically improve performance, signaling potential in optimizing alignment further to meet sophisticated human demands.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"1. The paper highlights an interesting challenge to the in-context-learning paradigm that even today's most powerful LLMs struggle with. \\n\\n2. The paper presents promising initial results that demonstrates the potential of addressing the challenge of specification-heavy tasks via more focused alignment.\", \"Weaknesses\": \"1. The success of the ICL approach highly depends on the amount and quality of information provided in the prompts. The results in table 1 are all based on a shortened version of the task description. Examples in the appendix show that such shortened descriptions are too concise, e.g., they do not even contain a single sentence definition of each label. The provided few-shots do not cover all labels. Longer prompts can still fit in at least a subset of the LLMs used for the experiments. The paper needs to experiment with more detailed prompts to confirm that the poor accuracy of the ICL approach is not due to the very concise set of instructions.\\n\\t \\n2. The alignment experiments in Section 4.2 only hold out one task at a time, but still tasks of the same type are included in the alignment dataset. The paper needs to report results with a whole task type (e.g., All 4 RE tasks) is held out to confirm that the gain in accuracy is not really due to similarity between tasks.\", \"Questions\": \"1. Comparing table 1 to table 4, Aligned ICL has worse accuracy than the unaligned  baseline on 3 tasks MAVEN-Subevent and the two sentiment analysis tasks. Do you have an explanation or an intuition for why that is the case for those three datasets?\\n\\t\\n2. In Section 3.1, the paper says that \\\"LLMs ignore all the contexts in 18 instances\\\". How did you find out about that?\"}"}]}
{"paper_id": "eADuHv62J5", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Long-tailed data distributions, where a few head categories have abundant data and many tail categories have scant data, are common in domains such as e-commerce, finance, biomedical science, and cybersecurity. Machine learning models typically perform well on head categories but poorly on tail categories.\nGap: Despite numerous studies to address long-tailed learning, challenges remain in characterizing the extent of long-tailedness, understanding the performance across different tasks and domains, and addressing cross-domain applications.\nHypothesis: The authors propose that a comprehensive benchmarking framework, considering the characterization of long-tailedness, data complexity across domains, and task heterogeneity, could provide a systematic view for evaluating long-tailed learning approaches.\nMethod: The paper introduces HeroLT, a comprehensive long-tailed learning benchmark integrating 15 state-of-the-art algorithms evaluated on 16 real-world datasets across five tasks (classification, multi-label text classification, image classification, instance segmentation, and node classification) in three domains (natural language processing, computer vision, and graph mining). HeroLT uses six evaluation metrics and conducts 304 experiments to offer a thorough performance evaluation. Novel aspects include introducing the Pareto-LT Ratio to measure dataset long-tailedness, providing a taxonomy of data complexity, and addressing heterogeneous tasks.\nConclusion: HeroLT's extensive benchmarking reveals that no single algorithm outperforms others across all tasks and domains, emphasizing the importance of scenario-based algorithm selection. The study also underscores the significance of characterizing long-tailedness and suggests future directions such as better theoretical analysis tools, cross-domain algorithm development, and addressing under-explored applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The long tail problem is a practical and valuable research topic.\\n2. Benchmark contains different data types and tasks.\", \"Weaknesses\": \"1. Most of the datasets and tasks seem to have been well constructed and the paper seems to merely concatenate them together without much additional integration.\\n2. The methods discussed in the paper appear to be somewhat dated. For instance, the image classification data that is prominently featured in Table 2 is based on methodologies that were largely developed before 2020. Considering that it is already near the end of 2023, this renders the insights provided by the paper less credible.\\n3. Referring to section C.1 of the paper, there is a misalignment in the model architectures and the number of epochs used when comparing different methods, which calls into question the article's claim of a 'fair evaluation'.\\n4. According to the four-point analysis in the upper half of the third page, the Pareto-LT Ratio still requires integration with the Gini coefficient and IF to describe the dataset. The former primarily represents the number of categories, while the latter two indicate data imbalance. So why not directly use the number of categories as a descriptor? Additionally, I do not find comprehensive experimental and theoretical evidences to support this four-point analysis.\", \"Questions\": \"See above.\"}"}]}
{"paper_id": "SL7djdVpde", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Variational Quantum Algorithms (VQAs) have gained attention during the Noisy Intermediate-Scale Quantum (NISQ) era for their potential to provide quantum advantage. A key challenge is implementing symmetries as hard constraints within VQAs, as typical VQAs were not initially designed to accommodate such constraints.\nGap: Current VQAs, like the Quantum Approximate Optimization Algorithm (QAOA) and methods for ground state energy estimation, often fail to natively handle hard symmetries and constraints, which limits their applicability in solving certain quantum chemistry and optimization problems.\nHypothesis: The hypothesis is that the Hamming Weight (HW) preserving ansatz can be utilized to enforce hard constraints in VQAs more effectively than existing methods, and can be theoretically linked to various physical symmetries and constraints, thus expanding its applicability beyond current use cases.\nMethod: The paper explores HW preserving ansatz, detailing its application in symmetry-preserving VQAs and leveraging quantum optimal control and overparameterization theories. This includes testing on unitary approximation, ground state energy estimation, and feature selection tasks using different HW preserving gates, particularly focusing on the Reconfigurable Beam Splitter (RBS) gate. Experimental setups involve verifying theoretical findings on constrained VQA problems.\nConclusion: The HW preserving ansatz shows superior performance in applying hard constraints within VQAs. The RBS gate, alongside its tested variations, achieves better outcomes on constrained VQA problems like ground state energy estimation and feature selection, indicating efficiency and supremacy over alternatives like the Hardware Efficient Ansatz (HEA) and other soft constraint methods.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"- This paper nicely summarizes and makes clear the idea of using Hamming weight (or in physics terminology, excitation) preserving ansatz for variational quantum algorithms. The numerical results verify the accuracy of the theory, which characterizes the expressibility of these ansatz.\\n- The experiments are nice, showing a modest improvement over existing methods.\", \"Weaknesses\": \"- Typos: \\\"relative\\\" should be \\\"relatively\\\" throughout\\n- Typo: In Figure 2, \\\"Haar measurement\\\" should be \\\"Haar measure\\\"\\n- Typo: \\\"Hartree-Fork\\\"\\n- Based on common gate sets of real machines, I feel the BS gate is quite artificial. It might make more sense to use parameterized $XX$ rotations with single-qubit $Z$ rotations. If the intention is to perform VQAs on near-term machines, then one might want to use more native instructions than the decompositions shown in Fig 5 and Fig 6.\\n- Related to the above point, the decomposition of the BS gate is addressed as a possible limitation. I am pretty sure a better decomposition is possible using $XX$, $YY$ and $ZZ$ rotations.\\n- I think this idea of preserving the Hamming weight is not very new, and the theoretical results are not very surprising to me. For this reason, I feel the contribution of this work is not so large, although the proposed ansatz appear to perform better than existing ones.\", \"Questions\": \"- In Equation 8, should the maximization be over $M$ such that $M\\\\geq M_c$?\\n- Where does the construction for the BS gate come from?\\n- Throughout this paper, it is stated that this is a \\\"revisiting\\\" of the Hamming weight preserving ansatz. What is the status of the previous work using these ansatz, and what are the specific improvements made in this paper compared to previous proposals?\"}"}]}
{"paper_id": "ulMXGO1fdH", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Spiking neural networks (SNNs) hold potential for energy-efficient machine learning by processing streaming data. Training SNNs on temporal sequences remains challenging, primarily done with backpropagation through time (BPTT), which is unsuitable for online learning. Alternatives like Real-Time Recurrent Learning (RTRL) provide exact gradients suitable for online learning but incur high computational costs, making them impractical for larger networks.\nGap: Current methods for approximating real-time gradient calculations, such as Online Spatio-Temporal Learning (OSTL) and Online Training Through Time (OTTT), restrict scaling and learning performance due to simplified assumptions about temporal dynamics, especially in deep networks. These methods often ignore the impact of previous neuronal outputs on downstream neurons, which leads to degraded learning performance in temporal sequence tasks.\nHypothesis: The authors propose that a novel approximation method, Online Training with Postsynaptic Estimates (OTPE), can improve online learning of feed-forward SNNs by capturing temporal effects on subsequent layers' activities that current approximations miss. This method would potentially provide better gradient alignment with exact calculations and enhance performance with minimal overhead.\nMethod: The authors developed OTPE, an algorithm for training feed-forward SNNs, which maintains a running weighted sum of the synaptic influence of past spikes, approximating the temporal effects in RTRL but with improved scalability. The algorithm applies a local layer-based derivative computation without surrogate derivatives when capturing post-synaptic temporal dynamics. They evaluated the learning performance on various datasets, including synthetic and realistic spiking datasets like Spiking Heidelberg Digits (SHD), and compared it against established algorithms such as BPTT, OTTT, and OSTL.\nConclusion: OTPE proved to have superior directional alignment with exact gradients and higher average performance in tasks requiring temporal credit assignment, such as the SHD. It demonstrated scalability and outperformed existing approximation methods like OTTT and OSTL in multi-layer configurations. This suggested that OTPE effectively captures temporal dependencies required for efficient online learning in SNNs.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"1. This paper considers facilitating online learning in SNNs, which is important for biological plausibility and neuromorphic hardware.\\n\\n2. Experiments demonstrate large improvements on gradient angle alignment and better performance than existing methods.\", \"Weaknesses\": \"1. The presentation can be improved. It is not quite clear what is the major component in the methodology that leads to the large improvement over existing methods. From Section 3, OTPE is very similar to OSTL and there is no explicit description of the key difference. From Section 3.1, AOTPE differs from OTTT only in that AOTPE uses a running weighted average of surrogate derivatives. Why this small change can almost double the cosine similarity given the similar time and space complexity? Is there more formal theoretical justification? From the derivation, OTPE also makes many approximations, why these approximations have less effect than OSTL/OTTT? Besides, what does the term \u201cpostsynaptic estimates\u201d mean?\\n\\n2. Scalability is mentioned multiple times in the paper. Are there more large-scale (datasets, network, etc.) results?\\n\\n3. In the abstract, \u201crate-based and time-based encoding\u201d is mentioned. However, there is no corresponding part in the paper.\", \"Questions\": \"See weakness.\"}"}]}
{"paper_id": "70A6oo3Il2", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Training deep neural networks towards zero training loss can lead to overfitting, as these networks often memorize entire training sets. Strategies to reduce overfitting include various regularization techniques. Recently, targeting a non-zero training loss threshold, termed as flood level, has been shown to improve generalization. However, existing flood regularization methods, such as the original Flood and iFlood, apply a constant flood level to all samples, assuming uniform sample difficulty.\nGap: Existing flood regularization methods treat all training samples as having similar difficulty by applying a constant flood level across the board. This does not account for samples with varying levels of uncertainty, noise, or difficulty, potentially limiting the effectiveness of the regularization.\nHypothesis: A flood regularization method that adapts the flood level based on individual sample difficulty can improve the generalization performance of models by preventing overfitting on noisy or hard-to-fit samples while allowing easier samples to converge towards lower training loss.\nMethod: The AdaFlood method involves using an auxiliary network to determine an adaptive flood level for each training sample based on its difficulty. This auxiliary network is trained on a subset of the training data, with a correction function applied to adjust predictions that might overestimate the loss of noisy samples. The adaptive flood levels are pre-computed before training the main network and can be shared across different runs. The method was tested on datasets with various input modalities, utilizing different neural network models and regularization techniques. Experimental comparisons were made against other flood regularizers on tasks including image and text classification, regression, and probability density estimation for asynchronous event sequences.\nConclusion: AdaFlood generally outperforms previous flood regularizers across a variety of tasks and is particularly robust to noise in the data. By adapting flood levels per sample difficulty, AdaFlood improves model generalization performance and calibration, demonstrating its versatility and effectiveness across different data modalities.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"-The proposed method is reasonable. Different samples has different difficulty. It is reasonable to use different loss.\\n-The paper is well-written and easy to follow.\", \"Weaknesses\": \"-This paper lacks experiments on large-scale dataset, such as ImageNet. CIFAR and SVHN are small datasets, which are very different from the real sense.\\n\\n-Does the adaptive strategy bring extra training cost? More detailed discussion is required.\", \"Questions\": \"See the weakness.\"}"}]}
{"paper_id": "s6bKLlF4Pe", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Reinforcement learning (RL) aims to train agents to perform tasks in environments by learning optimal policies that maximize rewards. Deep RL uses deep neural networks (DNNs) but is resource-intensive. Transfer learning allows agents to leverage knowledge from previous tasks to improve performance on new tasks. Previous work introduced successor features (SFs) as a means to disentangle environment dynamics from task-specific reward functions, potentially improving efficiency. However, theoretical foundations for SFs combined with deep RL remain underdeveloped, especially in the context of deep Q-networks.\nGap: The theoretical understanding of successor features (SFs) combined with deep learning models, specifically when using deep Q-networks (DQN) with SF and generalized policy improvement (GPI), is lacking. While empirical successes have been noted, formal convergence analysis and generalization guarantees are sparse, particularly in function approximation settings like DNNs.\nHypothesis: The authors propose that employing a successor feature framework with deep Q-networks (SF-DQN) and a generalized policy improvement operator (GPI) provides provable convergence and generalization guarantees, leading to improved transfer learning performance in RL compared to traditional methods like DQNs.\nMethod: The study involves developing a theoretic framework to analyze the convergence and generalization of SF-DQNs with GPI. This involves decomposing the Q-function into task-specific reward mapping and successor features using deep neural networks for approximation. The authors derive theories showing convergence rate and generalization, supported by numerical experiments on synthetic and real-world RL tasks. The theoretical analysis includes characterizing the distance between estimated and optimal Q-functions and comparing performance with traditional DQNs.\nConclusion: The authors conclude that SF-DQN with GPI significantly improves the convergence rate and transfer learning performance compared to traditional non-representation learning methods like DQNs. Theoretical results show SF-DQN converges with generalization guarantees, offering superior estimation accuracy for optimal Q-functions across related tasks. Numerical experiments corroborate these theoretical findings.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 1, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"Theoretical analysis of SF when combined with function approximation is quite interesting and can be insightful.\", \"Weaknesses\": \"- The first half of the paper is confusing as the authors seem to conflate optimal value/policy at the task level with some generic $Q^*$ and $\\\\pi^*$. It was hard to follow and figure out what they mean by the notations and/or possible notational mistake. \\n\\n- As an important point, note that the GPI theorem induces improvement for the case of several policies but **the same** reward (i.e., only one task). If there are multiple rewards, then maximum of their corresponding value functions is simply meaningless and does not represent any physical concept regarding a new reward function (see the comments bellow). [This is OK when using SF as the $w_i$ corresponds to the current task and only $\\\\psi_j$ comes from some previous task.]\\n\\n- There were a number of other typos (a proof read aside from the points mentioned above is strongly suggested).\", \"Questions\": \"- If you have multiple tasks, then what do you mean by $Q^*$ and $\\\\pi^*$? In fact, both $\\\\pi^*$ and $Q^*$ are not well-defined unless the reference for optimality is specified. For instance, you may define a total reward as the summation of all the task-level rewards, whose $\\\\pi^*$ and $Q^*$ would be different from the case where a total reward is a *weighted* sum of task-based rewards or some non-linear function of them. \\n\\n- Eq (13) is not a policy improvement operator (PI is to take an argmax of the current value to induce a new policy). \\n\\n- Last three lines of page 4 --> First, your reference to $Q^*$ is senseless. More importantly, this setup has nothing to do with your settings. Maximum of several $Q$ functions corresponding to various rewards may not even be a valid $Q$ function for a different reward signal. One can easily construct an MDP with say three tasks and two actions, where in a certain state, $a_1$ is the optimal action for task 3, while $\\\\max_{i\\\\in \\\\{ 1,2 \\\\} } Q_{i}$ give you action $a_2$. Indeed it is theoretically possible that $\\\\max_{i\\\\in \\\\{1,2\\\\}} Q_{i}$ gives you suboptimal action in *all* states. \\n\\n- Table 1 --> what is $\\\\phi_{i}(\\\\Theta^{*}_{i})$ ? $\\\\phi$ is supposed to be the feature vector. \\n\\n- Assumption 3 is unclear. Note that $\\\\pi(a|s) \\\\in [0,1]$, Hence, for sufficiently large $C$ this assumption is always true! Am I missing something?\\n\\n- Is (20) a s fair assumption? For most cases, $K$ would be a large number, so it looks like that the initial approximation of $\\\\psi_1$ must be already very good! Perhaps a better presentation could be that there has to be some pre-training for $\\\\psi_1$ before using this algorithm?\"}"}]}
{"paper_id": "x4OPJ7lHVU", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: In-context learning (ICL) enables large language models (LLMs) to adapt dynamically based on contextual exemplars, improving accuracy and relevance without fine-tuning, which is increasingly valued for integrating private data sources.\nGap: ICL responses can inadvertently leak sensitive private information contained in in-context exemplars, creating a privacy risk similar to traditional fine-tuning methods.\nHypothesis: The authors propose Differentially Private In-context Learning (DP-ICL), a paradigm that uses a noisy consensus mechanism among an ensemble of LLM's responses based on disjoint exemplar sets to generate differentially private responses.\nMethod: DP-ICL involves partitioning exemplar data into subsets, generating exemplar-query pairs, and processing these pairs through LLMs. The outputs are aggregated using differentially private mechanisms such as the Report-Noisy-Max with Gaussian noise for text classification, and embedding or keyword space aggregation for language generation tasks. Evaluation was conducted on text classification benchmarks (SST-2, Amazon, AGNews, TREC) and language generation tasks (DocVQA, SAMsum), showing a balanced trade-off between privacy and utility.\nConclusion: DP-ICL achieves a strong utility-privacy tradeoff, providing privacy guarantees while maintaining competitive performance compared to non-private methods. It paves the way for trustworthy and privacy-preserving applications of LLMs in sensitive domains.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 4, \"Contribution\": 4, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"- This paper is well-written and well-organized. \\n- The proposed paradigm is novel and is a novel paradigm for privatizing ICL tasks.\\n- The authors provide a clear and concise explanation of the DP-ICL paradigm. The figures and captions are easy to understand.\\n- Experiments on 4 benchmark datasets are extensive and sufficiently demonstrated the promise of the approach towards trustworthy usage of large language models.\", \"Weaknesses\": \"- Limitations can be discussed in more details. For example, \u201ca better-trained embedding-to-text model may yield further improvements in the model performances.\u201d Then what can be the potential candidate models here and why did the authors not adopt these models?\", \"Questions\": \"- How do you see the DP-ICL paradigm being applied in real-world scenarios, and what are some of the challenges that would need to be addressed?\\n- What are some of the limitations of the DP-ICL paradigm, and how might they be addressed in future research?\"}"}]}
{"paper_id": "rtzUW1FU3H", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large language models (LLMs) have shown significant promise in various NLP tasks, but they are limited by the fixed context window in the transformer architecture, which hampers them from effectively processing long input sequences. While many efforts aim to extend LLMs' context windows, assessing their real abilities to understand long contexts remains a challenge, necessitating high-quality benchmark datasets.\nGap: Existing benchmarks for assessing LLMs on long-text tasks suffer from several issues: shorter text lengths that do not meet modern models' capabilities, outdated documents included in training datasets, and an overemphasis on short dependency tasks. Real-world applications demand evaluation of long dependency tasks, which require integrative comprehension of information spanning entire documents.\nHypothesis: The paper posits that a high-quality benchmark, specifically designed and annotated to evaluate long-context understanding abilities of LLMs, can address the deficiencies of current datasets and aid in effectively assessing and advancing LLM capability in long-context comprehension.\nMethod: The authors introduce 'LooGLE,' a benchmark dataset comprising 145 contemporary documents (all post-2022) from diverse domains, each containing over 24k tokens paired with newly generated questions. LooGLE includes both short and long dependency tasks. For long dependency tasks, 1.1k QA pairs were manually created, cross-validated, and constructed to span the entire documents. The dataset was used to evaluate eight state-of-the-art LLMs, exploring how well they perform on long-context tasks relative to short-dependency tasks.\nConclusion: The evaluation demonstrated that commercial models surpass open-source models. LLMs perform adequately on short-dependency tasks but struggle with long-dependency ones. The introduction of retrieval-based methods improved performance, yet advancements in transformer architectures have not resolved long-context understanding challenges. LooGLE serves as an effective benchmark for assessing LLM long-context capabilities and informs future model development.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 1, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"- A curated dataset, thoughtfully designed with great efforts to prevent data leakage and ensure long dependency.\\n- Great efforts in assessing current state-of-the-art LLMs' long dependency capabilities such as information retrieval, reading comprehension and reasoning, computation and timeline reordering.\", \"Weaknesses\": \"- see questions.\\n- the paper is sprinkled with typos (refer to questions for a few).\", \"Questions\": \"- Is the benchmark english-only ? If so, this needs to be mentioned.\\n- How will the dataset be released ? For instance ZeroSCROLLS only released the inputs and evaluate through a system of leaderboard, will LooGLE be released the same way ? I'm concerned that revealing input/gold outputs pairs would lead to data leakage for future models.\\n- Concerning data collection, were the sourced documents (after 2022) subjected to any machine-generated verification ? I am concerned that ChatGPT-like texts might compromise the fairness of the evaluation.\\n- Are the open-source models instruction-tuned ? Commercial closed source models like GPT3.5 rely on RLHF or some instruction tuning techniques that enable them to better follow instructions. If the considered open-source baselines are not instruction-tuned, the comparison might be unfair since the prompt used for evaluation is the same and is instruction-based.\\n- Is it fair to have GPT4 both as baseline and evaluator ? I am also concerned about the creation of the dataset of short QAs (generated by GPT3.5). Is it fair to evaluate a model that was used to create the dataset ?\\n- In section 4.2, you mention human evaluation (3) but I cannot find any human evaluation both in the paper and the appendix.\\n- Section 4.3.2 is confusing, the experiments are based on the recent work from Liu et al. 2023, that accessing information in the middle of the document is more challenging for LLMs. If I understood correctly, you suggest concatenating the head and the tail of the inputs and give it to the LLMs as input. This would mean discarding the whole \\\"middle\\\" and leaving the beginning and the end. This puts an immediate limitation on information that can be retrieved by the LLMs. Also did you only use the same model (GPT4-32k) but with various context input length or different variants of GPT4 ? What is the last entry in Table 5 ? (GPT4). For long summarization, arxiv abstracts are highly biased towards the beginning of the article so it is expected that increasing context would result in a higher divergence between the generated summary (which will contain more and more details) and the (gold) abstract.\\n \\n**Typo**\\n- page 2: \\\"Mannually designed both short and long dependency tasks\\\"\\n- page 3: \\\"COmputation.\\\"\\n- page 7: \\\"Retrival\\\"\\n- page 8: \\\"GPT4 evalution\\\"\\n- table 4: \\\"Performence\\\"\\n- appendix page 3: \\\"Dispcrepancy\\\"\\n- not really typos but it is uninformative to report results of the order of \\\"e-300\\\" since at this point there is nothing to really compare.\"}"}]}
{"paper_id": "f4HohsyNEk", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Recent advancements in neural field representations, particularly those using volumetric rendering, have enabled impressive scene reconstructions with photorealistic quality but suffer from slow rendering and limited support for standard 3D applications. Volumetric methods generate meshes with poor topology, while differentiable rasterization methods produce higher quality meshes but struggle with initialization sensitivity. These challenges hinder efficient rendering and integration with existing graphics pipelines.\nGap: The problem with existing methods is the trade-off between mesh quality and rendering speed. Volumetric rendering provides detailed geometry but is slow and not easily compatible with standard 3D processing pipelines, while differentiable rasterization methods produce higher-quality meshes but suffer from poor initialization, making them less reliable.\nHypothesis: The authors propose a method, NeuManifold, that leverages neural volumetric rendering for initial geometry generation and refines these using differentiable rasterization. This approach can generate watertight manifold meshes that combine the strengths of volumetric and rasterization methods to produce high-quality meshes efficiently for rendering and use in various 3D applications.\nMethod: The proposed method consists of two main stages: 1) Initialization using neural volumetric fields to create an initial mesh structure, 2) Refinement through differentiable rasterization that utilizes a newly introduced Differentiable Marching Cubes (DiffMC) algorithm to enhance mesh quality and eliminate artifacts. This integrated approach improves rendering speed significantly, maintaining compatibility with 3D graphics applications.\nConclusion: The NeuManifold method delivers high-quality watertight manifold meshes that improve rendering speed by an order of magnitude compared to previous methods, while preserving detailed geometric quality. The meshes generated are compatible with modern rendering pipelines and suitable for downstream applications, including simulations. Despite potential challenges with specular surfaces, the method marks a significant advancement in efficient, high-quality 3D reconstruction.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"The writing quality is generally good, and well-articulated.  The paper provides thorough evaluation on multiple datasets, and comparisons to multiple baseline methods.  The end-to-end differentiability of the system allows refinement of the initial geometry based on a rendering loss, which improves the quality of the reconstruction result.  The implementation and engineering, such as deploying the optimized model in GLSL shaders, is notable.\", \"Weaknesses\": \"While the paper shows favorable results in terms of quality of reconstruction, it seems only about on par or marginally better than methods that it has been compared to.  It mostly connects components from existing works (TensoRF, nvdiffrast, Marching Cubes algorithm) into a well-engineered end-to-end pipeline, but I question whether the specific technical contributions of the paper are significant enough to stand on their own.\\n\\nThe paper claims that it is the first differentiable Marching Cubes implementation, yet it's not clear whether this is indeed the case (see for example: Deep Marching Cubes, MeshSDF, Neural Marching Cubes, Flexicubes).  Furthermore, the differentiable formulation of MC presented in the paper is compared only to DMTet, but not to any other methods that might be even closer in spirit (i.e. those mentioned in the previous remark).  Therefore, it is hard to evaluate the level of contribution.\", \"Questions\": \"- The paper strongly emphasizes the watertight manifoldness of the resulting meshes, but does not go into much detail about the specific technical contribution of this paper that results in this property.  Could you clarify whether this is indeed a contribution of this paper, or is it inherited from the other methods that the paper builds off of?\\n\\n- On page 4, it is stated that during the second, mesh optimization step, the *topology* is also optimized.  Could you elaborate/clarify what you mean by this exactly?\\n\\n- On page 5: \\\"given that most real-world objects tend to be axis-aligned\\\" -- could you briefly explain why this is a reasonable assertion to make?\\n\\n- SDF-based reconstruction methods (e.g. NeuS) claim that the raw density field is optimized only to satisfy the rendering objective (and is also biased away from the true surface), and therefore are not ideal for surface reconstruction.  In this paper, you argue that this is unnecessary and that using the density field directly is a better choice.  Could you elaborate on this choice?\"}"}]}
{"paper_id": "xh0XzueyCJ", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Diffusion models have shown transformative capabilities in graph generation, enabling efficient and realistic graph generation for various applications such as network optimization, social network analysis, and drug design.\nGap: Current diffusion models for graph generation lack precise control over non-differentiable constraints (hard constraints), which limits their applicability in practical scenarios where specific structural properties need to be controlled, such as in drug discovery.\nHypothesis: The authors propose that precise control over graph generation to satisfy hard constraints can be achieved by using projected diffusion methods, specifically PRODIGY, which allows for plug-and-play control without the need for retraining existing models.\nMethod: PRODIGY is a plug-and-play approach that samples from any pre-trained diffusion model, enforcing specified constraints by projecting the denoised samples at each timestep onto the constrained space. The method finds efficient projection operators for various graph constraints and uses a controlled sampling process that ensures specified hard constraints are met while maintaining data distribution fidelity. The method is tested on both generic and molecular graphs, improving constraint satisfaction significantly.\nConclusion: The proposed PRODIGY method successfully enables control over the generation of graphs to satisfy hard constraints, significantly improving performance on constraint satisfaction in both generic and molecular graph generation tasks. The method shows flexibility and effectiveness, maintaining proximity to the original data distribution while satisfying constraints.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 3, \"Strengths\": \"1. Precise control with arbitrary constraints is a desired property for graph generation. The authors propose a general solution for certain classes of constraints. The authors derive closed-form solution for many commonly used constraints under the proposed projection approach. The detailed derivation may also inspire future research where new constraints may appear.\\n2. The empirical results are comprehensive. The authors show the proposed method can effectively control the generation direction towards graphs that satisfy the constraints, on both 2D graphs and 3D graphs. They further demonstrate the sensitivity of the approach to the interpolation value $\\\\gamma_t$$. Analysis on the efficiency of the approach is also provided.\", \"Weaknesses\": \"1. Though when the constraints cover all the graphs in the test set, the generated distribution is largely unaffected, in scenarios where the constraints do take effect, the generated distribution obviously deviates from the original distribution. And the deviation is model-sensitive. For example, in Table 2, with edge count as the constraint, the performance of GDSS downgrades by two folds on Clus. while EDP-GNN is largely unaffected. Also, on the dataset of Enzymes and Grid, both GDSS and EDP-GNN suffer a lot from the constraints. Furthermore, Figure 12 shows a lot of unrealistic molecular graphs (e.g. very large rings, disconnected graphs, very long bridging bonds) with the constraints on atom count.\\n2. The proposed projection paradigm main only applied to constraints with simple calculation process so that the optimization problem has a closed-form solution. When the constraints become complicated (e.g. involves non-linearity), it might be arduous or impossible to find such closed-form solution.\", \"Questions\": \"1. Maybe a typo in section 4 right under equation 3: $\\\\Pi_c(z)\u00a0=\u00a0\\\\arg\\\\min_{z\\\\in\u00a0c}||z-x||^2$\uff0cshould be $\\\\Pi_c(x) = \\\\arg\\\\min_{z\\\\in c}||z-x||^2$\"}"}]}
{"paper_id": "ruQkcBfzpm", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Modular vision-language models (Vision-LLMs) align pretrained image encoders with large language models (LLMs), offering a more computationally efficient alternative to end-to-end training of large vision-language models from scratch. Such end-to-end training is costly and usually focused on English due to the abundance of available high-quality English image-text data.\nGap: The field has chiefly focused on English-only Vision-LLMs, while multilingual models are primarily obtained via expensive end-to-end pretraining. This results in smaller model sizes and limitations due to the scarcity of multilingual image data combined with text-only multilingual corpora.\nHypothesis: By leveraging a pretrained multilingual LLM and machine-translating high-quality English vision-and-language data, it is possible to efficiently create a multilingual Vision-LLM using significantly fewer resources compared to traditional methods.\nMethod: The authors developed mBLIP by realigning an image encoder, initially designed for English, to a multilingual LLM. They leveraged a mix of vision-and-language tasks, using machine-translated high-quality English data to 95 languages. The training process utilized parameter-efficient methods and ran on consumer-grade hardware, handling only a few million training examples and adjusting only 124 million parameters.\nConclusion: mBLIP demonstrated competitive performance with state-of-the-art models on the IGLUE benchmark and outperformed certain larger models like PaLI-X in image captioning tasks, despite being trained on magnitudes less data and fewer parameters. This supports that aligning a multilingual LLM with a well-aligned image encoder can yield efficient and effective multilingual Vision-LLMs.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 4, \"Strengths\": \"The approach is interesting in the sense that it covers a reasonable use case, that of multi-lingual LVLM by aligning frozen models. It seems like the authors found a good space where a solution did not exist in the literature.\\n\\nThe proposed approach seems technically reasonable. \\n\\nThe paper is mostly clear, the target application is clear and the explanations are usually clear (although some issues are flagged later on). \\n\\nResults are positive, although there isn't a lot to compare against.\", \"Weaknesses\": \"The paper does not seem to have a strong technical contribution. On the pro side, I like the application, and there's some analysis of the impact of the different pieces in Table 3, which offsets somewhat the lack of technical contribution.\\n\\nI am a bit unsure about the impact of LoRA. A task prompt has much lower capacity to adapt to any downstream task compared to adding LoRA (especially if LoRA adapters are added to every 1x1 layer). So I'm wondering if part of the performance is due to LoRA vs no LoRA.  \\n\\nAlso, if there is enough data to train LoRA adapters, why then there is no data to train a task prompt? \\n\\nis \\\"re-aligning\\\" actually fine-tuning?\", \"Questions\": \"Any answers to the \\\"weakness\\\" above. In particular: did I miss something with regards to novelty? can the use of LoRA be better justified?\"}"}]}
{"paper_id": "FNq3nIvP4F", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Recent advances in video generation have shown promising results in producing realistic content. However, these AI-generated videos are typically short, known as 'shot-level', and depict only one scene. For more extensive applications like storytelling, there is a need for generating coherent, long videos, or 'story-level' videos, which require creative transitions and predictions across different clips.\nGap: Current video generation methods focus on producing short videos, lacking the ability to generate long, coherent videos with smooth scene transitions. Techniques for scene transitions in traditional film rely on predefined methods which lack flexibility and creativity.\nHypothesis: The authors propose SEINE, a short-to-long video diffusion model that aims to generate high-quality, long videos. The model focuses on creative transition and prediction, ensuring coherence and satisfying the requirements of artistic storytelling across multiple scenes.\nMethod: The proposed SEINE model utilizes a random-mask video diffusion approach to generate transitions between scenes based on textual descriptions. By inputting the initial and final frames of scenes, along with text prompts, the model creates smooth transitions. SEINE is built on a pre-trained diffusion-based text-to-video model and includes a random-mask module to handle transitions and predictions. The model is trained using a combination of the WebVid10M dataset and other watermark-free datasets to enhance output quality. Performance is evaluated on metrics including temporal consistency, semantic similarity, and video-text semantic alignment through experiments.\nConclusion: SEINE demonstrates the ability to generate high-quality, long videos with creative transitions that maintain coherence and visual quality. The approach outperforms existing methods on various metrics, showing potential applications in long video generation, image-to-video animation, and autoregressive video prediction.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"This article addresses the limitation of existing models that can only generate shot-level videos and proposes a method to generate story-level videos using transitions. They extend an existing video generation framework and achieve impressive results in generating long videos. They also propose a reasonable evaluation framework to assess the proposed model, and a large number of demos and quantitative evaluations demonstrate the effectiveness of their approach. The contribution of this work is significant.\", \"Weaknesses\": \"The author should provide a more detailed description of the model for reproducibility, including training resources, training parameters, and so on. Additionally, the author should also report scores on commonly used evaluation metrics such as FID.\", \"Questions\": \"I wondered how many GPUs they used and how long it takes for training. Besides, as far as I know, FID is used to evaluate video generation quality in many papers, can they provide this to make their paper more solid?\"}"}]}
{"paper_id": "QQ5eVDIMu4", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Graph Neural Networks (GNNs) have shown effectiveness in processing data from specific domains, but their ability to handle diverse and unseen distributions remains a challenge. Existing literature addresses specific types of distribution shifts like graph size or node degree, which are limited when handling multifaceted, real-world distribution shifts.\nGap: Previous methods primarily focus on single-type distribution shifts, which don't account for complex, nuanced shifts present in real-world datasets. These shifts can involve multiple aspects simultaneously, such as node interactions and feature alterations, which conventional methods fail to accommodate, hindering GNN's generalization.\nHypothesis: The authors propose that graph distribution shifts can be considered as a mixture of components, with each component representing different types of shifts. Tackling distributional challenges by decomposing them into these components could improve GNN generalizability across complex shifts.\nMethod: The proposed GraphMETRO framework incorporates a mixture-of-experts (MoE) architecture, consisting of a gating model and multiple expert models aligned in a common representation space. The gating model identifies significant mixture components influencing the distribution shift, while each expert generates representations invariant to a specific type of component. These are aggregated to produce a representation that accounts for the complex distribution shift. The method is validated through experiments on synthetic and real-world datasets showing better performance over baseline methods.\nConclusion: GraphMETRO effectively handles complex distribution shifts by leveraging a mixture-of-experts approach. It outperforms existing methods on synthetic shifts and achieves state-of-the-art performance on real-world datasets, demonstrating its potential for improving GNN generalization capabilities in practical applications.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"- The paper aptly addresses OOD as a crucial issue for GNNs, pinpointing graph shift heterogeneity as the core challenge.\\n- Real-world datasets back the claims through experiments.\\n- Thorough ablation studies validate the learned graph shifts, a commendable effort.\", \"Weaknesses\": \"- The motivation behind the proposed method is not adequately substantiated. The primary basis given is that \\\"previous research has concentrated on addressing specific types of distribution shifts.\\\" However, this overlooks a plethora of prior works in the field. Contrary to the suggestion that graph shift heterogeneity is under-explored, numerous studies have delved into learning the \\\"environment generators\\\" for GNNs to detect graph shifts, as exemplified by [https://arxiv.org/abs/2202.02466]. Other works have focused on learning shift-specific transformations, such as [https://arxiv.org/abs/2211.02843]. Consequently, there exists a wide spectrum of approaches to tackle graph shift heterogeneity. The choice of approach in this paper, especially the emphasis on MOE, requires a more detailed and robust justification to elucidate its relevance and significance.\\n\\n- The presented assumption seems overly broad and lacks specificity. Additionally, the architectural design appears to be somewhat arbitrary. Consequently, it's challenging to discern the functionality, its underlying mechanism, and its improvements over existing methods.\\n\\n- The proposal is insufficient in its details, particularly concerning the implementation of specific model architectures, stochastic transformation, and the optimization process. Given the inclusion of shift learning midway and data augmentation initially, one would expect a more intricate optimization strategy than standard routines.\", \"Questions\": \"Please check Weaknesses.\"}"}]}
{"paper_id": "eGdhD93hZr", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Voice synthesis has seen significant advancements with the development of various applications, such as text-to-speech and voice conversion, usually built independently with distinct methodologies. The majority of current models rely heavily on annotated data. There is a growing interest in scaling these models to handle the diverse acoustic variations of human voice using self-supervised datasets, which can capture speaker identity, emotion, and prosody effectively.\nGap: The challenge lies in achieving zero-shot generalization for voice synthesis models when test data has acoustic diversities unseen during training. Additionally, existing models are developed in isolation for specific applications, resulting in scattered methodologies that lack a unified approach to voice generation and manipulation across languages and tasks.\nHypothesis: The authors propose MVoice, a large multimodal language model that can synthesize and manipulate multilingual voice signals at scale. The hypothesis is that MVoice, with its discrete token-based 'coarse-to-fine' design, can resolve scaling issues, provide superior voice quality, and adapt to various voice synthesis tasks in a unified framework without needing extensive annotations.\nMethod: MVoice employs a 'coarse-to-fine' design using self-supervised learning techniques: (1) Semantic Stage, which extracts semantic tokens from text or speech input to determine desired meanings, and (2) Acoustic Stage, which generates acoustic tokens conditioned on semantics. The model leverages a dataset of approximately 200K hours in 6 languages and covers various voice synthesis applications. MVoice uses a decoder-only transformer architecture to map inputs to semantic and acoustic tokens and integrates a unit-based vocoder to generate high-fidelity waveforms.\nConclusion: MVoice demonstrates state-of-the-art results in multilingual and cross-lingual zero-shot voice generation. It excels in generating high-quality, stylistically similar audio across various applications without requiring extensive supervisory data. The study highlights MVoice's scalability and control flexibility for handling diverse voice synthesis tasks. The model, however, has limitations concerning audio token dependency and computational demands for longer sequences.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"- Originality: \\nEven though discrete unit-based generative models been investigated before (e.g., Polyak et al. 2021, https://arxiv.org/pdf/2104.00355.pdf), this paper adds AR modeling and controllability to obtain a more unified model in terms of the variety of that can be performed with the same model. Some advantages of the model are\\n1. Experiments usually achieve good performance on several speech tasks as compared to the previously published studies. \\n2. Being controllable is an advantage of the model to adapt it to different use cases. \\n3. The modularity (semantic stage and then an acoustic stage) allows the model to perform different tasks.  \\n\\n- Quality: \\nThe paper proposed a two-stage speech model (semantic + acoustic models) which makes it somewhat interpretable. Experiments have been conducted on various tasks and the performance of those are on par with that of previous studies if not better. \\n\\n- Clarity: \\nAt a high-level, the paper is relatively easy to understand, but in terms of details, some parts have been left to the Appendix and it might make the paper difficult to read at times. \\n\\n- Significance: \\nSimilar to the comments in Originality above. This is a study towards investigating unified generative speech models.\", \"Weaknesses\": \"1. Experiments do not always show gain, and when they outperform, the improvements are limited. \\n\\n2. Even though the paper title mentions multilingual unified speech representations, the results on multilingual condition are worse than the monolingual ones. \\n\\n3. Overall, the main gains are coming from improved speaker similarity scores and not in terms of other evaluation criteria. \\n\\n4. The paper already mentions a few shortcomings and these are valid concerns. Regarding the dependency of the performance on the tokenizer quality makes the reason for using discrete tokens questionable. If there were continuous features, would it be better? If so, then wouldn't the first stage lose its property, i.e. being interpretable as semantic stage?\", \"Questions\": \"1. If there were continuous features, would the model be better? If so, then wouldn't the first stage lose its property, i.e. being interpretable as semantic stage?\\n\\n2. Which ASR models have been used to obtain the WERs on the synthetic datasets?\"}"}]}
{"paper_id": "6CZ50WgfCG", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Reinforcement Learning (RL) heavily relies on dense reward functions, often requiring significant human expertise to craft. Sparse rewards, while easier to obtain, pose challenges for RL due to exploration difficulties.\nGap: Existing methods for reward learning, such as Inverse RL and Adversarial Imitation Learning, do not focus on reward reuse for new tasks. These methods often result in rewards that cannot be effectively transferred to different tasks due to diminishing differences between agent and demonstration trajectories over time.\nHypothesis: The authors propose that it is possible to learn dense rewards for multi-stage tasks in a data-driven manner that are reusable across tasks with similar success conditions, reducing the human effort required to engineer rewards.\nMethod: The authors introduce DrS (Dense reward learning from Stages), which learns reusable dense rewards by leveraging the stage structures of tasks. The approach involves training discriminators to classify success and failure trajectories based on binary sparse rewards and stage indicators, with the discriminators providing dense rewards. The method is evaluated on the ManiSkill benchmark, across three task families: Pick-and-Place, Turn Faucet, and Open Cabinet Door, involving over 1000 task variants.\nConclusion: DrS effectively learns high-quality dense rewards that are reusable in unseen tasks, improving the performance and sample efficiency of RL algorithms compared to sparse rewards. The learned rewards achieve comparable results to human-engineered rewards in some tasks and substantially reduce the human effort needed in reward engineering. The method is robust to various stage configurations and has shown compatibility with different modalities of reward input.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"- This paper makes a contribution towards automating reward design, which is of paramount importance in the field of RL. Having access to dense rewards takes the burden off of exploration, which in turn reduces the number of samples required to solve a task.\\n- The method is a niche application of contrastive discriminator learning, which is well-established in the literature.\", \"Weaknesses\": \"- The method requires success and failure trajectories for each stage in the training data, which can be expensive to collect. \\n- The scope of the method is limited to a family of tasks that can be divided into stages. This prevents it from being applied to other tasks such as locomotion. It also means the method is less general compared to LLM-based rewards with external knowledge [1, 2].\\n- Similarly, the need for stage indicators prevents the method from scaling to real-world problems, which would arguably benefit more from automated reward design than simulated domains.\\n- The experiments do not demonstrate new skills that are unachievable by human-designed rewards.\\n\\nReferences:\\n1. Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, and Tao Yu. Text2reward: Automated dense reward function generation for reinforcement learning. arXiv preprint, 2023.\\n2. Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi \u201cJim\u201d Fan, Anima Anandkumar. Eureka: Human-Level Reward Design via Coding Large Language Models. arXiv preprint, 2023.\", \"Questions\": \"- Does the method work with pixel observations?\\n- If the method only works with states, does generalization come as a result of state abstraction or the method itself?\\n- Do the learned dense reward functions resemble hand-designed rewards?\"}"}]}
{"paper_id": "ZEZ0CPmoSI", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Non-convex optimization is a significant challenge in machine learning, especially when tackling high-dimensional or non-linear problems. Traditional methods often simplify these problems into convex ones, despite the practical benefits of directly handling non-convex optimization. The concept of matrix smoothness extends typical smoothness assumptions by leveraging matrix information, offering potential improvements in supervised model training, including communication efficiency in distributed systems using compressed gradient descent (CGD).\nGap: Existing approaches struggle with efficient optimization of smooth and non-convex functions, particularly regarding convergence and the impact of stepsize in both single-node and distributed settings. Traditional scalar stepsizes in compressed gradient methods do not effectively exploit the structure of non-convex problems.\nHypothesis: Utilizing a matrix-valued stepsize in compressed gradient descent algorithms for minimizing matrix-smooth non-convex objectives can improve convergence rates compared to scalar stepsizes. These matrix stepsizes can be tailored in a layer-wise manner to exploit the underlying model structure, potentially enabling 'compression for free.'\nMethod: The paper proposes two CGD variants with matrix stepsizes, analyzed in both single-node and distributed settings. It introduces a novel 'layer-wise' design for compression and stepsize matrices, particularly useful in neural networks. Theoretical analysis focuses on the convergence properties of the algorithms, employing assumptions on matrix smoothness and boundedness of functions. The proposed methods are demonstrated with empirical evidence of faster convergence compared to scalar alternatives.\nConclusion: Matrix-valued stepsizes in CGD algorithms improve convergence for non-convex optimizations, particularly when matched with model structures via layer-wise design. The results show practical gains in communication efficiency in distributed settings, supporting the idea of 'compression for free.' Empirical results validate the theoretical findings, demonstrating better performance than traditional methods.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"The paper investigates matrix step sizes of compressed gradient descent and provides convergence analysis.\", \"Weaknesses\": \"see questions.\", \"Questions\": \"1) The paper claims that the matrix step size improves performance but introduces problems. The presence of the neighboring term in Theorem 3 creates a significant neighborhood when $\\\\Delta^{inf}$ is large.\\n2) This paper uses the compressed gradient as shown in (3), which is a direct compression on the full gradient. Given the prevalence of error feedback techniques in handling compression errors, it is pertinent to consider whether the proposed matrix step size can be extended to incorporate error feedback methods. \\n3) The paper does not consider the use of SGD, local update, and partial participation techniques in distributed settings. \\n4) The paper introduces det-CGD1, but it is unclear why this algorithm, specifically in a distributed setting, can obtain Algorithm 1.\"}"}]}
{"paper_id": "8Wuvhh0LYW", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Large language models (LLMs) such as GPT-4 and LLaMA have shown remarkable performance across various natural language processing tasks, and are considered precursors to artificial general intelligence. However, their deployment in real-world applications is severely limited by high memory and computation requirements. Recent research on quantization methods, notably post-training quantization (PTQ), has aimed to reduce these resource demands.\nGap: Existing quantization methods in LLMs face significant performance degradation when applied to extremely low-bit quantization settings such as W4A4 or W2A16. These methods often rely on handcrafted quantization parameters, which can result in lower performance. Quantization-aware training (QAT), although more precise, incurs high computational costs, making it impractical for LLMs.\nHypothesis: The authors propose that an improved quantization technique called Omnidirectionally Calibrated Quantization (OmniQuant) can achieve higher performance across diverse quantization settings while maintaining the efficiency of PTQ. By integrating gradient optimization via Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET), the method can learn optimal quantization parameters, overcoming limitations of traditional PTQ approaches.\nMethod: OmniQuant introduces a new quantization technique for LLMs, utilizing a differentiable, block-wise error minimization framework. The method features two key components: Learnable Weight Clipping (LWC), which optimizes clipping thresholds for weight quantization, and Learnable Equivalent Transformation (LET), which shifts the quantization burden from activations to weights. This approach allows for efficient optimization over both weight-only and weight-activation quantization settings. Experiments were conducted on various models including the LLaMA family, with tests of differing bit-width configurations to validate performance.\nConclusion: OmniQuant successfully addresses the challenges of low-bit quantization in LLMs, demonstrating superior performance over traditional PTQ methods across a range of quantization configurations. It offers improvements in memory efficiency and inference speed on actual hardware setups. The research suggests that OmniQuant makes the quantization process more adaptable and efficient, potentially paving the way for more practical deployment of large language models.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 2, \"Contribution\": 3, \"Rating\": 8, \"Confidence\": 5, \"Strengths\": \"The biggest contribution of this work is the learnable weight clipping (LWC). \\n\\n1. When compared to AWQ [1] which only optimize the scale, LWC uses the same objective function (minimize output distortion) but also handles clipping.\\n\\n2. When compared to PACT, which only applies to activation with positive values, LWC is more general and applies to weight, activation, and negative values.\\n\\n3. When compared to LSQ, which is a combination of multiple optimization goals (step-size, gradient scaling, and clipping), LWC is straightforward to implement and speedy to train.\\n\\n[1] Lin, J., Tang, J., Tang, H., Yang, S., Dang, X. and Han, S., 2023. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. arXiv preprint arXiv:2306.00978.\", \"Weaknesses\": \"1. While I highlight the advantage of LWC over AWQ, PACK, and LSQ, this evaluation for PACK and LSQ is lacking.\\n\\n2. Authors should present Figure 1 (a) in a quantitative way. It lacks actual numbers for cost of time and performance.\\n\\n3. Learnable Equivalent Transformation (LET) isn't novel and can be seen in common quantization kernels such as LLM.int8. I suggest authors to elaborate more on comparison with PACK and LSQ instead of LET.\\n\\n4. The latency benchmark only contain fp16 and OmniQuant. Strong baseline such as GPTQ is lacking.\", \"Questions\": \"1. How does OmniQuant runtime efficiency compare to GPTQ and AWQ?\\n\\n2. What is the performance when compared to PACK and LSQ?\"}"}]}
{"paper_id": "kaAtQwhnM2", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Energy-based models (EBMs) are valuable for probabilistic modeling because of their flexibility in different domains. They are commonly used with continuous data using methods like contrastive divergence, which involves MCMC sampling. Training EBMs in discrete spaces is challenging due to the intractability of normalization and sampling difficulties.\nGap: Training EBMs on discrete data is problematic because existing approaches, like contrastive divergence, rely on MCMC sampling, which lacks theoretical guarantees and results in biased energy landscape estimates.\nHypothesis: The paper proposes Energy Discrepancy as a method to train discrete EBMs, which eliminates the need for complex sampling methods like MCMC by relying on energy function evaluations at data points and their perturbations with theoretical guarantees.\nMethod: The authors propose using Energy Discrepancy, which involves perturbations based on Bernoulli noise, deterministic transforms, and neighborhood structures, requiring only the evaluation of the energy function at perturbed data points. The method uses importance sampling with uninformed and gradient-informed proposals for estimating energy discrepancies, improving on contrastive sample constructions without demanding sampling.\nConclusion: The proposed method effectively trains EBMs across various discrete tasks without MCMC, achieving competitive results in Ising models, discrete density estimation, and discrete image modeling, while allowing for parallel evaluations, reducing computational demands, and providing theoretical consistency.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 4, \"Presentation\": 4, \"Contribution\": 4, \"Rating\": 8, \"Confidence\": 4, \"Strengths\": \"Very thorough ablations are presented in the appendix, investigating the effects of a number of parameters to configure their method.\", \"Weaknesses\": \"The results on discrete image modelling are not very strong, either in terms of sample quality or in terms of estimated NLL.\", \"Questions\": \"I\u2019m curious about the wall-clock speed of training. Your approach admits better parallelisation than existing techniques since it relies on importance sampling rather than MCMC. Do you have any wall-clock timing experiments investigating this? For example, how does the wall-clock time compare of your approach vs. CD-10? \\n\\nMust the GwG sampler be tuned differently when training with ED vs. GwG? i.e. do you find that sampling is less efficient at test-time when a model is trained with ED?\"}"}]}
{"paper_id": "QFo2wxQEW6", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Endovascular interventions are used for diagnosing and treating vascular diseases using flexible tools like guidewires and catheters. Recently, robotic systems have been developed to assist in these interventions, allowing remote operation by surgeons. However, these systems generally follow a master-slave model, heavily relying on surgeons and lacking autonomous capabilities. Current progress is limited due to the reliance on closed-source simulators and the difficulty in obtaining large-scale datasets for machine learning training due to costly and complex medical setups.\nGap: Progress towards autonomous catheterization is hindered by the widespread use of closed-source simulators and physical phantoms. Additionally, it's challenging to acquire large-scale datasets needed for training machine learning algorithms due to the expensive medical procedures involved.\nHypothesis: Developing an open-source, real-time simulator for endovascular interventions can significantly advance research and development in autonomous catheterization systems by facilitating machine learning algorithm training and addressing the limitations of existing closed-source environments.\nMethod: The authors introduce CathSim, the first open-source, real-time simulator specifically for endovascular interventions. CathSim is built on MuJoCo due to its computational efficiency and machine learning ecosystem integration. The simulator includes anatomically accurate models from Elastrat and supports AR/VR integration. It features a modular, upgradable design with high-fidelity visualizations and realistic guidewire simulation. CathSim is validated through experiments comparing its force measurements with real-world scenarios, using a load cell in physical models. The simulator also integrates an expert navigation network using Convolutional Neural Networks (CNN) and Multi-Layer Perceptrons (MLP) to process multimodal inputs, facilitating autonomous learning tasks such as imitation learning and force prediction.\nConclusion: CathSim provides a reliable open-source platform for simulating endovascular interventions, capable of mimicking real-world scenarios. The expert navigation network developed based on CathSim demonstrates effective performance in tasks like imitation learning and force prediction, suggesting the simulator's potential to accelerate autonomous catheterization research. The simulator allows for diverse algorithm development and is an invaluable asset for the medical simulation community.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 5, \"Confidence\": 3, \"Strengths\": \"* Open source simulator\\n* Simulator is validated via comparison to real world robot\\n* Expert navigation results beating the human baseline\", \"Weaknesses\": \"* ENN model architecture is not state-of-the-art, e.g. no transformer\\n* Unclear how trustworth the user study is. 10 participants in user study is a small sample size and it\u2019s unclear how they were recruited and how trustworthy their judgements are.\\n* Lack of relevant details, e.g. what is the expert policy that the ENN is trained with. Why don\u2019t you show results for the expert policy in the table?\\n* If an expert policy exists, why do you train an ML model (ENN) model for the expert navigation task?\\n* Some figures / tables are not easy to understand on their own, e.g. Table 2, for which e.g. instead of Q1 you could write \u201cAnatomical accuracy\u201d.\", \"Questions\": \"Overall, this seems to be very relevant work. However, in terms of ML it is not pushing the boundary of the state-of-the-art. This work might fit better to a more applied conference.\"}"}]}
{"paper_id": "SUUckMrHNY", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Camouflaged instance segmentation (CIS) is a challenging task within computer vision, as camouflaged objects blend seamlessly with their background, making standard detection and segmentation difficult. Transformers have been successful in various vision tasks but require large-scale data. Recent advancements have produced large datasets for camouflaged object detection, such as COD10K and NC4K, allowing transformers to achieve superior performance in this field. However, current models struggle with boundary precision, often misidentifying parts of camouflaged objects as distinct entities.\nGap: Current transformer models for camouflaged instance segmentation do not accurately identify the boundaries of camouflaged objects. This limitation is due to a primary focus on object detection rather than boundary precision, resulting in multiple object predictions for single camouflaged targets when edges are unclear.\nHypothesis: Incorporating traditional computer vision descriptors, specifically Local Binary Patterns (LBP), into the transformer model will improve boundary detection for camouflaged objects. By extracting edge-sensitive local features through the proposed local-feature-aware transformer ('\u03b1-Former'), the performance on camouflaged instance segmentation tasks can be enhanced.\nMethod: The \u03b1-Former integrates traditional descriptors with transformers using a unique pipeline. It uses a binary filter (inspired by LBP) to extract edge-sensitive local features, and a feature aggregation adapter (FAA) to incorporate these features into the transformer without altering its input distribution. The model is evaluated against datasets COD10K and NC4K with an encoder-decoder design that includes an edge-aware feature fusion module, enhancing boundary predictions. A comprehensive set of ablation studies validates the contribution of each module to performance improvements.\nConclusion: The \u03b1-Former outperforms existing state-of-the-art solutions in camouflaged instance segmentation on the COD10K and NC4K datasets by 2-3 average precision points. The combination of binary filters, feature aggregation adapters, and edge-aware modules effectively enhances boundary prediction and segmentation accuracy of camouflaged objects, demonstrating the feasibility and robustness of integrating traditional descriptors into transformer-based frameworks.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 5, \"Strengths\": \"* State-of-the-art performance on the camouflaged instance segmentation (CIS) task.\\n* Reasonable module design, such as learnable binary filter.\", \"Weaknesses\": \"* The effectiveness of the feature aggregation adapter is limited.\\nAccording to Table 2 in this paper, using adapter has a negative impact on the performance of traditional descriptors on NC4K. For example, AP of (LBP w/o adapter) 42.288, AP of LBP w/ adapter 41.794. And same situtation for Circle-LBP. Even for the Binary filter, the improvement is very limited. (42.610->42.936).\\n* Typo: There exists a mismatch figure caption between Figure 7 in this paper and Figure 2 in supplementary_material. (Predicted edge and Ground truth).\\n* Complex pipeline and missing ablation study: This paper introduces attention module, positional encoding, encoder, decoder, but missing results to demenstrate the effectiveness of each components and how each component help the camouflaged instance segmentation (CIS) task.\", \"Questions\": \"See Weakness.\"}"}]}
{"paper_id": "8Cc6qOPvFo", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The task of text-driven image editing involves translating a source image into a target domain while preserving the original image's structure or background. Traditional methods using Generative Adversarial Networks (GANs) have been effective, with approaches like StyleCLIP leveraging pretrained models for enhanced image translation. Recently, diffusion models have gained traction, offering promising results for text-image generation tasks by leveraging pretrained diffusion models such as Stable Diffusion.\nGap: Existing diffusion model-based methods for text-driven image editing often require significant computational resources and fine-tuning, which is computationally intensive. Furthermore, naive application of diffusion models can degrade the structural integrity of the source image during translation.\nHypothesis: The authors propose that a training-free, cycle-consistency-driven metric learning approach can effectively leverage pretrained diffusion models for text-driven image-to-image translation, preserving the original image's structure and background without the need for extensive computational resources.\nMethod: The method involves revising the naive forward and backward processes of diffusion models. Instead of fine-tuning, the authors propose two guidance terms during the translation process: a distance objective based on triplet loss to encourage alignment with target prompts and maintain source image structure, and a cycle-consistency objective that iterates between optimizing source and target latents. The approach employs the DDIM framework and utilizes guidance terms that integrate CLIP's text-image similarity scores and stable diffusion's latent space navigation. The experiments are conducted using Stable Diffusion on various tasks with publicly available datasets, avoiding additional fine-tuning.\nConclusion: The proposed method achieves superior performance in text-driven image-to-image translation tasks, effectively preserving the source image's structure and improving alignment with target text prompts. The cycle-consistency objective further enhances image preservation, demonstrating its effectiveness across various experimental settings.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"- **Simplicity & Effectiveness**: The proposed method is both straightforward and seemingly efficacious, as evidenced by the results presented in the paper.\", \"Weaknesses\": \"- **Evaluation Methods**: The evaluation could be more robust. The prompts used for evaluation are closely related to the original noun, reducing diversity and potentially biasing results.\\n- **Aspect Ratio Concerns**: The samples used for evaluation have been altered from their original aspect ratios. This could inadvertently disadvantage competing methods.\\n- **Comparison Choices**: The results from prompt-to-prompt evaluations seem to perform well on generated images rather than inverted real ones. The absence of a comparison with Null-text-inversion, which might be a more apt benchmark, raises questions.\\n- **Efficiency Metrics**: The paper would be more informative with a runtime efficiency comparison against other methods.\", \"Questions\": \"None\"}"}]}
{"paper_id": "MSe8YFbhUE", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Visual reinforcement learning (RL) has been applied to continuous control tasks using high-dimensional pixel inputs. There have been advances in areas like data augmentation, self-supervised representation learning, and high update-to-data ratios to improve sample efficiency of RL agents.\nGap: Existing visual RL methods often result in agents being motorically inactive in early training phases, preventing effective exploration. This inactivity is correlated with inactive neurons in the policy networks, limiting the agent's ability to learn efficiently.\nHypothesis: The decline in dormant ratio\u2014a measure of inactive neurons in a neural network\u2014acts as an early indicator of successful skill acquisition in RL agents, even before rewards increase. By minimizing this dormant ratio, the exploration-exploitation trade-off can be better managed, resulting in improved performance.\nMethod: The proposed DrM method includes three main components: (1) Periodic perturbation of neural network weights guided by the dormant ratio, using soft resets for refreshing weights when the dormant ratio is high; (2) A self-adaptive exploration noise schedule that adjusts noise based on the dormant ratio; (3) An exploitation mechanism that prioritizes actions based on a lower dormant ratio, allowing the agent to emphasize exploitation as it becomes more capable. Extensive experiments were conducted across various domains such as DeepMind Control Suite, MetaWorld, and Adroit, comparing DrM against state-of-the-art baselines like DrQ-v2, ALIX, and TACO.\nConclusion: DrM demonstrated significant improvements in sample efficiency, asymptotic performance, and robustness across tested environments. It was notably successful in solving previously unsolved tasks in Dog, Manipulator, and demonstration-free dexterous hand manipulation tasks in Adroit based solely on pixel observations.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 3, \"Presentation\": 3, \"Contribution\": 3, \"Rating\": 6, \"Confidence\": 4, \"Strengths\": \"The greatest strength of the paper is the significance of the results. DrM is the clear winner on *all* of the tasks reported on in the paper, and in some domains (particularly Adroit) it achieves a step change improvement. I like that the authors have considered three separate domains (DeepMind Control Suite, MetaWorld and Adroit), with multiple tasks from each.\\n\\nThe main idea of treating the dormant ratio as a progress proxy, and hence using it as the basis for adaptive hyperparameter scheduling, makes intuitive sense. While I have some minor qualms about the writing (see below), the paper was clear and easy-to-read overall. Lastly, the coverage of the huge amount of related work from the last 2-3 years is about as thorough as possible.\", \"Weaknesses\": \"My biggests concerns with the paper are its limited novelty a lack of proper ablations.\\n\\nNovelty:\\n\\nNone of the three mechanisms introduced by DrM are novel in-and-of themselves.\\n- As noted in the paper, \\\"dormant-ratio-guided perturbation\\\" follows the previous work of (D'Oro et al., 2023; Ash & Adams, 2020).\\n- Similarly, \\\"dormant-ratio-guided exploitation\\\" is based on the work of Ji et al. (2023).\\n- The idea of scheduling the exploration rate goes back a very long way, pre-dating deep RL. Some recent approaches, e.g., Agent57 (Badia et al.), also use a form of adaptive exploration.\\n\\nWhat *is* new is the idea of using the dormant ratio to schedule the hyperparameters of these methods. I don't disagree that this is novel; however, it's rather limited novelty in my opinion.\\n\\nAblations:\\n\\nWhile Figure 10 contains some ablations, the three mechanisms are each ablated in their entirety. I'd prefer it if only the adaptive scheduling component were ablated for each method, since this is the novel part. In other words, for a fair comparison, DrM ought to be compared against DrQ + perturbation resets (with fixed $\\\\alpha$) + \\\"Blended Exploration and Exploitation operator\\\" (with fixed $\\\\lambda$).\\n\\nMoreover, the ablations only consider one domain (Adroit) and the results are aggregated across all Adroit tasks. I'd prefer to see more ablations in greater granularity. For me, they're the most important part of the experiments, since without them it's very hard to understand *why* the improvements work. (The argument that DrM lowers the dormant ratio and hence improves performance seems very \\\"chicken-egg\\\" to me; it's unclear which is the cause and which is the effect. Only the dormant-ratio-guided perturbations seem to directly target the dormant ratio.)\\n\\nMinor things:\\n- Section 3.1 is too unscientific, e.g., the claim \\\"This suggests that the dormant ratio acts as an intrinsic metric, influenced more by the diversity and relevance of the agent's behaviors than by its received rewards\\\". This is all based on one example, and the claim that RL agents learn relevant behaviours before their returns improve sounds dubious to me. Another too-bold claim is: \\\"As the dormant ratio captures the intrinsic characteristics of an agent's policy network, all the hyperparameters in this approach are robust to tasks and domains.\\\" This is never established, and in fact Table 1 in the Appendix suggests that some parameters of DrM require domain-specific tuning.\\n- The claim in the abstract that DrM has \\\"no broken seeds (76 seeds in total)\\\" is a little vague (what constitutes \\\"broken\\\"?) and it's never really returned to in the main body of the paper.\", \"Questions\": \"- In the \\\"dormant-ratio-guided exploitation\\\" section, it's mentioned that \\\"value underestimation occurs as the agent starts to acquire skills, given that $\\\\pi$ is sub-optimal\\\". I don't quite follow this. As the agent starts to improve, wouldn't we expect $\\\\pi$ to become less sub-optimal?\"}"}]}
{"paper_id": "LvlqEaXJbe", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The paper highlights the increasing use of digital images and computer vision in biological studies, covering areas such as evolutionary biology, ecology, and biodiversity. It notes the explosion of computational methods to extract biologically relevant information from images, yet most approaches so far have been tailored for specific tasks.\nGap: Current computer vision methods for biology are often bespoke and not easily adaptable to new contexts, questions, or datasets. This limitation results from these methods needing substantial expertise in machine learning and are typically designed to handle specific taxa or tasks.\nHypothesis: The authors propose the development of a foundation model, similar to those used in natural language processing, that can generalize across various biology tasks and datasets. This model should utilize the rich hierarchical taxonomy of biological data to support fine-grained classification tasks across the tree of life.\nMethod: The authors develop BioCLIP, a large-scale multimodal model trained on a new dataset called TreeOfLife-10M, which includes over 10 million diverse biology images. The model uses a CLIP-style multimodal contrastive learning approach, leveraging taxonomic structures to learn representations that generalize across taxa. They benchmark the model on several fine-grained classification tasks and perform intrinsic evaluations to assess its hierarchical representation.\nConclusion: BioCLIP significantly outperforms existing computer vision models in zero-shot and few-shot classification tasks. The model demonstrates strong generalization by learning a hierarchical representation that mirrors the tree of life, thus offering a versatile tool for biological research.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 6, \"Confidence\": 5, \"Strengths\": \"1. The dataset collected in this paper is so far the largest biological image dataset which consists of over 10M images and 0.5M categories. Such a dataset is potentially very useful for training large models that target biology-related vision understanding tasks.\\n2. This work provides a detailed analysis of the effectiveness of training CLIP-type models on the collected dataset.\", \"Weaknesses\": \"1. The technical contribution is limited. The main contribution lies in the dataset collection and benchmarking of existing methods. \\n2. The presentation can be further improved. For example, it would be useful to add some examples with annotations from the dataset.\", \"Questions\": \"Even though this work does not provide much technical novelty, it takes a first step to build a large vision model and curate a highly diverse dataset. The analysis of the text types for the training of BIOCLIP is also interesting.\"}"}]}
{"paper_id": "LYGHdwyXUb", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Multi-task Reinforcement Learning (MTRL) revolves around learning multiple tasks simultaneously, potentially improving sample efficiency by sharing information across tasks. Existing MTRL methods mostly focus on sharing data and parameters, but they often overlook behavior sharing, which involves the transfer of learned actions between tasks.\nGap: Existing behavior-sharing approaches in MTRL are limited because they directly imitate policies from other tasks. This approach can be suboptimal if different tasks require different actions for the same states. The challenge is to preserve task-specific optimality while leveraging behavior sharing across tasks.\nHypothesis: The authors propose a novel behavior-sharing formulation that selectively leverages policies from other tasks as the current task\u2019s behavioral policy, thereby improving learning efficiency and sample use across multiple tasks.\nMethod: The proposed framework, Q-switch Mixture of Policies (QMP), estimates the shareability between task policies and uses them as behavior proposals to collect training data. The method involves a Q-switch that assesses the quality of behaviors from other tasks based on the agent's current state and training progress. Only those behaviors deemed beneficial are selectively incorporated. The approach was empirically tested on various MTRL environments involving manipulation, locomotion, and navigation tasks, demonstrating improved sample efficiency.\nConclusion: The QMP framework successfully improves sample efficiency by effectively identifying and sharing beneficial behaviors across tasks. The method is complementary to parameter-sharing strategies, and empirical results show enhanced performance in complex environments involving diverse tasks with potentially conflicting behaviors.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 3, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1.\\tThis paper is well-written and easy to follow\\n2.\\tSharing learned behaviors among tasks is an interesting and important topic in RL.\\n3.\\tAuthors conducted extensive experiments and analysis to empirically illustrate the effectiveness of QMP.\", \"Weaknesses\": \"1. Lack of theoretical analysis on the convergence (rate) of QMP.\\n2. Lack of intuition and detailed analysis on why the proposed method works (i.e., why could QMP make sharing desired behaviors among tasks possible? Please see Question 2 for my concern).\\n\\nPlease refer to my questions below for my concerns.\", \"Questions\": \"1. Why do you choose to roll out H steps instead of just one step after choosing one policy to collect data? QMP just uses the Q function under a particular state to choose the behavioral policy, which cannot guarantee that the selected behavior policy is helpful for the current task after stepping out of the considered state.\\n2. Will QMP cause undesired behaviors sharing? As I mentioned in Question 1, the selected behavioral policy will roll out for H(>1) steps, which may incurs sub-optimal behaviors.\\n3. Why choosing the behavioral policy based on the learned Q-function to collect data will essentially share desired behaviors among tasks? I understand that using Q-function as a proxy may help select more better actions. However, the Q-function may be biased and not learned well during training, which could even hurt the learning process.\\n4. Will QMP even slow down the training process? Say, the learned policy for the current task proposes an action, which is optimal but has an under-estimated Q function. Due to the biased Q function, QMP selects another policy to collect data, which chooses a sub-optimal action. Although the TD update will fix the estimated Q value of the sub-optimal action, it may be more efficient if we directly update the Q value of the optimal action, the thing that we really care about.\\n\\nI am willing to raise my scores if you could solve my concerns.\"}"}]}
{"paper_id": "3K3aWRpRNq", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: The field of 3D Structure-Based Drug Design (SBDD) utilizes the three-dimensional structures of target protein pockets to generate molecular geometric graphs. Geometric Diffusion Models (GDM) are considered state-of-the-art in this domain due to their ability to generate geometric graphs effectively. Recent advancements in deep learning have spurred the use of generative models that integrate protein structure information for informed molecular generation, which can accelerate drug discovery.\nGap: While GDMs are powerful in generating molecular structures, they often neglect crucial inter-molecular interactions like Van der Waals forces and hydrogen bonding due to their data-driven nature. This oversight can lead to atomic clashes where generated molecules are too close to protein structures, violating physical constraints and potentially resulting in chemically invalid products.\nHypothesis: The authors propose that by introducing a novel constrained sampling process that integrates non-convex constraints into the Langevin Dynamics of GDMs, they can prevent atomic clashes during molecular generation without altering the training process of the GDMs.\nMethod: The method involves incorporating a non-convex constraint within the Langevin Dynamics of GDM using proximal regularization techniques. This is done by defining a constraint function that prevents molecular atoms from approaching too closely to the protein surface (less than 2 \u00c5), which is then incorporated into the optimization process as a proximal regularization problem. The sampling process is modified using a Proximal Inexact Langevin Algorithm (PILA) to ensure molecular coordinates adhere to these constraints. The effectiveness of the method was empirically validated using datasets like CrossDock2020, with a focus on reducing atomic clash ratios without training modifications.\nConclusion: The proposed constrained sampling method significantly reduces atomic clashes in generated molecular structures compared to the original GDM processes. This approach enhances the stability of the drug design process while preserving the original generative model's performance, effectively addressing the shortcomings of existing diffusion models in generating clash-free molecular structures.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 1, \"Rating\": 3, \"Confidence\": 4, \"Strengths\": \"1. The problem formulation and presentation are clear. \\n2. The method is rationale and theoretical analysis is provided.\\n3. The proposed method improves the connectivity, mean CR, and stability of generated ligand molecules.\", \"Weaknesses\": \"1. This method is not novel to some extent, which is some kind of similar to the validity guidance proposed in DecompDiff.  \\n2. The experimental results on molecular properties of generated ligands are not satisfying. The improvement on Vina score is only marginal. It is notable that vina score is one of the most metrics for structure-based drug design and strongly related to intermolecular spatial interaction between pockets and generated ligands.\", \"Questions\": \"Can the author provide more comparison between the validity guidance proposed in DecompDiff and proximal sampling proposed in this paper? Can the vina score be further improved?\"}"}]}
{"paper_id": "r1IbewSnqq", "messages": [{"role": "system", "content": "You are a helpful assistant that extracts a structured summary from a paper. Please return only valid JSON with the following fields:\nSoundness, Presentation, Contribution, Rating, Confidence (all integers)\nStrengths, Weaknesses, and Questions (all strings).\nNo additional keys. No extra text."}, {"role": "user", "content": "Please read the following paper summary and produce reviewer scores.\n\n--- PAPER SUMMARY ---\n\nBackground: Federated Learning (FL) allows parties to collaboratively train a global model while keeping their local data private. Traditionally, the aggregation of local models in FL resembles a public decision-making process similar to elections, often using a simple weighted averaging method known as FedAvg. However, this aggregation method follows the one person one vote (1p1v) principle, making it vulnerable to poisoning attacks where malicious parties can influence the global model by adjusting their local model updates based on the amount of data they have shared.\nGap: The primary issue with FedAvg and similar methods is their susceptibility to poisoning attacks due to their reliance on the 1p1v principle, which allocates influence based on the size of data used in training rather than the reliability or trustworthiness of the participating parties. This can give undue influence to malicious parties, potentially skewing the global model.\nHypothesis: The authors propose that using a quadratic voting (QV) system in federated learning, termed FedQV, will mitigate the vulnerability to poisoning attacks by allowing parties to allocate more influence to updates they view as more valuable, with influence growing with the square root of updates instead of linearly as in 1p1v. The authors hypothesize that this approach not only discourages malicious behavior by limiting the impact of large datasets from malicious parties but also improves the overall robustness of model aggregation.\nMethod: The method involves adopting a quadratic voting mechanism where each party is allocated a voting budget. During each aggregation round, parties can spend a portion of their budget to influence the global model according to their preferences. The spending is designed such that the impact of votes is based on the square root of the number of credit votes spent. A truthfulness mechanism is added, using masked voting rules and budget limitations to prevent parties from predicting the impact of their votes, discouraging deceitful strategies. Unequal voting budgets are also used, where benign parties are allocated more influence based on a reputation score to further reduce the risk of poisoning attacks. The performance of the proposed FedQV method is tested against well-known real-world datasets and various state-of-the-art poisoning attacks.\nConclusion: The study concludes that FedQV offers improved robustness against poisoning attacks compared to traditional methods like FedAvg. The experiments demonstrate that FedQV can effectively limit the damage caused by malicious updates without sacrificing performance and can be seamlessly integrated with existing Byzantine-robust and privacy-preserving mechanisms to enhance its defense against both poisoning and privacy attacks. The proposed method is shown to consistently outperform FedAvg, especially under local model poisoning attacks, and the employment of reputation-based voting budgets further increases its effectiveness.\n\n\n--- REQUIRED CATEGORIES ---\nWe need the following fields in the output:\n1) Soundness\n2) Presentation\n3) Contribution\n4) Rating\n5) Confidence\n6) Strengths\n7) Weaknesses\n8) Questions\n\n--- POSSIBLE VALUES & DEFINITIONS ---\n\nSoundness, Presentation, and Contribution (1\u20134):\n- 1: poor\n- 2: fair\n- 3: good\n- 4: excellent\n\nRating (1\u201310):\n- 1: Trivial or wrong\n- 2: Strong rejection\n- 3: Clear rejection\n- 4: Ok but not good enough - rejection\n- 5: Marginally below acceptance threshold\n- 6: Marginally above acceptance threshold\n- 7: Good paper, accept\n- 8: Top 50% of accepted papers, clear accept\n- 9: Top 15% of accepted papers, strong accept\n- 10: Top 5% of accepted papers, seminal paper\n\nConfidence (1\u20135):\n- 1: The reviewer's evaluation is an educated guess\n- 2: The reviewer is willing to defend the evaluation, but might be wrong\n- 3: The reviewer is fairly confident that the evaluation is correct\n- 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n- 5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the topic\n\n--- OUTPUT FORMAT ---\nPlease produce only a JSON object with this exact structure (no extra keys):\n{\n  \"Soundness\": <int>,\n  \"Presentation\": <int>,\n  \"Contribution\": <int>,\n  \"Rating\": <int>,\n  \"Confidence\": <int>,\n  \"Strengths\": \"<string>\",\n  \"Weaknesses\": \"<string>\",\n  \"Questions\": \"<string>\"\n}\n\nNo explanations or markdown formatting. Only valid JSON."}, {"role": "assistant", "content": "{\"Soundness\": 2, \"Presentation\": 2, \"Contribution\": 2, \"Rating\": 3, \"Confidence\": 3, \"Strengths\": \"The paper proposes novel ideas by taking techniques from different domains.\\n\\nI think that the inclusion of incentive-based approaches to deal with corrupted parties is currently less explored that other techniques and can potentially provide new means to understand which attacks are realistic. Therefore, this is a valuable aspect of the contribution. \\n\\nOther ideas such as the integration of robust voting techniques and the use client-side measures are original and constructive in the discussion of byzantine resilience. In particular, techniques that are compatible with privacy enhancing technologies such as secure aggregation are valuable, especially given that an important use case of federated learning is privacy preserving machine learning.\", \"Weaknesses\": \"However, the contribution presents major problems which are listed below: \\n\\n1- The threat model of the paper is not clear. A well defined threat model is crucial when studying defenses against active adversaries. In the current contribution, I do not see what are the adversaries' possible behaviors and what are they willing to risk to harm the model (e.g. are they able to collude?). Therefore, it is not clear in practice in which scenarios would the truthfulness and convergence properties hold. This makes hard to evaluate the real impact of the contribution. \\n\\nFor example, consider an adversary which controls many parties and each party sends a harmful update in a different round. The adversary is willing to sacrifice parties to harm the model. Would this affect the truthfulness property? \\n\\n2- Clarity. The contribution is a combination of many ideas taken from different domains. However, the inclusion of each ingredient is not properly justified:\\n\\n2a- The paper introduces quadratic voting (QV) as a defense to reduce the impact of malicious users that falsely claim to have large datasets. However, the paper does not show how this kind of attack can affect other current Federated Learning defenses in practice and how QV is an effective defense. Moreover, the paper does not evaluate the resilience against any attack in which the adversary falsely claims to have a large dataset. Therefore, I am not sure if the initial motivation of QV is present in practice. \\n\\n2b- The paper introduces adaptive budgets as a reputation system. The original FedQV (Algorithm 1) already seems to penalize contributions. Therefore, the reputation protocol (Algorithm 2) appears a bit redundant. In the following sections of the paper, it is not clear when FedQV-Alg1 and FedQV-Alg2 are applied. \\n\\n2c- Compatibility with privacy enhancing primitives. In Section 3.2, the paper claims that the compatibility with privacy enhancing primitives (Secure Aggregation, Fully Homomorphic Encryption, Differential Privacy) is one of its main benefits of FedQV. However, I do not see a clear compatibility. For example, in Secure Aggregation the aggregation is not done by the server. Therefore, it not clear (i) how can the server adjust the weights of the aggregation properly and (ii) how can clients share similarity scores and local dataset size with the server without breaking the privacy guarantees. \\n\\n2d- Meaning of the truthfulness property: Section 4.2 claims that any possible corrupted contribution diminishes the probability of harm as server penalties reduce the influence of corrupted parties. This gives the impression that being honest is better for local utility than performing an attack. However, this is not what we can see empirically in section 5.  \\n\\n3- Evaluation of FedQV:  the proposed protocol claims to be easy to integrate with other resilience measures because similarities are computed in the client side. As said, this is an interesting feature, but is particularly vulnerable to client-side manipulations of the similarity scores and dataset sizes. It is true that protocol is evaluated against QV-adaptive, an attack that exploits this vulnerability. However, delegating the control of the defenses to possibly malicious clients is a high risk to take. Therefore, its evaluation of this risk requires a more comprehensive treatment. \\n\\nFor example, an important aspect of the evaluation which I feel is missed is (as said in point 2a) the resilience against an attack in which the adversary falsely claims to have a large dataset.\", \"Questions\": \"Please elaborate on the weaknesses outlined above.\"}"}]}
